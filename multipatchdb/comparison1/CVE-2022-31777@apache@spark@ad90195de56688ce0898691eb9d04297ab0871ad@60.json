{
  "cve_id": "CVE-2022-31777",
  "cve_desc": "A stored cross-site scripting (XSS) vulnerability in Apache Spark 3.2.1 and earlier, and 3.3.0, allows remote attackers to execute arbitrary JavaScript in the web browser of a user, by including a malicious payload into the logs which would be returned in logs rendered in the UI.",
  "repo": "apache/spark",
  "patch_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
  "patch_info": {
    "commit_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
    "repo": "apache/spark",
    "commit_url": "https://github.com/apache/spark/commit/ad90195de56688ce0898691eb9d04297ab0871ad",
    "files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ],
    "message": "[SPARK-39505][UI] Escape log content rendered in UI\n\n### What changes were proposed in this pull request?\n\nEscape log content rendered to the UI.\n\n### Why are the changes needed?\n\nLog content may contain reserved characters or other code in the log and be misinterpreted in the UI as HTML.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nExisting tests\n\nCloses #36902 from srowen/LogViewEscape.\n\nAuthored-by: Sean Owen <srowen@gmail.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
    "before_after_code_files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ]
  },
  "patch_diff": {
    "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js": [
      "File: core/src/main/resources/org/apache/spark/ui/static/log-view.js -> core/src/main/resources/org/apache/spark/ui/static/log-view.js",
      "--- Hunk 1 ---",
      "[Context before]",
      "85:       if (retStartByte == 0) {",
      "86:         disableMoreButton();",
      "87:       }",
      "90:       curLogLength = curLogLength + (startByte - retStartByte);",
      "91:       startByte = retStartByte;",
      "",
      "[Removed Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(cleanData);",
      "",
      "[Added Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(document.createTextNode(cleanData));",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "115:             var retLogLength = dataInfo[2];",
      "117:             var cleanData = data.substring(newlineIndex + 1);",
      "120:             curLogLength = curLogLength + (retEndByte - retStartByte);",
      "121:             endByte = retEndByte;",
      "",
      "[Removed Lines]",
      "118:             $(\"pre\", \".log-content\").append(cleanData);",
      "",
      "[Added Lines]",
      "118:             $(\"pre\", \".log-content\").append(document.createTextNode(cleanData));",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "17c56fc03b8e7269b293d6957c542eab9d723d52",
      "candidate_info": {
        "commit_hash": "17c56fc03b8e7269b293d6957c542eab9d723d52",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/17c56fc03b8e7269b293d6957c542eab9d723d52",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/NestedColumnAliasing.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ColumnPruningSuite.scala"
        ],
        "message": "[SPARK-38531][SQL] Fix the condition of \"Prune unrequired child index\" branch of ColumnPruning\n\n### What changes were proposed in this pull request?\n\nThe \"prune unrequired references\" branch has the condition:\n\n`case p  Project(_, g: Generate) if p.references != g.outputSet => `\n\nThis is wrong as generators like Inline will always enter this branch as long as it does not use all the generator output.\n\nExample:\n\ninput: <col1: array<struct<a: struct<a: int, b: int>, b: int>>>\n\nProject(a.a as x)\n\\- Generate(Inline(col1), ..., a, b)\n\np.references is [a]\ng.outputSet is [a, b]\n\nThis bug makes us never enter the GeneratorNestedColumnAliasing branch below thus miss some optimization opportunities. This PR changes the condition to check whether the child output is not used by the project and it is either not used by the generator or not already put into unrequiredChildOutput.\n\n### Why are the changes needed?\nThe wrong condition prevents some expressions like Inline, PosExplode from being optimized by rules after it. Before the PR, the test query added in the PR is not optimized since the optimization rule is not able to apply to it. After the PR the optimization rule can be correctly applied to the query.\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\nUnit tests.\n\nCloses #35864 from minyyy/gnca_wrong_cond.\n\nAuthored-by: minyyy <min.yang@databricks.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\n(cherry picked from commit 4b9343593eca780ca30ffda45244a71413577884)\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/NestedColumnAliasing.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/NestedColumnAliasing.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ColumnPruningSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ColumnPruningSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/NestedColumnAliasing.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/NestedColumnAliasing.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/NestedColumnAliasing.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/NestedColumnAliasing.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "312:   }",
          "313: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "315: object GeneratorUnrequiredChildrenPruning {",
          "316:   def unapply(plan: LogicalPlan): Option[LogicalPlan] = plan match {",
          "317:     case p @ Project(_, g: Generate) =>",
          "318:       val requiredAttrs = p.references ++ g.generator.references",
          "319:       val newChild = ColumnPruning.prunedChild(g.child, requiredAttrs)",
          "320:       val unrequired = g.generator.references -- p.references",
          "321:       val unrequiredIndices = newChild.output.zipWithIndex.filter(t => unrequired.contains(t._1))",
          "322:         .map(_._2)",
          "323:       if (!newChild.fastEquals(g.child) ||",
          "324:         unrequiredIndices.toSet != g.unrequiredChildIndex.toSet) {",
          "325:         Some(p.copy(child = g.copy(child = newChild, unrequiredChildIndex = unrequiredIndices)))",
          "326:       } else {",
          "327:         None",
          "328:       }",
          "329:     case _ => None",
          "330:   }",
          "331: }",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "831:       e.copy(child = prunedChild(child, e.references))",
          "843:     case GeneratorNestedColumnAliasing(rewrittenPlan) => rewrittenPlan",
          "",
          "[Removed Lines]",
          "834:     case p @ Project(_, g: Generate) if p.references != g.outputSet =>",
          "835:       val requiredAttrs = p.references -- g.producedAttributes ++ g.generator.references",
          "836:       val newChild = prunedChild(g.child, requiredAttrs)",
          "837:       val unrequired = g.generator.references -- p.references",
          "838:       val unrequiredIndices = newChild.output.zipWithIndex.filter(t => unrequired.contains(t._1))",
          "839:         .map(_._2)",
          "840:       p.copy(child = g.copy(child = newChild, unrequiredChildIndex = unrequiredIndices))",
          "",
          "[Added Lines]",
          "839:     case GeneratorUnrequiredChildrenPruning(rewrittenPlan) => rewrittenPlan",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "897:   })",
          "901:     if (!c.outputSet.subsetOf(allReferences)) {",
          "902:       Project(c.output.filter(allReferences.contains), c)",
          "903:     } else {",
          "",
          "[Removed Lines]",
          "900:   private def prunedChild(c: LogicalPlan, allReferences: AttributeSet) =",
          "",
          "[Added Lines]",
          "899:   def prunedChild(c: LogicalPlan, allReferences: AttributeSet): LogicalPlan =",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ColumnPruningSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ColumnPruningSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ColumnPruningSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ColumnPruningSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "24: import org.apache.spark.sql.catalyst.dsl.plans._",
          "25: import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder",
          "26: import org.apache.spark.sql.catalyst.expressions._",
          "27: import org.apache.spark.sql.catalyst.plans.{Inner, PlanTest}",
          "28: import org.apache.spark.sql.catalyst.plans.logical._",
          "29: import org.apache.spark.sql.catalyst.rules.RuleExecutor",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "27: import org.apache.spark.sql.catalyst.optimizer.NestedColumnAliasingSuite.collectGeneratedAliases",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "459:     val correctAnswer1 = Project(Seq('a), input).analyze",
          "460:     comparePlans(Optimize.execute(plan1.analyze), correctAnswer1)",
          "461:   }",
          "462: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "464:   test(\"SPARK-38531: Nested field pruning for Project and PosExplode\") {",
          "465:     val name = StructType.fromDDL(\"first string, middle string, last string\")",
          "466:     val employer = StructType.fromDDL(\"id int, company struct<name:string, address:string>\")",
          "467:     val contact = LocalRelation(",
          "468:       'id.int,",
          "469:       'name.struct(name),",
          "470:       'address.string,",
          "471:       'friends.array(name),",
          "472:       'relatives.map(StringType, name),",
          "473:       'employer.struct(employer))",
          "475:     val query = contact",
          "476:       .select('id, 'friends)",
          "477:       .generate(PosExplode('friends))",
          "478:       .select('col.getField(\"middle\"))",
          "479:       .analyze",
          "480:     val optimized = Optimize.execute(query)",
          "482:     val aliases = collectGeneratedAliases(optimized)",
          "484:     val expected = contact",
          "486:       .select(",
          "487:         'friends.getField(\"middle\").as(aliases(0)))",
          "488:       .generate(PosExplode($\"${aliases(0)}\"),",
          "489:         unrequiredChildIndex = Seq(0)) // unrequiredChildIndex is added.",
          "490:       .select('col.as(\"col.middle\"))",
          "491:       .analyze",
          "492:     comparePlans(optimized, expected)",
          "493:   }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "e52b0487583314ae159dab3496be3c28df3e56b7",
      "candidate_info": {
        "commit_hash": "e52b0487583314ae159dab3496be3c28df3e56b7",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/e52b0487583314ae159dab3496be3c28df3e56b7",
        "files": [
          "sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/InMemoryRelation.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/columnar/InMemoryColumnarQuerySuite.scala"
        ],
        "message": "[SPARK-39104][SQL] InMemoryRelation#isCachedColumnBuffersLoaded should be thread-safe\n\n### What changes were proposed in this pull request?\n\nAdd `synchronized` on method `isCachedColumnBuffersLoaded`\n\n### Why are the changes needed?\n\n`isCachedColumnBuffersLoaded` should has `synchronized` wrapped, otherwise may cause NPE when modify `_cachedColumnBuffers` concurrently.\n\n```\ndef isCachedColumnBuffersLoaded: Boolean = {\n  _cachedColumnBuffers != null && isCachedRDDLoaded\n}\n\ndef isCachedRDDLoaded: Boolean = {\n    _cachedColumnBuffersAreLoaded || {\n      val bmMaster = SparkEnv.get.blockManager.master\n      val rddLoaded = _cachedColumnBuffers.partitions.forall { partition =>\n        bmMaster.getBlockStatus(RDDBlockId(_cachedColumnBuffers.id, partition.index), false)\n          .exists { case(_, blockStatus) => blockStatus.isCached }\n      }\n      if (rddLoaded) {\n        _cachedColumnBuffersAreLoaded = rddLoaded\n      }\n      rddLoaded\n  }\n}\n```\n\n```\njava.lang.NullPointerException\n    at org.apache.spark.sql.execution.columnar.CachedRDDBuilder.isCachedRDDLoaded(InMemoryRelation.scala:247)\n    at org.apache.spark.sql.execution.columnar.CachedRDDBuilder.isCachedColumnBuffersLoaded(InMemoryRelation.scala:241)\n    at org.apache.spark.sql.execution.CacheManager.$anonfun$uncacheQuery$8(CacheManager.scala:189)\n    at org.apache.spark.sql.execution.CacheManager.$anonfun$uncacheQuery$8$adapted(CacheManager.scala:176)\n    at scala.collection.TraversableLike.$anonfun$filterImpl$1(TraversableLike.scala:304)\n    at scala.collection.Iterator.foreach(Iterator.scala:943)\n    at scala.collection.Iterator.foreach$(Iterator.scala:943)\n    at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n    at scala.collection.IterableLike.foreach(IterableLike.scala:74)\n    at scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n    at scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n    at scala.collection.TraversableLike.filterImpl(TraversableLike.scala:303)\n    at scala.collection.TraversableLike.filterImpl$(TraversableLike.scala:297)\n    at scala.collection.AbstractTraversable.filterImpl(Traversable.scala:108)\n    at scala.collection.TraversableLike.filter(TraversableLike.scala:395)\n    at scala.collection.TraversableLike.filter$(TraversableLike.scala:395)\n    at scala.collection.AbstractTraversable.filter(Traversable.scala:108)\n    at org.apache.spark.sql.execution.CacheManager.recacheByCondition(CacheManager.scala:219)\n    at org.apache.spark.sql.execution.CacheManager.uncacheQuery(CacheManager.scala:176)\n    at org.apache.spark.sql.Dataset.unpersist(Dataset.scala:3220)\n    at org.apache.spark.sql.Dataset.unpersist(Dataset.scala:3231)\n```\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nExisting UT.\n\nCloses #36496 from pan3793/SPARK-39104.\n\nAuthored-by: Cheng Pan <chengpan@apache.org>\nSigned-off-by: Sean Owen <srowen@gmail.com>\n(cherry picked from commit 3c8d8d7a864281fbe080316ad8de9b8eac80fa71)\nSigned-off-by: Sean Owen <srowen@gmail.com>",
        "before_after_code_files": [
          "sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/InMemoryRelation.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/InMemoryRelation.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/columnar/InMemoryColumnarQuerySuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/columnar/InMemoryColumnarQuerySuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/InMemoryRelation.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/InMemoryRelation.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/InMemoryRelation.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/InMemoryRelation.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "238:   }",
          "240:   def isCachedColumnBuffersLoaded: Boolean = {",
          "242:   }",
          "245:       _cachedColumnBuffersAreLoaded || {",
          "246:         val bmMaster = SparkEnv.get.blockManager.master",
          "247:         val rddLoaded = _cachedColumnBuffers.partitions.forall { partition =>",
          "",
          "[Removed Lines]",
          "241:     _cachedColumnBuffers != null && isCachedRDDLoaded",
          "244:   def isCachedRDDLoaded: Boolean = {",
          "",
          "[Added Lines]",
          "241:     if (_cachedColumnBuffers != null) {",
          "242:       synchronized {",
          "243:         return _cachedColumnBuffers != null && isCachedRDDLoaded",
          "244:       }",
          "245:     }",
          "246:     false",
          "249:   private def isCachedRDDLoaded: Boolean = {",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/execution/columnar/InMemoryColumnarQuerySuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/columnar/InMemoryColumnarQuerySuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/execution/columnar/InMemoryColumnarQuerySuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/execution/columnar/InMemoryColumnarQuerySuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: import java.nio.charset.StandardCharsets",
          "21: import java.sql.{Date, Timestamp}",
          "23: import org.apache.spark.rdd.RDD",
          "24: import org.apache.spark.sql.{DataFrame, QueryTest, Row}",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "22: import java.util.concurrent.atomic.AtomicInteger",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "563:       }",
          "564:     }",
          "565:   }",
          "566: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "568:   test(\"SPARK-39104: InMemoryRelation#isCachedColumnBuffersLoaded should be thread-safe\") {",
          "569:     val plan = spark.range(1).queryExecution.executedPlan",
          "570:     val serializer = new TestCachedBatchSerializer(true, 1)",
          "571:     val cachedRDDBuilder = CachedRDDBuilder(serializer, MEMORY_ONLY, plan, None)",
          "573:     @volatile var isCachedColumnBuffersLoaded = false",
          "574:     @volatile var stopped = false",
          "576:     val th1 = new Thread {",
          "577:       override def run(): Unit = {",
          "578:         while (!isCachedColumnBuffersLoaded && !stopped) {",
          "579:           cachedRDDBuilder.cachedColumnBuffers",
          "580:           cachedRDDBuilder.clearCache()",
          "581:         }",
          "582:       }",
          "583:     }",
          "585:     val th2 = new Thread {",
          "586:       override def run(): Unit = {",
          "587:         while (!isCachedColumnBuffersLoaded && !stopped) {",
          "588:           isCachedColumnBuffersLoaded = cachedRDDBuilder.isCachedColumnBuffersLoaded",
          "589:         }",
          "590:       }",
          "591:     }",
          "593:     val th3 = new Thread {",
          "594:       override def run(): Unit = {",
          "595:         Thread.sleep(3000L)",
          "596:         stopped = true",
          "597:       }",
          "598:     }",
          "600:     val exceptionCnt = new AtomicInteger",
          "601:     val exceptionHandler: Thread.UncaughtExceptionHandler = (_: Thread, cause: Throwable) => {",
          "602:         exceptionCnt.incrementAndGet",
          "603:         fail(cause)",
          "604:       }",
          "606:     th1.setUncaughtExceptionHandler(exceptionHandler)",
          "607:     th2.setUncaughtExceptionHandler(exceptionHandler)",
          "608:     th1.start()",
          "609:     th2.start()",
          "610:     th3.start()",
          "611:     th1.join()",
          "612:     th2.join()",
          "613:     th3.join()",
          "615:     cachedRDDBuilder.clearCache()",
          "617:     assert(exceptionCnt.get == 0)",
          "618:   }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "301d6e39e22297b3771ef502c3204c5fd84f2f9f",
      "candidate_info": {
        "commit_hash": "301d6e39e22297b3771ef502c3204c5fd84f2f9f",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/301d6e39e22297b3771ef502c3204c5fd84f2f9f",
        "files": [
          "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2StrategySuite.scala"
        ],
        "message": "[SPARK-39857][SQL][TESTS][FOLLOW-UP] Make \"translate complex expression\" pass with ANSI mode on\n\n### What changes were proposed in this pull request?\n\nThis PR fixes `translate complex expression` to pass with ANSI mode on. We do push `Abs` with ANSI mode on (https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/catalyst/util/V2ExpressionBuilder.scala#L93):\n\n```\n[info] - translate complex expression *** FAILED *** (22 milliseconds)\n[info]   Expected None, but got Some((ABS(cint) - 2) <= 1) (DataSourceV2StrategySuite.scala:325)\n[info]   org.scalatest.exceptions.TestFailedException:\n[info]   at org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:472)\n[info]   at org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:471)\n[info]   at org.scalatest.funsuite.AnyFunSuite.newAssertionFailedException(AnyFunSuite.scala:1563)\n[info]   at org.scalatest.Assertions.assertResult(Assertions.scala:867)\n[info]   at org.scalatest.Assertions.assertResult$(Assertions.scala:863)\n[info]   at org.scalatest.funsuite.AnyFunSuite.assertResult(AnyFunSuite.scala:1563)\n[info]   at org.apache.spark.sql.execution.datasources.v2.DataSourceV2StrategySuite.testTranslateFilter(DataSourceV2StrategySuite.scala:325)\n[info]   at org.apache.spark.sql.execution.datasources.v2.DataSourceV2StrategySuite.$anonfun$new$4(DataSourceV2StrategySuite.scala:176)\n[info]   at org.apache.spark.sql.execution.datasources.v2.DataSourceV2StrategySuite.$anonfun$new$4$adapted(DataSourceV2StrategySuite.scala:170)\n[info]   at scala.collection.immutable.List.foreach(List.scala:431)\n[info]   at org.apache.spark.sql.execution.datasources.v2.DataSourceV2StrategySuite.$anonfun$new$3(DataSourceV2StrategySuite.scala:170)\n[info]   at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n[info]   at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)\n[info]   at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)\n[info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)\n[info]   at org.scalatest.Transformer.apply(Transformer.scala:22)\n[info]   at org.scalatest.Transformer.apply(Transformer.scala:20)\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:190)\n[info]   at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:204)\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:188)\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:200)\n[info]   at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:200)\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:182)\n[info]   at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(SparkFunSuite.scala:65)\n[info]   at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:234)\n[info]   at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227)\n[info]   at org.apache.spark.SparkFunSuite.runTest(SparkFunSuite.scala:65)\n```\n\nhttps://github.com/apache/spark/runs/7595362617?check_suite_focus=true\n\n### Why are the changes needed?\n\nTo make the build pass with ANSI mode on.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo, test-only.\n\n### How was this patch tested?\n\nManually ran the unittest with ANSI mode on.\n\nCloses #37349 from HyukjinKwon/SPARK-39857-followup.\n\nLead-authored-by: Hyukjin Kwon <gurwls223@apache.org>\nCo-authored-by: Hyukjin Kwon <gurwls223@gmail.com>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>\n(cherry picked from commit c211abe970d9e88fd25cd859ea729e630d9491a7)\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>",
        "before_after_code_files": [
          "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2StrategySuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2StrategySuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2StrategySuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2StrategySuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2StrategySuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2StrategySuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "161:       testTranslateFilter(LessThanOrEqual(",
          "167:       testTranslateFilter(Or(",
          "",
          "[Removed Lines]",
          "164:         Subtract(Abs(attrInt), 2), 1), None)",
          "",
          "[Added Lines]",
          "164:         Subtract(Abs(attrInt, failOnError = false), 2), 1), None)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "187:       testTranslateFilter(Or(",
          "188:         And(",
          "189:           GreaterThan(attrInt, 1),",
          "192:         ),",
          "193:         And(",
          "194:           GreaterThan(attrInt, 50),",
          "",
          "[Removed Lines]",
          "191:           LessThan(Abs(attrInt), 10)",
          "",
          "[Added Lines]",
          "191:           LessThan(Abs(attrInt, failOnError = false), 10)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "198:       testTranslateFilter(Not(And(",
          "199:         Or(",
          "200:           LessThanOrEqual(attrInt, 1),",
          "203:         ),",
          "204:         Or(",
          "205:           LessThanOrEqual(attrInt, 50),",
          "",
          "[Removed Lines]",
          "202:           GreaterThanOrEqual(Abs(attrInt), 10)",
          "",
          "[Added Lines]",
          "202:           GreaterThanOrEqual(Abs(attrInt, failOnError = false), 10)",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "228:       testTranslateFilter(Or(",
          "229:         Or(",
          "230:           EqualTo(attrInt, 1),",
          "233:         ),",
          "234:         Or(",
          "235:           GreaterThan(attrInt, 0),",
          "",
          "[Removed Lines]",
          "232:           EqualTo(Abs(attrInt), 10)",
          "",
          "[Added Lines]",
          "232:           EqualTo(Abs(attrInt, failOnError = false), 10)",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "264:           LessThan(attrInt, 10)",
          "265:         ),",
          "266:         And(",
          "269:           IsNotNull(attrInt))), None)",
          "",
          "[Removed Lines]",
          "268:           EqualTo(Abs(attrInt), 6),",
          "",
          "[Added Lines]",
          "268:           EqualTo(Abs(attrInt, failOnError = false), 6),",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "294:           LessThan(attrInt, 10)",
          "295:         ),",
          "296:         Or(",
          "299:           IsNotNull(attrInt))), None)",
          "300:     }",
          "301:   }",
          "",
          "[Removed Lines]",
          "298:           EqualTo(Abs(attrInt), 6),",
          "",
          "[Added Lines]",
          "298:           EqualTo(Abs(attrInt, failOnError = false), 6),",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "5f396538bfcba7de7ab12dfa7621e7976f63fa92",
      "candidate_info": {
        "commit_hash": "5f396538bfcba7de7ab12dfa7621e7976f63fa92",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/5f396538bfcba7de7ab12dfa7621e7976f63fa92",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/datetimeExpressions.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/DateExpressionsSuite.scala"
        ],
        "message": "[SPARK-38990][SQL] Avoid `NullPointerException` when evaluating date_trunc/trunc format as a bound reference\n\n### What changes were proposed in this pull request?\n\nChange `TruncInstant.evalHelper` to pass the input row to `format.eval` when `format` is a not a literal (and therefore might be a bound reference).\n\n### Why are the changes needed?\n\nThis query fails with a `java.lang.NullPointerException`:\n```\nselect date_trunc(col1, col2)\nfrom values\n('week', timestamp'2012-01-01')\nas data(col1, col2);\n```\nThis only happens if the data comes from an inline table. When the source is an inline table, `ConvertToLocalRelation` attempts to evaluate the function against the data in interpreted mode.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nUpdate to unit tests.\n\nCloses #36312 from bersprockets/date_trunc_issue.\n\nAuthored-by: Bruce Robbins <bersprockets@gmail.com>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>\n(cherry picked from commit 2e4f4abf553cedec1fa8611b9494a01d24e6238a)\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/datetimeExpressions.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/datetimeExpressions.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/DateExpressionsSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/DateExpressionsSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/datetimeExpressions.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/datetimeExpressions.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/datetimeExpressions.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/datetimeExpressions.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "2114:     val level = if (format.foldable) {",
          "2115:       truncLevel",
          "2116:     } else {",
          "2118:     }",
          "2119:     if (level < minLevel) {",
          "",
          "[Removed Lines]",
          "2117:       DateTimeUtils.parseTruncLevel(format.eval().asInstanceOf[UTF8String])",
          "",
          "[Added Lines]",
          "2117:       DateTimeUtils.parseTruncLevel(format.eval(input).asInstanceOf[UTF8String])",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/DateExpressionsSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/DateExpressionsSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/DateExpressionsSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/DateExpressionsSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "30: import scala.util.Random",
          "32: import org.apache.spark.{SparkFunSuite, SparkUpgradeException}",
          "34: import org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection",
          "35: import org.apache.spark.sql.catalyst.util.{DateTimeUtils, IntervalUtils, TimestampFormatter}",
          "36: import org.apache.spark.sql.catalyst.util.DateTimeConstants._",
          "",
          "[Removed Lines]",
          "33: import org.apache.spark.sql.catalyst.InternalRow",
          "",
          "[Added Lines]",
          "33: import org.apache.spark.sql.catalyst.{CatalystTypeConverters, InternalRow}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "754:     checkEvaluation(",
          "755:       TruncDate(Literal.create(input, DateType), NonFoldableLiteral.create(fmt, StringType)),",
          "756:       expected)",
          "757:   }",
          "759:   test(\"TruncDate\") {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "758:     val catalystInput = CatalystTypeConverters.convertToCatalyst(input)",
          "759:     val inputRow = InternalRow(catalystInput, UTF8String.fromString(fmt))",
          "760:     checkEvaluation(",
          "761:       TruncDate(",
          "762:         BoundReference(ordinal = 0, dataType = DateType, nullable = true),",
          "763:         BoundReference(ordinal = 1, dataType = StringType, nullable = true)),",
          "764:       expected,",
          "765:       inputRow)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "780:       TruncTimestamp(",
          "781:         NonFoldableLiteral.create(fmt, StringType), Literal.create(input, TimestampType)),",
          "782:       expected)",
          "783:   }",
          "785:   test(\"TruncTimestamp\") {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "793:     val catalystInput = CatalystTypeConverters.convertToCatalyst(input)",
          "794:     val inputRow = InternalRow(UTF8String.fromString(fmt), catalystInput)",
          "795:     checkEvaluation(",
          "796:       TruncTimestamp(",
          "797:         BoundReference(ordinal = 0, dataType = StringType, nullable = true),",
          "798:         BoundReference(ordinal = 1, dataType = TimestampType, nullable = true)),",
          "799:       expected,",
          "800:       inputRow)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "de3673e02b620c00c741ae313ab5f40e56603515",
      "candidate_info": {
        "commit_hash": "de3673e02b620c00c741ae313ab5f40e56603515",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/de3673e02b620c00c741ae313ab5f40e56603515",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/EquivalentExpressions.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/conditionalExpressions.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/nullExpressions.scala"
        ],
        "message": "[SPARK-39105][SQL] Add ConditionalExpression trait\n\n### What changes were proposed in this pull request?\n\nAdd `ConditionalExpression` trait.\n\n### Why are the changes needed?\n\nFor developers, if a custom conditional like expression contains common sub expression then the evaluation order may be changed since Spark will pull out and eval the common sub expressions first during execution.\n\nAdd ConditionalExpression trait is friendly for developers.\n\n### Does this PR introduce _any_ user-facing change?\n\nno, add a new trait\n\n### How was this patch tested?\n\nPass existed test\n\nCloses #36455 from ulysses-you/SPARK-39105.\n\nAuthored-by: ulysses-you <ulyssesyou18@gmail.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\n(cherry picked from commit fa86a078bb7d57d7dbd48095fb06059a9bdd6c2e)\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/EquivalentExpressions.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/EquivalentExpressions.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/conditionalExpressions.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/conditionalExpressions.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/nullExpressions.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/nullExpressions.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/EquivalentExpressions.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/EquivalentExpressions.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/EquivalentExpressions.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/EquivalentExpressions.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "140:   private def childrenToRecurse(expr: Expression): Seq[Expression] = expr match {",
          "141:     case _: CodegenFallback => Nil",
          "146:     case other => other.children",
          "147:   }",
          "",
          "[Removed Lines]",
          "142:     case i: If => i.predicate :: Nil",
          "143:     case c: CaseWhen => c.children.head :: Nil",
          "144:     case c: Coalesce => c.children.head :: Nil",
          "145:     case n: NaNvl => n.left :: Nil",
          "",
          "[Added Lines]",
          "133:     case c: ConditionalExpression => c.alwaysEvaluatedInputs",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "151:   private def commonChildrenToRecurse(expr: Expression): Seq[Seq[Expression]] = expr match {",
          "152:     case _: CodegenFallback => Nil",
          "180:     case _ => Nil",
          "181:   }",
          "",
          "[Removed Lines]",
          "153:     case i: If => Seq(Seq(i.trueValue, i.falseValue))",
          "154:     case c: CaseWhen =>",
          "160:       val conditions = if (c.branches.length > 1) {",
          "161:         c.branches.map(_._1)",
          "162:       } else {",
          "165:         Nil",
          "166:       }",
          "169:       val values = if (c.elseValue.nonEmpty) {",
          "170:         c.branches.map(_._2) ++ c.elseValue",
          "171:       } else {",
          "172:         Nil",
          "173:       }",
          "175:       Seq(conditions, values)",
          "178:     case c: Coalesce if c.children.length > 1 => Seq(c.children)",
          "179:     case n: NaNvl => Seq(n.children)",
          "",
          "[Added Lines]",
          "141:     case c: ConditionalExpression => c.branchGroups",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "454:   protected def evalInternal(input: InternalRow): Any",
          "455: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "461: trait ConditionalExpression extends Expression {",
          "465:   def alwaysEvaluatedInputs: Seq[Expression]",
          "471:   def branchGroups: Seq[Seq[Expression]]",
          "472: }",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/conditionalExpressions.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/conditionalExpressions.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/conditionalExpressions.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/conditionalExpressions.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "37:   group = \"conditional_funcs\")",
          "39: case class If(predicate: Expression, trueValue: Expression, falseValue: Expression)",
          "42:   @transient",
          "43:   override lazy val inputTypesForMerging: Seq[DataType] = {",
          "",
          "[Removed Lines]",
          "40:   extends ComplexTypeMergingExpression with TernaryLike[Expression] {",
          "",
          "[Added Lines]",
          "40:   extends ComplexTypeMergingExpression with ConditionalExpression with TernaryLike[Expression] {",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "48:   override def second: Expression = trueValue",
          "49:   override def third: Expression = falseValue",
          "50:   override def nullable: Boolean = trueValue.nullable || falseValue.nullable",
          "52:   final override val nodePatterns : Seq[TreePattern] = Seq(IF)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "54:   override def alwaysEvaluatedInputs: Seq[Expression] = predicate :: Nil",
          "56:   override def branchGroups: Seq[Seq[Expression]] = Seq(Seq(trueValue, falseValue))",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "138: case class CaseWhen(",
          "139:     branches: Seq[(Expression, Expression)],",
          "140:     elseValue: Option[Expression] = None)",
          "143:   override def children: Seq[Expression] = branches.flatMap(b => b._1 :: b._2 :: Nil) ++ elseValue",
          "",
          "[Removed Lines]",
          "141:   extends ComplexTypeMergingExpression with Serializable {",
          "",
          "[Added Lines]",
          "147:   extends ComplexTypeMergingExpression with ConditionalExpression {",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "179:     }",
          "180:   }",
          "182:   override def eval(input: InternalRow): Any = {",
          "183:     var i = 0",
          "184:     val size = branches.size",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "192:   override def alwaysEvaluatedInputs: Seq[Expression] = children.head :: Nil",
          "194:   override def branchGroups: Seq[Seq[Expression]] = {",
          "200:     val conditions = if (branches.length > 1) {",
          "201:       branches.map(_._1)",
          "202:     } else {",
          "205:       Nil",
          "206:     }",
          "209:     val values = if (elseValue.nonEmpty) {",
          "210:       branches.map(_._2) ++ elseValue",
          "211:     } else {",
          "212:       Nil",
          "213:     }",
          "215:     Seq(conditions, values)",
          "216:   }",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/nullExpressions.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/nullExpressions.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/nullExpressions.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/nullExpressions.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "47:   since = \"1.0.0\",",
          "48:   group = \"conditional_funcs\")",
          "53:   override def nullable: Boolean = children.forall(_.nullable)",
          "",
          "[Removed Lines]",
          "50: case class Coalesce(children: Seq[Expression]) extends ComplexTypeMergingExpression {",
          "",
          "[Added Lines]",
          "50: case class Coalesce(children: Seq[Expression])",
          "51:   extends ComplexTypeMergingExpression with ConditionalExpression {",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "66:     }",
          "67:   }",
          "69:   override def eval(input: InternalRow): Any = {",
          "70:     var result: Any = null",
          "71:     val childIterator = children.iterator",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "73:   override def alwaysEvaluatedInputs: Seq[Expression] = children.head :: Nil",
          "75:   override def branchGroups: Seq[Seq[Expression]] = if (children.length > 1) {",
          "78:     Seq(children)",
          "79:   } else {",
          "80:     Nil",
          "81:   }",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "261:   since = \"1.5.0\",",
          "262:   group = \"conditional_funcs\")",
          "263: case class NaNvl(left: Expression, right: Expression)",
          "266:   override def dataType: DataType = left.dataType",
          "268:   override def inputTypes: Seq[AbstractDataType] =",
          "269:     Seq(TypeCollection(DoubleType, FloatType), TypeCollection(DoubleType, FloatType))",
          "271:   override def eval(input: InternalRow): Any = {",
          "272:     val value = left.eval(input)",
          "273:     if (value == null) {",
          "",
          "[Removed Lines]",
          "264:     extends BinaryExpression with ImplicitCastInputTypes {",
          "",
          "[Added Lines]",
          "278:     extends BinaryExpression with ConditionalExpression with ImplicitCastInputTypes {",
          "289:   override def alwaysEvaluatedInputs: Seq[Expression] = left :: Nil",
          "291:   override def branchGroups: Seq[Seq[Expression]] = Seq(children)",
          "",
          "---------------"
        ]
      }
    }
  ]
}