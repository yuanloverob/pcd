{
  "cve_id": "CVE-2023-50944",
  "cve_desc": "Apache Airflow, versions before 2.8.1, have a vulnerability that allows an authenticated user to access the source code of a DAG to which they don't have access.\u00a0This vulnerability is considered low since it requires an authenticated user to exploit it. Users are recommended to upgrade to version 2.8.1, which fixes this issue.",
  "repo": "apache/airflow",
  "patch_hash": "8d76538d6e105947272b000581c6fabec20146b1",
  "patch_info": {
    "commit_hash": "8d76538d6e105947272b000581c6fabec20146b1",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/8d76538d6e105947272b000581c6fabec20146b1",
    "files": [
      "airflow/api_connexion/endpoints/dag_source_endpoint.py",
      "airflow/models/dagcode.py",
      "tests/api_connexion/endpoints/test_dag_source_endpoint.py"
    ],
    "message": "Check DAG read permission before accessing DAG code (#36257)\n\n(cherry picked from commit 30ea37e0d247ce54c2d25b115e807fdb0074d795)",
    "before_after_code_files": [
      "airflow/api_connexion/endpoints/dag_source_endpoint.py||airflow/api_connexion/endpoints/dag_source_endpoint.py",
      "airflow/models/dagcode.py||airflow/models/dagcode.py",
      "tests/api_connexion/endpoints/test_dag_source_endpoint.py||tests/api_connexion/endpoints/test_dag_source_endpoint.py"
    ]
  },
  "patch_diff": {
    "airflow/api_connexion/endpoints/dag_source_endpoint.py||airflow/api_connexion/endpoints/dag_source_endpoint.py": [
      "File: airflow/api_connexion/endpoints/dag_source_endpoint.py -> airflow/api_connexion/endpoints/dag_source_endpoint.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "17: from __future__ import annotations",
      "19: from http import HTTPStatus",
      "21: from flask import Response, current_app, request",
      "22: from itsdangerous import BadSignature, URLSafeSerializer",
      "24: from airflow.api_connexion import security",
      "26: from airflow.api_connexion.schemas.dag_source_schema import dag_source_schema",
      "27: from airflow.auth.managers.models.resource_details import DagAccessEntity",
      "28: from airflow.models.dagcode import DagCode",
      "31: @security.requires_access_dag(\"GET\", DagAccessEntity.CODE)",
      "33:     \"\"\"Get source code using file token.\"\"\"",
      "34:     secret_key = current_app.config[\"SECRET_KEY\"]",
      "35:     auth_s = URLSafeSerializer(secret_key)",
      "36:     try:",
      "37:         path = auth_s.loads(file_token)",
      "39:     except (BadSignature, FileNotFoundError):",
      "40:         raise NotFound(\"Dag source not found\")",
      "",
      "[Removed Lines]",
      "25: from airflow.api_connexion.exceptions import NotFound",
      "32: def get_dag_source(*, file_token: str) -> Response:",
      "38:         dag_source = DagCode.code(path)",
      "",
      "[Added Lines]",
      "20: from typing import TYPE_CHECKING",
      "26: from airflow.api_connexion.exceptions import NotFound, PermissionDenied",
      "28: from airflow.api_connexion.security import get_readable_dags",
      "30: from airflow.models.dag import DagModel",
      "32: from airflow.utils.session import NEW_SESSION, provide_session",
      "34: if TYPE_CHECKING:",
      "35:     from sqlalchemy.orm import Session",
      "39: @provide_session",
      "40: def get_dag_source(*, file_token: str, session: Session = NEW_SESSION) -> Response:",
      "46:         dag_ids = session.query(DagModel.dag_id).filter(DagModel.fileloc == path).all()",
      "47:         readable_dags = get_readable_dags()",
      "48:         # Check if user has read access to all the DAGs defined in the file",
      "49:         if any(dag_id[0] not in readable_dags for dag_id in dag_ids):",
      "50:             raise PermissionDenied()",
      "51:         dag_source = DagCode.code(path, session=session)",
      "",
      "---------------"
    ],
    "airflow/models/dagcode.py||airflow/models/dagcode.py": [
      "File: airflow/models/dagcode.py -> airflow/models/dagcode.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "177:         return cls.code(fileloc)",
      "179:     @classmethod",
      "181:         \"\"\"Return source code for this DagCode object.",
      "183:         :return: source code as string",
      "184:         \"\"\"",
      "187:     @staticmethod",
      "188:     def _get_code_from_file(fileloc):",
      "",
      "[Removed Lines]",
      "180:     def code(cls, fileloc) -> str:",
      "185:         return cls._get_code_from_db(fileloc)",
      "",
      "[Added Lines]",
      "180:     @provide_session",
      "181:     def code(cls, fileloc, session: Session = NEW_SESSION) -> str:",
      "186:         return cls._get_code_from_db(fileloc, session)",
      "",
      "---------------"
    ],
    "tests/api_connexion/endpoints/test_dag_source_endpoint.py||tests/api_connexion/endpoints/test_dag_source_endpoint.py": [
      "File: tests/api_connexion/endpoints/test_dag_source_endpoint.py -> tests/api_connexion/endpoints/test_dag_source_endpoint.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "35: ROOT_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), os.pardir, os.pardir))",
      "36: EXAMPLE_DAG_FILE = os.path.join(\"airflow\", \"example_dags\", \"example_bash_operator.py\")",
      "39: @pytest.fixture(scope=\"module\")",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "37: EXAMPLE_DAG_ID = \"example_bash_operator\"",
      "38: TEST_DAG_ID = \"latest_only\"",
      "39: NOT_READABLE_DAG_ID = \"latest_only_with_trigger\"",
      "40: TEST_MULTIPLE_DAGS_ID = \"dataset_produces_1\"",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "45:         role_name=\"Test\",",
      "46:         permissions=[(permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG_CODE)],  # type: ignore",
      "47:     )",
      "48:     create_user(app, username=\"test_no_permissions\", role_name=\"TestNoPermissions\")  # type: ignore",
      "50:     yield app",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "52:     app.appbuilder.sm.sync_perm_for_dag(  # type: ignore",
      "53:         TEST_DAG_ID,",
      "54:         access_control={\"Test\": [permissions.ACTION_CAN_READ]},",
      "55:     )",
      "56:     app.appbuilder.sm.sync_perm_for_dag(  # type: ignore",
      "57:         EXAMPLE_DAG_ID,",
      "58:         access_control={\"Test\": [permissions.ACTION_CAN_READ]},",
      "59:     )",
      "60:     app.appbuilder.sm.sync_perm_for_dag(  # type: ignore",
      "61:         TEST_MULTIPLE_DAGS_ID,",
      "62:         access_control={\"Test\": [permissions.ACTION_CAN_READ]},",
      "63:     )",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "80:     def test_should_respond_200_text(self, url_safe_serializer):",
      "81:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
      "82:         dagbag.sync_to_db()",
      "87:         response = self.client.get(",
      "88:             url, headers={\"Accept\": \"text/plain\"}, environ_overrides={\"REMOTE_USER\": \"test\"}",
      "89:         )",
      "",
      "[Removed Lines]",
      "83:         first_dag: DAG = next(iter(dagbag.dags.values()))",
      "84:         dag_docstring = self._get_dag_file_docstring(first_dag.fileloc)",
      "86:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(first_dag.fileloc)}\"",
      "",
      "[Added Lines]",
      "99:         test_dag: DAG = dagbag.dags[TEST_DAG_ID]",
      "100:         dag_docstring = self._get_dag_file_docstring(test_dag.fileloc)",
      "102:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(test_dag.fileloc)}\"",
      "",
      "---------------",
      "--- Hunk 4 ---",
      "[Context before]",
      "95:     def test_should_respond_200_json(self, url_safe_serializer):",
      "96:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
      "97:         dagbag.sync_to_db()",
      "102:         response = self.client.get(",
      "103:             url, headers={\"Accept\": \"application/json\"}, environ_overrides={\"REMOTE_USER\": \"test\"}",
      "104:         )",
      "",
      "[Removed Lines]",
      "98:         first_dag: DAG = next(iter(dagbag.dags.values()))",
      "99:         dag_docstring = self._get_dag_file_docstring(first_dag.fileloc)",
      "101:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(first_dag.fileloc)}\"",
      "",
      "[Added Lines]",
      "114:         test_dag: DAG = dagbag.dags[TEST_DAG_ID]",
      "115:         dag_docstring = self._get_dag_file_docstring(test_dag.fileloc)",
      "117:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(test_dag.fileloc)}\"",
      "",
      "---------------",
      "--- Hunk 5 ---",
      "[Context before]",
      "110:     def test_should_respond_406(self, url_safe_serializer):",
      "111:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
      "112:         dagbag.sync_to_db()",
      "116:         response = self.client.get(",
      "117:             url, headers={\"Accept\": \"image/webp\"}, environ_overrides={\"REMOTE_USER\": \"test\"}",
      "118:         )",
      "",
      "[Removed Lines]",
      "113:         first_dag: DAG = next(iter(dagbag.dags.values()))",
      "115:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(first_dag.fileloc)}\"",
      "",
      "[Added Lines]",
      "129:         test_dag: DAG = dagbag.dags[TEST_DAG_ID]",
      "131:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(test_dag.fileloc)}\"",
      "",
      "---------------",
      "--- Hunk 6 ---",
      "[Context before]",
      "151:             environ_overrides={\"REMOTE_USER\": \"test_no_permissions\"},",
      "152:         )",
      "153:         assert response.status_code == 403",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "171:     def test_should_respond_403_not_readable(self, url_safe_serializer):",
      "172:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
      "173:         dagbag.sync_to_db()",
      "174:         dag: DAG = dagbag.dags[NOT_READABLE_DAG_ID]",
      "176:         response = self.client.get(",
      "177:             f\"/api/v1/dagSources/{url_safe_serializer.dumps(dag.fileloc)}\",",
      "178:             headers={\"Accept\": \"text/plain\"},",
      "179:             environ_overrides={\"REMOTE_USER\": \"test\"},",
      "180:         )",
      "181:         read_dag = self.client.get(",
      "182:             f\"/api/v1/dags/{NOT_READABLE_DAG_ID}\",",
      "183:             environ_overrides={\"REMOTE_USER\": \"test\"},",
      "184:         )",
      "185:         assert response.status_code == 403",
      "186:         assert read_dag.status_code == 403",
      "188:     def test_should_respond_403_some_dags_not_readable_in_the_file(self, url_safe_serializer):",
      "189:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
      "190:         dagbag.sync_to_db()",
      "191:         dag: DAG = dagbag.dags[TEST_MULTIPLE_DAGS_ID]",
      "193:         response = self.client.get(",
      "194:             f\"/api/v1/dagSources/{url_safe_serializer.dumps(dag.fileloc)}\",",
      "195:             headers={\"Accept\": \"text/plain\"},",
      "196:             environ_overrides={\"REMOTE_USER\": \"test\"},",
      "197:         )",
      "199:         read_dag = self.client.get(",
      "200:             f\"/api/v1/dags/{TEST_MULTIPLE_DAGS_ID}\",",
      "201:             environ_overrides={\"REMOTE_USER\": \"test\"},",
      "202:         )",
      "203:         assert response.status_code == 403",
      "204:         assert read_dag.status_code == 200",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "30ea37e0d247ce54c2d25b115e807fdb0074d795",
      "candidate_info": {
        "commit_hash": "30ea37e0d247ce54c2d25b115e807fdb0074d795",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/30ea37e0d247ce54c2d25b115e807fdb0074d795",
        "files": [
          "airflow/api_connexion/endpoints/dag_source_endpoint.py",
          "airflow/models/dagcode.py",
          "tests/api_connexion/endpoints/test_dag_source_endpoint.py"
        ],
        "message": "Check DAG read permission before accessing DAG code (#36257)",
        "before_after_code_files": [
          "airflow/api_connexion/endpoints/dag_source_endpoint.py||airflow/api_connexion/endpoints/dag_source_endpoint.py",
          "airflow/models/dagcode.py||airflow/models/dagcode.py",
          "tests/api_connexion/endpoints/test_dag_source_endpoint.py||tests/api_connexion/endpoints/test_dag_source_endpoint.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "diff_branch_cherry_pick": 1,
        "diff_branch_same_aad": 1,
        "olp_code_files": {
          "patch": [
            "airflow/api_connexion/endpoints/dag_source_endpoint.py||airflow/api_connexion/endpoints/dag_source_endpoint.py",
            "airflow/models/dagcode.py||airflow/models/dagcode.py",
            "tests/api_connexion/endpoints/test_dag_source_endpoint.py||tests/api_connexion/endpoints/test_dag_source_endpoint.py"
          ],
          "candidate": [
            "airflow/api_connexion/endpoints/dag_source_endpoint.py||airflow/api_connexion/endpoints/dag_source_endpoint.py",
            "airflow/models/dagcode.py||airflow/models/dagcode.py",
            "tests/api_connexion/endpoints/test_dag_source_endpoint.py||tests/api_connexion/endpoints/test_dag_source_endpoint.py"
          ]
        }
      },
      "candidate_diff": {
        "airflow/api_connexion/endpoints/dag_source_endpoint.py||airflow/api_connexion/endpoints/dag_source_endpoint.py": [
          "File: airflow/api_connexion/endpoints/dag_source_endpoint.py -> airflow/api_connexion/endpoints/dag_source_endpoint.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "17: from __future__ import annotations",
          "19: from http import HTTPStatus",
          "21: from flask import Response, current_app, request",
          "22: from itsdangerous import BadSignature, URLSafeSerializer",
          "24: from airflow.api_connexion import security",
          "26: from airflow.api_connexion.schemas.dag_source_schema import dag_source_schema",
          "27: from airflow.auth.managers.models.resource_details import DagAccessEntity",
          "28: from airflow.models.dagcode import DagCode",
          "31: @security.requires_access_dag(\"GET\", DagAccessEntity.CODE)",
          "33:     \"\"\"Get source code using file token.\"\"\"",
          "34:     secret_key = current_app.config[\"SECRET_KEY\"]",
          "35:     auth_s = URLSafeSerializer(secret_key)",
          "36:     try:",
          "37:         path = auth_s.loads(file_token)",
          "39:     except (BadSignature, FileNotFoundError):",
          "40:         raise NotFound(\"Dag source not found\")",
          "",
          "[Removed Lines]",
          "25: from airflow.api_connexion.exceptions import NotFound",
          "32: def get_dag_source(*, file_token: str) -> Response:",
          "38:         dag_source = DagCode.code(path)",
          "",
          "[Added Lines]",
          "20: from typing import TYPE_CHECKING",
          "26: from airflow.api_connexion.exceptions import NotFound, PermissionDenied",
          "28: from airflow.api_connexion.security import get_readable_dags",
          "30: from airflow.models.dag import DagModel",
          "32: from airflow.utils.session import NEW_SESSION, provide_session",
          "34: if TYPE_CHECKING:",
          "35:     from sqlalchemy.orm import Session",
          "39: @provide_session",
          "40: def get_dag_source(*, file_token: str, session: Session = NEW_SESSION) -> Response:",
          "46:         dag_ids = session.query(DagModel.dag_id).filter(DagModel.fileloc == path).all()",
          "47:         readable_dags = get_readable_dags()",
          "48:         # Check if user has read access to all the DAGs defined in the file",
          "49:         if any(dag_id[0] not in readable_dags for dag_id in dag_ids):",
          "50:             raise PermissionDenied()",
          "51:         dag_source = DagCode.code(path, session=session)",
          "",
          "---------------"
        ],
        "airflow/models/dagcode.py||airflow/models/dagcode.py": [
          "File: airflow/models/dagcode.py -> airflow/models/dagcode.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "177:         return cls.code(fileloc)",
          "179:     @classmethod",
          "181:         \"\"\"Return source code for this DagCode object.",
          "183:         :return: source code as string",
          "184:         \"\"\"",
          "187:     @staticmethod",
          "188:     def _get_code_from_file(fileloc):",
          "",
          "[Removed Lines]",
          "180:     def code(cls, fileloc) -> str:",
          "185:         return cls._get_code_from_db(fileloc)",
          "",
          "[Added Lines]",
          "180:     @provide_session",
          "181:     def code(cls, fileloc, session: Session = NEW_SESSION) -> str:",
          "186:         return cls._get_code_from_db(fileloc, session)",
          "",
          "---------------"
        ],
        "tests/api_connexion/endpoints/test_dag_source_endpoint.py||tests/api_connexion/endpoints/test_dag_source_endpoint.py": [
          "File: tests/api_connexion/endpoints/test_dag_source_endpoint.py -> tests/api_connexion/endpoints/test_dag_source_endpoint.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "35: ROOT_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), os.pardir, os.pardir))",
          "36: EXAMPLE_DAG_FILE = os.path.join(\"airflow\", \"example_dags\", \"example_bash_operator.py\")",
          "39: @pytest.fixture(scope=\"module\")",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "37: EXAMPLE_DAG_ID = \"example_bash_operator\"",
          "38: TEST_DAG_ID = \"latest_only\"",
          "39: NOT_READABLE_DAG_ID = \"latest_only_with_trigger\"",
          "40: TEST_MULTIPLE_DAGS_ID = \"dataset_produces_1\"",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "45:         role_name=\"Test\",",
          "46:         permissions=[(permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG_CODE)],  # type: ignore",
          "47:     )",
          "48:     create_user(app, username=\"test_no_permissions\", role_name=\"TestNoPermissions\")  # type: ignore",
          "50:     yield app",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "52:     app.appbuilder.sm.sync_perm_for_dag(  # type: ignore",
          "53:         TEST_DAG_ID,",
          "54:         access_control={\"Test\": [permissions.ACTION_CAN_READ]},",
          "55:     )",
          "56:     app.appbuilder.sm.sync_perm_for_dag(  # type: ignore",
          "57:         EXAMPLE_DAG_ID,",
          "58:         access_control={\"Test\": [permissions.ACTION_CAN_READ]},",
          "59:     )",
          "60:     app.appbuilder.sm.sync_perm_for_dag(  # type: ignore",
          "61:         TEST_MULTIPLE_DAGS_ID,",
          "62:         access_control={\"Test\": [permissions.ACTION_CAN_READ]},",
          "63:     )",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "80:     def test_should_respond_200_text(self, url_safe_serializer):",
          "81:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
          "82:         dagbag.sync_to_db()",
          "87:         response = self.client.get(",
          "88:             url, headers={\"Accept\": \"text/plain\"}, environ_overrides={\"REMOTE_USER\": \"test\"}",
          "89:         )",
          "",
          "[Removed Lines]",
          "83:         first_dag: DAG = next(iter(dagbag.dags.values()))",
          "84:         dag_docstring = self._get_dag_file_docstring(first_dag.fileloc)",
          "86:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(first_dag.fileloc)}\"",
          "",
          "[Added Lines]",
          "99:         test_dag: DAG = dagbag.dags[TEST_DAG_ID]",
          "100:         dag_docstring = self._get_dag_file_docstring(test_dag.fileloc)",
          "102:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(test_dag.fileloc)}\"",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "95:     def test_should_respond_200_json(self, url_safe_serializer):",
          "96:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
          "97:         dagbag.sync_to_db()",
          "102:         response = self.client.get(",
          "103:             url, headers={\"Accept\": \"application/json\"}, environ_overrides={\"REMOTE_USER\": \"test\"}",
          "104:         )",
          "",
          "[Removed Lines]",
          "98:         first_dag: DAG = next(iter(dagbag.dags.values()))",
          "99:         dag_docstring = self._get_dag_file_docstring(first_dag.fileloc)",
          "101:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(first_dag.fileloc)}\"",
          "",
          "[Added Lines]",
          "114:         test_dag: DAG = dagbag.dags[TEST_DAG_ID]",
          "115:         dag_docstring = self._get_dag_file_docstring(test_dag.fileloc)",
          "117:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(test_dag.fileloc)}\"",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "110:     def test_should_respond_406(self, url_safe_serializer):",
          "111:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
          "112:         dagbag.sync_to_db()",
          "116:         response = self.client.get(",
          "117:             url, headers={\"Accept\": \"image/webp\"}, environ_overrides={\"REMOTE_USER\": \"test\"}",
          "118:         )",
          "",
          "[Removed Lines]",
          "113:         first_dag: DAG = next(iter(dagbag.dags.values()))",
          "115:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(first_dag.fileloc)}\"",
          "",
          "[Added Lines]",
          "129:         test_dag: DAG = dagbag.dags[TEST_DAG_ID]",
          "131:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(test_dag.fileloc)}\"",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "151:             environ_overrides={\"REMOTE_USER\": \"test_no_permissions\"},",
          "152:         )",
          "153:         assert response.status_code == 403",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "171:     def test_should_respond_403_not_readable(self, url_safe_serializer):",
          "172:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
          "173:         dagbag.sync_to_db()",
          "174:         dag: DAG = dagbag.dags[NOT_READABLE_DAG_ID]",
          "176:         response = self.client.get(",
          "177:             f\"/api/v1/dagSources/{url_safe_serializer.dumps(dag.fileloc)}\",",
          "178:             headers={\"Accept\": \"text/plain\"},",
          "179:             environ_overrides={\"REMOTE_USER\": \"test\"},",
          "180:         )",
          "181:         read_dag = self.client.get(",
          "182:             f\"/api/v1/dags/{NOT_READABLE_DAG_ID}\",",
          "183:             environ_overrides={\"REMOTE_USER\": \"test\"},",
          "184:         )",
          "185:         assert response.status_code == 403",
          "186:         assert read_dag.status_code == 403",
          "188:     def test_should_respond_403_some_dags_not_readable_in_the_file(self, url_safe_serializer):",
          "189:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
          "190:         dagbag.sync_to_db()",
          "191:         dag: DAG = dagbag.dags[TEST_MULTIPLE_DAGS_ID]",
          "193:         response = self.client.get(",
          "194:             f\"/api/v1/dagSources/{url_safe_serializer.dumps(dag.fileloc)}\",",
          "195:             headers={\"Accept\": \"text/plain\"},",
          "196:             environ_overrides={\"REMOTE_USER\": \"test\"},",
          "197:         )",
          "199:         read_dag = self.client.get(",
          "200:             f\"/api/v1/dags/{TEST_MULTIPLE_DAGS_ID}\",",
          "201:             environ_overrides={\"REMOTE_USER\": \"test\"},",
          "202:         )",
          "203:         assert response.status_code == 403",
          "204:         assert read_dag.status_code == 200",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "8986f8f3beba11cf568e0f014883b997e4042f7b",
      "candidate_info": {
        "commit_hash": "8986f8f3beba11cf568e0f014883b997e4042f7b",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/8986f8f3beba11cf568e0f014883b997e4042f7b",
        "files": [
          "dev/breeze/src/airflow_breeze/params/shell_params.py",
          "dev/breeze/src/airflow_breeze/utils/docker_command_utils.py",
          "scripts/in_container/install_airflow_and_providers.py"
        ],
        "message": "Fix using extras when `--use-airflow-version` is used in Breeze (#36280)\n\nWhen we are installing a released version of Airflow in Breeze, we can\npass additional extras to install (For example, we need to pass celery\nextra in order to start airflow with celery executor.\n\nThe extras could be specified as:\n\n```\nbreeze start-airflow --use-airflow-version 2.8.0rc4  \\\n  --executor CeleryExecutor --airflow-extras \"celery\"\n\n```\n\nHowever recent refactors caused a problem that the extras added were\nspecified after version (which is rejected by newer versions of `pip`).\n\nThis PR fixes it and also moves the place where CeleryExecutor use\ntriggers adding celery extra when`--use-airflow-version` is used.\n\nThe warning about this is better visible when moving to Shell Params.\n\n(cherry picked from commit 329780649543ab7b9d593a2e2428073fbd4cf274)",
        "before_after_code_files": [
          "dev/breeze/src/airflow_breeze/params/shell_params.py||dev/breeze/src/airflow_breeze/params/shell_params.py",
          "dev/breeze/src/airflow_breeze/utils/docker_command_utils.py||dev/breeze/src/airflow_breeze/utils/docker_command_utils.py",
          "scripts/in_container/install_airflow_and_providers.py||scripts/in_container/install_airflow_and_providers.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/breeze/src/airflow_breeze/params/shell_params.py||dev/breeze/src/airflow_breeze/params/shell_params.py": [
          "File: dev/breeze/src/airflow_breeze/params/shell_params.py -> dev/breeze/src/airflow_breeze/params/shell_params.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "329:         if self.executor == \"CeleryExecutor\":",
          "330:             compose_file_list.append(DOCKER_COMPOSE_DIR / \"integration-celery.yml\")",
          "332:         compose_file_list.append(DOCKER_COMPOSE_DIR / \"base.yml\")",
          "333:         self.add_docker_in_docker(compose_file_list)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "331:             if self.use_airflow_version:",
          "332:                 current_extras = self.airflow_extras",
          "333:                 if \"celery\" not in current_extras.split(\",\"):",
          "334:                     get_console().print(",
          "335:                         \"[warning]Adding `celery` extras as it is implicitly needed by celery executor\"",
          "336:                     )",
          "337:                     self.airflow_extras = \",\".join(current_extras.split(\",\") + [\"celery\"])",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/utils/docker_command_utils.py||dev/breeze/src/airflow_breeze/utils/docker_command_utils.py": [
          "File: dev/breeze/src/airflow_breeze/utils/docker_command_utils.py -> dev/breeze/src/airflow_breeze/utils/docker_command_utils.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "751:             f\"Changing the executor to {SEQUENTIAL_EXECUTOR}.\\n\"",
          "752:         )",
          "753:         shell_params.executor = SEQUENTIAL_EXECUTOR",
          "768:     if shell_params.restart:",
          "769:         bring_compose_project_down(preserve_volumes=False, shell_params=shell_params)",
          "770:     if shell_params.include_mypy_volume:",
          "",
          "[Removed Lines]",
          "755:     if shell_params.executor == \"CeleryExecutor\" and shell_params.use_airflow_version:",
          "756:         if shell_params.airflow_extras and \"celery\" not in shell_params.airflow_extras.split():",
          "757:             get_console().print(",
          "758:                 f\"\\n[warning]CeleryExecutor requires airflow_extras: celery. \"",
          "759:                 f\"Adding celery to extras: '{shell_params.airflow_extras}'.\\n\"",
          "760:             )",
          "761:             shell_params.airflow_extras += \",celery\"",
          "762:         elif not shell_params.airflow_extras:",
          "763:             get_console().print(",
          "764:                 \"\\n[warning]CeleryExecutor requires airflow_extras: celery. \"",
          "765:                 \"Setting airflow extras to 'celery'.\\n\"",
          "766:             )",
          "767:             shell_params.airflow_extras = \"celery\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "scripts/in_container/install_airflow_and_providers.py||scripts/in_container/install_airflow_and_providers.py": [
          "File: scripts/in_container/install_airflow_and_providers.py -> scripts/in_container/install_airflow_and_providers.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "253:         )",
          "254:     else:",
          "255:         console.print(f\"\\nInstalling airflow via apache-airflow=={use_airflow_version}\")",
          "257:         airflow_constraints_location = get_airflow_constraints_location(",
          "258:             airflow_skip_constraints=airflow_skip_constraints,",
          "259:             airflow_constraints_mode=airflow_constraints_mode,",
          "",
          "[Removed Lines]",
          "256:         airflow_package_spec = f\"apache-airflow=={use_airflow_version}{airflow_extras}\"",
          "",
          "[Added Lines]",
          "256:         airflow_package_spec = f\"apache-airflow{airflow_extras}=={use_airflow_version}\"",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "3c7032513e7dadfaaad93efd47922f0d84bdfda2",
      "candidate_info": {
        "commit_hash": "3c7032513e7dadfaaad93efd47922f0d84bdfda2",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/3c7032513e7dadfaaad93efd47922f0d84bdfda2",
        "files": [
          "airflow/operators/python.py",
          "tests/operators/test_python.py"
        ],
        "message": "Allow PythonVirtualenvOperator.skip_on_exit_code to be zero (#36361)\n\n(cherry picked from commit b2f1882c584feceda02d3ee5af086b5098701518)",
        "before_after_code_files": [
          "airflow/operators/python.py||airflow/operators/python.py",
          "tests/operators/test_python.py||tests/operators/test_python.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/operators/python.py||airflow/operators/python.py": [
          "File: airflow/operators/python.py -> airflow/operators/python.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "386:             skip_on_exit_code",
          "387:             if isinstance(skip_on_exit_code, Container)",
          "388:             else [skip_on_exit_code]",
          "390:             else []",
          "391:         )",
          "",
          "[Removed Lines]",
          "389:             if skip_on_exit_code",
          "",
          "[Added Lines]",
          "389:             if skip_on_exit_code is not None",
          "",
          "---------------"
        ],
        "tests/operators/test_python.py||tests/operators/test_python.py": [
          "File: tests/operators/test_python.py -> tests/operators/test_python.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "847:         assert set(context) == declared_keys",
          "849:     @pytest.mark.parametrize(",
          "851:         [",
          "853:             ({\"skip_on_exit_code\": 100}, 100, TaskInstanceState.SKIPPED),",
          "856:             ({\"skip_on_exit_code\": 100}, 101, TaskInstanceState.FAILED),",
          "857:             ({\"skip_on_exit_code\": [100, 102]}, 101, TaskInstanceState.FAILED),",
          "859:         ],",
          "860:     )",
          "862:         def f(exit_code):",
          "863:             if exit_code != 0:",
          "864:                 raise SystemExit(exit_code)",
          "866:         if expected_state == TaskInstanceState.FAILED:",
          "867:             with pytest.raises(CalledProcessError):",
          "869:         else:",
          "870:             ti = self.run_as_task(",
          "871:                 f,",
          "872:                 return_ti=True,",
          "873:                 op_kwargs={\"exit_code\": actual_exit_code},",
          "875:             )",
          "876:             assert ti.state == expected_state",
          "",
          "[Removed Lines]",
          "850:         \"extra_kwargs, actual_exit_code, expected_state\",",
          "852:             (None, 99, TaskInstanceState.FAILED),",
          "854:             ({\"skip_on_exit_code\": [100]}, 100, TaskInstanceState.SKIPPED),",
          "855:             ({\"skip_on_exit_code\": (100, 101)}, 100, TaskInstanceState.SKIPPED),",
          "858:             ({\"skip_on_exit_code\": None}, 0, TaskInstanceState.SUCCESS),",
          "861:     def test_on_skip_exit_code(self, extra_kwargs, actual_exit_code, expected_state):",
          "868:                 self.run_as_task(f, op_kwargs={\"exit_code\": actual_exit_code}, **(extra_kwargs or {}))",
          "",
          "[Added Lines]",
          "850:         \"kwargs, actual_exit_code, expected_state\",",
          "852:             ({}, 0, TaskInstanceState.SUCCESS),",
          "853:             ({}, 100, TaskInstanceState.FAILED),",
          "854:             ({}, 101, TaskInstanceState.FAILED),",
          "855:             ({\"skip_on_exit_code\": None}, 0, TaskInstanceState.SUCCESS),",
          "856:             ({\"skip_on_exit_code\": None}, 100, TaskInstanceState.FAILED),",
          "857:             ({\"skip_on_exit_code\": None}, 101, TaskInstanceState.FAILED),",
          "858:             ({\"skip_on_exit_code\": 100}, 0, TaskInstanceState.SUCCESS),",
          "861:             ({\"skip_on_exit_code\": 0}, 0, TaskInstanceState.SKIPPED),",
          "862:             ({\"skip_on_exit_code\": [100]}, 0, TaskInstanceState.SUCCESS),",
          "863:             ({\"skip_on_exit_code\": [100]}, 100, TaskInstanceState.SKIPPED),",
          "864:             ({\"skip_on_exit_code\": [100]}, 101, TaskInstanceState.FAILED),",
          "866:             ({\"skip_on_exit_code\": (100,)}, 0, TaskInstanceState.SUCCESS),",
          "867:             ({\"skip_on_exit_code\": (100,)}, 100, TaskInstanceState.SKIPPED),",
          "868:             ({\"skip_on_exit_code\": (100,)}, 101, TaskInstanceState.FAILED),",
          "871:     def test_on_skip_exit_code(self, kwargs, actual_exit_code, expected_state):",
          "878:                 self.run_as_task(f, op_kwargs={\"exit_code\": actual_exit_code}, **kwargs)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "b3031114b8d0d3aff361475acbcd2d987b11298d",
      "candidate_info": {
        "commit_hash": "b3031114b8d0d3aff361475acbcd2d987b11298d",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/b3031114b8d0d3aff361475acbcd2d987b11298d",
        "files": [
          ".github/actions/build-ci-images/action.yml",
          "generated/provider_dependencies.json",
          "scripts/ci/pre_commit/pre_commit_update_providers_dependencies.py"
        ],
        "message": "Remove additional generation of dependencies when building CI images (#36283)\n\nWhen generated dependencies are not properly updated, we had a special\nstep where the dependencies were generated \"just in case\" before CI\nimage was built, because otherwise building the CI image could have\nfailed with strange \"failed because of conflicting dependencies\"\nwithout a clue what was the root cause.\n\nHowever, the pre-commit did not return error exit code - because for the\npre-commit, it is enough that a file is modified during pre-commit to\nfail the pre-commit in general.\n\nThat had a nasty side effect because the built CI image actually already\ncontained properly generated dependencies (by this step), and it did not\nproperly detected cases where the ones in the repository were added\nmanually and not generated with pre-commit.\n\nThis PR fixes it - instead of generating and building such image in\nCI it will now fail the CI image building step but with clear\ninstructions what to do.\n\nThe CI job step uses now regular breeze command rather than running\nthe script manually but also the script returns error code in case\nthe generated dependencies have been updated.\n\n(cherry picked from commit 33a2fbef9ff656c3522ea8dea5fff2e2c2645abf)",
        "before_after_code_files": [
          "scripts/ci/pre_commit/pre_commit_update_providers_dependencies.py||scripts/ci/pre_commit/pre_commit_update_providers_dependencies.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "scripts/ci/pre_commit/pre_commit_update_providers_dependencies.py||scripts/ci/pre_commit/pre_commit_update_providers_dependencies.py": [
          "File: scripts/ci/pre_commit/pre_commit_update_providers_dependencies.py -> scripts/ci/pre_commit/pre_commit_update_providers_dependencies.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "213:         DEPENDENCIES_JSON_FILE_PATH.write_text(json.dumps(unique_sorted_dependencies, indent=2) + \"\\n\")",
          "214:         if os.environ.get(\"CI\"):",
          "215:             console.print()",
          "217:             console.print(",
          "220:             )",
          "221:             console.print()",
          "222:         else:",
          "223:             console.print()",
          "",
          "[Removed Lines]",
          "216:             console.print(f\"[info]Written {DEPENDENCIES_JSON_FILE_PATH}\")",
          "218:                 f\"[yellow]You will need to run breeze locally and commit \"",
          "219:                 f\"{DEPENDENCIES_JSON_FILE_PATH.relative_to(AIRFLOW_SOURCES_ROOT)}!\\n\"",
          "",
          "[Added Lines]",
          "216:             console.print(f\"[info]There is a need to regenerate {DEPENDENCIES_JSON_FILE_PATH}\")",
          "218:                 f\"[red]You need to run the following command locally and commit generated \"",
          "219:                 f\"{DEPENDENCIES_JSON_FILE_PATH.relative_to(AIRFLOW_SOURCES_ROOT)} file:\\n\"",
          "221:             console.print(\"breeze static-checks --type update-providers-dependencies --all-files\")",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "227:             )",
          "228:             console.print(f\"[info]Written {DEPENDENCIES_JSON_FILE_PATH}\")",
          "229:             console.print()",
          "230:     else:",
          "231:         console.print(",
          "232:             \"[green]No need to regenerate dependencies!\\n[/]\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "231:         sys.exit(1)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "63c4546bb7e7d9b3c393755737796332184c6eb2",
      "candidate_info": {
        "commit_hash": "63c4546bb7e7d9b3c393755737796332184c6eb2",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/63c4546bb7e7d9b3c393755737796332184c6eb2",
        "files": [
          "airflow/providers/amazon/provider.yaml",
          "generated/provider_dependencies.json",
          "setup.py"
        ],
        "message": "Bump min version of amazon-provider related dependencies (#36660)\n\nThis is a regular bump of Amazon-provider related dependencies.\nThe way how botocore releases are done, they are putting a lot of\nstrain on `pip` to resolve the right set of dependencies, including\nlong backtracking, when there are too man versions available.\n\nTherefore, from time to time, we are bumping minimum version of\nAmazon-related dependencies to limit the impact frequent releases\nof boto and botocore has. Also it is generally fine to update min\nversion of dependencies for providers because at the very least\nusers can still use previously released providers in case they\nhave problem with those dependencies, also many of the updated\ndependencies contain fixes and feature we implicitly depend on and\nbumping them regulary is a good way to make sure all the functionalities\nof the Amazon provider are working as expected.\n\nAnother reason for the bump is that as of 1.33 version botocore and\nboto version stopped being shifted by 3 (previously boto3 1.28 was\nthe version corresponding to botocore 1.31). As of version 1.33 this\nproblem has been solved. See https://github.com/boto/boto3/issues/2702\n\nWatchtower min version is bumped to version 3 (which is 12 months old\neven if before we opted for much older (more than 2 years old) and again\nif users want to use older version of watchtower, they can opt for\nprevious provider version.\n\nThis change saves 5-6 minutes of backtracking when `pip` try to\nfind the right version of dependencies when upgrading to newer version.\n\nExtracted from #36537\n\n(cherry picked from commit 298c37d355eeadfccbd655efb2922d39ba17052c)",
        "before_after_code_files": [
          "setup.py||setup.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "setup.py||setup.py": [
          "File: setup.py -> setup.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "409: ]",
          "411: # make sure to update providers/amazon/provider.yaml botocore min version when you update it here",
          "414: _devel_only_amazon = [",
          "417:     f\"mypy-boto3-rds>={_MIN_BOTO3_VERSION}\",",
          "418:     f\"mypy-boto3-redshift-data>={_MIN_BOTO3_VERSION}\",",
          "419:     f\"mypy-boto3-s3>={_MIN_BOTO3_VERSION}\",",
          "421: ]",
          "423: _devel_only_azure = [",
          "",
          "[Removed Lines]",
          "412: _MIN_BOTO3_VERSION = \"1.28.0\"",
          "415:     \"aws_xray_sdk\",",
          "416:     \"moto[cloudformation,glue]>=4.2.9\",",
          "420:     f\"mypy-boto3-appflow>={_MIN_BOTO3_VERSION}\",",
          "",
          "[Added Lines]",
          "412: _MIN_BOTO3_VERSION = \"1.33.0\"",
          "415:     \"aws_xray_sdk>=2.12.0\",",
          "416:     \"moto[cloudformation,glue]>=4.2.12\",",
          "417:     f\"mypy-boto3-appflow>={_MIN_BOTO3_VERSION}\",",
          "",
          "---------------"
        ]
      }
    }
  ]
}