{
  "cve_id": "CVE-2022-31777",
  "cve_desc": "A stored cross-site scripting (XSS) vulnerability in Apache Spark 3.2.1 and earlier, and 3.3.0, allows remote attackers to execute arbitrary JavaScript in the web browser of a user, by including a malicious payload into the logs which would be returned in logs rendered in the UI.",
  "repo": "apache/spark",
  "patch_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
  "patch_info": {
    "commit_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
    "repo": "apache/spark",
    "commit_url": "https://github.com/apache/spark/commit/ad90195de56688ce0898691eb9d04297ab0871ad",
    "files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ],
    "message": "[SPARK-39505][UI] Escape log content rendered in UI\n\n### What changes were proposed in this pull request?\n\nEscape log content rendered to the UI.\n\n### Why are the changes needed?\n\nLog content may contain reserved characters or other code in the log and be misinterpreted in the UI as HTML.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nExisting tests\n\nCloses #36902 from srowen/LogViewEscape.\n\nAuthored-by: Sean Owen <srowen@gmail.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
    "before_after_code_files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ]
  },
  "patch_diff": {
    "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js": [
      "File: core/src/main/resources/org/apache/spark/ui/static/log-view.js -> core/src/main/resources/org/apache/spark/ui/static/log-view.js",
      "--- Hunk 1 ---",
      "[Context before]",
      "85:       if (retStartByte == 0) {",
      "86:         disableMoreButton();",
      "87:       }",
      "90:       curLogLength = curLogLength + (startByte - retStartByte);",
      "91:       startByte = retStartByte;",
      "",
      "[Removed Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(cleanData);",
      "",
      "[Added Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(document.createTextNode(cleanData));",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "115:             var retLogLength = dataInfo[2];",
      "117:             var cleanData = data.substring(newlineIndex + 1);",
      "120:             curLogLength = curLogLength + (retEndByte - retStartByte);",
      "121:             endByte = retEndByte;",
      "",
      "[Removed Lines]",
      "118:             $(\"pre\", \".log-content\").append(cleanData);",
      "",
      "[Added Lines]",
      "118:             $(\"pre\", \".log-content\").append(document.createTextNode(cleanData));",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "ca9138ee8b6d8645943b737cc4231fbb0154c8cb",
      "candidate_info": {
        "commit_hash": "ca9138ee8b6d8645943b737cc4231fbb0154c8cb",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/ca9138ee8b6d8645943b737cc4231fbb0154c8cb",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcUtils.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetUtils.scala"
        ],
        "message": "[SPARK-34960][SQL][DOCS][FOLLOWUP] Improve doc for DSv2 aggregate push down\n\n### What changes were proposed in this pull request?\n\nThis is a followup per comment in https://issues.apache.org/jira/browse/SPARK-34960, to improve the documentation for data source v2 aggregate push down of Parquet and ORC.\n\n* Unify SQL config docs between Parquet and ORC, and add the note that if statistics is missing from any file footer, exception would be thrown.\n* Also adding the same note for exception in Parquet and ORC methods to aggregate from statistics.\n\nThough in future Spark release, we may improve the behavior to fallback to aggregate from real data of file, in case any statistics are missing. We'd better to make a clear documentation for current behavior now.\n\n### Why are the changes needed?\n\nGive users & developers a better idea of when aggregate push down would throw exception.\nHave a better documentation for current behavior.\n\n### Does this PR introduce _any_ user-facing change?\n\nYes, the documentation change in SQL configs.\n\n### How was this patch tested?\n\nExisting tests as this is just documentation change.\n\nCloses #36311 from c21/agg-doc.\n\nAuthored-by: Cheng Su <chengsu@fb.com>\nSigned-off-by: huaxingao <huaxin_gao@apple.com>\n(cherry picked from commit 86b8757c2c4bab6a0f7a700cf2c690cdd7f31eba)\nSigned-off-by: huaxingao <huaxin_gao@apple.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcUtils.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcUtils.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetUtils.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetUtils.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "974:       .createWithDefault(10)",
          "976:   val PARQUET_AGGREGATE_PUSHDOWN_ENABLED = buildConf(\"spark.sql.parquet.aggregatePushdown\")",
          "980:     .version(\"3.3.0\")",
          "981:     .booleanConf",
          "982:     .createWithDefault(false)",
          "",
          "[Removed Lines]",
          "977:     .doc(\"If true, MAX/MIN/COUNT without filter and group by will be pushed\" +",
          "978:       \" down to Parquet for optimization. MAX/MIN/COUNT for complex types and timestamp\" +",
          "979:       \" can't be pushed down\")",
          "",
          "[Added Lines]",
          "977:     .doc(\"If true, aggregates will be pushed down to Parquet for optimization. Support MIN, MAX \" +",
          "978:       \"and COUNT as aggregate expression. For MIN/MAX, support boolean, integer, float and date \" +",
          "979:       \"type. For COUNT, support all data types. If statistics is missing from any Parquet file \" +",
          "980:       \"footer, exception would be thrown.\")",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1110:   val ORC_AGGREGATE_PUSHDOWN_ENABLED = buildConf(\"spark.sql.orc.aggregatePushdown\")",
          "1111:     .doc(\"If true, aggregates will be pushed down to ORC for optimization. Support MIN, MAX and \" +",
          "1112:       \"COUNT as aggregate expression. For MIN/MAX, support boolean, integer, float and date \" +",
          "1114:     .version(\"3.3.0\")",
          "1115:     .booleanConf",
          "1116:     .createWithDefault(false)",
          "",
          "[Removed Lines]",
          "1113:       \"type. For COUNT, support all data types.\")",
          "",
          "[Added Lines]",
          "1114:       \"type. For COUNT, support all data types. If statistics is missing from any ORC file \" +",
          "1115:       \"footer, exception would be thrown.\")",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcUtils.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcUtils.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcUtils.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcUtils.scala"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetUtils.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetUtils.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetUtils.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetUtils.scala"
        ]
      }
    },
    {
      "candidate_hash": "c07f65c51681107e869d2ebb46aa546ac3871e3a",
      "candidate_info": {
        "commit_hash": "c07f65c51681107e869d2ebb46aa546ac3871e3a",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/c07f65c51681107e869d2ebb46aa546ac3871e3a",
        "files": [
          "sql/core/src/test/scala/org/apache/spark/sql/BloomFilterAggregateQuerySuite.scala"
        ],
        "message": "[SPARK-32268][SQL][TESTS][FOLLOW-UP] Use function registry in the SparkSession\n\n### What changes were proposed in this pull request?\n\nThis PR proposes:\n1. Use the function registry in the Spark Session being used\n2. Move function registration into `beforeAll`\n\n### Why are the changes needed?\n\nRegistration of the function without `beforeAll` at `builtin` can affect other tests. See also https://lists.apache.org/thread/jp0ccqv10ht716g9xldm2ohdv3mpmmz1.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo, test-only.\n\n### How was this patch tested?\n\nUnittests fixed.\n\nCloses #36576 from HyukjinKwon/SPARK-32268-followup.\n\nAuthored-by: Hyukjin Kwon <gurwls223@apache.org>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>\n(cherry picked from commit c5351f85dec628a5c806893aa66777cbd77a4d65)\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>",
        "before_after_code_files": [
          "sql/core/src/test/scala/org/apache/spark/sql/BloomFilterAggregateQuerySuite.scala||sql/core/src/test/scala/org/apache/spark/sql/BloomFilterAggregateQuerySuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/core/src/test/scala/org/apache/spark/sql/BloomFilterAggregateQuerySuite.scala||sql/core/src/test/scala/org/apache/spark/sql/BloomFilterAggregateQuerySuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/BloomFilterAggregateQuerySuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/BloomFilterAggregateQuerySuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: package org.apache.spark.sql",
          "20: import org.apache.spark.sql.catalyst.FunctionIdentifier",
          "22: import org.apache.spark.sql.catalyst.expressions._",
          "23: import org.apache.spark.sql.catalyst.expressions.aggregate.BloomFilterAggregate",
          "24: import org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec",
          "",
          "[Removed Lines]",
          "21: import org.apache.spark.sql.catalyst.analysis.FunctionRegistry",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "35:   val funcId_bloom_filter_agg = new FunctionIdentifier(\"bloom_filter_agg\")",
          "36:   val funcId_might_contain = new FunctionIdentifier(\"might_contain\")",
          "52:   override def afterAll(): Unit = {",
          "55:     super.afterAll()",
          "56:   }",
          "",
          "[Removed Lines]",
          "39:   FunctionRegistry.builtin.registerFunction(funcId_bloom_filter_agg,",
          "40:     new ExpressionInfo(classOf[BloomFilterAggregate].getName, \"bloom_filter_agg\"),",
          "41:     (children: Seq[Expression]) => children.size match {",
          "42:       case 1 => new BloomFilterAggregate(children.head)",
          "43:       case 2 => new BloomFilterAggregate(children.head, children(1))",
          "44:       case 3 => new BloomFilterAggregate(children.head, children(1), children(2))",
          "45:     })",
          "48:   FunctionRegistry.builtin.registerFunction(funcId_might_contain,",
          "49:     new ExpressionInfo(classOf[BloomFilterMightContain].getName, \"might_contain\"),",
          "50:     (children: Seq[Expression]) => BloomFilterMightContain(children.head, children(1)))",
          "53:     FunctionRegistry.builtin.dropFunction(funcId_bloom_filter_agg)",
          "54:     FunctionRegistry.builtin.dropFunction(funcId_might_contain)",
          "",
          "[Added Lines]",
          "37:   override def beforeAll(): Unit = {",
          "38:     super.beforeAll()",
          "40:     spark.sessionState.functionRegistry.registerFunction(funcId_bloom_filter_agg,",
          "41:       new ExpressionInfo(classOf[BloomFilterAggregate].getName, \"bloom_filter_agg\"),",
          "42:       (children: Seq[Expression]) => children.size match {",
          "43:         case 1 => new BloomFilterAggregate(children.head)",
          "44:         case 2 => new BloomFilterAggregate(children.head, children(1))",
          "45:         case 3 => new BloomFilterAggregate(children.head, children(1), children(2))",
          "46:       })",
          "49:     spark.sessionState.functionRegistry.registerFunction(funcId_might_contain,",
          "50:       new ExpressionInfo(classOf[BloomFilterMightContain].getName, \"might_contain\"),",
          "51:       (children: Seq[Expression]) => BloomFilterMightContain(children.head, children(1)))",
          "52:   }",
          "55:     spark.sessionState.functionRegistry.dropFunction(funcId_bloom_filter_agg)",
          "56:     spark.sessionState.functionRegistry.dropFunction(funcId_might_contain)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "6306410db7bba03e5815a7fc3f366ac6baaa1d3c",
      "candidate_info": {
        "commit_hash": "6306410db7bba03e5815a7fc3f366ac6baaa1d3c",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/6306410db7bba03e5815a7fc3f366ac6baaa1d3c",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/catalog/SessionCatalog.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/catalog/SessionCatalogSuite.scala"
        ],
        "message": "[SPARK-38974][SQL] Filter registered functions with a given database name in list functions\n\n### What changes were proposed in this pull request?\n\nThis PR fixes a bug in list functions to filter out registered functions that do not belong to the specified database.\n\n### Why are the changes needed?\n\nTo fix a bug for `SHOW FUNCTIONS IN [db]`. Listed functions should only include all temporary functions and persistent functions in the specified database, instead of all registered functions in the function registry.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nUnit test\n\nCloses #36291 from allisonwang-db/spark-38974-list-functions.\n\nAuthored-by: allisonwang-db <allison.wang@databricks.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\n(cherry picked from commit cbfa0513421d5e9e9b7410d7f86b8e25df4ae548)\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/catalog/SessionCatalog.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/catalog/SessionCatalog.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/catalog/SessionCatalogSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/catalog/SessionCatalogSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/catalog/SessionCatalog.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/catalog/SessionCatalog.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/catalog/SessionCatalog.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/catalog/SessionCatalog.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "1753:     }",
          "1754:   }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1759:   private def listRegisteredFunctions(db: String, pattern: String): Seq[FunctionIdentifier] = {",
          "1760:     val functions = (functionRegistry.listFunction() ++ tableFunctionRegistry.listFunction())",
          "1761:       .filter(_.database.forall(_ == db))",
          "1762:     StringUtils.filterPattern(functions.map(_.unquotedString), pattern).map { f =>",
          "1764:       Try(parser.parseFunctionIdentifier(f)) match {",
          "1765:         case Success(e) => e",
          "1766:         case Failure(_) =>",
          "1768:           FunctionIdentifier(f)",
          "1769:       }",
          "1770:     }",
          "1771:   }",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1770:     requireDbExists(dbName)",
          "1771:     val dbFunctions = externalCatalog.listFunctions(dbName, pattern).map { f =>",
          "1772:       FunctionIdentifier(f, Some(dbName)) }",
          "1785:     val functions = dbFunctions ++ loadedFunctions",
          "",
          "[Removed Lines]",
          "1773:     val loadedFunctions = StringUtils",
          "1774:       .filterPattern(",
          "1775:         (functionRegistry.listFunction() ++ tableFunctionRegistry.listFunction())",
          "1776:           .map(_.unquotedString), pattern).map { f =>",
          "1778:         Try(parser.parseFunctionIdentifier(f)) match {",
          "1779:           case Success(e) => e",
          "1780:           case Failure(_) =>",
          "1782:             FunctionIdentifier(f)",
          "1783:         }",
          "1784:       }",
          "",
          "[Added Lines]",
          "1790:     val loadedFunctions = listRegisteredFunctions(db, pattern)",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/catalog/SessionCatalogSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/catalog/SessionCatalogSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/catalog/SessionCatalogSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/catalog/SessionCatalogSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "1574:     }",
          "1575:   }",
          "1577:   test(\"copy SessionCatalog state - temp views\") {",
          "1578:     withEmptyCatalog { original =>",
          "1579:       val tempTable1 = Range(1, 10, 1, 10)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1577:   test(\"SPARK-38974: list functions in database\") {",
          "1578:     withEmptyCatalog { catalog =>",
          "1579:       val tmpFunc = newFunc(\"func1\", None)",
          "1580:       val func1 = newFunc(\"func1\", Some(\"default\"))",
          "1581:       val func2 = newFunc(\"func2\", Some(\"db1\"))",
          "1582:       val builder = (e: Seq[Expression]) => e.head",
          "1583:       catalog.createDatabase(newDb(\"db1\"), ignoreIfExists = false)",
          "1584:       catalog.registerFunction(tmpFunc, overrideIfExists = false, functionBuilder = Some(builder))",
          "1585:       catalog.createFunction(func1, ignoreIfExists = false)",
          "1586:       catalog.createFunction(func2, ignoreIfExists = false)",
          "1588:       catalog.registerFunction(func2, overrideIfExists = false, functionBuilder = Some(builder))",
          "1590:       assert(catalog.listFunctions(\"default\", \"*\").map(_._1).toSet ==",
          "1591:         Set(FunctionIdentifier(\"func1\"), FunctionIdentifier(\"func1\", Some(\"default\")))",
          "1592:       )",
          "1593:     }",
          "1594:   }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "1fa3171f387adda9aeb1ca350b1422e1f80a69bd",
      "candidate_info": {
        "commit_hash": "1fa3171f387adda9aeb1ca350b1422e1f80a69bd",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/1fa3171f387adda9aeb1ca350b1422e1f80a69bd",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/types/Decimal.scala",
          "sql/core/src/test/resources/sql-tests/results/ansi/cast.sql.out",
          "sql/core/src/test/resources/sql-tests/results/ansi/decimalArithmeticOperations.sql.out",
          "sql/core/src/test/resources/sql-tests/results/ansi/interval.sql.out"
        ],
        "message": "[SPARK-39060][SQL][3.3] Typo in error messages of decimal overflow\n\n### What changes were proposed in this pull request?\n\nThis PR removes extra curly bracket from debug string for Decimal type in SQL.\n\nThis is a backport from master branch. Commit: 165ce4eb7d6d75201beb1bff879efa99fde24f94\n\n### Why are the changes needed?\n\nTypo in error messages of decimal overflow.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nBy running tests:\n```\n$ build/sbt \"sql/testOnly\"\n```\n\nCloses #36450 from vli-databricks/SPARK-39060-3.3.\n\nAuthored-by: Vitalii Li <vitalii.li@databricks.com>\nSigned-off-by: Max Gekk <max.gekk@gmail.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/types/Decimal.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/types/Decimal.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/types/Decimal.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/types/Decimal.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/types/Decimal.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/types/Decimal.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "228:   def toDebugString: String = {",
          "229:     if (decimalVal.ne(null)) {",
          "231:     } else {",
          "233:     }",
          "234:   }",
          "",
          "[Removed Lines]",
          "230:       s\"Decimal(expanded,$decimalVal,$precision,$scale})\"",
          "232:       s\"Decimal(compact,$longVal,$precision,$scale})\"",
          "",
          "[Added Lines]",
          "230:       s\"Decimal(expanded, $decimalVal, $precision, $scale)\"",
          "232:       s\"Decimal(compact, $longVal, $precision, $scale)\"",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "bcc646b6f3da194d44db36e68d57f0f0621f10fe",
      "candidate_info": {
        "commit_hash": "bcc646b6f3da194d44db36e68d57f0f0621f10fe",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/bcc646b6f3da194d44db36e68d57f0f0621f10fe",
        "files": [
          "sql/core/src/test/scala/org/apache/spark/sql/PlanStabilitySuite.scala"
        ],
        "message": "[SPARK-39442][SQL][TESTS] Update `PlanStabilitySuite` comments with `SPARK_ANSI_SQL_MODE`\n\n### What changes were proposed in this pull request?\n\nThis PR aims to update `PlanStabilitySuite` direction to prevent future mistakes.\n\n1. Add `SPARK_ANSI_SQL_MODE=true` explicitly because Apache Spark 3.3+ test coverage has ANSI and non-ANSI modes. We need to make it sure that both results are synced at the same time.\n```\n- SPARK_GENERATE_GOLDEN_FILES=1 build/sbt ...\n+ SPARK_GENERATE_GOLDEN_FILES=1 build/sbt ...\n+ SPARK_GENERATE_GOLDEN_FILES=1 SPARK_ANSI_SQL_MODE=true ...\n```\n\n2. The existing commands are human-readable but is not working. So, we had better have more simple command which is *copy-and-pasteable*.\n```\n- build/sbt \"sql/testOnly *PlanStability[WithStats]Suite\"\n+ build/sbt \"sql/testOnly *PlanStability*Suite\"\n```\n\n### Why are the changes needed?\n\nThis will help us update the test results more easily by preventing mistakes.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo. This is a dev-only doc.\n\n### How was this patch tested?\n\nManual review.\n\nCloses #36839 from dongjoon-hyun/SPARK-39442.\n\nAuthored-by: Dongjoon Hyun <dongjoon@apache.org>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>\n(cherry picked from commit d426c10e94be162547fb8990434cc87bdff28380)\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
        "before_after_code_files": [
          "sql/core/src/test/scala/org/apache/spark/sql/PlanStabilitySuite.scala||sql/core/src/test/scala/org/apache/spark/sql/PlanStabilitySuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/core/src/test/scala/org/apache/spark/sql/PlanStabilitySuite.scala||sql/core/src/test/scala/org/apache/spark/sql/PlanStabilitySuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/PlanStabilitySuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/PlanStabilitySuite.scala"
        ]
      }
    }
  ]
}