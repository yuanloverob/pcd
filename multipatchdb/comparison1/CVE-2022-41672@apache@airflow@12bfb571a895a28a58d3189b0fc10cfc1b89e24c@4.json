{
  "cve_id": "CVE-2022-41672",
  "cve_desc": "In Apache Airflow, prior to version 2.4.1, deactivating a user wouldn't prevent an already authenticated user from being able to continue using the UI or API.",
  "repo": "apache/airflow",
  "patch_hash": "12bfb571a895a28a58d3189b0fc10cfc1b89e24c",
  "patch_info": {
    "commit_hash": "12bfb571a895a28a58d3189b0fc10cfc1b89e24c",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/12bfb571a895a28a58d3189b0fc10cfc1b89e24c",
    "files": [
      "airflow/www/app.py",
      "airflow/www/extensions/init_security.py",
      "tests/test_utils/decorators.py",
      "tests/www/views/conftest.py",
      "tests/www/views/test_session.py",
      "tests/www/views/test_views_base.py"
    ],
    "message": "Check user is active (#26635)\n\n(cherry picked from commit 59707cdf7eacb698ca375b5220af30a39ca1018c)",
    "before_after_code_files": [
      "airflow/www/app.py||airflow/www/app.py",
      "airflow/www/extensions/init_security.py||airflow/www/extensions/init_security.py",
      "tests/test_utils/decorators.py||tests/test_utils/decorators.py",
      "tests/www/views/conftest.py||tests/www/views/conftest.py",
      "tests/www/views/test_session.py||tests/www/views/test_session.py",
      "tests/www/views/test_views_base.py||tests/www/views/test_views_base.py"
    ]
  },
  "patch_diff": {
    "airflow/www/app.py||airflow/www/app.py": [
      "File: airflow/www/app.py -> airflow/www/app.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "39: from airflow.www.extensions.init_jinja_globals import init_jinja_globals",
      "40: from airflow.www.extensions.init_manifest_files import configure_manifest_files",
      "41: from airflow.www.extensions.init_robots import init_robots",
      "43: from airflow.www.extensions.init_session import init_airflow_session_interface",
      "44: from airflow.www.extensions.init_views import (",
      "45:     init_api_connexion,",
      "",
      "[Removed Lines]",
      "42: from airflow.www.extensions.init_security import init_api_experimental_auth, init_xframe_protection",
      "",
      "[Added Lines]",
      "42: from airflow.www.extensions.init_security import (",
      "43:     init_api_experimental_auth,",
      "44:     init_check_user_active,",
      "45:     init_xframe_protection,",
      "46: )",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "152:         init_jinja_globals(flask_app)",
      "153:         init_xframe_protection(flask_app)",
      "154:         init_airflow_session_interface(flask_app)",
      "155:     return flask_app",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "159:         init_check_user_active(flask_app)",
      "",
      "---------------"
    ],
    "airflow/www/extensions/init_security.py||airflow/www/extensions/init_security.py": [
      "File: airflow/www/extensions/init_security.py -> airflow/www/extensions/init_security.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "19: import logging",
      "20: from importlib import import_module",
      "22: from airflow.configuration import conf",
      "23: from airflow.exceptions import AirflowConfigException, AirflowException",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "22: from flask import g, redirect, url_for",
      "23: from flask_login import logout_user",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "60:         except ImportError as err:",
      "61:             log.critical(\"Cannot import %s for API authentication due to: %s\", backend, err)",
      "62:             raise AirflowException(err)",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "68: def init_check_user_active(app):",
      "69:     @app.before_request",
      "70:     def check_user_active():",
      "71:         if g.user is not None and not g.user.is_anonymous and not g.user.is_active:",
      "72:             logout_user()",
      "73:             return redirect(url_for(app.appbuilder.sm.auth_view.endpoint + \".login\"))",
      "",
      "---------------"
    ],
    "tests/test_utils/decorators.py||tests/test_utils/decorators.py": [
      "File: tests/test_utils/decorators.py -> tests/test_utils/decorators.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "45:             \"init_xframe_protection\",",
      "46:             \"init_airflow_session_interface\",",
      "47:             \"init_appbuilder\",",
      "48:         ]",
      "50:         @functools.wraps(f)",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "48:             \"init_check_user_active\",",
      "",
      "---------------"
    ],
    "tests/www/views/conftest.py||tests/www/views/conftest.py": [
      "File: tests/www/views/conftest.py -> tests/www/views/conftest.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "58:             \"init_jinja_globals\",",
      "59:             \"init_plugins\",",
      "60:             \"init_airflow_session_interface\",",
      "61:         ]",
      "62:     )",
      "63:     def factory():",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "61:             \"init_check_user_active\",",
      "",
      "---------------"
    ],
    "tests/www/views/test_session.py||tests/www/views/test_session.py": [
      "File: tests/www/views/test_session.py -> tests/www/views/test_session.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "88:     new_session_cookie = get_session_cookie(user_client)",
      "89:     assert new_session_cookie is not None",
      "90:     assert old_session_cookie.value != new_session_cookie.value",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "93: def test_check_active_user(app, user_client):",
      "94:     user = app.appbuilder.sm.find_user(username=\"test_user\")",
      "95:     user.active = False",
      "96:     resp = user_client.get(\"/home\")",
      "97:     assert resp.status_code == 302",
      "98:     assert \"/login\" in resp.headers.get(\"Location\")",
      "100:     # And they were logged out",
      "101:     user.active = True",
      "102:     resp = user_client.get(\"/home\")",
      "103:     assert resp.status_code == 302",
      "104:     assert \"/login\" in resp.headers.get(\"Location\")",
      "",
      "---------------"
    ],
    "tests/www/views/test_views_base.py||tests/www/views/test_views_base.py": [
      "File: tests/www/views/test_views_base.py -> tests/www/views/test_views_base.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "30: from tests.test_utils.www import check_content_in_response, check_content_not_in_response",
      "34:     with assert_queries_count(16):",
      "36:     check_content_in_response('DAGs', resp)",
      "",
      "[Removed Lines]",
      "33: def test_index(admin_client):",
      "35:         resp = admin_client.get('/', follow_redirects=True)",
      "",
      "[Added Lines]",
      "33: def test_index_redirect(admin_client):",
      "34:     resp = admin_client.get('/')",
      "35:     assert resp.status_code == 302",
      "36:     assert '/home' in resp.headers.get(\"Location\")",
      "38:     resp = admin_client.get('/', follow_redirects=True)",
      "39:     check_content_in_response('DAGs', resp)",
      "42: def test_homepage_query_count(admin_client):",
      "44:         resp = admin_client.get('/home')",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "e0607211a76e4d5fd1e04d8c362b49c6396bebd7",
      "candidate_info": {
        "commit_hash": "e0607211a76e4d5fd1e04d8c362b49c6396bebd7",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/e0607211a76e4d5fd1e04d8c362b49c6396bebd7",
        "files": [
          "airflow/jobs/scheduler_job.py",
          "tests/jobs/test_scheduler_job.py"
        ],
        "message": "Don't update backfill run from the scheduler (#26342)\n\nDon't update backfill run from the scheduler\n\nWhen updating the state of paused dags with 'running' dagruns in the scheduler, we should not\nupdate the state of backfill run.\n\nCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>\n(cherry picked from commit b9c4e98d8f8bcc129cbb4079548bd521cd3981b9)",
        "before_after_code_files": [
          "airflow/jobs/scheduler_job.py||airflow/jobs/scheduler_job.py",
          "tests/jobs/test_scheduler_job.py||tests/jobs/test_scheduler_job.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/26688"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/jobs/scheduler_job.py||airflow/jobs/scheduler_job.py": [
          "File: airflow/jobs/scheduler_job.py -> airflow/jobs/scheduler_job.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "774:                     self.log.exception(\"Exception when executing DagFileProcessorAgent.end\")",
          "775:             self.log.info(\"Exited execute loop\")",
          "778:         try:",
          "779:             paused_dag_ids = DagModel.get_all_paused_dag_ids()",
          "780:             for dag_id in paused_dag_ids:",
          "",
          "[Removed Lines]",
          "777:     def _update_dag_run_state_for_paused_dags(self) -> None:",
          "",
          "[Added Lines]",
          "777:     @provide_session",
          "778:     def _update_dag_run_state_for_paused_dags(self, session: Session = NEW_SESSION) -> None:",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "784:                 dag = SerializedDagModel.get_dag(dag_id)",
          "785:                 if dag is None:",
          "786:                     continue",
          "788:                 for dag_run in dag_runs:",
          "789:                     dag_run.dag = dag",
          "790:                     _, callback_to_run = dag_run.update_state(execute_callbacks=False)",
          "",
          "[Removed Lines]",
          "787:                 dag_runs = DagRun.find(dag_id=dag_id, state=DagRunState.RUNNING)",
          "",
          "[Added Lines]",
          "788:                 dag_runs = session.query(DagRun).filter(",
          "789:                     DagRun.dag_id == dag_id,",
          "790:                     DagRun.state == DagRunState.RUNNING,",
          "791:                     DagRun.run_type != DagRunType.BACKFILL_JOB,",
          "792:                 )",
          "",
          "---------------"
        ],
        "tests/jobs/test_scheduler_job.py||tests/jobs/test_scheduler_job.py": [
          "File: tests/jobs/test_scheduler_job.py -> tests/jobs/test_scheduler_job.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "4439:             .scalar()",
          "4440:         ) > (timezone.utcnow() - timedelta(days=2))",
          "4443: @pytest.mark.need_serialized_dag",
          "4444: def test_schedule_dag_run_with_upstream_skip(dag_maker, session):",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "4442:     def test_update_dagrun_state_for_paused_dag_not_for_backfill(self, dag_maker, session):",
          "4443:         \"\"\"Test that the _update_dagrun_state_for_paused_dag does not affect backfilled dagruns\"\"\"",
          "4445:         with dag_maker('testdag') as dag:",
          "4446:             EmptyOperator(task_id='task1')",
          "4448:         # Backfill run",
          "4449:         backfill_run = dag_maker.create_dagrun(run_type=DagRunType.BACKFILL_JOB)",
          "4450:         ti = backfill_run.get_task_instances()[0]",
          "4451:         ti.set_state(TaskInstanceState.SUCCESS)",
          "4452:         dm = DagModel.get_dagmodel(dag.dag_id)",
          "4453:         dm.is_paused = True",
          "4454:         session.merge(dm)",
          "4455:         session.merge(ti)",
          "4456:         session.flush()",
          "4458:         # scheduled run",
          "4459:         scheduled_run = dag_maker.create_dagrun(",
          "4460:             execution_date=datetime.datetime(2022, 1, 1), run_type=DagRunType.SCHEDULED",
          "4461:         )",
          "4462:         ti = scheduled_run.get_task_instances()[0]",
          "4463:         ti.set_state(TaskInstanceState.SUCCESS)",
          "4464:         dm = DagModel.get_dagmodel(dag.dag_id)",
          "4465:         dm.is_paused = True",
          "4466:         session.merge(dm)",
          "4467:         session.merge(ti)",
          "4468:         session.flush()",
          "4470:         assert dag.dag_id in DagModel.get_all_paused_dag_ids()",
          "4471:         assert backfill_run.state == State.RUNNING",
          "4472:         assert scheduled_run.state == State.RUNNING",
          "4474:         self.scheduler_job = SchedulerJob(subdir=os.devnull)",
          "4475:         self.scheduler_job.executor = MockExecutor()",
          "4476:         self.scheduler_job._update_dag_run_state_for_paused_dags()",
          "4477:         session.flush()",
          "4479:         (backfill_run,) = DagRun.find(dag_id=dag.dag_id, run_type=DagRunType.BACKFILL_JOB, session=session)",
          "4480:         assert backfill_run.state == State.RUNNING",
          "4482:         (scheduled_run,) = DagRun.find(dag_id=dag.dag_id, run_type=DagRunType.SCHEDULED, session=session)",
          "4483:         assert scheduled_run.state == State.SUCCESS",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "5ae47cd45fac9ff286b1e147f538fba1beccedee",
      "candidate_info": {
        "commit_hash": "5ae47cd45fac9ff286b1e147f538fba1beccedee",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/5ae47cd45fac9ff286b1e147f538fba1beccedee",
        "files": [
          "airflow/utils/db.py"
        ],
        "message": "Suppress SQLALCHEMY_TRACK_MODIFICATIONS warning in db init (#26617)\n\nWe already have it set false in the main flask app config, but now that we do db init from orm, and we create a distinct flask app for that, and it doesn't get the same setting.\n\nCloses #26544\n\n(cherry picked from commit 051ba159e54b992ca0111107df86b8abfd8b7279)",
        "before_after_code_files": [
          "airflow/utils/db.py||airflow/utils/db.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/26688"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/utils/db.py||airflow/utils/db.py": [
          "File: airflow/utils/db.py -> airflow/utils/db.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "656:     def _create_flask_session_tbl():",
          "657:         flask_app = Flask(__name__)",
          "658:         flask_app.config['SQLALCHEMY_DATABASE_URI'] = conf.get('database', 'SQL_ALCHEMY_CONN')",
          "659:         db = SQLAlchemy(flask_app)",
          "660:         AirflowDatabaseSessionInterface(app=flask_app, db=db, table='session', key_prefix='')",
          "661:         db.create_all()",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "659:         flask_app.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = False",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "53f0ffd7676c0c0a8c351568b534c4da93d6e817",
      "candidate_info": {
        "commit_hash": "53f0ffd7676c0c0a8c351568b534c4da93d6e817",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/53f0ffd7676c0c0a8c351568b534c4da93d6e817",
        "files": [
          "airflow/jobs/backfill_job.py",
          "tests/jobs/test_backfill_job.py"
        ],
        "message": "Fix Deferrable stuck as \"scheduled\" during backfill (#26205)\n\nCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>\n(cherry picked from commit 3396d1f822caac7cbeb14e1e67679b8378a84a6c)",
        "before_after_code_files": [
          "airflow/jobs/backfill_job.py||airflow/jobs/backfill_job.py",
          "tests/jobs/test_backfill_job.py||tests/jobs/test_backfill_job.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/26688"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/jobs/backfill_job.py||airflow/jobs/backfill_job.py": [
          "File: airflow/jobs/backfill_job.py -> airflow/jobs/backfill_job.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "217:                 tis_to_be_scheduled.append(ti)",
          "218:                 ti_status.running.pop(reduced_key)",
          "219:                 ti_status.to_run[ti.key] = ti",
          "221:         # Batch schedule of task instances",
          "222:         if tis_to_be_scheduled:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "220:             # special case: Deferrable task can go from DEFERRED to SCHEDULED;",
          "221:             # when that happens, we need to put it back as in UP_FOR_RESCHEDULE",
          "222:             elif ti.state == TaskInstanceState.SCHEDULED:",
          "223:                 self.log.debug(\"Task instance %s is resumed from deferred state\", ti)",
          "224:                 ti_status.running.pop(ti.key)",
          "225:                 ti_status.to_run[ti.key] = ti",
          "",
          "---------------"
        ],
        "tests/jobs/test_backfill_job.py||tests/jobs/test_backfill_job.py": [
          "File: tests/jobs/test_backfill_job.py -> tests/jobs/test_backfill_job.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1416:         ti_status.to_run.clear()",
          "1418:         session.close()",
          "1420:     def test_dag_dagrun_infos_between(self, dag_maker):",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1418:         # test for scheduled",
          "1419:         ti.set_state(State.SCHEDULED)",
          "1420:         # Deferred tasks are put into scheduled by the triggerer",
          "1421:         # Check that they are put into to_run",
          "1422:         ti_status.running[ti.key] = ti",
          "1423:         job._update_counters(ti_status=ti_status, session=session)",
          "1424:         assert len(ti_status.running) == 0",
          "1425:         assert len(ti_status.succeeded) == 0",
          "1426:         assert len(ti_status.skipped) == 0",
          "1427:         assert len(ti_status.failed) == 0",
          "1428:         assert len(ti_status.to_run) == 1",
          "1430:         ti_status.to_run.clear()",
          "1431:         # test for deferred",
          "1432:         # if a task is deferred and it's not yet time for the triggerer",
          "1433:         # to reschedule it, we should leave it in ti_status.running",
          "1434:         ti.set_state(State.DEFERRED)",
          "1435:         ti_status.running[ti.key] = ti",
          "1436:         job._update_counters(ti_status=ti_status, session=session)",
          "1437:         assert len(ti_status.running) == 1",
          "1438:         assert len(ti_status.succeeded) == 0",
          "1439:         assert len(ti_status.skipped) == 0",
          "1440:         assert len(ti_status.failed) == 0",
          "1441:         assert len(ti_status.to_run) == 0",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "15dd9a840cf9a79ab876163321cc718673ca72e5",
      "candidate_info": {
        "commit_hash": "15dd9a840cf9a79ab876163321cc718673ca72e5",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/15dd9a840cf9a79ab876163321cc718673ca72e5",
        "files": [
          "airflow/compat/sqlalchemy.py",
          "airflow/migrations/versions/0001_1_5_0_current_schema.py",
          "airflow/migrations/versions/0002_1_5_0_create_is_encrypted.py",
          "airflow/migrations/versions/0023_1_8_2_add_max_tries_column_to_task_instance.py",
          "airflow/migrations/versions/0028_1_10_0_add_kubernetes_resource_checkpointing.py",
          "airflow/migrations/versions/0057_1_10_13_add_fab_tables.py",
          "airflow/migrations/versions/0058_1_10_13_increase_length_of_fab_ab_view_menu_.py",
          "airflow/migrations/versions/0059_2_0_0_drop_user_and_chart.py",
          "airflow/migrations/versions/0060_2_0_0_remove_id_column_from_xcom.py",
          "airflow/migrations/versions/0062_2_0_0_add_dagrun_run_type.py",
          "airflow/migrations/versions/0065_2_0_0_update_schema_for_smart_sensor.py",
          "airflow/migrations/versions/0068_2_0_0_drop_kuberesourceversion_and_.py",
          "airflow/utils/db.py",
          "airflow/www/fab_security/sqla/manager.py",
          "docs/apache-airflow/img/airflow_erd.sha256"
        ],
        "message": "Resolve deprecation warning re Table.exists() (#26616)\n\nThe deprecation warning tells us we need to change Table.exists() to Inspector.has_table().  It turns out we're already using the right method, but we weren't passing it the right object.  It wants a table name but we were giving it a table.\n\n(cherry picked from commit 2388bd72ba4b0012b0540a43aa9f0001840ff285)",
        "before_after_code_files": [
          "airflow/compat/sqlalchemy.py||airflow/compat/sqlalchemy.py",
          "airflow/migrations/versions/0001_1_5_0_current_schema.py||airflow/migrations/versions/0001_1_5_0_current_schema.py",
          "airflow/migrations/versions/0002_1_5_0_create_is_encrypted.py||airflow/migrations/versions/0002_1_5_0_create_is_encrypted.py",
          "airflow/migrations/versions/0023_1_8_2_add_max_tries_column_to_task_instance.py||airflow/migrations/versions/0023_1_8_2_add_max_tries_column_to_task_instance.py",
          "airflow/migrations/versions/0028_1_10_0_add_kubernetes_resource_checkpointing.py||airflow/migrations/versions/0028_1_10_0_add_kubernetes_resource_checkpointing.py",
          "airflow/migrations/versions/0057_1_10_13_add_fab_tables.py||airflow/migrations/versions/0057_1_10_13_add_fab_tables.py",
          "airflow/migrations/versions/0058_1_10_13_increase_length_of_fab_ab_view_menu_.py||airflow/migrations/versions/0058_1_10_13_increase_length_of_fab_ab_view_menu_.py",
          "airflow/migrations/versions/0059_2_0_0_drop_user_and_chart.py||airflow/migrations/versions/0059_2_0_0_drop_user_and_chart.py",
          "airflow/migrations/versions/0060_2_0_0_remove_id_column_from_xcom.py||airflow/migrations/versions/0060_2_0_0_remove_id_column_from_xcom.py",
          "airflow/migrations/versions/0062_2_0_0_add_dagrun_run_type.py||airflow/migrations/versions/0062_2_0_0_add_dagrun_run_type.py",
          "airflow/migrations/versions/0065_2_0_0_update_schema_for_smart_sensor.py||airflow/migrations/versions/0065_2_0_0_update_schema_for_smart_sensor.py",
          "airflow/migrations/versions/0068_2_0_0_drop_kuberesourceversion_and_.py||airflow/migrations/versions/0068_2_0_0_drop_kuberesourceversion_and_.py",
          "airflow/utils/db.py||airflow/utils/db.py",
          "airflow/www/fab_security/sqla/manager.py||airflow/www/fab_security/sqla/manager.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/26688"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/compat/sqlalchemy.py||airflow/compat/sqlalchemy.py": [
          "File: airflow/compat/sqlalchemy.py -> airflow/compat/sqlalchemy.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "airflow/migrations/versions/0001_1_5_0_current_schema.py||airflow/migrations/versions/0001_1_5_0_current_schema.py": [
          "File: airflow/migrations/versions/0001_1_5_0_current_schema.py -> airflow/migrations/versions/0001_1_5_0_current_schema.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "27: import sqlalchemy as sa",
          "28: from alembic import op",
          "32: from airflow.migrations.db_types import StringID",
          "34: # revision identifiers, used by Alembic.",
          "",
          "[Removed Lines]",
          "29: from sqlalchemy import func",
          "31: from airflow.compat.sqlalchemy import inspect",
          "",
          "[Added Lines]",
          "29: from sqlalchemy import func, inspect",
          "",
          "---------------"
        ],
        "airflow/migrations/versions/0002_1_5_0_create_is_encrypted.py||airflow/migrations/versions/0002_1_5_0_create_is_encrypted.py": [
          "File: airflow/migrations/versions/0002_1_5_0_create_is_encrypted.py -> airflow/migrations/versions/0002_1_5_0_create_is_encrypted.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "27: import sqlalchemy as sa",
          "28: from alembic import op",
          "32: # revision identifiers, used by Alembic.",
          "33: revision = '1507a7289a2f'",
          "",
          "[Removed Lines]",
          "30: from airflow.compat.sqlalchemy import inspect",
          "",
          "[Added Lines]",
          "29: from sqlalchemy import inspect",
          "",
          "---------------"
        ],
        "airflow/migrations/versions/0023_1_8_2_add_max_tries_column_to_task_instance.py||airflow/migrations/versions/0023_1_8_2_add_max_tries_column_to_task_instance.py": [
          "File: airflow/migrations/versions/0023_1_8_2_add_max_tries_column_to_task_instance.py -> airflow/migrations/versions/0023_1_8_2_add_max_tries_column_to_task_instance.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "26: import sqlalchemy as sa",
          "27: from alembic import op",
          "29: from sqlalchemy.ext.declarative import declarative_base",
          "31: from airflow import settings",
          "33: from airflow.models import DagBag",
          "35: # revision identifiers, used by Alembic.",
          "",
          "[Removed Lines]",
          "28: from sqlalchemy import Column, Integer, String",
          "32: from airflow.compat.sqlalchemy import inspect",
          "",
          "[Added Lines]",
          "28: from sqlalchemy import Column, Integer, String, inspect",
          "",
          "---------------"
        ],
        "airflow/migrations/versions/0028_1_10_0_add_kubernetes_resource_checkpointing.py||airflow/migrations/versions/0028_1_10_0_add_kubernetes_resource_checkpointing.py": [
          "File: airflow/migrations/versions/0028_1_10_0_add_kubernetes_resource_checkpointing.py -> airflow/migrations/versions/0028_1_10_0_add_kubernetes_resource_checkpointing.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "26: import sqlalchemy as sa",
          "27: from alembic import op",
          "31: # revision identifiers, used by Alembic.",
          "32: revision = '33ae817a1ff4'",
          "",
          "[Removed Lines]",
          "29: from airflow.compat.sqlalchemy import inspect",
          "",
          "[Added Lines]",
          "28: from sqlalchemy import inspect",
          "",
          "---------------"
        ],
        "airflow/migrations/versions/0057_1_10_13_add_fab_tables.py||airflow/migrations/versions/0057_1_10_13_add_fab_tables.py": [
          "File: airflow/migrations/versions/0057_1_10_13_add_fab_tables.py -> airflow/migrations/versions/0057_1_10_13_add_fab_tables.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "27: import sqlalchemy as sa",
          "28: from alembic import op",
          "32: # revision identifiers, used by Alembic.",
          "33: revision = '92c57b58940d'",
          "",
          "[Removed Lines]",
          "30: from airflow.compat.sqlalchemy import inspect",
          "",
          "[Added Lines]",
          "29: from sqlalchemy import inspect",
          "",
          "---------------"
        ],
        "airflow/migrations/versions/0058_1_10_13_increase_length_of_fab_ab_view_menu_.py||airflow/migrations/versions/0058_1_10_13_increase_length_of_fab_ab_view_menu_.py": [
          "File: airflow/migrations/versions/0058_1_10_13_increase_length_of_fab_ab_view_menu_.py -> airflow/migrations/versions/0058_1_10_13_increase_length_of_fab_ab_view_menu_.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "27: import sqlalchemy as sa",
          "28: from alembic import op",
          "31: from airflow.migrations.db_types import StringID",
          "33: # revision identifiers, used by Alembic.",
          "",
          "[Removed Lines]",
          "30: from airflow.compat.sqlalchemy import inspect",
          "",
          "[Added Lines]",
          "29: from sqlalchemy import inspect",
          "",
          "---------------"
        ],
        "airflow/migrations/versions/0059_2_0_0_drop_user_and_chart.py||airflow/migrations/versions/0059_2_0_0_drop_user_and_chart.py": [
          "File: airflow/migrations/versions/0059_2_0_0_drop_user_and_chart.py -> airflow/migrations/versions/0059_2_0_0_drop_user_and_chart.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "26: import sqlalchemy as sa",
          "27: from alembic import op",
          "28: from sqlalchemy.dialects import mysql",
          "32: # revision identifiers, used by Alembic.",
          "33: revision = 'cf5dc11e79ad'",
          "34: down_revision = '03afc6b6f902'",
          "",
          "[Removed Lines]",
          "30: from airflow.compat.sqlalchemy import inspect",
          "",
          "[Added Lines]",
          "28: from sqlalchemy import inspect",
          "",
          "---------------"
        ],
        "airflow/migrations/versions/0060_2_0_0_remove_id_column_from_xcom.py||airflow/migrations/versions/0060_2_0_0_remove_id_column_from_xcom.py": [
          "File: airflow/migrations/versions/0060_2_0_0_remove_id_column_from_xcom.py -> airflow/migrations/versions/0060_2_0_0_remove_id_column_from_xcom.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "27: from collections import defaultdict",
          "29: from alembic import op",
          "34: # revision identifiers, used by Alembic.",
          "35: revision = 'bbf4a7ad0465'",
          "",
          "[Removed Lines]",
          "30: from sqlalchemy import Column, Integer",
          "32: from airflow.compat.sqlalchemy import inspect",
          "",
          "[Added Lines]",
          "30: from sqlalchemy import Column, Integer, inspect",
          "",
          "---------------"
        ],
        "airflow/migrations/versions/0062_2_0_0_add_dagrun_run_type.py||airflow/migrations/versions/0062_2_0_0_add_dagrun_run_type.py": [
          "File: airflow/migrations/versions/0062_2_0_0_add_dagrun_run_type.py -> airflow/migrations/versions/0062_2_0_0_add_dagrun_run_type.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "28: import sqlalchemy as sa",
          "29: from alembic import op",
          "31: from sqlalchemy.ext.declarative import declarative_base",
          "34: from airflow.utils.types import DagRunType",
          "36: # revision identifiers, used by Alembic.",
          "",
          "[Removed Lines]",
          "30: from sqlalchemy import Column, Integer, String",
          "33: from airflow.compat.sqlalchemy import inspect",
          "",
          "[Added Lines]",
          "30: from sqlalchemy import Column, Integer, String, inspect",
          "",
          "---------------"
        ],
        "airflow/migrations/versions/0065_2_0_0_update_schema_for_smart_sensor.py||airflow/migrations/versions/0065_2_0_0_update_schema_for_smart_sensor.py": [
          "File: airflow/migrations/versions/0065_2_0_0_update_schema_for_smart_sensor.py -> airflow/migrations/versions/0065_2_0_0_update_schema_for_smart_sensor.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "27: import sqlalchemy as sa",
          "28: from alembic import op",
          "32: from airflow.migrations.db_types import TIMESTAMP, StringID",
          "34: # revision identifiers, used by Alembic.",
          "",
          "[Removed Lines]",
          "29: from sqlalchemy import func",
          "31: from airflow.compat.sqlalchemy import inspect",
          "",
          "[Added Lines]",
          "29: from sqlalchemy import func, inspect",
          "",
          "---------------"
        ],
        "airflow/migrations/versions/0068_2_0_0_drop_kuberesourceversion_and_.py||airflow/migrations/versions/0068_2_0_0_drop_kuberesourceversion_and_.py": [
          "File: airflow/migrations/versions/0068_2_0_0_drop_kuberesourceversion_and_.py -> airflow/migrations/versions/0068_2_0_0_drop_kuberesourceversion_and_.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "27: import sqlalchemy as sa",
          "28: from alembic import op",
          "32: # revision identifiers, used by Alembic.",
          "33: revision = 'bef4f3d11e8b'",
          "",
          "[Removed Lines]",
          "30: from airflow.compat.sqlalchemy import inspect",
          "",
          "[Added Lines]",
          "29: from sqlalchemy import inspect",
          "",
          "---------------"
        ],
        "airflow/utils/db.py||airflow/utils/db.py": [
          "File: airflow/utils/db.py -> airflow/utils/db.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "34: import airflow",
          "35: from airflow import settings",
          "37: from airflow.configuration import conf",
          "38: from airflow.exceptions import AirflowException",
          "39: from airflow.models import import_all_models",
          "",
          "[Removed Lines]",
          "36: from airflow.compat.sqlalchemy import has_table",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1644:     migration_ctx = MigrationContext.configure(connection)",
          "1645:     version = migration_ctx._version",
          "1647:         version.drop(connection)",
          "",
          "[Removed Lines]",
          "1646:     if has_table(connection, version):",
          "",
          "[Added Lines]",
          "1645:     if inspect(connection).has_table(version.name):",
          "",
          "---------------"
        ],
        "airflow/www/fab_security/sqla/manager.py||airflow/www/fab_security/sqla/manager.py": [
          "File: airflow/www/fab_security/sqla/manager.py -> airflow/www/fab_security/sqla/manager.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "22: from flask_appbuilder import const as c",
          "23: from flask_appbuilder.models.sqla import Base",
          "24: from flask_appbuilder.models.sqla.interface import SQLAInterface",
          "26: from sqlalchemy.orm.exc import MultipleResultsFound",
          "27: from werkzeug.security import generate_password_hash",
          "30: from airflow.www.fab_security.manager import BaseSecurityManager",
          "31: from airflow.www.fab_security.sqla.models import (",
          "32:     Action,",
          "",
          "[Removed Lines]",
          "25: from sqlalchemy import and_, func, literal",
          "29: from airflow.compat import sqlalchemy as sqla_compat",
          "",
          "[Added Lines]",
          "25: from sqlalchemy import and_, func, inspect, literal",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "99:     def create_db(self):",
          "100:         try:",
          "101:             engine = self.get_session.get_bind(mapper=None, clause=None)",
          "103:             if \"ab_user\" not in inspector.get_table_names():",
          "104:                 log.info(c.LOGMSG_INF_SEC_NO_DB)",
          "105:                 Base.metadata.create_all(engine)",
          "",
          "[Removed Lines]",
          "102:             inspector = sqla_compat.inspect(engine)",
          "",
          "[Added Lines]",
          "101:             inspector = inspect(engine)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "5ae2ae531bba1d15975ba501c13f130959e8dd38",
      "candidate_info": {
        "commit_hash": "5ae2ae531bba1d15975ba501c13f130959e8dd38",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/5ae2ae531bba1d15975ba501c13f130959e8dd38",
        "files": [
          "airflow/utils/json.py",
          "airflow/www/app.py",
          "airflow/www/utils.py",
          "airflow/www/views.py",
          "tests/www/test_app.py"
        ],
        "message": "Correctly set json_provider_class on Flask app so it uses our encoder (#26554)\n\nSetting `json_provider_class` where we did had no effect, as it turns\nout `Flask()` sets `self.json = self.json_provider_class(self)`, so we\nwere setting it too late.\n\n(cherry picked from commit 378dfbe2fe266f17859dbabd34b9bc8cd5c904ab)",
        "before_after_code_files": [
          "airflow/utils/json.py||airflow/utils/json.py",
          "airflow/www/app.py||airflow/www/app.py",
          "airflow/www/utils.py||airflow/www/utils.py",
          "airflow/www/views.py||airflow/www/views.py",
          "tests/www/test_app.py||tests/www/test_app.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/26688"
        ],
        "olp_code_files": {
          "patch": [
            "airflow/www/app.py||airflow/www/app.py"
          ],
          "candidate": [
            "airflow/www/app.py||airflow/www/app.py"
          ]
        }
      },
      "candidate_diff": {
        "airflow/utils/json.py||airflow/utils/json.py": [
          "File: airflow/utils/json.py -> airflow/utils/json.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "17: # under the License.",
          "18: from __future__ import annotations",
          "20: import logging",
          "21: from datetime import date, datetime",
          "22: from decimal import Decimal",
          "26: from airflow.utils.timezone import convert_to_utc, is_naive",
          "",
          "[Removed Lines]",
          "24: from flask.json import JSONEncoder",
          "",
          "[Added Lines]",
          "20: import json",
          "25: from flask.json.provider import JSONProvider",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "40: log = logging.getLogger(__name__)",
          "44:     \"\"\"Custom Airflow json encoder implementation.\"\"\"",
          "46:     def __init__(self, *args, **kwargs):",
          "",
          "[Removed Lines]",
          "43: class AirflowJsonEncoder(JSONEncoder):",
          "",
          "[Added Lines]",
          "44: class AirflowJsonEncoder(json.JSONEncoder):",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "107:                 return {}",
          "109:         raise TypeError(f\"Object of type '{obj.__class__.__name__}' is not JSON serializable\")",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "113: class AirflowJsonProvider(JSONProvider):",
          "114:     \"\"\"JSON Provider for Flask app to use AirflowJsonEncoder.\"\"\"",
          "116:     ensure_ascii: bool = True",
          "117:     sort_keys: bool = True",
          "119:     def dumps(self, obj, **kwargs):",
          "120:         kwargs.setdefault('ensure_ascii', self.ensure_ascii)",
          "121:         kwargs.setdefault('sort_keys', self.sort_keys)",
          "122:         return json.dumps(obj, **kwargs, cls=AirflowJsonEncoder)",
          "124:     def loads(self, s: str | bytes, **kwargs):",
          "125:         return json.loads(s, **kwargs)",
          "",
          "---------------"
        ],
        "airflow/www/app.py||airflow/www/app.py": [
          "File: airflow/www/app.py -> airflow/www/app.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "32: from airflow.exceptions import AirflowConfigException, RemovedInAirflow3Warning",
          "33: from airflow.logging_config import configure_logging",
          "34: from airflow.models import import_all_models",
          "36: from airflow.www.extensions.init_appbuilder import init_appbuilder",
          "37: from airflow.www.extensions.init_appbuilder_links import init_appbuilder_links",
          "38: from airflow.www.extensions.init_dagbag import init_dagbag",
          "",
          "[Removed Lines]",
          "35: from airflow.utils.json import AirflowJsonEncoder",
          "",
          "[Added Lines]",
          "35: from airflow.utils.json import AirflowJsonProvider",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "109:         flask_app.config['SQLALCHEMY_ENGINE_OPTIONS'] = settings.prepare_engine_args()",
          "111:     # Configure the JSON encoder used by `|tojson` filter from Flask",
          "114:     csrf.init_app(flask_app)",
          "",
          "[Removed Lines]",
          "112:     flask_app.json_provider_class = AirflowJsonEncoder",
          "",
          "[Added Lines]",
          "112:     flask_app.json_provider_class = AirflowJsonProvider",
          "113:     flask_app.json = AirflowJsonProvider(flask_app)",
          "",
          "---------------"
        ],
        "airflow/www/utils.py||airflow/www/utils.py": [
          "File: airflow/www/utils.py -> airflow/www/utils.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "24: from urllib.parse import urlencode",
          "26: import sqlalchemy as sqla",
          "28: from flask.helpers import flash",
          "29: from flask_appbuilder.forms import FieldConverter",
          "30: from flask_appbuilder.models.filters import BaseFilter",
          "",
          "[Removed Lines]",
          "27: from flask import Response, request, url_for",
          "",
          "[Added Lines]",
          "27: from flask import request, url_for",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "47: from airflow.utils import timezone",
          "48: from airflow.utils.code_utils import get_python_source",
          "49: from airflow.utils.helpers import alchemy_to_dict",
          "51: from airflow.utils.state import State, TaskInstanceState",
          "52: from airflow.www.forms import DateTimeWithTimezoneField",
          "53: from airflow.www.widgets import AirflowDateTimePickerWidget",
          "",
          "[Removed Lines]",
          "50: from airflow.utils.json import AirflowJsonEncoder",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "322:     return (int(time.mktime(dttm.timetuple())) * 1000,)",
          "332: def make_cache_key(*args, **kwargs):",
          "333:     \"\"\"Used by cache to get a unique key per URL\"\"\"",
          "334:     path = request.path",
          "",
          "[Removed Lines]",
          "325: def json_response(obj):",
          "326:     \"\"\"Returns a json response from a json serializable python object\"\"\"",
          "327:     return Response(",
          "328:         response=json.dumps(obj, indent=4, cls=AirflowJsonEncoder), status=200, mimetype=\"application/json\"",
          "329:     )",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "airflow/www/views.py||airflow/www/views.py": [
          "File: airflow/www/views.py -> airflow/www/views.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "37: from urllib.parse import parse_qsl, unquote, urlencode, urlparse",
          "39: import configupdater",
          "40: import lazy_object_proxy",
          "41: import markupsafe",
          "42: import nvd3",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "40: import flask.json",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "107: from airflow.ti_deps.dependencies_deps import RUNNING_DEPS, SCHEDULER_QUEUED_DEPS",
          "108: from airflow.timetables.base import DataInterval, TimeRestriction",
          "109: from airflow.timetables.interval import CronDataIntervalTimetable",
          "111: from airflow.utils.airflow_flask_app import get_airflow_app",
          "112: from airflow.utils.dag_edges import dag_edges",
          "113: from airflow.utils.dates import infer_time_unit, scale_time_units",
          "",
          "[Removed Lines]",
          "110: from airflow.utils import json as utils_json, timezone, yaml",
          "",
          "[Added Lines]",
          "111: from airflow.utils import timezone, yaml",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "575:             'latest_scheduler_heartbeat': latest_scheduler_heartbeat,",
          "576:         }",
          "580:     @expose('/home')",
          "581:     @auth.has_access(",
          "",
          "[Removed Lines]",
          "578:         return wwwutils.json_response(payload)",
          "",
          "[Added Lines]",
          "579:         return flask.json.jsonify(payload)",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "856:             filter_dag_ids = allowed_dag_ids",
          "858:         if not filter_dag_ids:",
          "861:         payload = {}",
          "862:         dag_state_stats = dag_state_stats.filter(dr.dag_id.in_(filter_dag_ids))",
          "",
          "[Removed Lines]",
          "859:             return wwwutils.json_response({})",
          "",
          "[Added Lines]",
          "860:             return flask.json.jsonify({})",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "873:                 count = data.get(dag_id, {}).get(state, 0)",
          "874:                 payload[dag_id].append({'state': state, 'count': count})",
          "878:     @expose('/task_stats', methods=['POST'])",
          "879:     @auth.has_access(",
          "",
          "[Removed Lines]",
          "876:         return wwwutils.json_response(payload)",
          "",
          "[Added Lines]",
          "877:         return flask.json.jsonify(payload)",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "889:         allowed_dag_ids = get_airflow_app().appbuilder.sm.get_accessible_dag_ids(g.user)",
          "891:         if not allowed_dag_ids:",
          "894:         # Filter by post parameters",
          "895:         selected_dag_ids = {unquote(dag_id) for dag_id in request.form.getlist('dag_ids') if dag_id}",
          "",
          "[Removed Lines]",
          "892:             return wwwutils.json_response({})",
          "",
          "[Added Lines]",
          "893:             return flask.json.jsonify({})",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "983:             for state in State.task_states:",
          "984:                 count = data.get(dag_id, {}).get(state, 0)",
          "985:                 payload[dag_id].append({'state': state, 'count': count})",
          "988:     @expose('/last_dagruns', methods=['POST'])",
          "989:     @auth.has_access(",
          "",
          "[Removed Lines]",
          "986:         return wwwutils.json_response(payload)",
          "",
          "[Added Lines]",
          "987:         return flask.json.jsonify(payload)",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "1006:             filter_dag_ids = allowed_dag_ids",
          "1008:         if not filter_dag_ids:",
          "1011:         last_runs_subquery = (",
          "1012:             session.query(",
          "",
          "[Removed Lines]",
          "1009:             return wwwutils.json_response({})",
          "",
          "[Added Lines]",
          "1010:             return flask.json.jsonify({})",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "1046:             }",
          "1047:             for r in query",
          "1048:         }",
          "1051:     @expose('/code')",
          "1052:     @auth.has_access(",
          "",
          "[Removed Lines]",
          "1049:         return wwwutils.json_response(resp)",
          "",
          "[Added Lines]",
          "1050:         return flask.json.jsonify(resp)",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "2104:             filter_dag_ids = allowed_dag_ids",
          "2106:         if not filter_dag_ids:",
          "2109:         dags = (",
          "2110:             session.query(DagRun.dag_id, sqla.func.count(DagRun.id))",
          "",
          "[Removed Lines]",
          "2107:             return wwwutils.json_response([])",
          "",
          "[Added Lines]",
          "2108:             return flask.json.jsonify([])",
          "",
          "---------------",
          "--- Hunk 11 ---",
          "[Context before]",
          "2127:                     'max_active_runs': max_active_runs,",
          "2128:                 }",
          "2129:             )",
          "2132:     def _mark_dagrun_state_as_failed(self, dag_id, dag_run_id, confirmed):",
          "2133:         if not dag_run_id:",
          "",
          "[Removed Lines]",
          "2130:         return wwwutils.json_response(payload)",
          "",
          "[Added Lines]",
          "2131:         return flask.json.jsonify(payload)",
          "",
          "---------------",
          "--- Hunk 12 ---",
          "[Context before]",
          "3410:                 for ti in dag.get_task_instances(dttm, dttm)",
          "3411:             }",
          "3415:     @expose('/object/grid_data')",
          "3416:     @auth.has_access(",
          "",
          "[Removed Lines]",
          "3413:         return json.dumps(task_instances, cls=utils_json.AirflowJsonEncoder)",
          "",
          "[Added Lines]",
          "3414:         return flask.json.jsonify(task_instances)",
          "",
          "---------------",
          "--- Hunk 13 ---",
          "[Context before]",
          "3465:             }",
          "3466:         # avoid spaces to reduce payload size",
          "3467:         return (",
          "3469:             {'Content-Type': 'application/json; charset=utf-8'},",
          "3470:         )",
          "",
          "[Removed Lines]",
          "3468:             htmlsafe_json_dumps(data, separators=(',', ':'), cls=utils_json.AirflowJsonEncoder),",
          "",
          "[Added Lines]",
          "3469:             htmlsafe_json_dumps(data, separators=(',', ':'), dumps=flask.json.dumps),",
          "",
          "---------------",
          "--- Hunk 14 ---",
          "[Context before]",
          "3508:                 .all()",
          "3509:             ]",
          "3510:         return (",
          "3512:             {'Content-Type': 'application/json; charset=utf-8'},",
          "3513:         )",
          "",
          "[Removed Lines]",
          "3511:             htmlsafe_json_dumps(data, separators=(',', ':'), cls=utils_json.AirflowJsonEncoder),",
          "",
          "[Added Lines]",
          "3512:             htmlsafe_json_dumps(data, separators=(',', ':'), dumps=flask.json.dumps),",
          "",
          "---------------",
          "--- Hunk 15 ---",
          "[Context before]",
          "3545:         }",
          "3547:         return (",
          "3549:             {'Content-Type': 'application/json; charset=utf-8'},",
          "3550:         )",
          "",
          "[Removed Lines]",
          "3548:             htmlsafe_json_dumps(data, separators=(',', ':'), cls=utils_json.AirflowJsonEncoder),",
          "",
          "[Added Lines]",
          "3549:             htmlsafe_json_dumps(data, separators=(',', ':'), dumps=flask.json.dumps),",
          "",
          "---------------",
          "--- Hunk 16 ---",
          "[Context before]",
          "5205:         query = unquote(request.args.get('query', ''))",
          "5207:         if not query:",
          "5210:         # Provide suggestions of dag_ids and owners",
          "5211:         dag_ids_query = session.query(",
          "",
          "[Removed Lines]",
          "5208:             return wwwutils.json_response([])",
          "",
          "[Added Lines]",
          "5209:             return flask.json.jsonify([])",
          "",
          "---------------",
          "--- Hunk 17 ---",
          "[Context before]",
          "5239:         payload = [",
          "5240:             row._asdict() for row in dag_ids_query.union(owners_query).order_by('name').limit(10).all()",
          "5241:         ]",
          "5245: class DagDependenciesView(AirflowBaseView):",
          "",
          "[Removed Lines]",
          "5242:         return wwwutils.json_response(payload)",
          "",
          "[Added Lines]",
          "5243:         return flask.json.jsonify(payload)",
          "",
          "---------------"
        ],
        "tests/www/test_app.py||tests/www/test_app.py": [
          "File: tests/www/test_app.py -> tests/www/test_app.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "241:         output = capsys.readouterr()",
          "242:         assert \"/login/\" in output.out",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "245: def test_app_can_json_serialize_k8s_pod():",
          "246:     # This is mostly testing that we have correctly configured the JSON provider to use. Testing the k8s pos",
          "247:     # is a side-effect of that.",
          "248:     k8s = pytest.importorskip('kubernetes.client.models')",
          "250:     pod = k8s.V1Pod(spec=k8s.V1PodSpec(containers=[k8s.V1Container(name=\"base\")]))",
          "251:     app = application.cached_app(testing=True)",
          "252:     assert app.json.dumps(pod) == '{\"spec\": {\"containers\": [{\"name\": \"base\"}]}}'",
          "",
          "---------------"
        ]
      }
    }
  ]
}