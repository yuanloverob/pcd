{
  "cve_id": "CVE-2023-42780",
  "cve_desc": "Apache Airflow, versions prior to 2.7.2, contains a security vulnerability that allows authenticated users of Airflow to list warnings for all DAGs, even if the user had no permission to see those DAGs. It would reveal the dag_ids and the stack-traces of import errors for those DAGs with import errors.\nUsers of Apache Airflow are advised to upgrade to version 2.7.2 or newer to mitigate the risk associated with this vulnerability.\n\n",
  "repo": "apache/airflow",
  "patch_hash": "cf4eb3fb9b5cf4a8369b890e39523d4c05eed161",
  "patch_info": {
    "commit_hash": "cf4eb3fb9b5cf4a8369b890e39523d4c05eed161",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/cf4eb3fb9b5cf4a8369b890e39523d4c05eed161",
    "files": [
      "airflow/api_connexion/endpoints/dag_warning_endpoint.py",
      "tests/api_connexion/endpoints/test_dag_warning_endpoint.py"
    ],
    "message": "Fix dag warning endpoint permissions (#34355)\n\n* Fix dag warning endpoint permissions\n\n* update the query to have an accurate result for total entries and pagination\n\n* add unit tests\n\n* Update test_dag_warning_endpoint.py\n\nCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>\n\n---------\n\nCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>\n(cherry picked from commit 3570bbfbea69e2965f91b9964ce28bc268c68129)",
    "before_after_code_files": [
      "airflow/api_connexion/endpoints/dag_warning_endpoint.py||airflow/api_connexion/endpoints/dag_warning_endpoint.py",
      "tests/api_connexion/endpoints/test_dag_warning_endpoint.py||tests/api_connexion/endpoints/test_dag_warning_endpoint.py"
    ]
  },
  "patch_diff": {
    "airflow/api_connexion/endpoints/dag_warning_endpoint.py||airflow/api_connexion/endpoints/dag_warning_endpoint.py": [
      "File: airflow/api_connexion/endpoints/dag_warning_endpoint.py -> airflow/api_connexion/endpoints/dag_warning_endpoint.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "16: # under the License.",
      "17: from __future__ import annotations",
      "19: from sqlalchemy import select",
      "20: from sqlalchemy.orm import Session",
      "22: from airflow.api_connexion import security",
      "23: from airflow.api_connexion.parameters import apply_sorting, check_limit, format_parameters",
      "24: from airflow.api_connexion.schemas.dag_warning_schema import (",
      "25:     DagWarningCollection,",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "19: from flask import g",
      "24: from airflow.api_connexion.exceptions import PermissionDenied",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "28: from airflow.api_connexion.types import APIResponse",
      "29: from airflow.models.dagwarning import DagWarning as DagWarningModel",
      "30: from airflow.security import permissions",
      "31: from airflow.utils.db import get_query_count",
      "32: from airflow.utils.session import NEW_SESSION, provide_session",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "33: from airflow.utils.airflow_flask_app import get_airflow_app",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "52:     allowed_filter_attrs = [\"dag_id\", \"warning_type\", \"message\", \"timestamp\"]",
      "53:     query = select(DagWarningModel)",
      "54:     if dag_id:",
      "55:         query = query.where(DagWarningModel.dag_id == dag_id)",
      "56:     if warning_type:",
      "57:         query = query.where(DagWarningModel.warning_type == warning_type)",
      "58:     total_entries = get_query_count(query, session=session)",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "58:         if not get_airflow_app().appbuilder.sm.can_read_dag(dag_id, g.user):",
      "59:             raise PermissionDenied(detail=f\"User not allowed to access this DAG: {dag_id}\")",
      "61:     else:",
      "62:         readable_dags = get_airflow_app().appbuilder.sm.get_accessible_dag_ids(g.user)",
      "63:         query = query.where(DagWarningModel.dag_id.in_(readable_dags))",
      "",
      "---------------"
    ],
    "tests/api_connexion/endpoints/test_dag_warning_endpoint.py||tests/api_connexion/endpoints/test_dag_warning_endpoint.py": [
      "File: tests/api_connexion/endpoints/test_dag_warning_endpoint.py -> tests/api_connexion/endpoints/test_dag_warning_endpoint.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "35:         app,  # type:ignore",
      "36:         username=\"test\",",
      "37:         role_name=\"Test\",",
      "39:     )",
      "40:     create_user(app, username=\"test_no_permissions\", role_name=\"TestNoPermissions\")  # type: ignore",
      "42:     yield minimal_app_for_api",
      "44:     delete_user(app, username=\"test\")  # type: ignore",
      "45:     delete_user(app, username=\"test_no_permissions\")  # type: ignore",
      "48: class TestBaseDagWarning:",
      "",
      "[Removed Lines]",
      "38:         permissions=[(permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG_WARNING)],  # type: ignore",
      "",
      "[Added Lines]",
      "38:         permissions=[",
      "39:             (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG_WARNING),",
      "40:             (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG),",
      "41:         ],  # type: ignore",
      "44:     create_user(",
      "45:         app,  # type:ignore",
      "46:         username=\"test_with_dag2_read\",",
      "47:         role_name=\"TestWithDag2Read\",",
      "48:         permissions=[",
      "49:             (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG_WARNING),",
      "50:             (permissions.ACTION_CAN_READ, f\"{permissions.RESOURCE_DAG_PREFIX}dag2\"),",
      "51:         ],  # type: ignore",
      "52:     )",
      "58:     delete_user(app, username=\"test_with_dag2_read\")  # type: ignore",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "147:             \"/api/v1/dagWarnings\", environ_overrides={\"REMOTE_USER\": \"test_no_permissions\"}",
      "148:         )",
      "149:         assert response.status_code == 403",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "164:     def test_should_raise_403_forbidden_when_user_has_no_dag_read_permission(self):",
      "165:         response = self.client.get(",
      "166:             \"/api/v1/dagWarnings\",",
      "167:             environ_overrides={\"REMOTE_USER\": \"test_with_dag2_read\"},",
      "168:             query_string={\"dag_id\": \"dag1\"},",
      "169:         )",
      "170:         assert response.status_code == 403",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "b6af4adf0f89b06b167e29cf6690abb581e8a34c",
      "candidate_info": {
        "commit_hash": "b6af4adf0f89b06b167e29cf6690abb581e8a34c",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/b6af4adf0f89b06b167e29cf6690abb581e8a34c",
        "files": [
          "airflow/www/session.py"
        ],
        "message": "Fix SesssionExemptMixin spelling (#34696)\n\nCo-authored-by: David Kalamarides <david.kalamarides@capitalone.com>\n(cherry picked from commit 63945c71241e7b1b278068e1786e610facd569e0)",
        "before_after_code_files": [
          "airflow/www/session.py||airflow/www/session.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/www/session.py||airflow/www/session.py": [
          "File: airflow/www/session.py -> airflow/www/session.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "21: from flask_session.sessions import SqlAlchemySessionInterface",
          "25:     \"\"\"Exempt certain blueprints/paths from autogenerated sessions.\"\"\"",
          "27:     def save_session(self, *args, **kwargs):",
          "",
          "[Removed Lines]",
          "24: class SesssionExemptMixin:",
          "",
          "[Added Lines]",
          "24: class SessionExemptMixin:",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "33:         return super().save_session(*args, **kwargs)",
          "37:     \"\"\"Session interface that exempts some routes and stores session data in the database.\"\"\"",
          "41:     \"\"\"Session interface that exempts some routes and stores session data in a signed cookie.\"\"\"",
          "",
          "[Removed Lines]",
          "36: class AirflowDatabaseSessionInterface(SesssionExemptMixin, SqlAlchemySessionInterface):",
          "40: class AirflowSecureCookieSessionInterface(SesssionExemptMixin, SecureCookieSessionInterface):",
          "",
          "[Added Lines]",
          "36: class AirflowDatabaseSessionInterface(SessionExemptMixin, SqlAlchemySessionInterface):",
          "40: class AirflowSecureCookieSessionInterface(SessionExemptMixin, SecureCookieSessionInterface):",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "472d25c39ce0152dc5285adb64e07a16bea87319",
      "candidate_info": {
        "commit_hash": "472d25c39ce0152dc5285adb64e07a16bea87319",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/472d25c39ce0152dc5285adb64e07a16bea87319",
        "files": [
          "airflow/cli/commands/webserver_command.py",
          "airflow/providers/databricks/hooks/databricks_sql.py",
          "airflow/providers/google/cloud/operators/datafusion.py",
          "scripts/ci/pre_commit/pre_commit_unittest_testcase.py",
          "tests/core/test_configuration.py",
          "tests/decorators/test_setup_teardown.py"
        ],
        "message": "Do not create lists we don't need (#33519)\n\n(cherry picked from commit 4154cc04ce9702b09e6f13d423686fdf4cb7b877)",
        "before_after_code_files": [
          "airflow/cli/commands/webserver_command.py||airflow/cli/commands/webserver_command.py",
          "airflow/providers/databricks/hooks/databricks_sql.py||airflow/providers/databricks/hooks/databricks_sql.py",
          "airflow/providers/google/cloud/operators/datafusion.py||airflow/providers/google/cloud/operators/datafusion.py",
          "scripts/ci/pre_commit/pre_commit_unittest_testcase.py||scripts/ci/pre_commit/pre_commit_unittest_testcase.py",
          "tests/core/test_configuration.py||tests/core/test_configuration.py",
          "tests/decorators/test_setup_teardown.py||tests/decorators/test_setup_teardown.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/cli/commands/webserver_command.py||airflow/cli/commands/webserver_command.py": [
          "File: airflow/cli/commands/webserver_command.py -> airflow/cli/commands/webserver_command.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "144:                 pass",
          "145:             return False",
          "150:     def _get_num_workers_running(self) -> int:",
          "151:         \"\"\"Return number of running Gunicorn workers processes.\"\"\"",
          "",
          "[Removed Lines]",
          "147:         ready_workers = [proc for proc in workers if ready_prefix_on_cmdline(proc)]",
          "148:         return len(ready_workers)",
          "",
          "[Added Lines]",
          "147:         nb_ready_workers = sum(1 for proc in workers if ready_prefix_on_cmdline(proc))",
          "148:         return nb_ready_workers",
          "",
          "---------------"
        ],
        "airflow/providers/databricks/hooks/databricks_sql.py||airflow/providers/databricks/hooks/databricks_sql.py": [
          "File: airflow/providers/databricks/hooks/databricks_sql.py -> airflow/providers/databricks/hooks/databricks_sql.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "19: from contextlib import closing",
          "20: from copy import copy",
          "23: from databricks import sql  # type: ignore[attr-defined]",
          "26: from airflow.exceptions import AirflowException",
          "27: from airflow.providers.common.sql.hooks.sql import DbApiHook, return_single_query_results",
          "28: from airflow.providers.databricks.hooks.databricks_base import BaseDatabricksHook",
          "30: LIST_SQL_ENDPOINTS_ENDPOINT = (\"GET\", \"api/2.0/sql/endpoints\")",
          "",
          "[Removed Lines]",
          "21: from typing import Any, Callable, Iterable, Mapping, TypeVar, overload",
          "24: from databricks.sql.client import Connection  # type: ignore[attr-defined]",
          "",
          "[Added Lines]",
          "21: from typing import TYPE_CHECKING, Any, Callable, Iterable, Mapping, TypeVar, overload",
          "29: if TYPE_CHECKING:",
          "30:     from databricks.sql.client import Connection",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "91:         result = self._do_api_call(LIST_SQL_ENDPOINTS_ENDPOINT)",
          "92:         if \"endpoints\" not in result:",
          "93:             raise AirflowException(\"Can't list Databricks SQL endpoints\")",
          "99:     def get_conn(self) -> Connection:",
          "100:         \"\"\"Returns a Databricks SQL connection object.\"\"\"",
          "",
          "[Removed Lines]",
          "94:         lst = [endpoint for endpoint in result[\"endpoints\"] if endpoint[\"name\"] == endpoint_name]",
          "95:         if not lst:",
          "96:             raise AirflowException(f\"Can't f Databricks SQL endpoint with name '{endpoint_name}'\")",
          "97:         return lst[0]",
          "",
          "[Added Lines]",
          "96:         try:",
          "97:             endpoint = next(endpoint for endpoint in result[\"endpoints\"] if endpoint[\"name\"] == endpoint_name)",
          "98:         except StopIteration:",
          "99:             raise AirflowException(f\"Can't find Databricks SQL endpoint with name '{endpoint_name}'\")",
          "100:         else:",
          "101:             return endpoint",
          "",
          "---------------"
        ],
        "airflow/providers/google/cloud/operators/datafusion.py||airflow/providers/google/cloud/operators/datafusion.py": [
          "File: airflow/providers/google/cloud/operators/datafusion.py -> airflow/providers/google/cloud/operators/datafusion.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "44:     @staticmethod",
          "45:     def get_project_id(instance):",
          "46:         instance = instance[\"name\"]",
          "48:         return project_id",
          "",
          "[Removed Lines]",
          "47:         project_id = [x for x in instance.split(\"/\") if x.startswith(\"airflow\")][0]",
          "",
          "[Added Lines]",
          "47:         project_id = next(x for x in instance.split(\"/\") if x.startswith(\"airflow\"))",
          "",
          "---------------"
        ],
        "scripts/ci/pre_commit/pre_commit_unittest_testcase.py||scripts/ci/pre_commit/pre_commit_unittest_testcase.py": [
          "File: scripts/ci/pre_commit/pre_commit_unittest_testcase.py -> scripts/ci/pre_commit/pre_commit_unittest_testcase.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "29:     classes = [c for c in node.body if isinstance(c, ast.ClassDef)]",
          "30:     for c in classes:",
          "31:         # Some classes are returned as an ast.Attribute, some as an ast.Name object. Not quite sure why",
          "36:             found += 1",
          "37:             print(f\"The class {c.name} inherits from TestCase, please use pytest instead\")",
          "39:     return found",
          "42: def main(*args: str) -> int:",
          "46: if __name__ == \"__main__\":",
          "",
          "[Removed Lines]",
          "32:         parent_classes = [base.attr for base in c.bases if isinstance(base, ast.Attribute)]",
          "33:         parent_classes.extend([base.id for base in c.bases if isinstance(base, ast.Name)])",
          "35:         if \"TestCase\" in parent_classes:",
          "43:     return sum([check_test_file(file) for file in args[1:]])",
          "",
          "[Added Lines]",
          "32:         if any(",
          "33:             (isinstance(base, ast.Attribute) and base.attr == \"TestCase\")",
          "34:             or (isinstance(base, ast.Name) and base.id == \"TestCase\")",
          "35:             for base in c.bases",
          "36:         ):",
          "43:     return sum(check_test_file(file) for file in args[1:])",
          "",
          "---------------"
        ],
        "tests/core/test_configuration.py||tests/core/test_configuration.py": [
          "File: tests/core/test_configuration.py -> tests/core/test_configuration.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1570:         all_sections_including_defaults = airflow_cfg.get_sections_including_defaults()",
          "1571:         assert \"core\" in all_sections_including_defaults",
          "1572:         assert \"test-section\" in all_sections_including_defaults",
          "1575:     def test_get_options_including_defaults(self):",
          "1576:         airflow_cfg = AirflowConfigParser()",
          "",
          "[Removed Lines]",
          "1573:         assert len([section for section in all_sections_including_defaults if section == \"core\"]) == 1",
          "",
          "[Added Lines]",
          "1573:         assert sum(1 for section in all_sections_including_defaults if section == \"core\") == 1",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1594:         assert \"dags_folder\" in all_core_options_including_defaults",
          "1595:         assert \"test-value\" == airflow_cfg.get(\"core\", \"new-test-key\")",
          "1596:         assert \"test-runner\" == airflow_cfg.get(\"core\", \"task_runner\")",
          "1600: def test_sensitive_values():",
          "",
          "[Removed Lines]",
          "1597:         assert len([option for option in all_core_options_including_defaults if option == \"task_runner\"]) == 1",
          "",
          "[Added Lines]",
          "1597:         assert sum(1 for option in all_core_options_including_defaults if option == \"task_runner\") == 1",
          "",
          "---------------"
        ],
        "tests/decorators/test_setup_teardown.py||tests/decorators/test_setup_teardown.py": [
          "File: tests/decorators/test_setup_teardown.py -> tests/decorators/test_setup_teardown.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "255:             mytask2()",
          "257:         assert len(dag.task_group.children) == 6",
          "259:         assert dag.task_group.children[\"setuptask\"].is_setup",
          "260:         assert dag.task_group.children[\"teardowntask\"].is_teardown",
          "261:         assert dag.task_group.children[\"setuptask2\"].is_setup",
          "",
          "[Removed Lines]",
          "258:         assert [x for x in dag.tasks if not x.downstream_list]  # no deps have been set",
          "",
          "[Added Lines]",
          "258:         assert sum(1 for x in dag.tasks if not x.downstream_list) == 6",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "248b88b5fa75bc53fd167a8856e670f77a07dbc1",
      "candidate_info": {
        "commit_hash": "248b88b5fa75bc53fd167a8856e670f77a07dbc1",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/248b88b5fa75bc53fd167a8856e670f77a07dbc1",
        "files": [
          "dev/breeze/src/airflow_breeze/commands/kubernetes_commands.py",
          "dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "dev/breeze/src/airflow_breeze/params/build_ci_params.py",
          "dev/breeze/src/airflow_breeze/params/build_prod_params.py",
          "dev/breeze/src/airflow_breeze/params/doc_build_params.py",
          "dev/breeze/src/airflow_breeze/params/shell_params.py",
          "dev/breeze/src/airflow_breeze/utils/coertions.py",
          "dev/breeze/src/airflow_breeze/utils/docker_command_utils.py",
          "dev/breeze/src/airflow_breeze/utils/run_utils.py",
          "dev/breeze/src/airflow_breeze/utils/selective_checks.py",
          "dev/breeze/tests/test_exclude_from_matrix.py",
          "dev/provider_packages/prepare_provider_packages.py"
        ],
        "message": "Simplify conditions on len() in dev (#33562)\n\n(cherry picked from commit 50abdcea19a909d7f048bb0ee4ac59cc0bcbb37c)",
        "before_after_code_files": [
          "dev/breeze/src/airflow_breeze/commands/kubernetes_commands.py||dev/breeze/src/airflow_breeze/commands/kubernetes_commands.py",
          "dev/breeze/src/airflow_breeze/commands/release_management_commands.py||dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "dev/breeze/src/airflow_breeze/params/build_ci_params.py||dev/breeze/src/airflow_breeze/params/build_ci_params.py",
          "dev/breeze/src/airflow_breeze/params/build_prod_params.py||dev/breeze/src/airflow_breeze/params/build_prod_params.py",
          "dev/breeze/src/airflow_breeze/params/doc_build_params.py||dev/breeze/src/airflow_breeze/params/doc_build_params.py",
          "dev/breeze/src/airflow_breeze/params/shell_params.py||dev/breeze/src/airflow_breeze/params/shell_params.py",
          "dev/breeze/src/airflow_breeze/utils/coertions.py||dev/breeze/src/airflow_breeze/utils/coertions.py",
          "dev/breeze/src/airflow_breeze/utils/docker_command_utils.py||dev/breeze/src/airflow_breeze/utils/docker_command_utils.py",
          "dev/breeze/src/airflow_breeze/utils/run_utils.py||dev/breeze/src/airflow_breeze/utils/run_utils.py",
          "dev/breeze/src/airflow_breeze/utils/selective_checks.py||dev/breeze/src/airflow_breeze/utils/selective_checks.py",
          "dev/breeze/tests/test_exclude_from_matrix.py||dev/breeze/tests/test_exclude_from_matrix.py",
          "dev/provider_packages/prepare_provider_packages.py||dev/provider_packages/prepare_provider_packages.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/breeze/src/airflow_breeze/commands/kubernetes_commands.py||dev/breeze/src/airflow_breeze/commands/kubernetes_commands.py": [
          "File: dev/breeze/src/airflow_breeze/commands/kubernetes_commands.py -> dev/breeze/src/airflow_breeze/commands/kubernetes_commands.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "386: def _delete_all_clusters():",
          "387:     clusters = list(K8S_CLUSTERS_PATH.iterdir())",
          "391:         get_console().print(\"\\n[info]Deleting clusters\")",
          "392:         for cluster_name in clusters:",
          "393:             resolved_path = cluster_name.resolve()",
          "",
          "[Removed Lines]",
          "388:     if len(clusters) == 0:",
          "389:         get_console().print(\"\\n[warning]No clusters.\\n\")",
          "390:     else:",
          "",
          "[Added Lines]",
          "388:     if clusters:",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "407:                     shutil.rmtree(cluster_name.resolve(), ignore_errors=True)",
          "408:                 else:",
          "409:                     resolved_path.unlink()",
          "412: @kubernetes_group.command(",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "408:     else:",
          "409:         get_console().print(\"\\n[warning]No clusters.\\n\")",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "508:     make_sure_kubernetes_tools_are_installed()",
          "509:     if all:",
          "510:         clusters = list(K8S_CLUSTERS_PATH.iterdir())",
          "515:             failed = False",
          "516:             get_console().print(\"[info]\\nCluster status:\\n\")",
          "517:             for cluster_name in clusters:",
          "",
          "[Removed Lines]",
          "511:         if len(clusters) == 0:",
          "512:             get_console().print(\"\\n[warning]No clusters.\\n\")",
          "513:             sys.exit(1)",
          "514:         else:",
          "",
          "[Added Lines]",
          "511:         if clusters:",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "529:             if failed:",
          "530:                 get_console().print(\"\\n[error]Some clusters are not healthy!\\n\")",
          "531:                 sys.exit(1)",
          "532:     else:",
          "533:         if not _status(",
          "534:             python=python,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "529:         else:",
          "530:             get_console().print(\"\\n[warning]No clusters.\\n\")",
          "531:             sys.exit(1)",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "1217: def logs(python: str, kubernetes_version: str, all: bool):",
          "1218:     if all:",
          "1219:         clusters = list(K8S_CLUSTERS_PATH.iterdir())",
          "1224:             get_console().print(\"[info]\\nDumping cluster logs:\\n\")",
          "1225:             for cluster_name in clusters:",
          "1226:                 name = cluster_name.name",
          "",
          "[Removed Lines]",
          "1220:         if len(clusters) == 0:",
          "1221:             get_console().print(\"\\n[warning]No clusters.\\n\")",
          "1222:             sys.exit(1)",
          "1223:         else:",
          "",
          "[Added Lines]",
          "1220:         if clusters:",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "1229:                     get_console().print(f\"[warning]\\nCould not get cluster from {name}. Skipping.\\n\")",
          "1230:                     continue",
          "1231:                 _logs(python=found_python, kubernetes_version=found_kubernetes_version)",
          "1232:     else:",
          "1233:         _logs(python=python, kubernetes_version=kubernetes_version)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1229:         else:",
          "1230:             get_console().print(\"\\n[warning]No clusters.\\n\")",
          "1231:             sys.exit(1)",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/commands/release_management_commands.py||dev/breeze/src/airflow_breeze/commands/release_management_commands.py": [
          "File: dev/breeze/src/airflow_breeze/commands/release_management_commands.py -> dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "840:             \"Provide the path of cloned airflow-site repo\\n\"",
          "841:         )",
          "842:         sys.exit(1)",
          "844:         get_console().print(",
          "845:             \"\\n[error]You need to specify at least one package to generate back references for\\n\"",
          "846:         )",
          "",
          "[Removed Lines]",
          "843:     if len(packages_plus_all_providers) == 0:",
          "",
          "[Added Lines]",
          "843:     if not packages_plus_all_providers:",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1419:     updated_constraint: tuple[str],",
          "1420: ) -> None:",
          "1421:     airflow_versions_array = airflow_versions.split(\",\")",
          "1423:         get_console().print(\"[error]No airflow versions specified - you provided empty string[/]\")",
          "1424:         sys.exit(1)",
          "",
          "[Removed Lines]",
          "1422:     if len(airflow_versions_array) == 0:",
          "",
          "[Added Lines]",
          "1422:     if not airflow_versions_array:",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/params/build_ci_params.py||dev/breeze/src/airflow_breeze/params/build_ci_params.py": [
          "File: dev/breeze/src/airflow_breeze/params/build_ci_params.py -> dev/breeze/src/airflow_breeze/params/build_ci_params.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "55:         extra_ci_flags.extend(",
          "56:             [\"--build-arg\", f\"AIRFLOW_CONSTRAINTS_REFERENCE={self.airflow_constraints_reference}\"]",
          "57:         )",
          "59:             extra_ci_flags.extend(",
          "60:                 [\"--build-arg\", f\"AIRFLOW_CONSTRAINTS_LOCATION={self.airflow_constraints_location}\"]",
          "61:             )",
          "",
          "[Removed Lines]",
          "58:         if self.airflow_constraints_location is not None and len(self.airflow_constraints_location) > 0:",
          "",
          "[Added Lines]",
          "58:         if self.airflow_constraints_location:",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/params/build_prod_params.py||dev/breeze/src/airflow_breeze/params/build_prod_params.py": [
          "File: dev/breeze/src/airflow_breeze/params/build_prod_params.py -> dev/breeze/src/airflow_breeze/params/build_prod_params.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "113:     @property",
          "114:     def extra_docker_build_flags(self) -> list[str]:",
          "115:         extra_build_flags = []",
          "117:             AIRFLOW_INSTALLATION_METHOD = (",
          "118:                 \"https://github.com/apache/airflow/archive/\"",
          "119:                 + self.install_airflow_reference",
          "",
          "[Removed Lines]",
          "116:         if len(self.install_airflow_reference) > 0:",
          "",
          "[Added Lines]",
          "116:         if self.install_airflow_reference:",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "126:                 ]",
          "127:             )",
          "128:             extra_build_flags.extend(self.args_for_remote_install)",
          "130:             if not re.match(r\"^[0-9\\.]+((a|b|rc|alpha|beta|pre)[0-9]+)?$\", self.install_airflow_version):",
          "131:                 get_console().print(",
          "132:                     f\"\\n[error]ERROR: Bad value for install-airflow-version:{self.install_airflow_version}\"",
          "",
          "[Removed Lines]",
          "129:         elif len(self.install_airflow_version) > 0:",
          "",
          "[Added Lines]",
          "129:         elif self.install_airflow_version:",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/params/doc_build_params.py||dev/breeze/src/airflow_breeze/params/doc_build_params.py": [
          "File: dev/breeze/src/airflow_breeze/params/doc_build_params.py -> dev/breeze/src/airflow_breeze/params/doc_build_params.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "42:             doc_args.append(\"--one-pass-only\")",
          "43:         if AIRFLOW_BRANCH != \"main\":",
          "44:             doc_args.append(\"--disable-provider-checks\")",
          "46:             for single_filter in self.package_filter:",
          "47:                 doc_args.extend([\"--package-filter\", single_filter])",
          "48:         return doc_args",
          "",
          "[Removed Lines]",
          "45:         if self.package_filter and len(self.package_filter) > 0:",
          "",
          "[Added Lines]",
          "45:         if self.package_filter:",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/params/shell_params.py||dev/breeze/src/airflow_breeze/params/shell_params.py": [
          "File: dev/breeze/src/airflow_breeze/params/shell_params.py -> dev/breeze/src/airflow_breeze/params/shell_params.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "261:             integrations = ALL_INTEGRATIONS",
          "262:         else:",
          "263:             integrations = self.integration",
          "267:         if \"trino\" in integrations and \"kerberos\" not in integrations:",
          "268:             get_console().print(",
          "269:                 \"[warning]Adding `kerberos` integration as it is implicitly needed by trino\",",
          "",
          "[Removed Lines]",
          "264:         if len(integrations) > 0:",
          "265:             for integration in integrations:",
          "266:                 compose_file_list.append(DOCKER_COMPOSE_DIR / f\"integration-{integration}.yml\")",
          "",
          "[Added Lines]",
          "264:         for integration in integrations:",
          "265:             compose_file_list.append(DOCKER_COMPOSE_DIR / f\"integration-{integration}.yml\")",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "274:     @property",
          "275:     def command_passed(self):",
          "279:         return cmd",
          "281:     @property",
          "",
          "[Removed Lines]",
          "276:         cmd = None",
          "277:         if len(self.extra_args) > 0:",
          "278:             cmd = str(self.extra_args[0])",
          "",
          "[Added Lines]",
          "275:         cmd = str(self.extra_args[0]) if self.extra_args else None",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/utils/coertions.py||dev/breeze/src/airflow_breeze/utils/coertions.py": [
          "File: dev/breeze/src/airflow_breeze/utils/coertions.py -> dev/breeze/src/airflow_breeze/utils/coertions.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "32: def one_or_none_set(iterable: Iterable[bool]) -> bool:",
          "",
          "[Removed Lines]",
          "33:     return len([i for i in iterable if i]) in (0, 1)",
          "",
          "[Added Lines]",
          "33:     return sum(1 for i in iterable if i) in (0, 1)",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/utils/docker_command_utils.py||dev/breeze/src/airflow_breeze/utils/docker_command_utils.py": [
          "File: dev/breeze/src/airflow_breeze/utils/docker_command_utils.py -> dev/breeze/src/airflow_breeze/utils/docker_command_utils.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "399:         )",
          "400:     for optional_arg in image_params.optional_image_args:",
          "401:         param_value = get_env_variable_value(optional_arg, params=image_params)",
          "403:             args_command.append(\"--build-arg\")",
          "404:             args_command.append(optional_arg.upper() + \"=\" + param_value)",
          "405:     args_command.extend(image_params.docker_cache_directive)",
          "",
          "[Removed Lines]",
          "402:         if len(param_value) > 0:",
          "",
          "[Added Lines]",
          "402:         if param_value:",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "809:         get_console().print(\"[warning]Could not detect docker builder. Using default.[/]\")",
          "810:         return \"default\"",
          "811:     context_list = output.stdout.splitlines()",
          "813:         get_console().print(\"[warning]Could not detect docker builder. Using default.[/]\")",
          "814:         return \"default\"",
          "816:         get_console().print(f\"[info]Using {context_list[0]} as context.[/]\")",
          "817:         return context_list[0]",
          "819:         for preferred_context in PREFERRED_CONTEXTS:",
          "820:             if preferred_context in context_list:",
          "821:                 get_console().print(f\"[info]Using {preferred_context} as context.[/]\")",
          "",
          "[Removed Lines]",
          "812:     if len(context_list) == 0:",
          "815:     if len(context_list) == 1:",
          "818:     if len(context_list) > 1:",
          "",
          "[Added Lines]",
          "812:     if not context_list:",
          "815:     elif len(context_list) == 1:",
          "818:     else:",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/utils/run_utils.py||dev/breeze/src/airflow_breeze/utils/run_utils.py": [
          "File: dev/breeze/src/airflow_breeze/utils/run_utils.py -> dev/breeze/src/airflow_breeze/utils/run_utils.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "97:             return False",
          "98:         if _arg.startswith(\"-\"):",
          "99:             return True",
          "101:             return True",
          "102:         if _arg.startswith(\"/\"):",
          "103:             # Skip any absolute paths",
          "",
          "[Removed Lines]",
          "100:         if len(_arg) == 0:",
          "",
          "[Added Lines]",
          "100:         if not _arg:",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/utils/selective_checks.py||dev/breeze/src/airflow_breeze/utils/selective_checks.py": [
          "File: dev/breeze/src/airflow_breeze/utils/selective_checks.py -> dev/breeze/src/airflow_breeze/utils/selective_checks.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "275:             get_console().print(",
          "276:                 \"[info]This PR had `allow suspended provider changes` label set so it will continue\"",
          "277:             )",
          "279:         return None",
          "280:     for provider in list(all_providers):",
          "281:         all_providers.update(",
          "",
          "[Removed Lines]",
          "278:     if len(all_providers) == 0:",
          "",
          "[Added Lines]",
          "278:     if not all_providers:",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "356:         if self._github_event in [GithubEvents.PUSH, GithubEvents.SCHEDULE, GithubEvents.WORKFLOW_DISPATCH]:",
          "357:             get_console().print(f\"[warning]Full tests needed because event is {self._github_event}[/]\")",
          "358:             return True",
          "360:             get_console().print(\"[warning]Running everything because env files changed[/]\")",
          "361:             return True",
          "362:         if FULL_TESTS_NEEDED_LABEL in self._pr_labels:",
          "",
          "[Removed Lines]",
          "359:         if len(self._matching_files(FileGroupForCi.ENVIRONMENT_FILES, CI_FILE_GROUP_MATCHES)) > 0:",
          "",
          "[Added Lines]",
          "359:         if self._matching_files(FileGroupForCi.ENVIRONMENT_FILES, CI_FILE_GROUP_MATCHES):",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "496:             get_console().print(f\"[warning]{source_area} enabled because we are running everything[/]\")",
          "497:             return True",
          "498:         matched_files = self._matching_files(source_area, CI_FILE_GROUP_MATCHES)",
          "500:             get_console().print(",
          "501:                 f\"[warning]{source_area} enabled because it matched {len(matched_files)} changed files[/]\"",
          "502:             )",
          "",
          "[Removed Lines]",
          "499:         if len(matched_files) > 0:",
          "",
          "[Added Lines]",
          "499:         if matched_files:",
          "",
          "---------------"
        ],
        "dev/breeze/tests/test_exclude_from_matrix.py||dev/breeze/tests/test_exclude_from_matrix.py": [
          "File: dev/breeze/tests/test_exclude_from_matrix.py -> dev/breeze/tests/test_exclude_from_matrix.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "43:     exclusion_list = excluded_combos(list_1, list_2)",
          "44:     assert representative_list == expected_representative_list",
          "45:     assert len(representative_list) == len(list_1) * len(list_2) - len(exclusion_list)",
          "",
          "[Removed Lines]",
          "46:     assert len(set(representative_list) & set(exclusion_list)) == 0",
          "",
          "[Added Lines]",
          "46:     assert not set(representative_list).intersection(exclusion_list)",
          "",
          "---------------"
        ],
        "dev/provider_packages/prepare_provider_packages.py||dev/provider_packages/prepare_provider_packages.py": [
          "File: dev/provider_packages/prepare_provider_packages.py -> dev/provider_packages/prepare_provider_packages.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1058:     changes_table += changes_table_for_version",
          "1059:     if verbose:",
          "1060:         print_changes_table(changes_table)",
          "1064: def get_provider_details(provider_package_id: str) -> ProviderPackageDetails:",
          "",
          "[Removed Lines]",
          "1061:     return True, list_of_list_of_changes if len(list_of_list_of_changes) > 0 else None, changes_table",
          "",
          "[Added Lines]",
          "1061:     return True, list_of_list_of_changes or None, changes_table",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1337:         version_suffix=version_suffix,",
          "1338:     )",
          "1339:     jinja_context[\"DETAILED_CHANGES_RST\"] = changes",
          "1341:     update_changelog_rst(",
          "1342:         jinja_context,",
          "1343:         provider_package_id,",
          "",
          "[Removed Lines]",
          "1340:     jinja_context[\"DETAILED_CHANGES_PRESENT\"] = len(changes) > 0",
          "",
          "[Added Lines]",
          "1340:     jinja_context[\"DETAILED_CHANGES_PRESENT\"] = bool(changes)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "7252e2b09858757f439a161c63cd38c75eab4ca7",
      "candidate_info": {
        "commit_hash": "7252e2b09858757f439a161c63cd38c75eab4ca7",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/7252e2b09858757f439a161c63cd38c75eab4ca7",
        "files": [
          "airflow/jobs/backfill_job_runner.py",
          "airflow/utils/dates.py"
        ],
        "message": "remove unnecessary map and rewrite it using list in Airflow core (#33764)\n\n(cherry picked from commit 4e545c8190d9e2a085d98c5097b7284099c7cd75)",
        "before_after_code_files": [
          "airflow/jobs/backfill_job_runner.py||airflow/jobs/backfill_job_runner.py",
          "airflow/utils/dates.py||airflow/utils/dates.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/jobs/backfill_job_runner.py||airflow/jobs/backfill_job_runner.py": [
          "File: airflow/jobs/backfill_job_runner.py -> airflow/jobs/backfill_job_runner.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "743:             if all(key.map_index == -1 for key in ti_keys):",
          "744:                 headers = [\"DAG ID\", \"Task ID\", \"Run ID\", \"Try number\"]",
          "746:             else:",
          "747:                 headers = [\"DAG ID\", \"Task ID\", \"Run ID\", \"Map Index\", \"Try number\"]",
          "",
          "[Removed Lines]",
          "745:                 sorted_ti_keys = map(lambda k: k[0:4], sorted_ti_keys)",
          "",
          "[Added Lines]",
          "745:                 sorted_ti_keys = (k[0:4] for k in sorted_ti_keys)",
          "",
          "---------------"
        ],
        "airflow/utils/dates.py||airflow/utils/dates.py": [
          "File: airflow/utils/dates.py -> airflow/utils/dates.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "243: def scale_time_units(time_seconds_arr: Collection[float], unit: TimeUnit) -> Collection[float]:",
          "244:     \"\"\"Convert an array of time durations in seconds to the specified time unit.\"\"\"",
          "245:     if unit == \"minutes\":",
          "247:     elif unit == \"hours\":",
          "249:     elif unit == \"days\":",
          "251:     return time_seconds_arr",
          "",
          "[Removed Lines]",
          "246:         return list(map(lambda x: x / 60, time_seconds_arr))",
          "248:         return list(map(lambda x: x / (60 * 60), time_seconds_arr))",
          "250:         return list(map(lambda x: x / (24 * 60 * 60), time_seconds_arr))",
          "",
          "[Added Lines]",
          "246:         return [x / 60 for x in time_seconds_arr]",
          "248:         return [x / (60 * 60) for x in time_seconds_arr]",
          "250:         return [x / (24 * 60 * 60) for x in time_seconds_arr]",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "f502e2a88cf7123e2fdea5a4556b1fba4d1144cf",
      "candidate_info": {
        "commit_hash": "f502e2a88cf7123e2fdea5a4556b1fba4d1144cf",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/f502e2a88cf7123e2fdea5a4556b1fba4d1144cf",
        "files": [
          "airflow/www/utils.py",
          "dev/stats/calculate_statistics_provider_testing_issues.py",
          "scripts/in_container/update_quarantined_test_status.py",
          "tests/models/test_dagcode.py",
          "tests/www/test_utils.py"
        ],
        "message": "Refactor integer division (#34180)\n\n(cherry picked from commit 9571d37cad376ab931dadf02304387c5286c1938)",
        "before_after_code_files": [
          "airflow/www/utils.py||airflow/www/utils.py",
          "dev/stats/calculate_statistics_provider_testing_issues.py||dev/stats/calculate_statistics_provider_testing_issues.py",
          "scripts/in_container/update_quarantined_test_status.py||scripts/in_container/update_quarantined_test_status.py",
          "tests/models/test_dagcode.py||tests/models/test_dagcode.py",
          "tests/www/test_utils.py||tests/www/test_utils.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/www/utils.py||airflow/www/utils.py": [
          "File: airflow/www/utils.py -> airflow/www/utils.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "340:     output.append(previous_node.format(href_link=page_link, disabled=is_disabled))",
          "343:     last_page = num_of_pages - 1",
          "345:     if current_page <= mid or num_of_pages < window:",
          "",
          "[Removed Lines]",
          "342:     mid = int(window / 2)",
          "",
          "[Added Lines]",
          "342:     mid = window // 2",
          "",
          "---------------"
        ],
        "dev/stats/calculate_statistics_provider_testing_issues.py||dev/stats/calculate_statistics_provider_testing_issues.py": [
          "File: dev/stats/calculate_statistics_provider_testing_issues.py -> dev/stats/calculate_statistics_provider_testing_issues.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "78:     users_commented: set[str]",
          "80:     def percent_tested(self) -> int:",
          "83:     def num_involved_users_who_commented(self) -> int:",
          "84:         return len(self.users_involved.intersection(self.users_commented))",
          "",
          "[Removed Lines]",
          "81:         return int(100.0 * self.tested_issues / self.num_issues)",
          "",
          "[Added Lines]",
          "81:         return 100 * self.tested_issues // self.num_issues",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "87:         return len(self.users_commented - self.users_involved)",
          "89:     def percent_commented_among_involved(self) -> int:",
          "92:     def __str__(self):",
          "93:         return (",
          "",
          "[Removed Lines]",
          "90:         return int(100.0 * self.num_involved_users_who_commented() / len(self.users_involved))",
          "",
          "[Added Lines]",
          "90:         return 100 * self.num_involved_users_who_commented() // len(self.users_involved)",
          "",
          "---------------"
        ],
        "scripts/in_container/update_quarantined_test_status.py||scripts/in_container/update_quarantined_test_status.py": [
          "File: scripts/in_container/update_quarantined_test_status.py -> scripts/in_container/update_quarantined_test_status.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "152:         return \"Stable\"",
          "153:     if all(history.states[0 : num_runs - 1]):",
          "154:         return \"Just one more\"",
          "156:         return \"Almost there\"",
          "157:     return \"Flaky\"",
          "",
          "[Removed Lines]",
          "155:     if all(history.states[0 : int(num_runs / 2)]):",
          "",
          "[Added Lines]",
          "155:     if all(history.states[0 : num_runs // 2]):",
          "",
          "---------------"
        ],
        "tests/models/test_dagcode.py||tests/models/test_dagcode.py": [
          "File: tests/models/test_dagcode.py -> tests/models/test_dagcode.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "81:         \"\"\"Dg code can be bulk written into database.\"\"\"",
          "82:         example_dags = make_example_dags(example_dags_module)",
          "83:         files = [dag.fileloc for dag in example_dags.values()]",
          "85:         with create_session() as session:",
          "86:             DagCode.bulk_sync_to_db(half_files, session=session)",
          "87:             session.commit()",
          "",
          "[Removed Lines]",
          "84:         half_files = files[: int(len(files) / 2)]",
          "",
          "[Added Lines]",
          "84:         half_files = files[: len(files) // 2]",
          "",
          "---------------"
        ],
        "tests/www/test_utils.py||tests/www/test_utils.py": [
          "File: tests/www/test_utils.py -> tests/www/test_utils.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "69:         assert min(window, total_pages) + extra_links == len(ulist_items)",
          "71:         page_items = ulist_items[2:-2]",
          "73:         all_nodes = []",
          "74:         pages = []",
          "",
          "[Removed Lines]",
          "72:         mid = int(len(page_items) / 2)",
          "",
          "[Added Lines]",
          "72:         mid = len(page_items) // 2",
          "",
          "---------------"
        ]
      }
    }
  ]
}