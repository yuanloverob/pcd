{
  "cve_id": "CVE-2023-50944",
  "cve_desc": "Apache Airflow, versions before 2.8.1, have a vulnerability that allows an authenticated user to access the source code of a DAG to which they don't have access.\u00a0This vulnerability is considered low since it requires an authenticated user to exploit it. Users are recommended to upgrade to version 2.8.1, which fixes this issue.",
  "repo": "apache/airflow",
  "patch_hash": "8d76538d6e105947272b000581c6fabec20146b1",
  "patch_info": {
    "commit_hash": "8d76538d6e105947272b000581c6fabec20146b1",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/8d76538d6e105947272b000581c6fabec20146b1",
    "files": [
      "airflow/api_connexion/endpoints/dag_source_endpoint.py",
      "airflow/models/dagcode.py",
      "tests/api_connexion/endpoints/test_dag_source_endpoint.py"
    ],
    "message": "Check DAG read permission before accessing DAG code (#36257)\n\n(cherry picked from commit 30ea37e0d247ce54c2d25b115e807fdb0074d795)",
    "before_after_code_files": [
      "airflow/api_connexion/endpoints/dag_source_endpoint.py||airflow/api_connexion/endpoints/dag_source_endpoint.py",
      "airflow/models/dagcode.py||airflow/models/dagcode.py",
      "tests/api_connexion/endpoints/test_dag_source_endpoint.py||tests/api_connexion/endpoints/test_dag_source_endpoint.py"
    ]
  },
  "patch_diff": {
    "airflow/api_connexion/endpoints/dag_source_endpoint.py||airflow/api_connexion/endpoints/dag_source_endpoint.py": [
      "File: airflow/api_connexion/endpoints/dag_source_endpoint.py -> airflow/api_connexion/endpoints/dag_source_endpoint.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "17: from __future__ import annotations",
      "19: from http import HTTPStatus",
      "21: from flask import Response, current_app, request",
      "22: from itsdangerous import BadSignature, URLSafeSerializer",
      "24: from airflow.api_connexion import security",
      "26: from airflow.api_connexion.schemas.dag_source_schema import dag_source_schema",
      "27: from airflow.auth.managers.models.resource_details import DagAccessEntity",
      "28: from airflow.models.dagcode import DagCode",
      "31: @security.requires_access_dag(\"GET\", DagAccessEntity.CODE)",
      "33:     \"\"\"Get source code using file token.\"\"\"",
      "34:     secret_key = current_app.config[\"SECRET_KEY\"]",
      "35:     auth_s = URLSafeSerializer(secret_key)",
      "36:     try:",
      "37:         path = auth_s.loads(file_token)",
      "39:     except (BadSignature, FileNotFoundError):",
      "40:         raise NotFound(\"Dag source not found\")",
      "",
      "[Removed Lines]",
      "25: from airflow.api_connexion.exceptions import NotFound",
      "32: def get_dag_source(*, file_token: str) -> Response:",
      "38:         dag_source = DagCode.code(path)",
      "",
      "[Added Lines]",
      "20: from typing import TYPE_CHECKING",
      "26: from airflow.api_connexion.exceptions import NotFound, PermissionDenied",
      "28: from airflow.api_connexion.security import get_readable_dags",
      "30: from airflow.models.dag import DagModel",
      "32: from airflow.utils.session import NEW_SESSION, provide_session",
      "34: if TYPE_CHECKING:",
      "35:     from sqlalchemy.orm import Session",
      "39: @provide_session",
      "40: def get_dag_source(*, file_token: str, session: Session = NEW_SESSION) -> Response:",
      "46:         dag_ids = session.query(DagModel.dag_id).filter(DagModel.fileloc == path).all()",
      "47:         readable_dags = get_readable_dags()",
      "48:         # Check if user has read access to all the DAGs defined in the file",
      "49:         if any(dag_id[0] not in readable_dags for dag_id in dag_ids):",
      "50:             raise PermissionDenied()",
      "51:         dag_source = DagCode.code(path, session=session)",
      "",
      "---------------"
    ],
    "airflow/models/dagcode.py||airflow/models/dagcode.py": [
      "File: airflow/models/dagcode.py -> airflow/models/dagcode.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "177:         return cls.code(fileloc)",
      "179:     @classmethod",
      "181:         \"\"\"Return source code for this DagCode object.",
      "183:         :return: source code as string",
      "184:         \"\"\"",
      "187:     @staticmethod",
      "188:     def _get_code_from_file(fileloc):",
      "",
      "[Removed Lines]",
      "180:     def code(cls, fileloc) -> str:",
      "185:         return cls._get_code_from_db(fileloc)",
      "",
      "[Added Lines]",
      "180:     @provide_session",
      "181:     def code(cls, fileloc, session: Session = NEW_SESSION) -> str:",
      "186:         return cls._get_code_from_db(fileloc, session)",
      "",
      "---------------"
    ],
    "tests/api_connexion/endpoints/test_dag_source_endpoint.py||tests/api_connexion/endpoints/test_dag_source_endpoint.py": [
      "File: tests/api_connexion/endpoints/test_dag_source_endpoint.py -> tests/api_connexion/endpoints/test_dag_source_endpoint.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "35: ROOT_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), os.pardir, os.pardir))",
      "36: EXAMPLE_DAG_FILE = os.path.join(\"airflow\", \"example_dags\", \"example_bash_operator.py\")",
      "39: @pytest.fixture(scope=\"module\")",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "37: EXAMPLE_DAG_ID = \"example_bash_operator\"",
      "38: TEST_DAG_ID = \"latest_only\"",
      "39: NOT_READABLE_DAG_ID = \"latest_only_with_trigger\"",
      "40: TEST_MULTIPLE_DAGS_ID = \"dataset_produces_1\"",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "45:         role_name=\"Test\",",
      "46:         permissions=[(permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG_CODE)],  # type: ignore",
      "47:     )",
      "48:     create_user(app, username=\"test_no_permissions\", role_name=\"TestNoPermissions\")  # type: ignore",
      "50:     yield app",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "52:     app.appbuilder.sm.sync_perm_for_dag(  # type: ignore",
      "53:         TEST_DAG_ID,",
      "54:         access_control={\"Test\": [permissions.ACTION_CAN_READ]},",
      "55:     )",
      "56:     app.appbuilder.sm.sync_perm_for_dag(  # type: ignore",
      "57:         EXAMPLE_DAG_ID,",
      "58:         access_control={\"Test\": [permissions.ACTION_CAN_READ]},",
      "59:     )",
      "60:     app.appbuilder.sm.sync_perm_for_dag(  # type: ignore",
      "61:         TEST_MULTIPLE_DAGS_ID,",
      "62:         access_control={\"Test\": [permissions.ACTION_CAN_READ]},",
      "63:     )",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "80:     def test_should_respond_200_text(self, url_safe_serializer):",
      "81:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
      "82:         dagbag.sync_to_db()",
      "87:         response = self.client.get(",
      "88:             url, headers={\"Accept\": \"text/plain\"}, environ_overrides={\"REMOTE_USER\": \"test\"}",
      "89:         )",
      "",
      "[Removed Lines]",
      "83:         first_dag: DAG = next(iter(dagbag.dags.values()))",
      "84:         dag_docstring = self._get_dag_file_docstring(first_dag.fileloc)",
      "86:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(first_dag.fileloc)}\"",
      "",
      "[Added Lines]",
      "99:         test_dag: DAG = dagbag.dags[TEST_DAG_ID]",
      "100:         dag_docstring = self._get_dag_file_docstring(test_dag.fileloc)",
      "102:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(test_dag.fileloc)}\"",
      "",
      "---------------",
      "--- Hunk 4 ---",
      "[Context before]",
      "95:     def test_should_respond_200_json(self, url_safe_serializer):",
      "96:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
      "97:         dagbag.sync_to_db()",
      "102:         response = self.client.get(",
      "103:             url, headers={\"Accept\": \"application/json\"}, environ_overrides={\"REMOTE_USER\": \"test\"}",
      "104:         )",
      "",
      "[Removed Lines]",
      "98:         first_dag: DAG = next(iter(dagbag.dags.values()))",
      "99:         dag_docstring = self._get_dag_file_docstring(first_dag.fileloc)",
      "101:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(first_dag.fileloc)}\"",
      "",
      "[Added Lines]",
      "114:         test_dag: DAG = dagbag.dags[TEST_DAG_ID]",
      "115:         dag_docstring = self._get_dag_file_docstring(test_dag.fileloc)",
      "117:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(test_dag.fileloc)}\"",
      "",
      "---------------",
      "--- Hunk 5 ---",
      "[Context before]",
      "110:     def test_should_respond_406(self, url_safe_serializer):",
      "111:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
      "112:         dagbag.sync_to_db()",
      "116:         response = self.client.get(",
      "117:             url, headers={\"Accept\": \"image/webp\"}, environ_overrides={\"REMOTE_USER\": \"test\"}",
      "118:         )",
      "",
      "[Removed Lines]",
      "113:         first_dag: DAG = next(iter(dagbag.dags.values()))",
      "115:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(first_dag.fileloc)}\"",
      "",
      "[Added Lines]",
      "129:         test_dag: DAG = dagbag.dags[TEST_DAG_ID]",
      "131:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(test_dag.fileloc)}\"",
      "",
      "---------------",
      "--- Hunk 6 ---",
      "[Context before]",
      "151:             environ_overrides={\"REMOTE_USER\": \"test_no_permissions\"},",
      "152:         )",
      "153:         assert response.status_code == 403",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "171:     def test_should_respond_403_not_readable(self, url_safe_serializer):",
      "172:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
      "173:         dagbag.sync_to_db()",
      "174:         dag: DAG = dagbag.dags[NOT_READABLE_DAG_ID]",
      "176:         response = self.client.get(",
      "177:             f\"/api/v1/dagSources/{url_safe_serializer.dumps(dag.fileloc)}\",",
      "178:             headers={\"Accept\": \"text/plain\"},",
      "179:             environ_overrides={\"REMOTE_USER\": \"test\"},",
      "180:         )",
      "181:         read_dag = self.client.get(",
      "182:             f\"/api/v1/dags/{NOT_READABLE_DAG_ID}\",",
      "183:             environ_overrides={\"REMOTE_USER\": \"test\"},",
      "184:         )",
      "185:         assert response.status_code == 403",
      "186:         assert read_dag.status_code == 403",
      "188:     def test_should_respond_403_some_dags_not_readable_in_the_file(self, url_safe_serializer):",
      "189:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
      "190:         dagbag.sync_to_db()",
      "191:         dag: DAG = dagbag.dags[TEST_MULTIPLE_DAGS_ID]",
      "193:         response = self.client.get(",
      "194:             f\"/api/v1/dagSources/{url_safe_serializer.dumps(dag.fileloc)}\",",
      "195:             headers={\"Accept\": \"text/plain\"},",
      "196:             environ_overrides={\"REMOTE_USER\": \"test\"},",
      "197:         )",
      "199:         read_dag = self.client.get(",
      "200:             f\"/api/v1/dags/{TEST_MULTIPLE_DAGS_ID}\",",
      "201:             environ_overrides={\"REMOTE_USER\": \"test\"},",
      "202:         )",
      "203:         assert response.status_code == 403",
      "204:         assert read_dag.status_code == 200",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "e1d2e7f3b52c40e34a5b947fc64cc6e641841c81",
      "candidate_info": {
        "commit_hash": "e1d2e7f3b52c40e34a5b947fc64cc6e641841c81",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/e1d2e7f3b52c40e34a5b947fc64cc6e641841c81",
        "files": [
          "BREEZE.rst",
          "TESTING.rst",
          "scripts/ci/pre_commit/common_precommit_utils.py"
        ],
        "message": "Update BREEZE.rst with different test example (#36234)\n\nUpdating the breeze docs with different pytest example as the function mentioned in the example is removed from the test_core.py\n\n(cherry picked from commit 71c726d52d5a8a30f59268cc175560a4244c8016)",
        "before_after_code_files": [
          "scripts/ci/pre_commit/common_precommit_utils.py||scripts/ci/pre_commit/common_precommit_utils.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "scripts/ci/pre_commit/common_precommit_utils.py||scripts/ci/pre_commit/common_precommit_utils.py": [
          "File: scripts/ci/pre_commit/common_precommit_utils.py -> scripts/ci/pre_commit/common_precommit_utils.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "80:     if os.environ.get(\"SKIP_BREEZE_PRE_COMMITS\"):",
          "81:         console.print(\"[yellow]Skipping breeze pre-commit as SKIP_BREEZE_PRE_COMMIT is set\")",
          "83:     if shutil.which(\"breeze\") is None:",
          "84:         console.print(",
          "85:             \"[red]The `breeze` command is not on path.[/]\\n\\n\"",
          "",
          "[Removed Lines]",
          "82:         sys.exit(1)",
          "",
          "[Added Lines]",
          "82:         sys.exit(0)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "cde72b6e0e6f7a03e3840a931f55feb0760b5134",
      "candidate_info": {
        "commit_hash": "cde72b6e0e6f7a03e3840a931f55feb0760b5134",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/cde72b6e0e6f7a03e3840a931f55feb0760b5134",
        "files": [
          "airflow/providers/hashicorp/_internal_client/vault_client.py",
          "airflow/providers/hashicorp/provider.yaml",
          "generated/provider_dependencies.json",
          "tests/providers/hashicorp/_internal_client/test_vault_client.py",
          "tests/providers/hashicorp/hooks/test_vault.py",
          "tests/providers/hashicorp/secrets/test_vault.py"
        ],
        "message": "Explicitly passing `raise_on_deleted_version=True` to `read_secret_version` in Hashicorp operator (#36532)\n\n* explicitly passing raise_on_deleted_version=True to read_secret_version\n\n* fix tests\n\n* update hvac version\n\n(cherry picked from commit cd5ab08d95aaf4c65e56a91f1843d04c09f27cb1)",
        "before_after_code_files": [
          "airflow/providers/hashicorp/_internal_client/vault_client.py||airflow/providers/hashicorp/_internal_client/vault_client.py",
          "tests/providers/hashicorp/_internal_client/test_vault_client.py||tests/providers/hashicorp/_internal_client/test_vault_client.py",
          "tests/providers/hashicorp/hooks/test_vault.py||tests/providers/hashicorp/hooks/test_vault.py",
          "tests/providers/hashicorp/secrets/test_vault.py||tests/providers/hashicorp/secrets/test_vault.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/providers/hashicorp/_internal_client/vault_client.py||airflow/providers/hashicorp/_internal_client/vault_client.py": [
          "File: airflow/providers/hashicorp/_internal_client/vault_client.py -> airflow/providers/hashicorp/_internal_client/vault_client.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "373:                 response = self.client.secrets.kv.v1.read_secret(path=secret_path, mount_point=mount_point)",
          "374:             else:",
          "375:                 response = self.client.secrets.kv.v2.read_secret_version(",
          "377:                 )",
          "378:         except InvalidPath:",
          "379:             self.log.debug(\"Secret not found %s with mount point %s\", secret_path, mount_point)",
          "",
          "[Removed Lines]",
          "376:                     path=secret_path, mount_point=mount_point, version=secret_version",
          "",
          "[Added Lines]",
          "376:                     path=secret_path,",
          "377:                     mount_point=mount_point,",
          "378:                     version=secret_version,",
          "379:                     raise_on_deleted_version=True,",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "422:         try:",
          "423:             mount_point, secret_path = self._parse_secret_path(secret_path)",
          "424:             return self.client.secrets.kv.v2.read_secret_version(",
          "426:             )",
          "427:         except InvalidPath:",
          "428:             self.log.debug(",
          "",
          "[Removed Lines]",
          "425:                 path=secret_path, mount_point=mount_point, version=secret_version",
          "",
          "[Added Lines]",
          "428:                 path=secret_path,",
          "429:                 mount_point=mount_point,",
          "430:                 version=secret_version,",
          "431:                 raise_on_deleted_version=True,",
          "",
          "---------------"
        ],
        "tests/providers/hashicorp/_internal_client/test_vault_client.py||tests/providers/hashicorp/_internal_client/test_vault_client.py": [
          "File: tests/providers/hashicorp/_internal_client/test_vault_client.py -> tests/providers/hashicorp/_internal_client/test_vault_client.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "641:         secret = vault_client.get_secret(secret_path=\"missing\")",
          "642:         assert secret is None",
          "643:         mock_client.secrets.kv.v2.read_secret_version.assert_called_once_with(",
          "645:         )",
          "647:     @mock.patch(\"airflow.providers.hashicorp._internal_client.vault_client.hvac\")",
          "",
          "[Removed Lines]",
          "644:             mount_point=\"secret\", path=\"missing\", version=None",
          "",
          "[Added Lines]",
          "644:             mount_point=\"secret\", path=\"missing\", version=None, raise_on_deleted_version=True",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "661:         assert secret is None",
          "662:         assert \"secret\" == vault_client.mount_point",
          "663:         mock_client.secrets.kv.v2.read_secret_version.assert_called_once_with(",
          "665:         )",
          "667:     @mock.patch(\"airflow.providers.hashicorp._internal_client.vault_client.hvac\")",
          "",
          "[Removed Lines]",
          "664:             mount_point=\"secret\", path=\"missing\", version=None",
          "",
          "[Added Lines]",
          "664:             mount_point=\"secret\", path=\"missing\", version=None, raise_on_deleted_version=True",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "716:         secret = vault_client.get_secret(secret_path=\"path/to/secret\")",
          "717:         assert {\"secret_key\": \"secret_value\"} == secret",
          "718:         mock_client.secrets.kv.v2.read_secret_version.assert_called_once_with(",
          "720:         )",
          "722:     @mock.patch(\"airflow.providers.hashicorp._internal_client.vault_client.hvac\")",
          "",
          "[Removed Lines]",
          "719:             mount_point=\"secret\", path=\"path/to/secret\", version=None",
          "",
          "[Added Lines]",
          "719:             mount_point=\"secret\", path=\"path/to/secret\", version=None, raise_on_deleted_version=True",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "754:         secret = vault_client.get_secret(secret_path=\"mount_point/path/to/secret\")",
          "755:         assert {\"secret_key\": \"secret_value\"} == secret",
          "756:         mock_client.secrets.kv.v2.read_secret_version.assert_called_once_with(",
          "758:         )",
          "760:     @mock.patch(\"airflow.providers.hashicorp._internal_client.vault_client.hvac\")",
          "",
          "[Removed Lines]",
          "757:             mount_point=\"mount_point\", path=\"path/to/secret\", version=None",
          "",
          "[Added Lines]",
          "757:             mount_point=\"mount_point\", path=\"path/to/secret\", version=None, raise_on_deleted_version=True",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "791:         secret = vault_client.get_secret(secret_path=\"missing\", secret_version=1)",
          "792:         assert {\"secret_key\": \"secret_value\"} == secret",
          "793:         mock_client.secrets.kv.v2.read_secret_version.assert_called_once_with(",
          "795:         )",
          "797:     @mock.patch(\"airflow.providers.hashicorp._internal_client.vault_client.hvac\")",
          "",
          "[Removed Lines]",
          "794:             mount_point=\"secret\", path=\"missing\", version=1",
          "",
          "[Added Lines]",
          "794:             mount_point=\"secret\", path=\"missing\", version=1, raise_on_deleted_version=True",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "1015:             \"auth\": None,",
          "1016:         } == metadata",
          "1017:         mock_client.secrets.kv.v2.read_secret_version.assert_called_once_with(",
          "1019:         )",
          "1021:     @mock.patch(\"airflow.providers.hashicorp._internal_client.vault_client.hvac\")",
          "",
          "[Removed Lines]",
          "1018:             mount_point=\"secret\", path=\"missing\", version=None",
          "",
          "[Added Lines]",
          "1018:             mount_point=\"secret\", path=\"missing\", version=None, raise_on_deleted_version=True",
          "",
          "---------------"
        ],
        "tests/providers/hashicorp/hooks/test_vault.py||tests/providers/hashicorp/hooks/test_vault.py": [
          "File: tests/providers/hashicorp/hooks/test_vault.py -> tests/providers/hashicorp/hooks/test_vault.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1005:         secret = test_hook.get_secret(secret_path=\"missing\")",
          "1006:         assert {\"secret_key\": \"secret_value\"} == secret",
          "1007:         mock_client.secrets.kv.v2.read_secret_version.assert_called_once_with(",
          "1009:         )",
          "1011:     @mock.patch(\"airflow.providers.hashicorp.hooks.vault.VaultHook.get_connection\")",
          "",
          "[Removed Lines]",
          "1008:             mount_point=\"secret\", path=\"missing\", version=None",
          "",
          "[Added Lines]",
          "1008:             mount_point=\"secret\", path=\"missing\", version=None, raise_on_deleted_version=True",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1044:         secret = test_hook.get_secret(secret_path=\"missing\", secret_version=1)",
          "1045:         assert {\"secret_key\": \"secret_value\"} == secret",
          "1046:         mock_client.secrets.kv.v2.read_secret_version.assert_called_once_with(",
          "1048:         )",
          "1050:     @mock.patch(\"airflow.providers.hashicorp.hooks.vault.VaultHook.get_connection\")",
          "",
          "[Removed Lines]",
          "1047:             mount_point=\"secret\", path=\"missing\", version=1",
          "",
          "[Added Lines]",
          "1047:             mount_point=\"secret\", path=\"missing\", version=1, raise_on_deleted_version=True",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "1189:             \"auth\": None,",
          "1190:         } == metadata",
          "1191:         mock_client.secrets.kv.v2.read_secret_version.assert_called_once_with(",
          "1193:         )",
          "1195:     @mock.patch(\"airflow.providers.hashicorp.hooks.vault.VaultHook.get_connection\")",
          "",
          "[Removed Lines]",
          "1192:             mount_point=\"secret\", path=\"missing\", version=None",
          "",
          "[Added Lines]",
          "1192:             mount_point=\"secret\", path=\"missing\", version=None, raise_on_deleted_version=True",
          "",
          "---------------"
        ],
        "tests/providers/hashicorp/secrets/test_vault.py||tests/providers/hashicorp/secrets/test_vault.py": [
          "File: tests/providers/hashicorp/secrets/test_vault.py -> tests/providers/hashicorp/secrets/test_vault.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "302:         test_client = VaultBackend(**kwargs)",
          "303:         assert test_client.get_conn_uri(conn_id=\"test_mysql\") is None",
          "304:         mock_client.secrets.kv.v2.read_secret_version.assert_called_once_with(",
          "306:         )",
          "307:         assert test_client.get_connection(conn_id=\"test_mysql\") is None",
          "",
          "[Removed Lines]",
          "305:             mount_point=\"airflow\", path=\"connections/test_mysql\", version=None",
          "",
          "[Added Lines]",
          "305:             mount_point=\"airflow\", path=\"connections/test_mysql\", version=None, raise_on_deleted_version=True",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "454:         test_client = VaultBackend(**kwargs)",
          "455:         assert test_client.get_variable(\"hello\") is None",
          "456:         mock_client.secrets.kv.v2.read_secret_version.assert_called_once_with(",
          "458:         )",
          "459:         assert test_client.get_variable(\"hello\") is None",
          "",
          "[Removed Lines]",
          "457:             mount_point=\"airflow\", path=\"variables/hello\", version=None",
          "",
          "[Added Lines]",
          "457:             mount_point=\"airflow\", path=\"variables/hello\", version=None, raise_on_deleted_version=True",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "bbf6ebc703f83963345acdbd69da089749fc1c4d",
      "candidate_info": {
        "commit_hash": "bbf6ebc703f83963345acdbd69da089749fc1c4d",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/bbf6ebc703f83963345acdbd69da089749fc1c4d",
        "files": [
          "airflow/decorators/__init__.pyi"
        ],
        "message": "Remove redundant `docker` decorator type annotations (#36406)\n\n(cherry picked from commit e3fd0d1a985fc99e4af8edaccda01f97cb9693d9)",
        "before_after_code_files": [
          "airflow/decorators/__init__.pyi||airflow/decorators/__init__.pyi"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/decorators/__init__.pyi||airflow/decorators/__init__.pyi": [
          "File: airflow/decorators/__init__.pyi -> airflow/decorators/__init__.pyi",
          "--- Hunk 1 ---",
          "[Context before]",
          "354:         privileged: bool = False,",
          "355:         cap_add: str | None = None,",
          "356:         extra_hosts: dict[str, str] | None = None,",
          "359:         timeout: int = 60,",
          "360:         device_requests: list[dict] | None = None,",
          "361:         log_opts_max_size: str | None = None,",
          "",
          "[Removed Lines]",
          "357:         retrieve_output: bool = False,",
          "358:         retrieve_output_path: str | None = None,",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "441:         :param cap_add: Include container capabilities",
          "442:         :param extra_hosts: Additional hostnames to resolve inside the container,",
          "443:             as a mapping of hostname to IP address.",
          "448:         :param device_requests: Expose host resources such as GPUs to the container.",
          "449:         :param log_opts_max_size: The maximum size of the log before it is rolled.",
          "450:             A positive integer plus a modifier representing the unit of measure (k, m, or g).",
          "",
          "[Removed Lines]",
          "444:         :param retrieve_output: Should this docker image consistently attempt to pull from and output",
          "445:             file before manually shutting down the image. Useful for cases where users want a pickle serialized",
          "446:             output that is not posted to logs",
          "447:         :param retrieve_output_path: path for output file that will be retrieved and passed to xcom",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "6e31c0f2d75e6fc7f5360ca125890e4f97723b11",
      "candidate_info": {
        "commit_hash": "6e31c0f2d75e6fc7f5360ca125890e4f97723b11",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/6e31c0f2d75e6fc7f5360ca125890e4f97723b11",
        "files": [
          "airflow/timetables/_cron.py",
          "kubernetes_tests/test_kubernetes_pod_operator.py",
          "tests/api_connexion/test_parameters.py",
          "tests/providers/cncf/kubernetes/utils/test_pod_manager.py",
          "tests/providers/openlineage/plugins/test_utils.py",
          "tests/serialization/test_serialized_objects.py",
          "tests/timetables/test_events_timetable.py",
          "tests/timetables/test_interval_timetable.py",
          "tests/timetables/test_trigger_timetable.py",
          "tests/timetables/test_workday_timetable.py"
        ],
        "message": "Use UTC explicitly in timetable tests (#36082)\n\n(cherry picked from commit c1d28b36e4ecfad6df2e5c0d412c8b7f8d38c11d)",
        "before_after_code_files": [
          "airflow/timetables/_cron.py||airflow/timetables/_cron.py",
          "kubernetes_tests/test_kubernetes_pod_operator.py||kubernetes_tests/test_kubernetes_pod_operator.py",
          "tests/api_connexion/test_parameters.py||tests/api_connexion/test_parameters.py",
          "tests/providers/cncf/kubernetes/utils/test_pod_manager.py||tests/providers/cncf/kubernetes/utils/test_pod_manager.py",
          "tests/providers/openlineage/plugins/test_utils.py||tests/providers/openlineage/plugins/test_utils.py",
          "tests/serialization/test_serialized_objects.py||tests/serialization/test_serialized_objects.py",
          "tests/timetables/test_events_timetable.py||tests/timetables/test_events_timetable.py",
          "tests/timetables/test_interval_timetable.py||tests/timetables/test_interval_timetable.py",
          "tests/timetables/test_trigger_timetable.py||tests/timetables/test_trigger_timetable.py",
          "tests/timetables/test_workday_timetable.py||tests/timetables/test_workday_timetable.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/timetables/_cron.py||airflow/timetables/_cron.py": [
          "File: airflow/timetables/_cron.py -> airflow/timetables/_cron.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "19: import datetime",
          "20: from typing import TYPE_CHECKING, Any",
          "22: from cron_descriptor import CasingTypeEnum, ExpressionDescriptor, FormatException, MissingFieldException",
          "23: from croniter import CroniterBadCronError, CroniterBadDateError, croniter",
          "26: from airflow.exceptions import AirflowTimetableInvalid",
          "27: from airflow.utils.dates import cron_presets",
          "",
          "[Removed Lines]",
          "24: from pendulum.tz.timezone import Timezone",
          "",
          "[Added Lines]",
          "22: import pendulum",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "30: if TYPE_CHECKING:",
          "31:     from pendulum import DateTime",
          "34: def _covers_every_hour(cron: croniter) -> bool:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "32:     from pendulum.tz.timezone import Timezone",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "66:         self._expression = cron_presets.get(cron, cron)",
          "68:         if isinstance(timezone, str):",
          "70:         self._timezone = timezone",
          "72:         try:",
          "",
          "[Removed Lines]",
          "69:             timezone = Timezone(timezone)",
          "",
          "[Added Lines]",
          "70:             timezone = pendulum.tz.timezone(timezone)",
          "",
          "---------------"
        ],
        "kubernetes_tests/test_kubernetes_pod_operator.py||kubernetes_tests/test_kubernetes_pod_operator.py": [
          "File: kubernetes_tests/test_kubernetes_pod_operator.py -> kubernetes_tests/test_kubernetes_pod_operator.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "26: from unittest.mock import ANY, MagicMock",
          "27: from uuid import uuid4",
          "29: import pytest",
          "30: from kubernetes import client",
          "31: from kubernetes.client import V1EnvVar, V1PodSecurityContext, V1SecurityContext, models as k8s",
          "32: from kubernetes.client.api_client import ApiClient",
          "33: from kubernetes.client.rest import ApiException",
          "36: from airflow.exceptions import AirflowException, AirflowSkipException",
          "37: from airflow.models.connection import Connection",
          "",
          "[Removed Lines]",
          "34: from pendulum.tz.timezone import Timezone",
          "",
          "[Added Lines]",
          "29: import pendulum",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "54: def create_context(task) -> Context:",
          "55:     dag = DAG(dag_id=\"dag\")",
          "57:     dag_run = DagRun(",
          "58:         dag_id=dag.dag_id,",
          "59:         execution_date=execution_date,",
          "",
          "[Removed Lines]",
          "56:     execution_date = timezone.datetime(2016, 1, 1, 1, 0, 0, tzinfo=Timezone(\"Europe/Amsterdam\"))",
          "",
          "[Added Lines]",
          "56:     execution_date = timezone.datetime(2016, 1, 1, 1, 0, 0, tzinfo=pendulum.tz.timezone(\"Europe/Amsterdam\"))",
          "",
          "---------------"
        ],
        "tests/api_connexion/test_parameters.py||tests/api_connexion/test_parameters.py": [
          "File: tests/api_connexion/test_parameters.py -> tests/api_connexion/test_parameters.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "19: from unittest import mock",
          "21: import pytest",
          "25: from airflow.api_connexion.exceptions import BadRequest",
          "26: from airflow.api_connexion.parameters import (",
          "",
          "[Removed Lines]",
          "22: from pendulum import DateTime",
          "23: from pendulum.tz.timezone import Timezone",
          "",
          "[Added Lines]",
          "21: import pendulum",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "107:         decorated_endpoint(param_a=\"2020-01-01T0:0:00+00:00\")",
          "111:     def test_should_propagate_exceptions(self):",
          "112:         decorator = format_parameters({\"param_a\": format_datetime})",
          "",
          "[Removed Lines]",
          "109:         endpoint.assert_called_once_with(param_a=DateTime(2020, 1, 1, 0, tzinfo=Timezone(\"UTC\")))",
          "",
          "[Added Lines]",
          "108:         endpoint.assert_called_once_with(param_a=pendulum.datetime(2020, 1, 1, 0, tz=\"UTC\"))",
          "",
          "---------------"
        ],
        "tests/providers/cncf/kubernetes/utils/test_pod_manager.py||tests/providers/cncf/kubernetes/utils/test_pod_manager.py": [
          "File: tests/providers/cncf/kubernetes/utils/test_pod_manager.py -> tests/providers/cncf/kubernetes/utils/test_pod_manager.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: from datetime import datetime",
          "21: from json.decoder import JSONDecodeError",
          "22: from types import SimpleNamespace",
          "24: from unittest import mock",
          "25: from unittest.mock import MagicMock",
          "",
          "[Removed Lines]",
          "23: from typing import cast",
          "",
          "[Added Lines]",
          "23: from typing import TYPE_CHECKING, cast",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "28: import pytest",
          "29: import time_machine",
          "30: from kubernetes.client.rest import ApiException",
          "33: from urllib3.exceptions import HTTPError as BaseHTTPError",
          "35: from airflow.exceptions import AirflowException",
          "",
          "[Removed Lines]",
          "31: from pendulum import DateTime",
          "32: from pendulum.tz.timezone import Timezone",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "43: )",
          "44: from airflow.utils.timezone import utc",
          "47: class TestPodManager:",
          "48:     def setup_method(self):",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "44: if TYPE_CHECKING:",
          "45:     from pendulum import DateTime",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "270:         status = self.pod_manager.fetch_container_logs(mock.MagicMock(), mock.MagicMock(), follow=True)",
          "274:     @mock.patch(\"airflow.providers.cncf.kubernetes.utils.pod_manager.PodManager.container_is_running\")",
          "275:     @mock.patch(\"airflow.providers.cncf.kubernetes.utils.pod_manager.PodManager.read_pod_logs\")",
          "",
          "[Removed Lines]",
          "272:         assert status.last_log_time == cast(DateTime, pendulum.parse(timestamp_string))",
          "",
          "[Added Lines]",
          "273:         assert status.last_log_time == cast(\"DateTime\", pendulum.parse(timestamp_string))",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "306:             mock_consumer_iter.side_effect = consumer_iter",
          "307:             mock_container_is_running.side_effect = [True, True, False]",
          "308:             status = self.pod_manager.fetch_container_logs(mock.MagicMock(), mock.MagicMock(), follow=True)",
          "310:         assert self.mock_progress_callback.call_count == expected_call_count",
          "312:     @mock.patch(\"airflow.providers.cncf.kubernetes.utils.pod_manager.PodManager.container_is_running\")",
          "",
          "[Removed Lines]",
          "309:         assert status.last_log_time == cast(DateTime, pendulum.parse(last_timestamp_string))",
          "",
          "[Added Lines]",
          "310:         assert status.last_log_time == cast(\"DateTime\", pendulum.parse(last_timestamp_string))",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "461:     def test_fetch_container_since_time(self, logs_available, container_running, mock_now):",
          "462:         \"\"\"If given since_time, should be used.\"\"\"",
          "463:         mock_pod = MagicMock()",
          "465:         logs_available.return_value = True",
          "466:         container_running.return_value = False",
          "467:         self.mock_kube_client.read_namespaced_pod_log.return_value = mock.MagicMock(",
          "468:             stream=mock.MagicMock(return_value=[b\"2021-01-01 hi\"])",
          "469:         )",
          "471:         self.pod_manager.fetch_container_logs(pod=mock_pod, container_name=\"base\", since_time=since_time)",
          "472:         args, kwargs = self.mock_kube_client.read_namespaced_pod_log.call_args_list[0]",
          "473:         assert kwargs[\"since_seconds\"] == 5",
          "",
          "[Removed Lines]",
          "464:         mock_now.return_value = DateTime(2020, 1, 1, 0, 0, 5, tzinfo=Timezone(\"UTC\"))",
          "470:         since_time = DateTime(2020, 1, 1, tzinfo=Timezone(\"UTC\"))",
          "",
          "[Added Lines]",
          "465:         mock_now.return_value = pendulum.datetime(2020, 1, 1, 0, 0, 5, tz=\"UTC\")",
          "471:         since_time = pendulum.datetime(2020, 1, 1, tz=\"UTC\")",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "488:         )",
          "489:         ret = self.pod_manager.fetch_container_logs(pod=mock_pod, container_name=\"base\", follow=follow)",
          "490:         assert len(container_running_mock.call_args_list) == is_running_calls",
          "492:         assert ret.running is exp_running",
          "494:     @pytest.mark.parametrize(",
          "",
          "[Removed Lines]",
          "491:         assert ret.last_log_time == DateTime(2021, 1, 1, tzinfo=Timezone(\"UTC\"))",
          "",
          "[Added Lines]",
          "492:         assert ret.last_log_time == pendulum.datetime(2021, 1, 1, tz=\"UTC\")",
          "",
          "---------------"
        ],
        "tests/providers/openlineage/plugins/test_utils.py||tests/providers/openlineage/plugins/test_utils.py": [
          "File: tests/providers/openlineage/plugins/test_utils.py -> tests/providers/openlineage/plugins/test_utils.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "23: from json import JSONEncoder",
          "24: from typing import Any",
          "26: import pytest",
          "27: from attrs import define",
          "28: from openlineage.client.utils import RedactMixin",
          "30: from pkg_resources import parse_version",
          "32: from airflow.models import DAG as AIRFLOW_DAG, DagModel",
          "",
          "[Removed Lines]",
          "29: from pendulum.tz.timezone import Timezone",
          "",
          "[Added Lines]",
          "26: import pendulum",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "86:         state=State.NONE, run_id=run_id, data_interval=dag.get_next_data_interval(dag_model)",
          "87:     )",
          "88:     assert dagrun.data_interval_start is not None",
          "91:     assert dagrun.data_interval_start, dagrun.data_interval_end == (start_date_tz, end_date_tz)",
          "",
          "[Removed Lines]",
          "89:     start_date_tz = datetime.datetime(2022, 1, 1, tzinfo=Timezone(\"UTC\"))",
          "90:     end_date_tz = datetime.datetime(2022, 1, 1, hour=2, tzinfo=Timezone(\"UTC\"))",
          "",
          "[Added Lines]",
          "89:     start_date_tz = datetime.datetime(2022, 1, 1, tzinfo=pendulum.tz.timezone(\"UTC\"))",
          "90:     end_date_tz = datetime.datetime(2022, 1, 1, hour=2, tzinfo=pendulum.tz.timezone(\"UTC\"))",
          "",
          "---------------"
        ],
        "tests/serialization/test_serialized_objects.py||tests/serialization/test_serialized_objects.py": [
          "File: tests/serialization/test_serialized_objects.py -> tests/serialization/test_serialized_objects.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: import json",
          "21: from datetime import datetime, timedelta",
          "23: import pytest",
          "24: from dateutil import relativedelta",
          "25: from kubernetes.client import models as k8s",
          "28: from airflow.datasets import Dataset",
          "29: from airflow.exceptions import SerializationError",
          "",
          "[Removed Lines]",
          "26: from pendulum.tz.timezone import Timezone",
          "",
          "[Added Lines]",
          "23: import pendulum",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "142:         (1, None, equals),",
          "143:         (datetime.utcnow(), DAT.DATETIME, equal_time),",
          "144:         (timedelta(minutes=2), DAT.TIMEDELTA, equals),",
          "146:         (relativedelta.relativedelta(hours=+1), DAT.RELATIVEDELTA, lambda a, b: a.hours == b.hours),",
          "147:         ({\"test\": \"dict\", \"test-1\": 1}, None, equals),",
          "148:         ([\"array_item\", 2], None, equals),",
          "",
          "[Removed Lines]",
          "145:         (Timezone(\"UTC\"), DAT.TIMEZONE, lambda a, b: a.name == b.name),",
          "",
          "[Added Lines]",
          "145:         (pendulum.tz.timezone(\"UTC\"), DAT.TIMEZONE, lambda a, b: a.name == b.name),",
          "",
          "---------------"
        ],
        "tests/timetables/test_events_timetable.py||tests/timetables/test_events_timetable.py": [
          "File: tests/timetables/test_events_timetable.py -> tests/timetables/test_events_timetable.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: import pendulum",
          "21: import pytest",
          "24: from airflow.timetables.base import DagRunInfo, DataInterval, TimeRestriction, Timetable",
          "25: from airflow.timetables.events import EventsTimetable",
          "29: EVENT_DATES = [",
          "36: ]",
          "38: EVENT_DATES_SORTED = [",
          "44: ]",
          "50: @pytest.fixture()",
          "",
          "[Removed Lines]",
          "23: from airflow.settings import TIMEZONE",
          "27: START_DATE = pendulum.DateTime(2021, 9, 4, tzinfo=TIMEZONE)  # Precedes all events",
          "30:     pendulum.DateTime(2021, 9, 6, tzinfo=TIMEZONE),",
          "31:     pendulum.DateTime(2021, 9, 7, tzinfo=TIMEZONE),",
          "32:     pendulum.DateTime(2021, 9, 8, tzinfo=TIMEZONE),",
          "33:     pendulum.DateTime(2021, 9, 8, tzinfo=TIMEZONE),  # deliberate duplicate, should be ignored",
          "34:     pendulum.DateTime(2021, 10, 9, tzinfo=TIMEZONE),  # deliberately out of order",
          "35:     pendulum.DateTime(2021, 9, 10, tzinfo=TIMEZONE),",
          "39:     pendulum.DateTime(2021, 9, 6, tzinfo=TIMEZONE),",
          "40:     pendulum.DateTime(2021, 9, 7, tzinfo=TIMEZONE),",
          "41:     pendulum.DateTime(2021, 9, 8, tzinfo=TIMEZONE),",
          "42:     pendulum.DateTime(2021, 9, 10, tzinfo=TIMEZONE),",
          "43:     pendulum.DateTime(2021, 10, 9, tzinfo=TIMEZONE),",
          "46: NON_EVENT_DATE = pendulum.DateTime(2021, 10, 1, tzinfo=TIMEZONE)",
          "47: MOST_RECENT_EVENT = pendulum.DateTime(2021, 9, 10, tzinfo=TIMEZONE)",
          "",
          "[Added Lines]",
          "25: from airflow.utils.timezone import utc",
          "27: START_DATE = pendulum.DateTime(2021, 9, 4, tzinfo=utc)  # Precedes all events",
          "30:     pendulum.DateTime(2021, 9, 6, tzinfo=utc),",
          "31:     pendulum.DateTime(2021, 9, 7, tzinfo=utc),",
          "32:     pendulum.DateTime(2021, 9, 8, tzinfo=utc),",
          "33:     pendulum.DateTime(2021, 9, 8, tzinfo=utc),  # deliberate duplicate, should be ignored",
          "34:     pendulum.DateTime(2021, 10, 9, tzinfo=utc),  # deliberately out of order",
          "35:     pendulum.DateTime(2021, 9, 10, tzinfo=utc),",
          "39:     pendulum.DateTime(2021, 9, 6, tzinfo=utc),",
          "40:     pendulum.DateTime(2021, 9, 7, tzinfo=utc),",
          "41:     pendulum.DateTime(2021, 9, 8, tzinfo=utc),",
          "42:     pendulum.DateTime(2021, 9, 10, tzinfo=utc),",
          "43:     pendulum.DateTime(2021, 10, 9, tzinfo=utc),",
          "46: NON_EVENT_DATE = pendulum.DateTime(2021, 10, 1, tzinfo=utc)",
          "47: MOST_RECENT_EVENT = pendulum.DateTime(2021, 9, 10, tzinfo=utc)",
          "",
          "---------------"
        ],
        "tests/timetables/test_interval_timetable.py||tests/timetables/test_interval_timetable.py": [
          "File: tests/timetables/test_interval_timetable.py -> tests/timetables/test_interval_timetable.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "25: import time_machine",
          "27: from airflow.exceptions import AirflowTimetableInvalid",
          "29: from airflow.timetables.base import DagRunInfo, DataInterval, TimeRestriction, Timetable",
          "30: from airflow.timetables.interval import CronDataIntervalTimetable, DeltaDataIntervalTimetable",
          "34: PREV_DATA_INTERVAL_START = START_DATE",
          "35: PREV_DATA_INTERVAL_END = START_DATE + datetime.timedelta(days=1)",
          "36: PREV_DATA_INTERVAL = DataInterval(start=PREV_DATA_INTERVAL_START, end=PREV_DATA_INTERVAL_END)",
          "37: PREV_DATA_INTERVAL_EXACT = DataInterval.exact(PREV_DATA_INTERVAL_END)",
          "40: YESTERDAY = CURRENT_TIME - datetime.timedelta(days=1)",
          "41: OLD_INTERVAL = DataInterval(start=YESTERDAY, end=CURRENT_TIME)",
          "44: HOURLY_TIMEDELTA_TIMETABLE = DeltaDataIntervalTimetable(datetime.timedelta(hours=1))",
          "45: HOURLY_RELATIVEDELTA_TIMETABLE = DeltaDataIntervalTimetable(dateutil.relativedelta.relativedelta(hours=1))",
          "48: DELTA_FROM_MIDNIGHT = datetime.timedelta(minutes=30, hours=16)",
          "",
          "[Removed Lines]",
          "28: from airflow.settings import TIMEZONE",
          "32: START_DATE = pendulum.DateTime(2021, 9, 4, tzinfo=TIMEZONE)",
          "39: CURRENT_TIME = pendulum.DateTime(2021, 9, 7, tzinfo=TIMEZONE)",
          "43: HOURLY_CRON_TIMETABLE = CronDataIntervalTimetable(\"@hourly\", TIMEZONE)",
          "47: CRON_TIMETABLE = CronDataIntervalTimetable(\"30 16 * * *\", TIMEZONE)",
          "",
          "[Added Lines]",
          "30: from airflow.utils.timezone import utc",
          "32: START_DATE = pendulum.DateTime(2021, 9, 4, tzinfo=utc)",
          "39: CURRENT_TIME = pendulum.DateTime(2021, 9, 7, tzinfo=utc)",
          "43: HOURLY_CRON_TIMETABLE = CronDataIntervalTimetable(\"@hourly\", utc)",
          "47: CRON_TIMETABLE = CronDataIntervalTimetable(\"30 16 * * *\", utc)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "148:     \"timetable, error_message\",",
          "149:     [",
          "150:         pytest.param(",
          "152:             \"[0 0 1 13 0] is not acceptable, out of range\",",
          "153:             id=\"invalid-cron\",",
          "154:         ),",
          "",
          "[Removed Lines]",
          "151:             CronDataIntervalTimetable(\"0 0 1 13 0\", TIMEZONE),",
          "",
          "[Added Lines]",
          "151:             CronDataIntervalTimetable(\"0 0 1 13 0\", utc),",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "191:     [",
          "192:         # Arbitrary trigger time.",
          "193:         pytest.param(",
          "195:             DataInterval(",
          "198:             ),",
          "199:             id=\"adhoc\",",
          "200:         ),",
          "201:         # Trigger time falls exactly on interval boundary.",
          "202:         pytest.param(",
          "204:             DataInterval(",
          "207:             ),",
          "208:             id=\"exact\",",
          "209:         ),",
          "",
          "[Removed Lines]",
          "194:             pendulum.DateTime(2022, 8, 8, 1, tzinfo=TIMEZONE),",
          "196:                 pendulum.DateTime(2022, 8, 7, tzinfo=TIMEZONE),",
          "197:                 pendulum.DateTime(2022, 8, 8, tzinfo=TIMEZONE),",
          "203:             pendulum.DateTime(2022, 8, 8, tzinfo=TIMEZONE),",
          "205:                 pendulum.DateTime(2022, 8, 7, tzinfo=TIMEZONE),",
          "206:                 pendulum.DateTime(2022, 8, 8, tzinfo=TIMEZONE),",
          "",
          "[Added Lines]",
          "194:             pendulum.DateTime(2022, 8, 8, 1, tzinfo=utc),",
          "196:                 pendulum.DateTime(2022, 8, 7, tzinfo=utc),",
          "197:                 pendulum.DateTime(2022, 8, 8, tzinfo=utc),",
          "203:             pendulum.DateTime(2022, 8, 8, tzinfo=utc),",
          "205:                 pendulum.DateTime(2022, 8, 7, tzinfo=utc),",
          "206:                 pendulum.DateTime(2022, 8, 8, tzinfo=utc),",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "213:     trigger_at: pendulum.DateTime,",
          "214:     expected_interval: DataInterval,",
          "215: ) -> None:",
          "217:     assert timetable.infer_manual_data_interval(run_after=trigger_at) == expected_interval",
          "",
          "[Removed Lines]",
          "216:     timetable = CronDataIntervalTimetable(\"@daily\", TIMEZONE)",
          "",
          "[Added Lines]",
          "216:     timetable = CronDataIntervalTimetable(\"@daily\", utc)",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "222:     [",
          "223:         pytest.param(",
          "224:             DataInterval(",
          "227:             ),",
          "228:             DagRunInfo.interval(",
          "231:             ),",
          "232:             id=\"exact\",",
          "233:         ),",
          "",
          "[Removed Lines]",
          "225:                 pendulum.DateTime(2022, 8, 7, tzinfo=TIMEZONE),",
          "226:                 pendulum.DateTime(2022, 8, 8, tzinfo=TIMEZONE),",
          "229:                 pendulum.DateTime(2022, 8, 8, tzinfo=TIMEZONE),",
          "230:                 pendulum.DateTime(2022, 8, 9, tzinfo=TIMEZONE),",
          "",
          "[Added Lines]",
          "225:                 pendulum.DateTime(2022, 8, 7, tzinfo=utc),",
          "226:                 pendulum.DateTime(2022, 8, 8, tzinfo=utc),",
          "229:                 pendulum.DateTime(2022, 8, 8, tzinfo=utc),",
          "230:                 pendulum.DateTime(2022, 8, 9, tzinfo=utc),",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "235:             # Previous data interval does not align with the current timetable.",
          "236:             # This is possible if the user edits a DAG with existing runs.",
          "237:             DataInterval(",
          "240:             ),",
          "241:             DagRunInfo.interval(",
          "244:             ),",
          "245:             id=\"changed\",",
          "246:         ),",
          "247:     ],",
          "248: )",
          "249: def test_cron_next_dagrun_info_alignment(last_data_interval: DataInterval, expected_info: DagRunInfo):",
          "251:     info = timetable.next_dagrun_info(",
          "252:         last_automated_data_interval=last_data_interval,",
          "253:         restriction=TimeRestriction(None, None, True),",
          "",
          "[Removed Lines]",
          "238:                 pendulum.DateTime(2022, 8, 7, 1, tzinfo=TIMEZONE),",
          "239:                 pendulum.DateTime(2022, 8, 8, 1, tzinfo=TIMEZONE),",
          "242:                 pendulum.DateTime(2022, 8, 8, tzinfo=TIMEZONE),",
          "243:                 pendulum.DateTime(2022, 8, 9, tzinfo=TIMEZONE),",
          "250:     timetable = CronDataIntervalTimetable(\"@daily\", TIMEZONE)",
          "",
          "[Added Lines]",
          "238:                 pendulum.DateTime(2022, 8, 7, 1, tzinfo=utc),",
          "239:                 pendulum.DateTime(2022, 8, 8, 1, tzinfo=utc),",
          "242:                 pendulum.DateTime(2022, 8, 8, tzinfo=utc),",
          "243:                 pendulum.DateTime(2022, 8, 9, tzinfo=utc),",
          "250:     timetable = CronDataIntervalTimetable(\"@daily\", utc)",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "269:     def test_entering_exact(self) -> None:",
          "270:         timetable = CronDataIntervalTimetable(\"0 3 * * *\", timezone=\"Europe/Zurich\")",
          "271:         restriction = TimeRestriction(",
          "273:             latest=None,",
          "274:             catchup=True,",
          "275:         )",
          "",
          "[Removed Lines]",
          "272:             earliest=pendulum.datetime(2023, 3, 24, tz=TIMEZONE),",
          "",
          "[Added Lines]",
          "272:             earliest=pendulum.datetime(2023, 3, 24, tz=utc),",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "277:         # Last run before DST. Interval starts and ends on 2am UTC (local time is +1).",
          "278:         next_info = timetable.next_dagrun_info(last_automated_data_interval=None, restriction=restriction)",
          "279:         assert next_info and next_info.data_interval == DataInterval(",
          "282:         )",
          "284:         # Crossing the DST switch. Interval starts on 2am UTC (local time +1)",
          "",
          "[Removed Lines]",
          "280:             pendulum.datetime(2023, 3, 24, 2, tz=TIMEZONE),",
          "281:             pendulum.datetime(2023, 3, 25, 2, tz=TIMEZONE),",
          "",
          "[Added Lines]",
          "280:             pendulum.datetime(2023, 3, 24, 2, tz=utc),",
          "281:             pendulum.datetime(2023, 3, 25, 2, tz=utc),",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "288:             restriction=restriction,",
          "289:         )",
          "290:         assert next_info and next_info.data_interval == DataInterval(",
          "293:         )",
          "295:         # In DST. Interval starts and ends on 1am UTC (local time is +2).",
          "",
          "[Removed Lines]",
          "291:             pendulum.datetime(2023, 3, 25, 2, tz=TIMEZONE),",
          "292:             pendulum.datetime(2023, 3, 26, 1, tz=TIMEZONE),",
          "",
          "[Added Lines]",
          "291:             pendulum.datetime(2023, 3, 25, 2, tz=utc),",
          "292:             pendulum.datetime(2023, 3, 26, 1, tz=utc),",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "298:             restriction=restriction,",
          "299:         )",
          "300:         assert next_info and next_info.data_interval == DataInterval(",
          "303:         )",
          "305:     def test_entering_skip(self) -> None:",
          "306:         timetable = CronDataIntervalTimetable(\"0 2 * * *\", timezone=\"Europe/Zurich\")",
          "307:         restriction = TimeRestriction(",
          "309:             latest=None,",
          "310:             catchup=True,",
          "311:         )",
          "",
          "[Removed Lines]",
          "301:             pendulum.datetime(2023, 3, 26, 1, tz=TIMEZONE),",
          "302:             pendulum.datetime(2023, 3, 27, 1, tz=TIMEZONE),",
          "308:             earliest=pendulum.datetime(2023, 3, 24, tz=TIMEZONE),",
          "",
          "[Added Lines]",
          "301:             pendulum.datetime(2023, 3, 26, 1, tz=utc),",
          "302:             pendulum.datetime(2023, 3, 27, 1, tz=utc),",
          "308:             earliest=pendulum.datetime(2023, 3, 24, tz=utc),",
          "",
          "---------------",
          "--- Hunk 11 ---",
          "[Context before]",
          "313:         # Last run before DST. Interval starts and ends on 1am UTC (local time is +1).",
          "314:         next_info = timetable.next_dagrun_info(last_automated_data_interval=None, restriction=restriction)",
          "315:         assert next_info and next_info.data_interval == DataInterval(",
          "318:         )",
          "320:         # Crossing the DST switch. Interval starts on 1am UTC (local time +1)",
          "",
          "[Removed Lines]",
          "316:             pendulum.datetime(2023, 3, 24, 1, tz=TIMEZONE),",
          "317:             pendulum.datetime(2023, 3, 25, 1, tz=TIMEZONE),",
          "",
          "[Added Lines]",
          "316:             pendulum.datetime(2023, 3, 24, 1, tz=utc),",
          "317:             pendulum.datetime(2023, 3, 25, 1, tz=utc),",
          "",
          "---------------",
          "--- Hunk 12 ---",
          "[Context before]",
          "325:             restriction=restriction,",
          "326:         )",
          "327:         assert next_info and next_info.data_interval == DataInterval(",
          "330:         )",
          "332:         # In DST. Interval starts on 1am UTC (local time is +2 but 2am local",
          "",
          "[Removed Lines]",
          "328:             pendulum.datetime(2023, 3, 25, 1, tz=TIMEZONE),",
          "329:             pendulum.datetime(2023, 3, 26, 1, tz=TIMEZONE),",
          "",
          "[Added Lines]",
          "328:             pendulum.datetime(2023, 3, 25, 1, tz=utc),",
          "329:             pendulum.datetime(2023, 3, 26, 1, tz=utc),",
          "",
          "---------------",
          "--- Hunk 13 ---",
          "[Context before]",
          "336:             restriction=restriction,",
          "337:         )",
          "338:         assert next_info and next_info.data_interval == DataInterval(",
          "341:         )",
          "343:     def test_exiting_exact(self) -> None:",
          "344:         timetable = CronDataIntervalTimetable(\"0 3 * * *\", timezone=\"Europe/Zurich\")",
          "345:         restriction = TimeRestriction(",
          "347:             latest=None,",
          "348:             catchup=True,",
          "349:         )",
          "",
          "[Removed Lines]",
          "339:             pendulum.datetime(2023, 3, 26, 1, tz=TIMEZONE),",
          "340:             pendulum.datetime(2023, 3, 27, 0, tz=TIMEZONE),",
          "346:             earliest=pendulum.datetime(2023, 10, 27, tz=TIMEZONE),",
          "",
          "[Added Lines]",
          "339:             pendulum.datetime(2023, 3, 26, 1, tz=utc),",
          "340:             pendulum.datetime(2023, 3, 27, 0, tz=utc),",
          "346:             earliest=pendulum.datetime(2023, 10, 27, tz=utc),",
          "",
          "---------------",
          "--- Hunk 14 ---",
          "[Context before]",
          "351:         # Last run in DST. Interval starts and ends on 1am UTC (local time is +2).",
          "352:         next_info = timetable.next_dagrun_info(last_automated_data_interval=None, restriction=restriction)",
          "353:         assert next_info and next_info.data_interval == DataInterval(",
          "356:         )",
          "358:         # Crossing the DST switch. Interval starts on 1am UTC (local time +2)",
          "",
          "[Removed Lines]",
          "354:             pendulum.datetime(2023, 10, 27, 1, tz=TIMEZONE),",
          "355:             pendulum.datetime(2023, 10, 28, 1, tz=TIMEZONE),",
          "",
          "[Added Lines]",
          "354:             pendulum.datetime(2023, 10, 27, 1, tz=utc),",
          "355:             pendulum.datetime(2023, 10, 28, 1, tz=utc),",
          "",
          "---------------",
          "--- Hunk 15 ---",
          "[Context before]",
          "362:             restriction=restriction,",
          "363:         )",
          "364:         assert next_info and next_info.data_interval == DataInterval(",
          "367:         )",
          "369:         # Out of DST. Interval starts and ends on 2am UTC (local time is +1).",
          "",
          "[Removed Lines]",
          "365:             pendulum.datetime(2023, 10, 28, 1, tz=TIMEZONE),",
          "366:             pendulum.datetime(2023, 10, 29, 2, tz=TIMEZONE),",
          "",
          "[Added Lines]",
          "365:             pendulum.datetime(2023, 10, 28, 1, tz=utc),",
          "366:             pendulum.datetime(2023, 10, 29, 2, tz=utc),",
          "",
          "---------------",
          "--- Hunk 16 ---",
          "[Context before]",
          "372:             restriction=restriction,",
          "373:         )",
          "374:         assert next_info and next_info.data_interval == DataInterval(",
          "377:         )",
          "379:     def test_exiting_fold(self) -> None:",
          "380:         timetable = CronDataIntervalTimetable(\"0 2 * * *\", timezone=\"Europe/Zurich\")",
          "381:         restriction = TimeRestriction(",
          "383:             latest=None,",
          "384:             catchup=True,",
          "385:         )",
          "",
          "[Removed Lines]",
          "375:             pendulum.datetime(2023, 10, 29, 2, tz=TIMEZONE),",
          "376:             pendulum.datetime(2023, 10, 30, 2, tz=TIMEZONE),",
          "382:             earliest=pendulum.datetime(2023, 10, 27, tz=TIMEZONE),",
          "",
          "[Added Lines]",
          "375:             pendulum.datetime(2023, 10, 29, 2, tz=utc),",
          "376:             pendulum.datetime(2023, 10, 30, 2, tz=utc),",
          "382:             earliest=pendulum.datetime(2023, 10, 27, tz=utc),",
          "",
          "---------------",
          "--- Hunk 17 ---",
          "[Context before]",
          "388:         # time is +2).",
          "389:         next_info = timetable.next_dagrun_info(last_automated_data_interval=None, restriction=restriction)",
          "390:         assert next_info and next_info.data_interval == DataInterval(",
          "393:         )",
          "395:         # Account for folding. Interval starts on 0am UTC (local time +2) and",
          "",
          "[Removed Lines]",
          "391:             pendulum.datetime(2023, 10, 27, 0, tz=TIMEZONE),",
          "392:             pendulum.datetime(2023, 10, 28, 0, tz=TIMEZONE),",
          "",
          "[Added Lines]",
          "391:             pendulum.datetime(2023, 10, 27, 0, tz=utc),",
          "392:             pendulum.datetime(2023, 10, 28, 0, tz=utc),",
          "",
          "---------------",
          "--- Hunk 18 ---",
          "[Context before]",
          "402:             restriction=restriction,",
          "403:         )",
          "404:         assert next_info and next_info.data_interval == DataInterval(",
          "407:         )",
          "409:         # Stepping out of DST. Interval starts from the folded 2am local time",
          "",
          "[Removed Lines]",
          "405:             pendulum.datetime(2023, 10, 28, 0, tz=TIMEZONE),",
          "406:             pendulum.datetime(2023, 10, 29, 1, tz=TIMEZONE),",
          "",
          "[Added Lines]",
          "405:             pendulum.datetime(2023, 10, 28, 0, tz=utc),",
          "406:             pendulum.datetime(2023, 10, 29, 1, tz=utc),",
          "",
          "---------------",
          "--- Hunk 19 ---",
          "[Context before]",
          "414:             restriction=restriction,",
          "415:         )",
          "416:         assert next_info and next_info.data_interval == DataInterval(",
          "419:         )",
          "",
          "[Removed Lines]",
          "417:             pendulum.datetime(2023, 10, 29, 1, tz=TIMEZONE),",
          "418:             pendulum.datetime(2023, 10, 30, 1, tz=TIMEZONE),",
          "",
          "[Added Lines]",
          "417:             pendulum.datetime(2023, 10, 29, 1, tz=utc),",
          "418:             pendulum.datetime(2023, 10, 30, 1, tz=utc),",
          "",
          "---------------",
          "--- Hunk 20 ---",
          "[Context before]",
          "429:     def test_7_to_8_entering(self):",
          "430:         timetable = CronDataIntervalTimetable(\"0 7-8 * * *\", timezone=\"America/Los_Angeles\")",
          "431:         restriction = TimeRestriction(",
          "433:             latest=None,",
          "434:             catchup=True,",
          "435:         )",
          "",
          "[Removed Lines]",
          "432:             earliest=pendulum.datetime(2020, 3, 7, tz=TIMEZONE),",
          "",
          "[Added Lines]",
          "432:             earliest=pendulum.datetime(2020, 3, 7, tz=utc),",
          "",
          "---------------",
          "--- Hunk 21 ---",
          "[Context before]",
          "440:             restriction=restriction,",
          "441:         )",
          "442:         assert next_info and next_info.data_interval == DataInterval(",
          "445:         )",
          "447:         # This interval ends an hour early since it includes the DST switch!",
          "",
          "[Removed Lines]",
          "443:             pendulum.datetime(2020, 3, 7, 7 + 8, tz=TIMEZONE),",
          "444:             pendulum.datetime(2020, 3, 7, 8 + 8, tz=TIMEZONE),",
          "",
          "[Added Lines]",
          "443:             pendulum.datetime(2020, 3, 7, 7 + 8, tz=utc),",
          "444:             pendulum.datetime(2020, 3, 7, 8 + 8, tz=utc),",
          "",
          "---------------",
          "--- Hunk 22 ---",
          "[Context before]",
          "450:             restriction=restriction,",
          "451:         )",
          "452:         assert next_info and next_info.data_interval == DataInterval(",
          "455:         )",
          "457:         # We're fully into DST so the interval is as expected.",
          "",
          "[Removed Lines]",
          "453:             pendulum.datetime(2020, 3, 7, 8 + 8, tz=TIMEZONE),",
          "454:             pendulum.datetime(2020, 3, 8, 7 + 7, tz=TIMEZONE),",
          "",
          "[Added Lines]",
          "453:             pendulum.datetime(2020, 3, 7, 8 + 8, tz=utc),",
          "454:             pendulum.datetime(2020, 3, 8, 7 + 7, tz=utc),",
          "",
          "---------------",
          "--- Hunk 23 ---",
          "[Context before]",
          "460:             restriction=restriction,",
          "461:         )",
          "462:         assert next_info and next_info.data_interval == DataInterval(",
          "465:         )",
          "467:     def test_7_and_9_entering(self):",
          "468:         timetable = CronDataIntervalTimetable(\"0 7,9 * * *\", timezone=\"America/Los_Angeles\")",
          "469:         restriction = TimeRestriction(",
          "471:             latest=None,",
          "472:             catchup=True,",
          "473:         )",
          "",
          "[Removed Lines]",
          "463:             pendulum.datetime(2020, 3, 8, 7 + 7, tz=TIMEZONE),",
          "464:             pendulum.datetime(2020, 3, 8, 8 + 7, tz=TIMEZONE),",
          "470:             earliest=pendulum.datetime(2020, 3, 7, tz=TIMEZONE),",
          "",
          "[Added Lines]",
          "463:             pendulum.datetime(2020, 3, 8, 7 + 7, tz=utc),",
          "464:             pendulum.datetime(2020, 3, 8, 8 + 7, tz=utc),",
          "470:             earliest=pendulum.datetime(2020, 3, 7, tz=utc),",
          "",
          "---------------",
          "--- Hunk 24 ---",
          "[Context before]",
          "478:             restriction=restriction,",
          "479:         )",
          "480:         assert next_info and next_info.data_interval == DataInterval(",
          "483:         )",
          "485:         # This interval ends an hour early since it includes the DST switch!",
          "",
          "[Removed Lines]",
          "481:             pendulum.datetime(2020, 3, 7, 7 + 8, tz=TIMEZONE),",
          "482:             pendulum.datetime(2020, 3, 7, 9 + 8, tz=TIMEZONE),",
          "",
          "[Added Lines]",
          "481:             pendulum.datetime(2020, 3, 7, 7 + 8, tz=utc),",
          "482:             pendulum.datetime(2020, 3, 7, 9 + 8, tz=utc),",
          "",
          "---------------",
          "--- Hunk 25 ---",
          "[Context before]",
          "488:             restriction=restriction,",
          "489:         )",
          "490:         assert next_info and next_info.data_interval == DataInterval(",
          "493:         )",
          "495:         # We're fully into DST so the interval is as expected.",
          "",
          "[Removed Lines]",
          "491:             pendulum.datetime(2020, 3, 7, 9 + 8, tz=TIMEZONE),",
          "492:             pendulum.datetime(2020, 3, 8, 7 + 7, tz=TIMEZONE),",
          "",
          "[Added Lines]",
          "491:             pendulum.datetime(2020, 3, 7, 9 + 8, tz=utc),",
          "492:             pendulum.datetime(2020, 3, 8, 7 + 7, tz=utc),",
          "",
          "---------------",
          "--- Hunk 26 ---",
          "[Context before]",
          "498:             restriction=restriction,",
          "499:         )",
          "500:         assert next_info and next_info.data_interval == DataInterval(",
          "503:         )",
          "506: def test_fold_scheduling():",
          "507:     timetable = CronDataIntervalTimetable(\"*/30 * * * *\", timezone=\"Europe/Zurich\")",
          "508:     restriction = TimeRestriction(",
          "510:         latest=None,",
          "511:         catchup=True,",
          "512:     )",
          "",
          "[Removed Lines]",
          "501:             pendulum.datetime(2020, 3, 8, 7 + 7, tz=TIMEZONE),",
          "502:             pendulum.datetime(2020, 3, 8, 9 + 7, tz=TIMEZONE),",
          "509:         earliest=pendulum.datetime(2023, 10, 28, 23, 30, tz=TIMEZONE),  # Locally 1:30 (DST).",
          "",
          "[Added Lines]",
          "501:             pendulum.datetime(2020, 3, 8, 7 + 7, tz=utc),",
          "502:             pendulum.datetime(2020, 3, 8, 9 + 7, tz=utc),",
          "509:         earliest=pendulum.datetime(2023, 10, 28, 23, 30, tz=utc),  # Locally 1:30 (DST).",
          "",
          "---------------",
          "--- Hunk 27 ---",
          "[Context before]",
          "517:         restriction=restriction,",
          "518:     )",
          "519:     assert next_info and next_info.data_interval == DataInterval(",
          "522:     )",
          "523:     next_info = timetable.next_dagrun_info(",
          "524:         last_automated_data_interval=next_info.data_interval,",
          "525:         restriction=restriction,",
          "526:     )",
          "527:     assert next_info and next_info.data_interval == DataInterval(",
          "530:     )",
          "532:     # Crossing into fold.",
          "",
          "[Removed Lines]",
          "520:         pendulum.datetime(2023, 10, 28, 23, 30, tz=TIMEZONE),",
          "521:         pendulum.datetime(2023, 10, 29, 0, 0, tz=TIMEZONE),  # Locally 2am (DST).",
          "528:         pendulum.datetime(2023, 10, 29, 0, 0, tz=TIMEZONE),",
          "529:         pendulum.datetime(2023, 10, 29, 0, 30, tz=TIMEZONE),  # Locally 2:30 (DST).",
          "",
          "[Added Lines]",
          "520:         pendulum.datetime(2023, 10, 28, 23, 30, tz=utc),",
          "521:         pendulum.datetime(2023, 10, 29, 0, 0, tz=utc),  # Locally 2am (DST).",
          "528:         pendulum.datetime(2023, 10, 29, 0, 0, tz=utc),",
          "529:         pendulum.datetime(2023, 10, 29, 0, 30, tz=utc),  # Locally 2:30 (DST).",
          "",
          "---------------",
          "--- Hunk 28 ---",
          "[Context before]",
          "535:         restriction=restriction,",
          "536:     )",
          "537:     assert next_info and next_info.data_interval == DataInterval(",
          "540:     )",
          "542:     # In the \"fold zone\".",
          "",
          "[Removed Lines]",
          "538:         pendulum.datetime(2023, 10, 29, 0, 30, tz=TIMEZONE),",
          "539:         pendulum.datetime(2023, 10, 29, 1, 0, tz=TIMEZONE),  # Locally 2am (fold, not DST).",
          "",
          "[Added Lines]",
          "538:         pendulum.datetime(2023, 10, 29, 0, 30, tz=utc),",
          "539:         pendulum.datetime(2023, 10, 29, 1, 0, tz=utc),  # Locally 2am (fold, not DST).",
          "",
          "---------------",
          "--- Hunk 29 ---",
          "[Context before]",
          "545:         restriction=restriction,",
          "546:     )",
          "547:     assert next_info and next_info.data_interval == DataInterval(",
          "550:     )",
          "552:     # Stepping out of fold.",
          "",
          "[Removed Lines]",
          "548:         pendulum.datetime(2023, 10, 29, 1, 0, tz=TIMEZONE),",
          "549:         pendulum.datetime(2023, 10, 29, 1, 30, tz=TIMEZONE),  # Locally 2am (fold, not DST).",
          "",
          "[Added Lines]",
          "548:         pendulum.datetime(2023, 10, 29, 1, 0, tz=utc),",
          "549:         pendulum.datetime(2023, 10, 29, 1, 30, tz=utc),  # Locally 2am (fold, not DST).",
          "",
          "---------------",
          "--- Hunk 30 ---",
          "[Context before]",
          "555:         restriction=restriction,",
          "556:     )",
          "557:     assert next_info and next_info.data_interval == DataInterval(",
          "560:     )",
          "",
          "[Removed Lines]",
          "558:         pendulum.datetime(2023, 10, 29, 1, 30, tz=TIMEZONE),",
          "559:         pendulum.datetime(2023, 10, 29, 2, 0, tz=TIMEZONE),  # Locally 3am (not DST).",
          "",
          "[Added Lines]",
          "558:         pendulum.datetime(2023, 10, 29, 1, 30, tz=utc),",
          "559:         pendulum.datetime(2023, 10, 29, 2, 0, tz=utc),  # Locally 3am (not DST).",
          "",
          "---------------"
        ],
        "tests/timetables/test_trigger_timetable.py||tests/timetables/test_trigger_timetable.py": [
          "File: tests/timetables/test_trigger_timetable.py -> tests/timetables/test_trigger_timetable.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "22: import dateutil.relativedelta",
          "23: import pendulum",
          "25: import pytest",
          "26: import time_machine",
          "28: from airflow.exceptions import AirflowTimetableInvalid",
          "29: from airflow.timetables.base import DagRunInfo, DataInterval, TimeRestriction",
          "30: from airflow.timetables.trigger import CronTriggerTimetable",
          "35: PREV_DATA_INTERVAL_END = START_DATE + datetime.timedelta(days=1)",
          "36: PREV_DATA_INTERVAL_EXACT = DataInterval.exact(PREV_DATA_INTERVAL_END)",
          "39: YESTERDAY = CURRENT_TIME - datetime.timedelta(days=1)",
          "43: DELTA_FROM_MIDNIGHT = datetime.timedelta(minutes=30, hours=16)",
          "",
          "[Removed Lines]",
          "24: import pendulum.tz",
          "32: TIMEZONE = pendulum.tz.timezone(\"UTC\")",
          "33: START_DATE = pendulum.DateTime(2021, 9, 4, tzinfo=TIMEZONE)",
          "38: CURRENT_TIME = pendulum.DateTime(2021, 9, 7, tzinfo=TIMEZONE)",
          "41: HOURLY_CRON_TRIGGER_TIMETABLE = CronTriggerTimetable(\"@hourly\", timezone=TIMEZONE)",
          "",
          "[Added Lines]",
          "30: from airflow.utils.timezone import utc",
          "32: START_DATE = pendulum.DateTime(2021, 9, 4, tzinfo=utc)",
          "37: CURRENT_TIME = pendulum.DateTime(2021, 9, 7, tzinfo=utc)",
          "40: HOURLY_CRON_TRIGGER_TIMETABLE = CronTriggerTimetable(\"@hourly\", timezone=utc)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "69:     next_start_time: pendulum.DateTime,",
          "70: ) -> None:",
          "71:     \"\"\"If ``catchup=False`` and start_date is a day before\"\"\"",
          "73:     next_info = timetable.next_dagrun_info(",
          "74:         last_automated_data_interval=last_automated_data_interval,",
          "75:         restriction=TimeRestriction(earliest=YESTERDAY, latest=None, catchup=False),",
          "",
          "[Removed Lines]",
          "72:     timetable = CronTriggerTimetable(\"30 16 * * *\", timezone=TIMEZONE)",
          "",
          "[Added Lines]",
          "71:     timetable = CronTriggerTimetable(\"30 16 * * *\", timezone=utc)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "81:     \"current_time, earliest, expected\",",
          "82:     [",
          "83:         pytest.param(",
          "85:             START_DATE,",
          "87:             id=\"current_time_on_boundary\",",
          "88:         ),",
          "89:         pytest.param(",
          "91:             START_DATE,",
          "93:             id=\"current_time_not_on_boundary\",",
          "94:         ),",
          "95:         pytest.param(",
          "97:             START_DATE,",
          "99:             id=\"current_time_miss_one_interval_on_boundary\",",
          "100:         ),",
          "101:         pytest.param(",
          "103:             START_DATE,",
          "105:             id=\"current_time_miss_one_interval_not_on_boundary\",",
          "106:         ),",
          "107:         pytest.param(",
          "111:             id=\"future_start_date\",",
          "112:         ),",
          "113:     ],",
          "",
          "[Removed Lines]",
          "84:             pendulum.DateTime(2022, 7, 27, 0, 0, 0, tzinfo=TIMEZONE),",
          "86:             DagRunInfo.exact(pendulum.DateTime(2022, 7, 27, 0, 0, 0, tzinfo=TIMEZONE)),",
          "90:             pendulum.DateTime(2022, 7, 27, 0, 30, 0, tzinfo=TIMEZONE),",
          "92:             DagRunInfo.exact(pendulum.DateTime(2022, 7, 27, 0, 0, 0, tzinfo=TIMEZONE)),",
          "96:             pendulum.DateTime(2022, 7, 27, 1, 0, 0, tzinfo=TIMEZONE),",
          "98:             DagRunInfo.exact(pendulum.DateTime(2022, 7, 27, 1, 0, 0, tzinfo=TIMEZONE)),",
          "102:             pendulum.DateTime(2022, 7, 27, 1, 30, 0, tzinfo=TIMEZONE),",
          "104:             DagRunInfo.exact(pendulum.DateTime(2022, 7, 27, 1, 0, 0, tzinfo=TIMEZONE)),",
          "108:             pendulum.DateTime(2022, 7, 27, 0, 30, 0, tzinfo=TIMEZONE),",
          "109:             pendulum.DateTime(2199, 12, 31, 22, 30, 0, tzinfo=TIMEZONE),",
          "110:             DagRunInfo.exact(pendulum.DateTime(2199, 12, 31, 23, 0, 0, tzinfo=TIMEZONE)),",
          "",
          "[Added Lines]",
          "83:             pendulum.DateTime(2022, 7, 27, 0, 0, 0, tzinfo=utc),",
          "85:             DagRunInfo.exact(pendulum.DateTime(2022, 7, 27, 0, 0, 0, tzinfo=utc)),",
          "89:             pendulum.DateTime(2022, 7, 27, 0, 30, 0, tzinfo=utc),",
          "91:             DagRunInfo.exact(pendulum.DateTime(2022, 7, 27, 0, 0, 0, tzinfo=utc)),",
          "95:             pendulum.DateTime(2022, 7, 27, 1, 0, 0, tzinfo=utc),",
          "97:             DagRunInfo.exact(pendulum.DateTime(2022, 7, 27, 1, 0, 0, tzinfo=utc)),",
          "101:             pendulum.DateTime(2022, 7, 27, 1, 30, 0, tzinfo=utc),",
          "103:             DagRunInfo.exact(pendulum.DateTime(2022, 7, 27, 1, 0, 0, tzinfo=utc)),",
          "107:             pendulum.DateTime(2022, 7, 27, 0, 30, 0, tzinfo=utc),",
          "108:             pendulum.DateTime(2199, 12, 31, 22, 30, 0, tzinfo=utc),",
          "109:             DagRunInfo.exact(pendulum.DateTime(2199, 12, 31, 23, 0, 0, tzinfo=utc)),",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "129:     \"last_automated_data_interval, earliest, expected\",",
          "130:     [",
          "131:         pytest.param(",
          "133:             START_DATE,",
          "135:             id=\"last_automated_on_boundary\",",
          "136:         ),",
          "137:         pytest.param(",
          "139:             START_DATE,",
          "141:             id=\"last_automated_not_on_boundary\",",
          "142:         ),",
          "143:         pytest.param(",
          "144:             None,",
          "147:             id=\"no_last_automated_with_earliest_on_boundary\",",
          "148:         ),",
          "149:         pytest.param(",
          "150:             None,",
          "153:             id=\"no_last_automated_with_earliest_not_on_boundary\",",
          "154:         ),",
          "155:         pytest.param(",
          "",
          "[Removed Lines]",
          "132:             DataInterval.exact(pendulum.DateTime(2022, 7, 27, 0, 0, 0, tzinfo=TIMEZONE)),",
          "134:             DagRunInfo.exact(pendulum.DateTime(2022, 7, 27, 1, 0, 0, tzinfo=TIMEZONE)),",
          "138:             DataInterval.exact(pendulum.DateTime(2022, 7, 27, 0, 30, 0, tzinfo=TIMEZONE)),",
          "140:             DagRunInfo.exact(pendulum.DateTime(2022, 7, 27, 1, 0, 0, tzinfo=TIMEZONE)),",
          "145:             pendulum.DateTime(2022, 7, 27, 0, 0, 0, tzinfo=TIMEZONE),",
          "146:             DagRunInfo.exact(pendulum.DateTime(2022, 7, 27, 0, 0, 0, tzinfo=TIMEZONE)),",
          "151:             pendulum.DateTime(2022, 7, 27, 0, 30, 0, tzinfo=TIMEZONE),",
          "152:             DagRunInfo.exact(pendulum.DateTime(2022, 7, 27, 1, 0, 0, tzinfo=TIMEZONE)),",
          "",
          "[Added Lines]",
          "131:             DataInterval.exact(pendulum.DateTime(2022, 7, 27, 0, 0, 0, tzinfo=utc)),",
          "133:             DagRunInfo.exact(pendulum.DateTime(2022, 7, 27, 1, 0, 0, tzinfo=utc)),",
          "137:             DataInterval.exact(pendulum.DateTime(2022, 7, 27, 0, 30, 0, tzinfo=utc)),",
          "139:             DagRunInfo.exact(pendulum.DateTime(2022, 7, 27, 1, 0, 0, tzinfo=utc)),",
          "144:             pendulum.DateTime(2022, 7, 27, 0, 0, 0, tzinfo=utc),",
          "145:             DagRunInfo.exact(pendulum.DateTime(2022, 7, 27, 0, 0, 0, tzinfo=utc)),",
          "150:             pendulum.DateTime(2022, 7, 27, 0, 30, 0, tzinfo=utc),",
          "151:             DagRunInfo.exact(pendulum.DateTime(2022, 7, 27, 1, 0, 0, tzinfo=utc)),",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "176:     # Runs every Monday on 16:30, covering the day before the run.",
          "177:     timetable = CronTriggerTimetable(",
          "178:         \"30 16 * * MON\",",
          "180:         interval=datetime.timedelta(hours=16, minutes=30),",
          "181:     )",
          "183:     next_info = timetable.next_dagrun_info(",
          "184:         last_automated_data_interval=DataInterval(",
          "187:         ),",
          "188:         restriction=TimeRestriction(earliest=START_DATE, latest=None, catchup=True),",
          "189:     )",
          "190:     assert next_info == DagRunInfo.interval(",
          "193:     )",
          "",
          "[Removed Lines]",
          "179:         timezone=TIMEZONE,",
          "185:             pendulum.DateTime(2022, 8, 1, tzinfo=TIMEZONE),",
          "186:             pendulum.DateTime(2022, 8, 1, 16, 30, tzinfo=TIMEZONE),",
          "191:         pendulum.DateTime(2022, 8, 8, tzinfo=TIMEZONE),",
          "192:         pendulum.DateTime(2022, 8, 8, 16, 30, tzinfo=TIMEZONE),",
          "",
          "[Added Lines]",
          "178:         timezone=utc,",
          "184:             pendulum.DateTime(2022, 8, 1, tzinfo=utc),",
          "185:             pendulum.DateTime(2022, 8, 1, 16, 30, tzinfo=utc),",
          "190:         pendulum.DateTime(2022, 8, 8, tzinfo=utc),",
          "191:         pendulum.DateTime(2022, 8, 8, 16, 30, tzinfo=utc),",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "200: def test_validate_failure() -> None:",
          "203:     with pytest.raises(AirflowTimetableInvalid) as ctx:",
          "204:         timetable.validate()",
          "",
          "[Removed Lines]",
          "201:     timetable = CronTriggerTimetable(\"0 0 1 13 0\", timezone=TIMEZONE)",
          "",
          "[Added Lines]",
          "200:     timetable = CronTriggerTimetable(\"0 0 1 13 0\", timezone=utc)",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "210:     [",
          "211:         (HOURLY_CRON_TRIGGER_TIMETABLE, {\"expression\": \"0 * * * *\", \"timezone\": \"UTC\", \"interval\": 0}),",
          "212:         (",
          "214:             {\"expression\": \"0 0 1 12 *\", \"timezone\": \"UTC\", \"interval\": 7200.0},",
          "215:         ),",
          "216:         (",
          "217:             CronTriggerTimetable(",
          "218:                 \"0 0 1 12 0\",",
          "220:                 interval=dateutil.relativedelta.relativedelta(weekday=dateutil.relativedelta.MO),",
          "221:             ),",
          "222:             {\"expression\": \"0 0 1 12 0\", \"timezone\": \"Asia/Taipei\", \"interval\": {\"weekday\": [0]}},",
          "",
          "[Removed Lines]",
          "213:             CronTriggerTimetable(\"0 0 1 12 *\", timezone=TIMEZONE, interval=datetime.timedelta(hours=2)),",
          "219:                 timezone=pendulum.tz.timezone(\"Asia/Taipei\"),",
          "",
          "[Added Lines]",
          "212:             CronTriggerTimetable(\"0 0 1 12 *\", timezone=utc, interval=datetime.timedelta(hours=2)),",
          "218:                 timezone=\"Asia/Taipei\",",
          "",
          "---------------"
        ],
        "tests/timetables/test_workday_timetable.py||tests/timetables/test_workday_timetable.py": [
          "File: tests/timetables/test_workday_timetable.py -> tests/timetables/test_workday_timetable.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "23: import pytest",
          "25: from airflow.example_dags.plugins.workday import AfterWorkdayTimetable",
          "27: from airflow.timetables.base import DagRunInfo, DataInterval, TimeRestriction, Timetable",
          "31: WEEK_1_WEEKDAYS = [",
          "37: ]",
          "45: @pytest.fixture()",
          "",
          "[Removed Lines]",
          "26: from airflow.settings import TIMEZONE",
          "29: START_DATE = pendulum.DateTime(2021, 9, 4, tzinfo=TIMEZONE)  # This is a Saturday.",
          "32:     pendulum.DateTime(2021, 9, 6, tzinfo=TIMEZONE),  # This is a US holiday",
          "33:     pendulum.DateTime(2021, 9, 7, tzinfo=TIMEZONE),",
          "34:     pendulum.DateTime(2021, 9, 8, tzinfo=TIMEZONE),",
          "35:     pendulum.DateTime(2021, 9, 9, tzinfo=TIMEZONE),",
          "36:     pendulum.DateTime(2021, 9, 10, tzinfo=TIMEZONE),",
          "39: WEEK_1_SATURDAY = pendulum.DateTime(2021, 9, 11, tzinfo=TIMEZONE)",
          "41: WEEK_2_MONDAY = pendulum.DateTime(2021, 9, 13, tzinfo=TIMEZONE)",
          "42: WEEK_2_TUESDAY = pendulum.DateTime(2021, 9, 14, tzinfo=TIMEZONE)",
          "",
          "[Added Lines]",
          "27: from airflow.utils.timezone import utc",
          "29: START_DATE = pendulum.DateTime(2021, 9, 4, tzinfo=utc)  # This is a Saturday.",
          "32:     pendulum.DateTime(2021, 9, 6, tzinfo=utc),  # This is a US holiday",
          "33:     pendulum.DateTime(2021, 9, 7, tzinfo=utc),",
          "34:     pendulum.DateTime(2021, 9, 8, tzinfo=utc),",
          "35:     pendulum.DateTime(2021, 9, 9, tzinfo=utc),",
          "36:     pendulum.DateTime(2021, 9, 10, tzinfo=utc),",
          "39: WEEK_1_SATURDAY = pendulum.DateTime(2021, 9, 11, tzinfo=utc)",
          "41: WEEK_2_MONDAY = pendulum.DateTime(2021, 9, 13, tzinfo=utc)",
          "42: WEEK_2_TUESDAY = pendulum.DateTime(2021, 9, 14, tzinfo=utc)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "8bb9e79b4d9157b87d467b000b89f0bf69c1d9db",
      "candidate_info": {
        "commit_hash": "8bb9e79b4d9157b87d467b000b89f0bf69c1d9db",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/8bb9e79b4d9157b87d467b000b89f0bf69c1d9db",
        "files": [
          "setup.py"
        ],
        "message": "Get rid of pyarrow-hotfix for CVE-2023-47248 (#36697)\n\nThe #35650 introduced a hotfix for Pyarrow CVE-2023-47248. So far\nwe have been blocked from removing it by Apache Beam that limited\nAirflow from bumping pyarrow to a version that was not vulnerable.\n\nThis is now possible since Apache Beam relesed 2.53.0 version on\n4th of January 2023 that allows to use non-vulnerable pyarrow.\n\nWe are now bumping both Pyarrow and Beam minimum versions to\nreflect that and remove pyarrow hotfix.\n\n(cherry picked from commit d105c7115f56f88d48a2888484a0ed7d1c01576f)",
        "before_after_code_files": [
          "setup.py||setup.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "setup.py||setup.py": [
          "File: setup.py -> setup.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "903:                 self.package_data[\"airflow\"].append(provider_relative_path)",
          "904:             # Add python_kubernetes_script.jinja2 to package data",
          "905:             self.package_data[\"airflow\"].append(\"providers/cncf/kubernetes/python_kubernetes_script.jinja2\")",
          "906:         else:",
          "907:             self.install_requires.extend(",
          "908:                 [",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "906:             # Add default email template to package data",
          "907:             self.package_data[\"airflow\"].append(\"providers/smtp/notifications/templates/email.html\")",
          "",
          "---------------"
        ]
      }
    }
  ]
}