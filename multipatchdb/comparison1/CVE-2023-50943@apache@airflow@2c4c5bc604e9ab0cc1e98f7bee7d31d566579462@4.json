{
  "cve_id": "CVE-2023-50943",
  "cve_desc": "Apache Airflow, versions before 2.8.1, have a vulnerability that allows a potential attacker to poison the XCom data by bypassing the protection of \"enable_xcom_pickling=False\" configuration setting resulting in poisoned data after XCom deserialization. This vulnerability is considered low since it requires a DAG author to exploit it. Users are recommended to upgrade to version 2.8.1 or later, which fixes this issue.",
  "repo": "apache/airflow",
  "patch_hash": "2c4c5bc604e9ab0cc1e98f7bee7d31d566579462",
  "patch_info": {
    "commit_hash": "2c4c5bc604e9ab0cc1e98f7bee7d31d566579462",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/2c4c5bc604e9ab0cc1e98f7bee7d31d566579462",
    "files": [
      "airflow/models/xcom.py",
      "tests/api_connexion/schemas/test_xcom_schema.py",
      "tests/models/test_xcom.py"
    ],
    "message": "Stop deserializing pickle when enable_xcom_pickling is False (#36255)\n\n* Stop deserializing pickle when enable_xcom_pickling is False\n\n* Fix unit tests\n\n(cherry picked from commit 63e97abec5d56bc62a293c93f5227f364561e51c)",
    "before_after_code_files": [
      "airflow/models/xcom.py||airflow/models/xcom.py",
      "tests/api_connexion/schemas/test_xcom_schema.py||tests/api_connexion/schemas/test_xcom_schema.py",
      "tests/models/test_xcom.py||tests/models/test_xcom.py"
    ]
  },
  "patch_diff": {
    "airflow/models/xcom.py||airflow/models/xcom.py": [
      "File: airflow/models/xcom.py -> airflow/models/xcom.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "685:             except pickle.UnpicklingError:",
      "686:                 return json.loads(result.value.decode(\"UTF-8\"), cls=XComDecoder, object_hook=object_hook)",
      "687:         else:",
      "693:     @staticmethod",
      "694:     def deserialize_value(result: XCom) -> Any:",
      "",
      "[Removed Lines]",
      "688:             try:",
      "689:                 return json.loads(result.value.decode(\"UTF-8\"), cls=XComDecoder, object_hook=object_hook)",
      "690:             except (json.JSONDecodeError, UnicodeDecodeError):",
      "691:                 return pickle.loads(result.value)",
      "",
      "[Added Lines]",
      "688:             # Since xcom_pickling is disabled, we should only try to deserialize with JSON",
      "689:             return json.loads(result.value.decode(\"UTF-8\"), cls=XComDecoder, object_hook=object_hook)",
      "",
      "---------------"
    ],
    "tests/api_connexion/schemas/test_xcom_schema.py||tests/api_connexion/schemas/test_xcom_schema.py": [
      "File: tests/api_connexion/schemas/test_xcom_schema.py -> tests/api_connexion/schemas/test_xcom_schema.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "30: from airflow.models import DagRun, XCom",
      "31: from airflow.utils.dates import parse_execution_date",
      "32: from airflow.utils.session import create_session",
      "34: pytestmark = pytest.mark.db_test",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "33: from tests.test_utils.config import conf_vars",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "188:     default_time = \"2016-04-02T21:00:00+00:00\"",
      "189:     default_time_parsed = parse_execution_date(default_time)",
      "191:     def test_serialize(self, create_xcom, session):",
      "192:         create_xcom(",
      "193:             dag_id=\"test_dag\",",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "192:     @conf_vars({(\"core\", \"enable_xcom_pickling\"): \"True\"})",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "208:             \"map_index\": -1,",
      "209:         }",
      "211:     def test_deserialize(self):",
      "212:         xcom_dump = {",
      "213:             \"key\": \"test_key\",",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "213:     @conf_vars({(\"core\", \"enable_xcom_pickling\"): \"True\"})",
      "",
      "---------------"
    ],
    "tests/models/test_xcom.py||tests/models/test_xcom.py": [
      "File: tests/models/test_xcom.py -> tests/models/test_xcom.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "140:             ret_value = XCom.get_value(key=\"xcom_test3\", ti_key=ti_key, session=session)",
      "141:         assert ret_value == {\"key\": \"value\"}",
      "144:         with conf_vars({(\"core\", \"enable_xcom_pickling\"): \"True\"}):",
      "145:             XCom.set(",
      "146:                 key=\"xcom_test3\",",
      "",
      "[Removed Lines]",
      "143:     def test_xcom_deserialize_with_pickle_to_json_switch(self, task_instance, session):",
      "",
      "[Added Lines]",
      "143:     def test_xcom_deserialize_pickle_when_xcom_pickling_is_disabled(self, task_instance, session):",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "151:                 session=session,",
      "152:             )",
      "153:         with conf_vars({(\"core\", \"enable_xcom_pickling\"): \"False\"}):",
      "163:     @conf_vars({(\"core\", \"xcom_enable_pickling\"): \"False\"})",
      "164:     def test_xcom_disable_pickle_type_fail_on_non_json(self, task_instance, session):",
      "",
      "[Removed Lines]",
      "154:             ret_value = XCom.get_one(",
      "155:                 key=\"xcom_test3\",",
      "156:                 dag_id=task_instance.dag_id,",
      "157:                 task_id=task_instance.task_id,",
      "158:                 run_id=task_instance.run_id,",
      "159:                 session=session,",
      "160:             )",
      "161:         assert ret_value == {\"key\": \"value\"}",
      "",
      "[Added Lines]",
      "154:             with pytest.raises(UnicodeDecodeError):",
      "155:                 XCom.get_one(",
      "156:                     key=\"xcom_test3\",",
      "157:                     dag_id=task_instance.dag_id,",
      "158:                     task_id=task_instance.task_id,",
      "159:                     run_id=task_instance.run_id,",
      "160:                     session=session,",
      "161:                 )",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "36422edc3738784394f5faa2452d7bc3b1801a33",
      "candidate_info": {
        "commit_hash": "36422edc3738784394f5faa2452d7bc3b1801a33",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/36422edc3738784394f5faa2452d7bc3b1801a33",
        "files": [
          "README.md",
          "RELEASE_NOTES.rst",
          "airflow/__init__.py",
          "airflow/api_connexion/openapi/v1.yaml",
          "airflow/reproducible_build.yaml",
          "docs/apache-airflow/installation/supported-versions.rst",
          "docs/docker-stack/README.md",
          "docs/docker-stack/docker-examples/extending/add-airflow-configuration/Dockerfile",
          "docs/docker-stack/docker-examples/extending/add-apt-packages/Dockerfile",
          "docs/docker-stack/docker-examples/extending/add-build-essential-extend/Dockerfile",
          "docs/docker-stack/docker-examples/extending/add-providers/Dockerfile",
          "docs/docker-stack/docker-examples/extending/add-pypi-packages/Dockerfile",
          "docs/docker-stack/docker-examples/extending/add-requirement-packages/Dockerfile",
          "docs/docker-stack/docker-examples/extending/custom-providers/Dockerfile",
          "docs/docker-stack/docker-examples/extending/embedding-dags/Dockerfile",
          "docs/docker-stack/docker-examples/extending/writable-directory/Dockerfile",
          "docs/docker-stack/entrypoint.rst",
          "generated/PYPI_README.md",
          "scripts/ci/pre_commit/pre_commit_supported_versions.py"
        ],
        "message": "Update version to 2.8.1",
        "before_after_code_files": [
          "airflow/__init__.py||airflow/__init__.py",
          "scripts/ci/pre_commit/pre_commit_supported_versions.py||scripts/ci/pre_commit/pre_commit_supported_versions.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/__init__.py||airflow/__init__.py": [
          "File: airflow/__init__.py -> airflow/__init__.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "26: \"\"\"",
          "27: from __future__ import annotations",
          "31: # flake8: noqa: F401",
          "",
          "[Removed Lines]",
          "29: __version__ = \"2.8.1.dev0\"",
          "",
          "[Added Lines]",
          "29: __version__ = \"2.8.1\"",
          "",
          "---------------"
        ],
        "scripts/ci/pre_commit/pre_commit_supported_versions.py||scripts/ci/pre_commit/pre_commit_supported_versions.py": [
          "File: scripts/ci/pre_commit/pre_commit_supported_versions.py -> scripts/ci/pre_commit/pre_commit_supported_versions.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "27: HEADERS = (\"Version\", \"Current Patch/Minor\", \"State\", \"First Release\", \"Limited Support\", \"EOL/Terminated\")",
          "29: SUPPORTED_VERSIONS = (",
          "31:     (\"1.10\", \"1.10.15\", \"EOL\", \"Aug 27, 2018\", \"Dec 17, 2020\", \"June 17, 2021\"),",
          "32:     (\"1.9\", \"1.9.0\", \"EOL\", \"Jan 03, 2018\", \"Aug 27, 2018\", \"Aug 27, 2018\"),",
          "33:     (\"1.8\", \"1.8.2\", \"EOL\", \"Mar 19, 2017\", \"Jan 03, 2018\", \"Jan 03, 2018\"),",
          "",
          "[Removed Lines]",
          "30:     (\"2\", \"2.8.0\", \"Supported\", \"Dec 17, 2020\", \"TBD\", \"TBD\"),",
          "",
          "[Added Lines]",
          "30:     (\"2\", \"2.8.1\", \"Supported\", \"Dec 17, 2020\", \"TBD\", \"TBD\"),",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "cbab5e4df34df123460dbf805790602a65c2173b",
      "candidate_info": {
        "commit_hash": "cbab5e4df34df123460dbf805790602a65c2173b",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/cbab5e4df34df123460dbf805790602a65c2173b",
        "files": [
          "airflow/www/templates/airflow/dag_details.html"
        ],
        "message": "rename concurrency label to max active tasks (#36691)\n\n(cherry picked from commit 9cbfed474c10891c1429cd538a2bf6c8d014096d)",
        "before_after_code_files": [
          "airflow/www/templates/airflow/dag_details.html||airflow/www/templates/airflow/dag_details.html"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/www/templates/airflow/dag_details.html||airflow/www/templates/airflow/dag_details.html": [
          "File: airflow/www/templates/airflow/dag_details.html -> airflow/www/templates/airflow/dag_details.html",
          "--- Hunk 1 ---",
          "[Context before]",
          "72:       <td>{{ active_runs | length }} / {{ dag.max_active_runs }}</td>",
          "73:     </tr>",
          "74:     <tr>",
          "76:       <td>{{ dag.max_active_tasks }}</td>",
          "77:     </tr>",
          "78:     <tr>",
          "",
          "[Removed Lines]",
          "75:       <th>Concurrency</th>",
          "",
          "[Added Lines]",
          "75:       <th>Max Active Tasks</th>",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "4f9aa6132e16cf51e4d5a3c2dd6920323c1b0d72",
      "candidate_info": {
        "commit_hash": "4f9aa6132e16cf51e4d5a3c2dd6920323c1b0d72",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/4f9aa6132e16cf51e4d5a3c2dd6920323c1b0d72",
        "files": [
          "airflow/operators/trigger_dagrun.py"
        ],
        "message": "explicit string cast required to force integer-type run_ids to be passed as strings instead of integers (#36756)\n\n(cherry picked from commit e2335a00cea898d83e17b3eb69959656daae883e)",
        "before_after_code_files": [
          "airflow/operators/trigger_dagrun.py||airflow/operators/trigger_dagrun.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/operators/trigger_dagrun.py||airflow/operators/trigger_dagrun.py": [
          "File: airflow/operators/trigger_dagrun.py -> airflow/operators/trigger_dagrun.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "157:             raise AirflowException(\"conf parameter should be JSON Serializable\")",
          "159:         if self.trigger_run_id:",
          "161:         else:",
          "162:             run_id = DagRun.generate_run_id(DagRunType.MANUAL, parsed_execution_date)",
          "",
          "[Removed Lines]",
          "160:             run_id = self.trigger_run_id",
          "",
          "[Added Lines]",
          "160:             run_id = str(self.trigger_run_id)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "a6a305f0dc5f49b60ac2e0153262b9741b6b503c",
      "candidate_info": {
        "commit_hash": "a6a305f0dc5f49b60ac2e0153262b9741b6b503c",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/a6a305f0dc5f49b60ac2e0153262b9741b6b503c",
        "files": [
          "airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py",
          "airflow/providers/cncf/kubernetes/executors/kubernetes_executor_types.py",
          "airflow/providers/cncf/kubernetes/executors/kubernetes_executor_utils.py",
          "tests/providers/cncf/kubernetes/executors/test_kubernetes_executor.py"
        ],
        "message": "Kubernetes executor running slots leak fix (#36240)\n\n---------\n\nCo-authored-by: gopal <gopal_dirisala@apple.com>\n(cherry picked from commit 49108e15eb2eb30e2ccb95c9332db7b38d35f2de)",
        "before_after_code_files": [
          "airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py||airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py",
          "airflow/providers/cncf/kubernetes/executors/kubernetes_executor_types.py||airflow/providers/cncf/kubernetes/executors/kubernetes_executor_types.py",
          "airflow/providers/cncf/kubernetes/executors/kubernetes_executor_utils.py||airflow/providers/cncf/kubernetes/executors/kubernetes_executor_utils.py",
          "tests/providers/cncf/kubernetes/executors/test_kubernetes_executor.py||tests/providers/cncf/kubernetes/executors/test_kubernetes_executor.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py||airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py": [
          "File: airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py -> airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "74:     raise",
          "75: from airflow.configuration import conf",
          "76: from airflow.executors.base_executor import BaseExecutor",
          "78: from airflow.providers.cncf.kubernetes.kube_config import KubeConfig",
          "79: from airflow.providers.cncf.kubernetes.kubernetes_helper_functions import annotations_to_key",
          "80: from airflow.utils.event_scheduler import EventScheduler",
          "",
          "[Removed Lines]",
          "77: from airflow.providers.cncf.kubernetes.executors.kubernetes_executor_types import POD_EXECUTOR_DONE_KEY",
          "",
          "[Added Lines]",
          "77: from airflow.providers.cncf.kubernetes.executors.kubernetes_executor_types import (",
          "78:     ADOPTED,",
          "79:     POD_EXECUTOR_DONE_KEY,",
          "80: )",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "450:     def _change_state(",
          "451:         self,",
          "452:         key: TaskInstanceKey,",
          "454:         pod_name: str,",
          "455:         namespace: str,",
          "456:         session: Session = NEW_SESSION,",
          "",
          "[Removed Lines]",
          "453:         state: TaskInstanceState | None,",
          "",
          "[Added Lines]",
          "456:         state: TaskInstanceState | str | None,",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "458:         if TYPE_CHECKING:",
          "459:             assert self.kube_scheduler",
          "461:         if state == TaskInstanceState.RUNNING:",
          "462:             self.event_buffer[key] = state, None",
          "463:             return",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "464:         if state == ADOPTED:",
          "465:             # When the task pod is adopted by another executor,",
          "466:             # then remove the task from the current executor running queue.",
          "467:             try:",
          "468:                 self.running.remove(key)",
          "469:             except KeyError:",
          "470:                 self.log.debug(\"TI key not in running: %s\", key)",
          "471:             return",
          "",
          "---------------"
        ],
        "airflow/providers/cncf/kubernetes/executors/kubernetes_executor_types.py||airflow/providers/cncf/kubernetes/executors/kubernetes_executor_types.py": [
          "File: airflow/providers/cncf/kubernetes/executors/kubernetes_executor_types.py -> airflow/providers/cncf/kubernetes/executors/kubernetes_executor_types.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "16: # under the License.",
          "17: from __future__ import annotations",
          "21: if TYPE_CHECKING:",
          "22:     from airflow.executors.base_executor import CommandType",
          "23:     from airflow.models.taskinstance import TaskInstanceKey",
          "",
          "[Removed Lines]",
          "19: from typing import TYPE_CHECKING, Any, Dict, Optional, Tuple",
          "",
          "[Added Lines]",
          "19: from typing import TYPE_CHECKING, Any, Dict, Optional, Tuple, Union",
          "21: ADOPTED = \"adopted\"",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "27:     KubernetesJobType = Tuple[TaskInstanceKey, CommandType, Any, Optional[str]]",
          "29:     # key, pod state, pod_name, namespace, resource_version",
          "32:     # pod_name, namespace, pod state, annotations, resource_version",
          "35: ALL_NAMESPACES = \"ALL_NAMESPACES\"",
          "36: POD_EXECUTOR_DONE_KEY = \"airflow_executor_done\"",
          "",
          "[Removed Lines]",
          "30:     KubernetesResultsType = Tuple[TaskInstanceKey, Optional[TaskInstanceState], str, str, str]",
          "33:     KubernetesWatchType = Tuple[str, str, Optional[TaskInstanceState], Dict[str, str], str]",
          "",
          "[Added Lines]",
          "31:     KubernetesResultsType = Tuple[TaskInstanceKey, Optional[Union[TaskInstanceState, str]], str, str, str]",
          "34:     KubernetesWatchType = Tuple[str, str, Optional[Union[TaskInstanceState, str]], Dict[str, str], str]",
          "",
          "---------------"
        ],
        "airflow/providers/cncf/kubernetes/executors/kubernetes_executor_utils.py||airflow/providers/cncf/kubernetes/executors/kubernetes_executor_utils.py": [
          "File: airflow/providers/cncf/kubernetes/executors/kubernetes_executor_utils.py -> airflow/providers/cncf/kubernetes/executors/kubernetes_executor_utils.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "41: try:",
          "42:     from airflow.providers.cncf.kubernetes.executors.kubernetes_executor_types import (",
          "43:         ALL_NAMESPACES,",
          "44:         POD_EXECUTOR_DONE_KEY,",
          "45:     )",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "43:         ADOPTED,",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "220:         pod = event[\"object\"]",
          "221:         annotations_string = annotations_for_logging_task_metadata(annotations)",
          "222:         \"\"\"Process status response.\"\"\"",
          "224:             # deletion_timestamp is set by kube server when a graceful deletion is requested.",
          "225:             # since kube server have received request to delete pod set TI state failed",
          "226:             if event[\"type\"] == \"DELETED\" and pod.metadata.deletion_timestamp:",
          "",
          "[Removed Lines]",
          "223:         if status == \"Pending\":",
          "",
          "[Added Lines]",
          "224:         if event[\"type\"] == \"DELETED\" and not pod.metadata.deletion_timestamp:",
          "225:             # This will happen only when the task pods are adopted by another executor.",
          "226:             # So, there is no change in the pod state.",
          "227:             # However, need to free the executor slot from the current executor.",
          "228:             self.log.info(\"Event: pod %s adopted, annotations: %s\", pod_name, annotations_string)",
          "229:             self.watcher_queue.put((pod_name, namespace, ADOPTED, annotations, resource_version))",
          "230:         elif status == \"Pending\":",
          "",
          "---------------"
        ],
        "tests/providers/cncf/kubernetes/executors/test_kubernetes_executor.py||tests/providers/cncf/kubernetes/executors/test_kubernetes_executor.py": [
          "File: tests/providers/cncf/kubernetes/executors/test_kubernetes_executor.py -> tests/providers/cncf/kubernetes/executors/test_kubernetes_executor.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "44:         KubernetesExecutor,",
          "45:         PodReconciliationError,",
          "46:     )",
          "48:     from airflow.providers.cncf.kubernetes.executors.kubernetes_executor_utils import (",
          "49:         AirflowKubernetesScheduler,",
          "50:         KubernetesJobWatcher,",
          "",
          "[Removed Lines]",
          "47:     from airflow.providers.cncf.kubernetes.executors.kubernetes_executor_types import POD_EXECUTOR_DONE_KEY",
          "",
          "[Added Lines]",
          "47:     from airflow.providers.cncf.kubernetes.executors.kubernetes_executor_types import (",
          "48:         ADOPTED,",
          "49:         POD_EXECUTOR_DONE_KEY,",
          "50:     )",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "644:         finally:",
          "645:             executor.end()",
          "647:     @pytest.mark.db_test",
          "648:     @pytest.mark.parametrize(",
          "649:         \"multi_namespace_mode_namespace_list, watchers_keys\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "650:     @pytest.mark.db_test",
          "651:     @mock.patch(\"airflow.providers.cncf.kubernetes.executors.kubernetes_executor_utils.KubernetesJobWatcher\")",
          "652:     @mock.patch(\"airflow.providers.cncf.kubernetes.kube_client.get_kube_client\")",
          "653:     @mock.patch(",
          "654:         \"airflow.providers.cncf.kubernetes.executors.kubernetes_executor_utils.AirflowKubernetesScheduler.delete_pod\"",
          "655:     )",
          "656:     def test_change_state_adopted(self, mock_delete_pod, mock_get_kube_client, mock_kubernetes_job_watcher):",
          "657:         executor = self.kubernetes_executor",
          "658:         executor.start()",
          "659:         try:",
          "660:             key = (\"dag_id\", \"task_id\", \"run_id\", \"try_number2\")",
          "661:             executor.running = {key}",
          "662:             executor._change_state(key, ADOPTED, \"pod_name\", \"default\")",
          "663:             assert len(executor.event_buffer) == 0",
          "664:             assert len(executor.running) == 0",
          "665:             mock_delete_pod.assert_not_called()",
          "666:         finally:",
          "667:             executor.end()",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "1372:         self._run()",
          "1373:         self.watcher.watcher_queue.put.assert_not_called()",
          "1377:         self.events.append({\"type\": \"DELETED\", \"object\": self.pod})",
          "1379:         self._run()",
          "1382:     def test_process_status_running_deleted(self):",
          "1383:         self.pod.status.phase = \"Running\"",
          "",
          "[Removed Lines]",
          "1375:     def test_process_status_succeeded_type_delete(self):",
          "1376:         self.pod.status.phase = \"Succeeded\"",
          "1380:         self.watcher.watcher_queue.put.assert_not_called()",
          "",
          "[Added Lines]",
          "1397:     @pytest.mark.parametrize(",
          "1398:         \"ti_state\",",
          "1399:         [",
          "1400:             TaskInstanceState.SUCCESS,",
          "1401:             TaskInstanceState.FAILED,",
          "1402:             TaskInstanceState.RUNNING,",
          "1403:             TaskInstanceState.QUEUED,",
          "1404:             TaskInstanceState.UP_FOR_RETRY,",
          "1405:         ],",
          "1406:     )",
          "1407:     def test_process_status_pod_adopted(self, ti_state):",
          "1408:         self.pod.status.phase = ti_state",
          "1410:         self.pod.metadata.deletion_timestamp = None",
          "1413:         self.watcher.watcher_queue.put.assert_called_once_with(",
          "1414:             (",
          "1415:                 self.pod.metadata.name,",
          "1416:                 self.watcher.namespace,",
          "1417:                 ADOPTED,",
          "1418:                 self.core_annotations,",
          "1419:                 self.pod.metadata.resource_version,",
          "1420:             )",
          "1421:         )",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "8bb9e79b4d9157b87d467b000b89f0bf69c1d9db",
      "candidate_info": {
        "commit_hash": "8bb9e79b4d9157b87d467b000b89f0bf69c1d9db",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/8bb9e79b4d9157b87d467b000b89f0bf69c1d9db",
        "files": [
          "setup.py"
        ],
        "message": "Get rid of pyarrow-hotfix for CVE-2023-47248 (#36697)\n\nThe #35650 introduced a hotfix for Pyarrow CVE-2023-47248. So far\nwe have been blocked from removing it by Apache Beam that limited\nAirflow from bumping pyarrow to a version that was not vulnerable.\n\nThis is now possible since Apache Beam relesed 2.53.0 version on\n4th of January 2023 that allows to use non-vulnerable pyarrow.\n\nWe are now bumping both Pyarrow and Beam minimum versions to\nreflect that and remove pyarrow hotfix.\n\n(cherry picked from commit d105c7115f56f88d48a2888484a0ed7d1c01576f)",
        "before_after_code_files": [
          "setup.py||setup.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "setup.py||setup.py": [
          "File: setup.py -> setup.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "903:                 self.package_data[\"airflow\"].append(provider_relative_path)",
          "904:             # Add python_kubernetes_script.jinja2 to package data",
          "905:             self.package_data[\"airflow\"].append(\"providers/cncf/kubernetes/python_kubernetes_script.jinja2\")",
          "906:         else:",
          "907:             self.install_requires.extend(",
          "908:                 [",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "906:             # Add default email template to package data",
          "907:             self.package_data[\"airflow\"].append(\"providers/smtp/notifications/templates/email.html\")",
          "",
          "---------------"
        ]
      }
    }
  ]
}