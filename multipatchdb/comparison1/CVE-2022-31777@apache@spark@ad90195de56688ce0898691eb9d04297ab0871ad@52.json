{
  "cve_id": "CVE-2022-31777",
  "cve_desc": "A stored cross-site scripting (XSS) vulnerability in Apache Spark 3.2.1 and earlier, and 3.3.0, allows remote attackers to execute arbitrary JavaScript in the web browser of a user, by including a malicious payload into the logs which would be returned in logs rendered in the UI.",
  "repo": "apache/spark",
  "patch_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
  "patch_info": {
    "commit_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
    "repo": "apache/spark",
    "commit_url": "https://github.com/apache/spark/commit/ad90195de56688ce0898691eb9d04297ab0871ad",
    "files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ],
    "message": "[SPARK-39505][UI] Escape log content rendered in UI\n\n### What changes were proposed in this pull request?\n\nEscape log content rendered to the UI.\n\n### Why are the changes needed?\n\nLog content may contain reserved characters or other code in the log and be misinterpreted in the UI as HTML.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nExisting tests\n\nCloses #36902 from srowen/LogViewEscape.\n\nAuthored-by: Sean Owen <srowen@gmail.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
    "before_after_code_files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ]
  },
  "patch_diff": {
    "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js": [
      "File: core/src/main/resources/org/apache/spark/ui/static/log-view.js -> core/src/main/resources/org/apache/spark/ui/static/log-view.js",
      "--- Hunk 1 ---",
      "[Context before]",
      "85:       if (retStartByte == 0) {",
      "86:         disableMoreButton();",
      "87:       }",
      "90:       curLogLength = curLogLength + (startByte - retStartByte);",
      "91:       startByte = retStartByte;",
      "",
      "[Removed Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(cleanData);",
      "",
      "[Added Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(document.createTextNode(cleanData));",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "115:             var retLogLength = dataInfo[2];",
      "117:             var cleanData = data.substring(newlineIndex + 1);",
      "120:             curLogLength = curLogLength + (retEndByte - retStartByte);",
      "121:             endByte = retEndByte;",
      "",
      "[Removed Lines]",
      "118:             $(\"pre\", \".log-content\").append(cleanData);",
      "",
      "[Added Lines]",
      "118:             $(\"pre\", \".log-content\").append(document.createTextNode(cleanData));",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "83a365edf163bdd30974756c6c58fdca2e16f7f3",
      "candidate_info": {
        "commit_hash": "83a365edf163bdd30974756c6c58fdca2e16f7f3",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/83a365edf163bdd30974756c6c58fdca2e16f7f3",
        "files": [
          "core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala",
          "core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala",
          "core/src/test/scala/org/apache/spark/rdd/HadoopRDDSuite.scala"
        ],
        "message": "[SPARK-38922][CORE] TaskLocation.apply throw NullPointerException\n\n### What changes were proposed in this pull request?\n\nTaskLocation.apply w/o NULL check may throw NPE and fail job scheduling\n\n```\n\nCaused by: java.lang.NullPointerException\n    at scala.collection.immutable.StringLike$class.stripPrefix(StringLike.scala:155)\n    at scala.collection.immutable.StringOps.stripPrefix(StringOps.scala:29)\n    at org.apache.spark.scheduler.TaskLocation$.apply(TaskLocation.scala:71)\n    at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$getPreferredLocsInternal\n```\n\nFor instance, `org.apache.spark.rdd.HadoopRDD#convertSplitLocationInfo` might generate unexpected `Some(null)` elements where should be replace by `Option.apply`\n\n### Why are the changes needed?\n\nfix NPE\n\n### Does this PR introduce _any_ user-facing change?\n\nno\n\n### How was this patch tested?\n\nnew tests\n\nCloses #36222 from yaooqinn/SPARK-38922.\n\nAuthored-by: Kent Yao <yao@apache.org>\nSigned-off-by: Kent Yao <yao@apache.org>\n(cherry picked from commit 33e07f3cd926105c6d28986eb6218f237505549e)\nSigned-off-by: Kent Yao <yao@apache.org>",
        "before_after_code_files": [
          "core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala||core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala",
          "core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala||core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala",
          "core/src/test/scala/org/apache/spark/rdd/HadoopRDDSuite.scala||core/src/test/scala/org/apache/spark/rdd/HadoopRDDSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala||core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala": [
          "File: core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala -> core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "454:        infos: Array[SplitLocationInfo]): Option[Seq[String]] = {",
          "455:     Option(infos).map(_.flatMap { loc =>",
          "456:       val locationStr = loc.getLocation",
          "458:         if (loc.isInMemory) {",
          "459:           logDebug(s\"Partition $locationStr is cached by Hadoop.\")",
          "460:           Some(HDFSCacheTaskLocation(locationStr).toString)",
          "",
          "[Removed Lines]",
          "457:       if (locationStr != \"localhost\") {",
          "",
          "[Added Lines]",
          "457:       if (locationStr != null && locationStr != \"localhost\") {",
          "",
          "---------------"
        ],
        "core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala||core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala": [
          "File: core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala -> core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "2737:     val rddPrefs = rdd.preferredLocations(rdd.partitions(partition)).toList",
          "2738:     if (rddPrefs.nonEmpty) {",
          "2740:     }",
          "",
          "[Removed Lines]",
          "2739:       return rddPrefs.map(TaskLocation(_))",
          "",
          "[Added Lines]",
          "2739:       return rddPrefs.filter(_ != null).map(TaskLocation(_))",
          "",
          "---------------"
        ],
        "core/src/test/scala/org/apache/spark/rdd/HadoopRDDSuite.scala||core/src/test/scala/org/apache/spark/rdd/HadoopRDDSuite.scala": [
          "File: core/src/test/scala/org/apache/spark/rdd/HadoopRDDSuite.scala -> core/src/test/scala/org/apache/spark/rdd/HadoopRDDSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "18: package org.apache.spark.rdd",
          "20: import org.apache.hadoop.mapred.SplitLocationInfo",
          "22: import org.apache.spark.SparkFunSuite",
          "24: class HadoopRDDSuite extends SparkFunSuite {",
          "26:   test(\"SPARK-38922: HadoopRDD convertSplitLocationInfo contains Some(null) cause NPE\") {",
          "27:     val locs = Array(new SplitLocationInfo(null, false))",
          "28:     assert(HadoopRDD.convertSplitLocationInfo(locs).get.isEmpty)",
          "29:   }",
          "30: }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "997e7f0af506410d76f1232acc00ad9518e9a804",
      "candidate_info": {
        "commit_hash": "997e7f0af506410d76f1232acc00ad9518e9a804",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/997e7f0af506410d76f1232acc00ad9518e9a804",
        "files": [
          "core/src/main/scala/org/apache/spark/ErrorInfo.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/trees/TreeNode.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/trees/TreeNodeSuite.scala"
        ],
        "message": "[SPARK-39234][SQL][3.3] Code clean up in SparkThrowableHelper.getMessage\n\n### What changes were proposed in this pull request?\n\n1. Remove the starting \"\\n\" in `Origin.context`. The \"\\n\" will be append in the method `SparkThrowableHelper.getMessage` instead.\n2. Code clean up the method SparkThrowableHelper.getMessage to eliminate redundant code.\n\n### Why are the changes needed?\n\nCode clean up to eliminate redundant code.\n### Does this PR introduce _any_ user-facing change?\n\nNo\n### How was this patch tested?\n\nExisting UT\n\nCloses #36669 from gengliangwang/portSPARK-39234.\n\nAuthored-by: Gengliang Wang <gengliang@apache.org>\nSigned-off-by: Gengliang Wang <gengliang@apache.org>",
        "before_after_code_files": [
          "core/src/main/scala/org/apache/spark/ErrorInfo.scala||core/src/main/scala/org/apache/spark/ErrorInfo.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/trees/TreeNode.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/trees/TreeNode.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/trees/TreeNodeSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/trees/TreeNodeSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "core/src/main/scala/org/apache/spark/ErrorInfo.scala||core/src/main/scala/org/apache/spark/ErrorInfo.scala": [
          "File: core/src/main/scala/org/apache/spark/ErrorInfo.scala -> core/src/main/scala/org/apache/spark/ErrorInfo.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "61:       queryContext: String = \"\"): String = {",
          "62:     val errorInfo = errorClassToInfoMap.getOrElse(errorClass,",
          "63:       throw new IllegalArgumentException(s\"Cannot find error class '$errorClass'\"))",
          "64:     String.format(errorInfo.messageFormat.replaceAll(\"<[a-zA-Z0-9_-]+>\", \"%s\"),",
          "66:   }",
          "68:   def getSqlState(errorClass: String): String = {",
          "",
          "[Removed Lines]",
          "65:       messageParameters: _*) + queryContext",
          "",
          "[Added Lines]",
          "64:     val displayQueryContext = if (queryContext.isEmpty) {",
          "65:       \"\"",
          "66:     } else {",
          "67:       s\"\\n$queryContext\"",
          "68:     }",
          "70:       messageParameters: _*) + displayQueryContext",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/trees/TreeNode.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/trees/TreeNode.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/trees/TreeNode.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/trees/TreeNode.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "89:         \"\"",
          "90:       }",
          "91:       val builder = new StringBuilder",
          "94:       val text = sqlText.get",
          "95:       val start = math.max(startIndex.get, 0)",
          "",
          "[Removed Lines]",
          "92:       builder ++= s\"\\n== SQL$objectContext$positionContext ==\\n\"",
          "",
          "[Added Lines]",
          "92:       builder ++= s\"== SQL$objectContext$positionContext ==\\n\"",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/trees/TreeNodeSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/trees/TreeNodeSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/trees/TreeNodeSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/trees/TreeNodeSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "876:       objectType = Some(\"VIEW\"),",
          "877:       objectName = Some(\"some_view\"))",
          "878:     val expected =",
          "881:         |...7890 + 1234567890 + 1234567890, cast('a'",
          "882:         |                                   ^^^^^^^^",
          "",
          "[Removed Lines]",
          "879:       \"\"\"",
          "880:         |== SQL of VIEW some_view(line 3, position 38) ==",
          "",
          "[Added Lines]",
          "879:       \"\"\"== SQL of VIEW some_view(line 3, position 38) ==",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "94f3e4113ef6fbf0940578bcb279f233e43c27f1",
      "candidate_info": {
        "commit_hash": "94f3e4113ef6fbf0940578bcb279f233e43c27f1",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/94f3e4113ef6fbf0940578bcb279f233e43c27f1",
        "files": [
          "external/kafka-0-10-sql/src/test/scala/org/apache/spark/sql/kafka010/KafkaMicroBatchSourceSuite.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/WholeStageCodegenSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecutionSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/streaming/continuous/ContinuousSuite.scala"
        ],
        "message": "[SPARK-39412][SQL] Exclude IllegalStateException from Spark's internal errors\n\n### What changes were proposed in this pull request?\nIn the PR, I propose to exclude `IllegalStateException` from the list of exceptions that are wrapped by `SparkException` with the `INTERNAL_ERROR` error class.\n\n### Why are the changes needed?\nSee explanation in SPARK-39412.\n\n### Does this PR introduce _any_ user-facing change?\nNo, the reverted changes haven't released yet.\n\n### How was this patch tested?\nBy running the modified test suites:\n```\n$ build/sbt \"test:testOnly *ContinuousSuite\"\n$ build/sbt \"test:testOnly *MicroBatchExecutionSuite\"\n$ build/sbt \"test:testOnly *KafkaMicroBatchV1SourceSuite\"\n$ build/sbt \"test:testOnly *KafkaMicroBatchV2SourceSuite\"\n$ build/sbt \"test:testOnly *.WholeStageCodegenSuite\"\n```\n\nCloses #36804 from MaxGekk/exclude-IllegalStateException.\n\nAuthored-by: Max Gekk <max.gekk@gmail.com>\nSigned-off-by: Max Gekk <max.gekk@gmail.com>\n(cherry picked from commit 19afe1341d277bc2d7dd47175d142a8c71141138)\nSigned-off-by: Max Gekk <max.gekk@gmail.com>",
        "before_after_code_files": [
          "external/kafka-0-10-sql/src/test/scala/org/apache/spark/sql/kafka010/KafkaMicroBatchSourceSuite.scala||external/kafka-0-10-sql/src/test/scala/org/apache/spark/sql/kafka010/KafkaMicroBatchSourceSuite.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala||sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/WholeStageCodegenSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/WholeStageCodegenSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecutionSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecutionSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/streaming/continuous/ContinuousSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/streaming/continuous/ContinuousSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "external/kafka-0-10-sql/src/test/scala/org/apache/spark/sql/kafka010/KafkaMicroBatchSourceSuite.scala||external/kafka-0-10-sql/src/test/scala/org/apache/spark/sql/kafka010/KafkaMicroBatchSourceSuite.scala": [
          "File: external/kafka-0-10-sql/src/test/scala/org/apache/spark/sql/kafka010/KafkaMicroBatchSourceSuite.scala -> external/kafka-0-10-sql/src/test/scala/org/apache/spark/sql/kafka010/KafkaMicroBatchSourceSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "34: import org.scalatest.concurrent.PatienceConfiguration.Timeout",
          "35: import org.scalatest.time.SpanSugar._",
          "38: import org.apache.spark.sql.{Dataset, ForeachWriter, Row, SparkSession}",
          "39: import org.apache.spark.sql.catalyst.util.CaseInsensitiveMap",
          "40: import org.apache.spark.sql.connector.read.streaming.SparkDataStream",
          "",
          "[Removed Lines]",
          "37: import org.apache.spark.{SparkException, SparkThrowable}",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "667:         testUtils.sendMessages(topic2, Array(\"6\"))",
          "668:       },",
          "669:       StartStream(),",
          "674:       })",
          "675:     )",
          "676:   }",
          "",
          "[Removed Lines]",
          "670:       ExpectFailure[SparkException](e => {",
          "671:         assert(e.asInstanceOf[SparkThrowable].getErrorClass === \"INTERNAL_ERROR\")",
          "673:         assert(e.getCause.getMessage.contains(\"was changed from 2 to 1\"))",
          "",
          "[Added Lines]",
          "669:       ExpectFailure[IllegalStateException](e => {",
          "671:         assert(e.getMessage.contains(\"was changed from 2 to 1\"))",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "767:       testStream(df)(",
          "768:         StartStream(checkpointLocation = metadataPath.getAbsolutePath),",
          "771:           Seq(",
          "772:             s\"maximum supported log version is v1, but encountered v99999\",",
          "773:             \"produced by a newer version of Spark and cannot be read by this version\"",
          "774:           ).foreach { message =>",
          "776:           }",
          "777:         }))",
          "778:     }",
          "",
          "[Removed Lines]",
          "769:         ExpectFailure[SparkException](e => {",
          "770:           assert(e.asInstanceOf[SparkThrowable].getErrorClass === \"INTERNAL_ERROR\")",
          "775:             assert(e.getCause.toString.contains(message))",
          "",
          "[Added Lines]",
          "767:         ExpectFailure[IllegalStateException](e => {",
          "772:             assert(e.toString.contains(message))",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala||sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala -> sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "489:   }",
          "494:   private[sql] def toInternalError(msg: String, e: Throwable): Throwable = e match {",
          "497:       new SparkException(",
          "498:         errorClass = \"INTERNAL_ERROR\",",
          "499:         messageParameters = Array(msg +",
          "",
          "[Removed Lines]",
          "495:     case e @ (_: java.lang.IllegalStateException | _: java.lang.NullPointerException |",
          "496:               _: java.lang.AssertionError) =>",
          "",
          "[Added Lines]",
          "495:     case e @ (_: java.lang.NullPointerException | _: java.lang.AssertionError) =>",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/execution/WholeStageCodegenSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/WholeStageCodegenSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/execution/WholeStageCodegenSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/execution/WholeStageCodegenSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: package org.apache.spark.sql.execution",
          "21: import org.apache.spark.sql.{Dataset, QueryTest, Row, SaveMode}",
          "22: import org.apache.spark.sql.catalyst.expressions.codegen.{ByteCodeStats, CodeAndComment, CodeGenerator}",
          "23: import org.apache.spark.sql.execution.adaptive.DisableAdaptiveExecutionSuite",
          "",
          "[Removed Lines]",
          "20: import org.apache.spark.SparkException",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "763:           \"SELECT AVG(v) FROM VALUES(1) t(v)\",",
          "765:           \"SELECT k, AVG(v) FROM VALUES((1, 1)) t(k, v) GROUP BY k\").foreach { query =>",
          "767:             sql(query).collect",
          "768:           }",
          "771:         }",
          "772:       }",
          "773:     }",
          "",
          "[Removed Lines]",
          "766:           val e = intercept[SparkException] {",
          "769:           assert(e.getErrorClass === \"INTERNAL_ERROR\")",
          "770:           assert(e.getCause.getMessage.contains(expectedErrMsg))",
          "",
          "[Added Lines]",
          "765:           val e = intercept[IllegalStateException] {",
          "768:           assert(e.getMessage.contains(expectedErrMsg))",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "787:           \"SELECT k, AVG(a + b), SUM(a + b + c) FROM VALUES((1, 1, 1, 1)) t(k, a, b, c) \" +",
          "788:             \"GROUP BY k\").foreach { query =>",
          "790:             sql(query).collect",
          "791:           }",
          "794:         }",
          "795:       }",
          "796:     }",
          "",
          "[Removed Lines]",
          "789:           val e = intercept[SparkException] {",
          "792:           assert(e.getErrorClass === \"INTERNAL_ERROR\")",
          "793:           assert(e.getCause.getMessage.contains(expectedErrMsg))",
          "",
          "[Added Lines]",
          "787:           val e = intercept[IllegalStateException] {",
          "790:           assert(e.getMessage.contains(expectedErrMsg))",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecutionSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecutionSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecutionSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecutionSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "22: import org.apache.commons.io.FileUtils",
          "23: import org.scalatest.BeforeAndAfter",
          "26: import org.apache.spark.sql.{DataFrame, Dataset, SparkSession}",
          "27: import org.apache.spark.sql.catalyst.plans.logical.Range",
          "28: import org.apache.spark.sql.connector.read.streaming",
          "",
          "[Removed Lines]",
          "25: import org.apache.spark.{SparkException, SparkThrowable}",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "94:     testStream(streamEvent) (",
          "95:       AddData(inputData, 1, 2, 3, 4, 5, 6),",
          "96:       StartStream(Trigger.Once, checkpointLocation = checkpointDir.getAbsolutePath),",
          "100:       }",
          "101:     )",
          "102:   }",
          "",
          "[Removed Lines]",
          "97:       ExpectFailure[SparkException] { e =>",
          "98:         assert(e.asInstanceOf[SparkThrowable].getErrorClass === \"INTERNAL_ERROR\")",
          "99:         assert(e.getCause.getMessage.contains(\"batch 3 doesn't exist\"))",
          "",
          "[Added Lines]",
          "96:       ExpectFailure[IllegalStateException] { e =>",
          "97:         assert(e.getMessage.contains(\"batch 3 doesn't exist\"))",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/streaming/continuous/ContinuousSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/streaming/continuous/ContinuousSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/streaming/continuous/ContinuousSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/streaming/continuous/ContinuousSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: import java.sql.Timestamp",
          "23: import org.apache.spark.scheduler.{SparkListener, SparkListenerTaskStart}",
          "24: import org.apache.spark.sql._",
          "25: import org.apache.spark.sql.execution.streaming._",
          "",
          "[Removed Lines]",
          "22: import org.apache.spark.{SparkContext, SparkException, SparkThrowable}",
          "",
          "[Added Lines]",
          "22: import org.apache.spark.{SparkContext, SparkException}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "441:       testStream(df)(",
          "442:         StartStream(Trigger.Continuous(1)),",
          "446:         }",
          "447:       )",
          "448:     }",
          "",
          "[Removed Lines]",
          "443:         ExpectFailure[SparkException] { e =>",
          "444:           assert(e.asInstanceOf[SparkThrowable].getErrorClass === \"INTERNAL_ERROR\")",
          "445:           e.getCause.getMessage.contains(\"queue has exceeded its maximum\")",
          "",
          "[Added Lines]",
          "443:         ExpectFailure[IllegalStateException] { e =>",
          "444:           e.getMessage.contains(\"queue has exceeded its maximum\")",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "008b3a347595cc47ff30853d7141b17bf7be4f13",
      "candidate_info": {
        "commit_hash": "008b3a347595cc47ff30853d7141b17bf7be4f13",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/008b3a347595cc47ff30853d7141b17bf7be4f13",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CollectionExpressionsSuite.scala"
        ],
        "message": "[SPARK-40152][SQL][TESTS] Add tests for SplitPart\n\n### What changes were proposed in this pull request?\n\nAdd tests for `SplitPart`.\n\n### Why are the changes needed?\n\nImprove test coverage.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nN/A.\n\nCloses #37626 from wangyum/SPARK-40152-2.\n\nAuthored-by: Yuming Wang <yumwang@ebay.com>\nSigned-off-by: Sean Owen <srowen@gmail.com>\n(cherry picked from commit 4f525eed7d5d461498aee68c4d3e57941f9aae2c)\nSigned-off-by: Sean Owen <srowen@gmail.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CollectionExpressionsSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CollectionExpressionsSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "2225:               case Some(value) =>",
          "2226:                 val defaultValueEval = value.genCode(ctx)",
          "2227:                 s\"\"\"",
          "2229:                   ${ev.isNull} = ${defaultValueEval.isNull};",
          "2230:                   ${ev.value} = ${defaultValueEval.value};",
          "2231:                 \"\"\".stripMargin",
          "",
          "[Removed Lines]",
          "2228:                   ${defaultValueEval.code};",
          "",
          "[Added Lines]",
          "2228:                   ${defaultValueEval.code}",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CollectionExpressionsSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CollectionExpressionsSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CollectionExpressionsSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CollectionExpressionsSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "2532:           Date.valueOf(\"2017-02-12\")))",
          "2533:     }",
          "2534:   }",
          "2535: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "2536:   test(\"SplitPart\") {",
          "2537:     val delimiter = Literal.create(\".\", StringType)",
          "2538:     val str = StringSplitSQL(Literal.create(\"11.12.13\", StringType), delimiter)",
          "2539:     val outOfBoundValue = Some(Literal.create(\"\", StringType))",
          "2541:     checkEvaluation(ElementAt(str, Literal(3), outOfBoundValue), UTF8String.fromString(\"13\"))",
          "2542:     checkEvaluation(ElementAt(str, Literal(1), outOfBoundValue), UTF8String.fromString(\"11\"))",
          "2543:     checkEvaluation(ElementAt(str, Literal(10), outOfBoundValue), UTF8String.fromString(\"\"))",
          "2544:     checkEvaluation(ElementAt(str, Literal(-10), outOfBoundValue), UTF8String.fromString(\"\"))",
          "2546:     checkEvaluation(ElementAt(StringSplitSQL(Literal.create(null, StringType), delimiter),",
          "2547:       Literal(1), outOfBoundValue), null)",
          "2548:     checkEvaluation(ElementAt(StringSplitSQL(Literal.create(\"11.12.13\", StringType),",
          "2549:       Literal.create(null, StringType)), Literal(1), outOfBoundValue), null)",
          "2551:     intercept[Exception] {",
          "2552:       checkEvaluation(ElementAt(str, Literal(0), outOfBoundValue), null)",
          "2553:     }.getMessage.contains(\"The index 0 is invalid\")",
          "2554:   }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "baaa3bbecd9f63aa0a71cf76de4b53d3c1dcf7a4",
      "candidate_info": {
        "commit_hash": "baaa3bbecd9f63aa0a71cf76de4b53d3c1dcf7a4",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/baaa3bbecd9f63aa0a71cf76de4b53d3c1dcf7a4",
        "files": [
          "python/pyspark/streaming/context.py",
          "python/pyspark/streaming/context.pyi",
          "python/pyspark/streaming/kinesis.py"
        ],
        "message": "[SPARK-37014][PYTHON] Inline type hints for python/pyspark/streaming/context.py\n\n### What changes were proposed in this pull request?\nInline type hints for python/pyspark/streaming/context.py from Inline type hints for python/pyspark/streaming/context.pyi.\n\n### Why are the changes needed?\nCurrently, there is type hint stub files python/pyspark/streaming/context.pyi to show the expected types for functions, but we can also take advantage of static type checking within the functions by inlining the type hints.\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\nExisting test.\n\nCloses #34293 from dchvn/SPARK-37014.\n\nAuthored-by: dch nguyen <dchvn.dgd@gmail.com>\nSigned-off-by: zero323 <mszymkiewicz@gmail.com>\n(cherry picked from commit c0c1f35cd9279bc1a7a50119be72a297162a9b55)\nSigned-off-by: zero323 <mszymkiewicz@gmail.com>",
        "before_after_code_files": [
          "python/pyspark/streaming/context.py||python/pyspark/streaming/context.py",
          "python/pyspark/streaming/context.pyi||python/pyspark/streaming/context.pyi",
          "python/pyspark/streaming/kinesis.py||python/pyspark/streaming/kinesis.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "python/pyspark/streaming/context.py||python/pyspark/streaming/context.py": [
          "File: python/pyspark/streaming/context.py -> python/pyspark/streaming/context.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "14: # See the License for the specific language governing permissions and",
          "15: # limitations under the License.",
          "16: #",
          "20: from pyspark import RDD, SparkConf",
          "21: from pyspark.serializers import NoOpSerializer, UTF8Deserializer, CloudPickleSerializer",
          "22: from pyspark.context import SparkContext",
          "23: from pyspark.storagelevel import StorageLevel",
          "24: from pyspark.streaming.dstream import DStream",
          "25: from pyspark.streaming.util import TransformFunction, TransformFunctionSerializer",
          "27: __all__ = [\"StreamingContext\"]",
          "30: class StreamingContext:",
          "31:     \"\"\"",
          "",
          "[Removed Lines]",
          "18: from py4j.java_gateway import java_import, is_instance_of",
          "",
          "[Added Lines]",
          "17: from typing import Any, Callable, List, Optional, TypeVar",
          "19: from py4j.java_gateway import java_import, is_instance_of, JavaObject",
          "26: from pyspark.streaming.listener import StreamingListener",
          "31: T = TypeVar(\"T\")",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "51:     # Reference to a currently active StreamingContext",
          "52:     _activeContext = None",
          "56:         self._sc = sparkContext",
          "57:         self._jvm = self._sc._jvm",
          "58:         self._jssc = jssc or self._initialize_context(self._sc, batchDuration)",
          "61:         self._ensure_initialized()",
          "62:         return self._jvm.JavaStreamingContext(sc._jsc, self._jduration(duration))",
          "65:         \"\"\"",
          "66:         Create Duration object given number of seconds",
          "67:         \"\"\"",
          "68:         return self._jvm.Duration(int(seconds * 1000))",
          "70:     @classmethod",
          "72:         SparkContext._ensure_initialized()",
          "73:         gw = SparkContext._gateway",
          "75:         java_import(gw.jvm, \"org.apache.spark.streaming.*\")",
          "76:         java_import(gw.jvm, \"org.apache.spark.streaming.api.java.*\")",
          "77:         java_import(gw.jvm, \"org.apache.spark.streaming.api.python.*\")",
          "",
          "[Removed Lines]",
          "54:     def __init__(self, sparkContext, batchDuration=None, jssc=None):",
          "60:     def _initialize_context(self, sc, duration):",
          "64:     def _jduration(self, seconds):",
          "71:     def _ensure_initialized(cls):",
          "",
          "[Added Lines]",
          "58:     def __init__(",
          "59:         self,",
          "60:         sparkContext: SparkContext,",
          "61:         batchDuration: Optional[int] = None,",
          "62:         jssc: Optional[JavaObject] = None,",
          "63:     ):",
          "68:     def _initialize_context(self, sc: SparkContext, duration: Optional[int]) -> JavaObject:",
          "70:         assert self._jvm is not None and duration is not None",
          "73:     def _jduration(self, seconds: int) -> JavaObject:",
          "77:         assert self._jvm is not None",
          "81:     def _ensure_initialized(cls) -> None:",
          "85:         assert gw is not None",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "83:         # register serializer for TransformFunction",
          "84:         # it happens before creating SparkContext when loading from checkpointing",
          "85:         cls._transformerSerializer = TransformFunctionSerializer(",
          "87:         )",
          "89:     @classmethod",
          "91:         \"\"\"",
          "92:         Either recreate a StreamingContext from checkpoint data or create a new StreamingContext.",
          "93:         If checkpoint data exists in the provided `checkpointPath`, then StreamingContext will be",
          "",
          "[Removed Lines]",
          "86:             SparkContext._active_spark_context, CloudPickleSerializer(), gw",
          "90:     def getOrCreate(cls, checkpointPath, setupFunc):",
          "",
          "[Added Lines]",
          "98:             SparkContext._active_spark_context,",
          "99:             CloudPickleSerializer(),",
          "100:             gw,",
          "104:     def getOrCreate(",
          "105:         cls, checkpointPath: str, setupFunc: Callable[[], \"StreamingContext\"]",
          "106:     ) -> \"StreamingContext\":",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "104:         cls._ensure_initialized()",
          "105:         gw = SparkContext._gateway",
          "107:         # Check whether valid checkpoint information exists in the given path",
          "108:         ssc_option = gw.jvm.StreamingContextPythonHelper().tryRecoverFromCheckpoint(checkpointPath)",
          "109:         if ssc_option.isEmpty():",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "123:         assert gw is not None",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "122:         sc = SparkContext._active_spark_context",
          "124:         # update ctx in serializer",
          "125:         cls._transformerSerializer.ctx = sc",
          "126:         return StreamingContext(sc, None, jssc)",
          "128:     @classmethod",
          "130:         \"\"\"",
          "131:         Return either the currently active StreamingContext (i.e., if there is a context started",
          "132:         but not stopped) or None.",
          "",
          "[Removed Lines]",
          "129:     def getActive(cls):",
          "",
          "[Added Lines]",
          "142:         assert sc is not None",
          "145:         assert cls._transformerSerializer is not None",
          "150:     def getActive(cls) -> Optional[\"StreamingContext\"]:",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "149:         return cls._activeContext",
          "151:     @classmethod",
          "153:         \"\"\"",
          "154:         Either return the active StreamingContext (i.e. currently started but not stopped),",
          "155:         or recreate a StreamingContext from checkpoint data or create a new StreamingContext",
          "",
          "[Removed Lines]",
          "152:     def getActiveOrCreate(cls, checkpointPath, setupFunc):",
          "",
          "[Added Lines]",
          "173:     def getActiveOrCreate(",
          "174:         cls, checkpointPath: str, setupFunc: Callable[[], \"StreamingContext\"]",
          "175:     ) -> \"StreamingContext\":",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "178:             return setupFunc()",
          "180:     @property",
          "182:         \"\"\"",
          "183:         Return SparkContext which is associated with this StreamingContext.",
          "184:         \"\"\"",
          "185:         return self._sc",
          "188:         \"\"\"",
          "189:         Start the execution of the streams.",
          "190:         \"\"\"",
          "191:         self._jssc.start()",
          "192:         StreamingContext._activeContext = self",
          "195:         \"\"\"",
          "196:         Wait for the execution to stop.",
          "",
          "[Removed Lines]",
          "181:     def sparkContext(self):",
          "187:     def start(self):",
          "194:     def awaitTermination(self, timeout=None):",
          "",
          "[Added Lines]",
          "204:     def sparkContext(self) -> SparkContext:",
          "210:     def start(self) -> None:",
          "217:     def awaitTermination(self, timeout: Optional[int] = None) -> None:",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "205:         else:",
          "206:             self._jssc.awaitTerminationOrTimeout(int(timeout * 1000))",
          "209:         \"\"\"",
          "210:         Wait for the execution to stop. Return `true` if it's stopped; or",
          "211:         throw the reported error during the execution; or `false` if the",
          "",
          "[Removed Lines]",
          "208:     def awaitTerminationOrTimeout(self, timeout):",
          "",
          "[Added Lines]",
          "231:     def awaitTerminationOrTimeout(self, timeout: int) -> None:",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "218:         \"\"\"",
          "219:         return self._jssc.awaitTerminationOrTimeout(int(timeout * 1000))",
          "222:         \"\"\"",
          "223:         Stop the execution of the streams, with option of ensuring all",
          "224:         received data has been processed.",
          "",
          "[Removed Lines]",
          "221:     def stop(self, stopSparkContext=True, stopGraceFully=False):",
          "",
          "[Added Lines]",
          "244:     def stop(self, stopSparkContext: bool = True, stopGraceFully: bool = False) -> None:",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "236:         if stopSparkContext:",
          "237:             self._sc.stop()",
          "240:         \"\"\"",
          "241:         Set each DStreams in this context to remember RDDs it generated",
          "242:         in the last given duration. DStreams remember RDDs only for a",
          "",
          "[Removed Lines]",
          "239:     def remember(self, duration):",
          "",
          "[Added Lines]",
          "262:     def remember(self, duration: int) -> None:",
          "",
          "---------------",
          "--- Hunk 11 ---",
          "[Context before]",
          "252:         \"\"\"",
          "253:         self._jssc.remember(self._jduration(duration))",
          "256:         \"\"\"",
          "257:         Sets the context to periodically checkpoint the DStream operations for master",
          "258:         fault-tolerance. The graph will be checkpointed every batch interval.",
          "",
          "[Removed Lines]",
          "255:     def checkpoint(self, directory):",
          "",
          "[Added Lines]",
          "278:     def checkpoint(self, directory: str) -> None:",
          "",
          "---------------",
          "--- Hunk 12 ---",
          "[Context before]",
          "264:         \"\"\"",
          "265:         self._jssc.checkpoint(directory)",
          "268:         \"\"\"",
          "269:         Create an input from TCP source hostname:port. Data is received using",
          "270:         a TCP socket and receive byte is interpreted as UTF8 encoded ``\\\\n`` delimited",
          "",
          "[Removed Lines]",
          "267:     def socketTextStream(self, hostname, port, storageLevel=StorageLevel.MEMORY_AND_DISK_2):",
          "",
          "[Added Lines]",
          "290:     def socketTextStream(",
          "291:         self, hostname: str, port: int, storageLevel: StorageLevel = StorageLevel.MEMORY_AND_DISK_2",
          "292:     ) -> \"DStream[str]\":",
          "",
          "---------------",
          "--- Hunk 13 ---",
          "[Context before]",
          "284:             self._jssc.socketTextStream(hostname, port, jlevel), self, UTF8Deserializer()",
          "285:         )",
          "288:         \"\"\"",
          "289:         Create an input stream that monitors a Hadoop-compatible file system",
          "290:         for new files and reads them as text files. Files must be written to the",
          "",
          "[Removed Lines]",
          "287:     def textFileStream(self, directory):",
          "",
          "[Added Lines]",
          "312:     def textFileStream(self, directory: str) -> \"DStream[str]\":",
          "",
          "---------------",
          "--- Hunk 14 ---",
          "[Context before]",
          "294:         \"\"\"",
          "295:         return DStream(self._jssc.textFileStream(directory), self, UTF8Deserializer())",
          "298:         \"\"\"",
          "299:         Create an input stream that monitors a Hadoop-compatible file system",
          "300:         for new files and reads them as flat binary files with records of",
          "",
          "[Removed Lines]",
          "297:     def binaryRecordsStream(self, directory, recordLength):",
          "",
          "[Added Lines]",
          "322:     def binaryRecordsStream(self, directory: str, recordLength: int) -> \"DStream[bytes]\":",
          "",
          "---------------",
          "--- Hunk 15 ---",
          "[Context before]",
          "313:             self._jssc.binaryRecordsStream(directory, recordLength), self, NoOpSerializer()",
          "314:         )",
          "317:         # make sure they have same serializer",
          "318:         if len(set(rdd._jrdd_deserializer for rdd in rdds)) > 1:",
          "319:             for i in range(len(rdds)):",
          "320:                 # reset them to sc.serializer",
          "321:                 rdds[i] = rdds[i]._reserialize()",
          "324:         \"\"\"",
          "325:         Create an input stream from a queue of RDDs or list. In each batch,",
          "326:         it will process either one or all of the RDDs returned by the queue.",
          "",
          "[Removed Lines]",
          "316:     def _check_serializers(self, rdds):",
          "323:     def queueStream(self, rdds, oneAtATime=True, default=None):",
          "",
          "[Added Lines]",
          "341:     def _check_serializers(self, rdds: List[RDD[T]]) -> None:",
          "348:     def queueStream(",
          "349:         self,",
          "350:         rdds: List[RDD[T]],",
          "351:         oneAtATime: bool = True,",
          "352:         default: Optional[RDD[T]] = None,",
          "353:     ) -> \"DStream[T]\":",
          "",
          "---------------",
          "--- Hunk 16 ---",
          "[Context before]",
          "339:         Changes to the queue after the stream is created will not be recognized.",
          "340:         \"\"\"",
          "341:         if default and not isinstance(default, RDD):",
          "344:         if not rdds and default:",
          "347:         if rdds and not isinstance(rdds[0], RDD):",
          "349:         self._check_serializers(rdds)",
          "351:         queue = self._jvm.PythonDStream.toRDDQueue([r._jrdd for r in rdds])",
          "352:         if default:",
          "353:             default = default._reserialize(rdds[0]._jrdd_deserializer)",
          "354:             jdstream = self._jssc.queueStream(queue, oneAtATime, default._jrdd)",
          "355:         else:",
          "356:             jdstream = self._jssc.queueStream(queue, oneAtATime)",
          "357:         return DStream(jdstream, self, rdds[0]._jrdd_deserializer)",
          "360:         \"\"\"",
          "361:         Create a new DStream in which each RDD is generated by applying",
          "362:         a function on RDDs of the DStreams. The order of the JavaRDDs in",
          "363:         the transform function parameter will be the same as the order",
          "364:         of corresponding DStreams in the list.",
          "365:         \"\"\"",
          "367:         # change the final serializer to sc.serializer",
          "368:         func = TransformFunction(",
          "369:             self._sc,",
          "370:             lambda t, *rdds: transformFunc(rdds),",
          "372:         )",
          "373:         jfunc = self._jvm.TransformFunction(func)",
          "374:         jdstream = self._jssc.transform(jdstreams, jfunc)",
          "375:         return DStream(jdstream, self, self._sc.serializer)",
          "378:         \"\"\"",
          "379:         Create a unified DStream from multiple DStreams of the same",
          "380:         type and same slide duration.",
          "",
          "[Removed Lines]",
          "342:             default = self._sc.parallelize(default)",
          "345:             rdds = [rdds]",
          "348:             rdds = [self._sc.parallelize(input) for input in rdds]",
          "359:     def transform(self, dstreams, transformFunc):",
          "366:         jdstreams = [d._jdstream for d in dstreams]",
          "377:     def union(self, *dstreams):",
          "",
          "[Added Lines]",
          "372:             default = self._sc.parallelize(default)  # type: ignore[arg-type]",
          "375:             rdds = [rdds]  # type: ignore[list-item]",
          "378:             rdds = [self._sc.parallelize(input) for input in rdds]  # type: ignore[arg-type]",
          "381:         assert self._jvm is not None",
          "385:             assert default is not None",
          "391:     def transform(",
          "392:         self, dstreams: List[\"DStream[Any]\"], transformFunc: Callable[..., RDD[T]]",
          "393:     ) -> \"DStream[T]\":",
          "400:         jdstreams = [d._jdstream for d in dstreams]  # type: ignore[attr-defined]",
          "408:         assert self._jvm is not None",
          "413:     def union(self, *dstreams: \"DStream[T]\") -> \"DStream[T]\":",
          "",
          "---------------",
          "--- Hunk 17 ---",
          "[Context before]",
          "383:             raise ValueError(\"should have at least one DStream to union\")",
          "384:         if len(dstreams) == 1:",
          "385:             return dstreams[0]",
          "387:             raise ValueError(\"All DStreams should have same serializer\")",
          "389:             raise ValueError(\"All DStreams should have same slide duration\")",
          "390:         jdstream_cls = SparkContext._jvm.org.apache.spark.streaming.api.java.JavaDStream",
          "391:         jpair_dstream_cls = SparkContext._jvm.org.apache.spark.streaming.api.java.JavaPairDStream",
          "392:         gw = SparkContext._gateway",
          "394:             cls = jdstream_cls",
          "396:             cls = jpair_dstream_cls",
          "397:         else:",
          "399:             raise TypeError(\"Unsupported Java DStream class %s\" % cls_name)",
          "400:         jdstreams = gw.new_array(cls, len(dstreams))",
          "401:         for i in range(0, len(dstreams)):",
          "406:         \"\"\"",
          "407:         Add a [[org.apache.spark.streaming.scheduler.StreamingListener]] object for",
          "408:         receiving system events related to streaming.",
          "409:         \"\"\"",
          "410:         self._jssc.addStreamingListener(",
          "411:             self._jvm.JavaStreamingListenerWrapper(",
          "412:                 self._jvm.PythonStreamingListenerWrapper(streamingListener)",
          "",
          "[Removed Lines]",
          "386:         if len(set(s._jrdd_deserializer for s in dstreams)) > 1:",
          "388:         if len(set(s._slideDuration for s in dstreams)) > 1:",
          "393:         if is_instance_of(gw, dstreams[0]._jdstream, jdstream_cls):",
          "395:         elif is_instance_of(gw, dstreams[0]._jdstream, jpair_dstream_cls):",
          "398:             cls_name = dstreams[0]._jdstream.getClass().getCanonicalName()",
          "402:             jdstreams[i] = dstreams[i]._jdstream",
          "403:         return DStream(self._jssc.union(jdstreams), self, dstreams[0]._jrdd_deserializer)",
          "405:     def addStreamingListener(self, streamingListener):",
          "",
          "[Added Lines]",
          "422:         if len(set(s._jrdd_deserializer for s in dstreams)) > 1:  # type: ignore[attr-defined]",
          "424:         if len(set(s._slideDuration for s in dstreams)) > 1:  # type: ignore[attr-defined]",
          "427:         assert SparkContext._jvm is not None",
          "431:         if is_instance_of(gw, dstreams[0]._jdstream, jdstream_cls):  # type: ignore[attr-defined]",
          "433:         elif is_instance_of(",
          "434:             gw, dstreams[0]._jdstream, jpair_dstream_cls  # type: ignore[attr-defined]",
          "435:         ):",
          "438:             cls_name = (",
          "439:                 dstreams[0]._jdstream.getClass().getCanonicalName()  # type: ignore[attr-defined]",
          "440:             )",
          "443:         assert gw is not None",
          "446:             jdstreams[i] = dstreams[i]._jdstream  # type: ignore[attr-defined]",
          "447:         return DStream(",
          "448:             self._jssc.union(jdstreams),",
          "449:             self,",
          "450:             dstreams[0]._jrdd_deserializer,  # type: ignore[attr-defined]",
          "451:         )",
          "453:     def addStreamingListener(self, streamingListener: StreamingListener) -> None:",
          "458:         assert self._jvm is not None",
          "",
          "---------------"
        ],
        "python/pyspark/streaming/context.pyi||python/pyspark/streaming/context.pyi": [
          "File: python/pyspark/streaming/context.pyi -> python/pyspark/streaming/context.pyi",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "python/pyspark/streaming/kinesis.py||python/pyspark/streaming/kinesis.py": [
          "File: python/pyspark/streaming/kinesis.py -> python/pyspark/streaming/kinesis.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "153:         The given AWS credentials will get saved in DStream checkpoints if checkpointing",
          "154:         is enabled. Make sure that your checkpoint directory is secure.",
          "155:         \"\"\"",
          "161:         try:",
          "162:             helper = jvm.org.apache.spark.streaming.kinesis.KinesisUtilsPythonHelper()",
          "",
          "[Removed Lines]",
          "156:         jlevel = ssc._sc._getJavaStorageLevel(storageLevel)  # type: ignore[attr-defined]",
          "157:         jduration = ssc._jduration(checkpointInterval)  # type: ignore[attr-defined]",
          "159:         jvm = ssc._jvm  # type: ignore[attr-defined]",
          "",
          "[Added Lines]",
          "156:         jlevel = ssc._sc._getJavaStorageLevel(storageLevel)",
          "157:         jduration = ssc._jduration(checkpointInterval)",
          "159:         jvm = ssc._jvm",
          "160:         assert jvm is not None",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "170:                 )",
          "171:             raise",
          "172:         jstream = helper.createStream(",
          "174:             kinesisAppName,",
          "175:             streamName,",
          "176:             endpointUrl,",
          "",
          "[Removed Lines]",
          "173:             ssc._jssc,  # type: ignore[attr-defined]",
          "",
          "[Added Lines]",
          "174:             ssc._jssc,",
          "",
          "---------------"
        ]
      }
    }
  ]
}