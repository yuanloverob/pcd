{
  "cve_id": "CVE-2023-50944",
  "cve_desc": "Apache Airflow, versions before 2.8.1, have a vulnerability that allows an authenticated user to access the source code of a DAG to which they don't have access.\u00a0This vulnerability is considered low since it requires an authenticated user to exploit it. Users are recommended to upgrade to version 2.8.1, which fixes this issue.",
  "repo": "apache/airflow",
  "patch_hash": "8d76538d6e105947272b000581c6fabec20146b1",
  "patch_info": {
    "commit_hash": "8d76538d6e105947272b000581c6fabec20146b1",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/8d76538d6e105947272b000581c6fabec20146b1",
    "files": [
      "airflow/api_connexion/endpoints/dag_source_endpoint.py",
      "airflow/models/dagcode.py",
      "tests/api_connexion/endpoints/test_dag_source_endpoint.py"
    ],
    "message": "Check DAG read permission before accessing DAG code (#36257)\n\n(cherry picked from commit 30ea37e0d247ce54c2d25b115e807fdb0074d795)",
    "before_after_code_files": [
      "airflow/api_connexion/endpoints/dag_source_endpoint.py||airflow/api_connexion/endpoints/dag_source_endpoint.py",
      "airflow/models/dagcode.py||airflow/models/dagcode.py",
      "tests/api_connexion/endpoints/test_dag_source_endpoint.py||tests/api_connexion/endpoints/test_dag_source_endpoint.py"
    ]
  },
  "patch_diff": {
    "airflow/api_connexion/endpoints/dag_source_endpoint.py||airflow/api_connexion/endpoints/dag_source_endpoint.py": [
      "File: airflow/api_connexion/endpoints/dag_source_endpoint.py -> airflow/api_connexion/endpoints/dag_source_endpoint.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "17: from __future__ import annotations",
      "19: from http import HTTPStatus",
      "21: from flask import Response, current_app, request",
      "22: from itsdangerous import BadSignature, URLSafeSerializer",
      "24: from airflow.api_connexion import security",
      "26: from airflow.api_connexion.schemas.dag_source_schema import dag_source_schema",
      "27: from airflow.auth.managers.models.resource_details import DagAccessEntity",
      "28: from airflow.models.dagcode import DagCode",
      "31: @security.requires_access_dag(\"GET\", DagAccessEntity.CODE)",
      "33:     \"\"\"Get source code using file token.\"\"\"",
      "34:     secret_key = current_app.config[\"SECRET_KEY\"]",
      "35:     auth_s = URLSafeSerializer(secret_key)",
      "36:     try:",
      "37:         path = auth_s.loads(file_token)",
      "39:     except (BadSignature, FileNotFoundError):",
      "40:         raise NotFound(\"Dag source not found\")",
      "",
      "[Removed Lines]",
      "25: from airflow.api_connexion.exceptions import NotFound",
      "32: def get_dag_source(*, file_token: str) -> Response:",
      "38:         dag_source = DagCode.code(path)",
      "",
      "[Added Lines]",
      "20: from typing import TYPE_CHECKING",
      "26: from airflow.api_connexion.exceptions import NotFound, PermissionDenied",
      "28: from airflow.api_connexion.security import get_readable_dags",
      "30: from airflow.models.dag import DagModel",
      "32: from airflow.utils.session import NEW_SESSION, provide_session",
      "34: if TYPE_CHECKING:",
      "35:     from sqlalchemy.orm import Session",
      "39: @provide_session",
      "40: def get_dag_source(*, file_token: str, session: Session = NEW_SESSION) -> Response:",
      "46:         dag_ids = session.query(DagModel.dag_id).filter(DagModel.fileloc == path).all()",
      "47:         readable_dags = get_readable_dags()",
      "48:         # Check if user has read access to all the DAGs defined in the file",
      "49:         if any(dag_id[0] not in readable_dags for dag_id in dag_ids):",
      "50:             raise PermissionDenied()",
      "51:         dag_source = DagCode.code(path, session=session)",
      "",
      "---------------"
    ],
    "airflow/models/dagcode.py||airflow/models/dagcode.py": [
      "File: airflow/models/dagcode.py -> airflow/models/dagcode.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "177:         return cls.code(fileloc)",
      "179:     @classmethod",
      "181:         \"\"\"Return source code for this DagCode object.",
      "183:         :return: source code as string",
      "184:         \"\"\"",
      "187:     @staticmethod",
      "188:     def _get_code_from_file(fileloc):",
      "",
      "[Removed Lines]",
      "180:     def code(cls, fileloc) -> str:",
      "185:         return cls._get_code_from_db(fileloc)",
      "",
      "[Added Lines]",
      "180:     @provide_session",
      "181:     def code(cls, fileloc, session: Session = NEW_SESSION) -> str:",
      "186:         return cls._get_code_from_db(fileloc, session)",
      "",
      "---------------"
    ],
    "tests/api_connexion/endpoints/test_dag_source_endpoint.py||tests/api_connexion/endpoints/test_dag_source_endpoint.py": [
      "File: tests/api_connexion/endpoints/test_dag_source_endpoint.py -> tests/api_connexion/endpoints/test_dag_source_endpoint.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "35: ROOT_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), os.pardir, os.pardir))",
      "36: EXAMPLE_DAG_FILE = os.path.join(\"airflow\", \"example_dags\", \"example_bash_operator.py\")",
      "39: @pytest.fixture(scope=\"module\")",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "37: EXAMPLE_DAG_ID = \"example_bash_operator\"",
      "38: TEST_DAG_ID = \"latest_only\"",
      "39: NOT_READABLE_DAG_ID = \"latest_only_with_trigger\"",
      "40: TEST_MULTIPLE_DAGS_ID = \"dataset_produces_1\"",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "45:         role_name=\"Test\",",
      "46:         permissions=[(permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG_CODE)],  # type: ignore",
      "47:     )",
      "48:     create_user(app, username=\"test_no_permissions\", role_name=\"TestNoPermissions\")  # type: ignore",
      "50:     yield app",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "52:     app.appbuilder.sm.sync_perm_for_dag(  # type: ignore",
      "53:         TEST_DAG_ID,",
      "54:         access_control={\"Test\": [permissions.ACTION_CAN_READ]},",
      "55:     )",
      "56:     app.appbuilder.sm.sync_perm_for_dag(  # type: ignore",
      "57:         EXAMPLE_DAG_ID,",
      "58:         access_control={\"Test\": [permissions.ACTION_CAN_READ]},",
      "59:     )",
      "60:     app.appbuilder.sm.sync_perm_for_dag(  # type: ignore",
      "61:         TEST_MULTIPLE_DAGS_ID,",
      "62:         access_control={\"Test\": [permissions.ACTION_CAN_READ]},",
      "63:     )",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "80:     def test_should_respond_200_text(self, url_safe_serializer):",
      "81:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
      "82:         dagbag.sync_to_db()",
      "87:         response = self.client.get(",
      "88:             url, headers={\"Accept\": \"text/plain\"}, environ_overrides={\"REMOTE_USER\": \"test\"}",
      "89:         )",
      "",
      "[Removed Lines]",
      "83:         first_dag: DAG = next(iter(dagbag.dags.values()))",
      "84:         dag_docstring = self._get_dag_file_docstring(first_dag.fileloc)",
      "86:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(first_dag.fileloc)}\"",
      "",
      "[Added Lines]",
      "99:         test_dag: DAG = dagbag.dags[TEST_DAG_ID]",
      "100:         dag_docstring = self._get_dag_file_docstring(test_dag.fileloc)",
      "102:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(test_dag.fileloc)}\"",
      "",
      "---------------",
      "--- Hunk 4 ---",
      "[Context before]",
      "95:     def test_should_respond_200_json(self, url_safe_serializer):",
      "96:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
      "97:         dagbag.sync_to_db()",
      "102:         response = self.client.get(",
      "103:             url, headers={\"Accept\": \"application/json\"}, environ_overrides={\"REMOTE_USER\": \"test\"}",
      "104:         )",
      "",
      "[Removed Lines]",
      "98:         first_dag: DAG = next(iter(dagbag.dags.values()))",
      "99:         dag_docstring = self._get_dag_file_docstring(first_dag.fileloc)",
      "101:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(first_dag.fileloc)}\"",
      "",
      "[Added Lines]",
      "114:         test_dag: DAG = dagbag.dags[TEST_DAG_ID]",
      "115:         dag_docstring = self._get_dag_file_docstring(test_dag.fileloc)",
      "117:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(test_dag.fileloc)}\"",
      "",
      "---------------",
      "--- Hunk 5 ---",
      "[Context before]",
      "110:     def test_should_respond_406(self, url_safe_serializer):",
      "111:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
      "112:         dagbag.sync_to_db()",
      "116:         response = self.client.get(",
      "117:             url, headers={\"Accept\": \"image/webp\"}, environ_overrides={\"REMOTE_USER\": \"test\"}",
      "118:         )",
      "",
      "[Removed Lines]",
      "113:         first_dag: DAG = next(iter(dagbag.dags.values()))",
      "115:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(first_dag.fileloc)}\"",
      "",
      "[Added Lines]",
      "129:         test_dag: DAG = dagbag.dags[TEST_DAG_ID]",
      "131:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(test_dag.fileloc)}\"",
      "",
      "---------------",
      "--- Hunk 6 ---",
      "[Context before]",
      "151:             environ_overrides={\"REMOTE_USER\": \"test_no_permissions\"},",
      "152:         )",
      "153:         assert response.status_code == 403",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "171:     def test_should_respond_403_not_readable(self, url_safe_serializer):",
      "172:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
      "173:         dagbag.sync_to_db()",
      "174:         dag: DAG = dagbag.dags[NOT_READABLE_DAG_ID]",
      "176:         response = self.client.get(",
      "177:             f\"/api/v1/dagSources/{url_safe_serializer.dumps(dag.fileloc)}\",",
      "178:             headers={\"Accept\": \"text/plain\"},",
      "179:             environ_overrides={\"REMOTE_USER\": \"test\"},",
      "180:         )",
      "181:         read_dag = self.client.get(",
      "182:             f\"/api/v1/dags/{NOT_READABLE_DAG_ID}\",",
      "183:             environ_overrides={\"REMOTE_USER\": \"test\"},",
      "184:         )",
      "185:         assert response.status_code == 403",
      "186:         assert read_dag.status_code == 403",
      "188:     def test_should_respond_403_some_dags_not_readable_in_the_file(self, url_safe_serializer):",
      "189:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
      "190:         dagbag.sync_to_db()",
      "191:         dag: DAG = dagbag.dags[TEST_MULTIPLE_DAGS_ID]",
      "193:         response = self.client.get(",
      "194:             f\"/api/v1/dagSources/{url_safe_serializer.dumps(dag.fileloc)}\",",
      "195:             headers={\"Accept\": \"text/plain\"},",
      "196:             environ_overrides={\"REMOTE_USER\": \"test\"},",
      "197:         )",
      "199:         read_dag = self.client.get(",
      "200:             f\"/api/v1/dags/{TEST_MULTIPLE_DAGS_ID}\",",
      "201:             environ_overrides={\"REMOTE_USER\": \"test\"},",
      "202:         )",
      "203:         assert response.status_code == 403",
      "204:         assert read_dag.status_code == 200",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "8c41a8f14d381e7b0f1068d2d2503408b97fc707",
      "candidate_info": {
        "commit_hash": "8c41a8f14d381e7b0f1068d2d2503408b97fc707",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/8c41a8f14d381e7b0f1068d2d2503408b97fc707",
        "files": [
          "airflow/migrations/versions/0126_2_7_0_add_index_to_task_instance_table.py",
          "airflow/migrations/versions/0133_2_8_1_refactor_dag_run_indexes.py",
          "airflow/models/taskinstance.py",
          "airflow/utils/db.py",
          "docs/apache-airflow/img/airflow_erd.sha256",
          "docs/apache-airflow/img/airflow_erd.svg",
          "docs/apache-airflow/migrations-ref.rst",
          "scripts/in_container/run_mypy.sh"
        ],
        "message": "Remove usused index on task instance (#36737)\n\nIndex is only helpful for a user's custom query -- not for airflow in general (see comment https://github.com/apache/airflow/pull/30762#issuecomment-1886658295).  Noticed that this query had zero scans over a period of months.  I also observed that it also takes up as much space as the table itself.  Since it's not generally useful, it doesn't belong in airflow OSS.\n\nReverts #30762\n\n(cherry picked from commit e20b400317ae4eb41181c5b0cee466eff768b521)",
        "before_after_code_files": [
          "airflow/migrations/versions/0126_2_7_0_add_index_to_task_instance_table.py||airflow/migrations/versions/0126_2_7_0_add_index_to_task_instance_table.py",
          "airflow/migrations/versions/0133_2_8_1_refactor_dag_run_indexes.py||airflow/migrations/versions/0133_2_8_1_refactor_dag_run_indexes.py",
          "airflow/models/taskinstance.py||airflow/models/taskinstance.py",
          "airflow/utils/db.py||airflow/utils/db.py",
          "scripts/in_container/run_mypy.sh||scripts/in_container/run_mypy.sh"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/migrations/versions/0126_2_7_0_add_index_to_task_instance_table.py||airflow/migrations/versions/0126_2_7_0_add_index_to_task_instance_table.py": [
          "File: airflow/migrations/versions/0126_2_7_0_add_index_to_task_instance_table.py -> airflow/migrations/versions/0126_2_7_0_add_index_to_task_instance_table.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "38: def upgrade():",
          "39:     \"\"\"Apply Add index to task_instance table\"\"\"",
          "48: def downgrade():",
          "49:     \"\"\"Unapply Add index to task_instance table\"\"\"",
          "",
          "[Removed Lines]",
          "40:     op.create_index(",
          "41:         \"ti_state_incl_start_date\",",
          "42:         \"task_instance\",",
          "43:         [\"dag_id\", \"task_id\", \"state\"],",
          "44:         postgresql_include=[\"start_date\"],",
          "45:     )",
          "50:     op.drop_index(\"ti_state_incl_start_date\", table_name=\"task_instance\")",
          "",
          "[Added Lines]",
          "40:     # We don't add this index anymore because it's not useful.",
          "41:     pass",
          "46:     # At 2.8.1 we removed this index as it is not used, and changed this migration not to add it",
          "47:     # So we use drop if exists (cus it might not be there)",
          "48:     import sqlalchemy",
          "49:     from contextlib import suppress",
          "51:     with suppress(sqlalchemy.exc.DatabaseError):  # mysql does not support drop if exists index",
          "52:         op.drop_index(\"ti_state_incl_start_date\", table_name=\"task_instance\", if_exists=True)",
          "",
          "---------------"
        ],
        "airflow/migrations/versions/0133_2_8_1_refactor_dag_run_indexes.py||airflow/migrations/versions/0133_2_8_1_refactor_dag_run_indexes.py": [
          "File: airflow/migrations/versions/0133_2_8_1_refactor_dag_run_indexes.py -> airflow/migrations/versions/0133_2_8_1_refactor_dag_run_indexes.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: #",
          "2: # Licensed to the Apache Software Foundation (ASF) under one",
          "3: # or more contributor license agreements.  See the NOTICE file",
          "4: # distributed with this work for additional information",
          "5: # regarding copyright ownership.  The ASF licenses this file",
          "6: # to you under the Apache License, Version 2.0 (the",
          "7: # \"License\"); you may not use this file except in compliance",
          "8: # with the License.  You may obtain a copy of the License at",
          "9: #",
          "10: #   http://www.apache.org/licenses/LICENSE-2.0",
          "11: #",
          "12: # Unless required by applicable law or agreed to in writing,",
          "13: # software distributed under the License is distributed on an",
          "14: # \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
          "15: # KIND, either express or implied.  See the License for the",
          "16: # specific language governing permissions and limitations",
          "17: # under the License.",
          "19: \"\"\"Drop unused TI index",
          "21: Revision ID: 88344c1d9134",
          "22: Revises: 10b52ebd31f7",
          "23: Create Date: 2024-01-11 11:54:48.232030",
          "25: \"\"\"",
          "27: import sqlalchemy as sa",
          "28: from alembic import op",
          "31: # revision identifiers, used by Alembic.",
          "32: revision = \"88344c1d9134\"",
          "33: down_revision = \"10b52ebd31f7\"",
          "34: branch_labels = None",
          "35: depends_on = None",
          "36: airflow_version = \"2.8.1\"",
          "39: def upgrade():",
          "40:     \"\"\"Apply refactor dag run indexes\"\"\"",
          "41:     # This index may have been created in 2.7 but we've since removed it from migrations",
          "42:     import sqlalchemy",
          "43:     from contextlib import suppress",
          "45:     with suppress(sqlalchemy.exc.DatabaseError):  # mysql does not support drop if exists index",
          "46:         op.drop_index(\"ti_state_incl_start_date\", table_name=\"task_instance\", if_exists=True)",
          "49: def downgrade():",
          "50:     \"\"\"Unapply refactor dag run indexes\"\"\"",
          "",
          "---------------"
        ],
        "airflow/models/taskinstance.py||airflow/models/taskinstance.py": [
          "File: airflow/models/taskinstance.py -> airflow/models/taskinstance.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1256:         # Existing \"ti_state_lkp\" is not enough for such query when this table has millions of rows, since",
          "1257:         # rows have to be fetched in order to retrieve the start_date column. With this index, INDEX ONLY SCAN",
          "1258:         # is performed and that query runs within milliseconds.",
          "1260:         Index(\"ti_pool\", pool, state, priority_weight),",
          "1261:         Index(\"ti_job_id\", job_id),",
          "1262:         Index(\"ti_trigger_id\", trigger_id),",
          "",
          "[Removed Lines]",
          "1259:         Index(\"ti_state_incl_start_date\", dag_id, task_id, state, postgresql_include=[\"start_date\"]),",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "airflow/utils/db.py||airflow/utils/db.py": [
          "File: airflow/utils/db.py -> airflow/utils/db.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "89:     \"2.6.2\": \"c804e5c76e3e\",",
          "90:     \"2.7.0\": \"405de8318b3a\",",
          "91:     \"2.8.0\": \"10b52ebd31f7\",",
          "92: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "92:     \"2.8.1\": \"88344c1d9134\",",
          "",
          "---------------"
        ],
        "scripts/in_container/run_mypy.sh||scripts/in_container/run_mypy.sh": [
          "File: scripts/in_container/run_mypy.sh -> scripts/in_container/run_mypy.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "23: ADDITIONAL_MYPY_OPTIONS=()",
          "25: if [[ ${SUSPENDED_PROVIDERS_FOLDERS=} != \"\" ]];",
          "26: then",
          "27:     for folder in ${SUSPENDED_PROVIDERS_FOLDERS=}",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "25: export MYPY_FORCE_COLOR=true",
          "26: export TERM=ansi",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "ae0ceb382dc0dfb91adc5d22b480c321ffbe3fe0",
      "candidate_info": {
        "commit_hash": "ae0ceb382dc0dfb91adc5d22b480c321ffbe3fe0",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/ae0ceb382dc0dfb91adc5d22b480c321ffbe3fe0",
        "files": [
          ".pre-commit-config.yaml",
          ".rat-excludes",
          "CONTRIBUTING.rst",
          "docs/apache-airflow-providers-fab/img/diagram_fab_auth_manager_airflow_architecture.png",
          "docs/apache-airflow/img/diagram_auth_manager_airflow_architecture.md5sum",
          "docs/apache-airflow/img/diagram_auth_manager_airflow_architecture.png",
          "docs/apache-airflow/img/diagram_auth_manager_airflow_architecture.py",
          "docs/apache-airflow/img/diagram_basic_airflow_architecture.md5sum",
          "docs/apache-airflow/img/diagram_basic_airflow_architecture.py",
          "docs/apache-airflow/img/diagram_dag_processor_airflow_architecture.md5sum",
          "docs/apache-airflow/img/diagram_dag_processor_airflow_architecture.py",
          "docs/apache-airflow/img/diagram_fab_auth_manager_airflow_architecture.md5sum",
          "docs/apache-airflow/img/diagram_fab_auth_manager_airflow_architecture.png",
          "docs/apache-airflow/img/diagram_fab_auth_manager_airflow_architecture.py",
          "docs/diagrams/python_multiprocess_logo.png",
          "images/diagrams/python_multiprocess_logo.png",
          "scripts/ci/pre_commit/pre_commit_generate_airflow_diagrams.py"
        ],
        "message": "Improve pre-commit to generate Airflow diagrams as a code (#36333)\n\nSince we are getting more diagrams generated in Airflow using the\n\"diagram as a code\" approach, this PR improves the pre-commit to be\nmore suitable to support generation of more of the images coming\nfrom different sources, placed in different directories and generated\nindependently, so that the whole process is more distributed and easy\nfor whoever creates diagrams to add their own diagram.\n\nThe changes implemented in this PR:\n\n* the code to generate the diagrams is now next to the diagram they\n  generate. It has the same name as the diagram, but it has the .py\n  extension. This way it is immediately visible where is the source\n  of each diagram (right next to each diagram)\n\n* each of the .py diagram Python files is runnable on its own. This\n  way you can easily regenerate the diagrams by running corresponding\n  Python file or even automate it by running \"save\" action and generate\n  the diagrams automatically by running the Python code every time\n  the file is saved. That makes a very nice workflow on iterating on\n  each diagram, independently from each othere\n\n* the pre-commit script is given a set of folders which should be\n  scanned and it finds and run the diagrams on pre-commmit. It also\n  creates and verifies the md5sum hash of the source Python file\n  separately for each diagram and only runs diagram generation when\n  the source file changed vs. last time the hash was saved and\n  committed. The hash sum is stored next to the image and sources\n  with .md5sum extension\n\nAlso updated documentation in the CONTRIBUTING.rst explaining how\nto generate the diagrams and what is the mechanism of that\ngeneration.\n\n(cherry picked from commit b35b08ec41814b6fe5d7388296db83a726e6d6d0)",
        "before_after_code_files": [
          "scripts/ci/pre_commit/pre_commit_generate_airflow_diagrams.py||scripts/ci/pre_commit/pre_commit_generate_airflow_diagrams.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "scripts/ci/pre_commit/pre_commit_generate_airflow_diagrams.py||scripts/ci/pre_commit/pre_commit_generate_airflow_diagrams.py": [
          "File: scripts/ci/pre_commit/pre_commit_generate_airflow_diagrams.py -> scripts/ci/pre_commit/pre_commit_generate_airflow_diagrams.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: from __future__ import annotations",
          "20: import hashlib",
          "22: from pathlib import Path",
          "29: from rich.console import Console",
          "31: console = Console(width=400, color_system=\"standard\")",
          "33: LOCAL_DIR = Path(__file__).parent",
          "34: AIRFLOW_SOURCES_ROOT = Path(__file__).parents[3]",
          "124: def main():",
          "138: if __name__ == \"__main__\":",
          "",
          "[Removed Lines]",
          "21: import os",
          "24: from diagrams import Cluster, Diagram, Edge",
          "25: from diagrams.custom import Custom",
          "26: from diagrams.onprem.client import User",
          "27: from diagrams.onprem.database import PostgreSQL",
          "28: from diagrams.programming.flowchart import MultipleDocuments",
          "35: DOCS_IMAGES_DIR = AIRFLOW_SOURCES_ROOT / \"docs\" / \"apache-airflow\" / \"img\"",
          "36: PYTHON_MULTIPROCESS_LOGO = AIRFLOW_SOURCES_ROOT / \"images\" / \"diagrams\" / \"python_multiprocess_logo.png\"",
          "38: BASIC_ARCHITECTURE_IMAGE_NAME = \"diagram_basic_airflow_architecture\"",
          "39: DAG_PROCESSOR_AIRFLOW_ARCHITECTURE_IMAGE_NAME = \"diagram_dag_processor_airflow_architecture\"",
          "40: DIAGRAM_HASH_FILE_NAME = \"diagram_hash.txt\"",
          "43: def generate_basic_airflow_diagram(filename: str):",
          "44:     basic_architecture_image_file = (DOCS_IMAGES_DIR / BASIC_ARCHITECTURE_IMAGE_NAME).with_suffix(\".png\")",
          "45:     console.print(f\"[bright_blue]Generating architecture image {basic_architecture_image_file}\")",
          "46:     with Diagram(name=\"\", show=False, direction=\"LR\", curvestyle=\"ortho\", filename=filename):",
          "47:         with Cluster(\"Parsing & Scheduling\"):",
          "48:             schedulers = Custom(\"Scheduler(s)\", PYTHON_MULTIPROCESS_LOGO.as_posix())",
          "50:         metadata_db = PostgreSQL(\"Metadata DB\")",
          "52:         dag_author = User(\"DAG Author\")",
          "53:         dag_files = MultipleDocuments(\"DAG files\")",
          "55:         dag_author >> Edge(color=\"black\", style=\"dashed\", reverse=False) >> dag_files",
          "57:         with Cluster(\"Execution\"):",
          "58:             workers = Custom(\"Worker(s)\", PYTHON_MULTIPROCESS_LOGO.as_posix())",
          "59:             triggerer = Custom(\"Triggerer(s)\", PYTHON_MULTIPROCESS_LOGO.as_posix())",
          "61:         schedulers - Edge(color=\"blue\", style=\"dashed\", taillabel=\"Executor\") - workers",
          "63:         schedulers >> Edge(color=\"red\", style=\"dotted\", reverse=True) >> metadata_db",
          "64:         workers >> Edge(color=\"red\", style=\"dotted\", reverse=True) >> metadata_db",
          "65:         triggerer >> Edge(color=\"red\", style=\"dotted\", reverse=True) >> metadata_db",
          "67:         operations_user = User(\"Operations User\")",
          "68:         with Cluster(\"UI\"):",
          "69:             webservers = Custom(\"Webserver(s)\", PYTHON_MULTIPROCESS_LOGO.as_posix())",
          "71:         webservers >> Edge(color=\"black\", style=\"dashed\", reverse=True) >> operations_user",
          "73:         metadata_db >> Edge(color=\"red\", style=\"dotted\", reverse=True) >> webservers",
          "75:         dag_files >> Edge(color=\"brown\", style=\"solid\") >> workers",
          "76:         dag_files >> Edge(color=\"brown\", style=\"solid\") >> schedulers",
          "77:         dag_files >> Edge(color=\"brown\", style=\"solid\") >> triggerer",
          "78:     console.print(f\"[green]Generating architecture image {basic_architecture_image_file}\")",
          "81: def generate_dag_processor_airflow_diagram(filename: str):",
          "82:     dag_processor_architecture_image_file = (",
          "83:         DOCS_IMAGES_DIR / DAG_PROCESSOR_AIRFLOW_ARCHITECTURE_IMAGE_NAME",
          "84:     ).with_suffix(\".png\")",
          "85:     console.print(f\"[bright_blue]Generating architecture image {dag_processor_architecture_image_file}\")",
          "86:     with Diagram(name=\"\", show=False, direction=\"LR\", curvestyle=\"ortho\", filename=filename):",
          "87:         operations_user = User(\"Operations User\")",
          "88:         with Cluster(\"No DAG Python Code Execution\", graph_attr={\"bgcolor\": \"lightgrey\"}):",
          "89:             with Cluster(\"Scheduling\"):",
          "90:                 schedulers = Custom(\"Scheduler(s)\", PYTHON_MULTIPROCESS_LOGO.as_posix())",
          "92:             with Cluster(\"UI\"):",
          "93:                 webservers = Custom(\"Webserver(s)\", PYTHON_MULTIPROCESS_LOGO.as_posix())",
          "95:         webservers >> Edge(color=\"black\", style=\"dashed\", reverse=True) >> operations_user",
          "97:         metadata_db = PostgreSQL(\"Metadata DB\")",
          "99:         dag_author = User(\"DAG Author\")",
          "100:         with Cluster(\"DAG Python Code Execution\"):",
          "101:             with Cluster(\"Execution\"):",
          "102:                 workers = Custom(\"Worker(s)\", PYTHON_MULTIPROCESS_LOGO.as_posix())",
          "103:                 triggerer = Custom(\"Triggerer(s)\", PYTHON_MULTIPROCESS_LOGO.as_posix())",
          "104:             with Cluster(\"Parsing\"):",
          "105:                 dag_processors = Custom(\"DAG\\nProcessor(s)\", PYTHON_MULTIPROCESS_LOGO.as_posix())",
          "106:             dag_files = MultipleDocuments(\"DAG files\")",
          "108:         dag_author >> Edge(color=\"black\", style=\"dashed\", reverse=False) >> dag_files",
          "110:         workers - Edge(color=\"blue\", style=\"dashed\", headlabel=\"Executor\") - schedulers",
          "112:         metadata_db >> Edge(color=\"red\", style=\"dotted\", reverse=True) >> webservers",
          "113:         metadata_db >> Edge(color=\"red\", style=\"dotted\", reverse=True) >> schedulers",
          "114:         dag_processors >> Edge(color=\"red\", style=\"dotted\", reverse=True) >> metadata_db",
          "115:         workers >> Edge(color=\"red\", style=\"dotted\", reverse=True) >> metadata_db",
          "116:         triggerer >> Edge(color=\"red\", style=\"dotted\", reverse=True) >> metadata_db",
          "118:         dag_files >> Edge(color=\"brown\", style=\"solid\") >> workers",
          "119:         dag_files >> Edge(color=\"brown\", style=\"solid\") >> dag_processors",
          "120:         dag_files >> Edge(color=\"brown\", style=\"solid\") >> triggerer",
          "121:     console.print(f\"[green]Generating architecture image {dag_processor_architecture_image_file}\")",
          "125:     hash_md5 = hashlib.md5()",
          "126:     hash_md5.update(Path(__file__).resolve().read_bytes())",
          "127:     my_file_hash = hash_md5.hexdigest()",
          "128:     hash_file = LOCAL_DIR / DIAGRAM_HASH_FILE_NAME",
          "129:     if not hash_file.exists() or not hash_file.read_text().strip() == str(my_file_hash).strip():",
          "130:         os.chdir(DOCS_IMAGES_DIR)",
          "131:         generate_basic_airflow_diagram(BASIC_ARCHITECTURE_IMAGE_NAME)",
          "132:         generate_dag_processor_airflow_diagram(DAG_PROCESSOR_AIRFLOW_ARCHITECTURE_IMAGE_NAME)",
          "133:         hash_file.write_text(str(my_file_hash) + \"\\n\")",
          "134:     else:",
          "135:         console.print(\"[bright_blue]No changes to generation script. Not regenerating the images.\")",
          "",
          "[Added Lines]",
          "21: import subprocess",
          "22: import sys",
          "33: def _get_file_hash(file_to_check: Path) -> str:",
          "34:     hash_md5 = hashlib.md5()",
          "35:     hash_md5.update(Path(file_to_check).resolve().read_bytes())",
          "36:     return hash_md5.hexdigest()",
          "40:     # get all files as arguments",
          "41:     for arg in sys.argv[1:]:",
          "42:         source_file = Path(arg).resolve()",
          "43:         checksum = _get_file_hash(source_file)",
          "44:         hash_file = source_file.with_suffix(\".md5sum\")",
          "45:         if not hash_file.exists() or not hash_file.read_text().strip() == str(checksum).strip():",
          "46:             console.print(f\"[bright_blue]Changes in {source_file}. Regenerating the image.\")",
          "47:             subprocess.run(",
          "48:                 [sys.executable, source_file.resolve().as_posix()], check=True, cwd=source_file.parent",
          "49:             )",
          "50:             hash_file.write_text(str(checksum) + \"\\n\")",
          "51:         else:",
          "52:             console.print(f\"[bright_blue]No changes in {source_file}. Not regenerating the image.\")",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "bfce9a4f20168001c1faec679e0baf9b051076dd",
      "candidate_info": {
        "commit_hash": "bfce9a4f20168001c1faec679e0baf9b051076dd",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/bfce9a4f20168001c1faec679e0baf9b051076dd",
        "files": [
          "Dockerfile",
          "dev/breeze/src/airflow_breeze/global_constants.py",
          "docs/docker-stack/build-arg-ref.rst",
          "images/breeze/output_prod-image_build.svg",
          "images/breeze/output_prod-image_build.txt"
        ],
        "message": "Remove common.io from chicken-egg providers. (#36284)\n\nNow that Airflow 2.8.0 is released, we can remove common.io from\nchicken-egg providers.\n\n(cherry picked from commit 34d500158769d1d197911a2cd4ac5818bcd117d2)",
        "before_after_code_files": [
          "dev/breeze/src/airflow_breeze/global_constants.py||dev/breeze/src/airflow_breeze/global_constants.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/breeze/src/airflow_breeze/global_constants.py||dev/breeze/src/airflow_breeze/global_constants.py": [
          "File: dev/breeze/src/airflow_breeze/global_constants.py -> dev/breeze/src/airflow_breeze/global_constants.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "430:     \"async\",",
          "431:     \"celery\",",
          "432:     \"cncf.kubernetes\",",
          "433:     \"docker\",",
          "434:     \"elasticsearch\",",
          "435:     \"ftp\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "433:     \"common.io\",",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "456:     # END OF EXTRAS LIST UPDATED BY PRE COMMIT",
          "457: ]",
          "466: def _exclusion(providers: Iterable[str]) -> str:",
          "",
          "[Removed Lines]",
          "459: CHICKEN_EGG_PROVIDERS = \" \".join(",
          "460:     [",
          "461:         \"common.io\",",
          "462:     ]",
          "463: )",
          "",
          "[Added Lines]",
          "460: CHICKEN_EGG_PROVIDERS = \" \".join([])",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "cbab5e4df34df123460dbf805790602a65c2173b",
      "candidate_info": {
        "commit_hash": "cbab5e4df34df123460dbf805790602a65c2173b",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/cbab5e4df34df123460dbf805790602a65c2173b",
        "files": [
          "airflow/www/templates/airflow/dag_details.html"
        ],
        "message": "rename concurrency label to max active tasks (#36691)\n\n(cherry picked from commit 9cbfed474c10891c1429cd538a2bf6c8d014096d)",
        "before_after_code_files": [
          "airflow/www/templates/airflow/dag_details.html||airflow/www/templates/airflow/dag_details.html"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/www/templates/airflow/dag_details.html||airflow/www/templates/airflow/dag_details.html": [
          "File: airflow/www/templates/airflow/dag_details.html -> airflow/www/templates/airflow/dag_details.html",
          "--- Hunk 1 ---",
          "[Context before]",
          "72:       <td>{{ active_runs | length }} / {{ dag.max_active_runs }}</td>",
          "73:     </tr>",
          "74:     <tr>",
          "76:       <td>{{ dag.max_active_tasks }}</td>",
          "77:     </tr>",
          "78:     <tr>",
          "",
          "[Removed Lines]",
          "75:       <th>Concurrency</th>",
          "",
          "[Added Lines]",
          "75:       <th>Max Active Tasks</th>",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "ee12a6d7b67e6027d02217ae1530826311ca2545",
      "candidate_info": {
        "commit_hash": "ee12a6d7b67e6027d02217ae1530826311ca2545",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/ee12a6d7b67e6027d02217ae1530826311ca2545",
        "files": [
          "airflow/cli/commands/scheduler_command.py",
          "tests/cli/commands/test_scheduler_command.py"
        ],
        "message": "Fix airflow-scheduler exiting with code 0 on exceptions (#36800)\n\n* Fix airflow-scheduler exiting with code 0 on exceptions\n\n* Fix static check\n\n(cherry picked from commit 1d5d5022b8fc92f23f9fdc3b61269e5c7acfaf39)",
        "before_after_code_files": [
          "airflow/cli/commands/scheduler_command.py||airflow/cli/commands/scheduler_command.py",
          "tests/cli/commands/test_scheduler_command.py||tests/cli/commands/test_scheduler_command.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/cli/commands/scheduler_command.py||airflow/cli/commands/scheduler_command.py": [
          "File: airflow/cli/commands/scheduler_command.py -> airflow/cli/commands/scheduler_command.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: import logging",
          "21: from argparse import Namespace",
          "23: from multiprocessing import Process",
          "25: from airflow import settings",
          "",
          "[Removed Lines]",
          "22: from contextlib import contextmanager",
          "",
          "[Added Lines]",
          "22: from contextlib import ExitStack, contextmanager",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "44:     ExecutorLoader.validate_database_executor_compatibility(job_runner.job.executor)",
          "45:     InternalApiConfig.force_database_direct_access()",
          "46:     enable_health_check = conf.getboolean(\"scheduler\", \"ENABLE_HEALTH_CHECK\")",
          "48:         try:",
          "49:             run_job(job=job_runner.job, execute_callable=job_runner._execute)",
          "50:         except Exception:",
          "51:             log.exception(\"Exception when running scheduler job\")",
          "54: @cli_utils.action_cli",
          "",
          "[Removed Lines]",
          "47:     with _serve_logs(args.skip_serve_logs), _serve_health_check(enable_health_check):",
          "",
          "[Added Lines]",
          "47:     with ExitStack() as stack:",
          "48:         stack.enter_context(_serve_logs(args.skip_serve_logs))",
          "49:         stack.enter_context(_serve_health_check(enable_health_check))",
          "55:             raise",
          "56:         finally:",
          "57:             # Ensure that the contexts are closed",
          "58:             stack.close()",
          "",
          "---------------"
        ],
        "tests/cli/commands/test_scheduler_command.py||tests/cli/commands/test_scheduler_command.py": [
          "File: tests/cli/commands/test_scheduler_command.py -> tests/cli/commands/test_scheduler_command.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "169:         mock_scheduler_job,",
          "170:     ):",
          "171:         args = self.parser.parse_args([\"scheduler\"])",
          "174:         # Make sure that run_job is called, that the exception has been logged, and that the serve_logs",
          "175:         # sub-process has been terminated",
          "",
          "[Removed Lines]",
          "172:         scheduler_command.scheduler(args)",
          "",
          "[Added Lines]",
          "172:         with pytest.raises(Exception, match=\"run_job failed\"):",
          "173:             scheduler_command.scheduler(args)",
          "",
          "---------------"
        ]
      }
    }
  ]
}