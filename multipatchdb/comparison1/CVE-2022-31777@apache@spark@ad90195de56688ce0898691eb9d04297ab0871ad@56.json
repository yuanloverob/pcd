{
  "cve_id": "CVE-2022-31777",
  "cve_desc": "A stored cross-site scripting (XSS) vulnerability in Apache Spark 3.2.1 and earlier, and 3.3.0, allows remote attackers to execute arbitrary JavaScript in the web browser of a user, by including a malicious payload into the logs which would be returned in logs rendered in the UI.",
  "repo": "apache/spark",
  "patch_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
  "patch_info": {
    "commit_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
    "repo": "apache/spark",
    "commit_url": "https://github.com/apache/spark/commit/ad90195de56688ce0898691eb9d04297ab0871ad",
    "files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ],
    "message": "[SPARK-39505][UI] Escape log content rendered in UI\n\n### What changes were proposed in this pull request?\n\nEscape log content rendered to the UI.\n\n### Why are the changes needed?\n\nLog content may contain reserved characters or other code in the log and be misinterpreted in the UI as HTML.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nExisting tests\n\nCloses #36902 from srowen/LogViewEscape.\n\nAuthored-by: Sean Owen <srowen@gmail.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
    "before_after_code_files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ]
  },
  "patch_diff": {
    "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js": [
      "File: core/src/main/resources/org/apache/spark/ui/static/log-view.js -> core/src/main/resources/org/apache/spark/ui/static/log-view.js",
      "--- Hunk 1 ---",
      "[Context before]",
      "85:       if (retStartByte == 0) {",
      "86:         disableMoreButton();",
      "87:       }",
      "90:       curLogLength = curLogLength + (startByte - retStartByte);",
      "91:       startByte = retStartByte;",
      "",
      "[Removed Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(cleanData);",
      "",
      "[Added Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(document.createTextNode(cleanData));",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "115:             var retLogLength = dataInfo[2];",
      "117:             var cleanData = data.substring(newlineIndex + 1);",
      "120:             curLogLength = curLogLength + (retEndByte - retStartByte);",
      "121:             endByte = retEndByte;",
      "",
      "[Removed Lines]",
      "118:             $(\"pre\", \".log-content\").append(cleanData);",
      "",
      "[Added Lines]",
      "118:             $(\"pre\", \".log-content\").append(document.createTextNode(cleanData));",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "1853eb117e24bcc0509d275c4caca6c033bf0ab9",
      "candidate_info": {
        "commit_hash": "1853eb117e24bcc0509d275c4caca6c033bf0ab9",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/1853eb117e24bcc0509d275c4caca6c033bf0ab9",
        "files": [
          "core/src/main/scala/org/apache/spark/SparkException.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/util/ArrowUtils.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/util/ArrowUtilsSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionErrorsSuite.scala"
        ],
        "message": "[SPARK-39187][SQL][3.3] Remove `SparkIllegalStateException`\n\n### What changes were proposed in this pull request?\nRemove `SparkIllegalStateException` and replace it by `IllegalStateException` where it was used.\n\nThis is a backport of https://github.com/apache/spark/pull/36550.\n\n### Why are the changes needed?\nTo improve code maintenance and be consistent to other places where `IllegalStateException` is used in illegal states (for instance, see https://github.com/apache/spark/pull/36524). After the PR https://github.com/apache/spark/pull/36500, the exception is substituted by `SparkException` w/ the `INTERNAL_ERROR` error class.\n\n### Does this PR introduce _any_ user-facing change?\nNo. Users shouldn't face to the exception in regular cases.\n\n### How was this patch tested?\nBy running the affected test suites:\n```\n$ build/sbt \"sql/test:testOnly *QueryExecutionErrorsSuite*\"\n$ build/sbt \"test:testOnly *ArrowUtilsSuite\"\n```\n\nAuthored-by: Max Gekk <max.gekkgmail.com>\nSigned-off-by: Max Gekk <max.gekkgmail.com>\n(cherry picked from commit 1a90512f605c490255f7b38215c207e64621475b)\nSigned-off-by: Max Gekk <max.gekkgmail.com>\n\nCloses #36558 from MaxGekk/remove-SparkIllegalStateException-3.3.\n\nAuthored-by: Max Gekk <max.gekk@gmail.com>\nSigned-off-by: Max Gekk <max.gekk@gmail.com>",
        "before_after_code_files": [
          "core/src/main/scala/org/apache/spark/SparkException.scala||core/src/main/scala/org/apache/spark/SparkException.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/util/ArrowUtils.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/util/ArrowUtils.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/util/ArrowUtilsSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/util/ArrowUtilsSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionErrorsSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionErrorsSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "core/src/main/scala/org/apache/spark/SparkException.scala||core/src/main/scala/org/apache/spark/SparkException.scala": [
          "File: core/src/main/scala/org/apache/spark/SparkException.scala -> core/src/main/scala/org/apache/spark/SparkException.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "158:   override def getErrorClass: String = errorClass",
          "159: }",
          "",
          "[Removed Lines]",
          "164: private[spark] class SparkIllegalStateException(",
          "165:     errorClass: String,",
          "166:     messageParameters: Array[String])",
          "167:   extends IllegalStateException(",
          "168:     SparkThrowableHelper.getMessage(errorClass, messageParameters)) with SparkThrowable {",
          "170:   override def getErrorClass: String = errorClass",
          "171: }",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "28: import org.apache.spark.sql.catalyst.trees.TreeNodeTag",
          "29: import org.apache.spark.sql.catalyst.util.{CharVarcharUtils, StringUtils, TypeUtils}",
          "30: import org.apache.spark.sql.connector.catalog.{LookupCatalog, SupportsPartitionManagement}",
          "32: import org.apache.spark.sql.internal.SQLConf",
          "33: import org.apache.spark.sql.types._",
          "34: import org.apache.spark.sql.util.SchemaUtils",
          "",
          "[Removed Lines]",
          "31: import org.apache.spark.sql.errors.{QueryCompilationErrors, QueryExecutionErrors}",
          "",
          "[Added Lines]",
          "31: import org.apache.spark.sql.errors.QueryCompilationErrors",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "571:                  |in operator ${operator.simpleString(SQLConf.get.maxToStringFields)}",
          "572:                \"\"\".stripMargin)",
          "577:           case f @ Filter(condition, _)",
          "578:             if PlanHelper.specialExpressionsInUnsupportedOperator(f).nonEmpty =>",
          "",
          "[Removed Lines]",
          "574:           case _: UnresolvedHint =>",
          "575:             throw QueryExecutionErrors.logicalHintOperatorNotRemovedDuringAnalysisError",
          "",
          "[Added Lines]",
          "574:           case _: UnresolvedHint => throw new IllegalStateException(",
          "575:             \"Logical hint operator should be removed during analysis.\")",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "34: import org.codehaus.commons.compiler.CompileException",
          "35: import org.codehaus.janino.InternalCompilerException",
          "38: import org.apache.spark.executor.CommitDeniedException",
          "39: import org.apache.spark.launcher.SparkLauncher",
          "40: import org.apache.spark.memory.SparkOutOfMemoryError",
          "",
          "[Removed Lines]",
          "37: import org.apache.spark.{Partition, SparkArithmeticException, SparkArrayIndexOutOfBoundsException, SparkClassNotFoundException, SparkConcurrentModificationException, SparkDateTimeException, SparkException, SparkFileAlreadyExistsException, SparkFileNotFoundException, SparkIllegalArgumentException, SparkIllegalStateException, SparkIndexOutOfBoundsException, SparkNoSuchElementException, SparkNoSuchMethodException, SparkNumberFormatException, SparkRuntimeException, SparkSecurityException, SparkSQLException, SparkSQLFeatureNotSupportedException, SparkUnsupportedOperationException, SparkUpgradeException}",
          "",
          "[Added Lines]",
          "37: import org.apache.spark.{Partition, SparkArithmeticException, SparkArrayIndexOutOfBoundsException, SparkClassNotFoundException, SparkConcurrentModificationException, SparkDateTimeException, SparkException, SparkFileAlreadyExistsException, SparkFileNotFoundException, SparkIllegalArgumentException, SparkIndexOutOfBoundsException, SparkNoSuchElementException, SparkNoSuchMethodException, SparkNumberFormatException, SparkRuntimeException, SparkSecurityException, SparkSQLException, SparkSQLFeatureNotSupportedException, SparkUnsupportedOperationException, SparkUpgradeException}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "69: object QueryExecutionErrors extends QueryErrorsBase {",
          "77:   def cannotEvaluateExpressionError(expression: Expression): Throwable = {",
          "78:     new SparkUnsupportedOperationException(errorClass = \"INTERNAL_ERROR\",",
          "79:       messageParameters = Array(s\"Cannot evaluate expression: $expression\"))",
          "",
          "[Removed Lines]",
          "71:   def logicalHintOperatorNotRemovedDuringAnalysisError(): Throwable = {",
          "72:     new SparkIllegalStateException(errorClass = \"INTERNAL_ERROR\",",
          "73:       messageParameters = Array(",
          "74:         \"Internal error: logical hint operator should have been removed during analysis\"))",
          "75:   }",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "137:   }",
          "139:   def cannotParseDecimalError(): Throwable = {",
          "141:       messageParameters = Array.empty)",
          "142:   }",
          "",
          "[Removed Lines]",
          "140:     new SparkIllegalStateException(errorClass = \"CANNOT_PARSE_DECIMAL\",",
          "",
          "[Added Lines]",
          "134:     new SparkRuntimeException(",
          "135:       errorClass = \"CANNOT_PARSE_DECIMAL\",",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/util/ArrowUtils.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/util/ArrowUtils.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/util/ArrowUtils.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/util/ArrowUtils.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "47:     case BinaryType => ArrowType.Binary.INSTANCE",
          "48:     case DecimalType.Fixed(precision, scale) => new ArrowType.Decimal(precision, scale)",
          "49:     case DateType => new ArrowType.Date(DateUnit.DAY)",
          "56:     case TimestampNTZType =>",
          "57:       new ArrowType.Timestamp(TimeUnit.MICROSECOND, null)",
          "58:     case NullType => ArrowType.Null.INSTANCE",
          "",
          "[Removed Lines]",
          "50:     case TimestampType =>",
          "51:       if (timeZoneId == null) {",
          "52:         throw QueryExecutionErrors.timeZoneIdNotSpecifiedForTimestampTypeError()",
          "53:       } else {",
          "54:         new ArrowType.Timestamp(TimeUnit.MICROSECOND, timeZoneId)",
          "55:       }",
          "",
          "[Added Lines]",
          "50:     case TimestampType if timeZoneId == null =>",
          "51:       throw new IllegalStateException(\"Missing timezoneId where it is mandatory.\")",
          "52:     case TimestampType => new ArrowType.Timestamp(TimeUnit.MICROSECOND, timeZoneId)",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/util/ArrowUtilsSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/util/ArrowUtilsSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/util/ArrowUtilsSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/util/ArrowUtilsSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "50:     roundtrip(DateType)",
          "51:     roundtrip(YearMonthIntervalType())",
          "52:     roundtrip(DayTimeIntervalType())",
          "54:       roundtrip(TimestampType)",
          "55:     }",
          "57:   }",
          "59:   test(\"timestamp\") {",
          "",
          "[Removed Lines]",
          "53:     val tsExMsg = intercept[UnsupportedOperationException] {",
          "56:     assert(tsExMsg.getMessage.contains(\"timeZoneId\"))",
          "",
          "[Added Lines]",
          "53:     val tsExMsg = intercept[IllegalStateException] {",
          "56:     assert(tsExMsg.getMessage.contains(\"timezoneId\"))",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionErrorsSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionErrorsSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionErrorsSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionErrorsSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "25: import org.apache.spark.sql.internal.SQLConf",
          "26: import org.apache.spark.sql.internal.SQLConf.LegacyBehaviorPolicy.EXCEPTION",
          "27: import org.apache.spark.sql.test.SharedSparkSession",
          "31: class QueryExecutionErrorsSuite extends QueryTest",
          "32:   with ParquetTest with OrcTest with SharedSparkSession {",
          "",
          "[Removed Lines]",
          "28: import org.apache.spark.sql.types.{StructType, TimestampType}",
          "29: import org.apache.spark.sql.util.ArrowUtils",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "228:     }",
          "229:   }",
          "243:   test(\"UNSUPPORTED_OPERATION - SPARK-36346: can't read Timestamp as TimestampNTZ\") {",
          "244:     withTempPath { file =>",
          "245:       sql(\"select timestamp_ltz'2019-03-21 00:02:03'\").write.orc(file.getCanonicalPath)",
          "",
          "[Removed Lines]",
          "231:   test(\"UNSUPPORTED_OPERATION: timeZoneId not specified while converting TimestampType to Arrow\") {",
          "232:     val schema = new StructType().add(\"value\", TimestampType)",
          "233:     val e = intercept[SparkUnsupportedOperationException] {",
          "234:       ArrowUtils.toArrowSchema(schema, null)",
          "235:     }",
          "237:     assert(e.getErrorClass === \"UNSUPPORTED_OPERATION\")",
          "238:     assert(e.getMessage === \"The operation is not supported: \" +",
          "239:       \"\\\"TIMESTAMP\\\" must supply timeZoneId parameter \" +",
          "240:       \"while converting to the arrow timestamp type.\")",
          "241:   }",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "ea0571e001e6ce4ac415f20142c39eedc18250e1",
      "candidate_info": {
        "commit_hash": "ea0571e001e6ce4ac415f20142c39eedc18250e1",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/ea0571e001e6ce4ac415f20142c39eedc18250e1",
        "files": [
          "sql/core/src/main/scala/org/apache/spark/sql/catalyst/util/V2ExpressionBuilder.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCV2Suite.scala"
        ],
        "message": "[SPARK-38997][SPARK-39037][SQL][FOLLOWUP] PushableColumnWithoutNestedColumn` need be translated to predicate too\n\n### What changes were proposed in this pull request?\nhttps://github.com/apache/spark/pull/35768 assume the expression in `And`, `Or` and `Not` must be predicate.\nhttps://github.com/apache/spark/pull/36370 and https://github.com/apache/spark/pull/36325 supported push down expressions in `GROUP BY` and `ORDER BY`. But the children of `And`, `Or` and `Not` can be `FieldReference.column(name)`.\n`FieldReference.column(name)` is not a predicate, so the assert may fail.\n\n### Why are the changes needed?\nThis PR fix the bug for `PushableColumnWithoutNestedColumn`.\n\n### Does this PR introduce _any_ user-facing change?\n'Yes'.\nLet the push-down framework more correctly.\n\n### How was this patch tested?\nNew tests\n\nCloses #36776 from beliefer/SPARK-38997_SPARK-39037_followup.\n\nAuthored-by: Jiaan Geng <beliefer@163.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\n(cherry picked from commit 125555cf2c1388b28fcc34beae09f971c5fadcb7)\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>",
        "before_after_code_files": [
          "sql/core/src/main/scala/org/apache/spark/sql/catalyst/util/V2ExpressionBuilder.scala||sql/core/src/main/scala/org/apache/spark/sql/catalyst/util/V2ExpressionBuilder.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCV2Suite.scala||sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCV2Suite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/core/src/main/scala/org/apache/spark/sql/catalyst/util/V2ExpressionBuilder.scala||sql/core/src/main/scala/org/apache/spark/sql/catalyst/util/V2ExpressionBuilder.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/catalyst/util/V2ExpressionBuilder.scala -> sql/core/src/main/scala/org/apache/spark/sql/catalyst/util/V2ExpressionBuilder.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "49:     case Literal(true, BooleanType) => Some(new AlwaysTrue())",
          "50:     case Literal(false, BooleanType) => Some(new AlwaysFalse())",
          "51:     case Literal(value, dataType) => Some(LiteralValue(value, dataType))",
          "53:       if (isPredicate && col.dataType.isInstanceOf[BooleanType]) {",
          "55:       } else {",
          "57:       }",
          "60:     case in @ InSet(child, hset) =>",
          "61:       generateExpression(child).map { v =>",
          "62:         val children =",
          "",
          "[Removed Lines]",
          "52:     case col @ pushableColumn(name) if nestedPredicatePushdownEnabled =>",
          "54:         Some(new V2Predicate(\"=\", Array(FieldReference(name), LiteralValue(true, BooleanType))))",
          "56:         Some(FieldReference(name))",
          "58:     case pushableColumn(name) if !nestedPredicatePushdownEnabled =>",
          "59:       Some(FieldReference.column(name))",
          "",
          "[Added Lines]",
          "52:     case col @ pushableColumn(name) =>",
          "53:       val ref = if (nestedPredicatePushdownEnabled) {",
          "54:         FieldReference(name)",
          "55:       } else {",
          "56:         FieldReference.column(name)",
          "57:       }",
          "59:         Some(new V2Predicate(\"=\", Array(ref, LiteralValue(true, BooleanType))))",
          "61:         Some(ref)",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCV2Suite.scala||sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCV2Suite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCV2Suite.scala -> sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCV2Suite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "851:         |[DEPT, CASE WHEN (SALARY > 8000.00) AND (SALARY < 10000.00) THEN SALARY ELSE 0.00 END],",
          "852:         |\"\"\".stripMargin.replaceAll(\"\\n\", \" \"))",
          "853:     checkAnswer(df5, Seq(Row(1, 0, 10000), Row(1, 9000, 9000), Row(2, 0, 22000), Row(6, 0, 12000)))",
          "854:   }",
          "856:   test(\"scan with aggregate push-down: DISTINCT SUM with group by\") {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "855:     val df6 = sql(",
          "856:       \"\"\"",
          "857:         |SELECT CASE WHEN SALARY > 8000 AND is_manager <> false THEN SALARY ELSE 0 END as key,",
          "858:         |  SUM(SALARY) FROM h2.test.employee GROUP BY key\"\"\".stripMargin)",
          "859:     checkAggregateRemoved(df6)",
          "860:     checkPushedInfo(df6,",
          "861:       \"\"\"",
          "862:         |PushedAggregates: [SUM(SALARY)],",
          "863:         |PushedFilters: [],",
          "864:         |PushedGroupByExpressions:",
          "865:         |[CASE WHEN (SALARY > 8000.00) AND (IS_MANAGER = true) THEN SALARY ELSE 0.00 END],",
          "866:         |\"\"\".stripMargin.replaceAll(\"\\n\", \" \"))",
          "867:     checkAnswer(df6, Seq(Row(0, 21000), Row(10000, 20000), Row(12000, 12000)))",
          "869:     val df7 = sql(",
          "870:       \"\"\"",
          "871:         |SELECT CASE WHEN SALARY > 8000 OR is_manager <> false THEN SALARY ELSE 0 END as key,",
          "872:         |  SUM(SALARY) FROM h2.test.employee GROUP BY key\"\"\".stripMargin)",
          "873:     checkAggregateRemoved(df7)",
          "874:     checkPushedInfo(df7,",
          "875:       \"\"\"",
          "876:         |PushedAggregates: [SUM(SALARY)],",
          "877:         |PushedFilters: [],",
          "878:         |PushedGroupByExpressions:",
          "879:         |[CASE WHEN (SALARY > 8000.00) OR (IS_MANAGER = true) THEN SALARY ELSE 0.00 END],",
          "880:         |\"\"\".stripMargin.replaceAll(\"\\n\", \" \"))",
          "881:     checkAnswer(df7, Seq(Row(10000, 20000), Row(12000, 24000), Row(9000, 9000)))",
          "883:     val df8 = sql(",
          "884:       \"\"\"",
          "885:         |SELECT CASE WHEN NOT(is_manager <> false) THEN SALARY ELSE 0 END as key,",
          "886:         |  SUM(SALARY) FROM h2.test.employee GROUP BY key\"\"\".stripMargin)",
          "887:     checkAggregateRemoved(df8)",
          "888:     checkPushedInfo(df8,",
          "889:       \"\"\"",
          "890:         |PushedAggregates: [SUM(SALARY)],",
          "891:         |PushedFilters: [],",
          "892:         |PushedGroupByExpressions:",
          "893:         |[CASE WHEN NOT (IS_MANAGER = true) THEN SALARY ELSE 0.00 END],",
          "894:         |\"\"\".stripMargin.replaceAll(\"\\n\", \" \"))",
          "895:     checkAnswer(df8, Seq(Row(0, 32000), Row(12000, 12000), Row(9000, 9000)))",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "7e2a1827757a8c0e356ab795387f094e81f5f37e",
      "candidate_info": {
        "commit_hash": "7e2a1827757a8c0e356ab795387f094e81f5f37e",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/7e2a1827757a8c0e356ab795387f094e81f5f37e",
        "files": [
          "resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/Client.scala"
        ],
        "message": "[SPARK-37205][FOLLOWUP] Should call non-static setTokensConf method\n\n### What changes were proposed in this pull request?\n\nThis fixes a bug in the original SPARK-37205 PR, where we treat the method `setTokensConf` as a static method, but it should be non-static instead.\n\n### Why are the changes needed?\n\nThe method `setTokensConf` is non-static so the current code will fail:\n```\n06/29/2022 - 17:28:16  - Exception in thread \"main\" java.lang.IllegalArgumentException: object is not an instance of declaring class\n06/29/2022 - 17:28:16  - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n06/29/2022 - 17:28:16  - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n06/29/2022 - 17:28:16  - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n06/29/2022 - 17:28:16  - at java.base/java.lang.reflect.Method.invoke(Method.java:566)\n```\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nManually tested this change internally and it now works.\n\nCloses #37037 from sunchao/SPARK-37205-fix.\n\nAuthored-by: Chao Sun <sunchao@apple.com>\nSigned-off-by: Chao Sun <sunchao@apple.com>",
        "before_after_code_files": [
          "resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/Client.scala||resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/Client.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/Client.scala||resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/Client.scala": [
          "File: resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/Client.scala -> resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/Client.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "391:           throw new SparkException(s\"Cannot find setTokensConf method in ${amContainer.getClass}.\" +",
          "392:               s\" Please check YARN version and make sure it is 2.9+ or 3.x\")",
          "393:       }",
          "395:     }",
          "396:   }",
          "",
          "[Removed Lines]",
          "394:       setTokensConfMethod.invoke(ByteBuffer.wrap(dob.getData))",
          "",
          "[Added Lines]",
          "394:       setTokensConfMethod.invoke(amContainer, ByteBuffer.wrap(dob.getData))",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "47c47b6e8641876f1336d3d5fbac01e47d931d43",
      "candidate_info": {
        "commit_hash": "47c47b6e8641876f1336d3d5fbac01e47d931d43",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/47c47b6e8641876f1336d3d5fbac01e47d931d43",
        "files": [
          "core/src/main/resources/error/error-classes.json",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/DateTimeUtils.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/UTF8StringUtils.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/types/Decimal.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/AnsiCastSuiteBase.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CastSuite.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/DateFormatterSuite.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/TimestampFormatterSuite.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/types/DecimalSuite.scala",
          "sql/core/src/test/resources/sql-tests/results/ansi/cast.sql.out",
          "sql/core/src/test/resources/sql-tests/results/ansi/date.sql.out",
          "sql/core/src/test/resources/sql-tests/results/ansi/datetime-parsing-invalid.sql.out",
          "sql/core/src/test/resources/sql-tests/results/ansi/interval.sql.out",
          "sql/core/src/test/resources/sql-tests/results/ansi/string-functions.sql.out",
          "sql/core/src/test/resources/sql-tests/results/postgreSQL/boolean.sql.out",
          "sql/core/src/test/resources/sql-tests/results/postgreSQL/float4.sql.out",
          "sql/core/src/test/resources/sql-tests/results/postgreSQL/float8.sql.out",
          "sql/core/src/test/resources/sql-tests/results/postgreSQL/int8.sql.out",
          "sql/core/src/test/resources/sql-tests/results/postgreSQL/text.sql.out",
          "sql/core/src/test/resources/sql-tests/results/postgreSQL/window_part2.sql.out",
          "sql/core/src/test/resources/sql-tests/results/postgreSQL/window_part3.sql.out",
          "sql/core/src/test/resources/sql-tests/results/postgreSQL/window_part4.sql.out",
          "sql/core/src/test/resources/sql-tests/results/timestampNTZ/timestamp-ansi.sql.out",
          "sql/core/src/test/scala/org/apache/spark/sql/SQLInsertTestSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/sources/InsertSuite.scala"
        ],
        "message": "[SPARK-39214][SQL][3.3] Improve errors related to CAST\n\n### What changes were proposed in this pull request?\nIn the PR, I propose to rename the error classes:\n1. INVALID_SYNTAX_FOR_CAST -> CAST_INVALID_INPUT\n2. CAST_CAUSES_OVERFLOW -> CAST_OVERFLOW\n\nand change error messages:\n\nCAST_INVALID_INPUT:\n`The value <value> of the type <sourceType> cannot be cast to <targetType> because it is malformed. ...`\n\nCAST_OVERFLOW:\n`The value <value> of the type <sourceType> cannot be cast to <targetType> due to an overflow....`\n\nAlso quote the SQL config `\"spark.sql.ansi.enabled\"` and a function name.\n\nThis is a backport of https://github.com/apache/spark/pull/36553.\n\n### Why are the changes needed?\nTo improve user experience with Spark SQL by making errors/error classes related to CAST more clear and **unified**.\n\n### Does this PR introduce _any_ user-facing change?\nYes, the PR changes user-facing error messages.\n\n### How was this patch tested?\nBy running the modified test suites:\n```\n$ build/sbt \"testOnly *CastSuite\"\n$ build/sbt \"test:testOnly *DateFormatterSuite\"\n$ build/sbt \"test:testOnly *TimestampFormatterSuite\"\n$ build/sbt \"testOnly *DSV2SQLInsertTestSuite\"\n$ build/sbt \"test:testOnly *InsertSuite\"\n$ build/sbt \"testOnly *AnsiCastSuiteWithAnsiModeOff\"\n```\n\nAuthored-by: Max Gekk <max.gekkgmail.com>\nSigned-off-by: Max Gekk <max.gekkgmail.com>\n(cherry picked from commit 66648e96e4283223003c80a1a997325bbd27f940)\nSigned-off-by: Max Gekk <max.gekkgmail.com>\n\nCloses #36591 from MaxGekk/error-class-improve-msg-2-3.3.\n\nAuthored-by: Max Gekk <max.gekk@gmail.com>\nSigned-off-by: Max Gekk <max.gekk@gmail.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/DateTimeUtils.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/DateTimeUtils.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/UTF8StringUtils.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/UTF8StringUtils.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/types/Decimal.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/types/Decimal.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/AnsiCastSuiteBase.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/AnsiCastSuiteBase.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CastSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CastSuite.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/DateFormatterSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/DateFormatterSuite.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/TimestampFormatterSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/TimestampFormatterSuite.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/types/DecimalSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/types/DecimalSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/SQLInsertTestSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/SQLInsertTestSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/sources/InsertSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/sources/InsertSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "854:           case _: NumberFormatException =>",
          "855:             val d = Cast.processFloatingPointSpecialLiterals(doubleStr, false)",
          "856:             if(ansiEnabled && d == null) {",
          "858:                 DoubleType, s, queryContext)",
          "859:             } else {",
          "860:               d",
          "",
          "[Removed Lines]",
          "857:               throw QueryExecutionErrors.invalidInputSyntaxForNumericError(",
          "",
          "[Added Lines]",
          "857:               throw QueryExecutionErrors.invalidInputInCastToNumberError(",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "880:           case _: NumberFormatException =>",
          "881:             val f = Cast.processFloatingPointSpecialLiterals(floatStr, true)",
          "882:             if (ansiEnabled && f == null) {",
          "884:                 FloatType, s, queryContext)",
          "885:             } else {",
          "886:               f",
          "",
          "[Removed Lines]",
          "883:               throw QueryExecutionErrors.invalidInputSyntaxForNumericError(",
          "",
          "[Added Lines]",
          "883:               throw QueryExecutionErrors.invalidInputInCastToNumberError(",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "1917:         (c, evPrim, evNull) =>",
          "1918:           val handleNull = if (ansiEnabled) {",
          "1919:             val errorContext = ctx.addReferenceObj(\"errCtx\", queryContext)",
          "1921:               s\"org.apache.spark.sql.types.FloatType$$.MODULE$$,$c, $errorContext);\"",
          "1922:           } else {",
          "1923:             s\"$evNull = true;\"",
          "",
          "[Removed Lines]",
          "1920:             s\"throw QueryExecutionErrors.invalidInputSyntaxForNumericError(\" +",
          "",
          "[Added Lines]",
          "1920:             s\"throw QueryExecutionErrors.invalidInputInCastToNumberError(\" +",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "1955:         (c, evPrim, evNull) =>",
          "1956:           val handleNull = if (ansiEnabled) {",
          "1957:             val errorContext = ctx.addReferenceObj(\"errCtx\", queryContext)",
          "1959:               s\"org.apache.spark.sql.types.DoubleType$$.MODULE$$, $c, $errorContext);\"",
          "1960:           } else {",
          "1961:             s\"$evNull = true;\"",
          "",
          "[Removed Lines]",
          "1958:             s\"throw QueryExecutionErrors.invalidInputSyntaxForNumericError(\" +",
          "",
          "[Added Lines]",
          "1958:             s\"throw QueryExecutionErrors.invalidInputInCastToNumberError(\" +",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/DateTimeUtils.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/DateTimeUtils.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/DateTimeUtils.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/DateTimeUtils.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "451:   def stringToTimestampAnsi(s: UTF8String, timeZoneId: ZoneId, errorContext: String = \"\"): Long = {",
          "452:     stringToTimestamp(s, timeZoneId).getOrElse {",
          "454:         s, StringType, TimestampType, errorContext)",
          "455:     }",
          "456:   }",
          "458:   def doubleToTimestampAnsi(d: Double, errorContext: String): Long = {",
          "459:     if (d.isNaN || d.isInfinite) {",
          "461:         d, DoubleType, TimestampType, errorContext)",
          "462:     } else {",
          "463:       DoubleExactNumeric.toLong(d * MICROS_PER_SECOND)",
          "",
          "[Removed Lines]",
          "453:       throw QueryExecutionErrors.cannotCastToDateTimeError(",
          "460:       throw QueryExecutionErrors.cannotCastToDateTimeError(",
          "",
          "[Added Lines]",
          "453:       throw QueryExecutionErrors.invalidInputInCastToDatetimeError(",
          "460:       throw QueryExecutionErrors.invalidInputInCastToDatetimeError(",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "508:   def stringToTimestampWithoutTimeZoneAnsi(s: UTF8String, errorContext: String): Long = {",
          "509:     stringToTimestampWithoutTimeZone(s, true).getOrElse {",
          "511:         s, StringType, TimestampNTZType, errorContext)",
          "512:     }",
          "513:   }",
          "",
          "[Removed Lines]",
          "510:       throw QueryExecutionErrors.cannotCastToDateTimeError(",
          "",
          "[Added Lines]",
          "510:       throw QueryExecutionErrors.invalidInputInCastToDatetimeError(",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "627:   def stringToDateAnsi(s: UTF8String, errorContext: String = \"\"): Int = {",
          "628:     stringToDate(s).getOrElse {",
          "630:         s, StringType, DateType, errorContext)",
          "631:     }",
          "632:   }",
          "",
          "[Removed Lines]",
          "629:       throw QueryExecutionErrors.cannotCastToDateTimeError(",
          "",
          "[Added Lines]",
          "629:       throw QueryExecutionErrors.invalidInputInCastToDatetimeError(",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/UTF8StringUtils.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/UTF8StringUtils.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/UTF8StringUtils.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/UTF8StringUtils.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "43:       f",
          "44:     } catch {",
          "45:       case e: NumberFormatException =>",
          "47:     }",
          "48:   }",
          "49: }",
          "",
          "[Removed Lines]",
          "46:         throw QueryExecutionErrors.invalidInputSyntaxForNumericError(to, s, errorContext)",
          "",
          "[Added Lines]",
          "46:         throw QueryExecutionErrors.invalidInputInCastToNumberError(to, s, errorContext)",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "84:   }",
          "86:   def castingCauseOverflowError(t: Any, from: DataType, to: DataType): ArithmeticException = {",
          "88:       messageParameters = Array(",
          "90:   }",
          "92:   def cannotChangeDecimalPrecisionError(",
          "",
          "[Removed Lines]",
          "87:     new SparkArithmeticException(errorClass = \"CAST_CAUSES_OVERFLOW\",",
          "89:         toSQLValue(t, from), toSQLType(to), toSQLConf(SQLConf.ANSI_ENABLED.key)))",
          "",
          "[Added Lines]",
          "87:     new SparkArithmeticException(",
          "88:       errorClass = \"CAST_OVERFLOW\",",
          "90:         toSQLValue(t, from),",
          "91:         toSQLType(from),",
          "92:         toSQLType(to),",
          "93:         toSQLConf(SQLConf.ANSI_ENABLED.key)))",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "104:         context))",
          "105:   }",
          "112:   }",
          "115:       to: DataType,",
          "116:       s: UTF8String,",
          "121:   }",
          "123:   def cannotCastFromNullTypeError(to: DataType): Throwable = {",
          "",
          "[Removed Lines]",
          "107:   def invalidInputSyntaxForNumericError(",
          "108:       e: NumberFormatException,",
          "109:       errorContext: String): NumberFormatException = {",
          "110:     new NumberFormatException(s\"${e.getMessage}. To return NULL instead, use 'try_cast'. \" +",
          "111:       s\"If necessary set ${SQLConf.ANSI_ENABLED.key} to false to bypass this error.\" + errorContext)",
          "114:   def invalidInputSyntaxForNumericError(",
          "117:       errorContext: String): NumberFormatException = {",
          "118:     new SparkNumberFormatException(errorClass = \"INVALID_SYNTAX_FOR_CAST\",",
          "119:       messageParameters = Array(toSQLType(to), toSQLValue(s, StringType),",
          "120:         SQLConf.ANSI_ENABLED.key, errorContext))",
          "",
          "[Added Lines]",
          "111:   def invalidInputInCastToDatetimeError(",
          "112:       value: Any,",
          "113:       from: DataType,",
          "114:       to: DataType,",
          "115:       errorContext: String): Throwable = {",
          "116:     new SparkDateTimeException(",
          "117:       errorClass = \"CAST_INVALID_INPUT\",",
          "118:       messageParameters = Array(",
          "119:         toSQLValue(value, from),",
          "120:         toSQLType(from),",
          "121:         toSQLType(to),",
          "122:         toSQLConf(SQLConf.ANSI_ENABLED.key),",
          "123:         errorContext))",
          "126:   def invalidInputSyntaxForBooleanError(",
          "127:       s: UTF8String,",
          "128:       errorContext: String): SparkRuntimeException = {",
          "129:     new SparkRuntimeException(",
          "130:       errorClass = \"CAST_INVALID_INPUT\",",
          "131:       messageParameters = Array(",
          "132:         toSQLValue(s, StringType),",
          "133:         toSQLType(StringType),",
          "134:         toSQLType(BooleanType),",
          "135:         toSQLConf(SQLConf.ANSI_ENABLED.key),",
          "136:         errorContext))",
          "137:   }",
          "139:   def invalidInputInCastToNumberError(",
          "142:       errorContext: String): SparkNumberFormatException = {",
          "143:     new SparkNumberFormatException(",
          "144:       errorClass = \"CAST_INVALID_INPUT\",",
          "145:       messageParameters = Array(",
          "146:         toSQLValue(s, StringType),",
          "147:         toSQLType(StringType),",
          "148:         toSQLType(to),",
          "149:         toSQLConf(SQLConf.ANSI_ENABLED.key),",
          "150:         errorContext))",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "1190:       \"SQLUserDefinedType nor registered with UDTRegistration.}\")",
          "1191:   }",
          "1201:   def unsupportedOperandTypeForSizeFunctionError(dataType: DataType): Throwable = {",
          "1202:     new UnsupportedOperationException(",
          "1203:       s\"The size function doesn't support the operand type ${dataType.getClass.getCanonicalName}\")",
          "",
          "[Removed Lines]",
          "1193:   def invalidInputSyntaxForBooleanError(",
          "1194:       s: UTF8String,",
          "1195:       errorContext: String): UnsupportedOperationException = {",
          "1196:     new UnsupportedOperationException(s\"invalid input syntax for type boolean: $s. \" +",
          "1197:       s\"To return NULL instead, use 'try_cast'. If necessary set ${SQLConf.ANSI_ENABLED.key} \" +",
          "1198:       \"to false to bypass this error.\" + errorContext)",
          "1199:   }",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/types/Decimal.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/types/Decimal.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/types/Decimal.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/types/Decimal.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "636:       }",
          "637:     } catch {",
          "638:       case _: NumberFormatException =>",
          "640:     }",
          "641:   }",
          "",
          "[Removed Lines]",
          "639:         throw QueryExecutionErrors.invalidInputSyntaxForNumericError(to, str, errorContext)",
          "",
          "[Added Lines]",
          "639:         throw QueryExecutionErrors.invalidInputInCastToNumberError(to, str, errorContext)",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/AnsiCastSuiteBase.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/AnsiCastSuiteBase.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/AnsiCastSuiteBase.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/AnsiCastSuiteBase.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: import java.sql.Timestamp",
          "21: import java.time.DateTimeException",
          "24: import org.apache.spark.sql.Row",
          "25: import org.apache.spark.sql.catalyst.InternalRow",
          "26: import org.apache.spark.sql.catalyst.util.DateTimeConstants.MILLIS_PER_SECOND",
          "27: import org.apache.spark.sql.catalyst.util.DateTimeTestUtils",
          "28: import org.apache.spark.sql.catalyst.util.DateTimeTestUtils.{withDefaultTimeZone, UTC}",
          "30: import org.apache.spark.sql.internal.SQLConf",
          "31: import org.apache.spark.sql.types._",
          "32: import org.apache.spark.unsafe.types.UTF8String",
          "",
          "[Removed Lines]",
          "23: import org.apache.spark.SparkArithmeticException",
          "29: import org.apache.spark.sql.errors.QueryExecutionErrors.toSQLValue",
          "",
          "[Added Lines]",
          "23: import org.apache.spark.{SparkArithmeticException, SparkRuntimeException}",
          "29: import org.apache.spark.sql.errors.QueryErrorsBase",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "44:   private def testIntMaxAndMin(dt: DataType): Unit = {",
          "45:     assert(Seq(IntegerType, ShortType, ByteType).contains(dt))",
          "",
          "[Removed Lines]",
          "42: abstract class AnsiCastSuiteBase extends CastSuiteBase {",
          "",
          "[Added Lines]",
          "42: abstract class AnsiCastSuiteBase extends CastSuiteBase with QueryErrorsBase {",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "172:     assert(cast(booleanLiteral, DateType).checkInputDataTypes().isFailure)",
          "173:   }",
          "175:   test(\"cast from invalid string to numeric should throw NumberFormatException\") {",
          "177:     Seq(IntegerType, ShortType, ByteType, LongType).foreach { dataType =>",
          "186:     }",
          "188:     Seq(DoubleType, FloatType, DecimalType.USER_DEFAULT).foreach { dataType =>",
          "195:     }",
          "196:   }",
          "198:   protected def checkCastToNumericError(l: Literal, to: DataType,",
          "199:       expectedDataTypeInErrorMsg: DataType, tryCastResult: Any): Unit = {",
          "200:     checkExceptionInExpression[NumberFormatException](",
          "202:   }",
          "204:   test(\"cast from invalid string array to numeric array should throw NumberFormatException\") {",
          "",
          "[Removed Lines]",
          "178:       checkExceptionInExpression[NumberFormatException](cast(\"string\", dataType),",
          "179:         s\"\"\"Invalid input syntax for type \"${dataType.sql}\": 'string'\"\"\")",
          "180:       checkExceptionInExpression[NumberFormatException](cast(\"123-string\", dataType),",
          "181:         s\"\"\"Invalid input syntax for type \"${dataType.sql}\": '123-string'\"\"\")",
          "182:       checkExceptionInExpression[NumberFormatException](cast(\"2020-07-19\", dataType),",
          "183:         s\"\"\"Invalid input syntax for type \"${dataType.sql}\": '2020-07-19'\"\"\")",
          "184:       checkExceptionInExpression[NumberFormatException](cast(\"1.23\", dataType),",
          "185:         s\"\"\"Invalid input syntax for type \"${dataType.sql}\": '1.23'\"\"\")",
          "189:       checkExceptionInExpression[NumberFormatException](cast(\"string\", dataType),",
          "190:         s\"\"\"Invalid input syntax for type \"${dataType.sql}\": 'string'\"\"\")",
          "191:       checkExceptionInExpression[NumberFormatException](cast(\"123.000.00\", dataType),",
          "192:         s\"\"\"Invalid input syntax for type \"${dataType.sql}\": '123.000.00'\"\"\")",
          "193:       checkExceptionInExpression[NumberFormatException](cast(\"abc.com\", dataType),",
          "194:         s\"\"\"Invalid input syntax for type \"${dataType.sql}\": 'abc.com'\"\"\")",
          "201:       cast(l, to), s\"\"\"Invalid input syntax for type \"${expectedDataTypeInErrorMsg.sql}\": 'true'\"\"\")",
          "",
          "[Added Lines]",
          "175:   private def castErrMsg(v: Any, to: DataType, from: DataType = StringType): String = {",
          "176:     s\"The value ${toSQLValue(v, from)} of the type ${toSQLType(from)} \" +",
          "177:     s\"cannot be cast to ${toSQLType(to)} because it is malformed.\"",
          "178:   }",
          "180:   private def castErrMsg(l: Literal, to: DataType, from: DataType): String = {",
          "181:     s\"The value ${toSQLValue(l.eval(), from)} of the type ${toSQLType(from)} \" +",
          "182:     s\"cannot be cast to ${toSQLType(to)} because it is malformed.\"",
          "183:   }",
          "185:   private def castErrMsg(l: Literal, to: DataType): String = {",
          "186:     castErrMsg(l, to, l.dataType)",
          "187:   }",
          "190:     def check(value: String, dataType: DataType): Unit = {",
          "191:       checkExceptionInExpression[NumberFormatException](cast(value, dataType),",
          "192:         castErrMsg(value, dataType))",
          "193:     }",
          "196:       check(\"string\", dataType)",
          "197:       check(\"123-string\", dataType)",
          "198:       check(\"2020-07-19\", dataType)",
          "199:       check(\"1.23\", dataType)",
          "203:       check(\"string\", dataType)",
          "204:       check(\"123.000.00\", dataType)",
          "205:       check(\"abc.com\", dataType)",
          "212:       cast(l, to), castErrMsg(\"true\", expectedDataTypeInErrorMsg))",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "246:     checkExceptionInExpression[NumberFormatException](",
          "247:       cast(\"abcd\", DecimalType(38, 1)),",
          "249:   }",
          "251:   protected def checkCastToBooleanError(l: Literal, to: DataType, tryCastResult: Any): Unit = {",
          "254:   }",
          "256:   test(\"ANSI mode: cast string to boolean with parse error\") {",
          "",
          "[Removed Lines]",
          "248:       s\"\"\"Invalid input syntax for type \"${DecimalType(38, 1).sql}\": 'abcd'\"\"\")",
          "252:     checkExceptionInExpression[UnsupportedOperationException](",
          "253:       cast(l, to), s\"invalid input syntax for type boolean\")",
          "",
          "[Added Lines]",
          "259:       castErrMsg(\"abcd\", DecimalType(38, 1)))",
          "263:     checkExceptionInExpression[SparkRuntimeException](",
          "264:       cast(l, to), \"\"\"cannot be cast to \"BOOLEAN\"\"\"\")",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "258:     checkCastToBooleanError(Literal(\"\"), BooleanType, null)",
          "259:   }",
          "267:   test(\"cast from timestamp II\") {",
          "268:     checkCastToTimestampError(Literal(Double.NaN), TimestampType)",
          "269:     checkCastToTimestampError(Literal(1.0 / 0.0), TimestampType)",
          "270:     checkCastToTimestampError(Literal(Float.NaN), TimestampType)",
          "",
          "[Removed Lines]",
          "261:   protected def checkCastToTimestampError(l: Literal, to: DataType): Unit = {",
          "262:     checkExceptionInExpression[DateTimeException](",
          "263:       cast(l, to),",
          "264:       s\"\"\"Invalid input syntax for type \"TIMESTAMP\": ${toSQLValue(l.eval(), l.dataType)}\"\"\")",
          "265:   }",
          "",
          "[Added Lines]",
          "273:     def checkCastToTimestampError(l: Literal, to: DataType): Unit = {",
          "274:       checkExceptionInExpression[DateTimeException](",
          "275:         cast(l, to),",
          "276:         \"\"\"cannot be cast to \"TIMESTAMP\" because it is malformed\"\"\")",
          "277:     }",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "276:     }",
          "277:   }",
          "279:   test(\"cast a timestamp before the epoch 1970-01-01 00:00:00Z II\") {",
          "280:     withDefaultTimeZone(UTC) {",
          "281:       val negativeTs = Timestamp.valueOf(\"1900-05-05 18:34:56.1\")",
          "282:       assert(negativeTs.getTime < 0)",
          "283:       Seq(ByteType, ShortType, IntegerType).foreach { dt =>",
          "284:         checkExceptionInExpression[SparkArithmeticException](",
          "286:       }",
          "287:     }",
          "288:   }",
          "",
          "[Removed Lines]",
          "285:           cast(negativeTs, dt), s\"\"\"to \"${dt.sql}\" causes overflow\"\"\")",
          "",
          "[Added Lines]",
          "289:   private def castOverflowErrMsg(v: Any, from: DataType, to: DataType): String = {",
          "290:     s\"The value ${toSQLValue(v, from)} of the type ${toSQLType(from)} cannot be \" +",
          "291:     s\"cast to ${toSQLType(to)} due to an overflow.\"",
          "292:   }",
          "300:           cast(negativeTs, dt),",
          "301:           castOverflowErrMsg(negativeTs, TimestampType, dt))",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "293:       assert(negativeTs.getTime < 0)",
          "294:       Seq(ByteType, ShortType, IntegerType).foreach { dt =>",
          "295:         checkExceptionInExpression[SparkArithmeticException](",
          "297:       }",
          "298:       val expectedSecs = Math.floorDiv(negativeTs.getTime, MILLIS_PER_SECOND)",
          "299:       checkEvaluation(cast(negativeTs, LongType), expectedSecs)",
          "",
          "[Removed Lines]",
          "296:           cast(negativeTs, dt), s\"\"\"to \"${dt.sql}\" causes overflow\"\"\")",
          "",
          "[Added Lines]",
          "312:           cast(negativeTs, dt),",
          "313:           castOverflowErrMsg(negativeTs, TimestampType, dt))",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "324:       val ret = cast(array_notNull, ArrayType(BooleanType, containsNull = false))",
          "325:       assert(ret.resolved == !isTryCast)",
          "326:       if (!isTryCast) {",
          "329:       }",
          "330:     }",
          "331:   }",
          "",
          "[Removed Lines]",
          "327:         checkExceptionInExpression[UnsupportedOperationException](",
          "328:           ret, \"invalid input syntax for type boolean\")",
          "",
          "[Added Lines]",
          "344:         checkExceptionInExpression[SparkRuntimeException](",
          "345:           ret, \"\"\"cannot be cast to \"BOOLEAN\"\"\"\")",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "372:       assert(ret.resolved == !isTryCast)",
          "373:       if (!isTryCast) {",
          "374:         checkExceptionInExpression[NumberFormatException](",
          "376:       }",
          "377:     }",
          "",
          "[Removed Lines]",
          "375:           ret, s\"\"\"Invalid input syntax for type \"${IntegerType.sql}\"\"\"\")",
          "",
          "[Added Lines]",
          "392:           ret,",
          "393:           castErrMsg(\"a\", IntegerType))",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "380:       val ret = cast(map_notNull, MapType(StringType, BooleanType, valueContainsNull = false))",
          "381:       assert(ret.resolved == !isTryCast)",
          "382:       if (!isTryCast) {",
          "385:       }",
          "386:     }",
          "",
          "[Removed Lines]",
          "383:         checkExceptionInExpression[UnsupportedOperationException](",
          "384:           ret, \"invalid input syntax for type boolean\")",
          "",
          "[Added Lines]",
          "401:         checkExceptionInExpression[SparkRuntimeException](",
          "402:           ret,",
          "403:           castErrMsg(\"123\", BooleanType))",
          "",
          "---------------",
          "--- Hunk 11 ---",
          "[Context before]",
          "390:       assert(ret.resolved == !isTryCast)",
          "391:       if (!isTryCast) {",
          "392:         checkExceptionInExpression[NumberFormatException](",
          "394:       }",
          "395:     }",
          "396:   }",
          "",
          "[Removed Lines]",
          "393:           ret, s\"\"\"Invalid input syntax for type \"${IntegerType.sql}\"\"\"\")",
          "",
          "[Added Lines]",
          "412:           ret,",
          "413:           castErrMsg(\"a\", IntegerType))",
          "",
          "---------------",
          "--- Hunk 12 ---",
          "[Context before]",
          "469:         StructField(\"c\", BooleanType, nullable = false))))",
          "470:       assert(ret.resolved == !isTryCast)",
          "471:       if (!isTryCast) {",
          "474:       }",
          "475:     }",
          "476:   }",
          "",
          "[Removed Lines]",
          "472:         checkExceptionInExpression[UnsupportedOperationException](",
          "473:           ret, \"invalid input syntax for type boolean\")",
          "",
          "[Added Lines]",
          "492:         checkExceptionInExpression[SparkRuntimeException](",
          "493:           ret,",
          "494:           castErrMsg(\"123\", BooleanType))",
          "",
          "---------------",
          "--- Hunk 13 ---",
          "[Context before]",
          "515:     assert(ret.resolved === !isTryCast)",
          "516:     if (!isTryCast) {",
          "517:       checkExceptionInExpression[NumberFormatException](",
          "519:     }",
          "520:   }",
          "",
          "[Removed Lines]",
          "518:         ret, s\"\"\"Invalid input syntax for type \"${IntegerType.sql}\"\"\"\")",
          "",
          "[Added Lines]",
          "539:         ret,",
          "540:         castErrMsg(\"true\", IntegerType))",
          "",
          "---------------",
          "--- Hunk 14 ---",
          "[Context before]",
          "524:       def checkCastWithParseError(str: String): Unit = {",
          "525:         checkExceptionInExpression[DateTimeException](",
          "526:           cast(Literal(str), TimestampType, Option(zid.getId)),",
          "528:       }",
          "530:       checkCastWithParseError(\"123\")",
          "",
          "[Removed Lines]",
          "527:           s\"\"\"Invalid input syntax for type \"TIMESTAMP\": '$str'\"\"\")",
          "",
          "[Added Lines]",
          "549:           castErrMsg(str, TimestampType))",
          "",
          "---------------",
          "--- Hunk 15 ---",
          "[Context before]",
          "545:       def checkCastWithParseError(str: String): Unit = {",
          "546:         checkExceptionInExpression[DateTimeException](",
          "547:           cast(Literal(str), DateType, Option(zid.getId)),",
          "549:       }",
          "551:       checkCastWithParseError(\"2015-13-18\")",
          "",
          "[Removed Lines]",
          "548:           s\"\"\"Invalid input syntax for type \"DATE\": '$str'\"\"\")",
          "",
          "[Added Lines]",
          "570:           castErrMsg(str, DateType))",
          "",
          "---------------",
          "--- Hunk 16 ---",
          "[Context before]",
          "573:       \"2021-06-17 00:00:00ABC\").foreach { invalidInput =>",
          "574:       checkExceptionInExpression[DateTimeException](",
          "575:         cast(invalidInput, TimestampNTZType),",
          "577:     }",
          "578:   }",
          "579: }",
          "",
          "[Removed Lines]",
          "576:         s\"\"\"Invalid input syntax for type \"TIMESTAMP_NTZ\": '$invalidInput'\"\"\")",
          "",
          "[Added Lines]",
          "598:         castErrMsg(invalidInput, TimestampNTZType))",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CastSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CastSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CastSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CastSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "592:       val e1 = intercept[ArithmeticException] {",
          "593:         Cast(Literal(Byte.MaxValue + 1), ByteType).eval()",
          "594:       }.getMessage",
          "596:       val e2 = intercept[ArithmeticException] {",
          "597:         Cast(Literal(Short.MaxValue + 1), ShortType).eval()",
          "598:       }.getMessage",
          "600:       val e3 = intercept[ArithmeticException] {",
          "601:         Cast(Literal(Int.MaxValue + 1L), IntegerType).eval()",
          "602:       }.getMessage",
          "604:     }",
          "605:   }",
          "",
          "[Removed Lines]",
          "595:       assert(e1.contains(\"Casting 128 to \\\"TINYINT\\\" causes overflow\"))",
          "599:       assert(e2.contains(\"Casting 32768 to \\\"SMALLINT\\\" causes overflow\"))",
          "603:       assert(e3.contains(\"Casting 2147483648L to \\\"INT\\\" causes overflow\"))",
          "",
          "[Added Lines]",
          "595:       assert(e1.contains(\"The value 128 of the type \\\"INT\\\" cannot be cast to \\\"TINYINT\\\"\"))",
          "599:       assert(e2.contains(\"The value 32768 of the type \\\"INT\\\" cannot be cast to \\\"SMALLINT\\\"\"))",
          "603:       assert(e3.contains(\"The value 2147483648L of the type \\\"BIGINT\\\" cannot be cast to \\\"INT\\\"\"))",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "619:     checkEvaluation(cast(Literal(\"2015-03-18T\"), TimestampType), null)",
          "620:   }",
          "622:   test(\"SPARK-36924: Cast DayTimeIntervalType to IntegralType\") {",
          "623:     DataTypeTestUtils.dayTimeIntervalTypes.foreach { dt =>",
          "624:       val v1 = Literal.create(Duration.ZERO, dt)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "622:   private def castOverflowErrMsg(targetType: DataType): String = {",
          "623:     s\"\"\"cannot be cast to \"${targetType.sql}\" due to an overflow.\"\"\"",
          "624:   }",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "642:           checkEvaluation(cast(v2, LongType), 25L)",
          "643:         case MINUTE =>",
          "644:           checkExceptionInExpression[ArithmeticException](cast(v2, ByteType),",
          "646:           checkEvaluation(cast(v2, ShortType), (MINUTES_PER_HOUR * 25 + 1).toShort)",
          "647:           checkEvaluation(cast(v2, IntegerType), (MINUTES_PER_HOUR * 25 + 1).toInt)",
          "648:           checkEvaluation(cast(v2, LongType), MINUTES_PER_HOUR * 25 + 1)",
          "649:         case SECOND =>",
          "650:           checkExceptionInExpression[ArithmeticException](cast(v2, ByteType),",
          "652:           checkExceptionInExpression[ArithmeticException](cast(v2, ShortType),",
          "654:           checkEvaluation(cast(v2, IntegerType), num.toInt)",
          "655:           checkEvaluation(cast(v2, LongType), num)",
          "656:       }",
          "",
          "[Removed Lines]",
          "645:             s\"\"\"Casting $v2 to \"TINYINT\" causes overflow\"\"\")",
          "651:             s\"\"\"Casting $v2 to \"TINYINT\" causes overflow\"\"\")",
          "653:             s\"\"\"Casting $v2 to \"SMALLINT\" causes overflow\"\"\")",
          "",
          "[Added Lines]",
          "649:             castOverflowErrMsg(ByteType))",
          "655:             castOverflowErrMsg(ByteType))",
          "657:             castOverflowErrMsg(ShortType))",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "659:       dt.endField match {",
          "660:         case DAY =>",
          "661:           checkExceptionInExpression[ArithmeticException](cast(v3, ByteType),",
          "663:           checkExceptionInExpression[ArithmeticException](cast(v3, ShortType),",
          "665:           checkEvaluation(cast(v3, IntegerType), (Long.MaxValue / MICROS_PER_DAY).toInt)",
          "666:           checkEvaluation(cast(v3, LongType), Long.MaxValue / MICROS_PER_DAY)",
          "667:         case HOUR =>",
          "668:           checkExceptionInExpression[ArithmeticException](cast(v3, ByteType),",
          "670:           checkExceptionInExpression[ArithmeticException](cast(v3, ShortType),",
          "672:           checkExceptionInExpression[ArithmeticException](cast(v3, IntegerType),",
          "674:           checkEvaluation(cast(v3, LongType), Long.MaxValue / MICROS_PER_HOUR)",
          "675:         case MINUTE =>",
          "676:           checkExceptionInExpression[ArithmeticException](cast(v3, ByteType),",
          "678:           checkExceptionInExpression[ArithmeticException](cast(v3, ShortType),",
          "680:           checkExceptionInExpression[ArithmeticException](cast(v3, IntegerType),",
          "682:           checkEvaluation(cast(v3, LongType), Long.MaxValue / MICROS_PER_MINUTE)",
          "683:         case SECOND =>",
          "684:           checkExceptionInExpression[ArithmeticException](cast(v3, ByteType),",
          "686:           checkExceptionInExpression[ArithmeticException](cast(v3, ShortType),",
          "688:           checkExceptionInExpression[ArithmeticException](cast(v3, IntegerType),",
          "690:           checkEvaluation(cast(v3, LongType), Long.MaxValue / MICROS_PER_SECOND)",
          "691:       }",
          "",
          "[Removed Lines]",
          "662:             s\"\"\"Casting $v3 to \"TINYINT\" causes overflow\"\"\")",
          "664:             s\"\"\"Casting $v3 to \"SMALLINT\" causes overflow\"\"\")",
          "669:             s\"\"\"Casting $v3 to \"TINYINT\" causes overflow\"\"\")",
          "671:             s\"\"\"Casting $v3 to \"SMALLINT\" causes overflow\"\"\")",
          "673:             s\"\"\"Casting $v3 to \"INT\" causes overflow\"\"\")",
          "677:             s\"\"\"Casting $v3 to \"TINYINT\" causes overflow\"\"\")",
          "679:             s\"\"\"Casting $v3 to \"SMALLINT\" causes overflow\"\"\")",
          "681:             s\"\"\"Casting $v3 to \"INT\" causes overflow\"\"\")",
          "685:             s\"\"\"Casting $v3 to \"TINYINT\" causes overflow\"\"\")",
          "687:             s\"\"\"Casting $v3 to \"SMALLINT\" causes overflow\"\"\")",
          "689:             s\"\"\"Casting $v3 to \"INT\" causes overflow\"\"\")",
          "",
          "[Added Lines]",
          "666:             castOverflowErrMsg(ByteType))",
          "668:             castOverflowErrMsg(ShortType))",
          "673:             castOverflowErrMsg(ByteType))",
          "675:             castOverflowErrMsg(ShortType))",
          "677:             castOverflowErrMsg(IntegerType))",
          "681:             castOverflowErrMsg(ByteType))",
          "683:             castOverflowErrMsg(ShortType))",
          "685:             castOverflowErrMsg(IntegerType))",
          "689:             castOverflowErrMsg(ByteType))",
          "691:             castOverflowErrMsg(ShortType))",
          "693:             castOverflowErrMsg(IntegerType))",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "694:       dt.endField match {",
          "695:         case DAY =>",
          "696:           checkExceptionInExpression[ArithmeticException](cast(v4, ByteType),",
          "698:           checkExceptionInExpression[ArithmeticException](cast(v4, ShortType),",
          "700:           checkEvaluation(cast(v4, IntegerType), (Long.MinValue / MICROS_PER_DAY).toInt)",
          "701:           checkEvaluation(cast(v4, LongType), Long.MinValue / MICROS_PER_DAY)",
          "702:         case HOUR =>",
          "703:           checkExceptionInExpression[ArithmeticException](cast(v4, ByteType),",
          "705:           checkExceptionInExpression[ArithmeticException](cast(v4, ShortType),",
          "707:           checkExceptionInExpression[ArithmeticException](cast(v4, IntegerType),",
          "709:           checkEvaluation(cast(v4, LongType), Long.MinValue / MICROS_PER_HOUR)",
          "710:         case MINUTE =>",
          "711:           checkExceptionInExpression[ArithmeticException](cast(v4, ByteType),",
          "713:           checkExceptionInExpression[ArithmeticException](cast(v4, ShortType),",
          "715:           checkExceptionInExpression[ArithmeticException](cast(v4, IntegerType),",
          "717:           checkEvaluation(cast(v4, LongType), Long.MinValue / MICROS_PER_MINUTE)",
          "718:         case SECOND =>",
          "719:           checkExceptionInExpression[ArithmeticException](cast(v4, ByteType),",
          "721:           checkExceptionInExpression[ArithmeticException](cast(v4, ShortType),",
          "723:           checkExceptionInExpression[ArithmeticException](cast(v4, IntegerType),",
          "725:           checkEvaluation(cast(v4, LongType), Long.MinValue / MICROS_PER_SECOND)",
          "726:       }",
          "727:     }",
          "",
          "[Removed Lines]",
          "697:             s\"\"\"Casting $v4 to \"TINYINT\" causes overflow\"\"\")",
          "699:             s\"\"\"Casting $v4 to \"SMALLINT\" causes overflow\"\"\")",
          "704:             s\"\"\"Casting $v4 to \"TINYINT\" causes overflow\"\"\")",
          "706:             s\"\"\"Casting $v4 to \"SMALLINT\" causes overflow\"\"\")",
          "708:             s\"\"\"Casting $v4 to \"INT\" causes overflow\"\"\")",
          "712:             s\"\"\"Casting $v4 to \"TINYINT\" causes overflow\"\"\")",
          "714:             s\"\"\"Casting $v4 to \"SMALLINT\" causes overflow\"\"\")",
          "716:             s\"\"\"Casting $v4 to \"INT\" causes overflow\"\"\")",
          "720:             s\"\"\"Casting $v4 to \"TINYINT\" causes overflow\"\"\")",
          "722:             s\"\"\"Casting $v4 to \"SMALLINT\" causes overflow\"\"\")",
          "724:             s\"\"\"Casting $v4 to \"INT\" causes overflow\"\"\")",
          "",
          "[Added Lines]",
          "701:             castOverflowErrMsg(ByteType))",
          "703:             castOverflowErrMsg(ShortType))",
          "708:             castOverflowErrMsg(ByteType))",
          "710:             castOverflowErrMsg(ShortType))",
          "712:             castOverflowErrMsg(IntegerType))",
          "716:             castOverflowErrMsg(ByteType))",
          "718:             castOverflowErrMsg(ShortType))",
          "720:             castOverflowErrMsg(IntegerType))",
          "724:             castOverflowErrMsg(ByteType))",
          "726:             castOverflowErrMsg(ShortType))",
          "728:             castOverflowErrMsg(IntegerType))",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "777:     ).foreach {",
          "778:       case (v, toType) =>",
          "779:         checkExceptionInExpression[ArithmeticException](cast(v, toType),",
          "781:     }",
          "783:     Seq(",
          "",
          "[Removed Lines]",
          "780:           s\"\"\"Casting $v to \"${toType.sql}\" causes overflow\"\"\")",
          "",
          "[Added Lines]",
          "784:           castOverflowErrMsg(toType))",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "792:     ).foreach {",
          "793:       case (v, toType) =>",
          "794:         checkExceptionInExpression[ArithmeticException](cast(v, toType),",
          "796:     }",
          "797:   }",
          "",
          "[Removed Lines]",
          "795:           s\"\"\"Casting ${v}L to \"${toType.sql}\" causes overflow\"\"\")",
          "",
          "[Added Lines]",
          "799:           castOverflowErrMsg(toType))",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "829:       case (v, dt, toType) =>",
          "830:         val value = Literal.create(v, dt)",
          "831:         checkExceptionInExpression[ArithmeticException](cast(value, toType),",
          "833:     }",
          "835:     Seq(",
          "",
          "[Removed Lines]",
          "832:           s\"\"\"Casting $value to \"${toType.sql}\" causes overflow\"\"\")",
          "",
          "[Added Lines]",
          "836:           castOverflowErrMsg(toType))",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "887:     ).foreach {",
          "888:       case (v, toType) =>",
          "889:         checkExceptionInExpression[ArithmeticException](cast(v, toType),",
          "891:     }",
          "893:     Seq(",
          "",
          "[Removed Lines]",
          "890:           s\"\"\"Casting $v to \"${toType.sql}\" causes overflow\"\"\")",
          "",
          "[Added Lines]",
          "894:           castOverflowErrMsg(toType))",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "898:     ).foreach {",
          "899:       case (v, toType) =>",
          "900:         checkExceptionInExpression[ArithmeticException](cast(v, toType),",
          "902:     }",
          "903:   }",
          "904: }",
          "",
          "[Removed Lines]",
          "901:           s\"\"\"Casting ${v}L to \"${toType.sql}\" causes overflow\"\"\")",
          "",
          "[Added Lines]",
          "905:           castOverflowErrMsg(toType))",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/DateFormatterSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/DateFormatterSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/DateFormatterSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/DateFormatterSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "208:     val errMsg = intercept[DateTimeException] {",
          "209:       formatter.parse(\"x123\")",
          "210:     }.getMessage",
          "212:   }",
          "213: }",
          "",
          "[Removed Lines]",
          "211:     assert(errMsg.contains(\"\"\"Invalid input syntax for type \"DATE\": 'x123'\"\"\"))",
          "",
          "[Added Lines]",
          "211:     assert(errMsg.contains(\"\"\"The value 'x123' of the type \"STRING\" cannot be cast to \"DATE\"\"\"\"))",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/TimestampFormatterSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/TimestampFormatterSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/TimestampFormatterSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/TimestampFormatterSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "453:       val errMsg = intercept[DateTimeException] {",
          "454:         formatter.parse(\"x123\")",
          "455:       }.getMessage",
          "457:     }",
          "458:   }",
          "",
          "[Removed Lines]",
          "456:       assert(errMsg.contains(\"\"\"Invalid input syntax for type \"TIMESTAMP\": 'x123'\"\"\"))",
          "",
          "[Added Lines]",
          "456:       assert(errMsg.contains(",
          "457:         \"\"\"The value 'x123' of the type \"STRING\" cannot be cast to \"TIMESTAMP\"\"\"\"))",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/types/DecimalSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/types/DecimalSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/types/DecimalSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/types/DecimalSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "285:     assert(Decimal.fromString(UTF8String.fromString(\"str\")) === null)",
          "286:     val e = intercept[NumberFormatException](Decimal.fromStringANSI(UTF8String.fromString(\"str\")))",
          "289:   }",
          "291:   test(\"SPARK-35841: Casting string to decimal type doesn't work \" +",
          "",
          "[Removed Lines]",
          "287:     assert(e.getMessage.contains(\"Invalid input syntax for type \" +",
          "288:       s\"\"\"\"${DecimalType.USER_DEFAULT.sql}\": 'str'\"\"\"))",
          "",
          "[Added Lines]",
          "287:     assert(e.getMessage.contains(",
          "288:       \"\"\"The value 'str' of the type \"STRING\" cannot be cast to \"DECIMAL(10,0)\"\"\"\"))",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/SQLInsertTestSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/SQLInsertTestSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/SQLInsertTestSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/SQLInsertTestSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "302:             val errorMsg = intercept[NumberFormatException] {",
          "303:               sql(\"insert into t partition(a='ansi') values('ansi')\")",
          "304:             }.getMessage",
          "306:           } else {",
          "307:             sql(\"insert into t partition(a='ansi') values('ansi')\")",
          "308:             checkAnswer(sql(\"select * from t\"), Row(\"ansi\", null) :: Nil)",
          "",
          "[Removed Lines]",
          "305:             assert(errorMsg.contains(\"\"\"Invalid input syntax for type \"INT\": 'ansi'\"\"\"))",
          "",
          "[Added Lines]",
          "305:             assert(errorMsg.contains(",
          "306:               \"\"\"The value 'ansi' of the type \"STRING\" cannot be cast to \"INT\"\"\"\"))",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/sources/InsertSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/sources/InsertSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/sources/InsertSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/sources/InsertSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "713:         var msg = intercept[SparkException] {",
          "714:           sql(s\"insert into t values($outOfRangeValue1)\")",
          "715:         }.getCause.getMessage",
          "718:         val outOfRangeValue2 = (Int.MinValue - 1L).toString",
          "719:         msg = intercept[SparkException] {",
          "720:           sql(s\"insert into t values($outOfRangeValue2)\")",
          "721:         }.getCause.getMessage",
          "723:       }",
          "724:     }",
          "725:   }",
          "",
          "[Removed Lines]",
          "716:         assert(msg.contains(s\"\"\"Casting ${outOfRangeValue1}L to \"INT\" causes overflow\"\"\"))",
          "722:         assert(msg.contains(s\"\"\"Casting ${outOfRangeValue2}L to \"INT\" causes overflow\"\"\"))",
          "",
          "[Added Lines]",
          "716:         assert(msg.contains(",
          "717:           s\"\"\"The value ${outOfRangeValue1}L of the type \"BIGINT\" cannot be cast to \"INT\"\"\"\"))",
          "723:         assert(msg.contains(",
          "724:           s\"\"\"The value ${outOfRangeValue2}L of the type \"BIGINT\" cannot be cast to \"INT\"\"\"\"))",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "733:         var msg = intercept[SparkException] {",
          "734:           sql(s\"insert into t values(${outOfRangeValue1}D)\")",
          "735:         }.getCause.getMessage",
          "738:         val outOfRangeValue2 = Math.nextDown(Long.MinValue)",
          "739:         msg = intercept[SparkException] {",
          "740:           sql(s\"insert into t values(${outOfRangeValue2}D)\")",
          "741:         }.getCause.getMessage",
          "743:       }",
          "744:     }",
          "745:   }",
          "",
          "[Removed Lines]",
          "736:         assert(msg.contains(s\"\"\"Casting ${outOfRangeValue1}D to \"BIGINT\" causes overflow\"\"\"))",
          "742:         assert(msg.contains(s\"\"\"Casting ${outOfRangeValue2}D to \"BIGINT\" causes overflow\"\"\"))",
          "",
          "[Added Lines]",
          "738:         assert(msg.contains(",
          "739:           s\"\"\"The value ${outOfRangeValue1}D of the type \"DOUBLE\" cannot be cast to \"BIGINT\"\"\"\"))",
          "745:         assert(msg.contains(",
          "746:           s\"\"\"The value ${outOfRangeValue2}D of the type \"DOUBLE\" cannot be cast to \"BIGINT\"\"\"\"))",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "4a0f0ff6c22b85cb0fc1eef842da8dbe4c90543a",
      "candidate_info": {
        "commit_hash": "4a0f0ff6c22b85cb0fc1eef842da8dbe4c90543a",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/4a0f0ff6c22b85cb0fc1eef842da8dbe4c90543a",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/finishAnalysis.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/QueryPlan.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ComputeCurrentTimeSuite.scala"
        ],
        "message": "[SPARK-39259][SQL][3.3] Evaluate timestamps consistently in subqueries\n\n### What changes were proposed in this pull request?\n\nApply the optimizer rule ComputeCurrentTime consistently across subqueries.\n\nThis is a backport of https://github.com/apache/spark/pull/36654.\n\n### Why are the changes needed?\n\nAt the moment timestamp functions like now() can return different values within a query if subqueries are involved\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nA new unit test was added\n\nCloses #36752 from olaky/SPARK-39259-spark_3_3.\n\nAuthored-by: Ole Sasse <ole.sasse@databricks.com>\nSigned-off-by: Max Gekk <max.gekk@gmail.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/finishAnalysis.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/finishAnalysis.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/QueryPlan.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/QueryPlan.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ComputeCurrentTimeSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ComputeCurrentTimeSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/finishAnalysis.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/finishAnalysis.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/finishAnalysis.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/finishAnalysis.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: package org.apache.spark.sql.catalyst.optimizer",
          "22: import org.apache.spark.sql.catalyst.CurrentUserContext.CURRENT_USER",
          "23: import org.apache.spark.sql.catalyst.expressions._",
          "24: import org.apache.spark.sql.catalyst.plans.logical._",
          "25: import org.apache.spark.sql.catalyst.rules._",
          "26: import org.apache.spark.sql.catalyst.trees.TreePattern._",
          "28: import org.apache.spark.sql.connector.catalog.CatalogManager",
          "29: import org.apache.spark.sql.types._",
          "30: import org.apache.spark.util.Utils",
          "",
          "[Removed Lines]",
          "20: import scala.collection.mutable",
          "27: import org.apache.spark.sql.catalyst.util.DateTimeUtils.{convertSpecialDate, convertSpecialTimestamp, convertSpecialTimestampNTZ}",
          "",
          "[Added Lines]",
          "20: import java.time.{Instant, LocalDateTime}",
          "27: import org.apache.spark.sql.catalyst.trees.TreePatternBits",
          "28: import org.apache.spark.sql.catalyst.util.DateTimeUtils",
          "29: import org.apache.spark.sql.catalyst.util.DateTimeUtils.{convertSpecialDate, convertSpecialTimestamp, convertSpecialTimestampNTZ, instantToMicros, localDateTimeToMicros}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "74: object ComputeCurrentTime extends Rule[LogicalPlan] {",
          "75:   def apply(plan: LogicalPlan): LogicalPlan = {",
          "80:     val timezone = Literal.create(conf.sessionLocalTimeZone, StringType)",
          "94:     }",
          "95:   }",
          "96: }",
          "",
          "[Removed Lines]",
          "76:     val currentDates = mutable.Map.empty[String, Literal]",
          "77:     val timeExpr = CurrentTimestamp()",
          "78:     val timestamp = timeExpr.eval(EmptyRow).asInstanceOf[Long]",
          "79:     val currentTime = Literal.create(timestamp, timeExpr.dataType)",
          "81:     val localTimestamps = mutable.Map.empty[String, Literal]",
          "83:     plan.transformAllExpressionsWithPruning(_.containsPattern(CURRENT_LIKE)) {",
          "84:       case currentDate @ CurrentDate(Some(timeZoneId)) =>",
          "85:         currentDates.getOrElseUpdate(timeZoneId, {",
          "86:           Literal.create(currentDate.eval().asInstanceOf[Int], DateType)",
          "87:         })",
          "88:       case CurrentTimestamp() | Now() => currentTime",
          "89:       case CurrentTimeZone() => timezone",
          "90:       case localTimestamp @ LocalTimestamp(Some(timeZoneId)) =>",
          "91:         localTimestamps.getOrElseUpdate(timeZoneId, {",
          "92:           Literal.create(localTimestamp.eval().asInstanceOf[Long], TimestampNTZType)",
          "93:         })",
          "",
          "[Added Lines]",
          "78:     val instant = Instant.now()",
          "79:     val currentTimestampMicros = instantToMicros(instant)",
          "80:     val currentTime = Literal.create(currentTimestampMicros, TimestampType)",
          "83:     def transformCondition(treePatternbits: TreePatternBits): Boolean = {",
          "84:       treePatternbits.containsPattern(CURRENT_LIKE)",
          "85:     }",
          "87:     plan.transformDownWithSubqueries(transformCondition) {",
          "88:       case subQuery =>",
          "89:         subQuery.transformAllExpressionsWithPruning(transformCondition) {",
          "90:           case cd: CurrentDate =>",
          "91:             Literal.create(DateTimeUtils.microsToDays(currentTimestampMicros, cd.zoneId), DateType)",
          "92:           case CurrentTimestamp() | Now() => currentTime",
          "93:           case CurrentTimeZone() => timezone",
          "94:           case localTimestamp: LocalTimestamp =>",
          "95:             val asDateTime = LocalDateTime.ofInstant(instant, localTimestamp.zoneId)",
          "96:             Literal.create(localDateTimeToMicros(asDateTime), TimestampNTZType)",
          "97:         }",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/QueryPlan.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/QueryPlan.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/QueryPlan.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/QueryPlan.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "456:   def transformWithSubqueries(f: PartialFunction[PlanType, PlanType]): PlanType =",
          "",
          "[Removed Lines]",
          "457:     transformDownWithSubqueries(f)",
          "",
          "[Added Lines]",
          "457:     transformDownWithSubqueries(AlwaysProcess.fn, UnknownRuleId)(f)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "483:     val g: PartialFunction[PlanType, PlanType] = new PartialFunction[PlanType, PlanType] {",
          "484:       override def isDefinedAt(x: PlanType): Boolean = true",
          "",
          "[Removed Lines]",
          "482:   def transformDownWithSubqueries(f: PartialFunction[PlanType, PlanType]): PlanType = {",
          "",
          "[Added Lines]",
          "482:   def transformDownWithSubqueries(",
          "483:     cond: TreePatternBits => Boolean = AlwaysProcess.fn, ruleId: RuleId = UnknownRuleId)",
          "484:     (f: PartialFunction[PlanType, PlanType])",
          "485: : PlanType = {",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "487:         val transformed = f.applyOrElse[PlanType, PlanType](plan, identity)",
          "488:         transformed transformExpressionsDown {",
          "489:           case planExpression: PlanExpression[PlanType] =>",
          "491:             planExpression.withNewPlan(newPlan)",
          "492:         }",
          "493:       }",
          "494:     }",
          "497:   }",
          "",
          "[Removed Lines]",
          "490:             val newPlan = planExpression.plan.transformDownWithSubqueries(f)",
          "496:     transformDown(g)",
          "",
          "[Added Lines]",
          "493:             val newPlan = planExpression.plan.transformDownWithSubqueries(cond, ruleId)(f)",
          "499:     transformDownWithPruning(cond, ruleId)(g)",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ComputeCurrentTimeSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ComputeCurrentTimeSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ComputeCurrentTimeSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ComputeCurrentTimeSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: import java.time.{LocalDateTime, ZoneId}",
          "22: import org.apache.spark.sql.catalyst.dsl.plans._",
          "24: import org.apache.spark.sql.catalyst.plans.PlanTest",
          "26: import org.apache.spark.sql.catalyst.rules.RuleExecutor",
          "27: import org.apache.spark.sql.catalyst.util.DateTimeUtils",
          "28: import org.apache.spark.sql.internal.SQLConf",
          "",
          "[Removed Lines]",
          "23: import org.apache.spark.sql.catalyst.expressions.{Alias, CurrentDate, CurrentTimestamp, CurrentTimeZone, Literal, LocalTimestamp}",
          "25: import org.apache.spark.sql.catalyst.plans.logical.{LocalRelation, LogicalPlan, Project}",
          "",
          "[Added Lines]",
          "22: import scala.collection.JavaConverters.mapAsScalaMap",
          "23: import scala.concurrent.duration._",
          "26: import org.apache.spark.sql.catalyst.expressions.{Alias, CurrentDate, CurrentTimestamp, CurrentTimeZone, InSubquery, ListQuery, Literal, LocalTimestamp, Now}",
          "28: import org.apache.spark.sql.catalyst.plans.logical.{Filter, LocalRelation, LogicalPlan, Project}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "41:     val plan = Optimize.execute(in.analyze).asInstanceOf[Project]",
          "42:     val max = (System.currentTimeMillis() + 1) * 1000",
          "49:     assert(lits.size == 2)",
          "50:     assert(lits(0) >= min && lits(0) <= max)",
          "51:     assert(lits(1) >= min && lits(1) <= max)",
          "",
          "[Removed Lines]",
          "44:     val lits = new scala.collection.mutable.ArrayBuffer[Long]",
          "45:     plan.transformAllExpressions { case e: Literal =>",
          "46:       lits += e.value.asInstanceOf[Long]",
          "47:       e",
          "48:     }",
          "",
          "[Added Lines]",
          "47:     val lits = literals[Long](plan)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "59:     val plan = Optimize.execute(in.analyze).asInstanceOf[Project]",
          "60:     val max = DateTimeUtils.currentDate(ZoneId.systemDefault())",
          "67:     assert(lits.size == 2)",
          "68:     assert(lits(0) >= min && lits(0) <= max)",
          "69:     assert(lits(1) >= min && lits(1) <= max)",
          "",
          "[Removed Lines]",
          "62:     val lits = new scala.collection.mutable.ArrayBuffer[Int]",
          "63:     plan.transformAllExpressions { case e: Literal =>",
          "64:       lits += e.value.asInstanceOf[Int]",
          "65:       e",
          "66:     }",
          "",
          "[Added Lines]",
          "61:     val lits = literals[Int](plan)",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "73:   test(\"SPARK-33469: Add current_timezone function\") {",
          "74:     val in = Project(Seq(Alias(CurrentTimeZone(), \"c\")()), LocalRelation())",
          "75:     val plan = Optimize.execute(in.analyze).asInstanceOf[Project]",
          "81:     assert(lits.size == 1)",
          "83:   }",
          "85:   test(\"analyzer should replace localtimestamp with literals\") {",
          "",
          "[Removed Lines]",
          "76:     val lits = new scala.collection.mutable.ArrayBuffer[String]",
          "77:     plan.transformAllExpressions { case e: Literal =>",
          "78:       lits += e.value.asInstanceOf[UTF8String].toString",
          "79:       e",
          "80:     }",
          "82:     assert(lits.head == SQLConf.get.sessionLocalTimeZone)",
          "",
          "[Added Lines]",
          "71:     val lits = literals[UTF8String](plan)",
          "73:     assert(lits.head == UTF8String.fromString(SQLConf.get.sessionLocalTimeZone))",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "92:     val plan = Optimize.execute(in.analyze).asInstanceOf[Project]",
          "93:     val max = DateTimeUtils.localDateTimeToMicros(LocalDateTime.now(zoneId))",
          "100:     assert(lits.size == 2)",
          "101:     assert(lits(0) >= min && lits(0) <= max)",
          "102:     assert(lits(1) >= min && lits(1) <= max)",
          "103:     assert(lits(0) == lits(1))",
          "104:   }",
          "105: }",
          "",
          "[Removed Lines]",
          "95:     val lits = new scala.collection.mutable.ArrayBuffer[Long]",
          "96:     plan.transformAllExpressions { case e: Literal =>",
          "97:       lits += e.value.asInstanceOf[Long]",
          "98:       e",
          "99:     }",
          "",
          "[Added Lines]",
          "86:     val lits = literals[Long](plan)",
          "93:   test(\"analyzer should use equal timestamps across subqueries\") {",
          "94:     val timestampInSubQuery = Project(Seq(Alias(LocalTimestamp(), \"timestamp1\")()), LocalRelation())",
          "95:     val listSubQuery = ListQuery(timestampInSubQuery)",
          "96:     val valueSearchedInSubQuery = Seq(Alias(LocalTimestamp(), \"timestamp2\")())",
          "97:     val inFilterWithSubQuery = InSubquery(valueSearchedInSubQuery, listSubQuery)",
          "98:     val input = Project(Nil, Filter(inFilterWithSubQuery, LocalRelation()))",
          "100:     val plan = Optimize.execute(input.analyze).asInstanceOf[Project]",
          "102:     val lits = literals[Long](plan)",
          "103:     assert(lits.size == 3) // transformDownWithSubqueries covers the inner timestamp twice",
          "104:     assert(lits.toSet.size == 1)",
          "105:   }",
          "107:   test(\"analyzer should use consistent timestamps for different timezones\") {",
          "108:     val localTimestamps = mapAsScalaMap(ZoneId.SHORT_IDS)",
          "109:       .map { case (zoneId, _) => Alias(LocalTimestamp(Some(zoneId)), zoneId)() }.toSeq",
          "110:     val input = Project(localTimestamps, LocalRelation())",
          "112:     val plan = Optimize.execute(input).asInstanceOf[Project]",
          "114:     val lits = literals[Long](plan)",
          "115:     assert(lits.size === localTimestamps.size)",
          "117:     val offsetsFromQuarterHour = lits.map( _ % Duration(15, MINUTES).toMicros).toSet",
          "118:     assert(offsetsFromQuarterHour.size == 1)",
          "119:   }",
          "121:   test(\"analyzer should use consistent timestamps for different timestamp functions\") {",
          "122:     val differentTimestamps = Seq(",
          "123:       Alias(CurrentTimestamp(), \"currentTimestamp\")(),",
          "124:       Alias(Now(), \"now\")(),",
          "125:       Alias(LocalTimestamp(Some(\"PLT\")), \"localTimestampWithTimezone\")()",
          "126:     )",
          "127:     val input = Project(differentTimestamps, LocalRelation())",
          "129:     val plan = Optimize.execute(input).asInstanceOf[Project]",
          "131:     val lits = literals[Long](plan)",
          "132:     assert(lits.size === differentTimestamps.size)",
          "134:     val offsetsFromQuarterHour = lits.map( _ % Duration(15, MINUTES).toMicros).toSet",
          "135:     assert(offsetsFromQuarterHour.size == 1)",
          "136:   }",
          "138:   private def literals[T](plan: LogicalPlan): Seq[T] = {",
          "139:     val literals = new scala.collection.mutable.ArrayBuffer[T]",
          "140:     plan.transformWithSubqueries { case subQuery =>",
          "141:       subQuery.transformAllExpressions { case expression: Literal =>",
          "142:         literals += expression.value.asInstanceOf[T]",
          "143:         expression",
          "144:       }",
          "145:     }",
          "146:     literals.asInstanceOf[Seq[T]]",
          "147:   }",
          "",
          "---------------"
        ]
      }
    }
  ]
}