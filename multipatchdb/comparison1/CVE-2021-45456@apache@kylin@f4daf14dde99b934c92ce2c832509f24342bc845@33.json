{
  "cve_id": "CVE-2021-45456",
  "cve_desc": "Apache kylin checks the legitimacy of the project before executing some commands with the project name passed in by the user. There is a mismatch between what is being checked and what is being used as the shell command argument in DiagnosisService. This may cause an illegal project name to pass the check and perform the following steps, resulting in a command injection vulnerability. This issue affects Apache Kylin 4.0.0.",
  "repo": "apache/kylin",
  "patch_hash": "f4daf14dde99b934c92ce2c832509f24342bc845",
  "patch_info": {
    "commit_hash": "f4daf14dde99b934c92ce2c832509f24342bc845",
    "repo": "apache/kylin",
    "commit_url": "https://github.com/apache/kylin/commit/f4daf14dde99b934c92ce2c832509f24342bc845",
    "files": [
      "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java",
      "core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java",
      "core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java",
      "server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java",
      "server/src/main/webapp/WEB-INF/web.xml"
    ],
    "message": "test fix",
    "before_after_code_files": [
      "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java||core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java",
      "core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java||core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java",
      "core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java||core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java",
      "server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java||server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java"
    ]
  },
  "patch_diff": {
    "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java||core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java": [
      "File: core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java -> core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java",
      "--- Hunk 1 ---",
      "[Context before]",
      "3403:     public String getKerberosPrincipal() {",
      "3404:         return getOptional(\"kylin.kerberos.principal\");",
      "3405:     }",
      "3406: }",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "3407:     public String getEncryptCipherIvSpec() {",
      "3408:         return getOptional(\"kylin.security.encrypt.cipher.ivSpec\", \"AAAAAAAAAAAAAAAA\");",
      "3409:     }",
      "",
      "---------------"
    ],
    "core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java||core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java": [
      "File: core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java -> core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java",
      "--- Hunk 1 ---",
      "[Context before]",
      "25: import java.security.NoSuchAlgorithmException;",
      "27: import org.apache.commons.codec.binary.Base64;",
      "29: import javax.crypto.Cipher;",
      "30: import javax.crypto.NoSuchPaddingException;",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "28: import org.apache.kylin.common.KylinConfig;",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "42:             InvalidKeyException, NoSuchPaddingException, NoSuchAlgorithmException, UnsupportedEncodingException {",
      "43:         Cipher cipher = Cipher.getInstance(\"AES/CFB/PKCS5Padding\");",
      "44:         final SecretKeySpec secretKey = new SecretKeySpec(key, \"AES\");",
      "46:         cipher.init(cipherMode, secretKey, ivSpec);",
      "47:         return cipher;",
      "48:     }",
      "",
      "[Removed Lines]",
      "45:         IvParameterSpec ivSpec = new IvParameterSpec(\"AAAAAAAAAAAAAAAA\".getBytes(\"UTF-8\"));",
      "",
      "[Added Lines]",
      "46:         IvParameterSpec ivSpec = new IvParameterSpec(KylinConfig.getInstanceFromEnv().getEncryptCipherIvSpec().getBytes(\"UTF-8\"));",
      "",
      "---------------"
    ],
    "core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java||core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java": [
      "File: core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java -> core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java",
      "--- Hunk 1 ---",
      "[Context before]",
      "19: package org.apache.kylin.common.util;",
      "21: import org.junit.Assert;",
      "22: import org.junit.Test;",
      "26:     @Test",
      "27:     public void testAESEncrypt(){",
      "",
      "[Removed Lines]",
      "24: public class EncryptUtilTest {",
      "",
      "[Added Lines]",
      "21: import org.junit.After;",
      "23: import org.junit.Before;",
      "26: public class EncryptUtilTest extends LocalFileMetadataTestCase {",
      "27:     @Before",
      "28:     public void setUp() throws Exception {",
      "29:         this.createTestMetadata();",
      "30:     }",
      "32:     @After",
      "33:     public void after() throws Exception {",
      "34:         this.cleanupTestMetadata();",
      "35:     }",
      "",
      "---------------"
    ],
    "server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java||server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java": [
      "File: server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java -> server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java",
      "--- Hunk 1 ---",
      "[Context before]",
      "87:     public String dumpProjectDiagnosisInfo(String project, File exportPath) throws IOException {",
      "88:         Message msg = MsgPicker.getMsg();",
      "89:         ProjectInstance projectInstance =",
      "90:                 ProjectManager.getInstance(KylinConfig.getInstanceFromEnv())",
      "92:         if (null == projectInstance) {",
      "93:             throw new BadRequestException(",
      "95:         }",
      "96:         aclEvaluate.checkProjectOperationPermission(projectInstance);",
      "98:         runDiagnosisCLI(args);",
      "99:         return getDiagnosisPackageName(exportPath);",
      "100:     }",
      "",
      "[Removed Lines]",
      "91:                         .getProject(ValidateUtil.convertStringToBeAlphanumericUnderscore(project));",
      "94:                     String.format(Locale.ROOT, msg.getDIAG_PROJECT_NOT_FOUND(), project));",
      "97:         String[] args = { project, exportPath.getAbsolutePath() };",
      "",
      "[Added Lines]",
      "89:         String projectName = ValidateUtil.convertStringToBeAlphanumericUnderscore(project);",
      "92:                         .getProject(projectName);",
      "95:                     String.format(Locale.ROOT, msg.getDIAG_PROJECT_NOT_FOUND(), projectName));",
      "98:         String[] args = { projectName, exportPath.getAbsolutePath() };",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "93b82957e855b9e2545b7722dd1379e218f495cb",
      "candidate_info": {
        "commit_hash": "93b82957e855b9e2545b7722dd1379e218f495cb",
        "repo": "apache/kylin",
        "commit_url": "https://github.com/apache/kylin/commit/93b82957e855b9e2545b7722dd1379e218f495cb",
        "files": [
          "server-base/src/main/java/org/apache/kylin/rest/controller/TableController.java"
        ],
        "message": "Avoid redundant error information",
        "before_after_code_files": [
          "server-base/src/main/java/org/apache/kylin/rest/controller/TableController.java||server-base/src/main/java/org/apache/kylin/rest/controller/TableController.java"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/kylin/pull/1893",
          "https://github.com/apache/kylin/pull/2018",
          "https://github.com/apache/kylin/pull/2125",
          "https://github.com/apache/kylin/pull/2033",
          "https://github.com/apache/kylin/pull/2112",
          "https://github.com/apache/kylin/pull/2115",
          "https://github.com/apache/kylin/pull/1865",
          "https://github.com/apache/kylin/pull/1913",
          "https://github.com/apache/kylin/pull/2135"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "server-base/src/main/java/org/apache/kylin/rest/controller/TableController.java||server-base/src/main/java/org/apache/kylin/rest/controller/TableController.java": [
          "File: server-base/src/main/java/org/apache/kylin/rest/controller/TableController.java -> server-base/src/main/java/org/apache/kylin/rest/controller/TableController.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "34: import org.apache.kylin.rest.exception.InternalErrorException;",
          "35: import org.apache.kylin.rest.exception.NotFoundException;",
          "36: import org.apache.kylin.rest.request.HiveTableRequest;",
          "38: import org.apache.kylin.rest.service.TableACLService;",
          "39: import org.apache.kylin.rest.service.TableService;",
          "40: import org.slf4j.Logger;",
          "",
          "[Removed Lines]",
          "37: import org.apache.kylin.rest.response.TableSnapshotResponse;",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "223:     @RequestMapping(value = \"/{project}/{tableName}/snapshots\", method = { RequestMethod.GET })",
          "224:     @ResponseBody",
          "226:             @PathVariable final String tableName) throws IOException {",
          "228:     }",
          "230:     @RequestMapping(value = \"/supported_datetime_patterns\", method = { RequestMethod.GET })",
          "",
          "[Removed Lines]",
          "225:     public List<TableSnapshotResponse> getTableSnapshots(@PathVariable final String project,",
          "227:         throw new UnsupportedOperationException(\"API getTableSnapshots is not supported in Kylin 4.0 .\");",
          "",
          "[Added Lines]",
          "224:     public void getTableSnapshots(@PathVariable final String project,",
          "226:         logger.warn(\"API getTableSnapshots is not supported in Kylin 4.0 .\");",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "b08c1be22eb51796fb58c3694e86e60f948337f6",
      "candidate_info": {
        "commit_hash": "b08c1be22eb51796fb58c3694e86e60f948337f6",
        "repo": "apache/kylin",
        "commit_url": "https://github.com/apache/kylin/commit/b08c1be22eb51796fb58c3694e86e60f948337f6",
        "files": [
          "build/conf/kylin-server-log4j.properties"
        ],
        "message": "KYLIN-4859 Log4J reinitialized/reconfigured by Spark Logging (#1537)",
        "before_after_code_files": [
          "build/conf/kylin-server-log4j.properties||build/conf/kylin-server-log4j.properties"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/kylin/pull/1893",
          "https://github.com/apache/kylin/pull/2018",
          "https://github.com/apache/kylin/pull/2125",
          "https://github.com/apache/kylin/pull/2033",
          "https://github.com/apache/kylin/pull/2112",
          "https://github.com/apache/kylin/pull/2115",
          "https://github.com/apache/kylin/pull/1865",
          "https://github.com/apache/kylin/pull/1913",
          "https://github.com/apache/kylin/pull/2135"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "build/conf/kylin-server-log4j.properties||build/conf/kylin-server-log4j.properties": [
          "File: build/conf/kylin-server-log4j.properties -> build/conf/kylin-server-log4j.properties",
          "--- Hunk 1 ---",
          "[Context before]",
          "26: log4j.appender.file.MaxBackupIndex=10",
          "28: #overall config",
          "",
          "[Removed Lines]",
          "29: log4j.rootLogger=INFO",
          "30: log4j.logger.org.apache.kylin=DEBUG,file",
          "31: log4j.logger.org.springframework=WARN,file",
          "32: log4j.logger.org.springframework.security=INFO,file",
          "33: log4j.logger.org.apache.kylin.spark.classloader=INFO,file",
          "",
          "[Added Lines]",
          "30: log4j.logger.org.apache.kylin=DEBUG",
          "31: log4j.logger.org.springframework=WARN",
          "32: log4j.logger.org.springframework.security=INFO",
          "33: log4j.logger.org.apache.kylin.spark.classloader=INFO",
          "34: log4j.logger.org.apache.spark=INFO",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "c9a3140729da6d28ddb8359b035567d2c4d1eca3",
      "candidate_info": {
        "commit_hash": "c9a3140729da6d28ddb8359b035567d2c4d1eca3",
        "repo": "apache/kylin",
        "commit_url": "https://github.com/apache/kylin/commit/c9a3140729da6d28ddb8359b035567d2c4d1eca3",
        "files": [
          "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java",
          "core-common/src/main/java/org/apache/kylin/common/QueryTrace.java",
          "kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/kylin/query/runtime/plans/ResultPlan.scala",
          "kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/kylin/query/util/SparkJobTrace.scala",
          "kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/kylin/query/util/SparkJobTraceV2.scala",
          "kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/spark/sql/metrics/AppStatus.scala"
        ],
        "message": "KYLIN-5111 Refine query metrics",
        "before_after_code_files": [
          "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java||core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java",
          "core-common/src/main/java/org/apache/kylin/common/QueryTrace.java||core-common/src/main/java/org/apache/kylin/common/QueryTrace.java",
          "kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/kylin/query/runtime/plans/ResultPlan.scala||kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/kylin/query/runtime/plans/ResultPlan.scala",
          "kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/kylin/query/util/SparkJobTrace.scala||kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/kylin/query/util/SparkJobTrace.scala",
          "kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/kylin/query/util/SparkJobTraceV2.scala||kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/kylin/query/util/SparkJobTraceV2.scala",
          "kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/spark/sql/metrics/AppStatus.scala||kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/spark/sql/metrics/AppStatus.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/kylin/pull/1893",
          "https://github.com/apache/kylin/pull/2018",
          "https://github.com/apache/kylin/pull/2125",
          "https://github.com/apache/kylin/pull/2033",
          "https://github.com/apache/kylin/pull/2112",
          "https://github.com/apache/kylin/pull/2115",
          "https://github.com/apache/kylin/pull/1913",
          "https://github.com/apache/kylin/pull/2135"
        ],
        "olp_code_files": {
          "patch": [
            "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java||core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java"
          ],
          "candidate": [
            "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java||core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java"
          ]
        }
      },
      "candidate_diff": {
        "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java||core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java": [
          "File: core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java -> core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "3339:         return Boolean.parseBoolean(this.getOptional(\"kylin.query.auto-sparder-context-enabled\", \"false\"));",
          "3340:     }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "3342:     public Integer sparkQueryMetrics() {",
          "3343:         return Integer.parseInt(this.getOptional(\"kylin.query.spark.metrics\", \"2\"));",
          "3344:     }",
          "",
          "---------------"
        ],
        "core-common/src/main/java/org/apache/kylin/common/QueryTrace.java||core-common/src/main/java/org/apache/kylin/common/QueryTrace.java": [
          "File: core-common/src/main/java/org/apache/kylin/common/QueryTrace.java -> core-common/src/main/java/org/apache/kylin/common/QueryTrace.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "34:     public static final String WAIT_FOR_EXECUTION = \"WAIT_FOR_EXECUTION\";",
          "35:     public static final String EXECUTION = \"EXECUTION\";",
          "36:     public static final String FETCH_RESULT = \"FETCH_RESULT\";",
          "39:     static final String PREPARATION = \"PREPARATION\";",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "37:     public static final String CALCULATE_STAT = \"CALCULATE_STAT\";",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/kylin/query/runtime/plans/ResultPlan.scala||kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/kylin/query/runtime/plans/ResultPlan.scala": [
          "File: kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/kylin/query/runtime/plans/ResultPlan.scala -> kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/kylin/query/runtime/plans/ResultPlan.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "27: import org.apache.kylin.common.util.HadoopUtil",
          "28: import org.apache.kylin.metadata.project.ProjectManager",
          "29: import org.apache.kylin.query.runtime.plans.ResultType.ResultType",
          "31: import org.apache.spark.internal.Logging",
          "32: import org.apache.spark.sql.{DataFrame, SparderContext}",
          "33: import org.apache.spark.sql.hive.utils.QueryMetricUtils",
          "34: import org.apache.spark.sql.utils.SparkTypeUtil",
          "35: import org.apache.spark.utils.SparderUtils",
          "37: import scala.collection.JavaConverters._",
          "",
          "[Removed Lines]",
          "30: import org.apache.kylin.query.util.SparkJobTrace",
          "",
          "[Added Lines]",
          "30: import org.apache.kylin.query.util.{AbstractSparkJobTrace, SparkJobTrace, SparkJobTraceV2}",
          "37: import java.util.TimeZone",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "107:       interruptOnCancel = true)",
          "108:     val currentTrace = QueryContextFacade.current().getQueryTrace",
          "109:     currentTrace.endLastSpan()",
          "111:     try {",
          "112:       val rows = df.collect()",
          "113:       jobTrace.jobFinished()",
          "",
          "[Removed Lines]",
          "110:     val jobTrace = new SparkJobTrace(jobGroup, currentTrace, sparkContext)",
          "",
          "[Added Lines]",
          "112:     val jobTrace = if(kylinConfig.sparkQueryMetrics == 2) {",
          "113:       new SparkJobTraceV2(jobGroup, currentTrace, sparkContext, TimeZone.getTimeZone(kylinConfig.getTimeZone).toZoneId)",
          "114:     } else if(kylinConfig.sparkQueryMetrics == 1)  {",
          "115:       new SparkJobTrace(jobGroup, currentTrace, sparkContext)",
          "116:     } else {",
          "117:       new AbstractSparkJobTrace()",
          "118:     }",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/kylin/query/util/SparkJobTrace.scala||kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/kylin/query/util/SparkJobTrace.scala": [
          "File: kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/kylin/query/util/SparkJobTrace.scala -> kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/kylin/query/util/SparkJobTrace.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "29: class SparkJobTrace(jobGroup: String,",
          "30:                     queryTrace: QueryTrace,",
          "31:                     sparkContext: SparkContext,",
          "34:   val appStatus = new AppStatus(sparkContext)",
          "",
          "[Removed Lines]",
          "32:                     startAt: Long = System.currentTimeMillis()) extends LogEx {",
          "",
          "[Added Lines]",
          "32:                     startAt: Long = System.currentTimeMillis()) extends AbstractSparkJobTrace {",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "57:     try {",
          "58:       val jobDataSeq = appStatus.getJobData(jobGroup)",
          "",
          "[Removed Lines]",
          "56:   def jobFinished(): Unit = {",
          "",
          "[Added Lines]",
          "56:   override def jobFinished(): Unit = {",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "116:   }",
          "117: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "119: class AbstractSparkJobTrace extends LogEx {",
          "120:   def jobFinished(): Unit = {}",
          "121: }",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/kylin/query/util/SparkJobTraceV2.scala||kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/kylin/query/util/SparkJobTraceV2.scala": [
          "File: kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/kylin/query/util/SparkJobTraceV2.scala -> kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/kylin/query/util/SparkJobTraceV2.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "19: package org.apache.kylin.query.util",
          "21: import org.apache.kylin.common.QueryTrace",
          "22: import org.apache.spark.SparkContext",
          "24: import java.time.ZoneId",
          "25: import java.time.format.DateTimeFormatter",
          "26: import java.util.{Date, Locale}",
          "28: class SparkJobTraceV2(jobGroup: String,",
          "29:                       queryTrace: QueryTrace,",
          "30:                       sparkContext: SparkContext,",
          "31:                       zoneId: ZoneId,",
          "32:                       detailEnabled: Boolean = false,",
          "33:                       startAt: Long = System.currentTimeMillis()) extends SparkJobTrace(jobGroup: String,",
          "34:   queryTrace: QueryTrace,",
          "35:   sparkContext: SparkContext,",
          "36:   startAt: Long) {",
          "38:   val PATTERN = \"HH:mm:ss.SSS\"",
          "39:   val bytesRead = \"bytesRead\"",
          "40:   val stageInfo = \"stageInfo\"",
          "41:   val runningTime = \"runningTime\"",
          "42:   val launchTime = \"launchTime\"",
          "45:   def dateStr(ts: Long): String = {",
          "46:     new Date(ts).toInstant.atZone(zoneId).toLocalDateTime.format(DateTimeFormatter.ofPattern(PATTERN, Locale.ROOT))",
          "47:   }",
          "49:   def dateStr(ts: java.util.Date): String = {",
          "50:     ts.toInstant.atZone(zoneId).toLocalDateTime.format(DateTimeFormatter.ofPattern(PATTERN, Locale.ROOT))",
          "51:   }",
          "53:   override def jobFinished(): Unit = {",
          "54:     logDebug(\"Query job finished.\")",
          "55:     val finishedAt = System.currentTimeMillis()",
          "56:     try {",
          "57:       val jobDataSeq = appStatus.getJobData(jobGroup)",
          "58:       val firstSubmissionTime = jobDataSeq.map(_.submissionTime).min",
          "59:       val lastCompletionTime = jobDataSeq.map(_.completionTime).max",
          "60:       var firstLaunchTime = -1L",
          "62:       if (jobDataSeq.isEmpty) {",
          "63:         endAbnormalExecutionTrace()",
          "64:         return",
          "65:       }",
          "67:       jobDataSeq.foreach(",
          "68:         job => {",
          "69:           val submissionTime = dateStr(job.submissionTime.getOrElse(new java.util.Date(0)))",
          "70:           val completionTime = dateStr(job.completionTime.getOrElse(new java.util.Date(0)))",
          "71:           val killedDesc = if (job.killedTasksSummary.isEmpty) {",
          "72:             \"EMPTY\"",
          "73:           } else {",
          "74:             job.killedTasksSummary.mkString(\"|\")",
          "75:           }",
          "76:           val stageMetrics = job.stageIds.flatMap(stageId => appStatus.calStageMetrics(stageId)).map(",
          "77:             stage => if (stage(bytesRead).nonEmpty && stage(runningTime).nonEmpty) {",
          "78:               if (firstLaunchTime == -1 || stage(launchTime).apply(0) < firstLaunchTime) firstLaunchTime = stage(launchTime).apply(0)",
          "79:               (\"Stage:%d,%d, launch time of first and last task are %s and %s, \" +",
          "80:                 \" bytesRead is [%s], runningTime is [%s]\").format(",
          "81:                 stage(stageInfo).apply(0),",
          "82:                 stage(stageInfo).apply(1),",
          "83:                 dateStr(stage(launchTime).apply(0)),",
          "84:                 dateStr(stage(launchTime).apply(1)),",
          "85:                 stage(bytesRead).mkString(\",\"),",
          "86:                 stage(runningTime).mkString(\", \")",
          "87:               )",
          "88:             } else {",
          "89:               (\"Stage:%d,%d\").format(stage(stageInfo).apply(0), stage(stageInfo).apply(1))",
          "90:             }).mkString(\";\")",
          "92:           logInfo(",
          "93:             s\"Job ${job.jobId} is submitted at ${submissionTime} and completed at ${completionTime}.It has ${job.numTasks} tasks, \" +",
          "94:               s\"succeed ${job.numCompletedTasks} tasks, ${job.numFailedTasks} failed tasks,${job.numKilledTasks} killed tasks. \" +",
          "95:               s\"Killed tasks info: ${killedDesc}. Stages ${stageMetrics}.\")",
          "96:         }",
          "97:       )",
          "99:       var jobExecutionTimeByKylin = finishedAt - startAt",
          "100:       if (firstSubmissionTime.isDefined) {",
          "101:         queryTrace.amendLast(QueryTrace.PREPARE_AND_SUBMIT_JOB, firstSubmissionTime.get.getTime)",
          "102:       } else {",
          "103:         logInfo(\"No firstSubmissionTime\")",
          "104:       }",
          "105:       if (firstSubmissionTime.isDefined && lastCompletionTime.isDefined) {",
          "106:         val jobExecutionTimeBySpark = lastCompletionTime.get.getTime - firstSubmissionTime.get.getTime",
          "107:         logInfo(s\"jobExecutionTime will change from ${jobExecutionTimeByKylin} to ${jobExecutionTimeBySpark} .\")",
          "108:         logInfo(s\"Kylin submit query at ${dateStr(startAt)}\" +",
          "109:           s\", first spark job submitted at ${dateStr(firstSubmissionTime.get.getTime)}\" +",
          "110:           s\", last spark job finished at ${dateStr(lastCompletionTime.get.getTime)}\" +",
          "111:           s\", query finished at ${dateStr(finishedAt)} .\")",
          "112:         jobExecutionTimeByKylin = jobExecutionTimeBySpark",
          "114:         queryTrace.appendSpan(QueryTrace.WAIT_FOR_EXECUTION, firstLaunchTime - firstSubmissionTime.get.getTime)",
          "115:         queryTrace.appendSpan(QueryTrace.EXECUTION, lastCompletionTime.get.getTime - firstLaunchTime)",
          "116:         queryTrace.appendSpan(QueryTrace.FETCH_RESULT, finishedAt.longValue - lastCompletionTime.get.getTime)",
          "117:         queryTrace.appendSpan(QueryTrace.CALCULATE_STAT, System.currentTimeMillis - finishedAt)",
          "118:       } else {",
          "119:         logInfo(\"No firstSubmissionTime or lastCompletionTime\")",
          "120:       }",
          "121:     } catch {",
          "122:       case e =>",
          "123:         logWarning(s\"Failed trace spark job execution for $jobGroup\", e)",
          "124:         endAbnormalExecutionTrace()",
          "125:     }",
          "126:   }",
          "127: }",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/spark/sql/metrics/AppStatus.scala||kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/spark/sql/metrics/AppStatus.scala": [
          "File: kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/spark/sql/metrics/AppStatus.scala -> kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/spark/sql/metrics/AppStatus.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "19: package org.apache.spark.sql.metrics",
          "22: import org.apache.spark.status.{TaskDataWrapper, TaskIndexNames}",
          "23: import org.apache.spark.util.Utils",
          "28:   def getTaskLaunchTime(stageId: Int, quantile: Double): Double = {",
          "29:     scanTasks(stageId, TaskIndexNames.LAUNCH_TIME, quantile) { t => t.launchTime }",
          "",
          "[Removed Lines]",
          "21: import org.apache.spark.{SparkContext, SparkStageInfo}",
          "24: import org.apache.spark.status.api.v1",
          "26: class AppStatus(sparkContext: SparkContext) {",
          "",
          "[Added Lines]",
          "21: import org.apache.spark.status.api.v1",
          "24: import org.apache.spark.utils.LogEx",
          "25: import org.apache.spark.{SparkContext, SparkStageInfo}",
          "27: class AppStatus(sparkContext: SparkContext) extends LogEx {",
          "29:   val defaultUnsortedQuantiles: Array[Double] = Array(0, 0.25, 0.5, 0.75, 1.0)",
          "31:   def calStageMetrics(stageId: Int, unsortedQuantiles: Array[Double] = defaultUnsortedQuantiles):",
          "32:   Seq[Map[String, IndexedSeq[Long]]] = {",
          "33:     val stageDataList: Seq[v1.StageData] = sparkContext.statusStore.stageData(stageId)",
          "34:     stageDataList.map(s => {",
          "37:       val stageKey = Array(s.stageId, s.attemptId)",
          "38:       val taskIterator = sparkContext.statusStore.store.view(classOf[TaskDataWrapper])",
          "39:         .parent(stageKey)",
          "40:         .index(TaskIndexNames.LAUNCH_TIME)",
          "41:         .iterator()",
          "43:       var firstLaunchTime = -1L",
          "44:       var lastLaunchTime = -1L",
          "45:       while (taskIterator.hasNext) {",
          "46:         val lt = taskIterator.next().launchTime",
          "47:         if (firstLaunchTime == -1) {",
          "48:           firstLaunchTime = lt",
          "49:         } else if (lt < firstLaunchTime) {",
          "50:           firstLaunchTime = lt",
          "51:         }",
          "52:         if (lastLaunchTime == -1) {",
          "53:           lastLaunchTime = lt",
          "54:         } else if (lt > lastLaunchTime) {",
          "55:           lastLaunchTime = lt",
          "56:         }",
          "57:       }",
          "60:       val metricsDistribution = sparkContext.statusStore.taskSummary(s.stageId, s.attemptId, unsortedQuantiles)",
          "61:       val bytesRead: IndexedSeq[Long] = if (metricsDistribution.nonEmpty) {",
          "62:         metricsDistribution.get.inputMetrics.bytesRead.map(x => x.toLong)",
          "63:       } else {",
          "64:         IndexedSeq.empty",
          "65:       }",
          "67:       val runningTime: IndexedSeq[Long] = if (metricsDistribution.nonEmpty) {",
          "68:         metricsDistribution.get.executorRunTime.map(x => x.toLong)",
          "69:       } else {",
          "70:         IndexedSeq.empty",
          "71:       }",
          "73:       Map[String, IndexedSeq[Long]](",
          "74:         \"stageInfo\" -> IndexedSeq(s.stageId, s.attemptId),",
          "75:         \"launchTime\" -> IndexedSeq(firstLaunchTime, lastLaunchTime),",
          "76:         \"bytesRead\" -> bytesRead,",
          "77:         \"runningTime\" -> runningTime",
          "78:       )",
          "79:     }).seq",
          "80:   }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "ad7b47245c959ea3e57efe1c8f5a737d0b1653ed",
      "candidate_info": {
        "commit_hash": "ad7b47245c959ea3e57efe1c8f5a737d0b1653ed",
        "repo": "apache/kylin",
        "commit_url": "https://github.com/apache/kylin/commit/ad7b47245c959ea3e57efe1c8f5a737d0b1653ed",
        "files": [
          "build/bin/system-cube.sh",
          "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java",
          "core-metrics/src/main/java/org/apache/kylin/metrics/QuerySparkMetrics.java",
          "examples/test_case_data/localmeta/kylin.properties",
          "kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/spark/sql/metrics/SparderMetricsListener.scala",
          "server-base/src/main/java/org/apache/kylin/rest/init/InitialTaskManager.java",
          "server-base/src/main/java/org/apache/kylin/rest/metrics/QueryMetricsFacade.java",
          "server-base/src/main/java/org/apache/kylin/rest/response/SQLResponse.java",
          "server-base/src/main/java/org/apache/kylin/rest/service/DashboardService.java",
          "server-base/src/main/java/org/apache/kylin/rest/service/QueryService.java",
          "server/src/test/java/org/apache/kylin/rest/metrics/QueryMetricsTest.java",
          "tool/src/main/java/org/apache/kylin/tool/metrics/systemcube/CubeDescCreator.java",
          "tool/src/main/java/org/apache/kylin/tool/metrics/systemcube/HiveTableCreator.java"
        ],
        "message": "add test case\n\nAdd metrics check",
        "before_after_code_files": [
          "build/bin/system-cube.sh||build/bin/system-cube.sh",
          "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java||core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java",
          "core-metrics/src/main/java/org/apache/kylin/metrics/QuerySparkMetrics.java||core-metrics/src/main/java/org/apache/kylin/metrics/QuerySparkMetrics.java",
          "examples/test_case_data/localmeta/kylin.properties||examples/test_case_data/localmeta/kylin.properties",
          "kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/spark/sql/metrics/SparderMetricsListener.scala||kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/spark/sql/metrics/SparderMetricsListener.scala",
          "server-base/src/main/java/org/apache/kylin/rest/init/InitialTaskManager.java||server-base/src/main/java/org/apache/kylin/rest/init/InitialTaskManager.java",
          "server-base/src/main/java/org/apache/kylin/rest/metrics/QueryMetricsFacade.java||server-base/src/main/java/org/apache/kylin/rest/metrics/QueryMetricsFacade.java",
          "server-base/src/main/java/org/apache/kylin/rest/response/SQLResponse.java||server-base/src/main/java/org/apache/kylin/rest/response/SQLResponse.java",
          "server-base/src/main/java/org/apache/kylin/rest/service/DashboardService.java||server-base/src/main/java/org/apache/kylin/rest/service/DashboardService.java",
          "server-base/src/main/java/org/apache/kylin/rest/service/QueryService.java||server-base/src/main/java/org/apache/kylin/rest/service/QueryService.java",
          "server/src/test/java/org/apache/kylin/rest/metrics/QueryMetricsTest.java||server/src/test/java/org/apache/kylin/rest/metrics/QueryMetricsTest.java",
          "tool/src/main/java/org/apache/kylin/tool/metrics/systemcube/CubeDescCreator.java||tool/src/main/java/org/apache/kylin/tool/metrics/systemcube/CubeDescCreator.java",
          "tool/src/main/java/org/apache/kylin/tool/metrics/systemcube/HiveTableCreator.java||tool/src/main/java/org/apache/kylin/tool/metrics/systemcube/HiveTableCreator.java"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/kylin/pull/1893",
          "https://github.com/apache/kylin/pull/2018",
          "https://github.com/apache/kylin/pull/2125",
          "https://github.com/apache/kylin/pull/2033",
          "https://github.com/apache/kylin/pull/2112",
          "https://github.com/apache/kylin/pull/2115",
          "https://github.com/apache/kylin/pull/1865",
          "https://github.com/apache/kylin/pull/1913",
          "https://github.com/apache/kylin/pull/2135"
        ],
        "olp_code_files": {
          "patch": [
            "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java||core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java"
          ],
          "candidate": [
            "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java||core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java"
          ]
        }
      },
      "candidate_diff": {
        "build/bin/system-cube.sh||build/bin/system-cube.sh": [
          "File: build/bin/system-cube.sh -> build/bin/system-cube.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "74:  cat <<-EOF > ${SINK_TOOLS_FILE}",
          "75:  [",
          "76:    [",
          "89:  ]",
          "90:  EOF",
          "91:   $KYLIN_HOME/bin/kylin.sh org.apache.kylin.tool.metrics.systemcube.SCCreator \\",
          "",
          "[Removed Lines]",
          "77:   \"org.apache.kylin.tool.metrics.systemcube.util.HiveSinkTool\",",
          "78:   {",
          "79:     \"storage_type\": 2,",
          "80:     \"cube_desc_override_properties\": [",
          "81:    \"java.util.HashMap\",",
          "82:    {",
          "83:      \"kylin.cube.algorithm\": \"INMEM\",",
          "84:      \"kylin.cube.max-building-segments\": \"1\"",
          "85:    }",
          "86:     ]",
          "87:   }",
          "88:    ]",
          "",
          "[Added Lines]",
          "77:     {",
          "78:        \"sink\": \"hive\",",
          "79:        \"storage_type\": 4,",
          "80:        \"cube_desc_override_properties\": {",
          "81:          \"kylin.cube.max-building-segments\": \"1\"",
          "82:        }",
          "83:     }",
          "84:     ]",
          "",
          "---------------"
        ],
        "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java||core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java": [
          "File: core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java -> core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "2364:     }",
          "2366:     public int getKylinMetricsCacheExpireSeconds() {",
          "2368:     }",
          "2370:     public int getKylinMetricsCacheMaxEntries() {",
          "",
          "[Removed Lines]",
          "2367:         return Integer.parseInt(this.getOptional(\"kylin.metrics.query-cache.expire-seconds\", \"600\"));",
          "",
          "[Added Lines]",
          "2367:         return Integer.parseInt(this.getOptional(\"kylin.metrics.query-cache.expire-seconds\", \"300\"));",
          "",
          "---------------"
        ],
        "core-metrics/src/main/java/org/apache/kylin/metrics/QuerySparkMetrics.java||core-metrics/src/main/java/org/apache/kylin/metrics/QuerySparkMetrics.java": [
          "File: core-metrics/src/main/java/org/apache/kylin/metrics/QuerySparkMetrics.java -> core-metrics/src/main/java/org/apache/kylin/metrics/QuerySparkMetrics.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "36: import java.util.Map;",
          "37: import java.util.concurrent.ConcurrentMap;",
          "38: import java.util.concurrent.Executors;",
          "39: import java.util.concurrent.TimeUnit;",
          "41: public class QuerySparkMetrics {",
          "42:     private static final Logger logger = LoggerFactory.getLogger(QuerySparkMetrics.class);",
          "44:     private static final int sparkMetricsNum = 10;",
          "45:     private org.apache.kylin.shaded.com.google.common.cache.Cache<String, QueryExecutionMetrics> queryExecutionMetricsMap;",
          "48:         queryExecutionMetricsMap = CacheBuilder.newBuilder()",
          "49:                 .maximumSize(KylinConfig.getInstanceFromEnv().getKylinMetricsCacheMaxEntries())",
          "50:                 .expireAfterWrite(KylinConfig.getInstanceFromEnv().getKylinMetricsCacheExpireSeconds(),",
          "51:                         TimeUnit.SECONDS)",
          "72:                 KylinConfig.getInstanceFromEnv().getKylinMetricsCacheExpireSeconds(),",
          "73:                 KylinConfig.getInstanceFromEnv().getKylinMetricsCacheExpireSeconds(), TimeUnit.SECONDS);",
          "75:     }",
          "77:     public static QuerySparkMetrics getInstance() {",
          "",
          "[Removed Lines]",
          "43:     private static final QuerySparkMetrics instance = new QuerySparkMetrics();",
          "47:     private QuerySparkMetrics() {",
          "52:                 .removalListener(new RemovalListener<String, QueryExecutionMetrics>() {",
          "53:                     @Override",
          "54:                     public void onRemoval(RemovalNotification<String, QueryExecutionMetrics> notification) {",
          "55:                         try {",
          "56:                             updateMetricsToReservoir(notification.getKey(), notification.getValue());",
          "57:                             logger.info(\"Query metrics {} is removed due to {}, update to metrics reservoir successful\",",
          "58:                                     notification.getKey(), notification.getCause());",
          "59:                         } catch(Exception e) {",
          "60:                             logger.warn(\"Query metrics {} is removed due to {}, update to metrics reservoir failed\",",
          "61:                                     notification.getKey(), notification.getCause());",
          "62:                         }",
          "63:                     }",
          "64:                 }).build();",
          "66:         Executors.newSingleThreadScheduledExecutor().scheduleWithFixedDelay(new Runnable() {",
          "67:                                                                                 @Override",
          "68:                                                                                 public void run() {",
          "69:                                                                                     queryExecutionMetricsMap.cleanUp();",
          "70:                                                                                 }",
          "71:                                                                             },",
          "",
          "[Added Lines]",
          "39: import java.util.concurrent.ScheduledExecutorService;",
          "44:     private static ScheduledExecutorService scheduledExecutor = null;",
          "45:     private static QuerySparkMetrics instance =",
          "46:             new QuerySparkMetrics(new QuerySparkMetricsRemovalListener());",
          "51:     private static class QuerySparkMetricsRemovalListener implements RemovalListener<String,",
          "52:             QueryExecutionMetrics> {",
          "53:         @Override",
          "54:         public void onRemoval(RemovalNotification<String, QueryExecutionMetrics> notification) {",
          "55:             try {",
          "56:                 updateMetricsToReservoir(notification.getKey(), notification.getValue());",
          "57:                 logger.info(\"Query metrics {} is removed due to {}, update to metrics reservoir successful\",",
          "58:                         notification.getKey(), notification.getCause());",
          "59:             } catch (Exception e) {",
          "60:                 logger.warn(\"Query metrics {} is removed due to {}, update to metrics reservoir failed\",",
          "61:                         notification.getKey(), notification.getCause());",
          "62:             }",
          "63:         }",
          "64:     }",
          "66:     private QuerySparkMetrics(RemovalListener removalListener) {",
          "67:         if (queryExecutionMetricsMap != null) {",
          "68:             queryExecutionMetricsMap.cleanUp();",
          "69:             queryExecutionMetricsMap = null;",
          "70:         }",
          "75:                 .removalListener(removalListener).build();",
          "77:         if (scheduledExecutor != null && !scheduledExecutor.isShutdown()) {",
          "78:             scheduledExecutor.shutdown();",
          "79:             scheduledExecutor = null;",
          "80:         }",
          "81:         scheduledExecutor = Executors.newSingleThreadScheduledExecutor();",
          "82:         scheduledExecutor.scheduleWithFixedDelay(new Runnable() {",
          "83:                                                      @Override",
          "84:                                                      public void run() {",
          "85:                                                          queryExecutionMetricsMap.cleanUp();",
          "86:                                                      }",
          "87:                                                  },",
          "90:     }",
          "93:     public static void init(RemovalListener removalListener) {",
          "94:         instance = new QuerySparkMetrics(removalListener);",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "159:         return queryExecutionMetricsMap.getIfPresent(queryId);",
          "160:     }",
          "175:         if (!KylinConfig.getInstanceFromEnv().isKylinMetricsReporterForQueryEnabled()) {",
          "176:             return;",
          "177:         }",
          "",
          "[Removed Lines]",
          "162:     public void setQueryRealization(String queryId, String realizationName, int realizationType, String cuboidIds) {",
          "163:         QueryExecutionMetrics queryExecutionMetrics = queryExecutionMetricsMap.getIfPresent(queryId);",
          "164:         if (queryExecutionMetrics != null) {",
          "165:             queryExecutionMetrics.setRealization(realizationName);",
          "166:             queryExecutionMetrics.setRealizationType(realizationType);",
          "167:             queryExecutionMetrics.setCuboidIds(cuboidIds);",
          "168:         }",
          "169:     }",
          "174:     public void updateMetricsToReservoir(String queryId, QueryExecutionMetrics queryExecutionMetrics) {",
          "",
          "[Added Lines]",
          "185:     public static void updateMetricsToReservoir(String queryId,",
          "186:                                                 QueryExecutionMetrics queryExecutionMetrics) {",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "187:             setSparkExecutionWrapper(queryExecutionMetricsEvent, queryExecutionMetrics.getSparderName(),",
          "188:                     queryExecutionMetrics.getExecutionId(), queryExecutionMetrics.getRealization(),",
          "190:                     queryExecutionMetrics.getStartTime(), queryExecutionMetrics.getEndTime());",
          "192:             setQueryMetrics(queryExecutionMetricsEvent, queryExecutionMetrics.getSqlDuration(),",
          "",
          "[Removed Lines]",
          "189:                     queryExecutionMetrics.getRealizationType(), queryExecutionMetrics.getCuboidIds(),",
          "",
          "[Added Lines]",
          "201:                     queryExecutionMetrics.getRealizationTypes(),",
          "202:                     queryExecutionMetrics.getCuboidIds(),",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "244:                     queryExecutionMetricsList[i] += sparkJobMetricsList[i];",
          "245:                 }",
          "246:             }",
          "248:                     queryExecutionMetricsList[0], queryExecutionMetricsList[1], queryExecutionMetricsList[2],",
          "249:                     queryExecutionMetricsList[3], queryExecutionMetricsList[4], queryExecutionMetricsList[5],",
          "250:                     queryExecutionMetricsList[6], queryExecutionMetricsList[7], queryExecutionMetricsList[8],",
          "",
          "[Removed Lines]",
          "247:             setSparkExecutionMetrics(queryExecutionMetricsEvent, queryExecutionMetrics.getExecutionDuration(),",
          "",
          "[Added Lines]",
          "260:             setSparkExecutionMetrics(queryExecutionMetricsEvent,",
          "261:                     queryExecutionMetrics.getEndTime() - queryExecutionMetrics.getStartTime(),",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "264:         metricsEvent.put(QuerySparkExecutionEnum.EXCEPTION.toString(), exception);",
          "265:     }",
          "269:         metricsEvent.put(QuerySparkExecutionEnum.SPARDER_NAME.toString(), sparderName);",
          "270:         metricsEvent.put(QuerySparkExecutionEnum.EXECUTION_ID.toString(), executionId);",
          "271:         metricsEvent.put(QuerySparkExecutionEnum.REALIZATION.toString(), realizationName);",
          "",
          "[Removed Lines]",
          "267:     private static void setSparkExecutionWrapper(RecordEvent metricsEvent, String sparderName, long executionId,",
          "268:             String realizationName, int realizationType, String cuboidIds, long startTime, long endTime) {",
          "",
          "[Added Lines]",
          "281:     private static void setSparkExecutionWrapper(RecordEvent metricsEvent, String sparderName,",
          "282:                                                  long executionId, String realizationName,",
          "283:                                                  String realizationType, String cuboidIds,",
          "284:                                                  long startTime, long endTime) {",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "365:         private long executionDuration;",
          "366:         private String queryId;",
          "367:         private String realization;",
          "369:         private String cuboidIds;",
          "370:         private long startTime;",
          "371:         private long endTime;",
          "",
          "[Removed Lines]",
          "368:         private int realizationType;",
          "",
          "[Added Lines]",
          "384:         private String realizationTypes;",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "512:             return realization;",
          "513:         }",
          "517:         }",
          "519:         public void setRealization(String realization) {",
          "520:             this.realization = realization;",
          "521:         }",
          "525:         }",
          "527:         public void setSparkJobMetricsMap(ConcurrentMap<Integer, SparkJobMetrics> sparkJobMetricsMap) {",
          "",
          "[Removed Lines]",
          "515:         public int getRealizationType() {",
          "516:             return realizationType;",
          "523:         public void setRealizationType(int realizationType) {",
          "524:             this.realizationType = realizationType;",
          "",
          "[Added Lines]",
          "531:         public String getRealizationTypes() {",
          "532:             return realizationTypes;",
          "539:         public void setRealizationTypes(String realizationTypes) {",
          "540:             this.realizationTypes = realizationTypes;",
          "",
          "---------------"
        ],
        "examples/test_case_data/localmeta/kylin.properties||examples/test_case_data/localmeta/kylin.properties": [
          "File: examples/test_case_data/localmeta/kylin.properties -> examples/test_case_data/localmeta/kylin.properties",
          "--- Hunk 1 ---",
          "[Context before]",
          "159: kylin.source.jdbc.user=",
          "160: kylin.source.jdbc.pass=",
          "",
          "[Removed Lines]",
          "162: kylin.query.auto-sparder-context=false",
          "",
          "[Added Lines]",
          "164: kylin.metrics.query-cache.expire-seconds=5",
          "165: kylin.metrics.query-cache.max-entries=2",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/spark/sql/metrics/SparderMetricsListener.scala||kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/spark/sql/metrics/SparderMetricsListener.scala": [
          "File: kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/spark/sql/metrics/SparderMetricsListener.scala -> kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/spark/sql/metrics/SparderMetricsListener.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "98:       val stageMetrics = stageInfo.taskMetrics",
          "99:       val sparkStageMetrics = new QuerySparkMetrics.SparkStageMetrics",
          "100:       sparkStageMetrics.setMetrics(stageMetrics.resultSize, stageMetrics.executorDeserializeCpuTime,",
          "102:         stageMetrics.jvmGCTime, stageMetrics.resultSerializationTime,",
          "103:         stageMetrics.memoryBytesSpilled, stageMetrics.diskBytesSpilled, stageMetrics.peakExecutionMemory)",
          "104:       queryExecutionMetrics.updateSparkStageMetrics(jobExecutionMap.apply(stageJobMap.apply(stageInfo.stageId)).queryId,",
          "",
          "[Removed Lines]",
          "101:         stageMetrics.executorDeserializeCpuTime, stageMetrics.executorRunTime, stageMetrics.executorCpuTime,",
          "",
          "[Added Lines]",
          "101:         stageMetrics.executorDeserializeTime, stageMetrics.executorRunTime, stageMetrics.executorCpuTime,",
          "",
          "---------------"
        ],
        "server-base/src/main/java/org/apache/kylin/rest/init/InitialTaskManager.java||server-base/src/main/java/org/apache/kylin/rest/init/InitialTaskManager.java": [
          "File: server-base/src/main/java/org/apache/kylin/rest/init/InitialTaskManager.java -> server-base/src/main/java/org/apache/kylin/rest/init/InitialTaskManager.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "21: import org.apache.commons.lang.StringUtils;",
          "22: import org.apache.kylin.common.KylinConfig;",
          "23: import org.apache.kylin.common.util.StringUtil;",
          "24: import org.apache.kylin.rest.metrics.QueryMetrics2Facade;",
          "25: import org.apache.kylin.rest.metrics.QueryMetricsFacade;",
          "26: import org.slf4j.Logger;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "24: import org.apache.kylin.metrics.QuerySparkMetrics;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "44:     private void runInitialTasks() {",
          "47:         QueryMetricsFacade.init();",
          "48:         QueryMetrics2Facade.init();",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "48:         QuerySparkMetrics.getInstance();",
          "",
          "---------------"
        ],
        "server-base/src/main/java/org/apache/kylin/rest/metrics/QueryMetricsFacade.java||server-base/src/main/java/org/apache/kylin/rest/metrics/QueryMetricsFacade.java": [
          "File: server-base/src/main/java/org/apache/kylin/rest/metrics/QueryMetricsFacade.java -> server-base/src/main/java/org/apache/kylin/rest/metrics/QueryMetricsFacade.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "93:             queryExecutionMetrics.setSqlIdCode(getSqlHashCode(sqlRequest.getSql()));",
          "94:             queryExecutionMetrics.setProject(norm(sqlRequest.getProject()));",
          "95:             queryExecutionMetrics.setQueryType(sqlResponse.isStorageCacheUsed() ? \"CACHE\" : \"PARQUET\");",
          "97:             queryExecutionMetrics.setSqlDuration(sqlResponse.getDuration());",
          "98:             queryExecutionMetrics.setTotalScanCount(sqlResponse.getTotalScanCount());",
          "99:             queryExecutionMetrics.setTotalScanBytes(sqlResponse.getTotalScanBytes());",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "96:             queryExecutionMetrics.setRealization(sqlResponse.getCube());",
          "97:             queryExecutionMetrics.setRealizationTypes(sqlResponse.getRealizationTypes());",
          "98:             queryExecutionMetrics.setCuboidIds(sqlResponse.getCuboidIds());",
          "",
          "---------------"
        ],
        "server-base/src/main/java/org/apache/kylin/rest/response/SQLResponse.java||server-base/src/main/java/org/apache/kylin/rest/response/SQLResponse.java": [
          "File: server-base/src/main/java/org/apache/kylin/rest/response/SQLResponse.java -> server-base/src/main/java/org/apache/kylin/rest/response/SQLResponse.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "49:     protected String cuboidIds;",
          "52:     protected int affectedRowCount;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "51:     protected String realizationTypes;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "286:         this.lazyQueryStartTime = lazyQueryStartTime;",
          "287:     }",
          "289:     @JsonIgnore",
          "290:     public List<QueryContext.CubeSegmentStatisticsResult> getCubeSegmentStatisticsList() {",
          "291:         try {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "291:     public String getRealizationTypes() {",
          "292:         return realizationTypes;",
          "293:     }",
          "295:     public void setRealizationTypes(String realizationTypes) {",
          "296:         this.realizationTypes = realizationTypes;",
          "297:     }",
          "",
          "---------------"
        ],
        "server-base/src/main/java/org/apache/kylin/rest/service/DashboardService.java||server-base/src/main/java/org/apache/kylin/rest/service/DashboardService.java": [
          "File: server-base/src/main/java/org/apache/kylin/rest/service/DashboardService.java -> server-base/src/main/java/org/apache/kylin/rest/service/DashboardService.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "32: import org.apache.kylin.metrics.lib.impl.TimePropertyEnum;",
          "33: import org.apache.kylin.metrics.property.JobPropertyEnum;",
          "34: import org.apache.kylin.metrics.property.QuerySparkExecutionEnum;",
          "36: import org.apache.kylin.rest.constant.Constant;",
          "37: import org.apache.kylin.rest.exception.BadRequestException;",
          "38: import org.apache.kylin.rest.request.PrepareSqlRequest;",
          "",
          "[Removed Lines]",
          "35: import org.apache.kylin.metrics.property.QuerySparkExecutionEnum;",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "server-base/src/main/java/org/apache/kylin/rest/service/QueryService.java||server-base/src/main/java/org/apache/kylin/rest/service/QueryService.java": [
          "File: server-base/src/main/java/org/apache/kylin/rest/service/QueryService.java -> server-base/src/main/java/org/apache/kylin/rest/service/QueryService.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "1180:         List<String> realizations = Lists.newLinkedList();",
          "1181:         StringBuilder cubeSb = new StringBuilder();",
          "1182:         StringBuilder cuboidIdsSb = new StringBuilder();",
          "1183:         StringBuilder logSb = new StringBuilder(\"Processed rows for each storageContext: \");",
          "1184:         QueryContext queryContext = QueryContextFacade.current();",
          "1185:         if (OLAPContext.getThreadLocalContexts() != null) { // contexts can be null in case of 'explain plan for'",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1183:         StringBuilder realizationTypeSb = new StringBuilder();",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1191:                     if (cubeSb.length() > 0) {",
          "1192:                         cubeSb.append(\",\");",
          "1193:                     }",
          "1194:                     Cuboid cuboid = ctx.storageContext.getCuboid();",
          "1195:                     if (cuboid != null) {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1195:                     cubeSb.append(ctx.realization.getCanonicalName());",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "1199:                         }",
          "1200:                         cuboidIdsSb.append(cuboid.getId());",
          "1201:                     }",
          "1203:                     logSb.append(ctx.storageContext.getProcessedRowCount()).append(\" \");",
          "1205:                     realizationName = ctx.realization.getName();",
          "1206:                     realizationType = ctx.realization.getStorageType();",
          "1208:                     realizations.add(realizationName);",
          "1209:                 }",
          "1212:             }",
          "1215:         }",
          "1216:         logger.info(logSb.toString());",
          "1218:         SQLResponse response = new SQLResponse(columnMetas, results, cubeSb.toString(), 0, isException,",
          "1219:                 exceptionMessage, isPartialResult, isPushDown);",
          "1220:         response.setCuboidIds(cuboidIdsSb.toString());",
          "1221:         response.setTotalScanCount(queryContext.getScannedRows());",
          "1222:         response.setTotalScanFiles((queryContext.getScanFiles() < 0) ? -1 :",
          "1223:                 queryContext.getScanFiles());",
          "",
          "[Removed Lines]",
          "1202:                     cubeSb.append(ctx.realization.getCanonicalName());",
          "1210:                 QuerySparkMetrics.getInstance().setQueryRealization(queryContext.getQueryId(), realizationName,",
          "1211:                         realizationType, cuboidIdsSb.toString());",
          "",
          "[Added Lines]",
          "1208:                     if (realizationTypeSb.length() > 0) {",
          "1209:                         realizationTypeSb.append(\",\");",
          "1210:                     }",
          "1211:                     realizationTypeSb.append(realizationType);",
          "1222:         response.setRealizationTypes(realizationTypeSb.toString());",
          "",
          "---------------"
        ],
        "server/src/test/java/org/apache/kylin/rest/metrics/QueryMetricsTest.java||server/src/test/java/org/apache/kylin/rest/metrics/QueryMetricsTest.java": [
          "File: server/src/test/java/org/apache/kylin/rest/metrics/QueryMetricsTest.java -> server/src/test/java/org/apache/kylin/rest/metrics/QueryMetricsTest.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "21: import java.lang.management.ManagementFactory;",
          "22: import java.util.ArrayList;",
          "23: import java.util.List;",
          "25: import javax.management.MBeanServer;",
          "26: import javax.management.ObjectName;",
          "28: import org.apache.kylin.common.QueryContext;",
          "29: import org.apache.kylin.common.QueryContextFacade;",
          "30: import org.apache.kylin.rest.request.SQLRequest;",
          "31: import org.apache.kylin.rest.response.SQLResponse;",
          "32: import org.apache.kylin.rest.service.ServiceTestBase;",
          "33: import org.junit.Assert;",
          "35: public class QueryMetricsTest extends ServiceTestBase {",
          "37:     private static MBeanServer mBeanServer;",
          "38:     private static ObjectName objectName;",
          "41:     public void setup() throws Exception {",
          "42:         super.setup();",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "24: import java.util.concurrent.atomic.AtomicInteger;",
          "29: import org.apache.commons.lang3.StringUtils;",
          "32: import org.apache.kylin.metrics.QuerySparkMetrics;",
          "36: import org.apache.kylin.shaded.com.google.common.cache.RemovalListener;",
          "37: import org.apache.kylin.shaded.com.google.common.cache.RemovalNotification;",
          "39: import org.junit.Before;",
          "40: import org.junit.Test;",
          "46:     private static AtomicInteger sparkMetricsReportCnt = new AtomicInteger(0);",
          "48:     @Before",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "45:         objectName = new ObjectName(\"Hadoop:service=Kylin,name=Server_Total\");",
          "46:     }",
          "49:     public void testQueryMetrics() throws Exception {",
          "50:         System.setProperty(\"kylin.server.query-metrics-enabled\", \"true\");",
          "51:         QueryMetricsFacade.init();",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "56:     @Test",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "111:         System.clearProperty(\"kylin.server.query-metrics-enabled\");",
          "112:     }",
          "115:     public void testQueryStatisticsResult() throws Exception {",
          "116:         System.setProperty(\"kylin.metrics.reporter-query-enabled\", \"true\");",
          "117:         QueryMetricsFacade.init();",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "122:     @Test",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "153:         System.clearProperty(\"kylin.server.query-metrics-enabled\");",
          "154:         System.out.println(\"------------testQueryStatisticsResult done------------\");",
          "155:     }",
          "156: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "165:     @Test",
          "166:     public void testQuerySparkMetrics() throws Exception {",
          "167:         sparkMetricsReportCnt.set(0);",
          "168:         System.setProperty(\"kylin.server.query-metrics-enabled\", \"true\");",
          "169:         QuerySparkMetrics.init(new QuerySparkMetricsTestRemovalListener());",
          "170:         QueryMetricsFacade.init();",
          "172:         SQLRequest sqlRequest = new SQLRequest();",
          "173:         sqlRequest.setSql(\"select * from TEST_KYLIN_FACT\");",
          "174:         sqlRequest.setProject(\"default\");",
          "176:         String queryId1 = \"1\";",
          "177:         generateSparkMetrics(queryId1);",
          "179:         SQLResponse sqlResponse = new SQLResponse();",
          "180:         sqlResponse.setDuration(10);",
          "181:         sqlResponse.setCube(\"test_cube\");",
          "182:         sqlResponse.setCuboidIds(\"12345\");",
          "183:         sqlResponse.setRealizationTypes(\"4\");",
          "184:         sqlResponse.setIsException(false);",
          "185:         sqlResponse.setTotalScanCount(100);",
          "186:         List<String> list1 = new ArrayList<>();",
          "187:         list1.add(\"111\");",
          "188:         list1.add(\"112\");",
          "189:         List<String> list2 = new ArrayList<>();",
          "190:         list2.add(\"111\");",
          "191:         list2.add(\"112\");",
          "192:         List<List<String>> results = new ArrayList<>();",
          "193:         results.add(list1);",
          "194:         results.add(list2);",
          "195:         sqlResponse.setResults(results);",
          "196:         sqlResponse.setStorageCacheUsed(true);",
          "198:         QueryMetricsFacade.updateMetrics(queryId1, sqlRequest, sqlResponse);",
          "200:         Thread.sleep(3000);",
          "202:         updateSparkMetrics(queryId1);",
          "204:         Assert.assertTrue(QuerySparkMetrics.getInstance().getQueryExecutionMetrics(queryId1) != null);",
          "205:         Assert.assertEquals(1L, mBeanServer.getAttribute(objectName, \"QueryCount\"));",
          "206:         Assert.assertEquals(1L, mBeanServer.getAttribute(objectName, \"QuerySuccessCount\"));",
          "207:         Assert.assertEquals(0L, mBeanServer.getAttribute(objectName, \"QueryFailCount\"));",
          "208:         Assert.assertEquals(1L, mBeanServer.getAttribute(objectName, \"CacheHitCount\"));",
          "210:         Assert.assertEquals(1L, mBeanServer.getAttribute(objectName, \"ScanRowCountNumOps\"));",
          "211:         Assert.assertEquals(100.0, mBeanServer.getAttribute(objectName, \"ScanRowCountAvgTime\"));",
          "212:         Assert.assertEquals(100.0, mBeanServer.getAttribute(objectName, \"ScanRowCountMaxTime\"));",
          "213:         Assert.assertEquals(100.0, mBeanServer.getAttribute(objectName, \"ScanRowCountMinTime\"));",
          "215:         Assert.assertEquals(1L, mBeanServer.getAttribute(objectName, \"ResultRowCountNumOps\"));",
          "216:         Assert.assertEquals(2.0, mBeanServer.getAttribute(objectName, \"ResultRowCountMaxTime\"));",
          "217:         Assert.assertEquals(2.0, mBeanServer.getAttribute(objectName, \"ResultRowCountAvgTime\"));",
          "218:         Assert.assertEquals(2.0, mBeanServer.getAttribute(objectName, \"ResultRowCountMinTime\"));",
          "220:         Assert.assertEquals(1L, mBeanServer.getAttribute(objectName, \"QueryLatencyNumOps\"));",
          "221:         Assert.assertEquals(10.0, mBeanServer.getAttribute(objectName, \"QueryLatencyMaxTime\"));",
          "222:         Assert.assertEquals(10.0, mBeanServer.getAttribute(objectName, \"QueryLatencyAvgTime\"));",
          "223:         Assert.assertEquals(10.0, mBeanServer.getAttribute(objectName, \"QueryLatencyMinTime\"));",
          "225:         String queryId2 = \"2\";",
          "226:         generateSparkMetrics(queryId2);",
          "228:         Thread.sleep(3000);",
          "230:         Assert.assertTrue(QuerySparkMetrics.getInstance().getQueryExecutionMetrics(queryId1) == null);",
          "231:         Assert.assertTrue(QuerySparkMetrics.getInstance().getQueryExecutionMetrics(queryId2) != null);",
          "233:         updateSparkMetrics(queryId2);",
          "235:         SQLResponse sqlResponse2 = new SQLResponse();",
          "236:         sqlResponse2.setDuration(10);",
          "237:         sqlResponse2.setCube(\"test_cube\");",
          "238:         sqlResponse2.setIsException(true);",
          "240:         QueryMetricsFacade.updateMetrics(queryId2, sqlRequest, sqlResponse2);",
          "242:         Thread.sleep(5000);",
          "244:         Assert.assertEquals(2L, mBeanServer.getAttribute(objectName, \"QueryCount\"));",
          "245:         Assert.assertEquals(1L, mBeanServer.getAttribute(objectName, \"QuerySuccessCount\"));",
          "246:         Assert.assertEquals(1L, mBeanServer.getAttribute(objectName, \"QueryFailCount\"));",
          "248:         Assert.assertTrue(QuerySparkMetrics.getInstance().getQueryExecutionMetrics(queryId2) == null);",
          "249:         Assert.assertEquals(2, sparkMetricsReportCnt.get());",
          "250:         System.clearProperty(\"kylin.server.query-metrics-enabled\");",
          "251:     }",
          "253:     public void generateSparkMetrics(String queryId) {",
          "254:         Integer id = Integer.valueOf(queryId);",
          "255:         long executionStartTime = 0;",
          "256:         long jobStartTime= 0;",
          "257:         long submitTime = 0;",
          "259:         if (id == 1) {",
          "260:             executionStartTime = 1610609727972L;",
          "261:             jobStartTime = 1610609772880L;",
          "262:             submitTime = 1610610480942L;",
          "263:         } else if (id == 2) {",
          "264:             executionStartTime = 1610609876545L;",
          "265:             jobStartTime = 1610609901699L;",
          "266:             submitTime = 16106105016542L;",
          "267:         }",
          "268:         QuerySparkMetrics.getInstance().onJobStart(queryId, \"test-sparder_\" + id, id,",
          "269:                 executionStartTime, id, jobStartTime);",
          "270:         QuerySparkMetrics.getInstance().onSparkStageStart(queryId, id, id, \"stageType_\" + id,",
          "271:                 submitTime);",
          "272:     }",
          "274:     public void updateSparkMetrics(String queryId) {",
          "275:         Integer id = Integer.valueOf(queryId);",
          "276:         long jobEndTime = 0;",
          "277:         long executionEndTime = 0;",
          "278:         boolean jobIsSuccess = true;",
          "280:         QuerySparkMetrics.SparkStageMetrics queryStageMetrics = new QuerySparkMetrics.SparkStageMetrics();",
          "281:         if (id == 1) {",
          "282:             jobEndTime = 1610610734401L;",
          "283:             executionEndTime = 1610612655793L;",
          "284:             jobIsSuccess = true;",
          "285:             queryStageMetrics.setMetrics(10000, 10, 10, 100, 10, 1, 10, 1000, 1000, 100);",
          "286:         } else if (id == 2) {",
          "287:             jobEndTime = 1610610750397L;",
          "288:             executionEndTime = 1610612685275L;",
          "289:             jobIsSuccess = false;",
          "290:             queryStageMetrics.setMetrics(20000, 20, 20, 200, 20, 2, 20, 2000, 2000, 200);",
          "291:         }",
          "292:         QuerySparkMetrics.getInstance().updateSparkStageMetrics(queryId, id, id, true,",
          "293:                 queryStageMetrics);",
          "294:         QuerySparkMetrics.getInstance().updateSparkJobMetrics(queryId, id, jobEndTime, jobIsSuccess);",
          "295:         QuerySparkMetrics.getInstance().updateExecutionMetrics(queryId, executionEndTime);",
          "296:     }",
          "298:     public static void verifyQuerySparkMetrics(String queryId,",
          "299:                                                QuerySparkMetrics.QueryExecutionMetrics queryExecutionMetrics) {",
          "300:         sparkMetricsReportCnt.getAndIncrement();",
          "301:         Assert.assertTrue(StringUtils.isNotBlank(queryId));",
          "302:         Assert.assertTrue(queryExecutionMetrics != null);",
          "304:         int id = Integer.valueOf(queryId);",
          "305:         Assert.assertTrue(queryExecutionMetrics.getSparkJobMetricsMap().get(id) != null);",
          "306:         Assert.assertTrue(queryExecutionMetrics.getSparkJobMetricsMap().get(id) != null);",
          "307:         Assert.assertEquals(queryExecutionMetrics.getSparderName(), \"test-sparder_\" + id);",
          "308:         Assert.assertEquals(queryExecutionMetrics.getSparkJobMetricsMap().get(id).getJobId(), id);",
          "310:         if (id == 1) {",
          "312:             Assert.assertEquals(queryExecutionMetrics.getUser(), \"ADMIN\");",
          "313:             Assert.assertEquals(queryExecutionMetrics.getProject(), \"DEFAULT\");",
          "314:             Assert.assertEquals(queryExecutionMetrics.getQueryType(), \"CACHE\");",
          "315:             Assert.assertEquals(queryExecutionMetrics.getRealization(), \"test_cube\");",
          "316:             Assert.assertEquals(queryExecutionMetrics.getRealizationTypes(), \"4\");",
          "317:             Assert.assertEquals(queryExecutionMetrics.getCuboidIds(), \"12345\");",
          "318:             Assert.assertEquals(queryExecutionMetrics.getTotalScanCount(), 100);",
          "319:             Assert.assertEquals(queryExecutionMetrics.getResultCount(), 2);",
          "320:             Assert.assertEquals(queryExecutionMetrics.getException(), \"NULL\");",
          "322:             Assert.assertEquals(queryExecutionMetrics.getSparderName(), \"test-sparder_\" + id);",
          "323:             Assert.assertEquals(queryExecutionMetrics.getException(), \"NULL\");",
          "326:             Assert.assertEquals(queryExecutionMetrics.getStartTime(), 1610609727972L);",
          "327:             Assert.assertEquals(queryExecutionMetrics.getEndTime(), 1610612655793L);",
          "328:             Assert.assertTrue(queryExecutionMetrics.getSparkJobMetricsMap().get(id).isSuccess());",
          "329:             Assert.assertEquals(queryExecutionMetrics.getSparkJobMetricsMap().get(id).getStartTime(), 1610609772880L);",
          "330:             Assert.assertEquals(queryExecutionMetrics.getSparkJobMetricsMap().get(id).getEndTime(), 1610610734401L);",
          "333:             Assert.assertEquals(queryExecutionMetrics.getSparkJobMetricsMap().get(id)",
          "334:                     .getSparkStageMetricsMap().size(), 1);",
          "335:             Assert.assertTrue(queryExecutionMetrics.getSparkJobMetricsMap().get(id)",
          "336:                     .getSparkStageMetricsMap().get(id) != null);",
          "337:             Assert.assertEquals(queryExecutionMetrics.getSparkJobMetricsMap().get(id)",
          "338:                     .getSparkStageMetricsMap().get(id).getStageType(), \"stageType_\" + id);",
          "339:             Assert.assertEquals(queryExecutionMetrics.getSparkJobMetricsMap().get(id)",
          "340:                     .getSparkStageMetricsMap().get(id).getStageId(), id);",
          "341:             Assert.assertEquals(queryExecutionMetrics.getSparkJobMetricsMap().get(id)",
          "342:                     .getSparkStageMetricsMap().get(id).getResultSize(), 10000);",
          "343:             Assert.assertEquals(queryExecutionMetrics.getSparkJobMetricsMap().get(id)",
          "344:                     .getSparkStageMetricsMap().get(id).getExecutorDeserializeCpuTime(), 10);",
          "345:             Assert.assertEquals(queryExecutionMetrics.getSparkJobMetricsMap().get(id)",
          "346:                     .getSparkStageMetricsMap().get(id).getExecutorDeserializeTime(), 10);",
          "347:             Assert.assertEquals(queryExecutionMetrics.getSparkJobMetricsMap().get(id)",
          "348:                     .getSparkStageMetricsMap().get(id).getExecutorRunTime(), 100);",
          "349:             Assert.assertEquals(queryExecutionMetrics.getSparkJobMetricsMap().get(id)",
          "350:                     .getSparkStageMetricsMap().get(id).getExecutorCpuTime(), 10);",
          "351:             Assert.assertEquals(queryExecutionMetrics.getSparkJobMetricsMap().get(id)",
          "352:                     .getSparkStageMetricsMap().get(id).getJvmGCTime(), 1);",
          "353:             Assert.assertEquals(queryExecutionMetrics.getSparkJobMetricsMap().get(id)",
          "354:                     .getSparkStageMetricsMap().get(id).getResultSerializationTime(), 10);",
          "355:             Assert.assertEquals(queryExecutionMetrics.getSparkJobMetricsMap().get(id)",
          "356:                     .getSparkStageMetricsMap().get(id).getMemoryBytesSpilled(), 1000);",
          "357:             Assert.assertEquals(queryExecutionMetrics.getSparkJobMetricsMap().get(id)",
          "358:                     .getSparkStageMetricsMap().get(id).getDiskBytesSpilled(), 1000);",
          "359:             Assert.assertEquals(queryExecutionMetrics.getSparkJobMetricsMap().get(id)",
          "360:                     .getSparkStageMetricsMap().get(id).getPeakExecutionMemory(), 100);",
          "361:         } else if (id == 2) {",
          "363:             Assert.assertEquals(queryExecutionMetrics.getUser(), \"ADMIN\");",
          "364:             Assert.assertEquals(queryExecutionMetrics.getProject(), \"DEFAULT\");",
          "365:             Assert.assertEquals(queryExecutionMetrics.getQueryType(), \"PARQUET\");",
          "366:             Assert.assertEquals(queryExecutionMetrics.getRealization(), \"test_cube\");",
          "367:             Assert.assertEquals(queryExecutionMetrics.getRealizationTypes(), null);",
          "368:             Assert.assertEquals(queryExecutionMetrics.getCuboidIds(), null);",
          "369:             Assert.assertEquals(queryExecutionMetrics.getTotalScanCount(), 0);",
          "370:             Assert.assertEquals(queryExecutionMetrics.getResultCount(), 0);",
          "371:             Assert.assertEquals(queryExecutionMetrics.getException(), \"NULL\");",
          "374:             Assert.assertEquals(queryExecutionMetrics.getStartTime(), 1610609876545L);",
          "375:             Assert.assertEquals(queryExecutionMetrics.getEndTime(), 1610612685275L);",
          "376:             Assert.assertFalse(queryExecutionMetrics.getSparkJobMetricsMap().get(id).isSuccess());",
          "377:             Assert.assertEquals(queryExecutionMetrics.getSparkJobMetricsMap().get(id).getStartTime(), 1610609901699L);",
          "378:             Assert.assertEquals(queryExecutionMetrics.getSparkJobMetricsMap().get(id).getEndTime(), 1610610750397L);",
          "381:             Assert.assertEquals(queryExecutionMetrics.getSparkJobMetricsMap().get(id)",
          "382:                     .getSparkStageMetricsMap().size(), 1);",
          "383:             Assert.assertTrue(queryExecutionMetrics.getSparkJobMetricsMap().get(id)",
          "384:                     .getSparkStageMetricsMap().get(id) != null);",
          "385:             Assert.assertEquals(queryExecutionMetrics.getSparkJobMetricsMap().get(id)",
          "386:                     .getSparkStageMetricsMap().get(id).getStageId(), id);",
          "387:             Assert.assertEquals(queryExecutionMetrics.getSparkJobMetricsMap().get(id)",
          "388:                     .getSparkStageMetricsMap().get(id).getResultSize(), 20000);",
          "389:             Assert.assertEquals(queryExecutionMetrics.getSparkJobMetricsMap().get(id)",
          "390:                     .getSparkStageMetricsMap().get(id).getExecutorDeserializeCpuTime(), 20);",
          "391:             Assert.assertEquals(queryExecutionMetrics.getSparkJobMetricsMap().get(id)",
          "392:                     .getSparkStageMetricsMap().get(id).getExecutorDeserializeTime(), 20);",
          "393:             Assert.assertEquals(queryExecutionMetrics.getSparkJobMetricsMap().get(id)",
          "394:                     .getSparkStageMetricsMap().get(id).getExecutorRunTime(), 200);",
          "395:             Assert.assertEquals(queryExecutionMetrics.getSparkJobMetricsMap().get(id)",
          "396:                     .getSparkStageMetricsMap().get(id).getExecutorCpuTime(), 20);",
          "397:             Assert.assertEquals(queryExecutionMetrics.getSparkJobMetricsMap().get(id)",
          "398:                     .getSparkStageMetricsMap().get(id).getJvmGCTime(), 2);",
          "399:             Assert.assertEquals(queryExecutionMetrics.getSparkJobMetricsMap().get(id)",
          "400:                     .getSparkStageMetricsMap().get(id).getResultSerializationTime(), 20);",
          "401:             Assert.assertEquals(queryExecutionMetrics.getSparkJobMetricsMap().get(id)",
          "402:                     .getSparkStageMetricsMap().get(id).getMemoryBytesSpilled(), 2000);",
          "403:             Assert.assertEquals(queryExecutionMetrics.getSparkJobMetricsMap().get(id)",
          "404:                     .getSparkStageMetricsMap().get(id).getDiskBytesSpilled(), 2000);",
          "405:             Assert.assertEquals(queryExecutionMetrics.getSparkJobMetricsMap().get(id)",
          "406:                     .getSparkStageMetricsMap().get(id).getPeakExecutionMemory(), 200);",
          "407:         }",
          "408:     }",
          "410:     private static class QuerySparkMetricsTestRemovalListener implements RemovalListener<String,",
          "411:             QuerySparkMetrics.QueryExecutionMetrics> {",
          "412:         @Override",
          "413:         public void onRemoval(RemovalNotification<String, QuerySparkMetrics.QueryExecutionMetrics> notification) {",
          "414:             try {",
          "415:                 verifyQuerySparkMetrics(notification.getKey(), notification.getValue());",
          "416:             } catch (Exception e) {",
          "417:                 e.printStackTrace();",
          "418:             }",
          "419:         }",
          "420:     }",
          "",
          "---------------"
        ],
        "tool/src/main/java/org/apache/kylin/tool/metrics/systemcube/CubeDescCreator.java||tool/src/main/java/org/apache/kylin/tool/metrics/systemcube/CubeDescCreator.java": [
          "File: tool/src/main/java/org/apache/kylin/tool/metrics/systemcube/CubeDescCreator.java -> tool/src/main/java/org/apache/kylin/tool/metrics/systemcube/CubeDescCreator.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "467:         desc.setDimensions(dimensionDescList);",
          "468:         desc.setMeasures(measureDescList);",
          "469:         desc.setRowkey(rowKeyDesc);",
          "471:         desc.setNotifyList(Lists.<String> newArrayList());",
          "472:         desc.setStatusNeedNotify(Lists.newArrayList(JobStatusEnum.ERROR.toString()));",
          "473:         desc.setAutoMergeTimeRanges(new long[] { 86400000L, 604800000L, 2419200000L });",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "470:         desc.setHbaseMapping(hBaseMapping);",
          "",
          "---------------"
        ],
        "tool/src/main/java/org/apache/kylin/tool/metrics/systemcube/HiveTableCreator.java||tool/src/main/java/org/apache/kylin/tool/metrics/systemcube/HiveTableCreator.java": [
          "File: tool/src/main/java/org/apache/kylin/tool/metrics/systemcube/HiveTableCreator.java -> tool/src/main/java/org/apache/kylin/tool/metrics/systemcube/HiveTableCreator.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "130:         columns.add(new Pair<>(QuerySparkExecutionEnum.SPARDER_NAME.toString(), HiveTypeEnum.HSTRING.toString()));",
          "131:         columns.add(new Pair<>(QuerySparkExecutionEnum.PROJECT.toString(), HiveTypeEnum.HSTRING.toString()));",
          "132:         columns.add(new Pair<>(QuerySparkExecutionEnum.REALIZATION.toString(), HiveTypeEnum.HSTRING.toString()));",
          "134:         columns.add(new Pair<>(QuerySparkExecutionEnum.CUBOID_IDS.toString(), HiveTypeEnum.HSTRING.toString()));",
          "136:         columns.add(new Pair<>(QuerySparkExecutionEnum.TYPE.toString(), HiveTypeEnum.HSTRING.toString()));",
          "",
          "[Removed Lines]",
          "133:         columns.add(new Pair<>(QuerySparkExecutionEnum.REALIZATION_TYPE.toString(), HiveTypeEnum.HINT.toString()));",
          "",
          "[Added Lines]",
          "133:         columns.add(new Pair<>(QuerySparkExecutionEnum.REALIZATION_TYPE.toString(), HiveTypeEnum.HSTRING.toString()));",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "a6f78cd53b951acf3ec966fa537dae3f2b82fd1e",
      "candidate_info": {
        "commit_hash": "a6f78cd53b951acf3ec966fa537dae3f2b82fd1e",
        "repo": "apache/kylin",
        "commit_url": "https://github.com/apache/kylin/commit/a6f78cd53b951acf3ec966fa537dae3f2b82fd1e",
        "files": [
          "core-common/src/main/java/org/apache/kylin/common/annotation/Clarification.java",
          "core-cube/src/main/java/org/apache/kylin/cube/CubeSegment.java",
          "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/application/SparkApplication.java",
          "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/builder/NBuildSourceInfo.java",
          "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CubeBuildJob.java",
          "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CuboidAggregator.scala",
          "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CuboidStatistics.scala",
          "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/ParentSourceChooser.scala",
          "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/ResourceDetectBeforeCubingJob.java",
          "kylin-spark-project/kylin-spark-metadata/src/main/scala/org/apache/kylin/engine/spark/metadata/MetaData.scala",
          "kylin-spark-project/kylin-spark-metadata/src/main/scala/org/apache/kylin/engine/spark/metadata/MetadataConverter.scala"
        ],
        "message": "KYLIN-4818 Calculate cuboid rowcount via HLL",
        "before_after_code_files": [
          "core-common/src/main/java/org/apache/kylin/common/annotation/Clarification.java||core-common/src/main/java/org/apache/kylin/common/annotation/Clarification.java",
          "core-cube/src/main/java/org/apache/kylin/cube/CubeSegment.java||core-cube/src/main/java/org/apache/kylin/cube/CubeSegment.java",
          "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/application/SparkApplication.java||kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/application/SparkApplication.java",
          "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/builder/NBuildSourceInfo.java||kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/builder/NBuildSourceInfo.java",
          "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CubeBuildJob.java||kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CubeBuildJob.java",
          "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CuboidAggregator.scala||kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CuboidAggregator.scala",
          "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CuboidStatistics.scala||kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CuboidStatistics.scala",
          "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/ParentSourceChooser.scala||kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/ParentSourceChooser.scala",
          "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/ResourceDetectBeforeCubingJob.java||kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/ResourceDetectBeforeCubingJob.java",
          "kylin-spark-project/kylin-spark-metadata/src/main/scala/org/apache/kylin/engine/spark/metadata/MetaData.scala||kylin-spark-project/kylin-spark-metadata/src/main/scala/org/apache/kylin/engine/spark/metadata/MetaData.scala",
          "kylin-spark-project/kylin-spark-metadata/src/main/scala/org/apache/kylin/engine/spark/metadata/MetadataConverter.scala||kylin-spark-project/kylin-spark-metadata/src/main/scala/org/apache/kylin/engine/spark/metadata/MetadataConverter.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/kylin/pull/1893",
          "https://github.com/apache/kylin/pull/2018",
          "https://github.com/apache/kylin/pull/2125",
          "https://github.com/apache/kylin/pull/2033",
          "https://github.com/apache/kylin/pull/2112",
          "https://github.com/apache/kylin/pull/2115",
          "https://github.com/apache/kylin/pull/1865",
          "https://github.com/apache/kylin/pull/1913",
          "https://github.com/apache/kylin/pull/2135"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "core-common/src/main/java/org/apache/kylin/common/annotation/Clarification.java||core-common/src/main/java/org/apache/kylin/common/annotation/Clarification.java": [
          "File: core-common/src/main/java/org/apache/kylin/common/annotation/Clarification.java -> core-common/src/main/java/org/apache/kylin/common/annotation/Clarification.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "30:         ElementType.CONSTRUCTOR, ElementType.LOCAL_VARIABLE, ElementType.PACKAGE})",
          "31: public @interface Clarification {",
          "37:     enum Priority {",
          "38:         MINOR,",
          "",
          "[Removed Lines]",
          "33:     Priority priority();",
          "35:     String msg() default \"N/A\";",
          "",
          "[Added Lines]",
          "33:     Priority priority() default Priority.MINOR;",
          "35:     String msg() default \"null\";",
          "37:     boolean deprecated() default false; // Please remove deprecated key when Kylin4 GA",
          "",
          "---------------"
        ],
        "core-cube/src/main/java/org/apache/kylin/cube/CubeSegment.java||core-cube/src/main/java/org/apache/kylin/cube/CubeSegment.java": [
          "File: core-cube/src/main/java/org/apache/kylin/cube/CubeSegment.java -> core-cube/src/main/java/org/apache/kylin/cube/CubeSegment.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "31: import java.util.concurrent.ConcurrentHashMap;",
          "33: import org.apache.kylin.common.KylinConfig;",
          "34: import org.apache.kylin.common.persistence.ResourceStore;",
          "35: import org.apache.kylin.common.util.Dictionary;",
          "36: import org.apache.kylin.common.util.Pair;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "34: import org.apache.kylin.common.annotation.Clarification;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "75:     @JsonProperty(\"date_range_end\")",
          "76:     private long dateRangeEnd;",
          "77:     @JsonProperty(\"source_offset_start\")",
          "78:     private long sourceOffsetStart;",
          "79:     @JsonProperty(\"source_offset_end\")",
          "80:     private long sourceOffsetEnd;",
          "81:     @JsonProperty(\"status\")",
          "82:     private SegmentStatusEnum status;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "79:     @Clarification(deprecated = true)",
          "82:     @Clarification(deprecated = true)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "107:     private String binarySignature; // a hash of cube schema and dictionary ID, used for sanity check",
          "109:     @JsonProperty(\"dictionaries\")",
          "110:     private ConcurrentHashMap<String, String> dictionaries; // table/column ==> dictionary resource path",
          "111:     @JsonProperty(\"snapshots\")",
          "112:     private ConcurrentHashMap<String, String> snapshots; // table name ==> snapshot resource path",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "113:     @Clarification(deprecated = true)",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "117:     @JsonProperty(\"source_partition_offset_start\")",
          "118:     @JsonInclude(JsonInclude.Include.NON_EMPTY)",
          "119:     private Map<Integer, Long> sourcePartitionOffsetStart = Maps.newHashMap();",
          "121:     @JsonProperty(\"source_partition_offset_end\")",
          "122:     @JsonInclude(JsonInclude.Include.NON_EMPTY)",
          "123:     private Map<Integer, Long> sourcePartitionOffsetEnd = Maps.newHashMap();",
          "125:     @JsonProperty(\"stream_source_checkpoint\")",
          "126:     private String streamSourceCheckpoint;",
          "128:     @JsonProperty(\"additionalInfo\")",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "123:     @Clarification(deprecated = true)",
          "128:     @Clarification(deprecated = true)",
          "132:     @Clarification(deprecated = true)",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "530:     }",
          "532:     public String getStatisticsResourcePath() {",
          "534:     }",
          "536:     public static String getStatisticsResourcePath(String cubeName, String cubeSegmentId) {",
          "538:     }",
          "540:     @Override",
          "",
          "[Removed Lines]",
          "533:         return getStatisticsResourcePath(this.getCubeInstance().getName(), this.getUuid());",
          "537:         return ResourceStore.CUBE_STATISTICS_ROOT + \"/\" + cubeName + \"/\" + cubeSegmentId + \".seq\";",
          "",
          "[Added Lines]",
          "540:         return getStatisticsResourcePath(this.getCubeInstance().getName(), this.getUuid(), \".seq\");",
          "541:     }",
          "543:     public String getPreciseStatisticsResourcePath() {",
          "544:         return getStatisticsResourcePath(this.getCubeInstance().getName(), this.getUuid(), \".json\");",
          "548:         return getStatisticsResourcePath(cubeName, cubeSegmentId, \".seq\");",
          "549:     }",
          "551:     public static String getStatisticsResourcePath(String cubeName, String cubeSegmentId, String suffix) {",
          "552:         return ResourceStore.CUBE_STATISTICS_ROOT + \"/\" + cubeName + \"/\" + cubeSegmentId + suffix;",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/application/SparkApplication.java||kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/application/SparkApplication.java": [
          "File: kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/application/SparkApplication.java -> kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/application/SparkApplication.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "219:         jobId = getParam(MetadataConstants.P_JOB_ID);",
          "220:         project = getParam(MetadataConstants.P_PROJECT_NAME);",
          "221:         if (getParam(MetadataConstants.P_CUBOID_NUMBER) != null) {",
          "223:         }",
          "224:         try (KylinConfig.SetAndUnsetThreadLocalConfig autoCloseConfig = KylinConfig",
          "225:                 .setAndUnsetThreadLocalConfig(MetaDumpUtil.loadKylinConfigFromHdfs(hdfsMetalUrl))) {",
          "",
          "[Removed Lines]",
          "222:             layoutSize = Integer.valueOf(getParam(MetadataConstants.P_CUBOID_NUMBER));",
          "",
          "[Added Lines]",
          "222:             layoutSize = Integer.parseInt(getParam(MetadataConstants.P_CUBOID_NUMBER));",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/builder/NBuildSourceInfo.java||kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/builder/NBuildSourceInfo.java": [
          "File: kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/builder/NBuildSourceInfo.java -> kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/builder/NBuildSourceInfo.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "34: public class NBuildSourceInfo {",
          "35:     protected static final Logger logger = LoggerFactory.getLogger(NBuildSourceInfo.class);",
          "38:     private String viewFactTablePath;",
          "39:     private SparkSession ss;",
          "40:     private long byteSize;",
          "",
          "[Removed Lines]",
          "37:     private Dataset<Row> flattableDS;",
          "",
          "[Added Lines]",
          "37:     private Dataset<Row> flatTableDS;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "52:         this.byteSize = byteSize;",
          "53:     }",
          "57:     }",
          "61:     }",
          "63:     public Dataset<Row> getParentDS() {",
          "",
          "[Removed Lines]",
          "55:     public void setFlattableDS(Dataset<Row> flattableDS) {",
          "56:         this.flattableDS = flattableDS;",
          "59:     public Dataset<Row> getFlattableDS() {",
          "60:         return flattableDS;",
          "",
          "[Added Lines]",
          "55:     public void setFlatTableDS(Dataset<Row> flatTableDS) {",
          "56:         this.flatTableDS = flatTableDS;",
          "59:     public Dataset<Row> getFlatTableDS() {",
          "60:         return flatTableDS;",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "66:             Preconditions.checkNotNull(ss, \"SparkSession is null is NBuildSourceInfo.\");",
          "67:             return ss.read().parquet(parentStoragePath);",
          "68:         } else {",
          "72:         }",
          "73:     }",
          "",
          "[Removed Lines]",
          "69:             Preconditions.checkState(flattableDS != null, \"Path and DS can no be empty at the same time.\");",
          "70:             logger.info(\"parent storage path not exists, use flattable dataset.\");",
          "71:             return flattableDS;",
          "",
          "[Added Lines]",
          "69:             Preconditions.checkState(flatTableDS != null, \"Path and DS can no be empty at the same time.\");",
          "70:             logger.info(\"parent storage path not exists, use flatTable dataset.\");",
          "71:             return flatTableDS;",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CubeBuildJob.java||kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CubeBuildJob.java": [
          "File: kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CubeBuildJob.java -> kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CubeBuildJob.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "59: import org.apache.kylin.metadata.MetadataConstants;",
          "60: import org.apache.kylin.metadata.model.IStorageAware;",
          "61: import org.apache.kylin.storage.StorageFactory;",
          "62: import org.apache.spark.sql.Dataset;",
          "63: import org.apache.spark.sql.Row;",
          "64: import org.apache.spark.sql.hive.utils.ResourceDetectUtils;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "62: import org.apache.spark.api.java.JavaRDD;",
          "63: import org.apache.spark.api.java.JavaSparkContext;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "69: import org.apache.kylin.shaded.com.google.common.collect.Lists;",
          "70: import org.apache.kylin.shaded.com.google.common.collect.Sets;",
          "72: import scala.collection.JavaConversions;",
          "74: public class CubeBuildJob extends SparkApplication {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "74: import scala.Tuple2;",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "114:                 ParentSourceChooser sourceChooser = new ParentSourceChooser(spanningTree, seg, jobId, ss, config, true);",
          "115:                 sourceChooser.decideSources();",
          "116:                 NBuildSourceInfo buildFromFlatTable = sourceChooser.flatTableSource();",
          "117:                 Map<Long, NBuildSourceInfo> buildFromLayouts = sourceChooser.reuseSources();",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "119:                 Tuple2<String, AggInfo>[] aggInfos =  sourceChooser.getAggInfo();",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "131:                 infos.recordSpanningTree(segId, spanningTree);",
          "133:                 logger.info(\"Updating segment info\");",
          "135:             }",
          "136:             updateSegmentSourceBytesSize(getParam(MetadataConstants.P_CUBE_ID),",
          "137:                     ResourceDetectUtils.getSegmentSourceSize(shareDir));",
          "",
          "[Removed Lines]",
          "134:                 updateSegmentInfo(getParam(MetadataConstants.P_CUBE_ID), seg, buildFromFlatTable.getFlattableDS().count());",
          "",
          "[Added Lines]",
          "138:                 assert buildFromFlatTable != null;",
          "139:                 updateSegmentInfo(getParam(MetadataConstants.P_CUBE_ID), seg, buildFromFlatTable.getFlatTableDS().count());",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "157:         List<CubeSegment> cubeSegments = Lists.newArrayList();",
          "158:         CubeSegment segment = cubeCopy.getSegmentById(segmentInfo.id());",
          "159:         segment.setSizeKB(segmentInfo.getAllLayoutSize() / 1024);",
          "160:         segment.setLastBuildTime(System.currentTimeMillis());",
          "161:         segment.setLastBuildJobID(getParam(MetadataConstants.P_JOB_ID));",
          "162:         segment.setInputRecords(sourceRowCount);",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "165:         List<String> cuboidStatics = new LinkedList<>();",
          "167:         String template = \"{\\\"cuboid\\\":%d, \\\"rows\\\": %d, \\\"size\\\": %d}\";",
          "168:         for (LayoutEntity layoutEntity : segmentInfo.getAllLayoutJava()) {",
          "169:             cuboidStatics.add(String.format(template, layoutEntity.getId(), layoutEntity.getRows(), layoutEntity.getByteSize()));",
          "170:         }",
          "172:         JavaSparkContext jsc = JavaSparkContext.fromSparkContext(ss.sparkContext());",
          "173:         JavaRDD<String> cuboidStatRdd = jsc.parallelize(cuboidStatics);",
          "174:         for (String cuboid : cuboidStatics) {",
          "175:             logger.info(\"Statistics \\t: {}\", cuboid);",
          "176:         }",
          "177:         String path = config.getHdfsWorkingDirectory() + segment.getPreciseStatisticsResourcePath();",
          "178:         logger.info(\"Saving {} {}\", path, segmentInfo);",
          "179:         try {",
          "180:             cuboidStatRdd.saveAsTextFile(path);",
          "181:         } catch (Exception e) {",
          "182:             logger.error(\"Error\", e);",
          "183:         }",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CuboidAggregator.scala||kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CuboidAggregator.scala": [
          "File: kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CuboidAggregator.scala -> kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CuboidAggregator.scala"
        ],
        "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CuboidStatistics.scala||kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CuboidStatistics.scala": [
          "File: kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CuboidStatistics.scala -> kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CuboidStatistics.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "19: package org.apache.kylin.engine.spark.job",
          "22: import org.apache.kylin.engine.spark.metadata.SegmentInfo",
          "23: import org.apache.kylin.measure.hllc.HLLCounter",
          "24: import org.apache.kylin.shaded.com.google.common.hash.{HashFunction, Hashing}",
          "25: import org.apache.spark.sql.{Dataset, Row}",
          "27: import scala.collection.mutable",
          "30: object CuboidStatistics {",
          "32:   def sample(inputDs: Dataset[Row], seg: SegmentInfo): Array[(String, AggInfo)] = {",
          "33:     seg.getAllLayout.map(x => x.getId)",
          "34:     val rkc = seg.allColumns.count(c => c.rowKey)",
          "35:     val res = inputDs.rdd //.sample(withReplacement = false, 0.3)",
          "36:       .mapPartitions(new CuboidStatistics(seg.getAllLayout.map(x => x.getId), rkc).statisticsInPartition)",
          "37:     val l = res.map(a => (a.key, a)).reduceByKey((a, b) => a.merge(b)).collect()",
          "38:     l.foreach(x => println(x._1 + \" >>><<< \" + x._2.cuboid.counter.getCountEstimate))",
          "39:     l",
          "40:   }",
          "41: }",
          "43: class CuboidStatistics(ids: List[Long], rkc: Int) extends Serializable {",
          "44:   private val info = mutable.Map[String, AggInfo]()",
          "45:   val allCuboidsBitSet: Array[Array[Integer]] = getCuboidBitSet(ids, rkc)",
          "46:   private val hf: HashFunction = Hashing.murmur3_128",
          "47:   private val rowHashCodesLong = new Array[Long](rkc)",
          "49:   def statisticsInPartition(rows: Iterator[Row]): Iterator[AggInfo] = {",
          "50:     init()",
          "51:     rows.foreach(update)",
          "52:     info.valuesIterator",
          "53:   }",
          "55:   def init(): Unit = {",
          "56:     ids.foreach(i => info.put(i.toString, AggInfo(i.toString)))",
          "57:   }",
          "59:   def update(r: Row): Unit = {",
          "60:     println(r)",
          "61:     updateCuboid(r)",
          "62:   }",
          "64:   def updateCuboid(r: Row): Unit = {",
          "67:     var idx = 0",
          "68:     while (idx < rkc) {",
          "69:       val hc = hf.newHasher",
          "70:       var colValue = r.get(idx).toString",
          "71:       if (colValue == null) colValue = \"0\"",
          "73:       rowHashCodesLong(idx) = hc.putUnencodedChars(colValue).hash().padToLong() + idx",
          "74:       idx += 1",
          "75:     }",
          "78:     val n = allCuboidsBitSet.length",
          "79:     idx = 0",
          "80:     while (idx < n) {",
          "81:       var value: Long = 0",
          "82:       var position = 0",
          "83:       while (position < allCuboidsBitSet(idx).length) {",
          "84:         value += rowHashCodesLong(allCuboidsBitSet(idx)(position))",
          "85:         position += 1",
          "86:       }",
          "87:       info(ids(idx).toString).cuboid.counter.addHashDirectly(value)",
          "88:       idx += 1",
          "89:     }",
          "90:   }",
          "92:   def getCuboidBitSet(cuboidIds: List[Long], nRowKey: Int): Array[Array[Integer]] = {",
          "93:     val allCuboidsBitSet: Array[Array[Integer]] = new Array[Array[Integer]](cuboidIds.length)",
          "94:     var j: Int = 0",
          "95:     while (j < cuboidIds.length) {",
          "96:       val cuboidId: Long = cuboidIds(j)",
          "97:       allCuboidsBitSet(j) = new Array[Integer](java.lang.Long.bitCount(cuboidId))",
          "98:       var mask: Long = 1L << (nRowKey - 1)",
          "99:       var position: Int = 0",
          "100:       var i: Int = 0",
          "101:       while (i < nRowKey) {",
          "102:         if ((mask & cuboidId) > 0) {",
          "103:           allCuboidsBitSet(j)(position) = i",
          "104:           position += 1",
          "105:         }",
          "106:         mask = mask >> 1",
          "107:         i += 1",
          "108:       }",
          "109:       j += 1",
          "110:     }",
          "111:     allCuboidsBitSet",
          "112:   }",
          "113: }",
          "115: case class AggInfo(key: String,",
          "116:                    cuboid: CuboidInfo = CuboidInfo(new HLLCounter()),",
          "117:                    sample: SampleInfo = SampleInfo(),",
          "118:                    dimension: DimensionInfo = DimensionInfo()) {",
          "119:   def merge(o: AggInfo): AggInfo = {",
          "120:     this.cuboid.counter.merge(o.cuboid.counter)",
          "121:     this",
          "122:   }",
          "123: }",
          "125: case class CuboidInfo(counter: HLLCounter = new HLLCounter(14))",
          "127: case class SampleInfo(data: Array[String] = new Array(3))",
          "129: case class DimensionInfo(range: mutable.Map[String, String] = mutable.Map[String, String]())",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/ParentSourceChooser.scala||kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/ParentSourceChooser.scala": [
          "File: kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/ParentSourceChooser.scala -> kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/ParentSourceChooser.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "39:   config: KylinConfig,",
          "40:   needEncoding: Boolean) extends Logging {",
          "43:   var reuseSources: java.util.Map[java.lang.Long, NBuildSourceInfo] = Maps.newHashMap()",
          "46:   var flatTableSource: NBuildSourceInfo = _",
          "51:     MetadataConverter.getCubeDesc(seg.getCube),",
          "54:   def decideSources(): Unit = {",
          "55:     toBuildTree.getRootIndexEntities.asScala.foreach { entity =>",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "42:   var aggInfo : Array[(String, AggInfo)]  = _",
          "50:   var detectStep = false",
          "57:   def setDetectStep(): Unit =",
          "58:     detectStep = true",
          "60:   def getAggInfo : Array[(String, AggInfo)] = aggInfo",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "72:         builder.checkDupKey()",
          "73:         seg = builder.buildSnapshot",
          "74:       }",
          "76:     }",
          "77:     flatTableSource.addCuboid(entity)",
          "78:   }",
          "",
          "[Removed Lines]",
          "75:       flatTableSource = getFlatTable()",
          "",
          "[Added Lines]",
          "83:       flatTableSource = getFlatTable",
          "85:       val rowKeyColumns: Seq[String] = seg.allColumns.filter(c => c.rowKey).map(c => c.id.toString)",
          "86:       if (aggInfo == null && !detectStep) {",
          "87:         logInfo(\"Start  sampling ...\")",
          "88:         val coreDs = flatTableSource.getFlatTableDS.select(rowKeyColumns.head, rowKeyColumns.tail: _*)",
          "89:         aggInfo = CuboidStatistics.sample(coreDs, seg)",
          "90:         logInfo(\"Finish sampling ...\")",
          "91:         val statisticsStr = aggInfo.sortBy(x => x._1).map(x => x._1 + \":\" + x._2.cuboid.counter.getCountEstimate).mkString(\"\\n\")",
          "92:         logInfo(statisticsStr)",
          "93:       } else {",
          "94:         logInfo(\"Skip sampling ...\")",
          "95:       }",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "91:     var path = \"\"",
          "92:     if (flatTableSource != null && flatTableSource.getToBuildCuboids.size() > config.getPersistFlatTableThreshold) {",
          "95:       if (df.schema.nonEmpty) {",
          "96:         val allUsedCols = flatTableSource.getToBuildCuboids.asScala.flatMap { c =>",
          "97:           val dims = c.getOrderedDimensions.keySet().asScala.map(_.toString)",
          "",
          "[Removed Lines]",
          "94:       val df = flatTableSource.getFlattableDS",
          "",
          "[Added Lines]",
          "114:       val df = flatTableSource.getFlatTableDS",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "152:     buildSource",
          "153:   }",
          "157:     val sourceInfo = new NBuildSourceInfo",
          "158:     sourceInfo.setSparkSession(ss)",
          "",
          "[Removed Lines]",
          "155:   private def getFlatTable(): NBuildSourceInfo = {",
          "",
          "[Added Lines]",
          "175:   private def getFlatTable: NBuildSourceInfo = {",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "163:     val flatTable = new CreateFlatTable(seg, toBuildTree, ss, sourceInfo)",
          "164:     val afterJoin: Dataset[Row] = flatTable.generateDataset(needEncoding, true)",
          "167:     logInfo(\"No suitable ready layouts could be reused, generate dataset from flat table.\")",
          "168:     sourceInfo",
          "",
          "[Removed Lines]",
          "165:     sourceInfo.setFlattableDS(afterJoin)",
          "",
          "[Added Lines]",
          "185:     sourceInfo.setFlatTableDS(afterJoin)",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/ResourceDetectBeforeCubingJob.java||kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/ResourceDetectBeforeCubingJob.java": [
          "File: kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/ResourceDetectBeforeCubingJob.java -> kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/ResourceDetectBeforeCubingJob.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "58:             ResourceDetectUtils.write(new Path(config.getJobTmpShareDir(project, jobId), ResourceDetectUtils.countDistinctSuffix()),",
          "59:                     ResourceDetectUtils.findCountDistinctMeasure(JavaConversions.asJavaCollection(seg.toBuildLayouts())));",
          "60:             ParentSourceChooser datasetChooser = new ParentSourceChooser(spanningTree, seg, jobId, ss, config, false);",
          "61:             datasetChooser.decideSources();",
          "62:             NBuildSourceInfo buildFromFlatTable = datasetChooser.flatTableSource();",
          "63:             if (buildFromFlatTable != null) {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "61:             datasetChooser.setDetectStep();",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-metadata/src/main/scala/org/apache/kylin/engine/spark/metadata/MetaData.scala||kylin-spark-project/kylin-spark-metadata/src/main/scala/org/apache/kylin/engine/spark/metadata/MetaData.scala": [
          "File: kylin-spark-project/kylin-spark-metadata/src/main/scala/org/apache/kylin/engine/spark/metadata/MetaData.scala -> kylin-spark-project/kylin-spark-metadata/src/main/scala/org/apache/kylin/engine/spark/metadata/MetaData.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "31:                  val dataType: DataType,",
          "32:                  val tableName: String,",
          "33:                  val tableAliasName: String,",
          "35:   def identity: String = s\"$tableAliasName.$columnName\"",
          "37:   def isColumnType: Boolean = true",
          "38: }",
          "40: object ColumnDesc {",
          "43: }",
          "45: case class LiteralColumnDesc(override val columnName: String,",
          "",
          "[Removed Lines]",
          "34:                  val id: Int) extends Serializable {",
          "41:   def apply(columnName: String, dataType: DataType, tableName: String, tableAliasName: String, id: Int):",
          "42:   ColumnDesc = new ColumnDesc(columnName, dataType, tableName, tableAliasName, id)",
          "",
          "[Added Lines]",
          "34:                  val id: Int,",
          "35:                  val rowKey: Boolean = false) extends Serializable {",
          "42:   def apply(columnName: String, dataType: DataType, tableName: String, tableAliasName: String, id: Int, rowKey: Boolean):",
          "43:   ColumnDesc = new ColumnDesc(columnName, dataType, tableName, tableAliasName, id, rowKey)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "120:     snapshotInfo = tableInfo",
          "121:   }",
          "124:     layouts.map(_.getByteSize).sum",
          "125:   }",
          "128:     snapshotInfo.asJava",
          "129:   }",
          "",
          "[Removed Lines]",
          "123:   def getAllLayoutSize(): Long = {",
          "127:   def getSnapShot2JavaMap(): java.util.Map[String, String] = {",
          "",
          "[Added Lines]",
          "124:   def getAllLayoutSize: Long = {",
          "128:   def getAllLayout: List[LayoutEntity] = {",
          "129:     layouts",
          "130:   }",
          "132:   def getAllLayoutJava: java.util.List[LayoutEntity] = {",
          "133:     val l: util.LinkedList[LayoutEntity] = new java.util.LinkedList()",
          "134:     layouts.foreach(o => l.add(o))",
          "135:     l",
          "136:   }",
          "138:   def getSnapShot2JavaMap: java.util.Map[String, String] = {",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-metadata/src/main/scala/org/apache/kylin/engine/spark/metadata/MetadataConverter.scala||kylin-spark-project/kylin-spark-metadata/src/main/scala/org/apache/kylin/engine/spark/metadata/MetadataConverter.scala": [
          "File: kylin-spark-project/kylin-spark-metadata/src/main/scala/org/apache/kylin/engine/spark/metadata/MetadataConverter.scala -> kylin-spark-project/kylin-spark-metadata/src/main/scala/org/apache/kylin/engine/spark/metadata/MetadataConverter.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "110:     val colToIndex = columnIDTuples.toMap",
          "111:     columnIDTuples",
          "112:       .foreach { co =>",
          "114:       }",
          "115:     dimensionIndex",
          "116:   }",
          "",
          "[Removed Lines]",
          "113:         dimensionIndex.put(co._2, toColumnDesc(co._1, co._2))",
          "",
          "[Added Lines]",
          "113:         dimensionIndex.put(co._2, toColumnDesc(co._1, co._2, set.contains(co._1)))",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "160:     val dimensionMap = dimensionMapping.toMap",
          "161:     val shardByColumnsId = shardByColumns.asScala.toList",
          "166:     val set = dimensionMapping.map(_._1).toSet",
          "167:     val refs = cubeInstance.getAllColumns.asScala.diff(set)",
          "",
          "[Removed Lines]",
          "162:             .map(column => dimensionMap.get(column))",
          "163:             .filter(v => v != null)",
          "164:             .map(column => Integer.valueOf(column.get))",
          "",
          "[Added Lines]",
          "162:       .map(column => dimensionMap.get(column))",
          "163:       .filter(v => v != null)",
          "164:       .map(column => Integer.valueOf(column.get))",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "204:     extractEntityAndMeasures(cubeInstance)._1.asJava",
          "205:   }",
          "208:     val dataType = SparkTypeUtil.toSparkType(KyDataType.getType(ref.getDatatype))",
          "209:     val columnDesc = if (ref.getColumnDesc.isComputedColumn) {",
          "210:       ComputedColumnDesc(ref.getName, dataType, ref.getTableRef.getTableName, ref.getTableRef.getAlias,",
          "211:         index, ref.getExpressionInSourceDB)",
          "212:     } else {",
          "214:     }",
          "215:     columnDesc",
          "216:   }",
          "",
          "[Removed Lines]",
          "207:   private def toColumnDesc(ref: TblColRef, index: Int = -1) = {",
          "213:       ColumnDesc(ref.getName, dataType, ref.getTableRef.getTableName, ref.getTableRef.getAlias, index)",
          "",
          "[Added Lines]",
          "207:   private def toColumnDesc(ref: TblColRef, index: Int = -1, rowKey: Boolean = false) = {",
          "213:       ColumnDesc(ref.getName, dataType, ref.getTableRef.getTableName, ref.getTableRef.getAlias, index, rowKey)",
          "",
          "---------------"
        ]
      }
    }
  ]
}