{
  "cve_id": "CVE-2022-31777",
  "cve_desc": "A stored cross-site scripting (XSS) vulnerability in Apache Spark 3.2.1 and earlier, and 3.3.0, allows remote attackers to execute arbitrary JavaScript in the web browser of a user, by including a malicious payload into the logs which would be returned in logs rendered in the UI.",
  "repo": "apache/spark",
  "patch_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
  "patch_info": {
    "commit_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
    "repo": "apache/spark",
    "commit_url": "https://github.com/apache/spark/commit/ad90195de56688ce0898691eb9d04297ab0871ad",
    "files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ],
    "message": "[SPARK-39505][UI] Escape log content rendered in UI\n\n### What changes were proposed in this pull request?\n\nEscape log content rendered to the UI.\n\n### Why are the changes needed?\n\nLog content may contain reserved characters or other code in the log and be misinterpreted in the UI as HTML.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nExisting tests\n\nCloses #36902 from srowen/LogViewEscape.\n\nAuthored-by: Sean Owen <srowen@gmail.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
    "before_after_code_files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ]
  },
  "patch_diff": {
    "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js": [
      "File: core/src/main/resources/org/apache/spark/ui/static/log-view.js -> core/src/main/resources/org/apache/spark/ui/static/log-view.js",
      "--- Hunk 1 ---",
      "[Context before]",
      "85:       if (retStartByte == 0) {",
      "86:         disableMoreButton();",
      "87:       }",
      "90:       curLogLength = curLogLength + (startByte - retStartByte);",
      "91:       startByte = retStartByte;",
      "",
      "[Removed Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(cleanData);",
      "",
      "[Added Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(document.createTextNode(cleanData));",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "115:             var retLogLength = dataInfo[2];",
      "117:             var cleanData = data.substring(newlineIndex + 1);",
      "120:             curLogLength = curLogLength + (retEndByte - retStartByte);",
      "121:             endByte = retEndByte;",
      "",
      "[Removed Lines]",
      "118:             $(\"pre\", \".log-content\").append(cleanData);",
      "",
      "[Added Lines]",
      "118:             $(\"pre\", \".log-content\").append(document.createTextNode(cleanData));",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "7e1a329af28820b10381526e4adab5a53d7deeda",
      "candidate_info": {
        "commit_hash": "7e1a329af28820b10381526e4adab5a53d7deeda",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/7e1a329af28820b10381526e4adab5a53d7deeda",
        "files": [
          "docs/sql-ref-functions-builtin.md",
          "docs/sql-ref-functions.md",
          "sql/gen-sql-functions-docs.py"
        ],
        "message": "[SPARK-39577][SQL][DOCS] Add SQL reference for built-in functions\n\n### What changes were proposed in this pull request?\nCurrently, Spark SQL reference missing many functions.\nUsers cannot find the needed functions.\n\n### Why are the changes needed?\nAdd SQL reference for built-in functions\n\n### Does this PR introduce _any_ user-facing change?\n'Yes'.\nUsers can find needed functions in SQL reference.\nBefore this PR, the built-in functions show below.\n![image](https://user-images.githubusercontent.com/8486025/175505440-a375dbe6-988c-4647-836d-746c681be19a.png)\n\nAfter this PR, the built-in functions show below.\n![image](https://user-images.githubusercontent.com/8486025/175916778-d55ab1eb-fd28-4362-a0b7-7e33f37eacb4.png)\n\nThe part of Mathematical Functions show below.\n![image](https://user-images.githubusercontent.com/8486025/175760329-1b185eca-3d92-4c4f-ba2d-980200bf1a5b.png)\n![image](https://user-images.githubusercontent.com/8486025/175760340-e93ea083-90e7-4710-9bf4-f45c2b57f8bc.png)\n![image](https://user-images.githubusercontent.com/8486025/175760345-638b8fb2-d7f6-4e51-943e-7915583c03db.png)\n![image](https://user-images.githubusercontent.com/8486025/175760355-99d7125a-dcdb-407f-8c8d-6fbd2ca60801.png)\n\nThe part of String Functions show below.\n![image](https://user-images.githubusercontent.com/8486025/175760377-2e26454f-75d1-4ad4-9fdd-060a0460d439.png)\n![image](https://user-images.githubusercontent.com/8486025/175760387-125483a4-0f13-45f8-9e60-4c66f1f3dc6f.png)\n![image](https://user-images.githubusercontent.com/8486025/175760396-44d0f0b2-645b-408b-bdd7-ac167f98a30a.png)\n![image](https://user-images.githubusercontent.com/8486025/175760405-c44b0661-c73f-437f-aa99-113dd25bb3fb.png)\n![image](https://user-images.githubusercontent.com/8486025/175760416-66af659d-7db8-4c89-8686-9bf8bf1fec20.png)\n![image](https://user-images.githubusercontent.com/8486025/175760422-6105ea4a-5ae1-42ef-aee2-1a88984b2c10.png)\n![image](https://user-images.githubusercontent.com/8486025/175760428-5ab34058-ac78-48c6-aaa0-9a21b324822f.png)\n\nThe part of Bitwise Functions show below.\n![image](https://user-images.githubusercontent.com/8486025/175760440-10623eff-6330-4407-b069-f74e29966b64.png)\n\nThe part of Conversion Functions show below.\n![image](https://user-images.githubusercontent.com/8486025/175760458-384267f7-b7bb-44a7-8200-3c5e04dbd71a.png)\n\nThe part of Conditional Functions show below.\n![image](https://user-images.githubusercontent.com/8486025/175762759-7546e799-d530-47ea-ac22-27dabd1fcf4a.png)\n\nThe part of Predicate Functions show below.\n![image](https://user-images.githubusercontent.com/8486025/175760534-7a996e8e-188c-44de-b8e1-e36f346ae58e.png)\n![image](https://user-images.githubusercontent.com/8486025/175760545-96222945-0dbf-4885-b23e-3043791f13d5.png)\n\nThe part of Csv Functions show below.\n![image](https://user-images.githubusercontent.com/8486025/175760588-e0860cf4-0457-4f22-a21c-880c7ef92db2.png)\n\nThe part of Misc Functions show below.\n![image](https://user-images.githubusercontent.com/8486025/175760614-93b5d9da-fed3-41d6-84c5-4d207b2f1175.png)\n![image](https://user-images.githubusercontent.com/8486025/175760623-fb699de6-6174-496f-b15d-43155f223ee6.png)\n\nThe part of Generator Functions show below.\n![image](https://user-images.githubusercontent.com/8486025/175760569-5f14bf3b-5844-4821-acce-232f6ce21372.png)\n\n### How was this patch tested?\nN/A\n\nCloses #36976 from beliefer/SPARK-39577.\n\nAuthored-by: Jiaan Geng <beliefer@163.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\n(cherry picked from commit 250cb912215e548b965aa2d1a27affe9f924cac7)\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>",
        "before_after_code_files": [
          "sql/gen-sql-functions-docs.py||sql/gen-sql-functions-docs.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/gen-sql-functions-docs.py||sql/gen-sql-functions-docs.py": [
          "File: sql/gen-sql-functions-docs.py -> sql/gen-sql-functions-docs.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "31: groups = {",
          "32:     \"agg_funcs\", \"array_funcs\", \"datetime_funcs\",",
          "33:     \"json_funcs\", \"map_funcs\", \"window_funcs\",",
          "34: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "34:     \"math_funcs\", \"conditional_funcs\", \"generator_funcs\",",
          "35:     \"predicate_funcs\", \"string_funcs\", \"misc_funcs\",",
          "36:     \"bitwise_funcs\", \"conversion_funcs\", \"csv_funcs\",",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "46:     for jinfo in filter(lambda x: x.getGroup() in groups, jinfos):",
          "47:         name = jinfo.getName()",
          "48:         usage = jinfo.getUsage()",
          "49:         usage = usage.replace(\"_FUNC_\", name) if usage is not None else usage",
          "50:         infos.append(ExpressionInfo(",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "51:         if (name == \"raise_error\"):",
          "52:             continue",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "108:         # Expected formats are as follows;",
          "109:         #  - `_FUNC_(...) - description`, or",
          "110:         #  - `_FUNC_ - description`",
          "112:         for (sig, description) in zip(usages, usages):",
          "113:             result.append(\"    <tr>\")",
          "114:             result.append(\"      <td>%s</td>\" % sig)",
          "",
          "[Removed Lines]",
          "111:         usages = iter(re.split(r\"(%s.*) - \" % info.name, info.usage.strip())[1:])",
          "",
          "[Added Lines]",
          "116:         func_name = info.name",
          "117:         if (info.name == \"*\" or info.name == \"+\"):",
          "118:             func_name = \"\\\\\" + func_name",
          "119:         elif (info.name == \"when\"):",
          "120:             func_name = \"CASE WHEN\"",
          "121:         usages = iter(re.split(r\"(.*%s.*) - \" % func_name, info.usage.strip())[1:])",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "dd6eca7550c25dbcad9f12caf9fccfcad981d33f",
      "candidate_info": {
        "commit_hash": "dd6eca7550c25dbcad9f12caf9fccfcad981d33f",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/dd6eca7550c25dbcad9f12caf9fccfcad981d33f",
        "files": [
          "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFilterSuite.scala"
        ],
        "message": "[SPARK-38825][SQL][TEST][FOLLOWUP] Add test for in(null) and notIn(null)\n\n### What changes were proposed in this pull request?\nAdd test for filter `in(null)` and `notIn(null)`\n\n### Why are the changes needed?\nto make tests more complete\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\n\nnew test\n\nCloses #36248 from huaxingao/inNotIn.\n\nAuthored-by: huaxingao <huaxin_gao@apple.com>\nSigned-off-by: huaxingao <huaxin_gao@apple.com>\n(cherry picked from commit b760e4a686939bdb837402286b8d3d8b445c5ed4)\nSigned-off-by: huaxingao <huaxin_gao@apple.com>",
        "before_after_code_files": [
          "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFilterSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFilterSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFilterSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFilterSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFilterSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFilterSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "1905:   test(\"SPARK-38825: in and notIn filters\") {",
          "1906:     import testImplicits._",
          "1907:     withTempPath { file =>",
          "1909:         .parquet(file.getCanonicalPath)",
          "1910:       var df = spark.read.parquet(file.getCanonicalPath)",
          "1914:       checkAnswer(notIn, Seq(Row(1), Row(2), Row(0), Row(-1), Row(99), Row(1000), Row(7), Row(2)))",
          "1917:         .write.mode(\"overwrite\").parquet(file.getCanonicalPath)",
          "1918:       df = spark.read.parquet(file.getCanonicalPath)",
          "1919:       in = df.filter(col(\"name\").isin(\"mary\", \"victor\", \"leo\", \"alex\"))",
          "1920:       notIn = df.filter(!col(\"name\").isin(\"mary\", \"victor\", \"leo\", \"alex\"))",
          "1921:       checkAnswer(in, Seq(Row(\"mary\"), Row(\"alex\"), Row(\"mary\")))",
          "1922:       checkAnswer(notIn, Seq(Row(\"martin\"), Row(\"lucy\"), Row(\"dan\")))",
          "1923:     }",
          "1924:   }",
          "1925: }",
          "",
          "[Removed Lines]",
          "1908:       Seq(1, 2, 0, -1, 99, 1000, 3, 7, 2).toDF(\"id\").coalesce(1).write.mode(\"overwrite\")",
          "1911:       var in = df.filter(col(\"id\").isin(100, 3, 11, 12, 13))",
          "1912:       var notIn = df.filter(!col(\"id\").isin(100, 3, 11, 12, 13))",
          "1913:       checkAnswer(in, Seq(Row(3)))",
          "1916:       Seq(\"mary\", \"martin\", \"lucy\", \"alex\", \"mary\", \"dan\").toDF(\"name\").coalesce(1)",
          "",
          "[Added Lines]",
          "1908:       Seq(1, 2, 0, -1, 99, Integer.MAX_VALUE, 1000, 3, 7, Integer.MIN_VALUE, 2)",
          "1909:         .toDF(\"id\").coalesce(1).write.mode(\"overwrite\")",
          "1912:       var in = df.filter(col(\"id\").isin(100, 3, 11, 12, 13, Integer.MAX_VALUE, Integer.MIN_VALUE))",
          "1913:       var notIn =",
          "1914:         df.filter(!col(\"id\").isin(100, 3, 11, 12, 13, Integer.MAX_VALUE, Integer.MIN_VALUE))",
          "1915:       checkAnswer(in, Seq(Row(3), Row(-2147483648), Row(2147483647)))",
          "1918:       Seq(\"mary\", \"martin\", \"lucy\", \"alex\", null, \"mary\", \"dan\").toDF(\"name\").coalesce(1)",
          "1926:       in = df.filter(col(\"name\").isin(\"mary\", \"victor\", \"leo\", \"alex\", null))",
          "1927:       notIn = df.filter(!col(\"name\").isin(\"mary\", \"victor\", \"leo\", \"alex\", null))",
          "1928:       checkAnswer(in, Seq(Row(\"mary\"), Row(\"alex\"), Row(\"mary\")))",
          "1929:       checkAnswer(notIn, Seq())",
          "1931:       in = df.filter(col(\"name\").isin(null))",
          "1932:       notIn = df.filter(!col(\"name\").isin(null))",
          "1933:       checkAnswer(in, Seq())",
          "1934:       checkAnswer(notIn, Seq())",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "7f317c93383b51bd3ce163b9f8b481f2203760f7",
      "candidate_info": {
        "commit_hash": "7f317c93383b51bd3ce163b9f8b481f2203760f7",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/7f317c93383b51bd3ce163b9f8b481f2203760f7",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/CharVarcharTestSuite.scala"
        ],
        "message": "[SPARK-37643][SQL] when charVarcharAsString is true, for char datatype predicate query should skip rpadding rule\n\n### What changes were proposed in this pull request?\nafter add ApplyCharTypePadding rule, when predicate query column data type is char, if column value length is less then defined,  will be right-padding, then query will get incorrect result\n\n### Why are the changes needed?\nfix query incorrect issue when predicate column data type is char, so in this case when charVarcharAsString is true, we should skip the rpadding rule.\n\n### Does this PR introduce _any_ user-facing change?\nbefore this fix, if we query with char data type for predicate, then we should be careful to set charVarcharAsString to true.\n\n### How was this patch tested?\nadd new UT.\n\nCloses #36187 from fhygh/charpredicatequery.\n\nAuthored-by: fhygh <283452027@qq.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\n(cherry picked from commit c1ea8b446d00dd0123a0fad93a3e143933419a76)\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/CharVarcharTestSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/CharVarcharTestSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "4185:   }",
          "4187:   override def apply(plan: LogicalPlan): LogicalPlan = {",
          "4188:     plan.resolveOperatorsUpWithPruning(_.containsAnyPattern(BINARY_COMPARISON, IN)) {",
          "4189:       case operator => operator.transformExpressionsUpWithPruning(",
          "4190:         _.containsAnyPattern(BINARY_COMPARISON, IN)) {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "4188:     if (SQLConf.get.charVarcharAsString) {",
          "4189:       return plan",
          "4190:     }",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/CharVarcharTestSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/CharVarcharTestSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/CharVarcharTestSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/CharVarcharTestSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "100:     }",
          "101:   }",
          "103:   test(\"varchar type values length check and trim: partitioned columns\") {",
          "104:     (0 to 5).foreach { n =>",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "103:   test(\"char type values should not be padded when charVarcharAsString is true\") {",
          "104:     withSQLConf(SQLConf.LEGACY_CHAR_VARCHAR_AS_STRING.key -> \"true\") {",
          "105:       withTable(\"t\") {",
          "106:         sql(s\"CREATE TABLE t(a STRING, b CHAR(5), c CHAR(5)) USING $format partitioned by (c)\")",
          "107:         sql(\"INSERT INTO t VALUES ('abc', 'abc', 'abc')\")",
          "108:         checkAnswer(sql(\"SELECT b FROM t WHERE b='abc'\"), Row(\"abc\"))",
          "109:         checkAnswer(sql(\"SELECT b FROM t WHERE b in ('abc')\"), Row(\"abc\"))",
          "110:         checkAnswer(sql(\"SELECT c FROM t WHERE c='abc'\"), Row(\"abc\"))",
          "111:         checkAnswer(sql(\"SELECT c FROM t WHERE c in ('abc')\"), Row(\"abc\"))",
          "112:       }",
          "113:     }",
          "114:   }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "284954a12576076965b3322656e08d09d76094f3",
      "candidate_info": {
        "commit_hash": "284954a12576076965b3322656e08d09d76094f3",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/284954a12576076965b3322656e08d09d76094f3",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/SimplifyConditionalsInPredicate.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/rules/RuleIdCollection.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/SimplifyConditionalsInPredicateSuite.scala",
          "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q34.sf100/explain.txt",
          "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q34.sf100/simplified.txt",
          "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q34/explain.txt",
          "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q34/simplified.txt",
          "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q53.sf100/explain.txt",
          "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q53/explain.txt",
          "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q63.sf100/explain.txt",
          "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q63/explain.txt",
          "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q73.sf100/explain.txt",
          "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q73.sf100/simplified.txt",
          "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q73/explain.txt",
          "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q73/simplified.txt",
          "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q89.sf100/explain.txt",
          "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q89/explain.txt",
          "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q21.sf100/explain.txt",
          "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q21/explain.txt",
          "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q34.sf100/simplified.txt",
          "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q34/explain.txt",
          "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q34/simplified.txt",
          "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q39a.sf100/explain.txt",
          "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q39a.sf100/simplified.txt",
          "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q39a/explain.txt",
          "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q39a/simplified.txt",
          "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q39b.sf100/explain.txt",
          "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q39b.sf100/simplified.txt",
          "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q39b/explain.txt",
          "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q39b/simplified.txt",
          "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q47.sf100/explain.txt",
          "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q47/explain.txt",
          "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q53.sf100/explain.txt",
          "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q53/explain.txt",
          "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q57.sf100/explain.txt",
          "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q57/explain.txt",
          "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q63.sf100/explain.txt",
          "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q63/explain.txt",
          "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q73.sf100/explain.txt",
          "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q73.sf100/simplified.txt",
          "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q73/explain.txt",
          "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q73/simplified.txt",
          "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q89.sf100/explain.txt",
          "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q89/explain.txt",
          "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q34.sf100/explain.txt",
          "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q34.sf100/simplified.txt",
          "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q34/explain.txt",
          "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q34/simplified.txt",
          "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q47.sf100/explain.txt",
          "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q47/explain.txt",
          "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q57.sf100/explain.txt",
          "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q57/explain.txt"
        ],
        "message": "Revert \"[SPARK-33861][SQL] Simplify conditional in predicate\"\n\nThis reverts commit 32d4a2b and 3aa4e11.\n\nCloses #37729 from wangyum/SPARK-33861.\n\nAuthored-by: Yuming Wang <yumwang@ebay.com>\nSigned-off-by: Yuming Wang <yumwang@ebay.com>\n(cherry picked from commit 43cbdc6ec9dbcf9ebe0b48e14852cec4af18b4ec)\nSigned-off-by: Yuming Wang <yumwang@ebay.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/SimplifyConditionalsInPredicate.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/SimplifyConditionalsInPredicate.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/rules/RuleIdCollection.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/rules/RuleIdCollection.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/SimplifyConditionalsInPredicateSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/SimplifyConditionalsInPredicateSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "115:         RemoveDispensableExpressions,",
          "116:         SimplifyBinaryComparison,",
          "117:         ReplaceNullWithFalseInPredicate,",
          "119:         PruneFilters,",
          "120:         SimplifyCasts,",
          "121:         SimplifyCaseConversionExpressions,",
          "",
          "[Removed Lines]",
          "118:         SimplifyConditionalsInPredicate,",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/SimplifyConditionalsInPredicate.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/SimplifyConditionalsInPredicate.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/SimplifyConditionalsInPredicate.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/SimplifyConditionalsInPredicate.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/rules/RuleIdCollection.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/rules/RuleIdCollection.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/rules/RuleIdCollection.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/rules/RuleIdCollection.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "153:       \"org.apache.spark.sql.catalyst.optimizer.SimplifyCaseConversionExpressions\" ::",
          "154:       \"org.apache.spark.sql.catalyst.optimizer.SimplifyCasts\" ::",
          "155:       \"org.apache.spark.sql.catalyst.optimizer.SimplifyConditionals\" ::",
          "157:       \"org.apache.spark.sql.catalyst.optimizer.SimplifyExtractValueOps\" ::",
          "158:       \"org.apache.spark.sql.catalyst.optimizer.TransposeWindow\" ::",
          "159:       \"org.apache.spark.sql.catalyst.optimizer.UnwrapCastInBinaryComparison\" ::  Nil",
          "",
          "[Removed Lines]",
          "156:       \"org.apache.spark.sql.catalyst.optimizer.SimplifyConditionalsInPredicate\" ::",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/SimplifyConditionalsInPredicateSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/SimplifyConditionalsInPredicateSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/SimplifyConditionalsInPredicateSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/SimplifyConditionalsInPredicateSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "a3ea6b4e00df1a6a5712db5dd228819044c09dc9",
      "candidate_info": {
        "commit_hash": "a3ea6b4e00df1a6a5712db5dd228819044c09dc9",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/a3ea6b4e00df1a6a5712db5dd228819044c09dc9",
        "files": [
          "core/src/main/resources/error/error-classes.json",
          "project/MimaExcludes.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/DateTimeUtils.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/UTF8StringUtils.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/types/Decimal.scala",
          "sql/core/src/test/resources/sql-tests/inputs/cast.sql",
          "sql/core/src/test/resources/sql-tests/results/ansi/cast.sql.out",
          "sql/core/src/test/resources/sql-tests/results/ansi/date.sql.out",
          "sql/core/src/test/resources/sql-tests/results/ansi/datetime-parsing-invalid.sql.out",
          "sql/core/src/test/resources/sql-tests/results/ansi/interval.sql.out",
          "sql/core/src/test/resources/sql-tests/results/ansi/string-functions.sql.out",
          "sql/core/src/test/resources/sql-tests/results/cast.sql.out",
          "sql/core/src/test/resources/sql-tests/results/postgreSQL/boolean.sql.out",
          "sql/core/src/test/resources/sql-tests/results/postgreSQL/float4.sql.out",
          "sql/core/src/test/resources/sql-tests/results/postgreSQL/float8.sql.out",
          "sql/core/src/test/resources/sql-tests/results/postgreSQL/text.sql.out",
          "sql/core/src/test/resources/sql-tests/results/postgreSQL/window_part2.sql.out",
          "sql/core/src/test/resources/sql-tests/results/postgreSQL/window_part3.sql.out",
          "sql/core/src/test/resources/sql-tests/results/postgreSQL/window_part4.sql.out"
        ],
        "message": "[SPARK-38908][SQL] Provide query context in runtime error of Casting from String to Number/Date/Timestamp/Boolean\n\n### What changes were proposed in this pull request?\n\nProvide query context in runtime error of Casting from String to Number/Date/Timestamp/Boolean.\nCasting Double/Float to Timestamp shares the same error method as casting String to Timestamp, so this PR also provides query context in its error.\n\n### Why are the changes needed?\n\nProvide SQL query context of runtime errors to users, so that they can understand it better.\n\n### Does this PR introduce _any_ user-facing change?\n\nYes, improve the runtime error message of Casting from String to Number/Date/Timestamp/Boolean\n\n### How was this patch tested?\n\nUT\n\nCloses #36206 from gengliangwang/castStringContext.\n\nAuthored-by: Gengliang Wang <gengliang@apache.org>\nSigned-off-by: Gengliang Wang <gengliang@apache.org>\n(cherry picked from commit 49fa2e0720d3ca681d817981cbc2c7b811de2706)\nSigned-off-by: Gengliang Wang <gengliang@apache.org>",
        "before_after_code_files": [
          "project/MimaExcludes.scala||project/MimaExcludes.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/DateTimeUtils.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/DateTimeUtils.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/UTF8StringUtils.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/UTF8StringUtils.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/types/Decimal.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/types/Decimal.scala",
          "sql/core/src/test/resources/sql-tests/inputs/cast.sql||sql/core/src/test/resources/sql-tests/inputs/cast.sql"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "project/MimaExcludes.scala||project/MimaExcludes.scala": [
          "File: project/MimaExcludes.scala -> project/MimaExcludes.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "64:     ProblemFilters.exclude[MissingClassProblem](\"org.apache.spark.sql.connector.read.partitioning.ClusteredDistribution\"),",
          "65:     ProblemFilters.exclude[MissingClassProblem](\"org.apache.spark.sql.connector.read.partitioning.Distribution\"),",
          "66:     ProblemFilters.exclude[DirectMissingMethodProblem](\"org.apache.spark.sql.connector.read.partitioning.Partitioning.*\"),",
          "68:   )",
          "",
          "[Removed Lines]",
          "67:     ProblemFilters.exclude[ReversedMissingMethodProblem](\"org.apache.spark.sql.connector.read.partitioning.Partitioning.*\")",
          "",
          "[Added Lines]",
          "67:     ProblemFilters.exclude[ReversedMissingMethodProblem](\"org.apache.spark.sql.connector.read.partitioning.Partitioning.*\"),",
          "71:     ProblemFilters.exclude[DirectMissingMethodProblem](\"org.apache.spark.sql.types.Decimal.fromStringANSI\")",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "467:           false",
          "468:         } else {",
          "469:           if (ansiEnabled) {",
          "471:           } else {",
          "472:             null",
          "473:           }",
          "",
          "[Removed Lines]",
          "470:             throw QueryExecutionErrors.invalidInputSyntaxForBooleanError(s)",
          "",
          "[Added Lines]",
          "470:             throw QueryExecutionErrors.invalidInputSyntaxForBooleanError(s, origin.context)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "499:     case StringType =>",
          "500:       buildCast[UTF8String](_, utfs => {",
          "501:         if (ansiEnabled) {",
          "503:         } else {",
          "504:           DateTimeUtils.stringToTimestamp(utfs, zoneId).orNull",
          "505:         }",
          "",
          "[Removed Lines]",
          "502:           DateTimeUtils.stringToTimestampAnsi(utfs, zoneId)",
          "",
          "[Added Lines]",
          "502:           DateTimeUtils.stringToTimestampAnsi(utfs, zoneId, origin.context)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "525:     case DoubleType =>",
          "526:       if (ansiEnabled) {",
          "528:       } else {",
          "529:         buildCast[Double](_, d => doubleToTimestamp(d))",
          "530:       }",
          "532:     case FloatType =>",
          "533:       if (ansiEnabled) {",
          "535:       } else {",
          "536:         buildCast[Float](_, f => doubleToTimestamp(f.toDouble))",
          "537:       }",
          "",
          "[Removed Lines]",
          "527:         buildCast[Double](_, d => doubleToTimestampAnsi(d))",
          "534:         buildCast[Float](_, f => doubleToTimestampAnsi(f.toDouble))",
          "",
          "[Added Lines]",
          "527:         buildCast[Double](_, d => doubleToTimestampAnsi(d, origin.context))",
          "534:         buildCast[Float](_, f => doubleToTimestampAnsi(f.toDouble, origin.context))",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "541:     case StringType =>",
          "542:       buildCast[UTF8String](_, utfs => {",
          "543:         if (ansiEnabled) {",
          "545:         } else {",
          "546:           DateTimeUtils.stringToTimestampWithoutTimeZone(utfs).orNull",
          "547:         }",
          "",
          "[Removed Lines]",
          "544:           DateTimeUtils.stringToTimestampWithoutTimeZoneAnsi(utfs)",
          "",
          "[Added Lines]",
          "544:           DateTimeUtils.stringToTimestampWithoutTimeZoneAnsi(utfs, origin.context)",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "574:   private[this] def castToDate(from: DataType): Any => Any = from match {",
          "575:     case StringType =>",
          "576:       if (ansiEnabled) {",
          "578:       } else {",
          "579:         buildCast[UTF8String](_, s => DateTimeUtils.stringToDate(s).orNull)",
          "580:       }",
          "",
          "[Removed Lines]",
          "577:         buildCast[UTF8String](_, s => DateTimeUtils.stringToDateAnsi(s))",
          "",
          "[Added Lines]",
          "577:         buildCast[UTF8String](_, s => DateTimeUtils.stringToDateAnsi(s, origin.context))",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "632:   private[this] def castToLong(from: DataType): Any => Any = from match {",
          "633:     case StringType if ansiEnabled =>",
          "635:     case StringType =>",
          "636:       val result = new LongWrapper()",
          "637:       buildCast[UTF8String](_, s => if (s.toLong(result)) result.value else null)",
          "",
          "[Removed Lines]",
          "634:       buildCast[UTF8String](_, UTF8StringUtils.toLongExact)",
          "",
          "[Added Lines]",
          "634:       buildCast[UTF8String](_, v => UTF8StringUtils.toLongExact(v, origin.context))",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "655:   private[this] def castToInt(from: DataType): Any => Any = from match {",
          "656:     case StringType if ansiEnabled =>",
          "658:     case StringType =>",
          "659:       val result = new IntWrapper()",
          "660:       buildCast[UTF8String](_, s => if (s.toInt(result)) result.value else null)",
          "",
          "[Removed Lines]",
          "657:       buildCast[UTF8String](_, UTF8StringUtils.toIntExact)",
          "",
          "[Added Lines]",
          "657:       buildCast[UTF8String](_, v => UTF8StringUtils.toIntExact(v, origin.context))",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "687:   private[this] def castToShort(from: DataType): Any => Any = from match {",
          "688:     case StringType if ansiEnabled =>",
          "690:     case StringType =>",
          "691:       val result = new IntWrapper()",
          "692:       buildCast[UTF8String](_, s => if (s.toShort(result)) {",
          "",
          "[Removed Lines]",
          "689:       buildCast[UTF8String](_, UTF8StringUtils.toShortExact)",
          "",
          "[Added Lines]",
          "689:       buildCast[UTF8String](_, v => UTF8StringUtils.toShortExact(v, origin.context))",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "734:   private[this] def castToByte(from: DataType): Any => Any = from match {",
          "735:     case StringType if ansiEnabled =>",
          "737:     case StringType =>",
          "738:       val result = new IntWrapper()",
          "739:       buildCast[UTF8String](_, s => if (s.toByte(result)) {",
          "",
          "[Removed Lines]",
          "736:       buildCast[UTF8String](_, UTF8StringUtils.toByteExact)",
          "",
          "[Added Lines]",
          "736:       buildCast[UTF8String](_, v => UTF8StringUtils.toByteExact(v, origin.context))",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "815:         if (d == null) null else changePrecision(d, target)",
          "816:       })",
          "817:     case StringType if ansiEnabled =>",
          "819:     case BooleanType =>",
          "820:       buildCast[Boolean](_, b => toPrecision(if (b) Decimal.ONE else Decimal.ZERO, target))",
          "821:     case DateType =>",
          "",
          "[Removed Lines]",
          "818:       buildCast[UTF8String](_, s => changePrecision(Decimal.fromStringANSI(s), target))",
          "",
          "[Added Lines]",
          "818:       buildCast[UTF8String](_,",
          "819:         s => changePrecision(Decimal.fromStringANSI(s, origin.context), target))",
          "",
          "---------------",
          "--- Hunk 11 ---",
          "[Context before]",
          "844:           case _: NumberFormatException =>",
          "845:             val d = Cast.processFloatingPointSpecialLiterals(doubleStr, false)",
          "846:             if(ansiEnabled && d == null) {",
          "848:             } else {",
          "849:               d",
          "850:             }",
          "",
          "[Removed Lines]",
          "847:               throw QueryExecutionErrors.invalidInputSyntaxForNumericError(s)",
          "",
          "[Added Lines]",
          "848:               throw QueryExecutionErrors.invalidInputSyntaxForNumericError(s, origin.context)",
          "",
          "---------------",
          "--- Hunk 12 ---",
          "[Context before]",
          "869:           case _: NumberFormatException =>",
          "870:             val f = Cast.processFloatingPointSpecialLiterals(floatStr, true)",
          "871:             if (ansiEnabled && f == null) {",
          "873:             } else {",
          "874:               f",
          "875:             }",
          "",
          "[Removed Lines]",
          "872:               throw QueryExecutionErrors.invalidInputSyntaxForNumericError(s)",
          "",
          "[Added Lines]",
          "873:               throw QueryExecutionErrors.invalidInputSyntaxForNumericError(s, origin.context)",
          "",
          "---------------",
          "--- Hunk 13 ---",
          "[Context before]",
          "1016:     case CalendarIntervalType => castToIntervalCode(from)",
          "1017:     case it: DayTimeIntervalType => castToDayTimeIntervalCode(from, it)",
          "1018:     case it: YearMonthIntervalType => castToYearMonthIntervalCode(from, it)",
          "1020:     case ByteType => castToByteCode(from, ctx)",
          "1021:     case ShortType => castToShortCode(from, ctx)",
          "1022:     case IntegerType => castToIntCode(from, ctx)",
          "",
          "[Removed Lines]",
          "1019:     case BooleanType => castToBooleanCode(from)",
          "",
          "[Added Lines]",
          "1020:     case BooleanType => castToBooleanCode(from, ctx)",
          "",
          "---------------",
          "--- Hunk 14 ---",
          "[Context before]",
          "1295:         val intOpt = ctx.freshVariable(\"intOpt\", classOf[Option[Integer]])",
          "1296:         (c, evPrim, evNull) =>",
          "1297:           if (ansiEnabled) {",
          "1298:             code\"\"\"",
          "1300:             \"\"\"",
          "1301:           } else {",
          "1302:             code\"\"\"",
          "",
          "[Removed Lines]",
          "1299:               $evPrim = org.apache.spark.sql.catalyst.util.DateTimeUtils.stringToDateAnsi($c);",
          "",
          "[Added Lines]",
          "1299:             val errorContext = ctx.addReferenceObj(\"errCtx\", origin.context)",
          "1301:               $evPrim = $dateTimeUtilsCls.stringToDateAnsi($c, $errorContext);",
          "",
          "---------------",
          "--- Hunk 15 ---",
          "[Context before]",
          "1373:               }",
          "1374:           \"\"\"",
          "1375:       case StringType if ansiEnabled =>",
          "1376:         (c, evPrim, evNull) =>",
          "1377:           code\"\"\"",
          "1379:               ${changePrecision(tmp, target, evPrim, evNull, canNullSafeCast, ctx)}",
          "1380:           \"\"\"",
          "1381:       case BooleanType =>",
          "",
          "[Removed Lines]",
          "1378:               Decimal $tmp = Decimal.fromStringANSI($c);",
          "",
          "[Added Lines]",
          "1378:         val errorContext = ctx.addReferenceObj(\"errCtx\", origin.context)",
          "1381:               Decimal $tmp = Decimal.fromStringANSI($c, $errorContext);",
          "",
          "---------------",
          "--- Hunk 16 ---",
          "[Context before]",
          "1432:       val longOpt = ctx.freshVariable(\"longOpt\", classOf[Option[Long]])",
          "1433:       (c, evPrim, evNull) =>",
          "1434:         if (ansiEnabled) {",
          "1435:           code\"\"\"",
          "1438:            \"\"\"",
          "1439:         } else {",
          "1440:           code\"\"\"",
          "",
          "[Removed Lines]",
          "1436:             $evPrim =",
          "1437:               org.apache.spark.sql.catalyst.util.DateTimeUtils.stringToTimestampAnsi($c, $zid);",
          "",
          "[Added Lines]",
          "1438:           val errorContext = ctx.addReferenceObj(\"errCtx\", origin.context)",
          "1440:             $evPrim = $dateTimeUtilsCls.stringToTimestampAnsi($c, $zid, $errorContext);",
          "",
          "---------------",
          "--- Hunk 17 ---",
          "[Context before]",
          "1471:     case DoubleType =>",
          "1472:       (c, evPrim, evNull) =>",
          "1473:         if (ansiEnabled) {",
          "1475:         } else {",
          "1476:           code\"\"\"",
          "1477:             if (Double.isNaN($c) || Double.isInfinite($c)) {",
          "",
          "[Removed Lines]",
          "1474:           code\"$evPrim = $dateTimeUtilsCls.doubleToTimestampAnsi($c);\"",
          "",
          "[Added Lines]",
          "1477:           val errorContext = ctx.addReferenceObj(\"errCtx\", origin.context)",
          "1478:           code\"$evPrim = $dateTimeUtilsCls.doubleToTimestampAnsi($c, $errorContext);\"",
          "",
          "---------------",
          "--- Hunk 18 ---",
          "[Context before]",
          "1484:     case FloatType =>",
          "1485:       (c, evPrim, evNull) =>",
          "1486:         if (ansiEnabled) {",
          "1488:         } else {",
          "1489:           code\"\"\"",
          "1490:             if (Float.isNaN($c) || Float.isInfinite($c)) {",
          "",
          "[Removed Lines]",
          "1487:           code\"$evPrim = $dateTimeUtilsCls.doubleToTimestampAnsi((double)$c);\"",
          "",
          "[Added Lines]",
          "1491:           val errorContext = ctx.addReferenceObj(\"errCtx\", origin.context)",
          "1492:           code\"$evPrim = $dateTimeUtilsCls.doubleToTimestampAnsi((double)$c, $errorContext);\"",
          "",
          "---------------",
          "--- Hunk 19 ---",
          "[Context before]",
          "1503:       val longOpt = ctx.freshVariable(\"longOpt\", classOf[Option[Long]])",
          "1504:       (c, evPrim, evNull) =>",
          "1505:         if (ansiEnabled) {",
          "1506:           code\"\"\"",
          "1509:            \"\"\"",
          "1510:         } else {",
          "1511:           code\"\"\"",
          "",
          "[Removed Lines]",
          "1507:             $evPrim =",
          "1508:               $dateTimeUtilsCls.stringToTimestampWithoutTimeZoneAnsi($c);",
          "",
          "[Added Lines]",
          "1511:           val errorContext = ctx.addReferenceObj(\"errCtx\", origin.context)",
          "1513:             $evPrim = $dateTimeUtilsCls.stringToTimestampWithoutTimeZoneAnsi($c, $errorContext);",
          "",
          "---------------",
          "--- Hunk 20 ---",
          "[Context before]",
          "1613:   private[this] def timestampToDoubleCode(ts: ExprValue): Block =",
          "1614:     code\"$ts / (double)$MICROS_PER_SECOND\"",
          "1617:     case StringType =>",
          "1618:       val stringUtils = inline\"${StringUtils.getClass.getName.stripSuffix(\"$\")}\"",
          "1619:       (c, evPrim, evNull) =>",
          "1620:         val castFailureCode = if (ansiEnabled) {",
          "1622:         } else {",
          "1623:           s\"$evNull = true;\"",
          "1624:         }",
          "",
          "[Removed Lines]",
          "1616:   private[this] def castToBooleanCode(from: DataType): CastFunction = from match {",
          "1621:           s\"throw QueryExecutionErrors.invalidInputSyntaxForBooleanError($c);\"",
          "",
          "[Added Lines]",
          "1621:   private[this] def castToBooleanCode(",
          "1622:       from: DataType,",
          "1623:       ctx: CodegenContext): CastFunction = from match {",
          "1628:           val errorContext = ctx.addReferenceObj(\"errCtx\", origin.context)",
          "1629:           s\"throw QueryExecutionErrors.invalidInputSyntaxForBooleanError($c, $errorContext);\"",
          "",
          "---------------",
          "--- Hunk 21 ---",
          "[Context before]",
          "1746:   private[this] def castToByteCode(from: DataType, ctx: CodegenContext): CastFunction = from match {",
          "1747:     case StringType if ansiEnabled =>",
          "1748:       val stringUtils = UTF8StringUtils.getClass.getCanonicalName.stripSuffix(\"$\")",
          "1750:     case StringType =>",
          "1751:       val wrapper = ctx.freshVariable(\"intWrapper\", classOf[UTF8String.IntWrapper])",
          "1752:       (c, evPrim, evNull) =>",
          "",
          "[Removed Lines]",
          "1749:       (c, evPrim, evNull) => code\"$evPrim = $stringUtils.toByteExact($c);\"",
          "",
          "[Added Lines]",
          "1757:       val errorContext = ctx.addReferenceObj(\"errCtx\", origin.context)",
          "1758:       (c, evPrim, evNull) => code\"$evPrim = $stringUtils.toByteExact($c, $errorContext);\"",
          "",
          "---------------",
          "--- Hunk 22 ---",
          "[Context before]",
          "1782:       ctx: CodegenContext): CastFunction = from match {",
          "1783:     case StringType if ansiEnabled =>",
          "1784:       val stringUtils = UTF8StringUtils.getClass.getCanonicalName.stripSuffix(\"$\")",
          "1786:     case StringType =>",
          "1787:       val wrapper = ctx.freshVariable(\"intWrapper\", classOf[UTF8String.IntWrapper])",
          "1788:       (c, evPrim, evNull) =>",
          "",
          "[Removed Lines]",
          "1785:       (c, evPrim, evNull) => code\"$evPrim = $stringUtils.toShortExact($c);\"",
          "",
          "[Added Lines]",
          "1794:       val errorContext = ctx.addReferenceObj(\"errCtx\", origin.context)",
          "1795:       (c, evPrim, evNull) => code\"$evPrim = $stringUtils.toShortExact($c, $errorContext);\"",
          "",
          "---------------",
          "--- Hunk 23 ---",
          "[Context before]",
          "1816:   private[this] def castToIntCode(from: DataType, ctx: CodegenContext): CastFunction = from match {",
          "1817:     case StringType if ansiEnabled =>",
          "1818:       val stringUtils = UTF8StringUtils.getClass.getCanonicalName.stripSuffix(\"$\")",
          "1820:     case StringType =>",
          "1821:       val wrapper = ctx.freshVariable(\"intWrapper\", classOf[UTF8String.IntWrapper])",
          "1822:       (c, evPrim, evNull) =>",
          "",
          "[Removed Lines]",
          "1819:       (c, evPrim, evNull) => code\"$evPrim = $stringUtils.toIntExact($c);\"",
          "",
          "[Added Lines]",
          "1829:       val errorContext = ctx.addReferenceObj(\"errCtx\", origin.context)",
          "1830:       (c, evPrim, evNull) => code\"$evPrim = $stringUtils.toIntExact($c, $errorContext);\"",
          "",
          "---------------",
          "--- Hunk 24 ---",
          "[Context before]",
          "1850:   private[this] def castToLongCode(from: DataType, ctx: CodegenContext): CastFunction = from match {",
          "1851:     case StringType if ansiEnabled =>",
          "1852:       val stringUtils = UTF8StringUtils.getClass.getCanonicalName.stripSuffix(\"$\")",
          "1854:     case StringType =>",
          "1855:       val wrapper = ctx.freshVariable(\"longWrapper\", classOf[UTF8String.LongWrapper])",
          "1856:       (c, evPrim, evNull) =>",
          "",
          "[Removed Lines]",
          "1853:       (c, evPrim, evNull) => code\"$evPrim = $stringUtils.toLongExact($c);\"",
          "",
          "[Added Lines]",
          "1864:       val errorContext = ctx.addReferenceObj(\"errCtx\", origin.context)",
          "1865:       (c, evPrim, evNull) => code\"$evPrim = $stringUtils.toLongExact($c, $errorContext);\"",
          "",
          "---------------",
          "--- Hunk 25 ---",
          "[Context before]",
          "1886:         val floatStr = ctx.freshVariable(\"floatStr\", StringType)",
          "1887:         (c, evPrim, evNull) =>",
          "1888:           val handleNull = if (ansiEnabled) {",
          "1890:           } else {",
          "1891:             s\"$evNull = true;\"",
          "1892:           }",
          "",
          "[Removed Lines]",
          "1889:             s\"throw QueryExecutionErrors.invalidInputSyntaxForNumericError($c);\"",
          "",
          "[Added Lines]",
          "1901:             val errorContext = ctx.addReferenceObj(\"errCtx\", origin.context)",
          "1902:             s\"throw QueryExecutionErrors.invalidInputSyntaxForNumericError($c, $errorContext);\"",
          "",
          "---------------",
          "--- Hunk 26 ---",
          "[Context before]",
          "1922:         val doubleStr = ctx.freshVariable(\"doubleStr\", StringType)",
          "1923:         (c, evPrim, evNull) =>",
          "1924:           val handleNull = if (ansiEnabled) {",
          "1926:           } else {",
          "1927:             s\"$evNull = true;\"",
          "1928:           }",
          "",
          "[Removed Lines]",
          "1925:             s\"throw QueryExecutionErrors.invalidInputSyntaxForNumericError($c);\"",
          "",
          "[Added Lines]",
          "1938:             val errorContext = ctx.addReferenceObj(\"errCtx\", origin.context)",
          "1939:             s\"throw QueryExecutionErrors.invalidInputSyntaxForNumericError($c, $errorContext);\"",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/DateTimeUtils.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/DateTimeUtils.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/DateTimeUtils.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/DateTimeUtils.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "448:     }",
          "449:   }",
          "452:     stringToTimestamp(s, timeZoneId).getOrElse {",
          "454:     }",
          "455:   }",
          "458:     if (d.isNaN || d.isInfinite) {",
          "460:     } else {",
          "461:       DoubleExactNumeric.toLong(d * MICROS_PER_SECOND)",
          "462:     }",
          "",
          "[Removed Lines]",
          "451:   def stringToTimestampAnsi(s: UTF8String, timeZoneId: ZoneId): Long = {",
          "453:       throw QueryExecutionErrors.cannotCastToDateTimeError(s, TimestampType)",
          "457:   def doubleToTimestampAnsi(d: Double): Long = {",
          "459:       throw QueryExecutionErrors.cannotCastToDateTimeError(d, TimestampType)",
          "",
          "[Added Lines]",
          "451:   def stringToTimestampAnsi(s: UTF8String, timeZoneId: ZoneId, errorContext: String = \"\"): Long = {",
          "453:       throw QueryExecutionErrors.cannotCastToDateTimeError(s, TimestampType, errorContext)",
          "457:   def doubleToTimestampAnsi(d: Double, errorContext: String): Long = {",
          "459:       throw QueryExecutionErrors.cannotCastToDateTimeError(d, TimestampType, errorContext)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "503:     stringToTimestampWithoutTimeZone(s, true)",
          "504:   }",
          "507:     stringToTimestampWithoutTimeZone(s, true).getOrElse {",
          "509:     }",
          "510:   }",
          "",
          "[Removed Lines]",
          "506:   def stringToTimestampWithoutTimeZoneAnsi(s: UTF8String): Long = {",
          "508:       throw QueryExecutionErrors.cannotCastToDateTimeError(s, TimestampNTZType)",
          "",
          "[Added Lines]",
          "506:   def stringToTimestampWithoutTimeZoneAnsi(s: UTF8String, errorContext: String): Long = {",
          "508:       throw QueryExecutionErrors.cannotCastToDateTimeError(s, TimestampNTZType, errorContext)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "621:     }",
          "622:   }",
          "625:     stringToDate(s).getOrElse {",
          "627:     }",
          "628:   }",
          "",
          "[Removed Lines]",
          "624:   def stringToDateAnsi(s: UTF8String): Int = {",
          "626:       throw QueryExecutionErrors.cannotCastToDateTimeError(s, DateType)",
          "",
          "[Added Lines]",
          "624:   def stringToDateAnsi(s: UTF8String, errorContext: String = \"\"): Int = {",
          "626:       throw QueryExecutionErrors.cannotCastToDateTimeError(s, DateType, errorContext)",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/UTF8StringUtils.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/UTF8StringUtils.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/UTF8StringUtils.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/UTF8StringUtils.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "26: object UTF8StringUtils {",
          "37:     try {",
          "38:       f",
          "39:     } catch {",
          "40:       case e: NumberFormatException =>",
          "42:     }",
          "43:   }",
          "44: }",
          "",
          "[Removed Lines]",
          "28:   def toLongExact(s: UTF8String): Long = withException(s.toLongExact)",
          "30:   def toIntExact(s: UTF8String): Int = withException(s.toIntExact)",
          "32:   def toShortExact(s: UTF8String): Short = withException(s.toShortExact)",
          "34:   def toByteExact(s: UTF8String): Byte = withException(s.toByteExact)",
          "36:   private def withException[A](f: => A): A = {",
          "41:         throw QueryExecutionErrors.invalidInputSyntaxForNumericError(e)",
          "",
          "[Added Lines]",
          "28:   def toLongExact(s: UTF8String, errorContext: String): Long =",
          "29:     withException(s.toLongExact, errorContext)",
          "31:   def toIntExact(s: UTF8String, errorContext: String): Int =",
          "32:     withException(s.toIntExact, errorContext)",
          "34:   def toShortExact(s: UTF8String, errorContext: String): Short =",
          "35:     withException(s.toShortExact, errorContext)",
          "37:   def toByteExact(s: UTF8String, errorContext: String): Byte =",
          "38:     withException(s.toByteExact, errorContext)",
          "40:   private def withException[A](f: => A, errorContext: String): A = {",
          "45:         throw QueryExecutionErrors.invalidInputSyntaxForNumericError(e, errorContext)",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "104:         decimalPrecision.toString, decimalScale.toString, SQLConf.ANSI_ENABLED.key, context))",
          "105:   }",
          "108:     new NumberFormatException(s\"${e.getMessage}. To return NULL instead, use 'try_cast'. \" +",
          "110:   }",
          "113:     new SparkNumberFormatException(errorClass = \"INVALID_INPUT_SYNTAX_FOR_NUMERIC_TYPE\",",
          "115:   }",
          "117:   def cannotCastFromNullTypeError(to: DataType): Throwable = {",
          "",
          "[Removed Lines]",
          "107:   def invalidInputSyntaxForNumericError(e: NumberFormatException): NumberFormatException = {",
          "109:       s\"If necessary set ${SQLConf.ANSI_ENABLED.key} to false to bypass this error.\")",
          "112:   def invalidInputSyntaxForNumericError(s: UTF8String): NumberFormatException = {",
          "114:       messageParameters = Array(toSQLValue(s, StringType), SQLConf.ANSI_ENABLED.key))",
          "",
          "[Added Lines]",
          "107:   def invalidInputSyntaxForNumericError(",
          "108:       e: NumberFormatException,",
          "109:       errorContext: String): NumberFormatException = {",
          "111:       s\"If necessary set ${SQLConf.ANSI_ENABLED.key} to false to bypass this error.\" + errorContext)",
          "114:   def invalidInputSyntaxForNumericError(",
          "115:       s: UTF8String,",
          "116:       errorContext: String): NumberFormatException = {",
          "118:       messageParameters = Array(toSQLValue(s, StringType), SQLConf.ANSI_ENABLED.key, errorContext))",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1044:       e)",
          "1045:   }",
          "1048:     new DateTimeException(s\"Cannot cast $value to $to. To return NULL instead, use 'try_cast'. \" +",
          "1050:   }",
          "1052:   def registeringStreamingQueryListenerError(e: Exception): Throwable = {",
          "",
          "[Removed Lines]",
          "1047:   def cannotCastToDateTimeError(value: Any, to: DataType): Throwable = {",
          "1049:       s\"If necessary set ${SQLConf.ANSI_ENABLED.key} to false to bypass this error.\")",
          "",
          "[Added Lines]",
          "1051:   def cannotCastToDateTimeError(value: Any, to: DataType, errorContext: String): Throwable = {",
          "1053:       s\"If necessary set ${SQLConf.ANSI_ENABLED.key} to false to bypass this error.\" + errorContext)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "1180:       \"SQLUserDefinedType nor registered with UDTRegistration.}\")",
          "1181:   }",
          "1184:     new UnsupportedOperationException(s\"invalid input syntax for type boolean: $s. \" +",
          "1185:       s\"To return NULL instead, use 'try_cast'. If necessary set ${SQLConf.ANSI_ENABLED.key} \" +",
          "1187:   }",
          "1189:   def unsupportedOperandTypeForSizeFunctionError(dataType: DataType): Throwable = {",
          "",
          "[Removed Lines]",
          "1183:   def invalidInputSyntaxForBooleanError(s: UTF8String): UnsupportedOperationException = {",
          "1186:       \"to false to bypass this error.\")",
          "",
          "[Added Lines]",
          "1187:   def invalidInputSyntaxForBooleanError(",
          "1188:       s: UTF8String,",
          "1189:       errorContext: String): UnsupportedOperationException = {",
          "1192:       \"to false to bypass this error.\" + errorContext)",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/types/Decimal.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/types/Decimal.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/types/Decimal.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/types/Decimal.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "613:     }",
          "614:   }",
          "617:     try {",
          "618:       val bigDecimal = stringToJavaBigDecimal(str)",
          "",
          "[Removed Lines]",
          "616:   def fromStringANSI(str: UTF8String): Decimal = {",
          "",
          "[Added Lines]",
          "616:   def fromStringANSI(str: UTF8String, errorContext: String = \"\"): Decimal = {",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "626:       }",
          "627:     } catch {",
          "628:       case _: NumberFormatException =>",
          "630:     }",
          "631:   }",
          "",
          "[Removed Lines]",
          "629:         throw QueryExecutionErrors.invalidInputSyntaxForNumericError(str)",
          "",
          "[Added Lines]",
          "629:         throw QueryExecutionErrors.invalidInputSyntaxForNumericError(str, errorContext)",
          "",
          "---------------"
        ],
        "sql/core/src/test/resources/sql-tests/inputs/cast.sql||sql/core/src/test/resources/sql-tests/inputs/cast.sql": [
          "File: sql/core/src/test/resources/sql-tests/inputs/cast.sql -> sql/core/src/test/resources/sql-tests/inputs/cast.sql",
          "--- Hunk 1 ---",
          "[Context before]",
          "88: select cast('23.45' as decimal(4, 2));",
          "89: select cast('123.45' as decimal(4, 2));",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "90: select cast('xyz' as decimal(4, 2));",
          "92: select cast('2022-01-01' as date);",
          "93: select cast('a' as date);",
          "94: select cast('2022-01-01 00:00:00' as timestamp);",
          "95: select cast('a' as timestamp);",
          "96: select cast('2022-01-01 00:00:00' as timestamp_ntz);",
          "97: select cast('a' as timestamp_ntz);",
          "99: select cast(cast('inf' as double) as timestamp);",
          "100: select cast(cast('inf' as float) as timestamp);",
          "",
          "---------------"
        ]
      }
    }
  ]
}