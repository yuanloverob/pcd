{
  "cve_id": "CVE-2021-45229",
  "cve_desc": "It was discovered that the \"Trigger DAG with config\" screen was susceptible to XSS attacks via the `origin` query argument. This issue affects Apache Airflow versions 2.2.3 and below.",
  "repo": "apache/airflow",
  "patch_hash": "628aa1f99c865d97d0b1c7c76e630e43a7b8d319",
  "patch_info": {
    "commit_hash": "628aa1f99c865d97d0b1c7c76e630e43a7b8d319",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/628aa1f99c865d97d0b1c7c76e630e43a7b8d319",
    "files": [
      "airflow/www/templates/airflow/trigger.html",
      "tests/www/views/test_views_trigger_dag.py"
    ],
    "message": "Simplify trigger cancel button (#21591)\n\nCo-authored-by: Jed Cunningham <jedcunningham@apache.org>\n(cherry picked from commit 65297673a318660fba76797e50d0c06804dfcafc)",
    "before_after_code_files": [
      "airflow/www/templates/airflow/trigger.html||airflow/www/templates/airflow/trigger.html",
      "tests/www/views/test_views_trigger_dag.py||tests/www/views/test_views_trigger_dag.py"
    ]
  },
  "patch_diff": {
    "airflow/www/templates/airflow/trigger.html||airflow/www/templates/airflow/trigger.html": [
      "File: airflow/www/templates/airflow/trigger.html -> airflow/www/templates/airflow/trigger.html",
      "--- Hunk 1 ---",
      "[Context before]",
      "63:       </label>",
      "64:     </div>",
      "65:     <button type=\"submit\" class=\"btn btn-primary\">Trigger</button>",
      "67:   </form>",
      "68: {% endblock %}",
      "",
      "[Removed Lines]",
      "66:     <button type=\"button\" class=\"btn\" onclick=\"location.href = '{{ origin }}'; return false\">Cancel</button>",
      "",
      "[Added Lines]",
      "66:     <a class=\"btn\" href=\"{{ origin }}\">Cancel</a>",
      "",
      "---------------"
    ],
    "tests/www/views/test_views_trigger_dag.py||tests/www/views/test_views_trigger_dag.py": [
      "File: tests/www/views/test_views_trigger_dag.py -> tests/www/views/test_views_trigger_dag.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "133:         (\"javascript:alert(1)\", \"/home\"),",
      "134:         (\"http://google.com\", \"/home\"),",
      "135:         (\"36539'%3balert(1)%2f%2f166\", \"/home\"),",
      "136:         (",
      "137:             \"%2Ftree%3Fdag_id%3Dexample_bash_operator';alert(33)//\",",
      "138:             \"/home\",",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "136:         (",
      "137:             '\"><script>alert(99)</script><a href=\"',",
      "138:             \"&#34;&gt;&lt;script&gt;alert(99)&lt;/script&gt;&lt;a href=&#34;\",",
      "139:         ),",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "145:     test_dag_id = \"example_bash_operator\"",
      "147:     resp = admin_client.get(f'trigger?dag_id={test_dag_id}&origin={test_origin}')",
      "156: @pytest.mark.parametrize(",
      "",
      "[Removed Lines]",
      "148:     check_content_in_response(",
      "149:         '<button type=\"button\" class=\"btn\" onclick=\"location.href = \\'{}\\'; return false\">'.format(",
      "150:             expected_origin",
      "151:         ),",
      "152:         resp,",
      "153:     )",
      "",
      "[Added Lines]",
      "152:     check_content_in_response(f'<a class=\"btn\" href=\"{expected_origin}\">Cancel</a>', resp)",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "80f4e5f8b374efb6454a8a659ee4a3a277b3d3d1",
      "candidate_info": {
        "commit_hash": "80f4e5f8b374efb6454a8a659ee4a3a277b3d3d1",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/80f4e5f8b374efb6454a8a659ee4a3a277b3d3d1",
        "files": [
          "scripts/ci/libraries/_initialization.sh"
        ],
        "message": "Fix breeze docker version parsing (#19182)\n\n(cherry picked from commit d70909422ec7ec2f6e5311d26c1b15e31c3bc188)",
        "before_after_code_files": [
          "scripts/ci/libraries/_initialization.sh||scripts/ci/libraries/_initialization.sh"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "scripts/ci/libraries/_initialization.sh||scripts/ci/libraries/_initialization.sh": [
          "File: scripts/ci/libraries/_initialization.sh -> scripts/ci/libraries/_initialization.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "935: function initialization::ver() {",
          "936:   # convert SemVer number to comparable string (strips pre-release version)",
          "937:   # shellcheck disable=SC2086,SC2183",
          "939: }",
          "941: function initialization::check_docker_version() {",
          "",
          "[Removed Lines]",
          "938:   printf \"%03d%03d%03d%.0s\" ${1//[.-]/}",
          "",
          "[Added Lines]",
          "938:   printf \"%03d%03d%03d%.0s\" ${1//[.-]/ }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "09602fcee7ccced177a78367486fbee9cc0508a3",
      "candidate_info": {
        "commit_hash": "09602fcee7ccced177a78367486fbee9cc0508a3",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/09602fcee7ccced177a78367486fbee9cc0508a3",
        "files": [
          "scripts/ci/libraries/_build_images.sh"
        ],
        "message": "Fix race condition when flake checks run in parallel (#20294)\n\nThe Flake checks run in parallel and when you had an image which\nrequired rebuild, it performed additional check on whether the\nimage needs build or \"pull+build\". When it was run in parallel\na temporary file containing hash of the remote image could be\noverwritten and emptied while another process was reading it\nwhich resulted in error when running flake command.\n\nThis has been changed - the files are now stored in a temporary\nfiles - unique to each of the processes running in parallel and\nthe file in question is moved as an atomic operation so it will\nnever become empty.\n\n(cherry picked from commit 0b7734ee36670a70363b5866f7a064a5a0d67be8)",
        "before_after_code_files": [
          "scripts/ci/libraries/_build_images.sh||scripts/ci/libraries/_build_images.sh"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "scripts/ci/libraries/_build_images.sh||scripts/ci/libraries/_build_images.sh": [
          "File: scripts/ci/libraries/_build_images.sh -> scripts/ci/libraries/_build_images.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "305: function build_images::get_remote_image_build_cache_hash() {",
          "306:     set +e",
          "307:     local remote_image_container_id_file",
          "309:     local remote_image_build_cache_file",
          "311:     # Pull remote manifest image",
          "312:     if ! docker_v pull \"${AIRFLOW_CI_REMOTE_MANIFEST_IMAGE}\" 2>/dev/null >/dev/null; then",
          "313:         verbosity::print_info",
          "",
          "[Removed Lines]",
          "308:     remote_image_container_id_file=\"${AIRFLOW_SOURCES}/manifests/remote-airflow-manifest-image-${PYTHON_MAJOR_MINOR_VERSION}\"",
          "310:     remote_image_build_cache_file=\"${AIRFLOW_SOURCES}/manifests/remote-build-cache-hash-${PYTHON_MAJOR_MINOR_VERSION}\"",
          "",
          "[Added Lines]",
          "308:     remote_image_container_id_file=\"$(mktemp)\"",
          "310:     remote_image_build_cache_file=$(mktemp)",
          "311:     local target_remote_cache_file=\"${AIRFLOW_SOURCES}/manifests/remote-build-cache-hash-${PYTHON_MAJOR_MINOR_VERSION}\"",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "326:     # Extract manifest and store it in local file",
          "327:     docker_v cp \"$(cat \"${remote_image_container_id_file}\"):/build-cache-hash\" \\",
          "328:         \"${remote_image_build_cache_file}\"",
          "329:     docker_v rm --force \"$(cat \"${remote_image_container_id_file}\")\"",
          "330:     rm -f \"${remote_image_container_id_file}\"",
          "331:     verbosity::print_info",
          "333:     verbosity::print_info",
          "334: }",
          "",
          "[Removed Lines]",
          "332:     verbosity::print_info \"Remote build cache hash: '$(cat \"${remote_image_build_cache_file}\")'\"",
          "",
          "[Added Lines]",
          "330:     # The `mv` is an atomic operation so even if we run it in parallel (for example in flake) it will",
          "331:     # never be empty (happened in the past)",
          "332:     mv \"${remote_image_build_cache_file}\" \"${target_remote_cache_file}\"",
          "336:     verbosity::print_info \"Remote build cache hash: '$(cat \"${target_remote_cache_file}\")'\"",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "79e995480822fd68c715c6ab5d83357721fa2d55",
      "candidate_info": {
        "commit_hash": "79e995480822fd68c715c6ab5d83357721fa2d55",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/79e995480822fd68c715c6ab5d83357721fa2d55",
        "files": [
          "airflow/providers/elasticsearch/log/es_task_handler.py",
          "airflow/utils/log/file_task_handler.py"
        ],
        "message": "Use compat data interval shim in log handlers (#21289)\n\n(cherry picked from commit 44bd211b19dcb75eeb53ced5bea2cf0c80654b1a)",
        "before_after_code_files": [
          "airflow/providers/elasticsearch/log/es_task_handler.py||airflow/providers/elasticsearch/log/es_task_handler.py",
          "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/providers/elasticsearch/log/es_task_handler.py||airflow/providers/elasticsearch/log/es_task_handler.py": [
          "File: airflow/providers/elasticsearch/log/es_task_handler.py -> airflow/providers/elasticsearch/log/es_task_handler.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "101:         self.context_set = False",
          "103:     def _render_log_id(self, ti: TaskInstance, try_number: int) -> str:",
          "106:         if self.json_format:",
          "109:             execution_date = self._clean_date(dag_run.execution_date)",
          "110:         else:",
          "113:             execution_date = dag_run.execution_date.isoformat()",
          "115:         return self.log_id_template.format(",
          "",
          "[Removed Lines]",
          "104:         dag_run = ti.dag_run",
          "107:             data_interval_start = self._clean_date(dag_run.data_interval_start)",
          "108:             data_interval_end = self._clean_date(dag_run.data_interval_end)",
          "111:             data_interval_start = dag_run.data_interval_start.isoformat()",
          "112:             data_interval_end = dag_run.data_interval_end.isoformat()",
          "",
          "[Added Lines]",
          "104:         dag_run = ti.get_dagrun()",
          "105:         try:",
          "106:             data_interval: Tuple[datetime, datetime] = ti.task.dag.get_run_data_interval(dag_run)",
          "107:         except AttributeError:  # ti.task is not always set.",
          "108:             data_interval = (dag_run.data_interval_start, dag_run.data_interval_end)",
          "111:             data_interval_start = self._clean_date(data_interval[0])",
          "112:             data_interval_end = self._clean_date(data_interval[1])",
          "115:             if data_interval[0]:",
          "116:                 data_interval_start = data_interval[0].isoformat()",
          "117:             else:",
          "118:                 data_interval_start = \"\"",
          "119:             if data_interval[1]:",
          "120:                 data_interval_end = data_interval[1].isoformat()",
          "121:             else:",
          "122:                 data_interval_end = \"\"",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "123:         )",
          "125:     @staticmethod",
          "127:         \"\"\"",
          "128:         Clean up a date value so that it is safe to query in elasticsearch",
          "129:         by removing reserved characters.",
          "133:         \"\"\"",
          "134:         return value.strftime(\"%Y_%m_%dT%H_%M_%S_%f\")",
          "136:     def _group_logs_by_host(self, logs):",
          "",
          "[Removed Lines]",
          "126:     def _clean_date(value: datetime) -> str:",
          "130:         # https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-query-string-query.html#_reserved_characters",
          "132:         :param execution_date: execution date of the dag run.",
          "",
          "[Added Lines]",
          "136:     def _clean_date(value: Optional[datetime]) -> str:",
          "141:         https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-query-string-query.html#_reserved_characters",
          "143:         if value is None:",
          "144:             return \"\"",
          "",
          "---------------"
        ],
        "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py": [
          "File: airflow/utils/log/file_task_handler.py -> airflow/utils/log/file_task_handler.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: \"\"\"File logging handler for tasks.\"\"\"",
          "19: import logging",
          "20: import os",
          "21: from pathlib import Path",
          "24: import httpx",
          "25: from itsdangerous import TimedJSONWebSignatureSerializer",
          "",
          "[Removed Lines]",
          "22: from typing import TYPE_CHECKING, Optional",
          "",
          "[Added Lines]",
          "21: from datetime import datetime",
          "23: from typing import TYPE_CHECKING, Optional, Tuple",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "82:                 context = Context(ti=ti, ts=ti.get_dagrun().logical_date.isoformat())",
          "83:             context[\"try_number\"] = try_number",
          "84:             return render_template_to_string(self.filename_jinja_template, context)",
          "93:     def _read_grouped_logs(self):",
          "94:         return False",
          "",
          "[Removed Lines]",
          "86:         return self.filename_template.format(",
          "87:             dag_id=ti.dag_id,",
          "88:             task_id=ti.task_id,",
          "89:             execution_date=ti.get_dagrun().logical_date.isoformat(),",
          "90:             try_number=try_number,",
          "91:         )",
          "",
          "[Added Lines]",
          "86:         elif self.filename_template:",
          "87:             dag_run = ti.get_dagrun()",
          "88:             try:",
          "89:                 data_interval: Tuple[datetime, datetime] = ti.task.dag.get_run_data_interval(dag_run)",
          "90:             except AttributeError:  # ti.task is not always set.",
          "91:                 data_interval = (dag_run.data_interval_start, dag_run.data_interval_end)",
          "92:             if data_interval[0]:",
          "93:                 data_interval_start = data_interval[0].isoformat()",
          "94:             else:",
          "95:                 data_interval_start = \"\"",
          "96:             if data_interval[1]:",
          "97:                 data_interval_end = data_interval[1].isoformat()",
          "98:             else:",
          "99:                 data_interval_end = \"\"",
          "100:             return self.filename_template.format(",
          "101:                 dag_id=ti.dag_id,",
          "102:                 task_id=ti.task_id,",
          "103:                 run_id=ti.run_id,",
          "104:                 data_interval_start=data_interval_start,",
          "105:                 data_interval_end=data_interval_end,",
          "106:                 execution_date=ti.get_dagrun().logical_date.isoformat(),",
          "107:                 try_number=try_number,",
          "108:             )",
          "109:         else:",
          "110:             raise RuntimeError(f\"Unable to render log filename for {ti}. This should never happen\")",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "b05722e8b98963f65cd5f615d4390bb6377ba660",
      "candidate_info": {
        "commit_hash": "b05722e8b98963f65cd5f615d4390bb6377ba660",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/b05722e8b98963f65cd5f615d4390bb6377ba660",
        "files": [
          "airflow/stats.py",
          "tests/core/test_stats.py"
        ],
        "message": "Correctly send timing metrics when using dogstatsd (fix schedule_delay metric) (#19973)\n\n(cherry picked from commit 5d405d9cda0b88909e6b726769381044477f4678)",
        "before_after_code_files": [
          "airflow/stats.py||airflow/stats.py",
          "tests/core/test_stats.py||tests/core/test_stats.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/stats.py||airflow/stats.py": [
          "File: airflow/stats.py -> airflow/stats.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "16: # specific language governing permissions and limitations",
          "17: # under the License.",
          "19: import logging",
          "20: import socket",
          "21: import string",
          "22: import textwrap",
          "23: import time",
          "24: from functools import wraps",
          "27: from airflow.configuration import conf",
          "28: from airflow.exceptions import AirflowConfigException, InvalidStatsNameException",
          "",
          "[Removed Lines]",
          "25: from typing import TYPE_CHECKING, Callable, Optional, TypeVar, cast",
          "",
          "[Added Lines]",
          "19: import datetime",
          "26: from typing import TYPE_CHECKING, Callable, List, Optional, TypeVar, Union, cast",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "65:         \"\"\"Gauge stat\"\"\"",
          "67:     @classmethod",
          "69:         \"\"\"Stats timing\"\"\"",
          "71:     @classmethod",
          "",
          "[Removed Lines]",
          "68:     def timing(cls, stat: str, dt) -> None:",
          "",
          "[Added Lines]",
          "69:     def timing(cls, stat: str, dt: Union[float, datetime.timedelta]) -> None:",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "331:         return None",
          "333:     @validate_stat",
          "335:         \"\"\"Stats timing\"\"\"",
          "336:         if self.allow_list_validator.test(stat):",
          "337:             tags = tags or []",
          "338:             return self.dogstatsd.timing(metric=stat, value=dt, tags=tags)",
          "339:         return None",
          "",
          "[Removed Lines]",
          "334:     def timing(self, stat, dt, tags=None):",
          "",
          "[Added Lines]",
          "335:     def timing(self, stat, dt: Union[float, datetime.timedelta], tags: Optional[List[str]] = None):",
          "339:             if isinstance(dt, datetime.timedelta):",
          "340:                 dt = dt.total_seconds()",
          "",
          "---------------"
        ],
        "tests/core/test_stats.py||tests/core/test_stats.py": [
          "File: tests/core/test_stats.py -> tests/core/test_stats.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "181:         self.dogstatsd_client.timed.assert_not_called()",
          "183:     def test_timing(self):",
          "184:         self.dogstatsd.timing(\"dummy_timer\", 123)",
          "185:         self.dogstatsd_client.timing.assert_called_once_with(metric='dummy_timer', value=123, tags=[])",
          "187:     def test_gauge(self):",
          "188:         self.dogstatsd.gauge(\"dummy\", 123)",
          "189:         self.dogstatsd_client.gauge.assert_called_once_with(metric='dummy', sample_rate=1, value=123, tags=[])",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "184:         import datetime",
          "189:         self.dogstatsd.timing(\"dummy_timer\", datetime.timedelta(seconds=123))",
          "190:         self.dogstatsd_client.timing.assert_called_with(metric='dummy_timer', value=123.0, tags=[])",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "1cbad378cb778fca879a522916c11d32d80ac84e",
      "candidate_info": {
        "commit_hash": "1cbad378cb778fca879a522916c11d32d80ac84e",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/1cbad378cb778fca879a522916c11d32d80ac84e",
        "files": [
          "airflow/migrations/versions/587bdf053233_adding_index_for_dag_id_in_job.py",
          "docs/apache-airflow/migrations-ref.rst"
        ],
        "message": "Reorder migrations to include bugfix in 2.2.4 (#21598)\n\n(cherry picked from commit 005cef042bc4184c24ad03c1b4ee40cdbaf96cb5)",
        "before_after_code_files": [
          "airflow/migrations/versions/587bdf053233_adding_index_for_dag_id_in_job.py||airflow/migrations/versions/587bdf053233_adding_index_for_dag_id_in_job.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/migrations/versions/587bdf053233_adding_index_for_dag_id_in_job.py||airflow/migrations/versions/587bdf053233_adding_index_for_dag_id_in_job.py": [
          "File: airflow/migrations/versions/587bdf053233_adding_index_for_dag_id_in_job.py -> airflow/migrations/versions/587bdf053233_adding_index_for_dag_id_in_job.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "19: \"\"\"adding index for dag_id in job",
          "21: Revision ID: 587bdf053233",
          "23: Create Date: 2021-12-14 10:20:12.482940",
          "25: \"\"\"",
          "",
          "[Removed Lines]",
          "22: Revises: f9da662e7089",
          "",
          "[Added Lines]",
          "22: Revises: c381b21cb7e4",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "29: # revision identifiers, used by Alembic.",
          "30: revision = '587bdf053233'",
          "32: branch_labels = None",
          "33: depends_on = None",
          "",
          "[Removed Lines]",
          "31: down_revision = 'f9da662e7089'",
          "",
          "[Added Lines]",
          "31: down_revision = 'c381b21cb7e4'",
          "",
          "---------------"
        ]
      }
    }
  ]
}