{
  "cve_id": "CVE-2022-31777",
  "cve_desc": "A stored cross-site scripting (XSS) vulnerability in Apache Spark 3.2.1 and earlier, and 3.3.0, allows remote attackers to execute arbitrary JavaScript in the web browser of a user, by including a malicious payload into the logs which would be returned in logs rendered in the UI.",
  "repo": "apache/spark",
  "patch_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
  "patch_info": {
    "commit_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
    "repo": "apache/spark",
    "commit_url": "https://github.com/apache/spark/commit/ad90195de56688ce0898691eb9d04297ab0871ad",
    "files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ],
    "message": "[SPARK-39505][UI] Escape log content rendered in UI\n\n### What changes were proposed in this pull request?\n\nEscape log content rendered to the UI.\n\n### Why are the changes needed?\n\nLog content may contain reserved characters or other code in the log and be misinterpreted in the UI as HTML.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nExisting tests\n\nCloses #36902 from srowen/LogViewEscape.\n\nAuthored-by: Sean Owen <srowen@gmail.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
    "before_after_code_files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ]
  },
  "patch_diff": {
    "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js": [
      "File: core/src/main/resources/org/apache/spark/ui/static/log-view.js -> core/src/main/resources/org/apache/spark/ui/static/log-view.js",
      "--- Hunk 1 ---",
      "[Context before]",
      "85:       if (retStartByte == 0) {",
      "86:         disableMoreButton();",
      "87:       }",
      "90:       curLogLength = curLogLength + (startByte - retStartByte);",
      "91:       startByte = retStartByte;",
      "",
      "[Removed Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(cleanData);",
      "",
      "[Added Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(document.createTextNode(cleanData));",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "115:             var retLogLength = dataInfo[2];",
      "117:             var cleanData = data.substring(newlineIndex + 1);",
      "120:             curLogLength = curLogLength + (retEndByte - retStartByte);",
      "121:             endByte = retEndByte;",
      "",
      "[Removed Lines]",
      "118:             $(\"pre\", \".log-content\").append(cleanData);",
      "",
      "[Added Lines]",
      "118:             $(\"pre\", \".log-content\").append(document.createTextNode(cleanData));",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "3241f20b13c0f0a390027b182a33e88b6a16cedd",
      "candidate_info": {
        "commit_hash": "3241f20b13c0f0a390027b182a33e88b6a16cedd",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/3241f20b13c0f0a390027b182a33e88b6a16cedd",
        "files": [
          "sql/core/src/test/scala/org/apache/spark/sql/ColumnExpressionSuite.scala"
        ],
        "message": "[SPARK-39093][SQL][FOLLOWUP] Fix Period test\n\n### What changes were proposed in this pull request?\n\nChange the Period test to use months rather than days.\n\n### Why are the changes needed?\n\nWhile the Period test as-is confirms that the compilation error is fixed, it doesn't confirm that the newly generated code is correct. Spark ignores any unit less than months in Periods. As a result, we are always testing `0/(num + 3)`, so the test doesn't verify that the code generated for the right-hand operand is correct.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo, only changes a test.\n\n### How was this patch tested?\n\nUnit test.\n\nCloses #36481 from bersprockets/SPARK-39093_followup.\n\nAuthored-by: Bruce Robbins <bersprockets@gmail.com>\nSigned-off-by: Gengliang Wang <gengliang@apache.org>\n(cherry picked from commit 670710e91dfb2ed27c24b139c50a6bcf03132024)\nSigned-off-by: Gengliang Wang <gengliang@apache.org>",
        "before_after_code_files": [
          "sql/core/src/test/scala/org/apache/spark/sql/ColumnExpressionSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/ColumnExpressionSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/core/src/test/scala/org/apache/spark/sql/ColumnExpressionSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/ColumnExpressionSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/ColumnExpressionSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/ColumnExpressionSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "2990:   }",
          "2992:   test(\"SPARK-39093: divide period by integral expression\") {",
          "2994:     checkAnswer(df.select($\"pd\" / ($\"num\" + 3)),",
          "2996:   }",
          "2998:   test(\"SPARK-39093: divide duration by integral expression\") {",
          "",
          "[Removed Lines]",
          "2993:     val df = Seq(((Period.ofDays(10)), 2)).toDF(\"pd\", \"num\")",
          "2995:       Seq((Period.ofDays(2))).toDF)",
          "",
          "[Added Lines]",
          "2993:     val df = Seq(((Period.ofMonths(10)), 2)).toDF(\"pd\", \"num\")",
          "2995:       Seq((Period.ofMonths(2))).toDF)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "eea586d0c3df2e4b479c2c19f51f115927efeec1",
      "candidate_info": {
        "commit_hash": "eea586d0c3df2e4b479c2c19f51f115927efeec1",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/eea586d0c3df2e4b479c2c19f51f115927efeec1",
        "files": [
          "sql/core/src/test/scala/org/apache/spark/sql/SubquerySuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/sources/BucketedReadSuite.scala"
        ],
        "message": "[SPARK-39412][SQL][FOLLOWUP][TESTS][3.3] Check `IllegalStateException` instead of Spark's internal errors\n\n### What changes were proposed in this pull request?\nIn the PR, I propose to correctly check `IllegalStateException` instead of `SparkException` w/ the `INTERNAL_ERROR` error class. The issues were introduced by https://github.com/apache/spark/pull/36804 merged to master and 3.3.\n\n### Why are the changes needed?\nTo fix test failures in GAs.\n\n### Does this PR introduce _any_ user-facing change?\nNo.\n\n### How was this patch tested?\nBy running the modified test suites:\n```\n$ build/sbt \"test:testOnly *BucketedReadWithoutHiveSupportSuite\"\n$ build/sbt \"test:testOnly *.AdaptiveQueryExecSuite\"\n$ build/sbt \"test:testOnly *.SubquerySuite\"\n```\n\nCloses #36824 from MaxGekk/fix-IllegalStateException-3.3.\n\nAuthored-by: Max Gekk <max.gekk@gmail.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
        "before_after_code_files": [
          "sql/core/src/test/scala/org/apache/spark/sql/SubquerySuite.scala||sql/core/src/test/scala/org/apache/spark/sql/SubquerySuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/sources/BucketedReadSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/sources/BucketedReadSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/core/src/test/scala/org/apache/spark/sql/SubquerySuite.scala||sql/core/src/test/scala/org/apache/spark/sql/SubquerySuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/SubquerySuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/SubquerySuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: import scala.collection.mutable.ArrayBuffer",
          "23: import org.apache.spark.sql.catalyst.expressions.SubqueryExpression",
          "24: import org.apache.spark.sql.catalyst.plans.logical.{Join, LogicalPlan, Sort}",
          "25: import org.apache.spark.sql.execution.{ColumnarToRowExec, ExecSubqueryExpression, FileSourceScanExec, InputAdapter, ReusedSubqueryExec, ScalarSubquery, SubqueryExec, WholeStageCodegenExec}",
          "",
          "[Removed Lines]",
          "22: import org.apache.spark.SparkException",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "147:   }",
          "149:   test(\"runtime error when the number of rows is greater than 1\") {",
          "151:       sql(\"select (select a from (select 1 as a union all select 2 as a) t) as b\").collect()",
          "152:     }",
          "156:       \"more than one row returned by a subquery used as an expression\"))",
          "157:   }",
          "",
          "[Removed Lines]",
          "150:     val e = intercept[SparkException] {",
          "154:     assert(e.getErrorClass ===  \"INTERNAL_ERROR\")",
          "155:     assert(e.getCause.getMessage.contains(",
          "",
          "[Added Lines]",
          "149:     val e = intercept[IllegalStateException] {",
          "153:     assert(e.getMessage.contains(",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "23: import org.apache.logging.log4j.Level",
          "24: import org.scalatest.PrivateMethodTester",
          "27: import org.apache.spark.scheduler.{SparkListener, SparkListenerEvent, SparkListenerJobStart}",
          "28: import org.apache.spark.sql.{Dataset, QueryTest, Row, SparkSession, Strategy}",
          "29: import org.apache.spark.sql.catalyst.optimizer.{BuildLeft, BuildRight}",
          "",
          "[Removed Lines]",
          "26: import org.apache.spark.SparkException",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "857:         df1.write.parquet(tableDir.getAbsolutePath)",
          "859:         val aggregated = spark.table(\"bucketed_table\").groupBy(\"i\").count()",
          "861:           aggregated.count()",
          "862:         }",
          "867:       }",
          "868:     }",
          "869:   }",
          "",
          "[Removed Lines]",
          "860:         val error = intercept[SparkException] {",
          "864:         assert(error.getErrorClass === \"INTERNAL_ERROR\")",
          "865:         assert(error.getCause.toString contains \"Invalid bucket file\")",
          "866:         assert(error.getCause.getSuppressed.size === 0)",
          "",
          "[Added Lines]",
          "859:         val error = intercept[IllegalStateException] {",
          "863:         assert(error.toString contains \"Invalid bucket file\")",
          "864:         assert(error.getSuppressed.size === 0)",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/sources/BucketedReadSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/sources/BucketedReadSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/sources/BucketedReadSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/sources/BucketedReadSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "23: import scala.util.Random",
          "26: import org.apache.spark.sql._",
          "27: import org.apache.spark.sql.catalyst.catalog.BucketSpec",
          "28: import org.apache.spark.sql.catalyst.expressions",
          "",
          "[Removed Lines]",
          "25: import org.apache.spark.SparkException",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "842:       df1.write.parquet(tableDir.getAbsolutePath)",
          "844:       val aggregated = spark.table(\"bucketed_table\").groupBy(\"i\").count()",
          "846:         aggregated.count()",
          "847:       }",
          "851:     }",
          "852:   }",
          "",
          "[Removed Lines]",
          "845:       val e = intercept[SparkException] {",
          "849:       assert(e.getErrorClass === \"INTERNAL_ERROR\")",
          "850:       assert(e.getCause.toString contains \"Invalid bucket file\")",
          "",
          "[Added Lines]",
          "844:       val e = intercept[IllegalStateException] {",
          "848:       assert(e.toString contains \"Invalid bucket file\")",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "1324f7d16802e684c04b3773c84be57667452a0b",
      "candidate_info": {
        "commit_hash": "1324f7d16802e684c04b3773c84be57667452a0b",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/1324f7d16802e684c04b3773c84be57667452a0b",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CTESubstitution.scala",
          "sql/core/src/test/resources/sql-tests/inputs/cte-nested.sql",
          "sql/core/src/test/resources/sql-tests/results/cte-legacy.sql.out",
          "sql/core/src/test/resources/sql-tests/results/cte-nested.sql.out",
          "sql/core/src/test/resources/sql-tests/results/cte-nonlegacy.sql.out",
          "sql/core/src/test/scala/org/apache/spark/sql/CTEInlineSuite.scala"
        ],
        "message": "[SPARK-40297][SQL] CTE outer reference nested in CTE main body cannot be resolved\n\nThis PR fixes a bug where a CTE reference cannot be resolved if this reference occurs in an inner CTE definition nested in the outer CTE's main body FROM clause. E.g.,\n```\nWITH cte_outer AS (\n  SELECT 1\n)\nSELECT * FROM (\n  WITH cte_inner AS (\n    SELECT * FROM cte_outer\n  )\n  SELECT * FROM cte_inner\n)\n```\n\nThis fix is to change the `CTESubstitution`'s traverse order from `resolveOperatorsUpWithPruning` to `resolveOperatorsDownWithPruning` and also to recursively call `traverseAndSubstituteCTE` for CTE main body.\n\nBug fix. Without the fix an `AnalysisException` would be thrown for CTE queries mentioned above.\n\nNo.\n\nAdded UTs.\n\nCloses #37751 from maryannxue/spark-40297.\n\nAuthored-by: Maryann Xue <maryann.xue@gmail.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CTESubstitution.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CTESubstitution.scala",
          "sql/core/src/test/resources/sql-tests/inputs/cte-nested.sql||sql/core/src/test/resources/sql-tests/inputs/cte-nested.sql",
          "sql/core/src/test/scala/org/apache/spark/sql/CTEInlineSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/CTEInlineSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CTESubstitution.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CTESubstitution.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CTESubstitution.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CTESubstitution.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "56:       case _ => false",
          "57:     }",
          "58:     val cteDefs = ArrayBuffer.empty[CTERelationDef]",
          "60:       LegacyBehaviorPolicy.withName(conf.getConf(LEGACY_CTE_PRECEDENCE_POLICY)) match {",
          "61:         case LegacyBehaviorPolicy.EXCEPTION =>",
          "62:           assertNoNameConflictsInCTE(plan)",
          "",
          "[Removed Lines]",
          "59:     val (substituted, lastSubstituted) =",
          "",
          "[Added Lines]",
          "59:     val (substituted, firstSubstituted) =",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "68:     }",
          "69:     if (cteDefs.isEmpty) {",
          "70:       substituted",
          "72:       WithCTE(substituted, cteDefs.toSeq)",
          "73:     } else {",
          "74:       var done = false",
          "75:       substituted.resolveOperatorsWithPruning(_ => !done) {",
          "77:           done = true",
          "78:           WithCTE(p, cteDefs.toSeq)",
          "79:       }",
          "",
          "[Removed Lines]",
          "71:     } else if (substituted eq lastSubstituted.get) {",
          "76:         case p if p eq lastSubstituted.get =>",
          "",
          "[Added Lines]",
          "71:     } else if (substituted eq firstSubstituted.get) {",
          "76:         case p if p eq firstSubstituted.get =>",
          "78:           done = true",
          "79:           WithCTE(p, cteDefs.toSeq)",
          "80:         case p if p.children.count(_.containsPattern(CTE)) > 1 =>",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "181:       isCommand: Boolean,",
          "182:       outerCTEDefs: Seq[(String, CTERelationDef)],",
          "183:       cteDefs: ArrayBuffer[CTERelationDef]): (LogicalPlan, Option[LogicalPlan]) = {",
          "186:         _.containsAnyPattern(UNRESOLVED_WITH, PLAN_EXPRESSION)) {",
          "187:       case UnresolvedWith(child: LogicalPlan, relations) =>",
          "188:         val resolvedCTERelations =",
          "193:       case other =>",
          "194:         other.transformExpressionsWithPruning(_.containsPattern(PLAN_EXPRESSION)) {",
          "195:           case e: SubqueryExpression => e.withNewPlan(apply(e.plan))",
          "196:         }",
          "197:     }",
          "199:   }",
          "201:   private def resolveCTERelations(",
          "",
          "[Removed Lines]",
          "184:     var lastSubstituted: Option[LogicalPlan] = None",
          "185:     val newPlan = plan.resolveOperatorsUpWithPruning(",
          "189:           resolveCTERelations(relations, isLegacy = false, isCommand, outerCTEDefs, cteDefs)",
          "190:         lastSubstituted = Some(substituteCTE(child, isCommand, resolvedCTERelations))",
          "191:         lastSubstituted.get",
          "198:     (newPlan, lastSubstituted)",
          "",
          "[Added Lines]",
          "189:     var firstSubstituted: Option[LogicalPlan] = None",
          "190:     val newPlan = plan.resolveOperatorsDownWithPruning(",
          "194:           resolveCTERelations(relations, isLegacy = false, isCommand, outerCTEDefs, cteDefs) ++",
          "195:             outerCTEDefs",
          "196:         val substituted = substituteCTE(",
          "197:           traverseAndSubstituteCTE(child, isCommand, resolvedCTERelations, cteDefs)._1,",
          "198:           isCommand,",
          "199:           resolvedCTERelations)",
          "200:         if (firstSubstituted.isEmpty) {",
          "201:           firstSubstituted = Some(substituted)",
          "202:         }",
          "203:         substituted",
          "210:     (newPlan, firstSubstituted)",
          "",
          "---------------"
        ],
        "sql/core/src/test/resources/sql-tests/inputs/cte-nested.sql||sql/core/src/test/resources/sql-tests/inputs/cte-nested.sql": [
          "File: sql/core/src/test/resources/sql-tests/inputs/cte-nested.sql -> sql/core/src/test/resources/sql-tests/inputs/cte-nested.sql",
          "--- Hunk 1 ---",
          "[Context before]",
          "146:     )",
          "147:     SELECT * FROM t3",
          "148:   )",
          "",
          "[Removed Lines]",
          "149: SELECT * FROM t2;",
          "",
          "[Added Lines]",
          "151: -- CTE nested in CTE main body FROM clause references outer CTE def",
          "152: WITH cte_outer AS (",
          "153:   SELECT 1",
          "154: )",
          "155: SELECT * FROM (",
          "156:   WITH cte_inner AS (",
          "157:     SELECT * FROM cte_outer",
          "158:   )",
          "159:   SELECT * FROM cte_inner",
          "160: );",
          "162: -- CTE double nested in CTE main body FROM clause references outer CTE def",
          "163: WITH cte_outer AS (",
          "164:   SELECT 1",
          "165: )",
          "166: SELECT * FROM (",
          "167:   WITH cte_inner AS (",
          "168:     SELECT * FROM (",
          "169:       WITH cte_inner_inner AS (",
          "170:         SELECT * FROM cte_outer",
          "171:       )",
          "172:       SELECT * FROM cte_inner_inner",
          "173:     )",
          "174:   )",
          "175:   SELECT * FROM cte_inner",
          "176: );",
          "178: -- Invalid reference to invisible CTE def nested CTE def",
          "179: WITH cte_outer AS (",
          "180:   WITH cte_invisible_inner AS (",
          "181:     SELECT 1",
          "182:   )",
          "183:   SELECT * FROM cte_invisible_inner",
          "184: )",
          "185: SELECT * FROM (",
          "186:   WITH cte_inner AS (",
          "187:     SELECT * FROM cte_invisible_inner",
          "188:   )",
          "189:   SELECT * FROM cte_inner",
          "190: );",
          "192: -- Invalid reference to invisible CTE def nested CTE def (in FROM)",
          "193: WITH cte_outer AS (",
          "194:   SELECT * FROM (",
          "195:     WITH cte_invisible_inner AS (",
          "196:       SELECT 1",
          "197:     )",
          "198:     SELECT * FROM cte_invisible_inner",
          "199:   )",
          "200: )",
          "201: SELECT * FROM (",
          "202:   WITH cte_inner AS (",
          "203:     SELECT * FROM cte_invisible_inner",
          "204:   )",
          "205:   SELECT * FROM cte_inner",
          "206: );",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/CTEInlineSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/CTEInlineSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/CTEInlineSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/CTEInlineSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: package org.apache.spark.sql",
          "20: import org.apache.spark.sql.catalyst.expressions.{And, GreaterThan, LessThan, Literal, Or}",
          "22: import org.apache.spark.sql.execution.adaptive._",
          "23: import org.apache.spark.sql.execution.exchange.ReusedExchangeExec",
          "24: import org.apache.spark.sql.internal.SQLConf",
          "",
          "[Removed Lines]",
          "21: import org.apache.spark.sql.catalyst.plans.logical.{Filter, Project, RepartitionOperation, WithCTE}",
          "",
          "[Added Lines]",
          "21: import org.apache.spark.sql.catalyst.plans.logical._",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "481:       }",
          "482:     }",
          "483:   }",
          "484: }",
          "486: class CTEInlineSuiteAEOff extends CTEInlineSuiteBase with DisableAdaptiveExecutionSuite",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "485:   test(\"Make sure CTESubstitution places WithCTE back in the plan correctly.\") {",
          "486:     withView(\"t\") {",
          "487:       Seq((0, 1), (1, 2)).toDF(\"c1\", \"c2\").createOrReplaceTempView(\"t\")",
          "490:       val df1 = sql(",
          "491:         s\"\"\"",
          "492:            |select count(v1.c3), count(v2.c3) from (",
          "493:            |  with",
          "494:            |  v1 as (",
          "495:            |    select c1, c2, rand() c3 from t",
          "496:            |  )",
          "497:            |  select * from v1",
          "498:            |) v1 join (",
          "499:            |  with",
          "500:            |  v2 as (",
          "501:            |    select c1, c2, rand() c3 from t",
          "502:            |  )",
          "503:            |  select * from v2",
          "504:            |) v2 on v1.c1 = v2.c1",
          "505:          \"\"\".stripMargin)",
          "506:       checkAnswer(df1, Row(2, 2) :: Nil)",
          "507:       df1.queryExecution.analyzed match {",
          "508:         case Aggregate(_, _, WithCTE(_, cteDefs)) => assert(cteDefs.length == 2)",
          "509:         case other => fail(s\"Expect pattern Aggregate(WithCTE(_)) but got $other\")",
          "510:       }",
          "513:       val df2 = sql(",
          "514:         s\"\"\"",
          "515:            |select count(v1.c3), count(v2.c3) from (",
          "516:            |  select c1, c2, rand() c3 from t",
          "517:            |) v1 join (",
          "518:            |  with",
          "519:            |  v2 as (",
          "520:            |    select c1, c2, rand() c3 from t",
          "521:            |  )",
          "522:            |  select * from v2",
          "523:            |) v2 on v1.c1 = v2.c1",
          "524:          \"\"\".stripMargin)",
          "525:       checkAnswer(df2, Row(2, 2) :: Nil)",
          "526:       df2.queryExecution.analyzed match {",
          "527:         case Aggregate(_, _, Join(_, SubqueryAlias(_, WithCTE(_, cteDefs)), _, _, _)) =>",
          "528:           assert(cteDefs.length == 1)",
          "529:         case other => fail(s\"Expect pattern Aggregate(Join(_, WithCTE(_))) but got $other\")",
          "530:       }",
          "533:       val df3 = sql(",
          "534:         s\"\"\"",
          "535:            |select count(v1.c3), count(v2.c3) from (",
          "536:            |  select c1, c2, rand() c3 from t",
          "537:            |) v1 join (",
          "538:            |  select * from (",
          "539:            |    with",
          "540:            |    v1 as (",
          "541:            |      select c1, c2, rand() c3 from t",
          "542:            |    )",
          "543:            |    select * from v1",
          "544:            |  )",
          "545:            |  union all",
          "546:            |  select * from (",
          "547:            |    with",
          "548:            |    v2 as (",
          "549:            |      select c1, c2, rand() c3 from t",
          "550:            |    )",
          "551:            |    select * from v2",
          "552:            |  )",
          "553:            |) v2 on v1.c1 = v2.c1",
          "554:          \"\"\".stripMargin)",
          "555:       checkAnswer(df3, Row(4, 4) :: Nil)",
          "556:       df3.queryExecution.analyzed match {",
          "557:         case Aggregate(_, _, Join(_, SubqueryAlias(_, WithCTE(_: Union, cteDefs)), _, _, _)) =>",
          "558:           assert(cteDefs.length == 2)",
          "559:         case other => fail(",
          "560:           s\"Expect pattern Aggregate(Join(_, (WithCTE(Union(_, _))))) but got $other\")",
          "561:       }",
          "564:       val df4 = sql(",
          "565:         s\"\"\"",
          "566:            |select count(v1.c3), count(v2.c3) from (",
          "567:            |  select c1, c2, rand() c3 from t",
          "568:            |) v1 join (",
          "569:            |  select * from (",
          "570:            |    with",
          "571:            |    v1 as (",
          "572:            |      select c1, c2, rand() c3 from t",
          "573:            |    )",
          "574:            |    select * from v1",
          "575:            |  )",
          "576:            |  union all",
          "577:            |  select c1, c2, rand() c3 from t",
          "578:            |) v2 on v1.c1 = v2.c1",
          "579:          \"\"\".stripMargin)",
          "580:       checkAnswer(df4, Row(4, 4) :: Nil)",
          "581:       df4.queryExecution.analyzed match {",
          "582:         case Aggregate(_, _, Join(_, SubqueryAlias(_, Union(children, _, _)), _, _, _))",
          "583:           if children.head.find(_.isInstanceOf[WithCTE]).isDefined =>",
          "584:           assert(",
          "585:             children.head.collect {",
          "586:               case w: WithCTE => w",
          "587:             }.head.cteDefs.length == 1)",
          "588:         case other => fail(",
          "589:           s\"Expect pattern Aggregate(Join(_, (WithCTE(Union(_, _))))) but got $other\")",
          "590:       }",
          "593:       val df5 = sql(",
          "594:         s\"\"\"",
          "595:            |select count(v1.c3), count(v2.c3) from (",
          "596:            |  with",
          "597:            |  v1 as (",
          "598:            |    select c1, c2, rand() c3 from t",
          "599:            |  )",
          "600:            |  select * from v1",
          "601:            |) v1 join (",
          "602:            |  select c1, c2, rand() c3 from t",
          "603:            |  union all",
          "604:            |  select * from (",
          "605:            |    with",
          "606:            |    v2 as (",
          "607:            |      select c1, c2, rand() c3 from t",
          "608:            |    )",
          "609:            |    select * from v2",
          "610:            |  )",
          "611:            |) v2 on v1.c1 = v2.c1",
          "612:          \"\"\".stripMargin)",
          "613:       checkAnswer(df5, Row(4, 4) :: Nil)",
          "614:       df5.queryExecution.analyzed match {",
          "615:         case Aggregate(_, _, WithCTE(_, cteDefs)) => assert(cteDefs.length == 2)",
          "616:         case other => fail(s\"Expect pattern Aggregate(WithCTE(_)) but got $other\")",
          "617:       }",
          "620:       val df6 = sql(",
          "621:         s\"\"\"",
          "622:            |with",
          "623:            |v1 as (",
          "624:            |  select c1, c2, rand() c3 from t",
          "625:            |)",
          "626:            |select count(v1.c3), count(v2.c3) from",
          "627:            |v1 join (",
          "628:            |  with",
          "629:            |  v2 as (",
          "630:            |    select c1, c2, rand() c3 from t",
          "631:            |  )",
          "632:            |  select * from v2",
          "633:            |) v2 on v1.c1 = v2.c1",
          "634:          \"\"\".stripMargin)",
          "635:       checkAnswer(df6, Row(2, 2) :: Nil)",
          "636:       df6.queryExecution.analyzed match {",
          "637:         case WithCTE(_, cteDefs) => assert(cteDefs.length == 2)",
          "638:         case other => fail(s\"Expect pattern WithCTE(_) but got $other\")",
          "639:       }",
          "640:     }",
          "641:   }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "cb9492534820ce6b2b419a062926da7a7bf09b6a",
      "candidate_info": {
        "commit_hash": "cb9492534820ce6b2b419a062926da7a7bf09b6a",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/cb9492534820ce6b2b419a062926da7a7bf09b6a",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala"
        ],
        "message": "[SPARK-39548][SQL] CreateView Command with a window clause query hit a wrong window definition not found issue\n\n### What changes were proposed in this pull request?\n\n1. In the inline CTE code path, fix a bug that top down style unresolved window expression check leads to mis-clarification of a defined window expression.\n2. Move unresolved window expression check in project to `CheckAnalysis`.\n\n### Why are the changes needed?\n\nThis bug fails a correct query.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nUT\n\nCloses #36947 from amaliujia/improvewindow.\n\nAuthored-by: Rui Wang <rui.wang@databricks.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\n(cherry picked from commit 4718d59c6c4e201bf940303a4311dfb753372395)\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala||sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "452:   object WindowsSubstitution extends Rule[LogicalPlan] {",
          "454:       _.containsAnyPattern(WITH_WINDOW_DEFINITION, UNRESOLVED_WINDOW_EXPRESSION), ruleId) {",
          "456:       case WithWindowDefinition(windowDefinitions, child) => child.resolveExpressions {",
          "",
          "[Removed Lines]",
          "453:     def apply(plan: LogicalPlan): LogicalPlan = plan.resolveOperatorsDownWithPruning(",
          "",
          "[Added Lines]",
          "453:     def apply(plan: LogicalPlan): LogicalPlan = plan.resolveOperatorsUpWithPruning(",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "459:             throw QueryCompilationErrors.windowSpecificationNotDefinedError(windowName))",
          "460:           WindowExpression(c, windowSpecDefinition)",
          "461:       }",
          "470:     }",
          "471:   }",
          "",
          "[Removed Lines]",
          "463:       case p @ Project(projectList, _) =>",
          "464:         projectList.foreach(_.transformDownWithPruning(",
          "465:           _.containsPattern(UNRESOLVED_WINDOW_EXPRESSION), ruleId) {",
          "466:           case UnresolvedWindowExpression(_, windowSpec) =>",
          "467:             throw QueryCompilationErrors.windowSpecificationNotDefinedError(windowSpec.name)",
          "468:         })",
          "469:         p",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "26: import org.apache.spark.sql.catalyst.plans._",
          "27: import org.apache.spark.sql.catalyst.plans.logical._",
          "28: import org.apache.spark.sql.catalyst.trees.TreeNodeTag",
          "29: import org.apache.spark.sql.catalyst.util.{CharVarcharUtils, StringUtils, TypeUtils}",
          "30: import org.apache.spark.sql.connector.catalog.{LookupCatalog, SupportsPartitionManagement}",
          "31: import org.apache.spark.sql.errors.QueryCompilationErrors",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "29: import org.apache.spark.sql.catalyst.trees.TreePattern.UNRESOLVED_WINDOW_EXPRESSION",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "226:             failAnalysis(\"grouping_id() can only be used with GroupingSets/Cube/Rollup\")",
          "228:           case e: Expression if e.children.exists(_.isInstanceOf[WindowFunction]) &&",
          "230:             val w = e.children.find(_.isInstanceOf[WindowFunction]).get",
          "231:             failAnalysis(s\"Window function $w requires an OVER clause.\")",
          "",
          "[Removed Lines]",
          "229:               !e.isInstanceOf[WindowExpression] =>",
          "",
          "[Added Lines]",
          "230:               !e.isInstanceOf[WindowExpression] && e.resolved =>",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "523:               s\"\"\"Only a single table generating function is allowed in a SELECT clause, found:",
          "524:                  | ${exprs.map(_.sql).mkString(\",\")}\"\"\".stripMargin)",
          "526:           case j: Join if !j.duplicateResolved =>",
          "527:             val conflictingAttributes = j.left.outputSet.intersect(j.right.outputSet)",
          "528:             failAnalysis(",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "527:           case p @ Project(projectList, _) =>",
          "528:             projectList.foreach(_.transformDownWithPruning(",
          "529:               _.containsPattern(UNRESOLVED_WINDOW_EXPRESSION)) {",
          "530:               case UnresolvedWindowExpression(_, windowSpec) =>",
          "531:                 throw QueryCompilationErrors.windowSpecificationNotDefinedError(windowSpec.name)",
          "532:             })",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala||sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "4503:         \"\"\".stripMargin),",
          "4504:       Seq(Row(2), Row(1)))",
          "4505:   }",
          "4506: }",
          "4508: case class Foo(bar: Option[String])",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "4507:   test(\"SPARK-39548: CreateView will make queries go into inline CTE code path thus\" +",
          "4508:     \"trigger a mis-clarified `window definition not found` issue\") {",
          "4509:     sql(",
          "4510:       \"\"\"",
          "4511:         |create or replace temporary view test_temp_view as",
          "4512:         |with step_1 as (",
          "4513:         |select * , min(a) over w2 as min_a_over_w2 from",
          "4514:         |(select 1 as a, 2 as b, 3 as c) window w2 as (partition by b order by c)) , step_2 as",
          "4515:         |(",
          "4516:         |select *, max(e) over w1 as max_a_over_w1",
          "4517:         |from (select 1 as e, 2 as f, 3 as g)",
          "4518:         |join step_1 on true",
          "4519:         |window w1 as (partition by f order by g)",
          "4520:         |)",
          "4521:         |select *",
          "4522:         |from step_2",
          "4523:         |\"\"\".stripMargin)",
          "4525:     checkAnswer(",
          "4526:       sql(\"select * from test_temp_view\"),",
          "4527:       Row(1, 2, 3, 1, 2, 3, 1, 1))",
          "4528:   }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "f7268008b139e2dcf6987432ddb33c4d4489a399",
      "candidate_info": {
        "commit_hash": "f7268008b139e2dcf6987432ddb33c4d4489a399",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/f7268008b139e2dcf6987432ddb33c4d4489a399",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryErrorsBase.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryParsingErrors.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/encoders/EncoderResolutionSuite.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/AnsiCastSuiteBase.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CastSuite.scala",
          "sql/core/src/test/resources/sql-tests/results/postgreSQL/float4.sql.out",
          "sql/core/src/test/resources/sql-tests/results/postgreSQL/float8.sql.out",
          "sql/core/src/test/resources/sql-tests/results/postgreSQL/int8.sql.out",
          "sql/core/src/test/scala/org/apache/spark/sql/DatasetSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryCompilationErrorsSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionErrorsSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/sources/InsertSuite.scala"
        ],
        "message": "[SPARK-38926][SQL][3.3] Output types in error messages in SQL style\n\n### What changes were proposed in this pull request?\nIn the PR, I propose to upper case SQL types in error messages similar to the SQL standard. I added new util functions `toSQLType()` to the trait `QueryErrorsBase`, and applied it in `Query.*Errors` (also modified tests in `Query.*ErrorsSuite`). For example:\n\nBefore:\n```sql\nCannot up cast b.`b` from decimal(38,18) to bigint.\n```\n\nAfter:\n```sql\nCannot up cast b.`b` from DECIMAL(38,18) to BIGINT.\n```\n\n### Why are the changes needed?\nTo improve user experience with Spark SQL. The changes highlight SQL types in error massages and make them more visible for users.\n\n### Does this PR introduce _any_ user-facing change?\nNo since error classes haven't been released yet.\n\n### How was this patch tested?\nBy running the modified test suites:\n```\n$ build/sbt \"test:testOnly *QueryParsingErrorsSuite\"\n$ build/sbt \"test:testOnly *QueryCompilationErrorsSuite\"\n$ build/sbt \"test:testOnly *QueryExecutionErrorsSuite\"\n$ build/sbt \"testOnly *CastSuite\"\n$ build/sbt \"testOnly *AnsiCastSuiteWithAnsiModeOn\"\n$ build/sbt \"testOnly *EncoderResolutionSuite\"\n$ build/sbt \"test:testOnly *DatasetSuite\"\n$ build/sbt \"test:testOnly *InsertSuite\"\n```\n\nAuthored-by: Max Gekk <max.gekkgmail.com>\nSigned-off-by: Max Gekk <max.gekkgmail.com>\n(cherry picked from commit 0d16159bfa85ed346843e0952f37922a579c011e)\nSigned-off-by: Max Gekk <max.gekkgmail.com>\n\nCloses #36247 from MaxGekk/error-class-toSQLType-3.3.\n\nAuthored-by: Max Gekk <max.gekk@gmail.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryErrorsBase.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryErrorsBase.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryParsingErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryParsingErrors.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/encoders/EncoderResolutionSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/encoders/EncoderResolutionSuite.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/AnsiCastSuiteBase.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/AnsiCastSuiteBase.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CastSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CastSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/DatasetSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/DatasetSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryCompilationErrorsSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/errors/QueryCompilationErrorsSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionErrorsSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionErrorsSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/sources/InsertSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/sources/InsertSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "51:   def groupingIDMismatchError(groupingID: GroupingID, groupByExprs: Seq[Expression]): Throwable = {",
          "52:     new AnalysisException(",
          "",
          "[Removed Lines]",
          "49: object QueryCompilationErrors {",
          "",
          "[Added Lines]",
          "49: object QueryCompilationErrors extends QueryErrorsBase {",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "161:       errorClass = \"CANNOT_UP_CAST_DATATYPE\",",
          "162:       messageParameters = Array(",
          "163:         fromStr,",
          "166:         s\"The type path of the target object is:\\n\" + walkedTypePath.mkString(\"\", \"\\n\", \"\\n\") +",
          "167:           \"You can either add an explicit cast to the input data or choose a higher precision \" +",
          "168:           \"type of the field in the target object\"",
          "",
          "[Removed Lines]",
          "164:         from.dataType.catalogString,",
          "165:         to.catalogString,",
          "",
          "[Added Lines]",
          "164:         toSQLType(from.dataType),",
          "165:         toSQLType(to),",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryErrorsBase.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryErrorsBase.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryErrorsBase.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryErrorsBase.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "44:   def toSQLValue(v: Any, t: DataType): String = {",
          "45:     litToErrorValue(Literal.create(v, t))",
          "46:   }",
          "47: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "48:   def toSQLType(t: DataType): String = {",
          "49:     t.sql",
          "50:   }",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "92:   def castingCauseOverflowError(t: Any, dataType: DataType): ArithmeticException = {",
          "93:     new SparkArithmeticException(errorClass = \"CAST_CAUSES_OVERFLOW\",",
          "95:   }",
          "97:   def cannotChangeDecimalPrecisionError(",
          "",
          "[Removed Lines]",
          "94:       messageParameters = Array(toSQLValue(t), dataType.catalogString, SQLConf.ANSI_ENABLED.key))",
          "",
          "[Added Lines]",
          "94:       messageParameters = Array(toSQLValue(t), toSQLType(dataType), SQLConf.ANSI_ENABLED.key))",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "252:     new SparkRuntimeException(",
          "253:       errorClass = \"UNSUPPORTED_FEATURE\",",
          "254:       messageParameters = Array(",
          "257:   }",
          "259:   def noDefaultForDataTypeError(dataType: DataType): RuntimeException = {",
          "",
          "[Removed Lines]",
          "255:         s\"pivoting by the value '${v.toString}' of the column data type\" +",
          "256:         s\" '${dataType.catalogString}'.\"))",
          "",
          "[Added Lines]",
          "255:         s\"pivoting by the value '${v.toString}' of the column data type ${toSQLType(dataType)}.\"))",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "1651:     new SparkUnsupportedOperationException(",
          "1652:       errorClass = \"UNSUPPORTED_OPERATION\",",
          "1653:       messageParameters = Array(",
          "1656:     )",
          "1657:   }",
          "",
          "[Removed Lines]",
          "1654:         s\"${TimestampType.catalogString} must supply timeZoneId parameter \" +",
          "1655:           s\"while converting to ArrowType\")",
          "",
          "[Added Lines]",
          "1653:         s\"${toSQLType(TimestampType)} must supply timeZoneId parameter \" +",
          "1654:           s\"while converting to the arrow timestamp type.\")",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "1969:   def cannotConvertOrcTimestampToTimestampNTZError(): Throwable = {",
          "1970:     new SparkUnsupportedOperationException(",
          "1971:       errorClass = \"UNSUPPORTED_OPERATION\",",
          "1973:   }",
          "1975:   def cannotConvertOrcTimestampNTZToTimestampLTZError(): Throwable = {",
          "1976:     new SparkUnsupportedOperationException(",
          "1977:       errorClass = \"UNSUPPORTED_OPERATION\",",
          "1980:   }",
          "1982:   def writePartitionExceedConfigSizeWhenDynamicPartitionError(",
          "",
          "[Removed Lines]",
          "1972:       messageParameters = Array(\"Unable to convert timestamp of Orc to data type 'timestamp_ntz'\"))",
          "1978:       messageParameters =",
          "1979:         Array(\"Unable to convert timestamp ntz of Orc to data type 'timestamp_ltz'\"))",
          "",
          "[Added Lines]",
          "1971:       messageParameters = Array(",
          "1972:         s\"Unable to convert ${toSQLType(TimestampType)} of Orc to \" +",
          "1973:         s\"data type ${toSQLType(TimestampNTZType)}.\"))",
          "1979:       messageParameters = Array(",
          "1980:         s\"Unable to convert ${toSQLType(TimestampNTZType)} of Orc to \" +",
          "1981:         s\"data type ${toSQLType(TimestampType)}.\"))",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryParsingErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryParsingErrors.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryParsingErrors.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryParsingErrors.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "23: import org.apache.spark.sql.catalyst.parser.SqlBaseParser._",
          "24: import org.apache.spark.sql.catalyst.trees.Origin",
          "25: import org.apache.spark.sql.connector.catalog.CatalogV2Implicits._",
          "33:   def invalidInsertIntoError(ctx: InsertIntoContext): Throwable = {",
          "34:     new ParseException(\"Invalid InsertIntoContext\", ctx)",
          "",
          "[Removed Lines]",
          "31: object QueryParsingErrors {",
          "",
          "[Added Lines]",
          "26: import org.apache.spark.sql.types.StringType",
          "32: object QueryParsingErrors extends QueryErrorsBase {",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "301:   }",
          "303:   def showFunctionsInvalidPatternError(pattern: String, ctx: ParserRuleContext): Throwable = {",
          "306:   }",
          "308:   def duplicateCteDefinitionNamesError(duplicateNames: String, ctx: CtesContext): Throwable = {",
          "",
          "[Removed Lines]",
          "304:     new ParseException(s\"Invalid pattern in SHOW FUNCTIONS: $pattern. It must be \" +",
          "305:       \"a string literal.\", ctx)",
          "",
          "[Added Lines]",
          "305:     new ParseException(",
          "306:       errorClass = \"INVALID_SQL_SYNTAX\",",
          "307:       messageParameters = Array(",
          "308:         s\"Invalid pattern in SHOW FUNCTIONS: $pattern. \" +",
          "309:         s\"It must be a ${toSQLType(StringType)} literal.\"),",
          "310:       ctx)",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/encoders/EncoderResolutionSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/encoders/EncoderResolutionSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/encoders/EncoderResolutionSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/encoders/EncoderResolutionSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "88:     val attrs = Seq('arr.array(StringType))",
          "89:     assert(intercept[AnalysisException](encoder.resolveAndBind(attrs)).message ==",
          "90:       s\"\"\"",
          "92:          |The type path of the target object is:",
          "93:          |- array element class: \"scala.Long\"",
          "94:          |- field (class: \"scala.Array\", name: \"arr\")",
          "",
          "[Removed Lines]",
          "91:          |Cannot up cast array element from string to bigint.",
          "",
          "[Added Lines]",
          "91:          |Cannot up cast array element from STRING to BIGINT.",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "211:       val attrs = Seq(attr)",
          "212:       assert(intercept[AnalysisException](encoder.resolveAndBind(attrs)).message ==",
          "213:         s\"\"\"",
          "215:            |The type path of the target object is:",
          "216:            |- root class: \"java.lang.String\"",
          "217:            |You can either add an explicit cast to the input data or choose a higher precision type",
          "",
          "[Removed Lines]",
          "214:            |Cannot up cast a from ${attr.dataType.catalogString} to string.",
          "",
          "[Added Lines]",
          "214:            |Cannot up cast a from ${attr.dataType.sql} to STRING.",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "225:     }.message",
          "226:     assert(msg1 ==",
          "227:       s\"\"\"",
          "229:          |The type path of the target object is:",
          "230:          |- field (class: \"scala.Int\", name: \"b\")",
          "231:          |- root class: \"org.apache.spark.sql.catalyst.encoders.StringIntClass\"",
          "",
          "[Removed Lines]",
          "228:          |Cannot up cast b from bigint to int.",
          "",
          "[Added Lines]",
          "228:          |Cannot up cast b from BIGINT to INT.",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "238:     }.message",
          "239:     assert(msg2 ==",
          "240:       s\"\"\"",
          "242:          |The type path of the target object is:",
          "243:          |- field (class: \"scala.Long\", name: \"b\")",
          "244:          |- field (class: \"org.apache.spark.sql.catalyst.encoders.StringLongClass\", name: \"b\")",
          "",
          "[Removed Lines]",
          "241:          |Cannot up cast b.`b` from decimal(38,18) to bigint.",
          "",
          "[Added Lines]",
          "241:          |Cannot up cast b.`b` from DECIMAL(38,18) to BIGINT.",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/AnsiCastSuiteBase.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/AnsiCastSuiteBase.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/AnsiCastSuiteBase.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/AnsiCastSuiteBase.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "279:       assert(negativeTs.getTime < 0)",
          "280:       Seq(ByteType, ShortType, IntegerType).foreach { dt =>",
          "281:         checkExceptionInExpression[SparkArithmeticException](",
          "283:       }",
          "284:     }",
          "285:   }",
          "",
          "[Removed Lines]",
          "282:           cast(negativeTs, dt), s\"to ${dt.catalogString} causes overflow\")",
          "",
          "[Added Lines]",
          "282:           cast(negativeTs, dt), s\"to ${dt.sql} causes overflow\")",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "290:       assert(negativeTs.getTime < 0)",
          "291:       Seq(ByteType, ShortType, IntegerType).foreach { dt =>",
          "292:         checkExceptionInExpression[SparkArithmeticException](",
          "294:       }",
          "295:       val expectedSecs = Math.floorDiv(negativeTs.getTime, MILLIS_PER_SECOND)",
          "296:       checkEvaluation(cast(negativeTs, LongType), expectedSecs)",
          "",
          "[Removed Lines]",
          "293:           cast(negativeTs, dt), s\"to ${dt.catalogString} causes overflow\")",
          "",
          "[Added Lines]",
          "293:           cast(negativeTs, dt), s\"to ${dt.sql} causes overflow\")",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CastSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CastSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CastSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CastSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "592:       val e1 = intercept[ArithmeticException] {",
          "593:         Cast(Literal(Byte.MaxValue + 1), ByteType).eval()",
          "594:       }.getMessage",
          "596:       val e2 = intercept[ArithmeticException] {",
          "597:         Cast(Literal(Short.MaxValue + 1), ShortType).eval()",
          "598:       }.getMessage",
          "600:       val e3 = intercept[ArithmeticException] {",
          "601:         Cast(Literal(Int.MaxValue + 1L), IntegerType).eval()",
          "602:       }.getMessage",
          "604:     }",
          "605:   }",
          "",
          "[Removed Lines]",
          "595:       assert(e1.contains(\"Casting 128 to tinyint causes overflow\"))",
          "599:       assert(e2.contains(\"Casting 32768 to smallint causes overflow\"))",
          "603:       assert(e3.contains(\"Casting 2147483648L to int causes overflow\"))",
          "",
          "[Added Lines]",
          "595:       assert(e1.contains(\"Casting 128 to TINYINT causes overflow\"))",
          "599:       assert(e2.contains(\"Casting 32768 to SMALLINT causes overflow\"))",
          "603:       assert(e3.contains(\"Casting 2147483648L to INT causes overflow\"))",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "642:           checkEvaluation(cast(v2, LongType), 25L)",
          "643:         case MINUTE =>",
          "644:           checkExceptionInExpression[ArithmeticException](cast(v2, ByteType),",
          "646:           checkEvaluation(cast(v2, ShortType), (MINUTES_PER_HOUR * 25 + 1).toShort)",
          "647:           checkEvaluation(cast(v2, IntegerType), (MINUTES_PER_HOUR * 25 + 1).toInt)",
          "648:           checkEvaluation(cast(v2, LongType), MINUTES_PER_HOUR * 25 + 1)",
          "649:         case SECOND =>",
          "650:           checkExceptionInExpression[ArithmeticException](cast(v2, ByteType),",
          "652:           checkExceptionInExpression[ArithmeticException](cast(v2, ShortType),",
          "654:           checkEvaluation(cast(v2, IntegerType), num.toInt)",
          "655:           checkEvaluation(cast(v2, LongType), num)",
          "656:       }",
          "",
          "[Removed Lines]",
          "645:             s\"Casting $v2 to tinyint causes overflow\")",
          "651:             s\"Casting $v2 to tinyint causes overflow\")",
          "653:             s\"Casting $v2 to smallint causes overflow\")",
          "",
          "[Added Lines]",
          "645:             s\"Casting $v2 to TINYINT causes overflow\")",
          "651:             s\"Casting $v2 to TINYINT causes overflow\")",
          "653:             s\"Casting $v2 to SMALLINT causes overflow\")",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "659:       dt.endField match {",
          "660:         case DAY =>",
          "661:           checkExceptionInExpression[ArithmeticException](cast(v3, ByteType),",
          "663:           checkExceptionInExpression[ArithmeticException](cast(v3, ShortType),",
          "665:           checkEvaluation(cast(v3, IntegerType), (Long.MaxValue / MICROS_PER_DAY).toInt)",
          "666:           checkEvaluation(cast(v3, LongType), Long.MaxValue / MICROS_PER_DAY)",
          "667:         case HOUR =>",
          "668:           checkExceptionInExpression[ArithmeticException](cast(v3, ByteType),",
          "670:           checkExceptionInExpression[ArithmeticException](cast(v3, ShortType),",
          "672:           checkExceptionInExpression[ArithmeticException](cast(v3, IntegerType),",
          "674:           checkEvaluation(cast(v3, LongType), Long.MaxValue / MICROS_PER_HOUR)",
          "675:         case MINUTE =>",
          "676:           checkExceptionInExpression[ArithmeticException](cast(v3, ByteType),",
          "678:           checkExceptionInExpression[ArithmeticException](cast(v3, ShortType),",
          "680:           checkExceptionInExpression[ArithmeticException](cast(v3, IntegerType),",
          "682:           checkEvaluation(cast(v3, LongType), Long.MaxValue / MICROS_PER_MINUTE)",
          "683:         case SECOND =>",
          "684:           checkExceptionInExpression[ArithmeticException](cast(v3, ByteType),",
          "686:           checkExceptionInExpression[ArithmeticException](cast(v3, ShortType),",
          "688:           checkExceptionInExpression[ArithmeticException](cast(v3, IntegerType),",
          "690:           checkEvaluation(cast(v3, LongType), Long.MaxValue / MICROS_PER_SECOND)",
          "691:       }",
          "",
          "[Removed Lines]",
          "662:             s\"Casting $v3 to tinyint causes overflow\")",
          "664:             s\"Casting $v3 to smallint causes overflow\")",
          "669:             s\"Casting $v3 to tinyint causes overflow\")",
          "671:             s\"Casting $v3 to smallint causes overflow\")",
          "673:             s\"Casting $v3 to int causes overflow\")",
          "677:             s\"Casting $v3 to tinyint causes overflow\")",
          "679:             s\"Casting $v3 to smallint causes overflow\")",
          "681:             s\"Casting $v3 to int causes overflow\")",
          "685:             s\"Casting $v3 to tinyint causes overflow\")",
          "687:             s\"Casting $v3 to smallint causes overflow\")",
          "689:             s\"Casting $v3 to int causes overflow\")",
          "",
          "[Added Lines]",
          "662:             s\"Casting $v3 to TINYINT causes overflow\")",
          "664:             s\"Casting $v3 to SMALLINT causes overflow\")",
          "669:             s\"Casting $v3 to TINYINT causes overflow\")",
          "671:             s\"Casting $v3 to SMALLINT causes overflow\")",
          "673:             s\"Casting $v3 to INT causes overflow\")",
          "677:             s\"Casting $v3 to TINYINT causes overflow\")",
          "679:             s\"Casting $v3 to SMALLINT causes overflow\")",
          "681:             s\"Casting $v3 to INT causes overflow\")",
          "685:             s\"Casting $v3 to TINYINT causes overflow\")",
          "687:             s\"Casting $v3 to SMALLINT causes overflow\")",
          "689:             s\"Casting $v3 to INT causes overflow\")",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "694:       dt.endField match {",
          "695:         case DAY =>",
          "696:           checkExceptionInExpression[ArithmeticException](cast(v4, ByteType),",
          "698:           checkExceptionInExpression[ArithmeticException](cast(v4, ShortType),",
          "700:           checkEvaluation(cast(v4, IntegerType), (Long.MinValue / MICROS_PER_DAY).toInt)",
          "701:           checkEvaluation(cast(v4, LongType), Long.MinValue / MICROS_PER_DAY)",
          "702:         case HOUR =>",
          "703:           checkExceptionInExpression[ArithmeticException](cast(v4, ByteType),",
          "705:           checkExceptionInExpression[ArithmeticException](cast(v4, ShortType),",
          "707:           checkExceptionInExpression[ArithmeticException](cast(v4, IntegerType),",
          "709:           checkEvaluation(cast(v4, LongType), Long.MinValue / MICROS_PER_HOUR)",
          "710:         case MINUTE =>",
          "711:           checkExceptionInExpression[ArithmeticException](cast(v4, ByteType),",
          "713:           checkExceptionInExpression[ArithmeticException](cast(v4, ShortType),",
          "715:           checkExceptionInExpression[ArithmeticException](cast(v4, IntegerType),",
          "717:           checkEvaluation(cast(v4, LongType), Long.MinValue / MICROS_PER_MINUTE)",
          "718:         case SECOND =>",
          "719:           checkExceptionInExpression[ArithmeticException](cast(v4, ByteType),",
          "721:           checkExceptionInExpression[ArithmeticException](cast(v4, ShortType),",
          "723:           checkExceptionInExpression[ArithmeticException](cast(v4, IntegerType),",
          "725:           checkEvaluation(cast(v4, LongType), Long.MinValue / MICROS_PER_SECOND)",
          "726:       }",
          "727:     }",
          "",
          "[Removed Lines]",
          "697:             s\"Casting $v4 to tinyint causes overflow\")",
          "699:             s\"Casting $v4 to smallint causes overflow\")",
          "704:             s\"Casting $v4 to tinyint causes overflow\")",
          "706:             s\"Casting $v4 to smallint causes overflow\")",
          "708:             s\"Casting $v4 to int causes overflow\")",
          "712:             s\"Casting $v4 to tinyint causes overflow\")",
          "714:             s\"Casting $v4 to smallint causes overflow\")",
          "716:             s\"Casting $v4 to int causes overflow\")",
          "720:             s\"Casting $v4 to tinyint causes overflow\")",
          "722:             s\"Casting $v4 to smallint causes overflow\")",
          "724:             s\"Casting $v4 to int causes overflow\")",
          "",
          "[Added Lines]",
          "697:             s\"Casting $v4 to TINYINT causes overflow\")",
          "699:             s\"Casting $v4 to SMALLINT causes overflow\")",
          "704:             s\"Casting $v4 to TINYINT causes overflow\")",
          "706:             s\"Casting $v4 to SMALLINT causes overflow\")",
          "708:             s\"Casting $v4 to INT causes overflow\")",
          "712:             s\"Casting $v4 to TINYINT causes overflow\")",
          "714:             s\"Casting $v4 to SMALLINT causes overflow\")",
          "716:             s\"Casting $v4 to INT causes overflow\")",
          "720:             s\"Casting $v4 to TINYINT causes overflow\")",
          "722:             s\"Casting $v4 to SMALLINT causes overflow\")",
          "724:             s\"Casting $v4 to INT causes overflow\")",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "777:     ).foreach {",
          "778:       case (v, toType) =>",
          "779:         checkExceptionInExpression[ArithmeticException](cast(v, toType),",
          "781:     }",
          "783:     Seq(",
          "",
          "[Removed Lines]",
          "780:           s\"Casting $v to ${toType.catalogString} causes overflow\")",
          "",
          "[Added Lines]",
          "780:           s\"Casting $v to ${toType.sql} causes overflow\")",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "792:     ).foreach {",
          "793:       case (v, toType) =>",
          "794:         checkExceptionInExpression[ArithmeticException](cast(v, toType),",
          "796:     }",
          "797:   }",
          "",
          "[Removed Lines]",
          "795:           s\"Casting ${v}L to ${toType.catalogString} causes overflow\")",
          "",
          "[Added Lines]",
          "795:           s\"Casting ${v}L to ${toType.sql} causes overflow\")",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "829:       case (v, dt, toType) =>",
          "830:         val value = Literal.create(v, dt)",
          "831:         checkExceptionInExpression[ArithmeticException](cast(value, toType),",
          "833:     }",
          "835:     Seq(",
          "",
          "[Removed Lines]",
          "832:           s\"Casting $value to ${toType.catalogString} causes overflow\")",
          "",
          "[Added Lines]",
          "832:           s\"Casting $value to ${toType.sql} causes overflow\")",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "887:     ).foreach {",
          "888:       case (v, toType) =>",
          "889:         checkExceptionInExpression[ArithmeticException](cast(v, toType),",
          "891:     }",
          "893:     Seq(",
          "",
          "[Removed Lines]",
          "890:           s\"Casting $v to ${toType.catalogString} causes overflow\")",
          "",
          "[Added Lines]",
          "890:           s\"Casting $v to ${toType.sql} causes overflow\")",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "898:     ).foreach {",
          "899:       case (v, toType) =>",
          "900:         checkExceptionInExpression[ArithmeticException](cast(v, toType),",
          "902:     }",
          "903:   }",
          "904: }",
          "",
          "[Removed Lines]",
          "901:           s\"Casting ${v}L to ${toType.catalogString} causes overflow\")",
          "",
          "[Added Lines]",
          "901:           s\"Casting ${v}L to ${toType.sql} causes overflow\")",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/DatasetSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/DatasetSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/DatasetSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/DatasetSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "1951:         .map(b => b - 1)",
          "1952:         .collect()",
          "1953:     }",
          "1955:   }",
          "1957:   test(\"SPARK-26690: checkpoints should be executed with an execution id\") {",
          "",
          "[Removed Lines]",
          "1954:     assert(thrownException.message.contains(\"Cannot up cast id from bigint to tinyint\"))",
          "",
          "[Added Lines]",
          "1954:     assert(thrownException.message.contains(\"Cannot up cast id from BIGINT to TINYINT\"))",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryCompilationErrorsSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/errors/QueryCompilationErrorsSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/errors/QueryCompilationErrorsSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/errors/QueryCompilationErrorsSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "37:     }.message",
          "38:     assert(msg1 ===",
          "39:       s\"\"\"",
          "41:          |The type path of the target object is:",
          "42:          |- field (class: \"scala.Int\", name: \"b\")",
          "43:          |- root class: \"org.apache.spark.sql.errors.StringIntClass\"",
          "",
          "[Removed Lines]",
          "40:          |Cannot up cast b from bigint to int.",
          "",
          "[Added Lines]",
          "40:          |Cannot up cast b from BIGINT to INT.",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "51:     }.message",
          "52:     assert(msg2 ===",
          "53:       s\"\"\"",
          "55:          |The type path of the target object is:",
          "56:          |- field (class: \"scala.Long\", name: \"b\")",
          "57:          |- field (class: \"org.apache.spark.sql.errors.StringLongClass\", name: \"b\")",
          "",
          "[Removed Lines]",
          "54:          |Cannot up cast b.`b` from decimal(38,18) to bigint.",
          "",
          "[Added Lines]",
          "54:          |Cannot up cast b.`b` from DECIMAL(38,18) to BIGINT.",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionErrorsSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionErrorsSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionErrorsSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionErrorsSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "142:         .collect()",
          "143:     }",
          "144:     assert(e2.getMessage === \"The feature is not supported: pivoting by the value\" +",
          "146:   }",
          "148:   test(\"UNSUPPORTED_FEATURE: unsupported pivot operations\") {",
          "",
          "[Removed Lines]",
          "145:       \"\"\" '[dotnet,Dummies]' of the column data type 'struct<col1:string,training:string>'.\"\"\")",
          "",
          "[Added Lines]",
          "145:       \"\"\" '[dotnet,Dummies]' of the column data type STRUCT<col1: STRING, training: STRING>.\"\"\")",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "237:     assert(e.getErrorClass === \"UNSUPPORTED_OPERATION\")",
          "238:     assert(e.getMessage === \"The operation is not supported: \" +",
          "240:   }",
          "242:   test(\"UNSUPPORTED_OPERATION - SPARK-36346: can't read Timestamp as TimestampNTZ\") {",
          "",
          "[Removed Lines]",
          "239:       \"timestamp must supply timeZoneId parameter while converting to ArrowType\")",
          "",
          "[Added Lines]",
          "239:       \"TIMESTAMP must supply timeZoneId parameter while converting to the arrow timestamp type.\")",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "250:         assert(e.getErrorClass === \"UNSUPPORTED_OPERATION\")",
          "251:         assert(e.getMessage === \"The operation is not supported: \" +",
          "253:       }",
          "254:     }",
          "255:   }",
          "",
          "[Removed Lines]",
          "252:           \"Unable to convert timestamp of Orc to data type 'timestamp_ntz'\")",
          "",
          "[Added Lines]",
          "252:           \"Unable to convert TIMESTAMP of Orc to data type TIMESTAMP_NTZ.\")",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "265:         assert(e.getErrorClass === \"UNSUPPORTED_OPERATION\")",
          "266:         assert(e.getMessage === \"The operation is not supported: \" +",
          "268:       }",
          "269:     }",
          "270:   }",
          "",
          "[Removed Lines]",
          "267:           \"Unable to convert timestamp ntz of Orc to data type 'timestamp_ltz'\")",
          "",
          "[Added Lines]",
          "267:           \"Unable to convert TIMESTAMP_NTZ of Orc to data type TIMESTAMP.\")",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/sources/InsertSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/sources/InsertSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/sources/InsertSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/sources/InsertSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "713:         var msg = intercept[SparkException] {",
          "714:           sql(s\"insert into t values($outOfRangeValue1)\")",
          "715:         }.getCause.getMessage",
          "718:         val outOfRangeValue2 = (Int.MinValue - 1L).toString",
          "719:         msg = intercept[SparkException] {",
          "720:           sql(s\"insert into t values($outOfRangeValue2)\")",
          "721:         }.getCause.getMessage",
          "723:       }",
          "724:     }",
          "725:   }",
          "",
          "[Removed Lines]",
          "716:         assert(msg.contains(s\"Casting ${outOfRangeValue1}L to int causes overflow\"))",
          "722:         assert(msg.contains(s\"Casting ${outOfRangeValue2}L to int causes overflow\"))",
          "",
          "[Added Lines]",
          "716:         assert(msg.contains(s\"Casting ${outOfRangeValue1}L to INT causes overflow\"))",
          "722:         assert(msg.contains(s\"Casting ${outOfRangeValue2}L to INT causes overflow\"))",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "733:         var msg = intercept[SparkException] {",
          "734:           sql(s\"insert into t values(${outOfRangeValue1}D)\")",
          "735:         }.getCause.getMessage",
          "738:         val outOfRangeValue2 = Math.nextDown(Long.MinValue)",
          "739:         msg = intercept[SparkException] {",
          "740:           sql(s\"insert into t values(${outOfRangeValue2}D)\")",
          "741:         }.getCause.getMessage",
          "743:       }",
          "744:     }",
          "745:   }",
          "",
          "[Removed Lines]",
          "736:         assert(msg.contains(s\"Casting ${outOfRangeValue1}D to bigint causes overflow\"))",
          "742:         assert(msg.contains(s\"Casting ${outOfRangeValue2}D to bigint causes overflow\"))",
          "",
          "[Added Lines]",
          "736:         assert(msg.contains(s\"Casting ${outOfRangeValue1}D to BIGINT causes overflow\"))",
          "742:         assert(msg.contains(s\"Casting ${outOfRangeValue2}D to BIGINT causes overflow\"))",
          "",
          "---------------"
        ]
      }
    }
  ]
}