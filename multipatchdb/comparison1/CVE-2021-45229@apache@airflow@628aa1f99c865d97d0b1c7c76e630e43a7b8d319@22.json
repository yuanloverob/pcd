{
  "cve_id": "CVE-2021-45229",
  "cve_desc": "It was discovered that the \"Trigger DAG with config\" screen was susceptible to XSS attacks via the `origin` query argument. This issue affects Apache Airflow versions 2.2.3 and below.",
  "repo": "apache/airflow",
  "patch_hash": "628aa1f99c865d97d0b1c7c76e630e43a7b8d319",
  "patch_info": {
    "commit_hash": "628aa1f99c865d97d0b1c7c76e630e43a7b8d319",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/628aa1f99c865d97d0b1c7c76e630e43a7b8d319",
    "files": [
      "airflow/www/templates/airflow/trigger.html",
      "tests/www/views/test_views_trigger_dag.py"
    ],
    "message": "Simplify trigger cancel button (#21591)\n\nCo-authored-by: Jed Cunningham <jedcunningham@apache.org>\n(cherry picked from commit 65297673a318660fba76797e50d0c06804dfcafc)",
    "before_after_code_files": [
      "airflow/www/templates/airflow/trigger.html||airflow/www/templates/airflow/trigger.html",
      "tests/www/views/test_views_trigger_dag.py||tests/www/views/test_views_trigger_dag.py"
    ]
  },
  "patch_diff": {
    "airflow/www/templates/airflow/trigger.html||airflow/www/templates/airflow/trigger.html": [
      "File: airflow/www/templates/airflow/trigger.html -> airflow/www/templates/airflow/trigger.html",
      "--- Hunk 1 ---",
      "[Context before]",
      "63:       </label>",
      "64:     </div>",
      "65:     <button type=\"submit\" class=\"btn btn-primary\">Trigger</button>",
      "67:   </form>",
      "68: {% endblock %}",
      "",
      "[Removed Lines]",
      "66:     <button type=\"button\" class=\"btn\" onclick=\"location.href = '{{ origin }}'; return false\">Cancel</button>",
      "",
      "[Added Lines]",
      "66:     <a class=\"btn\" href=\"{{ origin }}\">Cancel</a>",
      "",
      "---------------"
    ],
    "tests/www/views/test_views_trigger_dag.py||tests/www/views/test_views_trigger_dag.py": [
      "File: tests/www/views/test_views_trigger_dag.py -> tests/www/views/test_views_trigger_dag.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "133:         (\"javascript:alert(1)\", \"/home\"),",
      "134:         (\"http://google.com\", \"/home\"),",
      "135:         (\"36539'%3balert(1)%2f%2f166\", \"/home\"),",
      "136:         (",
      "137:             \"%2Ftree%3Fdag_id%3Dexample_bash_operator';alert(33)//\",",
      "138:             \"/home\",",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "136:         (",
      "137:             '\"><script>alert(99)</script><a href=\"',",
      "138:             \"&#34;&gt;&lt;script&gt;alert(99)&lt;/script&gt;&lt;a href=&#34;\",",
      "139:         ),",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "145:     test_dag_id = \"example_bash_operator\"",
      "147:     resp = admin_client.get(f'trigger?dag_id={test_dag_id}&origin={test_origin}')",
      "156: @pytest.mark.parametrize(",
      "",
      "[Removed Lines]",
      "148:     check_content_in_response(",
      "149:         '<button type=\"button\" class=\"btn\" onclick=\"location.href = \\'{}\\'; return false\">'.format(",
      "150:             expected_origin",
      "151:         ),",
      "152:         resp,",
      "153:     )",
      "",
      "[Added Lines]",
      "152:     check_content_in_response(f'<a class=\"btn\" href=\"{expected_origin}\">Cancel</a>', resp)",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "40bf532395d5a3daa985117562747341af1b976c",
      "candidate_info": {
        "commit_hash": "40bf532395d5a3daa985117562747341af1b976c",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/40bf532395d5a3daa985117562747341af1b976c",
        "files": [
          "scripts/ci/testing/ci_run_airflow_testing.sh"
        ],
        "message": "Decrease likelihood of memory issue in CI (#18852)\n\nThis PR attempts to decrease the likelihood of memory issues for\nCI for non-committers. The MSSQL and MYSQL Provider and Integration\ntests when run together with other tests in parallel (for MSSQL even\nstandalone) might cause memory problems (143 or 137 exit code).\n\nThis PR changes the approach slightly for low-memory conditions:\n\n1) MSSQL - both Integration and Providers tests are skipped\n   entirely (they will be run in High-Mem case so we will see if\n   there are any problems anyway)\n\n2) MySQL - both Integration and Providers tests are run separately\n   which will lead to slightly longer test runs but likely this\n   will save us from the occasional memory issues.\n\n(cherry picked from commit 5d9e5f69b9d9c7d4f4e5e5c040ace0589b541a91)",
        "before_after_code_files": [
          "scripts/ci/testing/ci_run_airflow_testing.sh||scripts/ci/testing/ci_run_airflow_testing.sh"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "scripts/ci/testing/ci_run_airflow_testing.sh||scripts/ci/testing/ci_run_airflow_testing.sh": [
          "File: scripts/ci/testing/ci_run_airflow_testing.sh -> scripts/ci/testing/ci_run_airflow_testing.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "77:     echo",
          "79:     local run_integration_tests_separately=\"false\"",
          "80:     # shellcheck disable=SC2153",
          "81:     local test_types_to_run=${TEST_TYPES}",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "80:     local run_providers_tests_separately=\"false\"",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "92:             echo \"\"",
          "93:             echo \"${COLOR_YELLOW}Integration tests will be run separately at the end after cleaning up docker${COLOR_RESET}\"",
          "94:             echo \"\"",
          "98:             if [[ ${BACKEND} == \"mssql\" ]]; then",
          "101:             else",
          "103:             fi",
          "104:         fi",
          "105:     fi",
          "",
          "[Removed Lines]",
          "95:             # Remove Integration from list of tests to run in parallel",
          "96:             test_types_to_run=\"${test_types_to_run//Integration/}\"",
          "97:             run_integration_tests_separately=\"true\"",
          "99:               # Skip running \"Integration\" tests for low memory condition for mssql",
          "100:               run_integration_tests_separately=\"false\"",
          "102:               run_integration_tests_separately=\"true\"",
          "",
          "[Added Lines]",
          "97:                 # Skip running \"Integration\" and \"Providers\" tests for low memory condition for mssql",
          "98:                 # Both might lead to memory issues even in run on their own. We have no need to",
          "99:                 # Test those specifically for MSSQL (and they will be tested in `main` as there",
          "100:                 # We have no memory limits",
          "101:                 test_types_to_run=\"${test_types_to_run//Integration/}\"",
          "102:                 run_integration_tests_separately=\"false\"",
          "103:                 test_types_to_run=\"${test_types_to_run//Providers/}\"",
          "104:                 run_providers_tests_separately=\"false\"",
          "105:             elif [[ ${BACKEND} == \"mysql\" ]]; then",
          "106:                 # Separate \"Integration\" and \"Providers\" tests for low memory condition for mysql",
          "107:                 # To not run them in parallel with other tests as this often leads to memory issue",
          "108:                 # (Error 137 or 143).",
          "109:                 test_types_to_run=\"${test_types_to_run//Integration/}\"",
          "110:                 run_integration_tests_separately=\"true\"",
          "111:                 test_types_to_run=\"${test_types_to_run//Providers/}\"",
          "112:                 run_providers_tests_separately=\"true\"",
          "114:                 # Remove Integration from list of tests to run in parallel",
          "115:                 # and run them separately for all other backends",
          "116:                 test_types_to_run=\"${test_types_to_run//Integration/}\"",
          "117:                 run_integration_tests_separately=\"true\"",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "109:     parallel::initialize_monitoring",
          "111:     run_test_types_in_parallel \"${@}\"",
          "112:     if [[ ${run_integration_tests_separately} == \"true\" ]]; then",
          "113:         parallel::cleanup_runner",
          "114:         test_types_to_run=\"Integration\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "127:     if [[ ${run_providers_tests_separately} == \"true\" ]]; then",
          "128:         parallel::cleanup_runner",
          "129:         test_types_to_run=\"Providers\"",
          "130:         run_test_types_in_parallel \"${@}\"",
          "131:     fi",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "04b380ba9ba6b304098de46376ec3ad27e444344",
      "candidate_info": {
        "commit_hash": "04b380ba9ba6b304098de46376ec3ad27e444344",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/04b380ba9ba6b304098de46376ec3ad27e444344",
        "files": [
          "dev/ISSUE_TEMPLATE.md.jinja2",
          "dev/README_RELEASE_AIRFLOW.md",
          "dev/prepare_release_issue.py"
        ],
        "message": "Add script to generate issue for status of testing of the rc (#19247)\n\n(cherry picked from commit 849a94b5a40c51a7344d158a02a39449ebd720f2)",
        "before_after_code_files": [
          "dev/ISSUE_TEMPLATE.md.jinja2||dev/ISSUE_TEMPLATE.md.jinja2",
          "dev/prepare_release_issue.py||dev/prepare_release_issue.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/ISSUE_TEMPLATE.md.jinja2||dev/ISSUE_TEMPLATE.md.jinja2": [
          "File: dev/ISSUE_TEMPLATE.md.jinja2 -> dev/ISSUE_TEMPLATE.md.jinja2",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "3: We have a kind request for all the contributors to the latest [Apache Airflow RC {{version}}](https://pypi.org/project/apache-airflow/{{version}}/).",
          "5: Could you please help us to test the RC versions of Airflow?",
          "7: Please let us know in the comment if the issue is addressed in the latest RC.",
          "9: {% for pr_number in pr_list %}",
          "10:     {%- set pr = pull_requests[pr_number] -%}",
          "11: - [ ] [{{ pr.title }} (#{{ pr.number }})]({{ pr.html_url }}): {{ user_logins[pr_number] }}",
          "12:         {%- if linked_issues[pr_number] %}",
          "13:      Linked issues:",
          "14:             {%- for linked_issue in linked_issues[pr_number] %}",
          "15:      - [{{ linked_issue.title }} (#{{ linked_issue.number }})]({{ linked_issue.html_url }})",
          "16:             {%- endfor %}",
          "17:         {%- endif %}",
          "18: {% endfor %}",
          "20: Thanks to all who contributed to the release (probably not a complete list!):",
          "21: {{ all_user_logins }}",
          "",
          "---------------"
        ],
        "dev/prepare_release_issue.py||dev/prepare_release_issue.py": [
          "File: dev/prepare_release_issue.py -> dev/prepare_release_issue.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: #!/usr/bin/env python3",
          "2: # Licensed to the Apache Software Foundation (ASF) under one",
          "3: # or more contributor license agreements.  See the NOTICE file",
          "4: # distributed with this work for additional information",
          "5: # regarding copyright ownership.  The ASF licenses this file",
          "6: # to you under the Apache License, Version 2.0 (the",
          "7: # \"License\"); you may not use this file except in compliance",
          "8: # with the License.  You may obtain a copy of the License at",
          "9: #",
          "10: #   http://www.apache.org/licenses/LICENSE-2.0",
          "11: #",
          "12: # Unless required by applicable law or agreed to in writing,",
          "13: # software distributed under the License is distributed on an",
          "14: # \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
          "15: # KIND, either express or implied.  See the License for the",
          "16: # specific language governing permissions and limitations",
          "17: # under the License.",
          "19: import logging",
          "20: import os",
          "21: import re",
          "22: import subprocess",
          "23: import textwrap",
          "24: from collections import defaultdict",
          "25: from typing import Any, Dict, List, NamedTuple, Optional, Set",
          "27: import click",
          "28: from github import Github, Issue, PullRequest, UnknownObjectException",
          "29: from rich.console import Console",
          "30: from rich.progress import Progress",
          "32: logger = logging.getLogger(__name__)",
          "34: console = Console(width=400, color_system=\"standard\")",
          "36: MY_DIR_PATH = os.path.dirname(__file__)",
          "37: SOURCE_DIR_PATH = os.path.abspath(os.path.join(MY_DIR_PATH, os.pardir))",
          "38: PR_PATTERN = re.compile(r\".*\\(#([0-9]+)\\)\")",
          "39: ISSUE_MATCH_IN_BODY = re.compile(r\" #([0-9]+)[^0-9]\")",
          "42: @click.group(context_settings={'help_option_names': ['-h', '--help'], 'max_content_width': 500})",
          "43: def cli():",
          "44:     ...",
          "47: option_verbose = click.option(",
          "48:     \"--verbose\",",
          "49:     is_flag=True,",
          "50:     help=\"Print verbose information about performed steps\",",
          "51: )",
          "53: option_previous_release = click.option(",
          "54:     \"--previous-release\",",
          "55:     type=str,",
          "56:     required=True,",
          "57:     help=\"commit reference (for example hash or tag) of the previous release.\",",
          "58: )",
          "60: option_current_release = click.option(",
          "61:     \"--current-release\",",
          "62:     type=str,",
          "63:     required=True,",
          "64:     help=\"commit reference (for example hash or tag) of the current release.\",",
          "65: )",
          "67: option_github_token = click.option(",
          "68:     \"--github-token\",",
          "69:     type=str,",
          "70:     required=True,",
          "71:     help=textwrap.dedent(",
          "72:         \"\"\"",
          "73:         Github token used to authenticate.",
          "74:         You can set omit it if you have GITHUB_TOKEN env variable set",
          "75:         Can be generated with:",
          "76:         https://github.com/settings/tokens/new?description=Read%20sssues&scopes=repo:status\"\"\"",
          "77:     ),",
          "78:     envvar='GITHUB_TOKEN',",
          "79: )",
          "81: option_excluded_pr_list = click.option(",
          "82:     \"--excluded-pr-list\", type=str, default='', help=\"Coma-separated list of PRs to exclude from the issue.\"",
          "83: )",
          "85: option_limit_pr_count = click.option(",
          "86:     \"--limit-pr-count\",",
          "87:     type=int,",
          "88:     default=None,",
          "89:     help=\"Limit PR count processes (useful for testing small subset of PRs).\",",
          "90: )",
          "93: def get_git_log_command(",
          "94:     verbose: bool, from_commit: Optional[str] = None, to_commit: Optional[str] = None",
          "95: ) -> List[str]:",
          "96:     \"\"\"",
          "97:     Get git command to run for the current repo from the current folder (which is the package folder).",
          "98:     :param verbose: whether to print verbose info while getting the command",
          "99:     :param from_commit: if present - base commit from which to start the log from",
          "100:     :param to_commit: if present - final commit which should be the start of the log",
          "101:     :return: git command to run",
          "102:     \"\"\"",
          "103:     git_cmd = [",
          "104:         \"git\",",
          "105:         \"log\",",
          "106:         \"--pretty=format:%H %h %cd %s\",",
          "107:         \"--date=short\",",
          "108:     ]",
          "109:     if from_commit and to_commit:",
          "110:         git_cmd.append(f\"{from_commit}...{to_commit}\")",
          "111:     elif from_commit:",
          "112:         git_cmd.append(from_commit)",
          "113:     git_cmd.extend(['--', '.'])",
          "114:     if verbose:",
          "115:         console.print(f\"Command to run: '{' '.join(git_cmd)}'\")",
          "116:     return git_cmd",
          "119: class Change(NamedTuple):",
          "120:     \"\"\"Stores details about commits\"\"\"",
          "122:     full_hash: str",
          "123:     short_hash: str",
          "124:     date: str",
          "125:     message: str",
          "126:     message_without_backticks: str",
          "127:     pr: Optional[int]",
          "130: def get_change_from_line(line: str):",
          "131:     split_line = line.split(\" \", maxsplit=3)",
          "132:     message = split_line[3]",
          "133:     pr = None",
          "134:     pr_match = PR_PATTERN.match(message)",
          "135:     if pr_match:",
          "136:         pr = pr_match.group(1)",
          "137:     return Change(",
          "138:         full_hash=split_line[0],",
          "139:         short_hash=split_line[1],",
          "140:         date=split_line[2],",
          "141:         message=message,",
          "142:         message_without_backticks=message.replace(\"`\", \"'\").replace(\"&39;\", \"'\"),",
          "143:         pr=int(pr) if pr else None,",
          "144:     )",
          "147: def get_changes(verbose: bool, previous_release: str, current_release: str) -> List[Change]:",
          "148:     change_strings = subprocess.check_output(",
          "149:         get_git_log_command(verbose, from_commit=previous_release, to_commit=current_release),",
          "150:         cwd=SOURCE_DIR_PATH,",
          "151:         universal_newlines=True,",
          "152:     )",
          "153:     return [get_change_from_line(line) for line in change_strings.split(\"\\n\")]",
          "156: def render_template(",
          "157:     template_name: str,",
          "158:     context: Dict[str, Any],",
          "159:     autoescape: bool = True,",
          "160:     keep_trailing_newline: bool = False,",
          "161: ) -> str:",
          "162:     \"\"\"",
          "163:     Renders template based on it's name. Reads the template from <name>_TEMPLATE.md.jinja2 in current dir.",
          "164:     :param template_name: name of the template to use",
          "165:     :param context: Jinja2 context",
          "166:     :param autoescape: Whether to autoescape HTML",
          "167:     :param keep_trailing_newline: Whether to keep the newline in rendered output",
          "168:     :return: rendered template",
          "169:     \"\"\"",
          "170:     import jinja2",
          "172:     template_loader = jinja2.FileSystemLoader(searchpath=MY_DIR_PATH)",
          "173:     template_env = jinja2.Environment(",
          "174:         loader=template_loader,",
          "175:         undefined=jinja2.StrictUndefined,",
          "176:         autoescape=autoescape,",
          "177:         keep_trailing_newline=keep_trailing_newline,",
          "178:     )",
          "179:     template = template_env.get_template(f\"{template_name}_TEMPLATE.md.jinja2\")",
          "180:     content: str = template.render(context)",
          "181:     return content",
          "184: def print_issue_content(",
          "185:     current_release: str,",
          "186:     pull_requests: Dict[int, PullRequest.PullRequest],",
          "187:     linked_issues: Dict[int, List[Issue.Issue]],",
          "188:     users: Dict[int, Set[str]],",
          "189: ):",
          "190:     pr_list = list(pull_requests.keys())",
          "191:     pr_list.sort()",
          "192:     user_logins: Dict[int, str] = {pr: \"@\" + \" @\".join(users[pr]) for pr in users}",
          "193:     all_users: Set[str] = set()",
          "194:     for user_list in users.values():",
          "195:         all_users.update(user_list)",
          "196:     all_user_logins = \"@\" + \" @\".join(all_users)",
          "197:     content = render_template(",
          "198:         template_name='ISSUE',",
          "199:         context={",
          "200:             'version': current_release,",
          "201:             'pr_list': pr_list,",
          "202:             'pull_requests': pull_requests,",
          "203:             'linked_issues': linked_issues,",
          "204:             'users': users,",
          "205:             'user_logins': user_logins,",
          "206:             'all_user_logins': all_user_logins,",
          "207:         },",
          "208:         autoescape=False,",
          "209:         keep_trailing_newline=True,",
          "210:     )",
          "211:     print(content)",
          "214: @cli.command()",
          "215: @option_github_token",
          "216: @option_previous_release",
          "217: @option_current_release",
          "218: @option_excluded_pr_list",
          "219: @option_verbose",
          "220: @option_limit_pr_count",
          "221: def generate_issue_content(",
          "222:     github_token: str,",
          "223:     previous_release: str,",
          "224:     current_release: str,",
          "225:     excluded_pr_list: str,",
          "226:     verbose: bool,",
          "227:     limit_pr_count: Optional[int],",
          "228: ):",
          "229:     if excluded_pr_list:",
          "230:         excluded_prs = [int(pr) for pr in excluded_pr_list.split(\",\")]",
          "231:     else:",
          "232:         excluded_prs = []",
          "233:     changes = get_changes(verbose, previous_release, current_release)",
          "234:     prs = list(",
          "235:         filter(lambda pr: pr is not None and pr not in excluded_prs, [change.pr for change in changes])",
          "236:     )",
          "237:     g = Github(github_token)",
          "238:     repo = g.get_repo(\"apache/airflow\")",
          "239:     pull_requests: Dict[int, PullRequest.PullRequest] = {}",
          "240:     linked_issues: Dict[int, List[Issue.Issue]] = defaultdict(lambda: [])",
          "241:     users: Dict[int, Set[str]] = defaultdict(lambda: set())",
          "242:     count_prs = len(prs)",
          "243:     if limit_pr_count:",
          "244:         count_prs = limit_pr_count",
          "245:     with Progress(console=console) as progress:",
          "246:         task = progress.add_task(f\"Retrieving {count_prs} PRs \", total=count_prs)",
          "247:         for i in range(count_prs):",
          "248:             pr_number = prs[i]",
          "249:             progress.console.print(",
          "250:                 f\"Retrieving PR#{pr_number}: \" f\"https://github.com/apache/airflow/pull/{pr_number}\"",
          "251:             )",
          "252:             try:",
          "253:                 pr = repo.get_pull(pr_number)",
          "254:             except UnknownObjectException:",
          "255:                 # Fallback to issue if PR not found",
          "256:                 try:",
          "257:                     pr = repo.get_issue(pr_number)  # (same fields as PR)",
          "258:                 except UnknownObjectException:",
          "259:                     console.print(f\"[red]The PR #{pr_number} could not be found[/]\")",
          "260:                 continue",
          "261:             pull_requests[pr_number] = pr",
          "262:             # GitHub does not have linked issues in PR - but we quite rigorously add Fixes/Closes",
          "263:             # Relate so we can find those from the body",
          "264:             if pr.body:",
          "265:                 body = pr.body.replace(\"\\n\", \" \").replace(\"\\r\", \" \")",
          "266:                 for issue_match in ISSUE_MATCH_IN_BODY.finditer(body):",
          "267:                     linked_issue_number = int(issue_match.group(1))",
          "268:                     progress.console.print(",
          "269:                         f\"Retrieving Linked issue PR#{linked_issue_number}: \"",
          "270:                         f\"https://github.com/apache/airflow/issue/{linked_issue_number}\"",
          "271:                     )",
          "272:                     try:",
          "273:                         linked_issues[pr_number].append(repo.get_issue(linked_issue_number))",
          "274:                     except UnknownObjectException:",
          "275:                         progress.console.print(",
          "276:                             f\"Failed to retrieve linked issue #{linked_issue_number}: Unknown Issue\"",
          "277:                         )",
          "278:             users[pr_number].add(pr.user.login)",
          "279:             for linked_issue in linked_issues[pr_number]:",
          "280:                 users[pr_number].add(linked_issue.user.login)",
          "281:             progress.advance(task)",
          "282:     print_issue_content(current_release, pull_requests, linked_issues, users)",
          "285: if __name__ == \"__main__\":",
          "286:     cli()",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "ef0a98090748e3f6310e0e0701afaccf5282c66f",
      "candidate_info": {
        "commit_hash": "ef0a98090748e3f6310e0e0701afaccf5282c66f",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/ef0a98090748e3f6310e0e0701afaccf5282c66f",
        "files": [
          "scripts/docker/install_mysql.sh"
        ],
        "message": "Switch to new MySQL public key (#20912)\n\nMySQL changed key used to sign their apt packages. This caused\ndocker building failing for prod images as MySQL could not be\ninstalled.\n\nNew Public Key is used instead.\n\nFixes: #20911\n(cherry picked from commit 7e29506037fa543f5d9b438320db064cdb820c7b)",
        "before_after_code_files": [
          "scripts/docker/install_mysql.sh||scripts/docker/install_mysql.sh"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "scripts/docker/install_mysql.sh||scripts/docker/install_mysql.sh": [
          "File: scripts/docker/install_mysql.sh -> scripts/docker/install_mysql.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "44:         exit 1",
          "45:     fi",
          "48:     readonly key",
          "50:     GNUPGHOME=\"$(mktemp -d)\"",
          "",
          "[Removed Lines]",
          "47:     local key=\"A4A9406876FCBD3C456770C88C718D3B5072E1F5\"",
          "",
          "[Added Lines]",
          "47:     local key=\"467B942D3A79BD29\"",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "319d32768c988dfcfff51893be692f5ae39ca811",
      "candidate_info": {
        "commit_hash": "319d32768c988dfcfff51893be692f5ae39ca811",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/319d32768c988dfcfff51893be692f5ae39ca811",
        "files": [
          "scripts/ci/libraries/_start_end.sh",
          "scripts/ci/libraries/_testing.sh",
          "scripts/ci/testing/ci_run_single_airflow_test_in_docker.sh"
        ],
        "message": "Fix dumping container logs on error (#19645)\n\nWhen we optimized tests for memory use we added cleanup of all\ncontainers after each test suite. Unfortunately it caused\ndumping container logs to stop working because this dumping was\ndone only only when the script was exiting.\n\nThis PR moves dumping container logs to between the test run and\ncleanup, so that we can see the logs when there is a test failure.\n\nRelated to: #19633 where the logs were not dumped and it made the\nanalysis much more difficult.\n\n(cherry picked from commit 7cda7d4b5e413925bf639976e77ebf2442b4bff9)",
        "before_after_code_files": [
          "scripts/ci/libraries/_start_end.sh||scripts/ci/libraries/_start_end.sh",
          "scripts/ci/libraries/_testing.sh||scripts/ci/libraries/_testing.sh",
          "scripts/ci/testing/ci_run_single_airflow_test_in_docker.sh||scripts/ci/testing/ci_run_single_airflow_test_in_docker.sh"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "scripts/ci/libraries/_start_end.sh||scripts/ci/libraries/_start_end.sh": [
          "File: scripts/ci/libraries/_start_end.sh -> scripts/ci/libraries/_start_end.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "71:     fi",
          "72: }",
          "89: #",
          "90: # Trap function executed always at the end of the script. In case of verbose output it also",
          "91: # Prints the exit code that the script exits with. Removes verbosity of commands in case it was run with",
          "",
          "[Removed Lines]",
          "74: function start_end::dump_container_logs() {",
          "75:     start_end::group_start \"${COLOR_BLUE}Dumping container logs ${container}${COLOR_RESET}\"",
          "76:     local container=\"${1}\"",
          "77:     local dump_file",
          "78:     dump_file=${AIRFLOW_SOURCES}/files/container_logs_${container}_$(date \"+%Y-%m-%d\")_${CI_BUILD_ID}_${CI_JOB_ID}.log",
          "79:     echo \"${COLOR_BLUE}###########################################################################################${COLOR_RESET}\"",
          "80:     echo \"                   Dumping logs from ${container} container\"",
          "81:     echo \"${COLOR_BLUE}###########################################################################################${COLOR_RESET}\"",
          "82:     docker_v logs \"${container}\" > \"${dump_file}\"",
          "83:     echo \"                   Container ${container} logs dumped to ${dump_file}\"",
          "84:     echo \"${COLOR_BLUE}###########################################################################################${COLOR_RESET}\"",
          "85:     start_end::group_end",
          "86: }",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "106:         echo",
          "107:         echo \"${COLOR_RED}ERROR: The previous step completed with error. Please take a look at output above ${COLOR_RESET}\"",
          "108:         echo",
          "116:         verbosity::print_info \"${COLOR_RED}###########################################################################################${COLOR_RESET}\"",
          "117:         verbosity::print_info \"${COLOR_RED}                   EXITING WITH STATUS CODE ${exit_code}${COLOR_RESET}\"",
          "118:         verbosity::print_info \"${COLOR_RED}###########################################################################################${COLOR_RESET}\"",
          "",
          "[Removed Lines]",
          "109:         if [[ ${CI} == \"true\" ]]; then",
          "110:             local container",
          "111:             for container in $(docker ps --format '{{.Names}}')",
          "112:             do",
          "113:                 start_end::dump_container_logs \"${container}\"",
          "114:             done",
          "115:         fi",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "scripts/ci/libraries/_testing.sh||scripts/ci/libraries/_testing.sh": [
          "File: scripts/ci/libraries/_testing.sh -> scripts/ci/libraries/_testing.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "114:     fi",
          "115:     readonly TEST_TYPES",
          "116: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "118: function testing::dump_container_logs() {",
          "119:     start_end::group_start \"${COLOR_BLUE}Dumping container logs ${container}${COLOR_RESET}\"",
          "120:     local container=\"${1}\"",
          "121:     local dump_file",
          "122:     dump_file=${AIRFLOW_SOURCES}/files/container_logs_${container}_$(date \"+%Y-%m-%d\")_${CI_BUILD_ID}_${CI_JOB_ID}.log",
          "123:     echo \"${COLOR_BLUE}###########################################################################################${COLOR_RESET}\"",
          "124:     echo \"                   Dumping logs from ${container} container\"",
          "125:     echo \"${COLOR_BLUE}###########################################################################################${COLOR_RESET}\"",
          "126:     docker_v logs \"${container}\" > \"${dump_file}\"",
          "127:     echo \"                   Container ${container} logs dumped to ${dump_file}\"",
          "128:     echo \"${COLOR_BLUE}###########################################################################################${COLOR_RESET}\"",
          "129:     start_end::group_end",
          "130: }",
          "",
          "---------------"
        ],
        "scripts/ci/testing/ci_run_single_airflow_test_in_docker.sh||scripts/ci/testing/ci_run_single_airflow_test_in_docker.sh": [
          "File: scripts/ci/testing/ci_run_single_airflow_test_in_docker.sh -> scripts/ci/testing/ci_run_single_airflow_test_in_docker.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "126:          run airflow \"${@}\"",
          "127:     docker ps",
          "128:     exit_code=$?",
          "129:     docker-compose --log-level INFO -f \"${SCRIPTS_CI_DIR}/docker-compose/base.yml\" \\",
          "130:         \"${INTEGRATIONS[@]}\" \\",
          "131:         --project-name \"airflow-${TEST_TYPE}-${BACKEND}\" \\",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "129:     if [[ ${exit_code} != \"0\" && ${CI} == \"true\" ]]; then",
          "130:         docker ps --all",
          "131:         local container",
          "132:         for container in $(docker ps --all --format '{{.Names}}')",
          "133:         do",
          "134:             testing::dump_container_logs \"${container}\"",
          "135:         done",
          "136:     fi",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "cd558102f7e1cc712d557b1876ecbbf4288714b8",
      "candidate_info": {
        "commit_hash": "cd558102f7e1cc712d557b1876ecbbf4288714b8",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/cd558102f7e1cc712d557b1876ecbbf4288714b8",
        "files": [
          "scripts/ci/testing/ci_run_single_airflow_test_in_docker.sh"
        ],
        "message": "Add more complete instruction for reproducing failed integration tests (#19646)\n\nWhen integration tests are failing, breeze prints the exact\nreproduction step to recreate the same environment. However when\nintegration tests were enabled it missed the --integration\nflags that were necessary to enable the integrations.\n\nThis PR adds the --integration flags to the instructions and also\nadds the comment that Kerberos integration currently does not work\nwith docker-compose v2.\n\n(cherry picked from commit 7b700bbe32e18708bd0affbc59c43ce9b3420e28)",
        "before_after_code_files": [
          "scripts/ci/testing/ci_run_single_airflow_test_in_docker.sh||scripts/ci/testing/ci_run_single_airflow_test_in_docker.sh"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "scripts/ci/testing/ci_run_single_airflow_test_in_docker.sh||scripts/ci/testing/ci_run_single_airflow_test_in_docker.sh": [
          "File: scripts/ci/testing/ci_run_single_airflow_test_in_docker.sh -> scripts/ci/testing/ci_run_single_airflow_test_in_docker.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "26: DOCKER_COMPOSE_LOCAL=()",
          "27: INTEGRATIONS=()",
          "29: function prepare_tests() {",
          "30:     DOCKER_COMPOSE_LOCAL+=(\"-f\" \"${SCRIPTS_CI_DIR}/docker-compose/files.yml\")",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "28: INTEGRATION_BREEZE_FLAGS=()",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "60:     for _INT in ${ENABLED_INTEGRATIONS}",
          "61:     do",
          "64:     done",
          "66:     readonly INTEGRATIONS",
          "",
          "[Removed Lines]",
          "62:         INTEGRATIONS+=(\"-f\")",
          "63:         INTEGRATIONS+=(\"${SCRIPTS_CI_DIR}/docker-compose/integration-${_INT}.yml\")",
          "",
          "[Added Lines]",
          "63:         INTEGRATIONS+=(\"-f\" \"${SCRIPTS_CI_DIR}/docker-compose/integration-${_INT}.yml\")",
          "64:         INTEGRATION_BREEZE_FLAGS+=(\"--integration\" \"${_INT}\")",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "158:         echo \"${COLOR_RED}***********************************************************************************************${COLOR_RESET}\"",
          "159:         echo",
          "160:         echo \"${COLOR_BLUE}***********************************************************************************************${COLOR_RESET}\"",
          "163:         echo \"${COLOR_BLUE}Then you can run failed tests with:${COLOR_RESET}\"",
          "164:         echo \"${COLOR_YELLOW}pytest [TEST_NAME]${COLOR_RESET}\"",
          "165:         echo \"${COLOR_BLUE}***********************************************************************************************${COLOR_RESET}\"",
          "",
          "[Removed Lines]",
          "161:         echo \"${COLOR_BLUE}Reproduce the failed tests on your local machine:${COLOR_RESET}\"",
          "162:         echo \"${COLOR_YELLOW}./breeze --github-image-id ${GITHUB_REGISTRY_PULL_IMAGE_TAG=} --backend ${BACKEND} ${EXTRA_ARGS}--python ${PYTHON_MAJOR_MINOR_VERSION} --db-reset --skip-mounting-local-sources --test-type ${TEST_TYPE} shell${COLOR_RESET}\"",
          "",
          "[Added Lines]",
          "162:         echo \"${COLOR_BLUE}Reproduce the failed tests on your local machine (note that you need to use docker-compose v1 rather than v2 to enable Kerberos integration):${COLOR_RESET}\"",
          "163:         echo \"${COLOR_YELLOW}./breeze --github-image-id ${GITHUB_REGISTRY_PULL_IMAGE_TAG=} --backend ${BACKEND} ${EXTRA_ARGS}--python ${PYTHON_MAJOR_MINOR_VERSION} --db-reset --skip-mounting-local-sources --test-type ${TEST_TYPE} ${INTEGRATION_BREEZE_FLAGS[*]} shell${COLOR_RESET}\"",
          "",
          "---------------"
        ]
      }
    }
  ]
}