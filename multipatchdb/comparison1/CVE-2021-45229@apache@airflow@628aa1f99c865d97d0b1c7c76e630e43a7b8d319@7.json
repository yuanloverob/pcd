{
  "cve_id": "CVE-2021-45229",
  "cve_desc": "It was discovered that the \"Trigger DAG with config\" screen was susceptible to XSS attacks via the `origin` query argument. This issue affects Apache Airflow versions 2.2.3 and below.",
  "repo": "apache/airflow",
  "patch_hash": "628aa1f99c865d97d0b1c7c76e630e43a7b8d319",
  "patch_info": {
    "commit_hash": "628aa1f99c865d97d0b1c7c76e630e43a7b8d319",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/628aa1f99c865d97d0b1c7c76e630e43a7b8d319",
    "files": [
      "airflow/www/templates/airflow/trigger.html",
      "tests/www/views/test_views_trigger_dag.py"
    ],
    "message": "Simplify trigger cancel button (#21591)\n\nCo-authored-by: Jed Cunningham <jedcunningham@apache.org>\n(cherry picked from commit 65297673a318660fba76797e50d0c06804dfcafc)",
    "before_after_code_files": [
      "airflow/www/templates/airflow/trigger.html||airflow/www/templates/airflow/trigger.html",
      "tests/www/views/test_views_trigger_dag.py||tests/www/views/test_views_trigger_dag.py"
    ]
  },
  "patch_diff": {
    "airflow/www/templates/airflow/trigger.html||airflow/www/templates/airflow/trigger.html": [
      "File: airflow/www/templates/airflow/trigger.html -> airflow/www/templates/airflow/trigger.html",
      "--- Hunk 1 ---",
      "[Context before]",
      "63:       </label>",
      "64:     </div>",
      "65:     <button type=\"submit\" class=\"btn btn-primary\">Trigger</button>",
      "67:   </form>",
      "68: {% endblock %}",
      "",
      "[Removed Lines]",
      "66:     <button type=\"button\" class=\"btn\" onclick=\"location.href = '{{ origin }}'; return false\">Cancel</button>",
      "",
      "[Added Lines]",
      "66:     <a class=\"btn\" href=\"{{ origin }}\">Cancel</a>",
      "",
      "---------------"
    ],
    "tests/www/views/test_views_trigger_dag.py||tests/www/views/test_views_trigger_dag.py": [
      "File: tests/www/views/test_views_trigger_dag.py -> tests/www/views/test_views_trigger_dag.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "133:         (\"javascript:alert(1)\", \"/home\"),",
      "134:         (\"http://google.com\", \"/home\"),",
      "135:         (\"36539'%3balert(1)%2f%2f166\", \"/home\"),",
      "136:         (",
      "137:             \"%2Ftree%3Fdag_id%3Dexample_bash_operator';alert(33)//\",",
      "138:             \"/home\",",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "136:         (",
      "137:             '\"><script>alert(99)</script><a href=\"',",
      "138:             \"&#34;&gt;&lt;script&gt;alert(99)&lt;/script&gt;&lt;a href=&#34;\",",
      "139:         ),",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "145:     test_dag_id = \"example_bash_operator\"",
      "147:     resp = admin_client.get(f'trigger?dag_id={test_dag_id}&origin={test_origin}')",
      "156: @pytest.mark.parametrize(",
      "",
      "[Removed Lines]",
      "148:     check_content_in_response(",
      "149:         '<button type=\"button\" class=\"btn\" onclick=\"location.href = \\'{}\\'; return false\">'.format(",
      "150:             expected_origin",
      "151:         ),",
      "152:         resp,",
      "153:     )",
      "",
      "[Added Lines]",
      "152:     check_content_in_response(f'<a class=\"btn\" href=\"{expected_origin}\">Cancel</a>', resp)",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "49e582aa6cd70d31891eed6dce71bd6c7557e6e7",
      "candidate_info": {
        "commit_hash": "49e582aa6cd70d31891eed6dce71bd6c7557e6e7",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/49e582aa6cd70d31891eed6dce71bd6c7557e6e7",
        "files": [
          "breeze",
          "scripts/ci/docker-compose/_docker.env",
          "scripts/ci/docker-compose/base.yml",
          "scripts/ci/libraries/_initialization.sh",
          "scripts/ci/testing/ci_run_single_airflow_test_in_docker.sh",
          "scripts/in_container/entrypoint_ci.sh"
        ],
        "message": "Remove adding of \"test-run\" variables to dc_ci script (#18903)\n\nThe RUN_*TEST variables are not part of the environment\nso they are not set when the dc_ci is generated they are\noverridden by Breeze when particular commands are executed.\nTherefore we should not hard-code those values in dc_ci script\n(this is useful for debugging to have the script but it is only\nthere for environment configuration)\n\n(cherry picked from commit 7d3b6b51c0227f6251fd5b0023970c19fcc3c402)",
        "before_after_code_files": [
          "scripts/ci/docker-compose/_docker.env||scripts/ci/docker-compose/_docker.env",
          "scripts/ci/libraries/_initialization.sh||scripts/ci/libraries/_initialization.sh",
          "scripts/ci/testing/ci_run_single_airflow_test_in_docker.sh||scripts/ci/testing/ci_run_single_airflow_test_in_docker.sh",
          "scripts/in_container/entrypoint_ci.sh||scripts/in_container/entrypoint_ci.sh"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "scripts/ci/docker-compose/_docker.env||scripts/ci/docker-compose/_docker.env": [
          "File: scripts/ci/docker-compose/_docker.env -> scripts/ci/docker-compose/_docker.env",
          "--- Hunk 1 ---",
          "[Context before]",
          "53: PYTHONDONTWRITEBYTECODE",
          "54: PYTHON_MAJOR_MINOR_VERSION",
          "55: RUN_TESTS",
          "57: RUN_SYSTEM_TESTS",
          "58: START_AIRFLOW",
          "59: TEST_TYPE",
          "",
          "[Removed Lines]",
          "56: RUN_INTEGRATION_TESTS",
          "",
          "[Added Lines]",
          "56: LIST_OF_INTEGRATION_TESTS_TO_RUN",
          "",
          "---------------"
        ],
        "scripts/ci/libraries/_initialization.sh||scripts/ci/libraries/_initialization.sh": [
          "File: scripts/ci/libraries/_initialization.sh -> scripts/ci/libraries/_initialization.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "594:     export RUN_TESTS=${RUN_TESTS:=\"false\"}",
          "596:     # Do not run integration tests by default",
          "602: }",
          "",
          "[Removed Lines]",
          "597:     export RUN_INTEGRATION_TESTS=${RUN_INTEGRATION_TESTS:=\"false\"}",
          "599:     # Do not run system tests by default",
          "600:     export RUN_SYSTEM_TESTS=${RUN_SYSTEM_TESTS:=\"false\"}",
          "",
          "[Added Lines]",
          "597:     export LIST_OF_INTEGRATION_TESTS_TO_RUN=${LIST_OF_INTEGRATION_TESTS_TO_RUN:=\"\"}",
          "599:     # Do not run system tests by default (they can be enabled by setting the RUN_SYSTEM_TESTS variable to \"true\")",
          "600:     export RUN_SYSTEM_TESTS=${RUN_SYSTEM_TESTS:=\"\"}",
          "",
          "---------------"
        ],
        "scripts/ci/testing/ci_run_single_airflow_test_in_docker.sh||scripts/ci/testing/ci_run_single_airflow_test_in_docker.sh": [
          "File: scripts/ci/testing/ci_run_single_airflow_test_in_docker.sh -> scripts/ci/testing/ci_run_single_airflow_test_in_docker.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "52:     if [[ ${TEST_TYPE:=} == \"Integration\" ]]; then",
          "53:         export ENABLED_INTEGRATIONS=\"${AVAILABLE_INTEGRATIONS}\"",
          "55:     else",
          "56:         export ENABLED_INTEGRATIONS=\"\"",
          "58:     fi",
          "60:     for _INT in ${ENABLED_INTEGRATIONS}",
          "",
          "[Removed Lines]",
          "54:         export RUN_INTEGRATION_TESTS=\"${AVAILABLE_INTEGRATIONS}\"",
          "57:         export RUN_INTEGRATION_TESTS=\"\"",
          "",
          "[Added Lines]",
          "54:         export LIST_OF_INTEGRATION_TESTS_TO_RUN=\"${AVAILABLE_INTEGRATIONS}\"",
          "57:         export LIST_OF_INTEGRATION_TESTS_TO_RUN=\"\"",
          "",
          "---------------"
        ],
        "scripts/in_container/entrypoint_ci.sh||scripts/in_container/entrypoint_ci.sh": [
          "File: scripts/in_container/entrypoint_ci.sh -> scripts/in_container/entrypoint_ci.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "324: readonly SELECTED_TESTS CLI_TESTS API_TESTS PROVIDERS_TESTS CORE_TESTS WWW_TESTS \\",
          "325:     ALL_TESTS ALL_PRESELECTED_TESTS",
          "328:     # Integration tests",
          "330:     do",
          "331:         EXTRA_PYTEST_ARGS+=(\"--integration\" \"${INT}\")",
          "332:     done",
          "",
          "[Removed Lines]",
          "327: if [[ -n ${RUN_INTEGRATION_TESTS=} ]]; then",
          "329:     for INT in ${RUN_INTEGRATION_TESTS}",
          "",
          "[Added Lines]",
          "327: if [[ -n ${LIST_OF_INTEGRATION_TESTS_TO_RUN=} ]]; then",
          "329:     for INT in ${LIST_OF_INTEGRATION_TESTS_TO_RUN}",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "25d7bbe256ddbc0416470c74a04524b4fbfa8d9b",
      "candidate_info": {
        "commit_hash": "25d7bbe256ddbc0416470c74a04524b4fbfa8d9b",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/25d7bbe256ddbc0416470c74a04524b4fbfa8d9b",
        "files": [
          "UPDATING.md",
          "airflow/jobs/scheduler_job.py",
          "airflow/sensors/base.py",
          "docs/apache-airflow/concepts/deferring.rst",
          "docs/apache-airflow/concepts/smart-sensors.rst"
        ],
        "message": "Deprecate smart sensors (#20151)\n\nSmart sensors are being replaced with Deferrable Operators. As they were\nmarked as an early-access feature, we can remove them before Airflow 3.\n\n(cherry picked from commit 77813b40db99683dcf14b557f9cddc50080c9a6a)",
        "before_after_code_files": [
          "airflow/jobs/scheduler_job.py||airflow/jobs/scheduler_job.py",
          "airflow/sensors/base.py||airflow/sensors/base.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/jobs/scheduler_job.py||airflow/jobs/scheduler_job.py": [
          "File: airflow/jobs/scheduler_job.py -> airflow/jobs/scheduler_job.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "49: from airflow.ti_deps.dependencies_states import EXECUTION_STATES",
          "50: from airflow.utils import timezone",
          "51: from airflow.utils.callback_requests import DagCallbackRequest, TaskCallbackRequest",
          "52: from airflow.utils.event_scheduler import EventScheduler",
          "53: from airflow.utils.retries import MAX_DB_RETRIES, retry_db_transaction, run_with_db_retries",
          "54: from airflow.utils.session import create_session, provide_session",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "52: from airflow.utils.docs import get_docs_url",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "147:         self.dagbag = DagBag(dag_folder=self.subdir, read_dags_from_db=True, load_op_links=False)",
          "149:     def register_signals(self) -> None:",
          "150:         \"\"\"Register signals that stop child processes\"\"\"",
          "151:         signal.signal(signal.SIGINT, self._exit_gracefully)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "150:         if conf.getboolean('smart_sensor', 'use_smart_sensor'):",
          "151:             compatible_sensors = set(",
          "152:                 map(lambda l: l.strip(), conf.get('smart_sensor', 'sensors_enabled').split(','))",
          "153:             )",
          "154:             docs_url = get_docs_url('concepts/smart-sensors.html#migrating-to-deferrable-operators')",
          "155:             warnings.warn(",
          "156:                 f'Smart sensors are deprecated, yet can be used for {compatible_sensors} sensors.'",
          "157:                 f' Please use Deferrable Operators instead. See {docs_url} for more info.',",
          "158:                 DeprecationWarning,",
          "159:             )",
          "",
          "---------------"
        ],
        "airflow/sensors/base.py||airflow/sensors/base.py": [
          "File: airflow/sensors/base.py -> airflow/sensors/base.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "19: import datetime",
          "20: import hashlib",
          "21: import time",
          "22: from datetime import timedelta",
          "23: from typing import Any, Callable, Dict, Iterable",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "22: import warnings",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "39: # Google Provider before 3.0.0 imported apply_defaults from here.",
          "40: # See  https://github.com/apache/airflow/issues/16035",
          "41: from airflow.utils.decorators import apply_defaults  # noqa: F401",
          "44: class BaseSensorOperator(BaseOperator, SkipMixin):",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "43: from airflow.utils.docs import get_docs_url",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "154:         :param context: TaskInstance template context from the ti.",
          "155:         :return: boolean",
          "156:         \"\"\"",
          "157:         poke_context = self.get_poke_context(context)",
          "158:         execution_context = self.get_execution_context(context)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "159:         docs_url = get_docs_url('concepts/smart-sensors.html#migrating-to-deferrable-operators')",
          "160:         warnings.warn(",
          "161:             'Your sensor is using Smart Sensors, which are deprecated.'",
          "162:             f' Please use Deferrable Operators instead. See {docs_url} for more info.',",
          "163:             DeprecationWarning,",
          "164:         )",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "94b923d766630e16804cd5dd7bc090dd0e63542a",
      "candidate_info": {
        "commit_hash": "94b923d766630e16804cd5dd7bc090dd0e63542a",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/94b923d766630e16804cd5dd7bc090dd0e63542a",
        "files": [
          ".github/workflows/ci.yml",
          "scripts/ci/docker-compose/_docker.env",
          "scripts/ci/docker-compose/base.yml",
          "scripts/in_container/entrypoint_ci.sh"
        ],
        "message": "Disable test code coverage for PRs (#19523)\n\nThe test code coverage took a lot of memory (2-3GB) when core\ntests were running (specifically test_kubernetes_executor.py) and\nthe memory was kept for the duration of whole test.\n\nThis caused intermittent memory issues on public GitHub runners.\n\nThe change only uses test coverage for `main` builds where we have\na lot of memory available.\n\n(cherry picked from commit 83011b7f87eef81ddd34722f1bc9842237491cad)",
        "before_after_code_files": [
          "scripts/ci/docker-compose/_docker.env||scripts/ci/docker-compose/_docker.env",
          "scripts/in_container/entrypoint_ci.sh||scripts/in_container/entrypoint_ci.sh"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "scripts/ci/docker-compose/_docker.env||scripts/ci/docker-compose/_docker.env": [
          "File: scripts/ci/docker-compose/_docker.env -> scripts/ci/docker-compose/_docker.env",
          "--- Hunk 1 ---",
          "[Context before]",
          "30: DEFAULT_CONSTRAINTS_BRANCH",
          "31: ENABLED_INTEGRATIONS",
          "32: ENABLED_SYSTEMS",
          "33: GITHUB_ACTIONS",
          "34: GITHUB_REGISTRY_PULL_IMAGE_TAG",
          "35: HOST_USER_ID",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "33: ENABLE_TEST_COVERAGE",
          "",
          "---------------"
        ],
        "scripts/in_container/entrypoint_ci.sh||scripts/in_container/entrypoint_ci.sh": [
          "File: scripts/in_container/entrypoint_ci.sh -> scripts/in_container/entrypoint_ci.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "205:     \"--verbosity=0\"",
          "206:     \"--strict-markers\"",
          "207:     \"--durations=100\"",
          "212:     \"--maxfail=50\"",
          "213:     \"--junitxml=${RESULT_LOG_FILE}\"",
          "214:     # timeouts in seconds for individual tests",
          "215:     \"--timeouts-order\"",
          "",
          "[Removed Lines]",
          "208:     \"--cov=airflow/\"",
          "209:     \"--cov-config=.coveragerc\"",
          "210:     \"--cov-report=xml:/files/coverage-${TEST_TYPE}-${BACKEND}.xml\"",
          "211:     \"--color=yes\"",
          "",
          "[Added Lines]",
          "209:     \"--color=yes\"",
          "210:     \"--pythonwarnings=ignore::DeprecationWarning\"",
          "211:     \"--pythonwarnings=ignore::PendingDeprecationWarning\"",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "240:     )",
          "241: fi",
          "243: declare -a SELECTED_TESTS CLI_TESTS API_TESTS PROVIDERS_TESTS CORE_TESTS WWW_TESTS \\",
          "244:     ALL_TESTS ALL_PRESELECTED_TESTS ALL_OTHER_TESTS",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "242: if [[ ${ENABLE_TEST_COVERAGE:=\"false\"} == \"true\" ]]; then",
          "243:     EXTRA_PYTEST_ARGS+=(",
          "244:         \"--cov=airflow/\"",
          "245:         \"--cov-config=.coveragerc\"",
          "246:         \"--cov-report=xml:/files/coverage-${TEST_TYPE}-${BACKEND}.xml\"",
          "247:     )",
          "248: fi",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "ef319b06949951b144003d5362179fd8790c5ded",
      "candidate_info": {
        "commit_hash": "ef319b06949951b144003d5362179fd8790c5ded",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/ef319b06949951b144003d5362179fd8790c5ded",
        "files": [
          ".pre-commit-config.yaml",
          "BREEZE.rst",
          "STATIC_CODE_CHECKS.rst",
          "airflow/kubernetes/pod.py",
          "airflow/kubernetes/pod_launcher.py",
          "airflow/kubernetes/pod_runtime_info_env.py",
          "airflow/kubernetes/volume.py",
          "airflow/kubernetes/volume_mount.py",
          "airflow/migrations/versions/2c6edca13270_resource_based_permissions.py",
          "airflow/migrations/versions/a13f7613ad25_resource_based_permissions_for_default_.py",
          "airflow/timetables/base.py",
          "airflow/utils/db.py",
          "airflow/www/decorators.py",
          "airflow/www/views.py",
          "breeze-complete",
          "docs/exts/exampleinclude.py",
          "tests/task/__init__.py"
        ],
        "message": "Add autoflake precommit to automatically remove unused code (#20466)\n\n(cherry picked from commit f0cf15cfe3a2f16741063d0368216b7067f38245)",
        "before_after_code_files": [
          "airflow/kubernetes/pod.py||airflow/kubernetes/pod.py",
          "airflow/kubernetes/pod_launcher.py||airflow/kubernetes/pod_launcher.py",
          "airflow/kubernetes/pod_runtime_info_env.py||airflow/kubernetes/pod_runtime_info_env.py",
          "airflow/kubernetes/volume.py||airflow/kubernetes/volume.py",
          "airflow/kubernetes/volume_mount.py||airflow/kubernetes/volume_mount.py",
          "airflow/migrations/versions/2c6edca13270_resource_based_permissions.py||airflow/migrations/versions/2c6edca13270_resource_based_permissions.py",
          "airflow/migrations/versions/a13f7613ad25_resource_based_permissions_for_default_.py||airflow/migrations/versions/a13f7613ad25_resource_based_permissions_for_default_.py",
          "airflow/timetables/base.py||airflow/timetables/base.py",
          "airflow/utils/db.py||airflow/utils/db.py",
          "airflow/www/decorators.py||airflow/www/decorators.py",
          "airflow/www/views.py||airflow/www/views.py",
          "tests/task/__init__.py||tests/task/__init__.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/kubernetes/pod.py||airflow/kubernetes/pod.py": [
          "File: airflow/kubernetes/pod.py -> airflow/kubernetes/pod.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "24: import warnings",
          "26: with warnings.catch_warnings():",
          "29: warnings.warn(",
          "30:     \"This module is deprecated. Please use `kubernetes.client.models for V1ResourceRequirements and Port.\",",
          "",
          "[Removed Lines]",
          "27:     from airflow.providers.cncf.kubernetes.backcompat.pod import Port, Resources",
          "",
          "[Added Lines]",
          "27:     from airflow.providers.cncf.kubernetes.backcompat.pod import Port, Resources  # noqa: autoflake",
          "",
          "---------------"
        ],
        "airflow/kubernetes/pod_launcher.py||airflow/kubernetes/pod_launcher.py": [
          "File: airflow/kubernetes/pod_launcher.py -> airflow/kubernetes/pod_launcher.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "19: This module is deprecated.",
          "20: Please use :mod:`kubernetes.client.models` for V1ResourceRequirements and Port.",
          "21: \"\"\"",
          "",
          "[Removed Lines]",
          "22: # flake8: noqa",
          "24: from airflow.kubernetes.pod_launcher_deprecated import PodLauncher, PodStatus",
          "",
          "[Added Lines]",
          "22: from airflow.kubernetes.pod_launcher_deprecated import PodLauncher, PodStatus  # noqa: autoflake",
          "",
          "---------------"
        ],
        "airflow/kubernetes/pod_runtime_info_env.py||airflow/kubernetes/pod_runtime_info_env.py": [
          "File: airflow/kubernetes/pod_runtime_info_env.py -> airflow/kubernetes/pod_runtime_info_env.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "16: # specific language governing permissions and limitations",
          "17: # under the License.",
          "18: \"\"\"This module is deprecated. Please use :mod:`kubernetes.client.models.V1EnvVar`.\"\"\"",
          "21: import warnings",
          "23: with warnings.catch_warnings():",
          "26: warnings.warn(",
          "27:     \"This module is deprecated. Please use `kubernetes.client.models.V1EnvVar`.\",",
          "",
          "[Removed Lines]",
          "19: # flake8: noqa",
          "24:     from airflow.providers.cncf.kubernetes.backcompat.pod_runtime_info_env import PodRuntimeInfoEnv",
          "",
          "[Added Lines]",
          "22:     from airflow.providers.cncf.kubernetes.backcompat.pod_runtime_info_env import PodRuntimeInfoEnv  # noqa",
          "",
          "---------------"
        ],
        "airflow/kubernetes/volume.py||airflow/kubernetes/volume.py": [
          "File: airflow/kubernetes/volume.py -> airflow/kubernetes/volume.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "16: # specific language governing permissions and limitations",
          "17: # under the License.",
          "18: \"\"\"This module is deprecated. Please use :mod:`kubernetes.client.models.V1Volume`.\"\"\"",
          "22: import warnings",
          "24: with warnings.catch_warnings():",
          "27: warnings.warn(",
          "28:     \"This module is deprecated. Please use `kubernetes.client.models.V1Volume`.\",",
          "",
          "[Removed Lines]",
          "19: # flake8: noqa",
          "25:     from airflow.providers.cncf.kubernetes.backcompat.volume import Volume",
          "",
          "[Added Lines]",
          "22:     from airflow.providers.cncf.kubernetes.backcompat.volume import Volume  # noqa: autoflake",
          "",
          "---------------"
        ],
        "airflow/kubernetes/volume_mount.py||airflow/kubernetes/volume_mount.py": [
          "File: airflow/kubernetes/volume_mount.py -> airflow/kubernetes/volume_mount.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "16: # specific language governing permissions and limitations",
          "17: # under the License.",
          "18: \"\"\"This module is deprecated. Please use :mod:`kubernetes.client.models.V1VolumeMount`.\"\"\"",
          "22: import warnings",
          "24: with warnings.catch_warnings():",
          "27: warnings.warn(",
          "28:     \"This module is deprecated. Please use `kubernetes.client.models.V1VolumeMount`.\",",
          "",
          "[Removed Lines]",
          "19: # flake8: noqa",
          "25:     from airflow.providers.cncf.kubernetes.backcompat.volume_mount import VolumeMount",
          "",
          "[Added Lines]",
          "22:     from airflow.providers.cncf.kubernetes.backcompat.volume_mount import VolumeMount  # noqa: autoflake",
          "",
          "---------------"
        ],
        "airflow/migrations/versions/2c6edca13270_resource_based_permissions.py||airflow/migrations/versions/2c6edca13270_resource_based_permissions.py": [
          "File: airflow/migrations/versions/2c6edca13270_resource_based_permissions.py -> airflow/migrations/versions/2c6edca13270_resource_based_permissions.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "318: def downgrade():",
          "319:     \"\"\"Unapply Resource based permissions.\"\"\"",
          "",
          "[Removed Lines]",
          "320:     pass",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "airflow/migrations/versions/a13f7613ad25_resource_based_permissions_for_default_.py||airflow/migrations/versions/a13f7613ad25_resource_based_permissions_for_default_.py": [
          "File: airflow/migrations/versions/a13f7613ad25_resource_based_permissions_for_default_.py -> airflow/migrations/versions/a13f7613ad25_resource_based_permissions_for_default_.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "170: def downgrade():",
          "171:     \"\"\"Unapply Resource based permissions.\"\"\"",
          "",
          "[Removed Lines]",
          "172:     pass",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "airflow/timetables/base.py||airflow/timetables/base.py": [
          "File: airflow/timetables/base.py -> airflow/timetables/base.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "146:         :raises: AirflowTimetableInvalid on validation failure.",
          "147:         \"\"\"",
          "150:     @property",
          "151:     def summary(self) -> str:",
          "",
          "[Removed Lines]",
          "148:         pass",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "airflow/utils/db.py||airflow/utils/db.py": [
          "File: airflow/utils/db.py -> airflow/utils/db.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "663:     except (exc.OperationalError, exc.ProgrammingError):",
          "664:         # fallback if tables hasn't been created yet",
          "665:         session.rollback()",
          "667:     if dups:",
          "668:         yield (",
          "669:             'Seems you have non unique conn_id in connection table.\\n'",
          "",
          "[Removed Lines]",
          "666:         pass",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "686:     except (exc.OperationalError, exc.ProgrammingError, exc.InternalError):",
          "687:         # fallback if tables hasn't been created yet",
          "688:         session.rollback()",
          "691:     if n_nulls:",
          "692:         yield (",
          "",
          "[Removed Lines]",
          "689:         pass",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "airflow/www/decorators.py||airflow/www/decorators.py": [
          "File: airflow/www/decorators.py -> airflow/www/decorators.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "65:                     logger.exception(",
          "66:                         \"Failed to parse execution_date from the request: %s\", execution_date_value",
          "67:                     )",
          "70:             session.add(log)",
          "",
          "[Removed Lines]",
          "68:                     pass",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "airflow/www/views.py||airflow/www/views.py": [
          "File: airflow/www/views.py -> airflow/www/views.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "4726: class CustomUserLDAPModelView(MultiResourceUserMixin, UserLDAPModelView):",
          "4727:     \"\"\"Customize permission names for FAB's builtin UserLDAPModelView.\"\"\"",
          "4732: class CustomUserOAuthModelView(MultiResourceUserMixin, UserOAuthModelView):",
          "4733:     \"\"\"Customize permission names for FAB's builtin UserOAuthModelView.\"\"\"",
          "4738: class CustomUserOIDModelView(MultiResourceUserMixin, UserOIDModelView):",
          "4739:     \"\"\"Customize permission names for FAB's builtin UserOIDModelView.\"\"\"",
          "4744: class CustomUserRemoteUserModelView(MultiResourceUserMixin, UserRemoteUserModelView):",
          "4745:     \"\"\"Customize permission names for FAB's builtin UserRemoteUserModelView.\"\"\"",
          "",
          "[Removed Lines]",
          "4729:     pass",
          "4735:     pass",
          "4741:     pass",
          "4747:     pass",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "tests/task/__init__.py||tests/task/__init__.py": [
          "File: tests/task/__init__.py -> tests/task/__init__.py"
        ]
      }
    },
    {
      "candidate_hash": "c836e71b2f39e069ae880eb7a2eacfb420672fd6",
      "candidate_info": {
        "commit_hash": "c836e71b2f39e069ae880eb7a2eacfb420672fd6",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/c836e71b2f39e069ae880eb7a2eacfb420672fd6",
        "files": [
          "airflow/kubernetes/secret.py"
        ],
        "message": "Bugfix: Deepcopying Kubernetes Secrets attributes causing issues (#20318)\n\nEncountered a nasty bug where somebody basically implemented their own KubernetesPodSensor, which failed after more than one attempt when using mode=\"poke\" + a volume + a secret.\n\nRoot cause turned out to be in `secret.attach_to_pod()`. In here, a volume and volumemount is created to mount the secret. A deepcopy() is made of the given Pod spec. In order to avoid appending to None, there is this line: `cp_pod.spec.volumes = pod.spec.volumes or []`. In case a volume is set on the Pod spec, a reference is created to the original pod spec volumes, which in turn was a reference to `self.volumes`. As a result, each secret resulted in a volume added to `self.volumes`, which resulted in an error when running the sensor a second time because the secret volume was already mounted during the first sensor attempt.\n\nThis PR references the deepcopied object instead, and creates a new list if pod.spec.volumes is None.\n\nCo-authored-by: Bas Harenslak <bas@astronomer.io>\n(cherry picked from commit 2409760694b668213a111712bb1162884c23dd2d)",
        "before_after_code_files": [
          "airflow/kubernetes/secret.py||airflow/kubernetes/secret.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/kubernetes/secret.py||airflow/kubernetes/secret.py": [
          "File: airflow/kubernetes/secret.py -> airflow/kubernetes/secret.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "91:     def attach_to_pod(self, pod: k8s.V1Pod) -> k8s.V1Pod:",
          "92:         \"\"\"Attaches to pod\"\"\"",
          "93:         cp_pod = copy.deepcopy(pod)",
          "94:         if self.deploy_type == 'volume':",
          "95:             volume, volume_mount = self.to_volume_secret()",
          "97:             cp_pod.spec.volumes.append(volume)",
          "99:             cp_pod.spec.containers[0].volume_mounts.append(volume_mount)",
          "100:         if self.deploy_type == 'env' and self.key is not None:",
          "101:             env = self.to_env_secret()",
          "103:             cp_pod.spec.containers[0].env.append(env)",
          "104:         if self.deploy_type == 'env' and self.key is None:",
          "105:             env_from = self.to_env_from_secret()",
          "107:             cp_pod.spec.containers[0].env_from.append(env_from)",
          "108:         return cp_pod",
          "110:     def __eq__(self, other):",
          "",
          "[Removed Lines]",
          "96:             cp_pod.spec.volumes = pod.spec.volumes or []",
          "98:             cp_pod.spec.containers[0].volume_mounts = pod.spec.containers[0].volume_mounts or []",
          "102:             cp_pod.spec.containers[0].env = cp_pod.spec.containers[0].env or []",
          "106:             cp_pod.spec.containers[0].env_from = cp_pod.spec.containers[0].env_from or []",
          "",
          "[Added Lines]",
          "97:             if cp_pod.spec.volumes is None:",
          "98:                 cp_pod.spec.volumes = []",
          "100:             if cp_pod.spec.containers[0].volume_mounts is None:",
          "101:                 cp_pod.spec.containers[0].volume_mounts = []",
          "106:             if cp_pod.spec.containers[0].env is None:",
          "107:                 cp_pod.spec.containers[0].env = []",
          "112:             if cp_pod.spec.containers[0].env_from is None:",
          "113:                 cp_pod.spec.containers[0].env_from = []",
          "",
          "---------------"
        ]
      }
    }
  ]
}