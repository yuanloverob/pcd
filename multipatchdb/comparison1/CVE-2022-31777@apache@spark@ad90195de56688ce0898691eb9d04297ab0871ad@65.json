{
  "cve_id": "CVE-2022-31777",
  "cve_desc": "A stored cross-site scripting (XSS) vulnerability in Apache Spark 3.2.1 and earlier, and 3.3.0, allows remote attackers to execute arbitrary JavaScript in the web browser of a user, by including a malicious payload into the logs which would be returned in logs rendered in the UI.",
  "repo": "apache/spark",
  "patch_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
  "patch_info": {
    "commit_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
    "repo": "apache/spark",
    "commit_url": "https://github.com/apache/spark/commit/ad90195de56688ce0898691eb9d04297ab0871ad",
    "files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ],
    "message": "[SPARK-39505][UI] Escape log content rendered in UI\n\n### What changes were proposed in this pull request?\n\nEscape log content rendered to the UI.\n\n### Why are the changes needed?\n\nLog content may contain reserved characters or other code in the log and be misinterpreted in the UI as HTML.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nExisting tests\n\nCloses #36902 from srowen/LogViewEscape.\n\nAuthored-by: Sean Owen <srowen@gmail.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
    "before_after_code_files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ]
  },
  "patch_diff": {
    "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js": [
      "File: core/src/main/resources/org/apache/spark/ui/static/log-view.js -> core/src/main/resources/org/apache/spark/ui/static/log-view.js",
      "--- Hunk 1 ---",
      "[Context before]",
      "85:       if (retStartByte == 0) {",
      "86:         disableMoreButton();",
      "87:       }",
      "90:       curLogLength = curLogLength + (startByte - retStartByte);",
      "91:       startByte = retStartByte;",
      "",
      "[Removed Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(cleanData);",
      "",
      "[Added Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(document.createTextNode(cleanData));",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "115:             var retLogLength = dataInfo[2];",
      "117:             var cleanData = data.substring(newlineIndex + 1);",
      "120:             curLogLength = curLogLength + (retEndByte - retStartByte);",
      "121:             endByte = retEndByte;",
      "",
      "[Removed Lines]",
      "118:             $(\"pre\", \".log-content\").append(cleanData);",
      "",
      "[Added Lines]",
      "118:             $(\"pre\", \".log-content\").append(document.createTextNode(cleanData));",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "80efaa2e979276643df35b03f9c44c31340a62b3",
      "candidate_info": {
        "commit_hash": "80efaa2e979276643df35b03f9c44c31340a62b3",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/80efaa2e979276643df35b03f9c44c31340a62b3",
        "files": [
          "sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/OffHeapColumnVector.java",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetInteroperabilitySuite.scala"
        ],
        "message": "[SPARK-34863][SQL][FOLLOW-UP] Handle IsAllNull in OffHeapColumnVector\n\n### What changes were proposed in this pull request?\n\nThis PR fixes an issue of reading null columns with the vectorised Parquet reader when the entire column is null or does not exist. This is especially noticeable when performing a merge or schema evolution in Parquet.\n\nThe issue is only exposed with the `OffHeapColumnVector` which does not handle `isAllNull` flag - `OnHeapColumnVector` already handles `isAllNull` so everything works fine there.\n\n### Why are the changes needed?\n\nThe change is needed to correctly read null columns using the vectorised reader in the off-heap mode.\n\n### Does this PR introduce _any_ user-facing change?\nNo.\n\n### How was this patch tested?\n\nI updated the existing unit tests to ensure we cover off-heap mode. I confirmed that the tests pass with the fix and fail without.\n\nCloses #36366 from sadikovi/fix-off-heap-cv.\n\nAuthored-by: Ivan Sadikov <ivan.sadikov@databricks.com>\nSigned-off-by: Chao Sun <sunchao@apple.com>",
        "before_after_code_files": [
          "sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/OffHeapColumnVector.java||sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/OffHeapColumnVector.java",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetInteroperabilitySuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetInteroperabilitySuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/OffHeapColumnVector.java||sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/OffHeapColumnVector.java": [
          "File: sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/OffHeapColumnVector.java -> sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/OffHeapColumnVector.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "133:   @Override",
          "134:   public boolean isNullAt(int rowId) {",
          "136:   }",
          "",
          "[Removed Lines]",
          "135:     return Platform.getByte(null, nulls + rowId) == 1;",
          "",
          "[Added Lines]",
          "135:     return isAllNull || Platform.getByte(null, nulls + rowId) == 1;",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "376:   }",
          "378:   test(\"vectorized reader: missing array\") {",
          "395:         )",
          "396:       }",
          "397:     }",
          "398:   }",
          "",
          "[Removed Lines]",
          "379:     withSQLConf(SQLConf.PARQUET_VECTORIZED_READER_NESTED_COLUMN_ENABLED.key -> \"true\") {",
          "380:       val data = Seq(",
          "381:         Tuple1(null),",
          "382:         Tuple1(Seq()),",
          "383:         Tuple1(Seq(\"a\", \"b\", \"c\")),",
          "384:         Tuple1(Seq(null))",
          "385:       )",
          "387:       val readSchema = new StructType().add(\"_2\", new ArrayType(",
          "388:         new StructType().add(\"a\", LongType, nullable = true),",
          "389:         containsNull = true)",
          "390:       )",
          "392:       withParquetFile(data) { file =>",
          "393:         checkAnswer(spark.read.schema(readSchema).parquet(file),",
          "394:           Row(null) :: Row(null) :: Row(null) :: Row(null) :: Nil",
          "",
          "[Added Lines]",
          "379:     Seq(true, false).foreach { offheapEnabled =>",
          "380:       withSQLConf(",
          "381:           SQLConf.PARQUET_VECTORIZED_READER_NESTED_COLUMN_ENABLED.key -> \"true\",",
          "382:           SQLConf.COLUMN_VECTOR_OFFHEAP_ENABLED.key -> offheapEnabled.toString) {",
          "383:         val data = Seq(",
          "384:           Tuple1(null),",
          "385:           Tuple1(Seq()),",
          "386:           Tuple1(Seq(\"a\", \"b\", \"c\")),",
          "387:           Tuple1(Seq(null))",
          "388:         )",
          "390:         val readSchema = new StructType().add(\"_2\", new ArrayType(",
          "391:           new StructType().add(\"a\", LongType, nullable = true),",
          "392:           containsNull = true)",
          "395:         withParquetFile(data) { file =>",
          "396:           checkAnswer(spark.read.schema(readSchema).parquet(file),",
          "397:             Row(null) :: Row(null) :: Row(null) :: Row(null) :: Nil",
          "398:           )",
          "399:         }",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "666:   }",
          "668:   test(\"vectorized reader: missing all struct fields\") {",
          "686:       }",
          "687:     }",
          "688:   }",
          "690:   test(\"vectorized reader: missing some struct fields\") {",
          "708:       }",
          "709:     }",
          "710:   }",
          "",
          "[Removed Lines]",
          "669:     withSQLConf(SQLConf.PARQUET_VECTORIZED_READER_NESTED_COLUMN_ENABLED.key -> \"true\") {",
          "670:       val data = Seq(",
          "671:         Tuple1((1, \"a\")),",
          "672:         Tuple1((2, null)),",
          "673:         Tuple1(null)",
          "674:       )",
          "676:       val readSchema = new StructType().add(\"_1\",",
          "677:         new StructType()",
          "678:             .add(\"_3\", IntegerType, nullable = true)",
          "679:             .add(\"_4\", LongType, nullable = true),",
          "680:         nullable = true)",
          "682:       withParquetFile(data) { file =>",
          "683:         checkAnswer(spark.read.schema(readSchema).parquet(file),",
          "684:           Row(null) :: Row(null) :: Row(null) :: Nil",
          "685:         )",
          "691:     withSQLConf(SQLConf.PARQUET_VECTORIZED_READER_NESTED_COLUMN_ENABLED.key -> \"true\") {",
          "692:       val data = Seq(",
          "693:         Tuple1((1, \"a\")),",
          "694:         Tuple1((2, null)),",
          "695:         Tuple1(null)",
          "696:       )",
          "698:       val readSchema = new StructType().add(\"_1\",",
          "699:         new StructType()",
          "700:             .add(\"_1\", IntegerType, nullable = true)",
          "701:             .add(\"_3\", LongType, nullable = true),",
          "702:         nullable = true)",
          "704:       withParquetFile(data) { file =>",
          "705:         checkAnswer(spark.read.schema(readSchema).parquet(file),",
          "706:           Row(null) :: Row(Row(1, null)) :: Row(Row(2, null)) :: Nil",
          "707:         )",
          "",
          "[Added Lines]",
          "673:     Seq(true, false).foreach { offheapEnabled =>",
          "674:       withSQLConf(",
          "675:           SQLConf.PARQUET_VECTORIZED_READER_NESTED_COLUMN_ENABLED.key -> \"true\",",
          "676:           SQLConf.COLUMN_VECTOR_OFFHEAP_ENABLED.key -> offheapEnabled.toString) {",
          "677:         val data = Seq(",
          "678:           Tuple1((1, \"a\")),",
          "679:           Tuple1((2, null)),",
          "680:           Tuple1(null)",
          "681:         )",
          "683:         val readSchema = new StructType().add(\"_1\",",
          "684:           new StructType()",
          "685:               .add(\"_3\", IntegerType, nullable = true)",
          "686:               .add(\"_4\", LongType, nullable = true),",
          "687:           nullable = true)",
          "689:         withParquetFile(data) { file =>",
          "690:           checkAnswer(spark.read.schema(readSchema).parquet(file),",
          "691:             Row(null) :: Row(null) :: Row(null) :: Nil",
          "692:           )",
          "693:         }",
          "699:     Seq(true, false).foreach { offheapEnabled =>",
          "700:       withSQLConf(",
          "701:           SQLConf.PARQUET_VECTORIZED_READER_NESTED_COLUMN_ENABLED.key -> \"true\",",
          "702:           SQLConf.COLUMN_VECTOR_OFFHEAP_ENABLED.key -> offheapEnabled.toString) {",
          "703:         val data = Seq(",
          "704:           Tuple1((1, \"a\")),",
          "705:           Tuple1((2, null)),",
          "706:           Tuple1(null)",
          "707:         )",
          "709:         val readSchema = new StructType().add(\"_1\",",
          "710:           new StructType()",
          "711:               .add(\"_1\", IntegerType, nullable = true)",
          "712:               .add(\"_3\", LongType, nullable = true),",
          "713:           nullable = true)",
          "715:         withParquetFile(data) { file =>",
          "716:           checkAnswer(spark.read.schema(readSchema).parquet(file),",
          "717:             Row(null) :: Row(Row(1, null)) :: Row(Row(2, null)) :: Nil",
          "718:           )",
          "719:         }",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetInteroperabilitySuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetInteroperabilitySuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetInteroperabilitySuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetInteroperabilitySuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "104:     Seq(false, true).foreach { legacyMode =>",
          "145:             )",
          "146:           }",
          "147:         }",
          "148:       }",
          "",
          "[Removed Lines]",
          "105:       withSQLConf(SQLConf.PARQUET_WRITE_LEGACY_FORMAT.key -> legacyMode.toString) {",
          "106:         withTempPath { tableDir =>",
          "107:           val schema1 = StructType(",
          "108:             StructField(\"col-0\", ArrayType(",
          "109:               StructType(",
          "110:                 StructField(\"col-0\", IntegerType, true) ::",
          "111:                 Nil",
          "112:               ),",
          "113:               containsNull = false // allows to create 2-level Parquet LIST type in legacy mode",
          "114:             )) ::",
          "115:             Nil",
          "116:           )",
          "117:           val row1 = Row(Seq(Row(1)))",
          "118:           val df1 = spark.createDataFrame(spark.sparkContext.parallelize(row1 :: Nil, 1), schema1)",
          "119:           df1.write.parquet(tableDir.getAbsolutePath)",
          "121:           val schema2 = StructType(",
          "122:             StructField(\"col-0\", ArrayType(",
          "123:               StructType(",
          "124:                 StructField(\"col-0\", IntegerType, true) ::",
          "125:                 StructField(\"col-1\", IntegerType, true) :: // additional field",
          "126:                 Nil",
          "127:               ),",
          "128:               containsNull = false",
          "129:             )) ::",
          "130:             Nil",
          "131:           )",
          "132:           val row2 = Row(Seq(Row(1, 2)))",
          "133:           val df2 = spark.createDataFrame(spark.sparkContext.parallelize(row2 :: Nil, 1), schema2)",
          "134:           df2.write.mode(\"append\").parquet(tableDir.getAbsolutePath)",
          "138:           withAllParquetReaders {",
          "139:             checkAnswer(",
          "140:               spark.read.schema(schema2).parquet(tableDir.getAbsolutePath),",
          "141:               Seq(",
          "142:                 Row(Seq(Row(1, null))),",
          "143:                 Row(Seq(Row(1, 2)))",
          "144:               )",
          "",
          "[Added Lines]",
          "105:       Seq(false, true).foreach { offheapEnabled =>",
          "106:         withSQLConf(",
          "107:             SQLConf.PARQUET_WRITE_LEGACY_FORMAT.key -> legacyMode.toString,",
          "108:             SQLConf.COLUMN_VECTOR_OFFHEAP_ENABLED.key -> offheapEnabled.toString) {",
          "109:           withTempPath { tableDir =>",
          "110:             val schema1 = StructType(",
          "111:               StructField(\"col-0\", ArrayType(",
          "112:                 StructType(",
          "113:                   StructField(\"col-0\", IntegerType, true) ::",
          "114:                   Nil",
          "115:                 ),",
          "116:                 containsNull = false // allows to create 2-level Parquet LIST type in legacy mode",
          "117:               )) ::",
          "118:               Nil",
          "119:             )",
          "120:             val row1 = Row(Seq(Row(1)))",
          "121:             val df1 = spark.createDataFrame(spark.sparkContext.parallelize(row1 :: Nil, 1), schema1)",
          "122:             df1.write.parquet(tableDir.getAbsolutePath)",
          "124:             val schema2 = StructType(",
          "125:               StructField(\"col-0\", ArrayType(",
          "126:                 StructType(",
          "127:                   StructField(\"col-0\", IntegerType, true) ::",
          "128:                   StructField(\"col-1\", IntegerType, true) :: // additional field",
          "129:                   Nil",
          "130:                 ),",
          "131:                 containsNull = false",
          "132:               )) ::",
          "133:               Nil",
          "135:             val row2 = Row(Seq(Row(1, 2)))",
          "136:             val df2 = spark.createDataFrame(spark.sparkContext.parallelize(row2 :: Nil, 1), schema2)",
          "137:             df2.write.mode(\"append\").parquet(tableDir.getAbsolutePath)",
          "141:             withAllParquetReaders {",
          "142:               checkAnswer(",
          "143:                 spark.read.schema(schema2).parquet(tableDir.getAbsolutePath),",
          "144:                 Seq(",
          "145:                   Row(Seq(Row(1, null))),",
          "146:                   Row(Seq(Row(1, 2)))",
          "147:                 )",
          "148:               )",
          "149:             }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "505248df16b9785e56a206db62129f6eff945483",
      "candidate_info": {
        "commit_hash": "505248df16b9785e56a206db62129f6eff945483",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/505248df16b9785e56a206db62129f6eff945483",
        "files": [
          "sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCSuite.scala"
        ],
        "message": "[SPARK-39258][TESTS] Fix `Hide credentials in show create table`\n\n### What changes were proposed in this pull request?\n[SPARK-35378-FOLLOWUP](https://github.com/apache/spark/pull/36632) changes the return value of `CommandResultExec.executeCollect()` from `InternalRow` to `UnsafeRow`, this change causes the result of `r.tostring` in the following code:\n\nhttps://github.com/apache/spark/blob/de73753bb2e5fd947f237e731ff05aa9f2711677/sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCSuite.scala#L1143-L1148\n\nchange from\n\n```\n[CREATE TABLE tab1 (\n  NAME STRING,\n  THEID INT)\nUSING org.apache.spark.sql.jdbc\nOPTIONS (\n  'dbtable' = 'TEST.PEOPLE',\n  'password' = '*********(redacted)',\n  'url' = '*********(redacted)',\n  'user' = 'testUser')\n]\n```\nto\n\n```\n[0,10000000d5,5420455441455243,62617420454c4241,414e20200a282031,4e4952545320454d,45485420200a2c47,a29544e49204449,726f20474e495355,6568636170612e67,732e6b726170732e,a6362646a2e6c71,20534e4f4954504f,7462642720200a28,203d2027656c6261,45502e5453455427,200a2c27454c504f,6f77737361702720,2a27203d20276472,2a2a2a2a2a2a2a2a,6574636164657228,2720200a2c272964,27203d20276c7275,2a2a2a2a2a2a2a2a,746361646572282a,20200a2c27296465,3d20277265737527,7355747365742720,a29277265]\n```\n\nand the UT `JDBCSuite$Hide credentials in show create table` failed in master branch.\n\nThis pr is  change to use `executeCollectPublic()` instead of `executeCollect()` to fix this UT.\n\n### Why are the changes needed?\nFix UT failed in mater branch after [SPARK-35378-FOLLOWUP](https://github.com/apache/spark/pull/36632)\n\n### Does this PR introduce _any_ user-facing change?\nNO.\n\n### How was this patch tested?\n\n- GitHub Action pass\n- Manual test\n\nRun `mvn clean install -DskipTests -pl sql/core -am -Dtest=none -DwildcardSuites=org.apache.spark.sql.jdbc.JDBCSuite`\n\n**Before**\n\n```\n- Hide credentials in show create table *** FAILED ***\n  \"[0,10000000d5,5420455441455243,62617420454c4241,414e20200a282031,4e4952545320454d,45485420200a2c47,a29544e49204449,726f20474e495355,6568636170612e67,732e6b726170732e,a6362646a2e6c71,20534e4f4954504f,7462642720200a28,203d2027656c6261,45502e5453455427,200a2c27454c504f,6f77737361702720,2a27203d20276472,2a2a2a2a2a2a2a2a,6574636164657228,2720200a2c272964,27203d20276c7275,2a2a2a2a2a2a2a2a,746361646572282a,20200a2c27296465,3d20277265737527,7355747365742720,a29277265]\" did not contain \"TEST.PEOPLE\" (JDBCSuite.scala:1146)\n\n```\n\n**After**\n\n```\nRun completed in 24 seconds, 868 milliseconds.\nTotal number of tests run: 93\nSuites: completed 2, aborted 0\nTests: succeeded 93, failed 0, canceled 0, ignored 0, pending 0\nAll tests passed.\n```\n\nCloses #36637 from LuciferYang/SPARK-39258.\n\nAuthored-by: yangjie01 <yangjie01@baidu.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>\n(cherry picked from commit 6eb15d12ae6bd77412dbfbf46eb8dbeec1eab466)\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
        "before_after_code_files": [
          "sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "1141:          \"\"\".stripMargin)",
          "1143:       val show = ShowCreateTableCommand(TableIdentifier(tableName), ShowCreateTable.getoutputAttrs)",
          "1145:         assert(!r.toString.contains(password))",
          "1146:         assert(r.toString.contains(dbTable))",
          "1147:         assert(r.toString.contains(userName))",
          "",
          "[Removed Lines]",
          "1144:       spark.sessionState.executePlan(show).executedPlan.executeCollect().foreach { r =>",
          "",
          "[Added Lines]",
          "1144:       spark.sessionState.executePlan(show).executedPlan.executeCollectPublic().foreach { r =>",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1154:       }",
          "1156:       withSQLConf(SQLConf.SQL_OPTIONS_REDACTION_PATTERN.key -> \"(?i)dbtable|user\") {",
          "1158:           assert(!r.toString.contains(password))",
          "1159:           assert(!r.toString.contains(dbTable))",
          "1160:           assert(!r.toString.contains(userName))",
          "",
          "[Removed Lines]",
          "1157:         spark.sessionState.executePlan(show).executedPlan.executeCollect().foreach { r =>",
          "",
          "[Added Lines]",
          "1157:         spark.sessionState.executePlan(show).executedPlan.executeCollectPublic().foreach { r =>",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "76fe1bf41268bc928b0637510728dde1aa8c5805",
      "candidate_info": {
        "commit_hash": "76fe1bf41268bc928b0637510728dde1aa8c5805",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/76fe1bf41268bc928b0637510728dde1aa8c5805",
        "files": [
          "core/src/main/scala/org/apache/spark/executor/Executor.scala",
          "core/src/test/scala/org/apache/spark/executor/CoarseGrainedExecutorBackendSuite.scala",
          "core/src/test/scala/org/apache/spark/executor/ExecutorSuite.scala"
        ],
        "message": "[SPARK-38916][CORE] Tasks not killed caused by race conditions between killTask() and launchTask()\n\n### What changes were proposed in this pull request?\n\nThis PR fixes the race conditions between the killTask() call and the launchTask() call that sometimes causes tasks not to be killed properly. If killTask() probes the map of pendingTasksLaunches before launchTask() has had a chance to put the corresponding task into that map, the kill flag will be lost and the subsequent launchTask() call will just proceed and run that task without knowing this task should be killed instead. The fix adds a kill mark during the killTask() call so that subsequent launchTask() can pick up the kill mark and call kill() on the TaskLauncher. If killTask() happens to happen after the task has finished and thus makes the kill mark useless, it will be cleaned up in a background thread.\n\n### Why are the changes needed?\n\nBug fix.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nAdded UTs.\n\nCloses #36238 from maryannxue/spark-38916.\n\nAuthored-by: Maryann Xue <maryann.xue@gmail.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\n(cherry picked from commit bb5092b9af60afdceeccb239d14be660f77ae0ea)\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>",
        "before_after_code_files": [
          "core/src/main/scala/org/apache/spark/executor/Executor.scala||core/src/main/scala/org/apache/spark/executor/Executor.scala",
          "core/src/test/scala/org/apache/spark/executor/CoarseGrainedExecutorBackendSuite.scala||core/src/test/scala/org/apache/spark/executor/CoarseGrainedExecutorBackendSuite.scala",
          "core/src/test/scala/org/apache/spark/executor/ExecutorSuite.scala||core/src/test/scala/org/apache/spark/executor/ExecutorSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "core/src/main/scala/org/apache/spark/executor/Executor.scala||core/src/main/scala/org/apache/spark/executor/Executor.scala": [
          "File: core/src/main/scala/org/apache/spark/executor/Executor.scala -> core/src/main/scala/org/apache/spark/executor/Executor.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "84:   private val EMPTY_BYTE_BUFFER = ByteBuffer.wrap(new Array[Byte](0))",
          "89:   Utils.checkHost(executorHostname)",
          "",
          "[Removed Lines]",
          "86:   private val conf = env.conf",
          "",
          "[Added Lines]",
          "86:   private[executor] val conf = env.conf",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "108:     val threadFactory = new ThreadFactoryBuilder()",
          "109:       .setDaemon(true)",
          "110:       .setNameFormat(\"Executor task launch worker-%d\")",
          "",
          "[Removed Lines]",
          "107:   private val threadPool = {",
          "",
          "[Added Lines]",
          "107:   private[executor] val threadPool = {",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "174:   private val maxResultSize = conf.get(MAX_RESULT_SIZE)",
          "",
          "[Removed Lines]",
          "177:   private val runningTasks = new ConcurrentHashMap[Long, TaskRunner]",
          "",
          "[Added Lines]",
          "177:   private[executor] val runningTasks = new ConcurrentHashMap[Long, TaskRunner]",
          "180:   private val KILL_MARK_TTL_MS = 10000L",
          "184:   private[executor] val killMarks = new ConcurrentHashMap[Long, (Boolean, String, Long)]",
          "186:   private val killMarkCleanupTask = new Runnable {",
          "187:     override def run(): Unit = {",
          "188:       val oldest = System.currentTimeMillis() - KILL_MARK_TTL_MS",
          "189:       val iter = killMarks.entrySet().iterator()",
          "190:       while (iter.hasNext) {",
          "191:         if (iter.next().getValue._3 < oldest) {",
          "192:           iter.remove()",
          "193:         }",
          "194:       }",
          "195:     }",
          "196:   }",
          "199:   private val killMarkCleanupService =",
          "200:     ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"executor-kill-mark-cleanup\")",
          "202:   killMarkCleanupService.scheduleAtFixedRate(",
          "203:     killMarkCleanupTask, KILL_MARK_TTL_MS, KILL_MARK_TTL_MS, TimeUnit.MILLISECONDS)",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "264:     decommissioned = true",
          "265:   }",
          "267:   def launchTask(context: ExecutorBackend, taskDescription: TaskDescription): Unit = {",
          "270:     threadPool.execute(tr)",
          "271:     if (decommissioned) {",
          "272:       log.error(s\"Launching a task while in decommissioned state.\")",
          "",
          "[Removed Lines]",
          "268:     val tr = new TaskRunner(context, taskDescription, plugins)",
          "269:     runningTasks.put(taskDescription.taskId, tr)",
          "",
          "[Added Lines]",
          "293:   private[executor] def createTaskRunner(context: ExecutorBackend,",
          "294:     taskDescription: TaskDescription) = new TaskRunner(context, taskDescription, plugins)",
          "297:     val taskId = taskDescription.taskId",
          "298:     val tr = createTaskRunner(context, taskDescription)",
          "299:     runningTasks.put(taskId, tr)",
          "300:     val killMark = killMarks.get(taskId)",
          "301:     if (killMark != null) {",
          "302:       tr.kill(killMark._1, killMark._2)",
          "303:       killMarks.remove(taskId)",
          "304:     }",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "274:   }",
          "276:   def killTask(taskId: Long, interruptThread: Boolean, reason: String): Unit = {",
          "277:     val taskRunner = runningTasks.get(taskId)",
          "278:     if (taskRunner != null) {",
          "279:       if (taskReaperEnabled) {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "312:     killMarks.put(taskId, (interruptThread, reason, System.currentTimeMillis()))",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "296:       } else {",
          "297:         taskRunner.kill(interruptThread = interruptThread, reason = reason)",
          "298:       }",
          "299:     }",
          "300:   }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "336:       killMarks.remove(taskId)",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "334:       if (threadPool != null) {",
          "335:         threadPool.shutdown()",
          "336:       }",
          "337:       if (replClassLoader != null && plugins != null) {",
          "339:         Utils.withContextClassLoader(replClassLoader) {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "375:       if (killMarkCleanupService != null) {",
          "376:         killMarkCleanupService.shutdown()",
          "377:       }",
          "",
          "---------------"
        ],
        "core/src/test/scala/org/apache/spark/executor/CoarseGrainedExecutorBackendSuite.scala||core/src/test/scala/org/apache/spark/executor/CoarseGrainedExecutorBackendSuite.scala": [
          "File: core/src/test/scala/org/apache/spark/executor/CoarseGrainedExecutorBackendSuite.scala -> core/src/test/scala/org/apache/spark/executor/CoarseGrainedExecutorBackendSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: import java.io.File",
          "21: import java.nio.ByteBuffer",
          "22: import java.util.Properties",
          "24: import scala.collection.mutable",
          "25: import scala.concurrent.duration._",
          "27: import org.json4s.{DefaultFormats, Extraction}",
          "28: import org.json4s.JsonAST.{JArray, JObject}",
          "29: import org.json4s.JsonDSL._",
          "31: import org.scalatest.concurrent.Eventually.{eventually, timeout}",
          "32: import org.scalatestplus.mockito.MockitoSugar",
          "",
          "[Removed Lines]",
          "30: import org.mockito.Mockito.when",
          "",
          "[Added Lines]",
          "23: import java.util.concurrent.ConcurrentHashMap",
          "25: import scala.collection.concurrent.TrieMap",
          "32: import org.mockito.ArgumentMatchers.any",
          "33: import org.mockito.Mockito._",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "38: import org.apache.spark.resource.TestResourceIDs._",
          "39: import org.apache.spark.rpc.RpcEnv",
          "40: import org.apache.spark.scheduler.TaskDescription",
          "42: import org.apache.spark.serializer.JavaSerializer",
          "45: class CoarseGrainedExecutorBackendSuite extends SparkFunSuite",
          "46:     with LocalSparkContext with MockitoSugar {",
          "",
          "[Removed Lines]",
          "41: import org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.LaunchTask",
          "43: import org.apache.spark.util.{SerializableBuffer, Utils}",
          "",
          "[Added Lines]",
          "44: import org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.{KillTask, LaunchTask}",
          "46: import org.apache.spark.util.{SerializableBuffer, ThreadUtils, Utils}",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "356:     assert(arg.bindAddress == \"bindaddress1\")",
          "357:   }",
          "359:   private def createMockEnv(conf: SparkConf, serializer: JavaSerializer,",
          "360:       rpcEnv: Option[RpcEnv] = None): SparkEnv = {",
          "361:     val mockEnv = mock[SparkEnv]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "366:   test(s\"Tasks launched should always be cancelled.\")  {",
          "367:     val conf = new SparkConf",
          "368:     val securityMgr = new SecurityManager(conf)",
          "369:     val serializer = new JavaSerializer(conf)",
          "370:     val threadPool = ThreadUtils.newDaemonFixedThreadPool(32, \"test-executor\")",
          "371:     var backend: CoarseGrainedExecutorBackend = null",
          "373:     try {",
          "374:       val rpcEnv = RpcEnv.create(\"1\", \"localhost\", 0, conf, securityMgr)",
          "375:       val env = createMockEnv(conf, serializer, Some(rpcEnv))",
          "376:       backend = new CoarseGrainedExecutorBackend(env.rpcEnv, rpcEnv.address.hostPort, \"1\",",
          "377:         \"host1\", \"host1\", 4, env, None,",
          "378:         resourceProfile = ResourceProfile.getOrCreateDefaultProfile(conf))",
          "380:       backend.rpcEnv.setupEndpoint(\"Executor 1\", backend)",
          "381:       backend.executor = mock[Executor](CALLS_REAL_METHODS)",
          "382:       val executor = backend.executor",
          "384:       when(executor.threadPool).thenReturn(threadPool)",
          "385:       val runningTasks = spy(new ConcurrentHashMap[Long, Executor#TaskRunner])",
          "386:       when(executor.runningTasks).thenAnswer(_ => runningTasks)",
          "387:       when(executor.conf).thenReturn(conf)",
          "390:       val data = ByteBuffer.wrap(Array[Byte](1, 2, 3, 4))",
          "392:       val numTasks = 1000",
          "393:       val tasksKilled = new TrieMap[Long, Boolean]()",
          "394:       val tasksExecuted = new TrieMap[Long, Boolean]()",
          "397:       val taskDescriptions = (1 to numTasks).map {",
          "398:         taskId => new TaskDescription(taskId, 2, \"1\", \"TASK ${taskId}\", 19,",
          "399:           1, mutable.Map.empty, mutable.Map.empty, mutable.Map.empty, new Properties, 1,",
          "400:           Map(GPU -> new ResourceInformation(GPU, Array(\"0\", \"1\"))), data)",
          "401:       }",
          "402:       assert(taskDescriptions.length == numTasks)",
          "404:       def getFakeTaskRunner(taskDescription: TaskDescription): Executor#TaskRunner = {",
          "405:         new executor.TaskRunner(backend, taskDescription, None) {",
          "406:           override def run(): Unit = {",
          "407:             tasksExecuted.put(taskDescription.taskId, true)",
          "408:             logInfo(s\"task ${taskDescription.taskId} runs.\")",
          "409:           }",
          "411:           override def kill(interruptThread: Boolean, reason: String): Unit = {",
          "412:             logInfo(s\"task ${taskDescription.taskId} killed.\")",
          "413:             tasksKilled.put(taskDescription.taskId, true)",
          "414:           }",
          "415:         }",
          "416:       }",
          "419:       val firstLaunchTask = getFakeTaskRunner(taskDescriptions(1))",
          "420:       val otherTasks = taskDescriptions.slice(1, numTasks).map(getFakeTaskRunner(_)).toArray",
          "421:       assert (otherTasks.length == numTasks - 1)",
          "423:       doReturn(firstLaunchTask, otherTasks: _*).when(executor).",
          "424:         createTaskRunner(any(), any())",
          "427:       taskDescriptions.foreach { taskDescription =>",
          "428:         val buffer = new SerializableBuffer(TaskDescription.encode(taskDescription))",
          "429:         backend.self.send(LaunchTask(buffer))",
          "430:         Thread.sleep(1)",
          "431:         backend.self.send(KillTask(taskDescription.taskId, \"exec1\", false, \"test\"))",
          "432:       }",
          "434:       eventually(timeout(10.seconds)) {",
          "435:         verify(runningTasks, times(numTasks)).put(any(), any())",
          "436:       }",
          "438:       assert(tasksExecuted.size == tasksKilled.size,",
          "439:         s\"Tasks killed ${tasksKilled.size} != tasks executed ${tasksExecuted.size}\")",
          "440:       assert(tasksExecuted.keySet == tasksKilled.keySet)",
          "441:       logInfo(s\"Task executed ${tasksExecuted.size}, task killed ${tasksKilled.size}\")",
          "442:     } finally {",
          "443:       if (backend != null) {",
          "444:         backend.rpcEnv.shutdown()",
          "445:       }",
          "446:       threadPool.shutdownNow()",
          "447:     }",
          "448:   }",
          "454:   test(s\"Tasks not launched should always be cancelled.\")  {",
          "455:     val conf = new SparkConf",
          "456:     val securityMgr = new SecurityManager(conf)",
          "457:     val serializer = new JavaSerializer(conf)",
          "458:     val threadPool = ThreadUtils.newDaemonFixedThreadPool(32, \"test-executor\")",
          "459:     var backend: CoarseGrainedExecutorBackend = null",
          "461:     try {",
          "462:       val rpcEnv = RpcEnv.create(\"1\", \"localhost\", 0, conf, securityMgr)",
          "463:       val env = createMockEnv(conf, serializer, Some(rpcEnv))",
          "464:       backend = new CoarseGrainedExecutorBackend(env.rpcEnv, rpcEnv.address.hostPort, \"1\",",
          "465:         \"host1\", \"host1\", 4, env, None,",
          "466:         resourceProfile = ResourceProfile.getOrCreateDefaultProfile(conf))",
          "468:       backend.rpcEnv.setupEndpoint(\"Executor 1\", backend)",
          "469:       backend.executor = mock[Executor](CALLS_REAL_METHODS)",
          "470:       val executor = backend.executor",
          "472:       when(executor.threadPool).thenReturn(threadPool)",
          "473:       val runningTasks = spy(new ConcurrentHashMap[Long, Executor#TaskRunner])",
          "474:       when(executor.runningTasks).thenAnswer(_ => runningTasks)",
          "475:       when(executor.conf).thenReturn(conf)",
          "478:       val data = ByteBuffer.wrap(Array[Byte](1, 2, 3, 4))",
          "480:       val numTasks = 1000",
          "481:       val tasksKilled = new TrieMap[Long, Boolean]()",
          "482:       val tasksExecuted = new TrieMap[Long, Boolean]()",
          "485:       val taskDescriptions = (1 to numTasks).map {",
          "486:         taskId => new TaskDescription(taskId, 2, \"1\", \"TASK ${taskId}\", 19,",
          "487:           1, mutable.Map.empty, mutable.Map.empty, mutable.Map.empty, new Properties, 1,",
          "488:           Map(GPU -> new ResourceInformation(GPU, Array(\"0\", \"1\"))), data)",
          "489:       }",
          "490:       assert(taskDescriptions.length == numTasks)",
          "492:       def getFakeTaskRunner(taskDescription: TaskDescription): Executor#TaskRunner = {",
          "493:         new executor.TaskRunner(backend, taskDescription, None) {",
          "494:           override def run(): Unit = {",
          "495:             tasksExecuted.put(taskDescription.taskId, true)",
          "496:             logInfo(s\"task ${taskDescription.taskId} runs.\")",
          "497:           }",
          "499:           override def kill(interruptThread: Boolean, reason: String): Unit = {",
          "500:             logInfo(s\"task ${taskDescription.taskId} killed.\")",
          "501:             tasksKilled.put(taskDescription.taskId, true)",
          "502:           }",
          "503:         }",
          "504:       }",
          "507:       val firstLaunchTask = getFakeTaskRunner(taskDescriptions(1))",
          "508:       val otherTasks = taskDescriptions.slice(1, numTasks).map(getFakeTaskRunner(_)).toArray",
          "509:       assert (otherTasks.length == numTasks - 1)",
          "511:       doReturn(firstLaunchTask, otherTasks: _*).when(executor).",
          "512:         createTaskRunner(any(), any())",
          "516:       taskDescriptions.foreach { taskDescription =>",
          "517:         val buffer = new SerializableBuffer(TaskDescription.encode(taskDescription))",
          "518:         backend.self.send(KillTask(taskDescription.taskId, \"exec1\", false, \"test\"))",
          "519:         backend.self.send(LaunchTask(buffer))",
          "520:       }",
          "522:       eventually(timeout(10.seconds)) {",
          "523:         verify(runningTasks, times(numTasks)).put(any(), any())",
          "524:       }",
          "526:       assert(tasksExecuted.size == tasksKilled.size,",
          "527:         s\"Tasks killed ${tasksKilled.size} != tasks executed ${tasksExecuted.size}\")",
          "528:       assert(tasksExecuted.keySet == tasksKilled.keySet)",
          "529:       logInfo(s\"Task executed ${tasksExecuted.size}, task killed ${tasksKilled.size}\")",
          "530:     } finally {",
          "531:       if (backend != null) {",
          "532:         backend.rpcEnv.shutdown()",
          "533:       }",
          "534:       threadPool.shutdownNow()",
          "535:     }",
          "536:   }",
          "",
          "---------------"
        ],
        "core/src/test/scala/org/apache/spark/executor/ExecutorSuite.scala||core/src/test/scala/org/apache/spark/executor/ExecutorSuite.scala": [
          "File: core/src/test/scala/org/apache/spark/executor/ExecutorSuite.scala -> core/src/test/scala/org/apache/spark/executor/ExecutorSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "22: import java.net.URL",
          "23: import java.nio.ByteBuffer",
          "24: import java.util.Properties",
          "26: import java.util.concurrent.atomic.AtomicBoolean",
          "28: import scala.collection.immutable",
          "",
          "[Removed Lines]",
          "25: import java.util.concurrent.{ConcurrentHashMap, CountDownLatch, TimeUnit}",
          "",
          "[Added Lines]",
          "25: import java.util.concurrent.{CountDownLatch, TimeUnit}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "321:       nonZeroAccumulator.add(1)",
          "322:       metrics.registerAccumulator(nonZeroAccumulator)",
          "331:       val mockTaskRunner = mock[executor.TaskRunner]",
          "332:       val mockTask = mock[Task[Any]]",
          "333:       when(mockTask.metrics).thenReturn(metrics)",
          "",
          "[Removed Lines]",
          "324:       val executorClass = classOf[Executor]",
          "325:       val tasksMap = {",
          "326:         val field =",
          "327:           executorClass.getDeclaredField(\"org$apache$spark$executor$Executor$$runningTasks\")",
          "328:         field.setAccessible(true)",
          "329:         field.get(executor).asInstanceOf[ConcurrentHashMap[Long, executor.TaskRunner]]",
          "330:       }",
          "",
          "[Added Lines]",
          "324:       val tasksMap = executor.runningTasks",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "ae890785c4babdca555272ec318bd8439746848e",
      "candidate_info": {
        "commit_hash": "ae890785c4babdca555272ec318bd8439746848e",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/ae890785c4babdca555272ec318bd8439746848e",
        "files": [
          "core/src/main/resources/error/error-classes.json",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/decimalExpressions.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/mathExpressions.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/types/Decimal.scala",
          "sql/core/src/test/resources/sql-tests/inputs/ansi/cast.sql",
          "sql/core/src/test/resources/sql-tests/inputs/cast.sql",
          "sql/core/src/test/resources/sql-tests/results/ansi/cast.sql.out",
          "sql/core/src/test/resources/sql-tests/results/ansi/decimalArithmeticOperations.sql.out",
          "sql/core/src/test/resources/sql-tests/results/ansi/interval.sql.out",
          "sql/core/src/test/resources/sql-tests/results/cast.sql.out"
        ],
        "message": "[SPARK-38762][SQL] Provide query context in Decimal overflow errors\n\n### What changes were proposed in this pull request?\n\nProvide query context in Decimal overflow errors:\n* explicit casting other data types as decimal\n* implicit casting in decimal operations, including add/subtract/multiply/divide/reminder/pmod\n\n### Why are the changes needed?\n\nProvide SQL query context of runtime errors to users, so that they can understand it better.\n\n### Does this PR introduce _any_ user-facing change?\n\nYes, improve the runtime error message of decimal overflow\n\n### How was this patch tested?\n\nExisting UT\nAlso test `sql/core/src/test/resources/sql-tests/inputs/cast.sql` under ANSI mode.\n\nCloses #36040 from gengliangwang/decimalContext.\n\nAuthored-by: Gengliang Wang <gengliang@apache.org>\nSigned-off-by: Gengliang Wang <gengliang@apache.org>\n(cherry picked from commit 7de321005b02dcfe61d2954eeb569bf54ac6e78f)\nSigned-off-by: Gengliang Wang <gengliang@apache.org>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/decimalExpressions.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/decimalExpressions.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/mathExpressions.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/mathExpressions.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/types/Decimal.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/types/Decimal.scala",
          "sql/core/src/test/resources/sql-tests/inputs/ansi/cast.sql||sql/core/src/test/resources/sql-tests/inputs/ansi/cast.sql",
          "sql/core/src/test/resources/sql-tests/inputs/cast.sql||sql/core/src/test/resources/sql-tests/inputs/cast.sql"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "793:         null",
          "794:       } else {",
          "795:         throw QueryExecutionErrors.cannotChangeDecimalPrecisionError(",
          "797:       }",
          "798:     }",
          "799:   }",
          "",
          "[Removed Lines]",
          "796:           value, decimalType.precision, decimalType.scale)",
          "",
          "[Added Lines]",
          "796:           value, decimalType.precision, decimalType.scale, origin.context)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "984:     }",
          "985:   }",
          "987:   override def doGenCode(ctx: CodegenContext, ev: ExprCode): ExprCode = {",
          "988:     val eval = child.genCode(ctx)",
          "989:     val nullSafeCast = nullSafeCastFunction(child.dataType, dataType, ctx)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "987:   def errorContextCode(codegenContext: CodegenContext): String = {",
          "988:     codegenContext.addReferenceObj(\"errCtx\", origin.context)",
          "989:   }",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "1320:     }",
          "1321:   }",
          "1325:     if (canNullSafeCast) {",
          "1326:       code\"\"\"",
          "1327:          |$d.changePrecision(${decimalType.precision}, ${decimalType.scale});",
          "",
          "[Removed Lines]",
          "1323:   private[this] def changePrecision(d: ExprValue, decimalType: DecimalType,",
          "1324:       evPrim: ExprValue, evNull: ExprValue, canNullSafeCast: Boolean): Block = {",
          "",
          "[Added Lines]",
          "1327:   private[this] def changePrecision(",
          "1328:       d: ExprValue,",
          "1329:       decimalType: DecimalType,",
          "1330:       evPrim: ExprValue,",
          "1331:       evNull: ExprValue,",
          "1332:       canNullSafeCast: Boolean,",
          "1333:       ctx: CodegenContext): Block = {",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "1333:       } else {",
          "1334:         s\"\"\"",
          "1335:            |throw QueryExecutionErrors.cannotChangeDecimalPrecisionError(",
          "1337:          \"\"\".stripMargin",
          "1338:       }",
          "1339:       code\"\"\"",
          "",
          "[Removed Lines]",
          "1336:            |  $d, ${decimalType.precision}, ${decimalType.scale});",
          "",
          "[Added Lines]",
          "1345:            |  $d, ${decimalType.precision}, ${decimalType.scale}, ${errorContextCode(ctx)});",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "1360:               if ($tmp == null) {",
          "1361:                 $evNull = true;",
          "1362:               } else {",
          "1364:               }",
          "1365:           \"\"\"",
          "1366:       case StringType if ansiEnabled =>",
          "1367:         (c, evPrim, evNull) =>",
          "1368:           code\"\"\"",
          "1369:               Decimal $tmp = Decimal.fromStringANSI($c);",
          "1371:           \"\"\"",
          "1372:       case BooleanType =>",
          "1373:         (c, evPrim, evNull) =>",
          "1374:           code\"\"\"",
          "1375:             Decimal $tmp = $c ? Decimal.apply(1) : Decimal.apply(0);",
          "1377:           \"\"\"",
          "1378:       case DateType =>",
          "",
          "[Removed Lines]",
          "1363:                 ${changePrecision(tmp, target, evPrim, evNull, canNullSafeCast)}",
          "1370:               ${changePrecision(tmp, target, evPrim, evNull, canNullSafeCast)}",
          "1376:             ${changePrecision(tmp, target, evPrim, evNull, canNullSafeCast)}",
          "",
          "[Added Lines]",
          "1372:                 ${changePrecision(tmp, target, evPrim, evNull, canNullSafeCast, ctx)}",
          "1379:               ${changePrecision(tmp, target, evPrim, evNull, canNullSafeCast, ctx)}",
          "1385:             ${changePrecision(tmp, target, evPrim, evNull, canNullSafeCast, ctx)}",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "1384:           code\"\"\"",
          "1385:             Decimal $tmp = Decimal.apply(",
          "1386:               scala.math.BigDecimal.valueOf(${timestampToDoubleCode(c)}));",
          "1388:           \"\"\"",
          "1389:       case DecimalType() =>",
          "1390:         (c, evPrim, evNull) =>",
          "1391:           code\"\"\"",
          "1392:             Decimal $tmp = $c.clone();",
          "1394:           \"\"\"",
          "1395:       case x: IntegralType =>",
          "1396:         (c, evPrim, evNull) =>",
          "1397:           code\"\"\"",
          "1398:             Decimal $tmp = Decimal.apply((long) $c);",
          "1400:           \"\"\"",
          "1401:       case x: FractionalType =>",
          "",
          "[Removed Lines]",
          "1387:             ${changePrecision(tmp, target, evPrim, evNull, canNullSafeCast)}",
          "1393:             ${changePrecision(tmp, target, evPrim, evNull, canNullSafeCast)}",
          "1399:             ${changePrecision(tmp, target, evPrim, evNull, canNullSafeCast)}",
          "",
          "[Added Lines]",
          "1396:             ${changePrecision(tmp, target, evPrim, evNull, canNullSafeCast, ctx)}",
          "1402:             ${changePrecision(tmp, target, evPrim, evNull, canNullSafeCast, ctx)}",
          "1408:             ${changePrecision(tmp, target, evPrim, evNull, canNullSafeCast, ctx)}",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "1404:           code\"\"\"",
          "1405:             try {",
          "1406:               Decimal $tmp = Decimal.apply(scala.math.BigDecimal.valueOf((double) $c));",
          "1408:             } catch (java.lang.NumberFormatException e) {",
          "1409:               $evNull = true;",
          "1410:             }",
          "",
          "[Removed Lines]",
          "1407:               ${changePrecision(tmp, target, evPrim, evNull, canNullSafeCast)}",
          "",
          "[Added Lines]",
          "1416:               ${changePrecision(tmp, target, evPrim, evNull, canNullSafeCast, ctx)}",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/decimalExpressions.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/decimalExpressions.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/decimalExpressions.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/decimalExpressions.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "137:       dataType.precision,",
          "138:       dataType.scale,",
          "139:       Decimal.ROUND_HALF_UP,",
          "142:   override protected def doGenCode(ctx: CodegenContext, ev: ExprCode): ExprCode = {",
          "143:     nullSafeCodeGen(ctx, ev, eval => {",
          "144:       s\"\"\"",
          "145:          |${ev.value} = $eval.toPrecision(",
          "147:          |${ev.isNull} = ${ev.value} == null;",
          "148:        \"\"\".stripMargin",
          "149:     })",
          "150:   }",
          "",
          "[Removed Lines]",
          "140:       nullOnOverflow)",
          "146:          |  ${dataType.precision}, ${dataType.scale}, Decimal.ROUND_HALF_UP(), $nullOnOverflow);",
          "",
          "[Added Lines]",
          "140:       nullOnOverflow,",
          "141:       origin.context)",
          "144:     val errorContextCode = if (nullOnOverflow) {",
          "145:       ctx.addReferenceObj(\"errCtx\", origin.context)",
          "146:     } else {",
          "147:       \"\\\"\\\"\"",
          "148:     }",
          "153:          |  ${dataType.precision}, ${dataType.scale}, Decimal.ROUND_HALF_UP(), $nullOnOverflow, $errorContextCode);",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "168:   override def eval(input: InternalRow): Any = {",
          "169:     val value = child.eval(input)",
          "170:     if (value == null) {",
          "172:     } else {",
          "173:       value.asInstanceOf[Decimal].toPrecision(",
          "174:         dataType.precision,",
          "175:         dataType.scale,",
          "176:         Decimal.ROUND_HALF_UP,",
          "178:     }",
          "179:   }",
          "181:   override protected def doGenCode(ctx: CodegenContext, ev: ExprCode): ExprCode = {",
          "182:     val childGen = child.genCode(ctx)",
          "183:     val nullHandling = if (nullOnOverflow) {",
          "184:       \"\"",
          "185:     } else {",
          "187:     }",
          "188:     val code = code\"\"\"",
          "189:        |${childGen.code}",
          "190:        |boolean ${ev.isNull} = ${childGen.isNull};",
          "",
          "[Removed Lines]",
          "171:       if (nullOnOverflow) null else throw QueryExecutionErrors.overflowInSumOfDecimalError",
          "177:         nullOnOverflow)",
          "186:       s\"throw QueryExecutionErrors.overflowInSumOfDecimalError();\"",
          "",
          "[Added Lines]",
          "179:       if (nullOnOverflow) null",
          "180:       else throw QueryExecutionErrors.overflowInSumOfDecimalError(origin.context)",
          "186:         nullOnOverflow,",
          "187:         origin.context)",
          "193:     val errorContextCode = if (nullOnOverflow) {",
          "194:       ctx.addReferenceObj(\"errCtx\", origin.context)",
          "195:     } else {",
          "196:       \"\\\"\\\"\"",
          "197:     }",
          "201:       s\"throw QueryExecutionErrors.overflowInSumOfDecimalError($errorContextCode);\"",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "193:        |  $nullHandling",
          "194:        |} else {",
          "195:        |  ${ev.value} = ${childGen.value}.toPrecision(",
          "197:        |  ${ev.isNull} = ${ev.value} == null;",
          "198:        |}",
          "199:        |\"\"\".stripMargin",
          "201:     ev.copy(code = code)",
          "202:   }",
          "",
          "[Removed Lines]",
          "196:        |    ${dataType.precision}, ${dataType.scale}, Decimal.ROUND_HALF_UP(), $nullOnOverflow);",
          "",
          "[Added Lines]",
          "212:        |    ${dataType.precision}, ${dataType.scale}, Decimal.ROUND_HALF_UP(), $nullOnOverflow, $errorContextCode);",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/mathExpressions.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/mathExpressions.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/mathExpressions.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/mathExpressions.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "1530:         if (_scale >= 0) {",
          "1531:           s\"\"\"",
          "1532:             ${ev.value} = ${ce.value}.toPrecision(${ce.value}.precision(), $s,",
          "1534:             ${ev.isNull} = ${ev.value} == null;\"\"\"",
          "1535:        } else {",
          "1536:           s\"\"\"",
          "",
          "[Removed Lines]",
          "1533:             Decimal.$modeStr(), true);",
          "",
          "[Added Lines]",
          "1533:             Decimal.$modeStr(), true, \"\");",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "95:   }",
          "97:   def cannotChangeDecimalPrecisionError(",
          "99:     new SparkArithmeticException(errorClass = \"CANNOT_CHANGE_DECIMAL_PRECISION\",",
          "100:       messageParameters = Array(value.toDebugString,",
          "102:   }",
          "104:   def invalidInputSyntaxForNumericError(e: NumberFormatException): NumberFormatException = {",
          "",
          "[Removed Lines]",
          "98:       value: Decimal, decimalPrecision: Int, decimalScale: Int): ArithmeticException = {",
          "101:         decimalPrecision.toString, decimalScale.toString, SQLConf.ANSI_ENABLED.key))",
          "",
          "[Added Lines]",
          "98:       value: Decimal,",
          "99:       decimalPrecision: Int,",
          "100:       decimalScale: Int,",
          "101:       context: String): ArithmeticException = {",
          "104:         decimalPrecision.toString, decimalScale.toString, SQLConf.ANSI_ENABLED.key, context))",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "215:     ansiIllegalArgumentError(e.getMessage)",
          "216:   }",
          "220:   }",
          "222:   def overflowInIntegralDivideError(context: String): ArithmeticException = {",
          "",
          "[Removed Lines]",
          "218:   def overflowInSumOfDecimalError(): ArithmeticException = {",
          "219:     arithmeticOverflowError(\"Overflow in sum of decimals\")",
          "",
          "[Added Lines]",
          "221:   def overflowInSumOfDecimalError(context: String): ArithmeticException = {",
          "222:     arithmeticOverflowError(\"Overflow in sum of decimals\", errorContext = context)",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/types/Decimal.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/types/Decimal.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/types/Decimal.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/types/Decimal.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "356:       precision: Int,",
          "357:       scale: Int,",
          "358:       roundMode: BigDecimal.RoundingMode.Value = ROUND_HALF_UP,",
          "360:     val copy = clone()",
          "361:     if (copy.changePrecision(precision, scale, roundMode)) {",
          "362:       copy",
          "",
          "[Removed Lines]",
          "359:       nullOnOverflow: Boolean = true): Decimal = {",
          "",
          "[Added Lines]",
          "359:       nullOnOverflow: Boolean = true,",
          "360:       context: String = \"\"): Decimal = {",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "364:       if (nullOnOverflow) {",
          "365:         null",
          "366:       } else {",
          "368:       }",
          "369:     }",
          "370:   }",
          "",
          "[Removed Lines]",
          "367:         throw QueryExecutionErrors.cannotChangeDecimalPrecisionError(this, precision, scale)",
          "",
          "[Added Lines]",
          "368:         throw QueryExecutionErrors.cannotChangeDecimalPrecisionError(",
          "369:           this, precision, scale, context)",
          "",
          "---------------"
        ],
        "sql/core/src/test/resources/sql-tests/inputs/ansi/cast.sql||sql/core/src/test/resources/sql-tests/inputs/ansi/cast.sql": [
          "File: sql/core/src/test/resources/sql-tests/inputs/ansi/cast.sql -> sql/core/src/test/resources/sql-tests/inputs/ansi/cast.sql",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: --IMPORT cast.sql",
          "",
          "---------------"
        ],
        "sql/core/src/test/resources/sql-tests/inputs/cast.sql||sql/core/src/test/resources/sql-tests/inputs/cast.sql": [
          "File: sql/core/src/test/resources/sql-tests/inputs/cast.sql -> sql/core/src/test/resources/sql-tests/inputs/cast.sql",
          "--- Hunk 1 ---",
          "[Context before]",
          "84: select cast('\\t\\t true \\n\\r ' as boolean);",
          "85: select cast('\\t\\n false \\t\\r' as boolean);",
          "86: select cast('\\t\\n xyz \\t\\r' as boolean);",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "88: select cast('23.45' as decimal(4, 2));",
          "89: select cast('123.45' as decimal(4, 2));",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "2e102b8bd233441bb2dd74e1870de5b8218d5331",
      "candidate_info": {
        "commit_hash": "2e102b8bd233441bb2dd74e1870de5b8218d5331",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/2e102b8bd233441bb2dd74e1870de5b8218d5331",
        "files": [
          "python/pyspark/sql/tests/test_udf.py",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryErrorsBase.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryParsingErrors.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ExtractPythonUDFFromJoinConditionSuite.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/DDLParserSuite.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/PlanParserSuite.scala",
          "sql/core/src/test/resources/sql-tests/results/join-lateral.sql.out",
          "sql/core/src/test/resources/sql-tests/results/transform.sql.out",
          "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryCompilationErrorsDSv2Suite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryCompilationErrorsSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionErrorsSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryParsingErrorsSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/SparkScriptTransformationSuite.scala"
        ],
        "message": "[SPARK-38949][SQL][3.3] Wrap SQL statements by double quotes in error messages\n\n### What changes were proposed in this pull request?\nIn the PR, I propose to wrap any SQL statement in error messages by double quotes \"\", and apply new implementation of `QueryErrorsBase.toSQLStmt()` to all exceptions in `Query.*Errors` w/ error classes. Also this PR modifies all affected tests, see the list in the section \"How was this patch tested?\".\n\n### Why are the changes needed?\nTo improve user experience with Spark SQL by highlighting SQL statements in error massage and make them more visible to users.\n\n### Does this PR introduce _any_ user-facing change?\nYes. The changes might influence on error messages that are visible to users.\n\nBefore:\n```sql\nThe operation DESC PARTITION is not allowed\n```\n\nAfter:\n```sql\nThe operation \"DESC PARTITION\" is not allowed\n```\n\n### How was this patch tested?\nBy running affected test suites:\n```\n$ build/sbt \"sql/testOnly *QueryExecutionErrorsSuite\"\n$ build/sbt \"sql/testOnly *QueryParsingErrorsSuite\"\n$ build/sbt \"sql/testOnly *QueryCompilationErrorsSuite\"\n$ build/sbt \"test:testOnly *QueryCompilationErrorsDSv2Suite\"\n$ build/sbt \"test:testOnly *ExtractPythonUDFFromJoinConditionSuite\"\n$ build/sbt \"testOnly *PlanParserSuite\"\n$ build/sbt \"sql/testOnly *SQLQueryTestSuite -- -z transform.sql\"\n$ build/sbt \"sql/testOnly *SQLQueryTestSuite -- -z join-lateral.sql\"\n$ build/sbt \"sql/testOnly *SQLQueryTestSuite -- -z describe.sql\"\n```\n\nAuthored-by: Max Gekk <max.gekkgmail.com>\nSigned-off-by: Max Gekk <max.gekkgmail.com>\n(cherry picked from commit 5aba2b38beae6e1baf6f0c6f9eb3b65cf607fe77)\nSigned-off-by: Max Gekk <max.gekkgmail.com>\n\nCloses #36286 from MaxGekk/error-class-apply-toSQLStmt-3.3.\n\nAuthored-by: Max Gekk <max.gekk@gmail.com>\nSigned-off-by: Max Gekk <max.gekk@gmail.com>",
        "before_after_code_files": [
          "python/pyspark/sql/tests/test_udf.py||python/pyspark/sql/tests/test_udf.py",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryErrorsBase.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryErrorsBase.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryParsingErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryParsingErrors.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ExtractPythonUDFFromJoinConditionSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ExtractPythonUDFFromJoinConditionSuite.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/DDLParserSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/DDLParserSuite.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/PlanParserSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/PlanParserSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryCompilationErrorsDSv2Suite.scala||sql/core/src/test/scala/org/apache/spark/sql/errors/QueryCompilationErrorsDSv2Suite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryCompilationErrorsSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/errors/QueryCompilationErrorsSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionErrorsSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionErrorsSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryParsingErrorsSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/errors/QueryParsingErrorsSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/SparkScriptTransformationSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/SparkScriptTransformationSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "python/pyspark/sql/tests/test_udf.py||python/pyspark/sql/tests/test_udf.py": [
          "File: python/pyspark/sql/tests/test_udf.py -> python/pyspark/sql/tests/test_udf.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "258:         def runWithJoinType(join_type, type_string):",
          "259:             with self.assertRaisesRegex(",
          "260:                 AnalysisException,",
          "262:             ):",
          "263:                 left.join(right, [f(\"a\", \"b\"), left.a1 == right.b1], join_type).collect()",
          "271:     def test_udf_as_join_condition(self):",
          "272:         left = self.spark.createDataFrame([Row(a=1, a1=1, a2=1), Row(a=2, a1=2, a2=2)])",
          "",
          "[Removed Lines]",
          "261:                 \"Using PythonUDF in join condition of join type %s is not supported\" % type_string,",
          "265:         runWithJoinType(\"full\", \"FullOuter\")",
          "266:         runWithJoinType(\"left\", \"LeftOuter\")",
          "267:         runWithJoinType(\"right\", \"RightOuter\")",
          "268:         runWithJoinType(\"leftanti\", \"LeftAnti\")",
          "269:         runWithJoinType(\"leftsemi\", \"LeftSemi\")",
          "",
          "[Added Lines]",
          "261:                 \"\"\"Using PythonUDF in join condition of join type \"%s\" is not supported\"\"\"",
          "262:                 % type_string,",
          "266:         runWithJoinType(\"full\", \"FULL OUTER\")",
          "267:         runWithJoinType(\"left\", \"LEFT OUTER\")",
          "268:         runWithJoinType(\"right\", \"RIGHT OUTER\")",
          "269:         runWithJoinType(\"leftanti\", \"LEFT ANTI\")",
          "270:         runWithJoinType(\"leftsemi\", \"LEFT SEMI\")",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "1161:         }",
          "1162:         if (join.LATERAL != null) {",
          "1163:           if (!Seq(Inner, Cross, LeftOuter).contains(joinType)) {",
          "1165:           }",
          "1166:           LateralJoin(left, LateralSubquery(plan(join.right)), joinType, condition)",
          "1167:         } else {",
          "",
          "[Removed Lines]",
          "1164:             throw QueryParsingErrors.unsupportedLateralJoinTypeError(ctx, joinType.toString)",
          "",
          "[Added Lines]",
          "1164:             throw QueryParsingErrors.unsupportedLateralJoinTypeError(ctx, joinType.sql)",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "94:   def unsupportedIfNotExistsError(tableName: String): Throwable = {",
          "95:     new AnalysisException(",
          "96:       errorClass = \"UNSUPPORTED_FEATURE\",",
          "98:   }",
          "100:   def nonPartitionColError(partitionName: String): Throwable = {",
          "",
          "[Removed Lines]",
          "97:       messageParameters = Array(s\"IF NOT EXISTS for the table '$tableName' by INSERT INTO.\"))",
          "",
          "[Added Lines]",
          "97:       messageParameters = Array(",
          "98:         s\"${toSQLStmt(\"IF NOT EXISTS\")} for the table '$tableName' \" +",
          "99:         s\"by ${toSQLStmt(\"INSERT INTO\")}.\"))",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1576:     new AnalysisException(",
          "1577:       errorClass = \"UNSUPPORTED_FEATURE\",",
          "1578:       messageParameters = Array(",
          "1580:   }",
          "1582:   def conflictingAttributesInJoinConditionError(",
          "",
          "[Removed Lines]",
          "1579:         s\"Using PythonUDF in join condition of join type $joinType is not supported\"))",
          "",
          "[Added Lines]",
          "1581:         \"Using PythonUDF in join condition of join type \" +",
          "1582:         s\"${toSQLStmt(joinType.sql)} is not supported.\"))",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryErrorsBase.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryErrorsBase.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryErrorsBase.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryErrorsBase.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: package org.apache.spark.sql.errors",
          "20: import org.apache.spark.sql.catalyst.expressions.Literal",
          "21: import org.apache.spark.sql.types.{DataType, DoubleType, FloatType}",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "20: import java.util.Locale",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "45:     litToErrorValue(Literal.create(v, t))",
          "46:   }",
          "48:   def toSQLType(t: DataType): String = {",
          "49:     t.sql",
          "50:   }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "51:   def toSQLStmt(text: String): String = {",
          "52:     \"\\\"\" + text.toUpperCase(Locale.ROOT) + \"\\\"\"",
          "53:   }",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "1927:   def repeatedPivotsUnsupportedError(): Throwable = {",
          "1928:     new SparkUnsupportedOperationException(",
          "1929:       errorClass = \"UNSUPPORTED_FEATURE\",",
          "1931:   }",
          "1933:   def pivotNotAfterGroupByUnsupportedError(): Throwable = {",
          "1934:     new SparkUnsupportedOperationException(",
          "1935:       errorClass = \"UNSUPPORTED_FEATURE\",",
          "1937:   }",
          "1939:   def invalidAesKeyLengthError(actualLength: Int): RuntimeException = {",
          "",
          "[Removed Lines]",
          "1930:       messageParameters = Array(\"Repeated pivots.\"))",
          "1936:       messageParameters = Array(\"Pivot not after a groupBy.\"))",
          "",
          "[Added Lines]",
          "1930:       messageParameters = Array(s\"Repeated ${toSQLStmt(\"pivot\")}s.\"))",
          "1936:       messageParameters = Array(s\"${toSQLStmt(\"pivot\")} not after a ${toSQLStmt(\"group by\")}.\"))",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryParsingErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryParsingErrors.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryParsingErrors.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryParsingErrors.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "91:   }",
          "93:   def transformNotSupportQuantifierError(ctx: ParserRuleContext): Throwable = {",
          "96:   }",
          "98:   def transformWithSerdeUnsupportedError(ctx: ParserRuleContext): Throwable = {",
          "101:   }",
          "103:   def lateralWithPivotInFromClauseNotAllowedError(ctx: FromClauseContext): Throwable = {",
          "",
          "[Removed Lines]",
          "94:     new ParseException(\"UNSUPPORTED_FEATURE\",",
          "95:       Array(\"TRANSFORM does not support DISTINCT/ALL in inputs\"), ctx)",
          "99:     new ParseException(\"UNSUPPORTED_FEATURE\",",
          "100:       Array(\"TRANSFORM with serde is only supported in hive mode\"), ctx)",
          "",
          "[Added Lines]",
          "94:     new ParseException(",
          "95:       errorClass = \"UNSUPPORTED_FEATURE\",",
          "96:       messageParameters = Array(s\"${toSQLStmt(\"TRANSFORM\")} does not support\" +",
          "97:         s\" ${toSQLStmt(\"DISTINCT\")}/${toSQLStmt(\"ALL\")} in inputs\"),",
          "98:       ctx)",
          "102:     new ParseException(",
          "103:       errorClass = \"UNSUPPORTED_FEATURE\",",
          "104:       messageParameters = Array(",
          "105:         s\"${toSQLStmt(\"TRANSFORM\")} with serde is only supported in hive mode\"),",
          "106:       ctx)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "105:   }",
          "107:   def lateralJoinWithNaturalJoinUnsupportedError(ctx: ParserRuleContext): Throwable = {",
          "109:   }",
          "111:   def lateralJoinWithUsingJoinUnsupportedError(ctx: ParserRuleContext): Throwable = {",
          "113:   }",
          "115:   def unsupportedLateralJoinTypeError(ctx: ParserRuleContext, joinType: String): Throwable = {",
          "117:   }",
          "119:   def invalidLateralJoinRelationError(ctx: RelationPrimaryContext): Throwable = {",
          "121:   }",
          "123:   def repetitiveWindowDefinitionError(name: String, ctx: WindowClauseContext): Throwable = {",
          "",
          "[Removed Lines]",
          "108:     new ParseException(\"UNSUPPORTED_FEATURE\", Array(\"LATERAL join with NATURAL join.\"), ctx)",
          "112:     new ParseException(\"UNSUPPORTED_FEATURE\", Array(\"LATERAL join with USING join.\"), ctx)",
          "116:     new ParseException(\"UNSUPPORTED_FEATURE\", Array(s\"LATERAL join type '$joinType'.\"), ctx)",
          "120:     new ParseException(\"INVALID_SQL_SYNTAX\", Array(\"LATERAL can only be used with subquery.\"), ctx)",
          "",
          "[Added Lines]",
          "114:     new ParseException(",
          "115:       errorClass = \"UNSUPPORTED_FEATURE\",",
          "116:       messageParameters = Array(s\"${toSQLStmt(\"LATERAL\")} join with ${toSQLStmt(\"NATURAL\")} join.\"),",
          "117:       ctx)",
          "121:     new ParseException(",
          "122:       errorClass = \"UNSUPPORTED_FEATURE\",",
          "123:       messageParameters = Array(s\"${toSQLStmt(\"LATERAL\")} join with ${toSQLStmt(\"USING\")} join.\"),",
          "124:       ctx)",
          "128:     new ParseException(",
          "129:       errorClass = \"UNSUPPORTED_FEATURE\",",
          "130:       messageParameters = Array(s\"${toSQLStmt(\"LATERAL\")} join type ${toSQLStmt(joinType)}.\"),",
          "131:       ctx)",
          "135:     new ParseException(",
          "136:       errorClass = \"INVALID_SQL_SYNTAX\",",
          "137:       messageParameters = Array(s\"${toSQLStmt(\"LATERAL\")} can only be used with subquery.\"),",
          "138:       ctx)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "136:   }",
          "138:   def naturalCrossJoinUnsupportedError(ctx: RelationContext): Throwable = {",
          "140:   }",
          "142:   def emptyInputForTableSampleError(ctx: ParserRuleContext): Throwable = {",
          "",
          "[Removed Lines]",
          "139:     new ParseException(\"UNSUPPORTED_FEATURE\", Array(\"NATURAL CROSS JOIN.\"), ctx)",
          "",
          "[Added Lines]",
          "157:     new ParseException(\"UNSUPPORTED_FEATURE\", Array(toSQLStmt(\"NATURAL CROSS JOIN\") + \".\"), ctx)",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "298:   }",
          "300:   def showFunctionsUnsupportedError(identifier: String, ctx: IdentifierContext): Throwable = {",
          "302:   }",
          "304:   def showFunctionsInvalidPatternError(pattern: String, ctx: ParserRuleContext): Throwable = {",
          "305:     new ParseException(",
          "306:       errorClass = \"INVALID_SQL_SYNTAX\",",
          "307:       messageParameters = Array(",
          "309:         s\"It must be a ${toSQLType(StringType)} literal.\"),",
          "310:       ctx)",
          "311:   }",
          "",
          "[Removed Lines]",
          "301:     new ParseException(s\"SHOW $identifier FUNCTIONS not supported\", ctx)",
          "308:         s\"Invalid pattern in SHOW FUNCTIONS: $pattern. \" +",
          "",
          "[Added Lines]",
          "319:     new ParseException(",
          "320:       errorClass = \"INVALID_SQL_SYNTAX\",",
          "321:       messageParameters = Array(",
          "322:         s\"${toSQLStmt(\"SHOW\")} $identifier ${toSQLStmt(\"FUNCTIONS\")} not supported\"),",
          "323:       ctx)",
          "330:         s\"Invalid pattern in ${toSQLStmt(\"SHOW FUNCTIONS\")}: $pattern. \" +",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ExtractPythonUDFFromJoinConditionSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ExtractPythonUDFFromJoinConditionSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ExtractPythonUDFFromJoinConditionSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ExtractPythonUDFFromJoinConditionSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "187:           condition = Some(unevaluableJoinCond))",
          "188:         Optimize.execute(query.analyze)",
          "189:       }",
          "191:         \"The feature is not supported: \" +",
          "194:       val query2 = testRelationLeft.join(",
          "195:         testRelationRight,",
          "",
          "[Removed Lines]",
          "190:       assert(e.message.contentEquals(",
          "192:         s\"Using PythonUDF in join condition of join type $joinType is not supported\"))",
          "",
          "[Added Lines]",
          "190:       assert(e.message ==",
          "192:         s\"\"\"Using PythonUDF in join condition of join type \"${joinType.sql}\" is not supported.\"\"\")",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/DDLParserSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/DDLParserSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/DDLParserSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/DDLParserSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "2048:     comparePlans(",
          "2049:       parsePlan(\"SHOW FUNCTIONS IN db LIKE 'funct*'\"),",
          "2050:       ShowFunctions(UnresolvedNamespace(Seq(\"db\")), true, true, Some(\"funct*\")))",
          "2053:     intercept(\"SHOW FUNCTIONS IN db f1\",",
          "2055:     intercept(\"SHOW FUNCTIONS IN db LIKE f1\",",
          "2059:     comparePlans(",
          "",
          "[Removed Lines]",
          "2051:     val sql = \"SHOW other FUNCTIONS\"",
          "2052:     intercept(sql, s\"$sql not supported\")",
          "2054:       \"Invalid pattern in SHOW FUNCTIONS: f1\")",
          "2056:       \"Invalid pattern in SHOW FUNCTIONS: f1\")",
          "",
          "[Added Lines]",
          "2051:     intercept(\"SHOW other FUNCTIONS\", \"\\\"SHOW\\\" other \\\"FUNCTIONS\\\" not supported\")",
          "2053:       \"Invalid pattern in \\\"SHOW FUNCTIONS\\\": f1\")",
          "2055:       \"Invalid pattern in \\\"SHOW FUNCTIONS\\\": f1\")",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/PlanParserSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/PlanParserSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/PlanParserSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/PlanParserSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "1254:         |    \"escapeChar\" = \"\\\\\")",
          "1255:         |FROM testData",
          "1256:       \"\"\".stripMargin,",
          "1258:   }",
          "",
          "[Removed Lines]",
          "1257:       \"TRANSFORM with serde is only supported in hive mode\")",
          "",
          "[Added Lines]",
          "1257:       \"\\\"TRANSFORM\\\" with serde is only supported in hive mode\")",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryCompilationErrorsDSv2Suite.scala||sql/core/src/test/scala/org/apache/spark/sql/errors/QueryCompilationErrorsDSv2Suite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/errors/QueryCompilationErrorsDSv2Suite.scala -> sql/core/src/test/scala/org/apache/spark/sql/errors/QueryCompilationErrorsDSv2Suite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "44:         checkAnswer(spark.table(tbl), spark.emptyDataFrame)",
          "45:         assert(e.getMessage === \"The feature is not supported: \" +",
          "47:         assert(e.getErrorClass === \"UNSUPPORTED_FEATURE\")",
          "48:         assert(e.getSqlState === \"0A000\")",
          "49:       }",
          "",
          "[Removed Lines]",
          "46:           s\"IF NOT EXISTS for the table '$tbl' by INSERT INTO.\")",
          "",
          "[Added Lines]",
          "46:           s\"\"\"\"IF NOT EXISTS\" for the table '$tbl' by \"INSERT INTO\".\"\"\")",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryCompilationErrorsSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/errors/QueryCompilationErrorsSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/errors/QueryCompilationErrorsSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/errors/QueryCompilationErrorsSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "149:     assert(e.getSqlState === \"0A000\")",
          "150:     assert(e.message ===",
          "151:       \"The feature is not supported: \" +",
          "153:   }",
          "155:   test(\"UNSUPPORTED_FEATURE: Using pandas UDF aggregate expression with pivot\") {",
          "",
          "[Removed Lines]",
          "152:       \"Using PythonUDF in join condition of join type LeftOuter is not supported\")",
          "",
          "[Added Lines]",
          "152:       \"Using PythonUDF in join condition of join type \\\"LEFT OUTER\\\" is not supported.\")",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionErrorsSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionErrorsSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionErrorsSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionErrorsSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "156:     }",
          "157:     assert(e1.getErrorClass === \"UNSUPPORTED_FEATURE\")",
          "158:     assert(e1.getSqlState === \"0A000\")",
          "161:     val e2 = intercept[SparkUnsupportedOperationException] {",
          "162:       trainingSales",
          "",
          "[Removed Lines]",
          "159:     assert(e1.getMessage === \"The feature is not supported: Repeated pivots.\")",
          "",
          "[Added Lines]",
          "159:     assert(e1.getMessage === \"\"\"The feature is not supported: Repeated \"PIVOT\"s.\"\"\")",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "167:     }",
          "168:     assert(e2.getErrorClass === \"UNSUPPORTED_FEATURE\")",
          "169:     assert(e2.getSqlState === \"0A000\")",
          "171:   }",
          "173:   test(\"INCONSISTENT_BEHAVIOR_CROSS_VERSION: \" +",
          "",
          "[Removed Lines]",
          "170:     assert(e2.getMessage === \"The feature is not supported: Pivot not after a groupBy.\")",
          "",
          "[Added Lines]",
          "170:     assert(e2.getMessage === \"\"\"The feature is not supported: \"PIVOT\" not after a \"GROUP BY\".\"\"\")",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryParsingErrorsSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/errors/QueryParsingErrorsSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/errors/QueryParsingErrorsSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/errors/QueryParsingErrorsSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "42:       sqlState = \"0A000\",",
          "43:       message =",
          "44:         \"\"\"",
          "46:           |",
          "47:           |== SQL ==",
          "48:           |SELECT * FROM t1 NATURAL JOIN LATERAL (SELECT c1 + c2 AS c2)",
          "",
          "[Removed Lines]",
          "45:           |The feature is not supported: LATERAL join with NATURAL join.(line 1, pos 14)",
          "",
          "[Added Lines]",
          "47:           |The feature is not supported: \"LATERAL\" join with \"NATURAL\" join.(line 1, pos 14)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "57:       sqlState = \"0A000\",",
          "58:       message =",
          "59:         \"\"\"",
          "61:           |",
          "62:           |== SQL ==",
          "63:           |SELECT * FROM t1 JOIN LATERAL (SELECT c1 + c2 AS c2) USING (c2)",
          "",
          "[Removed Lines]",
          "60:           |The feature is not supported: LATERAL join with USING join.(line 1, pos 14)",
          "",
          "[Added Lines]",
          "62:           |The feature is not supported: \"LATERAL\" join with \"USING\" join.(line 1, pos 14)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "66:   }",
          "68:   test(\"UNSUPPORTED_FEATURE: Unsupported LATERAL join type\") {",
          "74:       validateParsingError(",
          "76:         errorClass = \"UNSUPPORTED_FEATURE\",",
          "77:         sqlState = \"0A000\",",
          "78:         message =",
          "79:           s\"\"\"",
          "81:             |",
          "82:             |== SQL ==",
          "84:             |--------------^^^",
          "85:             |\"\"\".stripMargin)",
          "86:     }",
          "",
          "[Removed Lines]",
          "69:     Seq(",
          "70:       (\"RIGHT OUTER\", \"RightOuter\"),",
          "71:       (\"FULL OUTER\", \"FullOuter\"),",
          "72:       (\"LEFT SEMI\", \"LeftSemi\"),",
          "73:       (\"LEFT ANTI\", \"LeftAnti\")).foreach { pair =>",
          "75:         sqlText = s\"SELECT * FROM t1 ${pair._1} JOIN LATERAL (SELECT c1 + c2 AS c3) ON c2 = c3\",",
          "80:             |The feature is not supported: LATERAL join type '${pair._2}'.(line 1, pos 14)",
          "83:             |SELECT * FROM t1 ${pair._1} JOIN LATERAL (SELECT c1 + c2 AS c3) ON c2 = c3",
          "",
          "[Added Lines]",
          "71:     Seq(\"RIGHT OUTER\", \"FULL OUTER\", \"LEFT SEMI\", \"LEFT ANTI\").foreach { joinType =>",
          "73:         sqlText = s\"SELECT * FROM t1 $joinType JOIN LATERAL (SELECT c1 + c2 AS c3) ON c2 = c3\",",
          "78:             |The feature is not supported: \"LATERAL\" join type \"$joinType\".(line 1, pos 14)",
          "81:             |SELECT * FROM t1 $joinType JOIN LATERAL (SELECT c1 + c2 AS c3) ON c2 = c3",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "101:         sqlState = \"42000\",",
          "102:         message =",
          "103:           s\"\"\"",
          "105:             |",
          "106:             |== SQL ==",
          "107:             |$sqlText",
          "",
          "[Removed Lines]",
          "104:             |Invalid SQL syntax: LATERAL can only be used with subquery.(line 1, pos $pos)",
          "",
          "[Added Lines]",
          "102:             |Invalid SQL syntax: \"LATERAL\" can only be used with subquery.(line 1, pos $pos)",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "117:       sqlState = \"0A000\",",
          "118:       message =",
          "119:         \"\"\"",
          "121:           |",
          "122:           |== SQL ==",
          "123:           |SELECT * FROM a NATURAL CROSS JOIN b",
          "",
          "[Removed Lines]",
          "120:           |The feature is not supported: NATURAL CROSS JOIN.(line 1, pos 14)",
          "",
          "[Added Lines]",
          "118:           |The feature is not supported: \"NATURAL CROSS JOIN\".(line 1, pos 14)",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "177:       sqlState = \"0A000\",",
          "178:       message =",
          "179:         \"\"\"",
          "182:           |",
          "183:           |== SQL ==",
          "184:           |SELECT TRANSFORM(DISTINCT a) USING 'a' FROM t",
          "",
          "[Removed Lines]",
          "180:           |The feature is not supported: \"\"\".stripMargin +",
          "181:         \"\"\"TRANSFORM does not support DISTINCT/ALL in inputs(line 1, pos 17)",
          "",
          "[Added Lines]",
          "178:           |The feature is not supported: \"TRANSFORM\" does not support \"DISTINCT\"/\"ALL\" in inputs(line 1, pos 17)",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "194:       sqlState = \"0A000\",",
          "195:       message =",
          "196:         \"\"\"",
          "199:           |",
          "200:           |== SQL ==",
          "203:           |^^^",
          "204:           |\"\"\".stripMargin)",
          "205:   }",
          "",
          "[Removed Lines]",
          "197:           |The feature is not supported: \"\"\".stripMargin +",
          "198:         \"\"\"TRANSFORM with serde is only supported in hive mode(line 1, pos 0)",
          "201:           |SELECT TRANSFORM(a) ROW FORMAT SERDE \"\"\".stripMargin +",
          "202:         \"\"\"'org.apache.hadoop.hive.serde2.OpenCSVSerde' USING 'a' FROM t",
          "",
          "[Added Lines]",
          "194:           |The feature is not supported: \"TRANSFORM\" with serde is only supported in hive mode(line 1, pos 0)",
          "197:           |SELECT TRANSFORM(a) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde' USING 'a' FROM t",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/execution/SparkScriptTransformationSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/SparkScriptTransformationSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/execution/SparkScriptTransformationSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/execution/SparkScriptTransformationSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "56:             |FROM v",
          "57:           \"\"\".stripMargin)",
          "58:       }.getMessage",
          "60:     }",
          "61:   }",
          "62: }",
          "",
          "[Removed Lines]",
          "59:       assert(e.contains(\"TRANSFORM with serde is only supported in hive mode\"))",
          "",
          "[Added Lines]",
          "59:       assert(e.contains(\"\\\"TRANSFORM\\\" with serde is only supported in hive mode\"))",
          "",
          "---------------"
        ]
      }
    }
  ]
}