{
  "cve_id": "CVE-2021-45456",
  "cve_desc": "Apache kylin checks the legitimacy of the project before executing some commands with the project name passed in by the user. There is a mismatch between what is being checked and what is being used as the shell command argument in DiagnosisService. This may cause an illegal project name to pass the check and perform the following steps, resulting in a command injection vulnerability. This issue affects Apache Kylin 4.0.0.",
  "repo": "apache/kylin",
  "patch_hash": "f4daf14dde99b934c92ce2c832509f24342bc845",
  "patch_info": {
    "commit_hash": "f4daf14dde99b934c92ce2c832509f24342bc845",
    "repo": "apache/kylin",
    "commit_url": "https://github.com/apache/kylin/commit/f4daf14dde99b934c92ce2c832509f24342bc845",
    "files": [
      "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java",
      "core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java",
      "core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java",
      "server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java",
      "server/src/main/webapp/WEB-INF/web.xml"
    ],
    "message": "test fix",
    "before_after_code_files": [
      "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java||core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java",
      "core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java||core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java",
      "core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java||core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java",
      "server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java||server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java"
    ]
  },
  "patch_diff": {
    "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java||core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java": [
      "File: core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java -> core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java",
      "--- Hunk 1 ---",
      "[Context before]",
      "3403:     public String getKerberosPrincipal() {",
      "3404:         return getOptional(\"kylin.kerberos.principal\");",
      "3405:     }",
      "3406: }",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "3407:     public String getEncryptCipherIvSpec() {",
      "3408:         return getOptional(\"kylin.security.encrypt.cipher.ivSpec\", \"AAAAAAAAAAAAAAAA\");",
      "3409:     }",
      "",
      "---------------"
    ],
    "core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java||core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java": [
      "File: core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java -> core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java",
      "--- Hunk 1 ---",
      "[Context before]",
      "25: import java.security.NoSuchAlgorithmException;",
      "27: import org.apache.commons.codec.binary.Base64;",
      "29: import javax.crypto.Cipher;",
      "30: import javax.crypto.NoSuchPaddingException;",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "28: import org.apache.kylin.common.KylinConfig;",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "42:             InvalidKeyException, NoSuchPaddingException, NoSuchAlgorithmException, UnsupportedEncodingException {",
      "43:         Cipher cipher = Cipher.getInstance(\"AES/CFB/PKCS5Padding\");",
      "44:         final SecretKeySpec secretKey = new SecretKeySpec(key, \"AES\");",
      "46:         cipher.init(cipherMode, secretKey, ivSpec);",
      "47:         return cipher;",
      "48:     }",
      "",
      "[Removed Lines]",
      "45:         IvParameterSpec ivSpec = new IvParameterSpec(\"AAAAAAAAAAAAAAAA\".getBytes(\"UTF-8\"));",
      "",
      "[Added Lines]",
      "46:         IvParameterSpec ivSpec = new IvParameterSpec(KylinConfig.getInstanceFromEnv().getEncryptCipherIvSpec().getBytes(\"UTF-8\"));",
      "",
      "---------------"
    ],
    "core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java||core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java": [
      "File: core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java -> core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java",
      "--- Hunk 1 ---",
      "[Context before]",
      "19: package org.apache.kylin.common.util;",
      "21: import org.junit.Assert;",
      "22: import org.junit.Test;",
      "26:     @Test",
      "27:     public void testAESEncrypt(){",
      "",
      "[Removed Lines]",
      "24: public class EncryptUtilTest {",
      "",
      "[Added Lines]",
      "21: import org.junit.After;",
      "23: import org.junit.Before;",
      "26: public class EncryptUtilTest extends LocalFileMetadataTestCase {",
      "27:     @Before",
      "28:     public void setUp() throws Exception {",
      "29:         this.createTestMetadata();",
      "30:     }",
      "32:     @After",
      "33:     public void after() throws Exception {",
      "34:         this.cleanupTestMetadata();",
      "35:     }",
      "",
      "---------------"
    ],
    "server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java||server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java": [
      "File: server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java -> server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java",
      "--- Hunk 1 ---",
      "[Context before]",
      "87:     public String dumpProjectDiagnosisInfo(String project, File exportPath) throws IOException {",
      "88:         Message msg = MsgPicker.getMsg();",
      "89:         ProjectInstance projectInstance =",
      "90:                 ProjectManager.getInstance(KylinConfig.getInstanceFromEnv())",
      "92:         if (null == projectInstance) {",
      "93:             throw new BadRequestException(",
      "95:         }",
      "96:         aclEvaluate.checkProjectOperationPermission(projectInstance);",
      "98:         runDiagnosisCLI(args);",
      "99:         return getDiagnosisPackageName(exportPath);",
      "100:     }",
      "",
      "[Removed Lines]",
      "91:                         .getProject(ValidateUtil.convertStringToBeAlphanumericUnderscore(project));",
      "94:                     String.format(Locale.ROOT, msg.getDIAG_PROJECT_NOT_FOUND(), project));",
      "97:         String[] args = { project, exportPath.getAbsolutePath() };",
      "",
      "[Added Lines]",
      "89:         String projectName = ValidateUtil.convertStringToBeAlphanumericUnderscore(project);",
      "92:                         .getProject(projectName);",
      "95:                     String.format(Locale.ROOT, msg.getDIAG_PROJECT_NOT_FOUND(), projectName));",
      "98:         String[] args = { projectName, exportPath.getAbsolutePath() };",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "1e43119b58ab4cdad4484ae58a795fc27b68ec8e",
      "candidate_info": {
        "commit_hash": "1e43119b58ab4cdad4484ae58a795fc27b68ec8e",
        "repo": "apache/kylin",
        "commit_url": "https://github.com/apache/kylin/commit/1e43119b58ab4cdad4484ae58a795fc27b68ec8e",
        "files": [
          "build/bin/kylin.sh"
        ],
        "message": "KYLIN-4441 Add restart command to kylin.sh\n\n(cherry picked from commit fa4f72d2b12a4e6da60674dfd706ac7ca5534657)",
        "before_after_code_files": [
          "build/bin/kylin.sh||build/bin/kylin.sh"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/kylin/pull/1893",
          "https://github.com/apache/kylin/pull/2018",
          "https://github.com/apache/kylin/pull/2125",
          "https://github.com/apache/kylin/pull/2033",
          "https://github.com/apache/kylin/pull/2112",
          "https://github.com/apache/kylin/pull/2115",
          "https://github.com/apache/kylin/pull/1865",
          "https://github.com/apache/kylin/pull/1913",
          "https://github.com/apache/kylin/pull/2135"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "build/bin/kylin.sh||build/bin/kylin.sh": [
          "File: build/bin/kylin.sh -> build/bin/kylin.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "328: # stop command",
          "329: elif [ \"$1\" == \"stop\" ]",
          "330: then",
          "331:     if [ -f \"${KYLIN_HOME}/pid\" ]",
          "332:     then",
          "333:         PID=`cat $KYLIN_HOME/pid`",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "331: function retrieveStopCommand() {",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "341:             for ((i=0; i<$LOOP_COUNTER; i++))",
          "342:             do",
          "344:                 sleep $WAIT_TIME",
          "345:                 if ps -p $PID > /dev/null ; then",
          "346:                     echo \"Stopping in progress. Will check after $WAIT_TIME secs again...\"",
          "",
          "[Removed Lines]",
          "343:                 # wait to process stopped",
          "",
          "[Added Lines]",
          "344:                 # wait to process stopped",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "362:                 fi",
          "363:             fi",
          "366:             rm -rf ${KYLIN_HOME}/pid",
          "367:             echo \"Kylin with pid ${PID} has been stopped.\"",
          "369:         else",
          "371:         fi",
          "372:     else",
          "373:         quit \"Kylin is not running\"",
          "374:     fi",
          "376: # streaming command",
          "377: elif [ \"$1\" == \"streaming\" ]",
          "378: then",
          "",
          "[Removed Lines]",
          "365:             # process is killed , remove pid file",
          "368:             exit 0",
          "370:            quit \"Kylin with pid ${PID} is not running\"",
          "",
          "[Added Lines]",
          "366:             # process is killed , remove pid file",
          "369:             return 0",
          "371:            echo \"Kylin with pid ${PID} is not running\"",
          "372:            return 1",
          "374:     else",
          "375:         return 1",
          "376:     fi",
          "377: }",
          "379: if [ \"$2\" == \"--reload-dependency\" ]",
          "380: then",
          "381:     reload_dependency=1",
          "382: fi",
          "384: # start command",
          "385: if [ \"$1\" == \"start\" ]",
          "386: then",
          "387:     retrieveStartCommand",
          "388:     ${start_command} >> ${KYLIN_HOME}/logs/kylin.out 2>&1 & echo $! > ${KYLIN_HOME}/pid &",
          "389:     rm -f $lockfile",
          "391:     echo \"\"",
          "392:     echo \"A new Kylin instance is started by $USER. To stop it, run 'kylin.sh stop'\"",
          "393:     echo \"Check the log at ${KYLIN_HOME}/logs/kylin.log\"",
          "394:     echo \"Web UI is at http://${kylin_rest_address_arr}/kylin\"",
          "395:     exit 0",
          "397: # run command",
          "398: elif [ \"$1\" == \"run\" ]",
          "399: then",
          "400:     retrieveStartCommand",
          "401:     ${start_command}",
          "402:     rm -f $lockfile",
          "404: # stop command",
          "405: elif [ \"$1\" == \"stop\" ]",
          "406: then",
          "407:     retrieveStopCommand",
          "408:     if [[ $? == 0 ]]",
          "409:     then",
          "410:         exit 0",
          "415: # restart command",
          "416: elif [ \"$1\" == \"restart\" ]",
          "417: then",
          "418:     echo \"Restarting kylin...\"",
          "419:     echo \"--> Stopping kylin first if it's running...\"",
          "420:     retrieveStopCommand",
          "421:     if [[ $? != 0 ]]",
          "422:     then",
          "423:         echo \"Kylin is not running, now start it\"",
          "424:     fi",
          "425:     echo \"--> Start kylin...\"",
          "426:     retrieveStartCommand",
          "427:     ${start_command} >> ${KYLIN_HOME}/logs/kylin.out 2>&1 & echo $! > ${KYLIN_HOME}/pid &",
          "428:     rm -f $lockfile",
          "430:     echo \"\"",
          "431:     echo \"A new Kylin instance is started by $USER. To stop it, run 'kylin.sh stop'\"",
          "432:     echo \"Check the log at ${KYLIN_HOME}/logs/kylin.log\"",
          "433:     echo \"Web UI is at http://${kylin_rest_address_arr}/kylin\"",
          "434:     exit 0",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "498: then",
          "499:     runTool \"$@\"",
          "500: else",
          "502: fi",
          "",
          "[Removed Lines]",
          "501:     quit \"Usage: 'kylin.sh [-v] start' or 'kylin.sh [-v] stop'\"",
          "",
          "[Added Lines]",
          "561:     quit \"Usage: 'kylin.sh [-v] start' or 'kylin.sh [-v] stop' or 'kylin.sh [-v] restart'\"",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "0dc4c06641966e4fb2975f1ff738de65ff5d846c",
      "candidate_info": {
        "commit_hash": "0dc4c06641966e4fb2975f1ff738de65ff5d846c",
        "repo": "apache/kylin",
        "commit_url": "https://github.com/apache/kylin/commit/0dc4c06641966e4fb2975f1ff738de65ff5d846c",
        "files": [
          "core-cube/src/main/java/org/apache/kylin/cube/model/CubeBuildTypeEnum.java",
          "core-job/src/main/java/org/apache/kylin/job/constant/ExecutableConstants.java",
          "core-metadata/src/main/java/org/apache/kylin/metadata/model/TableDesc.java",
          "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/SparkBatchCubingEngineParquet.java",
          "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/FilterRecommendCuboidJob.java",
          "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/JobStepFactory.java",
          "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/JobStepType.java",
          "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NResourceDetectStep.java",
          "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkBatchOptimizeJobCheckpointBuilder.java",
          "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkCleanupHdfsStorageStep.java",
          "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkCubingUtil.java",
          "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkLocalStep.java",
          "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkOptimizingJob.java",
          "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkOptimizingStep.java",
          "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkUpdateCubeInfoAfterOptimizeStep.java",
          "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/OptimizeBuildJob.java",
          "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/ResourceDetectBeforeOptimizingJob.java",
          "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/utils/UpdateMetadataUtil.java",
          "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/BuildJobInfos.scala",
          "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CubeBuildJob.java",
          "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CuboidStatisticsJob.scala",
          "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/LogJobInfoUtils.scala",
          "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/ParentSourceChooser.scala",
          "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/ResourceDetectBeforeCubingJob.java",
          "kylin-spark-project/kylin-spark-metadata/src/main/java/org/apache/kylin/engine/spark/metadata/cube/ManagerHub.java",
          "kylin-spark-project/kylin-spark-metadata/src/main/scala/org/apache/kylin/engine/spark/metadata/MetaData.scala",
          "kylin-spark-project/kylin-spark-metadata/src/main/scala/org/apache/kylin/engine/spark/metadata/MetadataConverter.scala",
          "kylin-spark-project/kylin-spark-test/src/test/java/org/apache/kylin/engine/spark2/NOptimizeJobTest.java",
          "server-base/src/main/java/org/apache/kylin/rest/controller/CubeController.java",
          "server-base/src/main/java/org/apache/kylin/rest/service/JobService.java"
        ],
        "message": "KYLIN-4966 Refresh the existing segment according to the new cuboid list in kylin4",
        "before_after_code_files": [
          "core-cube/src/main/java/org/apache/kylin/cube/model/CubeBuildTypeEnum.java||core-cube/src/main/java/org/apache/kylin/cube/model/CubeBuildTypeEnum.java",
          "core-job/src/main/java/org/apache/kylin/job/constant/ExecutableConstants.java||core-job/src/main/java/org/apache/kylin/job/constant/ExecutableConstants.java",
          "core-metadata/src/main/java/org/apache/kylin/metadata/model/TableDesc.java||core-metadata/src/main/java/org/apache/kylin/metadata/model/TableDesc.java",
          "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/SparkBatchCubingEngineParquet.java||kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/SparkBatchCubingEngineParquet.java",
          "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/FilterRecommendCuboidJob.java||kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/FilterRecommendCuboidJob.java",
          "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/JobStepFactory.java||kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/JobStepFactory.java",
          "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/JobStepType.java||kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/JobStepType.java",
          "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NResourceDetectStep.java||kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NResourceDetectStep.java",
          "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkBatchOptimizeJobCheckpointBuilder.java||kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkBatchOptimizeJobCheckpointBuilder.java",
          "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkCleanupHdfsStorageStep.java||kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkCleanupHdfsStorageStep.java",
          "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkCubingUtil.java||kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkCubingUtil.java",
          "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkLocalStep.java||kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkLocalStep.java",
          "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkOptimizingJob.java||kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkOptimizingJob.java",
          "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkOptimizingStep.java||kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkOptimizingStep.java",
          "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkUpdateCubeInfoAfterOptimizeStep.java||kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkUpdateCubeInfoAfterOptimizeStep.java",
          "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/OptimizeBuildJob.java||kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/OptimizeBuildJob.java",
          "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/ResourceDetectBeforeOptimizingJob.java||kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/ResourceDetectBeforeOptimizingJob.java",
          "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/utils/UpdateMetadataUtil.java||kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/utils/UpdateMetadataUtil.java",
          "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/BuildJobInfos.scala||kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/BuildJobInfos.scala",
          "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CubeBuildJob.java||kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CubeBuildJob.java",
          "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CuboidStatisticsJob.scala||kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CuboidStatisticsJob.scala",
          "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/LogJobInfoUtils.scala||kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/LogJobInfoUtils.scala",
          "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/ParentSourceChooser.scala||kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/ParentSourceChooser.scala",
          "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/ResourceDetectBeforeCubingJob.java||kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/ResourceDetectBeforeCubingJob.java",
          "kylin-spark-project/kylin-spark-metadata/src/main/java/org/apache/kylin/engine/spark/metadata/cube/ManagerHub.java||kylin-spark-project/kylin-spark-metadata/src/main/java/org/apache/kylin/engine/spark/metadata/cube/ManagerHub.java",
          "kylin-spark-project/kylin-spark-metadata/src/main/scala/org/apache/kylin/engine/spark/metadata/MetaData.scala||kylin-spark-project/kylin-spark-metadata/src/main/scala/org/apache/kylin/engine/spark/metadata/MetaData.scala",
          "kylin-spark-project/kylin-spark-metadata/src/main/scala/org/apache/kylin/engine/spark/metadata/MetadataConverter.scala||kylin-spark-project/kylin-spark-metadata/src/main/scala/org/apache/kylin/engine/spark/metadata/MetadataConverter.scala",
          "kylin-spark-project/kylin-spark-test/src/test/java/org/apache/kylin/engine/spark2/NOptimizeJobTest.java||kylin-spark-project/kylin-spark-test/src/test/java/org/apache/kylin/engine/spark2/NOptimizeJobTest.java",
          "server-base/src/main/java/org/apache/kylin/rest/controller/CubeController.java||server-base/src/main/java/org/apache/kylin/rest/controller/CubeController.java",
          "server-base/src/main/java/org/apache/kylin/rest/service/JobService.java||server-base/src/main/java/org/apache/kylin/rest/service/JobService.java"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/kylin/pull/1893",
          "https://github.com/apache/kylin/pull/2018",
          "https://github.com/apache/kylin/pull/2125",
          "https://github.com/apache/kylin/pull/2033",
          "https://github.com/apache/kylin/pull/2112",
          "https://github.com/apache/kylin/pull/2115",
          "https://github.com/apache/kylin/pull/1865",
          "https://github.com/apache/kylin/pull/1913",
          "https://github.com/apache/kylin/pull/2135"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "core-cube/src/main/java/org/apache/kylin/cube/model/CubeBuildTypeEnum.java||core-cube/src/main/java/org/apache/kylin/cube/model/CubeBuildTypeEnum.java": [
          "File: core-cube/src/main/java/org/apache/kylin/cube/model/CubeBuildTypeEnum.java -> core-cube/src/main/java/org/apache/kylin/cube/model/CubeBuildTypeEnum.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "38:     REFRESH,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "43:     OPTIMIZE,",
          "",
          "---------------"
        ],
        "core-job/src/main/java/org/apache/kylin/job/constant/ExecutableConstants.java||core-job/src/main/java/org/apache/kylin/job/constant/ExecutableConstants.java": [
          "File: core-job/src/main/java/org/apache/kylin/job/constant/ExecutableConstants.java -> core-job/src/main/java/org/apache/kylin/job/constant/ExecutableConstants.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "94:     public static final String FLINK_SPECIFIC_CONFIG_NAME_MERGE_DICTIONARY = \"mergedict\";",
          "96:     public static final String STEP_NAME_DETECT_RESOURCE = \"Detect Resource\";",
          "98: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "97:     public static final String STEP_NAME_BUILD_CUBOID_FROM_PARENT_CUBOID = \"Build recommend cuboid from parent cuboid\";",
          "",
          "---------------"
        ],
        "core-metadata/src/main/java/org/apache/kylin/metadata/model/TableDesc.java||core-metadata/src/main/java/org/apache/kylin/metadata/model/TableDesc.java": [
          "File: core-metadata/src/main/java/org/apache/kylin/metadata/model/TableDesc.java -> core-metadata/src/main/java/org/apache/kylin/metadata/model/TableDesc.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "26: import org.apache.kylin.common.KylinConfig;",
          "27: import org.apache.kylin.common.persistence.ResourceStore;",
          "28: import org.apache.kylin.common.persistence.RootPersistentEntity;",
          "30: import org.apache.kylin.common.util.StringSplitter;",
          "31: import org.apache.kylin.metadata.project.ProjectInstance;",
          "32: import org.apache.kylin.metadata.project.ProjectManager;",
          "",
          "[Removed Lines]",
          "29: import org.apache.kylin.common.util.Pair;",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/SparkBatchCubingEngineParquet.java||kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/SparkBatchCubingEngineParquet.java": [
          "File: kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/SparkBatchCubingEngineParquet.java -> kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/SparkBatchCubingEngineParquet.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "19: package org.apache.kylin.engine.spark;",
          "21: import org.apache.kylin.engine.spark.job.NSparkCubingJob;",
          "22: import org.apache.kylin.cube.CubeSegment;",
          "23: import org.apache.kylin.cube.model.CubeDesc;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "21: import org.apache.kylin.engine.spark.job.NSparkOptimizingJob;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "51:     @Override",
          "52:     public DefaultChainedExecutable createBatchOptimizeJob(CubeSegment optimizeSegment, String submitter) {",
          "54:     }",
          "56:     @Override",
          "",
          "[Removed Lines]",
          "53:         return null;",
          "",
          "[Added Lines]",
          "54:         return NSparkOptimizingJob.optimize(optimizeSegment, submitter);",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/FilterRecommendCuboidJob.java||kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/FilterRecommendCuboidJob.java": [
          "File: kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/FilterRecommendCuboidJob.java -> kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/FilterRecommendCuboidJob.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "19: package org.apache.kylin.engine.spark.job;",
          "21: import org.apache.hadoop.conf.Configuration;",
          "22: import org.apache.hadoop.fs.FileSystem;",
          "23: import org.apache.hadoop.fs.FileStatus;",
          "24: import org.apache.hadoop.fs.FileUtil;",
          "25: import org.apache.hadoop.fs.Path;",
          "26: import org.apache.kylin.common.util.HadoopUtil;",
          "27: import org.apache.kylin.cube.CubeInstance;",
          "28: import org.apache.kylin.cube.CubeManager;",
          "29: import org.apache.kylin.cube.CubeSegment;",
          "30: import org.apache.kylin.cube.cuboid.CuboidModeEnum;",
          "31: import org.apache.kylin.engine.mr.steps.CubingExecutableUtil;",
          "32: import org.apache.kylin.engine.spark.application.SparkApplication;",
          "33: import org.apache.kylin.engine.spark.metadata.cube.PathManager;",
          "34: import org.apache.kylin.shaded.com.google.common.base.Preconditions;",
          "35: import org.slf4j.Logger;",
          "36: import org.slf4j.LoggerFactory;",
          "38: import java.io.IOException;",
          "39: import java.util.Collections;",
          "40: import java.util.Set;",
          "42: public class FilterRecommendCuboidJob extends SparkApplication {",
          "43:     protected static final Logger logger = LoggerFactory.getLogger(FilterRecommendCuboidJob.class);",
          "45:     private long baseCuboid;",
          "46:     private Set<Long> recommendCuboids;",
          "48:     private FileSystem fs = HadoopUtil.getWorkingFileSystem();",
          "49:     private Configuration conf = HadoopUtil.getCurrentConfiguration();",
          "51:     public FilterRecommendCuboidJob() {",
          "53:     }",
          "55:     public String getCuboidRootPath(CubeSegment segment) {",
          "56:         return PathManager.getSegmentParquetStoragePath(segment.getCubeInstance(), segment.getName(),",
          "57:                 segment.getStorageLocationIdentifier());",
          "58:     }",
          "60:     @Override",
          "61:     protected void doExecute() throws Exception {",
          "62:         infos.clearReusedCuboids();",
          "63:         final CubeManager mgr = CubeManager.getInstance(config);",
          "64:         final CubeInstance cube = mgr.getCube(CubingExecutableUtil.getCubeName(this.getParams())).latestCopyForWrite();",
          "65:         final CubeSegment optimizeSegment = cube.getSegmentById(CubingExecutableUtil.getSegmentId(this.getParams()));",
          "67:         CubeSegment oldSegment = optimizeSegment.getCubeInstance().getOriginalSegmentToOptimize(optimizeSegment);",
          "68:         Preconditions.checkNotNull(oldSegment,",
          "69:                 \"cannot find the original segment to be optimized by \" + optimizeSegment);",
          "71:         infos.recordReusedCuboids(Collections.singleton(cube.getCuboidsByMode(CuboidModeEnum.RECOMMEND_EXISTING)));",
          "73:         baseCuboid = cube.getCuboidScheduler().getBaseCuboidId();",
          "74:         recommendCuboids = cube.getCuboidsRecommend();",
          "76:         Preconditions.checkNotNull(recommendCuboids, \"The recommend cuboid map could not be null\");",
          "78:         Path originalCuboidPath = new Path(getCuboidRootPath(oldSegment));",
          "80:         try {",
          "81:             for (FileStatus cuboid : fs.listStatus(originalCuboidPath)) {",
          "82:                 String cuboidId = cuboid.getPath().getName();",
          "83:                 if (cuboidId.equals(String.valueOf(baseCuboid)) || recommendCuboids.contains(Long.valueOf(cuboidId))) {",
          "84:                     Path optimizeCuboidPath = new Path(getCuboidRootPath(optimizeSegment) + \"/\" + cuboidId);",
          "85:                     FileUtil.copy(fs, cuboid.getPath(), fs, optimizeCuboidPath, false, true, conf);",
          "86:                     logger.info(\"Copy cuboid {} storage from original segment to optimized segment\", cuboidId);",
          "87:                 }",
          "88:             }",
          "89:         } catch (IOException e) {",
          "90:             logger.error(\"Failed to filter cuboid\", e);",
          "91:             throw e;",
          "92:         }",
          "93:     }",
          "95:     public static void main(String[] args) {",
          "96:         FilterRecommendCuboidJob filterRecommendCuboidJob = new FilterRecommendCuboidJob();",
          "97:         filterRecommendCuboidJob.execute(args);",
          "98:     }",
          "100:     @Override",
          "101:     protected String generateInfo() {",
          "102:         return LogJobInfoUtils.filterRecommendCuboidJobInfo();",
          "103:     }",
          "104: }",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/JobStepFactory.java||kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/JobStepFactory.java": [
          "File: kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/JobStepFactory.java -> kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/JobStepFactory.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "21: import org.apache.kylin.common.KylinConfig;",
          "22: import org.apache.kylin.cube.CubeInstance;",
          "23: import org.apache.kylin.job.execution.DefaultChainedExecutable;",
          "25: public class JobStepFactory {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "23: import org.apache.kylin.job.constant.ExecutableConstants;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "41:         case MERGING:",
          "42:             step = new NSparkMergingStep(config.getSparkMergeClassName());",
          "43:             break;",
          "44:         case CLEAN_UP_AFTER_MERGE:",
          "45:             step = new NSparkUpdateMetaAndCleanupAfterMergeStep();",
          "46:             break;",
          "47:         default:",
          "48:             throw new IllegalArgumentException();",
          "49:         }",
          "51:         step.setParams(parent.getParams());",
          "52:         step.setProject(parent.getProject());",
          "53:         step.setTargetSubject(parent.getTargetSubject());",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "45:         case OPTIMIZING:",
          "46:             step = new NSparkOptimizingStep(OptimizeBuildJob.class.getName());",
          "47:             break;",
          "51:         case FILTER_RECOMMEND_CUBOID:",
          "52:             step = new NSparkLocalStep();",
          "53:             step.setSparkSubmitClassName(FilterRecommendCuboidJob.class.getName());",
          "54:             step.setName(ExecutableConstants.STEP_NAME_FILTER_RECOMMEND_CUBOID_DATA_FOR_OPTIMIZATION);",
          "55:             break;",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/JobStepType.java||kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/JobStepType.java": [
          "File: kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/JobStepType.java -> kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/JobStepType.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "21: public enum JobStepType {",
          "22:     RESOURCE_DETECT,",
          "25: }",
          "",
          "[Removed Lines]",
          "24:     CLEAN_UP_AFTER_MERGE, CUBING, MERGING",
          "",
          "[Added Lines]",
          "24:     CLEAN_UP_AFTER_MERGE, CUBING, MERGING, OPTIMIZING,",
          "26:     FILTER_RECOMMEND_CUBOID",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NResourceDetectStep.java||kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NResourceDetectStep.java": [
          "File: kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NResourceDetectStep.java -> kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NResourceDetectStep.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "19: package org.apache.kylin.engine.spark.job;",
          "25: import org.apache.kylin.job.constant.ExecutableConstants;",
          "27: import org.apache.kylin.job.execution.DefaultChainedExecutable;",
          "36:     public NResourceDetectStep() {",
          "",
          "[Removed Lines]",
          "21: import java.util.Map;",
          "22: import java.util.Set;",
          "24: import org.apache.kylin.common.KylinConfig;",
          "26: import org.apache.kylin.job.execution.AbstractExecutable;",
          "29: public class NResourceDetectStep extends NSparkExecutable {",
          "31:     private final static String[] excludedSparkConf = new String[] {\"spark.executor.cores\",",
          "32:             \"spark.executor.memoryOverhead\", \"spark.executor.extraJavaOptions\",",
          "33:             \"spark.executor.instances\", \"spark.executor.memory\", \"spark.executor.extraClassPath\"};",
          "",
          "[Added Lines]",
          "24: public class NResourceDetectStep extends NSparkLocalStep {",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "44:             this.setSparkSubmitClassName(ResourceDetectBeforeMergingJob.class.getName());",
          "46:             this.setSparkSubmitClassName(ResourceDetectBeforeSampling.class.getName());",
          "48:             throw new IllegalArgumentException(\"Unsupported resource detect for \" + parent.getName() + \" job\");",
          "49:         }",
          "50:         this.setName(ExecutableConstants.STEP_NAME_DETECT_RESOURCE);",
          "51:     }",
          "76: }",
          "",
          "[Removed Lines]",
          "53:     @Override",
          "54:     protected Set<String> getMetadataDumpList(KylinConfig config) {",
          "55:         AbstractExecutable parent = getParentExecutable();",
          "56:         if (parent instanceof DefaultChainedExecutable) {",
          "57:             return ((DefaultChainedExecutable) parent).getMetadataDumpList(config);",
          "58:         }",
          "59:         throw new IllegalStateException(\"Unsupported resource detect for non chained executable!\");",
          "60:     }",
          "62:     @Override",
          "63:     protected Map<String, String> getSparkConfigOverride(KylinConfig config) {",
          "64:         Map<String, String> sparkConfigOverride = super.getSparkConfigOverride(config);",
          "66:         sparkConfigOverride.put(\"spark.master\", \"local\");",
          "67:         sparkConfigOverride.put(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\");",
          "68:         sparkConfigOverride.put(\"spark.sql.adaptive.enabled\", \"false\");",
          "69:         for (String sparkConf : excludedSparkConf) {",
          "70:             if (sparkConfigOverride.containsKey(sparkConf)) {",
          "71:                 sparkConfigOverride.remove(sparkConf);",
          "72:             }",
          "73:         }",
          "74:         return sparkConfigOverride;",
          "75:     }",
          "",
          "[Added Lines]",
          "39:         } else if (parent instanceof NSparkOptimizingJob) {",
          "40:             this.setSparkSubmitClassName(ResourceDetectBeforeOptimizingJob.class.getName());",
          "41:         } else {",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkBatchOptimizeJobCheckpointBuilder.java||kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkBatchOptimizeJobCheckpointBuilder.java": [
          "File: kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkBatchOptimizeJobCheckpointBuilder.java -> kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkBatchOptimizeJobCheckpointBuilder.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "19: package org.apache.kylin.engine.spark.job;",
          "21: import com.google.common.base.Preconditions;",
          "22: import org.apache.kylin.common.KylinConfig;",
          "23: import org.apache.kylin.cube.CubeInstance;",
          "24: import org.apache.kylin.engine.mr.steps.CubingExecutableUtil;",
          "25: import org.apache.kylin.job.constant.ExecutableConstants;",
          "26: import org.apache.kylin.job.execution.CheckpointExecutable;",
          "27: import org.apache.kylin.metadata.project.ProjectInstance;",
          "28: import org.apache.kylin.metadata.project.ProjectManager;",
          "30: import java.text.SimpleDateFormat;",
          "31: import java.util.Date;",
          "32: import java.util.List;",
          "33: import java.util.Locale;",
          "35: public class NSparkBatchOptimizeJobCheckpointBuilder {",
          "36:     protected SimpleDateFormat format = new SimpleDateFormat(\"z yyyy-MM-dd HH:mm:ss\", Locale.ROOT);",
          "38:     final protected CubeInstance cube;",
          "39:     final protected String submitter;",
          "41:     public NSparkBatchOptimizeJobCheckpointBuilder(CubeInstance cube, String submitter) {",
          "42:         this.cube = cube;",
          "43:         this.submitter = submitter;",
          "45:         Preconditions.checkNotNull(cube.getFirstSegment(), \"Cube \" + cube + \" is empty!!!\");",
          "46:     }",
          "48:     public CheckpointExecutable build() {",
          "49:         KylinConfig kylinConfig = cube.getConfig();",
          "50:         List<ProjectInstance> projList = ProjectManager.getInstance(kylinConfig).findProjects(cube.getType(),",
          "51:                 cube.getName());",
          "52:         if (projList == null || projList.size() == 0) {",
          "53:             throw new RuntimeException(\"Cannot find the project containing the cube \" + cube.getName() + \"!!!\");",
          "54:         } else if (projList.size() >= 2) {",
          "55:             throw new RuntimeException(\"Find more than one project containing the cube \" + cube.getName()",
          "56:                     + \". It does't meet the uniqueness requirement!!! \");",
          "57:         }",
          "59:         CheckpointExecutable checkpointJob = new CheckpointExecutable();",
          "60:         checkpointJob.setSubmitter(submitter);",
          "61:         CubingExecutableUtil.setCubeName(cube.getName(), checkpointJob.getParams());",
          "62:         checkpointJob.setName(",
          "63:                 cube.getName() + \" - OPTIMIZE CHECKPOINT - \" + format.format(new Date(System.currentTimeMillis())));",
          "64:         checkpointJob.setProjectName(projList.get(0).getName());",
          "67:         checkpointJob.addTask(createUpdateCubeInfoAfterCheckpointStep());",
          "70:         checkpointJob.addTask(createCleanupHdfsStorageStep());",
          "72:         return checkpointJob;",
          "73:     }",
          "75:     private NSparkUpdateCubeInfoAfterOptimizeStep createUpdateCubeInfoAfterCheckpointStep() {",
          "76:         NSparkUpdateCubeInfoAfterOptimizeStep result = new NSparkUpdateCubeInfoAfterOptimizeStep();",
          "77:         result.setName(ExecutableConstants.STEP_NAME_UPDATE_CUBE_INFO);",
          "78:         CubingExecutableUtil.setCubeName(cube.getName(), result.getParams());",
          "79:         return result;",
          "80:     }",
          "82:     private NSparkCleanupHdfsStorageStep createCleanupHdfsStorageStep() {",
          "83:         NSparkCleanupHdfsStorageStep result = new NSparkCleanupHdfsStorageStep();",
          "84:         result.setName(ExecutableConstants.STEP_NAME_GARBAGE_COLLECTION_HDFS);",
          "85:         CubingExecutableUtil.setCubeName(cube.getName(), result.getParams());",
          "86:         return result;",
          "87:     }",
          "88: }",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkCleanupHdfsStorageStep.java||kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkCleanupHdfsStorageStep.java": [
          "File: kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkCleanupHdfsStorageStep.java -> kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkCleanupHdfsStorageStep.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "19: package org.apache.kylin.engine.spark.job;",
          "21: import org.apache.hadoop.fs.FileStatus;",
          "22: import org.apache.hadoop.fs.FileSystem;",
          "23: import org.apache.hadoop.fs.Path;",
          "24: import org.apache.kylin.common.KylinConfig;",
          "25: import org.apache.kylin.common.util.HadoopUtil;",
          "26: import org.apache.kylin.cube.CubeInstance;",
          "27: import org.apache.kylin.cube.CubeManager;",
          "28: import org.apache.kylin.engine.mr.steps.CubingExecutableUtil;",
          "29: import org.apache.kylin.job.exception.ExecuteException;",
          "30: import org.apache.kylin.job.execution.AbstractExecutable;",
          "31: import org.apache.kylin.job.execution.DefaultChainedExecutable;",
          "32: import org.apache.kylin.job.execution.ExecutableContext;",
          "33: import org.apache.kylin.job.execution.ExecuteResult;",
          "34: import org.slf4j.Logger;",
          "35: import org.slf4j.LoggerFactory;",
          "37: import java.io.IOException;",
          "38: import java.util.List;",
          "39: import java.util.Set;",
          "40: import java.util.stream.Collectors;",
          "42: public class NSparkCleanupHdfsStorageStep extends NSparkExecutable {",
          "43:     private static final Logger logger = LoggerFactory.getLogger(NSparkCleanupHdfsStorageStep.class);",
          "44:     private FileSystem fs = HadoopUtil.getWorkingFileSystem();",
          "46:     public NSparkCleanupHdfsStorageStep() {",
          "47:         super();",
          "48:     }",
          "50:     @Override",
          "51:     protected ExecuteResult doWork(ExecutableContext context) throws ExecuteException {",
          "52:         final CubeManager cubeManager = CubeManager.getInstance(context.getConfig());",
          "53:         final CubeInstance cube = cubeManager.getCube(CubingExecutableUtil.getCubeName(this.getParams()));",
          "55:         List<String> segments = cube.getSegments().stream().map(segment -> {",
          "56:             return segment.getName() + \"_\" + segment.getStorageLocationIdentifier();",
          "57:         }).collect(Collectors.toList());",
          "58:         String project = cube.getProject();",
          "61:         Path cubePath = new Path(context.getConfig().getHdfsWorkingDirectory(project) + \"/parquet/\" + cube.getName());",
          "62:         try {",
          "63:             if (fs.exists(cubePath)) {",
          "64:                 FileStatus[] segmentStatus = fs.listStatus(cubePath);",
          "65:                 if (segmentStatus != null) {",
          "66:                     for (FileStatus status : segmentStatus) {",
          "67:                         String segment = status.getPath().getName();",
          "68:                         if (!segments.contains(segment)) {",
          "69:                             logger.info(\"Deleting old segment storage {}\", status.getPath());",
          "70:                             fs.delete(status.getPath(), true);",
          "71:                         }",
          "72:                     }",
          "73:                 }",
          "74:             } else {",
          "75:                 logger.warn(\"Cube path doesn't exist! The path is \" + cubePath);",
          "76:             }",
          "77:             return new ExecuteResult();",
          "78:         } catch (IOException e) {",
          "79:             logger.error(\"Failed to clean old segment storage\", e);",
          "80:             return ExecuteResult.createError(e);",
          "81:         }",
          "82:     }",
          "84:     @Override",
          "85:     protected Set<String> getMetadataDumpList(KylinConfig config) {",
          "86:         AbstractExecutable parent = getParentExecutable();",
          "87:         return ((DefaultChainedExecutable) parent).getMetadataDumpList(config);",
          "88:     }",
          "90: }",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkCubingUtil.java||kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkCubingUtil.java": [
          "File: kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkCubingUtil.java -> kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkCubingUtil.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "19: package org.apache.kylin.engine.spark.job;",
          "21: import org.apache.kylin.cube.CubeSegment;",
          "22: import org.apache.kylin.metadata.model.Segments;",
          "23: import org.apache.spark.sql.Column;",
          "24: import org.spark_project.guava.collect.Sets;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "22: import org.apache.kylin.engine.spark.metadata.cube.PathManager;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "79:         return withoutDot;",
          "80:     }",
          "86:     }",
          "88:     static Set<String> toSegmentNames(Segments<CubeSegment> segments) {",
          "",
          "[Removed Lines]",
          "82:     public static String getStoragePath(CubeSegment nDataSegment, Long layoutId) {",
          "83:         String hdfsWorkingDir = nDataSegment.getConfig().getReadHdfsWorkingDirectory();",
          "84:         return hdfsWorkingDir + getStoragePathWithoutPrefix(nDataSegment.getProject(),",
          "85:                 nDataSegment.getCubeInstance().getId(), nDataSegment.getUuid(), layoutId);",
          "",
          "[Added Lines]",
          "83:     public static String getStoragePath(CubeSegment segment, Long layoutId) {",
          "84:         return PathManager.getParquetStoragePath(segment.getCubeInstance(), segment.getName(), segment.getStorageLocationIdentifier(), layoutId);",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkLocalStep.java||kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkLocalStep.java": [
          "File: kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkLocalStep.java -> kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkLocalStep.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "19: package org.apache.kylin.engine.spark.job;",
          "21: import org.apache.kylin.common.KylinConfig;",
          "22: import org.apache.kylin.job.execution.AbstractExecutable;",
          "23: import org.apache.kylin.job.execution.DefaultChainedExecutable;",
          "25: import java.util.Map;",
          "26: import java.util.Set;",
          "28: public class NSparkLocalStep extends NSparkExecutable {",
          "29:     private final static String[] excludedSparkConf = new String[] {\"spark.executor.cores\",",
          "30:             \"spark.executor.memoryOverhead\", \"spark.executor.extraJavaOptions\",",
          "31:             \"spark.executor.instances\", \"spark.executor.memory\", \"spark.executor.extraClassPath\"};",
          "33:     @Override",
          "34:     protected Set<String> getMetadataDumpList(KylinConfig config) {",
          "35:         AbstractExecutable parent = getParentExecutable();",
          "36:         if (parent instanceof DefaultChainedExecutable) {",
          "37:             return ((DefaultChainedExecutable) parent).getMetadataDumpList(config);",
          "38:         }",
          "39:         throw new IllegalStateException(\"Unsupported \" + this.getName() + \" for non chained executable!\");",
          "40:     }",
          "42:     @Override",
          "43:     protected Map<String, String> getSparkConfigOverride(KylinConfig config) {",
          "44:         Map<String, String> sparkConfigOverride = super.getSparkConfigOverride(config);",
          "45:         overrideSparkConf(sparkConfigOverride);",
          "46:         for (String sparkConf : excludedSparkConf) {",
          "47:             if (sparkConfigOverride.containsKey(sparkConf)) {",
          "48:                 sparkConfigOverride.remove(sparkConf);",
          "49:             }",
          "50:         }",
          "51:         return sparkConfigOverride;",
          "52:     }",
          "54:     protected void overrideSparkConf(Map<String, String> sparkConfigOverride) {",
          "56:         sparkConfigOverride.put(\"spark.master\", \"local\");",
          "57:         sparkConfigOverride.put(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\");",
          "58:         sparkConfigOverride.put(\"spark.sql.adaptive.enabled\", \"false\");",
          "59:     }",
          "60: }",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkOptimizingJob.java||kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkOptimizingJob.java": [
          "File: kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkOptimizingJob.java -> kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkOptimizingJob.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "19: package org.apache.kylin.engine.spark.job;",
          "21: import org.apache.kylin.common.KylinConfig;",
          "22: import org.apache.kylin.cube.CubeInstance;",
          "23: import org.apache.kylin.cube.CubeManager;",
          "24: import org.apache.kylin.cube.CubeSegment;",
          "25: import org.apache.kylin.engine.mr.CubingJob;",
          "26: import org.apache.kylin.engine.mr.steps.CubingExecutableUtil;",
          "27: import org.apache.kylin.engine.spark.utils.MetaDumpUtil;",
          "28: import org.apache.kylin.shaded.com.google.common.base.Preconditions;",
          "29: import org.apache.kylin.metadata.MetadataConstants;",
          "30: import org.apache.kylin.shaded.com.google.common.collect.Lists;",
          "31: import org.slf4j.Logger;",
          "32: import org.slf4j.LoggerFactory;",
          "34: import java.text.SimpleDateFormat;",
          "35: import java.util.Date;",
          "36: import java.util.Locale;",
          "37: import java.util.TimeZone;",
          "38: import java.util.UUID;",
          "39: import java.util.Set;",
          "41: public class NSparkOptimizingJob extends CubingJob {",
          "42:     private static final Logger logger = LoggerFactory.getLogger(NSparkOptimizingJob.class);",
          "43:     private static final String DEPLOY_ENV_NAME = \"envName\";",
          "45:     public static NSparkOptimizingJob optimize(CubeSegment optimizedSegment, String submitter) {",
          "46:         return NSparkOptimizingJob.optimize(optimizedSegment, submitter, CubingJobTypeEnum.OPTIMIZE, UUID.randomUUID().toString());",
          "47:     }",
          "49:     public static NSparkOptimizingJob optimize(CubeSegment optimizedSegment, String submitter, CubingJobTypeEnum jobType, String jobId) {",
          "50:         logger.info(\"SPARK_V2 new job to OPTIMIZE a segment \" + optimizedSegment);",
          "51:         CubeSegment oldSegment = optimizedSegment.getCubeInstance().getOriginalSegmentToOptimize(optimizedSegment);",
          "52:         Preconditions.checkNotNull(oldSegment, \"cannot find the original segment to be optimized by \" + optimizedSegment);",
          "53:         CubeInstance cube = optimizedSegment.getCubeInstance();",
          "55:         NSparkOptimizingJob job = new NSparkOptimizingJob();",
          "56:         SimpleDateFormat format = new SimpleDateFormat(\"z yyyy-MM-dd HH:mm:ss\", Locale.ROOT);",
          "57:         format.setTimeZone(TimeZone.getTimeZone(cube.getConfig().getTimeZone()));",
          "59:         StringBuilder builder = new StringBuilder();",
          "60:         builder.append(jobType).append(\" CUBE - \");",
          "61:         builder.append(optimizedSegment.getCubeInstance().getDisplayName()).append(\" - \").append(optimizedSegment.getName())",
          "62:                 .append(\" - \");",
          "64:         builder.append(format.format(new Date(System.currentTimeMillis())));",
          "65:         job.setName(builder.toString());",
          "66:         job.setId(jobId);",
          "67:         job.setTargetSubject(optimizedSegment.getModel().getUuid());",
          "68:         job.setTargetSegments(Lists.newArrayList(String.valueOf(optimizedSegment.getUuid())));",
          "69:         job.setProject(optimizedSegment.getProject());",
          "70:         job.setSubmitter(submitter);",
          "72:         job.setParam(MetadataConstants.P_JOB_ID, jobId);",
          "73:         job.setParam(MetadataConstants.P_PROJECT_NAME, cube.getProject());",
          "74:         job.setParam(MetadataConstants.P_TARGET_MODEL, job.getTargetSubject());",
          "75:         job.setParam(MetadataConstants.P_CUBE_ID, cube.getId());",
          "76:         job.setParam(MetadataConstants.P_CUBE_NAME, cube.getName());",
          "77:         job.setParam(MetadataConstants.P_SEGMENT_IDS, String.join(\",\", job.getTargetSegments()));",
          "78:         job.setParam(CubingExecutableUtil.SEGMENT_ID, optimizedSegment.getUuid());",
          "79:         job.setParam(MetadataConstants.SEGMENT_NAME, optimizedSegment.getName());",
          "80:         job.setParam(MetadataConstants.P_DATA_RANGE_START, optimizedSegment.getSegRange().start.toString());",
          "81:         job.setParam(MetadataConstants.P_DATA_RANGE_END, optimizedSegment.getSegRange().end.toString());",
          "82:         job.setParam(MetadataConstants.P_OUTPUT_META_URL, cube.getConfig().getMetadataUrl().toString());",
          "83:         job.setParam(MetadataConstants.P_JOB_TYPE, String.valueOf(jobType));",
          "84:         job.setParam(MetadataConstants.P_CUBOID_NUMBER, String.valueOf(cube.getDescriptor().getAllCuboids().size()));",
          "87:         JobStepFactory.addStep(job, JobStepType.FILTER_RECOMMEND_CUBOID, cube);",
          "90:         JobStepFactory.addStep(job, JobStepType.RESOURCE_DETECT, cube);",
          "93:         JobStepFactory.addStep(job, JobStepType.OPTIMIZING, cube);",
          "95:         return job;",
          "96:     }",
          "98:     @Override",
          "99:     public Set<String> getMetadataDumpList(KylinConfig config) {",
          "100:         String cubeId = getParam(MetadataConstants.P_CUBE_ID);",
          "101:         CubeInstance cubeInstance = CubeManager.getInstance(config).getCubeByUuid(cubeId);",
          "102:         return MetaDumpUtil.collectCubeMetadata(cubeInstance);",
          "103:     }",
          "105:     public String getDeployEnvName() {",
          "106:         return getParam(DEPLOY_ENV_NAME);",
          "107:     }",
          "108: }",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkOptimizingStep.java||kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkOptimizingStep.java": [
          "File: kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkOptimizingStep.java -> kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkOptimizingStep.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "19: package org.apache.kylin.engine.spark.job;",
          "21: import org.apache.kylin.common.KylinConfig;",
          "22: import org.apache.kylin.cube.CubeInstance;",
          "23: import org.apache.kylin.cube.CubeManager;",
          "24: import org.apache.kylin.engine.mr.CubingJob;",
          "25: import org.apache.kylin.engine.spark.metadata.cube.PathManager;",
          "26: import org.apache.kylin.engine.spark.utils.MetaDumpUtil;",
          "27: import org.apache.kylin.engine.spark.utils.UpdateMetadataUtil;",
          "28: import org.apache.kylin.job.constant.ExecutableConstants;",
          "29: import org.apache.kylin.job.exception.ExecuteException;",
          "30: import org.apache.kylin.job.execution.ExecuteResult;",
          "31: import org.apache.kylin.metadata.MetadataConstants;",
          "32: import org.apache.kylin.shaded.com.google.common.collect.Maps;",
          "33: import org.slf4j.Logger;",
          "34: import org.slf4j.LoggerFactory;",
          "36: import java.io.IOException;",
          "37: import java.util.Arrays;",
          "38: import java.util.Map;",
          "39: import java.util.Set;",
          "41: public class NSparkOptimizingStep extends NSparkExecutable {",
          "42:     private static final Logger logger = LoggerFactory.getLogger(NSparkOptimizingStep.class);",
          "45:     public NSparkOptimizingStep() {",
          "46:     }",
          "48:     public NSparkOptimizingStep(String sparkSubmitClassName) {",
          "49:         this.setSparkSubmitClassName(sparkSubmitClassName);",
          "50:         this.setName(ExecutableConstants.STEP_NAME_BUILD_CUBOID_FROM_PARENT_CUBOID);",
          "51:     }",
          "53:     @Override",
          "54:     protected Set<String> getMetadataDumpList(KylinConfig config) {",
          "55:         String cubeId = getParam(MetadataConstants.P_CUBE_ID);",
          "56:         CubeInstance cubeInstance = CubeManager.getInstance(config).getCubeByUuid(cubeId);",
          "57:         return MetaDumpUtil.collectCubeMetadata(cubeInstance);",
          "58:     }",
          "60:     public static class Mockup {",
          "61:         public static void main(String[] args) {",
          "62:             logger.info(NSparkCubingStep.Mockup.class + \".main() invoked, args: \" + Arrays.toString(args));",
          "63:         }",
          "64:     }",
          "66:     @Override",
          "67:     public boolean needMergeMetadata() {",
          "68:         return true;",
          "69:     }",
          "71:     @Override",
          "72:     protected Map<String, String> getJobMetricsInfo(KylinConfig config) {",
          "73:         CubeManager cubeManager = CubeManager.getInstance(config);",
          "74:         CubeInstance cube = cubeManager.getCube(getCubeName());",
          "75:         Map<String, String> joblogInfo = Maps.newHashMap();",
          "76:         joblogInfo.put(CubingJob.SOURCE_SIZE_BYTES, String.valueOf(cube.getInputRecordSizeBytes()));",
          "77:         joblogInfo.put(CubingJob.CUBE_SIZE_BYTES, String.valueOf(cube.getSizeKB()));",
          "78:         return joblogInfo;",
          "79:     }",
          "81:     @Override",
          "82:     public void cleanup(ExecuteResult result) throws ExecuteException {",
          "84:         if (result != null && result.state().ordinal() == ExecuteResult.State.SUCCEED.ordinal()) {",
          "85:             PathManager.deleteJobTempPath(getConfig(), getParam(MetadataConstants.P_PROJECT_NAME),",
          "86:                     getParam(MetadataConstants.P_JOB_ID));",
          "87:         }",
          "88:     }",
          "90:     @Override",
          "91:     protected void updateMetaAfterOperation(KylinConfig config) throws IOException {",
          "92:         UpdateMetadataUtil.syncLocalMetadataToRemote(config, this);",
          "93:     }",
          "94: }",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkUpdateCubeInfoAfterOptimizeStep.java||kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkUpdateCubeInfoAfterOptimizeStep.java": [
          "File: kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkUpdateCubeInfoAfterOptimizeStep.java -> kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkUpdateCubeInfoAfterOptimizeStep.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "19: package org.apache.kylin.engine.spark.job;",
          "21: import org.apache.kylin.cube.CubeInstance;",
          "22: import org.apache.kylin.cube.CubeManager;",
          "23: import org.apache.kylin.cube.CubeSegment;",
          "24: import org.apache.kylin.engine.mr.common.CuboidStatsReaderUtil;",
          "25: import org.apache.kylin.engine.mr.steps.CubingExecutableUtil;",
          "26: import org.apache.kylin.engine.mr.steps.UpdateCubeInfoAfterCheckpointStep;",
          "27: import org.apache.kylin.job.exception.ExecuteException;",
          "28: import org.apache.kylin.job.execution.ExecutableContext;",
          "29: import org.apache.kylin.job.execution.ExecuteResult;",
          "30: import org.apache.kylin.metadata.model.SegmentStatusEnum;",
          "31: import org.slf4j.Logger;",
          "32: import org.slf4j.LoggerFactory;",
          "34: import java.util.List;",
          "35: import java.util.Map;",
          "36: import java.util.Set;",
          "38: public class NSparkUpdateCubeInfoAfterOptimizeStep extends NSparkExecutable {",
          "39:     private static final Logger logger = LoggerFactory.getLogger(UpdateCubeInfoAfterCheckpointStep.class);",
          "41:     public NSparkUpdateCubeInfoAfterOptimizeStep() {",
          "42:         super();",
          "43:     }",
          "45:     @Override",
          "46:     protected ExecuteResult doWork(ExecutableContext context) throws ExecuteException {",
          "47:         final CubeManager cubeManager = CubeManager.getInstance(context.getConfig());",
          "48:         final CubeInstance cube = cubeManager.getCube(CubingExecutableUtil.getCubeName(this.getParams()));",
          "50:         Set<Long> recommendCuboids = cube.getCuboidsRecommend();",
          "51:         try {",
          "52:             List<CubeSegment> newSegments = cube.getSegments(SegmentStatusEnum.READY_PENDING);",
          "53:             Map<Long, Long> recommendCuboidsWithStats = CuboidStatsReaderUtil",
          "54:                     .readCuboidStatsFromSegments(recommendCuboids, newSegments);",
          "55:             if (recommendCuboidsWithStats == null) {",
          "56:                 throw new RuntimeException(\"Fail to get statistics info for recommended cuboids after optimization!!!\");",
          "57:             }",
          "58:             cubeManager.promoteCheckpointOptimizeSegments(cube, recommendCuboidsWithStats,",
          "59:                     newSegments.toArray(new CubeSegment[newSegments.size()]));",
          "60:             return new ExecuteResult();",
          "61:         } catch (Exception e) {",
          "62:             logger.error(\"fail to update cube after build\", e);",
          "63:             return ExecuteResult.createError(e);",
          "64:         }",
          "65:     }",
          "66: }",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/OptimizeBuildJob.java||kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/OptimizeBuildJob.java": [
          "File: kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/OptimizeBuildJob.java -> kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/OptimizeBuildJob.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "19: package org.apache.kylin.engine.spark.job;",
          "21: import org.apache.hadoop.conf.Configuration;",
          "22: import org.apache.hadoop.fs.ContentSummary;",
          "23: import org.apache.hadoop.fs.FileStatus;",
          "24: import org.apache.hadoop.fs.FileSystem;",
          "25: import org.apache.hadoop.fs.Path;",
          "26: import org.apache.kylin.common.persistence.ResourceStore;",
          "27: import org.apache.kylin.common.util.HadoopUtil;",
          "28: import org.apache.kylin.cube.CubeInstance;",
          "29: import org.apache.kylin.cube.CubeManager;",
          "30: import org.apache.kylin.cube.CubeSegment;",
          "31: import org.apache.kylin.cube.CubeUpdate;",
          "32: import org.apache.kylin.cube.cuboid.CuboidModeEnum;",
          "33: import org.apache.kylin.engine.mr.common.CubeStatsWriter;",
          "34: import org.apache.kylin.engine.mr.steps.CubingExecutableUtil;",
          "35: import org.apache.kylin.engine.spark.NSparkCubingEngine;",
          "36: import org.apache.kylin.engine.spark.application.SparkApplication;",
          "37: import org.apache.kylin.engine.spark.builder.NBuildSourceInfo;",
          "38: import org.apache.kylin.engine.spark.metadata.SegmentInfo;",
          "39: import org.apache.kylin.engine.spark.metadata.cube.ManagerHub;",
          "40: import org.apache.kylin.engine.spark.metadata.cube.PathManager;",
          "41: import org.apache.kylin.engine.spark.metadata.cube.model.ForestSpanningTree;",
          "42: import org.apache.kylin.engine.spark.metadata.cube.model.LayoutEntity;",
          "43: import org.apache.kylin.engine.spark.metadata.cube.model.SpanningTree;",
          "44: import org.apache.kylin.engine.spark.utils.JobMetrics;",
          "45: import org.apache.kylin.engine.spark.utils.JobMetricsUtils;",
          "46: import org.apache.kylin.engine.spark.utils.Metrics;",
          "47: import org.apache.kylin.engine.spark.utils.QueryExecutionCache;",
          "48: import org.apache.kylin.engine.spark.utils.BuildUtils;",
          "49: import org.apache.kylin.metadata.model.IStorageAware;",
          "50: import org.apache.kylin.shaded.com.google.common.base.Preconditions;",
          "51: import org.apache.kylin.measure.hllc.HLLCounter;",
          "52: import org.apache.kylin.metadata.MetadataConstants;",
          "53: import org.apache.kylin.shaded.com.google.common.base.Joiner;",
          "54: import org.apache.kylin.shaded.com.google.common.collect.Lists;",
          "55: import org.apache.kylin.shaded.com.google.common.collect.Maps;",
          "56: import org.apache.kylin.storage.StorageFactory;",
          "57: import org.apache.spark.sql.Dataset;",
          "58: import org.apache.spark.sql.Row;",
          "59: import org.apache.spark.sql.hive.utils.ResourceDetectUtils;",
          "60: import org.slf4j.Logger;",
          "61: import org.slf4j.LoggerFactory;",
          "62: import scala.Tuple2;",
          "63: import scala.collection.JavaConversions;",
          "65: import java.io.IOException;",
          "67: import java.util.HashMap;",
          "68: import java.util.List;",
          "69: import java.util.Map;",
          "70: import java.util.Set;",
          "71: import java.util.Collection;",
          "72: import java.util.LinkedList;",
          "73: import java.util.ArrayList;",
          "74: import java.util.UUID;",
          "76: import java.util.stream.Collectors;",
          "78: public class OptimizeBuildJob extends SparkApplication {",
          "79:     private static final Logger logger = LoggerFactory.getLogger(OptimizeBuildJob.class);",
          "81:     private Map<Long, HLLCounter> cuboidHLLMap = Maps.newHashMap();",
          "82:     protected static String TEMP_DIR_SUFFIX = \"_temp\";",
          "84:     private BuildLayoutWithUpdate buildLayoutWithUpdate;",
          "85:     private Map<Long, Short> cuboidShardNum = Maps.newConcurrentMap();",
          "86:     private Map<Long, Long> cuboidsRowCount = Maps.newConcurrentMap();",
          "88:     private Configuration conf = HadoopUtil.getCurrentConfiguration();",
          "89:     private CubeManager cubeManager;",
          "90:     private CubeInstance cubeInstance;",
          "91:     private SegmentInfo optSegInfo;",
          "92:     private SegmentInfo originalSegInfo;",
          "93:     private CubeSegment optSeg;",
          "94:     private CubeSegment originalSeg;",
          "95:     private long baseCuboidId;",
          "97:     public static void main(String[] args) {",
          "98:         OptimizeBuildJob optimizeBuildJob = new OptimizeBuildJob();",
          "99:         optimizeBuildJob.execute(args);",
          "100:     }",
          "102:     @Override",
          "103:     protected void doExecute() throws Exception {",
          "104:         String segmentId = getParam(CubingExecutableUtil.SEGMENT_ID);",
          "105:         String cubeId = getParam(MetadataConstants.P_CUBE_ID);",
          "107:         cubeManager = CubeManager.getInstance(config);",
          "108:         cubeInstance = cubeManager.getCubeByUuid(cubeId);",
          "109:         optSeg = cubeInstance.getSegmentById(segmentId);",
          "110:         originalSeg = cubeInstance.getOriginalSegmentToOptimize(optSeg);",
          "111:         originalSegInfo = ManagerHub.getSegmentInfo(config, cubeId, originalSeg.getUuid());",
          "113:         calculateCuboidFromBaseCuboid();",
          "114:         buildCuboidFromParent(cubeId);",
          "115:     }",
          "117:     private void calculateCuboidFromBaseCuboid() throws IOException {",
          "118:         logger.info(\"Start to calculate cuboid statistics for optimized segment\");",
          "119:         long start = System.currentTimeMillis();",
          "121:         baseCuboidId = cubeInstance.getCuboidScheduler().getBaseCuboidId();",
          "122:         LayoutEntity baseCuboid = originalSegInfo.getAllLayoutJava().stream()",
          "123:                 .filter(layoutEntity -> layoutEntity.getId() == baseCuboidId).iterator().next();",
          "124:         Dataset<Row> baseCuboidDS = StorageFactory",
          "125:                 .createEngineAdapter(baseCuboid, NSparkCubingEngine.NSparkCubingStorage.class)",
          "126:                 .getFrom(PathManager.getParquetStoragePath(config, cubeInstance.getName(), optSeg.getName(),",
          "127:                         optSeg.getStorageLocationIdentifier(), String.valueOf(baseCuboid.getId())), ss);",
          "129:         Map<Long, HLLCounter> hllMap = new HashMap<>();",
          "131:         for (Tuple2<Object, AggInfo> cuboidData : CuboidStatisticsJob.statistics(baseCuboidDS,",
          "132:                 originalSegInfo, getNewCuboidIds())) {",
          "133:             hllMap.put((Long) cuboidData._1, cuboidData._2.cuboid().counter());",
          "134:         }",
          "136:         String jobTmpDir = config.getJobTmpDir(project) + \"/\" + jobId;",
          "137:         Path statisticsDir = new Path(jobTmpDir + \"/\" + ResourceStore.CUBE_STATISTICS_ROOT + \"/\"",
          "138:                 + cubeInstance.getUuid() + \"/\" + optSeg.getUuid() + \"/\");",
          "140:         CubeStatsWriter.writeCuboidStatistics(conf, statisticsDir, hllMap, 1, -1);",
          "142:         logger.info(\"Calculate cuboid statistics from base cuboid job takes {} ms\",",
          "143:                 (System.currentTimeMillis() - start));",
          "144:     }",
          "146:     private void buildCuboidFromParent(String cubeId) throws IOException {",
          "147:         logger.info(\"Start to build recommend cuboid for optimized segment\");",
          "148:         long start = System.currentTimeMillis();",
          "149:         optSegInfo = ManagerHub.getSegmentInfo(config, cubeId, optSeg.getUuid(), CuboidModeEnum.RECOMMEND);",
          "150:         buildLayoutWithUpdate = new BuildLayoutWithUpdate(config);",
          "152:         infos.clearAddCuboids();",
          "154:         SpanningTree spanningTree;",
          "155:         ParentSourceChooser sourceChooser;",
          "156:         try {",
          "157:             spanningTree = new ForestSpanningTree(JavaConversions.asJavaCollection(optSegInfo.toBuildLayouts()));",
          "158:             logger.info(\"There are {} cuboids to be built in segment {}.\", optSegInfo.toBuildLayouts().size(),",
          "159:                     optSegInfo.name());",
          "160:             for (LayoutEntity cuboid : JavaConversions.asJavaCollection(optSegInfo.toBuildLayouts())) {",
          "161:                 logger.debug(\"Cuboid {} has row keys: {}\", cuboid.getId(),",
          "162:                         Joiner.on(\", \").join(cuboid.getOrderedDimensions().keySet()));",
          "163:             }",
          "166:             optSegInfo.removeLayout(baseCuboidId);",
          "167:             sourceChooser = new ParentSourceChooser(spanningTree, optSegInfo, optSeg, jobId, ss, config, false);",
          "168:             sourceChooser.decideSources();",
          "169:             Map<Long, NBuildSourceInfo> buildFromLayouts = sourceChooser.reuseSources();",
          "171:             infos.clearCuboidsNumPerLayer(optSegInfo.id());",
          "174:             if (!buildFromLayouts.isEmpty()) {",
          "175:                 build(buildFromLayouts.values(), optSegInfo, spanningTree);",
          "176:             }",
          "177:             infos.recordSpanningTree(optSegInfo.id(), spanningTree);",
          "179:             logger.info(\"Updating segment info\");",
          "180:             updateOptimizeSegmentInfo();",
          "181:         } finally {",
          "182:             logger.info(\"Building job takes {} ms\", (System.currentTimeMillis() - start));",
          "183:         }",
          "184:     }",
          "186:     private long[] getNewCuboidIds() {",
          "187:         Set<Long> recommendCuboidsSet = cubeInstance.getCuboidsByMode(CuboidModeEnum.RECOMMEND_MISSING);",
          "188:         Preconditions.checkNotNull(recommendCuboidsSet, \"The recommend cuboid map could not be null\");",
          "189:         long[] recommendCuboid = new long[recommendCuboidsSet.size()];",
          "190:         int i = 0;",
          "191:         for (long cuboidId : recommendCuboidsSet) {",
          "192:             recommendCuboid[i++] = cuboidId;",
          "193:         }",
          "194:         return recommendCuboid;",
          "195:     }",
          "197:     protected void updateOptimizeSegmentInfo() throws IOException {",
          "198:         CubeInstance cubeCopy = optSeg.getCubeInstance().latestCopyForWrite();",
          "199:         List<CubeSegment> cubeSegments = Lists.newArrayList();",
          "200:         CubeUpdate update = new CubeUpdate(cubeCopy);",
          "202:         optSeg.setSizeKB(optSegInfo.getAllLayoutSize() / 1024);",
          "203:         optSeg.setLastBuildTime(System.currentTimeMillis());",
          "204:         optSeg.setLastBuildJobID(jobId);",
          "205:         optSeg.setInputRecords(originalSeg.getInputRecords());",
          "206:         Map<Long, Short> existingShardNums = originalSeg.getCuboidShardNums();",
          "207:         for (Long cuboidId : cubeCopy.getCuboidsByMode(CuboidModeEnum.RECOMMEND_EXISTING)) {",
          "208:             cuboidShardNum.putIfAbsent(cuboidId, existingShardNums.get(cuboidId));",
          "209:         }",
          "210:         optSeg.setCuboidShardNums(cuboidShardNum);",
          "211:         optSeg.setInputRecordsSize(originalSeg.getInputRecordsSize());",
          "212:         Map<String, String> additionalInfo = optSeg.getAdditionalInfo();",
          "213:         additionalInfo.put(\"storageType\", \"\" + IStorageAware.ID_PARQUET);",
          "214:         optSeg.setAdditionalInfo(additionalInfo);",
          "215:         cubeSegments.add(optSeg);",
          "216:         update.setToUpdateSegs(cubeSegments.toArray(new CubeSegment[0]));",
          "217:         cubeManager.updateCube(update);",
          "218:     }",
          "220:     private void build(Collection<NBuildSourceInfo> buildSourceInfos, SegmentInfo seg, SpanningTree st) {",
          "222:         List<NBuildSourceInfo> theFirstLevelBuildInfos = buildLayer(buildSourceInfos, seg, st);",
          "223:         LinkedList<List<NBuildSourceInfo>> queue = new LinkedList<>();",
          "225:         if (!theFirstLevelBuildInfos.isEmpty()) {",
          "226:             queue.offer(theFirstLevelBuildInfos);",
          "227:         }",
          "229:         while (!queue.isEmpty()) {",
          "230:             List<NBuildSourceInfo> buildInfos = queue.poll();",
          "231:             List<NBuildSourceInfo> theNextLayer = buildLayer(buildInfos, seg, st);",
          "232:             if (!theNextLayer.isEmpty()) {",
          "233:                 queue.offer(theNextLayer);",
          "234:             }",
          "235:         }",
          "236:     }",
          "239:     private List<NBuildSourceInfo> buildLayer(Collection<NBuildSourceInfo> buildSourceInfos, SegmentInfo seg,",
          "240:                                               SpanningTree st) {",
          "241:         int cuboidsNumInLayer = 0;",
          "244:         List<LayoutEntity> allIndexesInCurrentLayer = new ArrayList<>();",
          "245:         for (NBuildSourceInfo info : buildSourceInfos) {",
          "246:             Collection<LayoutEntity> toBuildCuboids = info.getToBuildCuboids();",
          "247:             infos.recordParent2Children(info.getLayout(),",
          "248:                     toBuildCuboids.stream().map(LayoutEntity::getId).collect(Collectors.toList()));",
          "249:             cuboidsNumInLayer += toBuildCuboids.size();",
          "250:             Preconditions.checkState(!toBuildCuboids.isEmpty(), \"To be built cuboids is empty.\");",
          "251:             Dataset<Row> parentDS = info.getParentDS();",
          "253:             if (info.getLayoutId() == ParentSourceChooser.FLAT_TABLE_FLAG()) {",
          "254:                 cuboidsRowCount.putIfAbsent(info.getLayoutId(), parentDS.count());",
          "255:             }",
          "257:             for (LayoutEntity index : toBuildCuboids) {",
          "258:                 Preconditions.checkNotNull(parentDS, \"Parent dataset is null when building.\");",
          "259:                 if (!cubeInstance.getCuboidsByMode(CuboidModeEnum.RECOMMEND_EXISTING).contains(index.getId())) {",
          "260:                     infos.recordAddCuboids(index.getId());",
          "261:                     buildLayoutWithUpdate.submit(new BuildLayoutWithUpdate.JobEntity() {",
          "262:                         @Override",
          "263:                         public String getName() {",
          "264:                             return \"build-cuboid-\" + index.getId();",
          "265:                         }",
          "267:                         @Override",
          "268:                         public LayoutEntity build() throws IOException {",
          "269:                             return buildCuboid(seg, index, parentDS, st, info.getLayoutId());",
          "270:                         }",
          "272:                         @Override",
          "273:                         public NBuildSourceInfo getBuildSourceInfo() {",
          "274:                             return info;",
          "275:                         }",
          "276:                     }, config);",
          "277:                 } else {",
          "278:                     try {",
          "279:                         updateExistingLayout(index, info.getLayoutId());",
          "280:                     } catch (IOException e) {",
          "281:                         logger.error(\"Failed to update existing cuboid info: {}\", index.getId());",
          "282:                     }",
          "283:                 }",
          "284:                 allIndexesInCurrentLayer.add(index);",
          "285:             }",
          "286:         }",
          "288:         infos.recordCuboidsNumPerLayer(seg.id(), cuboidsNumInLayer);",
          "289:         buildLayoutWithUpdate.updateLayout(seg, config);",
          "292:         st.decideTheNextLayer(allIndexesInCurrentLayer, seg);",
          "293:         return constructTheNextLayerBuildInfos(st, seg, allIndexesInCurrentLayer);",
          "294:     }",
          "297:     private List<NBuildSourceInfo> constructTheNextLayerBuildInfos(SpanningTree st, SegmentInfo seg,",
          "298:                                                                    Collection<LayoutEntity> allIndexesInCurrentLayer) {",
          "300:         List<NBuildSourceInfo> childrenBuildSourceInfos = new ArrayList<>();",
          "301:         for (LayoutEntity index : allIndexesInCurrentLayer) {",
          "302:             Collection<LayoutEntity> children = st.getChildrenByIndexPlan(index);",
          "304:             if (!children.isEmpty()) {",
          "305:                 NBuildSourceInfo theRootLevelBuildInfos = new NBuildSourceInfo();",
          "306:                 theRootLevelBuildInfos.setSparkSession(ss);",
          "307:                 String path = PathManager.getParquetStoragePath(config, getParam(MetadataConstants.P_CUBE_NAME), seg.name(), seg.identifier(),",
          "308:                         String.valueOf(index.getId()));",
          "309:                 theRootLevelBuildInfos.setLayoutId(index.getId());",
          "310:                 theRootLevelBuildInfos.setParentStoragePath(path);",
          "311:                 theRootLevelBuildInfos.setToBuildCuboids(children);",
          "312:                 childrenBuildSourceInfos.add(theRootLevelBuildInfos);",
          "313:             }",
          "314:         }",
          "316:         return childrenBuildSourceInfos;",
          "317:     }",
          "319:     @Override",
          "320:     protected String calculateRequiredCores() throws Exception {",
          "321:         if (config.getSparkEngineTaskImpactInstanceEnabled()) {",
          "322:             Path shareDir = config.getJobTmpShareDir(project, jobId);",
          "323:             String maxLeafTasksNums = maxLeafTasksNums(shareDir);",
          "324:             logger.info(\"The maximum number of tasks required to run the job is {}\", maxLeafTasksNums);",
          "325:             int factor = config.getSparkEngineTaskCoreFactor();",
          "326:             int i = Double.valueOf(maxLeafTasksNums).intValue() / factor;",
          "327:             logger.info(\"require cores: \" + i);",
          "328:             return String.valueOf(i);",
          "329:         } else {",
          "330:             return config.getSparkEngineRequiredTotalCores();",
          "331:         }",
          "332:     }",
          "334:     private String maxLeafTasksNums(Path shareDir) throws IOException {",
          "335:         FileSystem fs = HadoopUtil.getWorkingFileSystem();",
          "336:         FileStatus[] fileStatuses = fs.listStatus(shareDir,",
          "337:                 path -> path.toString().endsWith(ResourceDetectUtils.cubingDetectItemFileSuffix()));",
          "338:         return ResourceDetectUtils.selectMaxValueInFiles(fileStatuses);",
          "339:     }",
          "341:     private LayoutEntity buildCuboid(SegmentInfo seg, LayoutEntity cuboid, Dataset<Row> parent,",
          "342:                                      SpanningTree spanningTree, long parentId) throws IOException {",
          "343:         String parentName = String.valueOf(parentId);",
          "344:         if (parentId == ParentSourceChooser.FLAT_TABLE_FLAG()) {",
          "345:             parentName = \"flat table\";",
          "346:         }",
          "347:         logger.info(\"Build index:{}, in segment:{}\", cuboid.getId(), seg.id());",
          "348:         LayoutEntity layoutEntity = cuboid;",
          "349:         Set<Integer> dimIndexes = cuboid.getOrderedDimensions().keySet();",
          "350:         if (cuboid.isTableIndex()) {",
          "351:             Dataset<Row> afterPrj = parent.select(NSparkCubingUtil.getColumns(dimIndexes));",
          "353:             logger.info(\"Build layout:{}, in index:{}\", layoutEntity.getId(), cuboid.getId());",
          "354:             ss.sparkContext().setJobDescription(\"build \" + layoutEntity.getId() + \" from parent \" + parentName);",
          "355:             Set<Integer> orderedDims = layoutEntity.getOrderedDimensions().keySet();",
          "356:             Dataset<Row> afterSort = afterPrj.select(NSparkCubingUtil.getColumns(orderedDims))",
          "357:                     .sortWithinPartitions(NSparkCubingUtil.getColumns(orderedDims));",
          "358:             saveAndUpdateLayout(afterSort, seg, layoutEntity, parentId);",
          "359:         } else {",
          "360:             Dataset<Row> afterAgg = CuboidAggregator.agg(ss, parent, dimIndexes, cuboid.getOrderedMeasures(),",
          "361:                     spanningTree, false);",
          "362:             logger.info(\"Build layout:{}, in index:{}\", layoutEntity.getId(), cuboid.getId());",
          "363:             ss.sparkContext().setJobDescription(\"build \" + layoutEntity.getId() + \" from parent \" + parentName);",
          "364:             Set<Integer> rowKeys = layoutEntity.getOrderedDimensions().keySet();",
          "366:             Dataset<Row> afterSort = afterAgg",
          "367:                     .select(NSparkCubingUtil.getColumns(rowKeys, layoutEntity.getOrderedMeasures().keySet()))",
          "368:                     .sortWithinPartitions(NSparkCubingUtil.getColumns(rowKeys));",
          "370:             saveAndUpdateLayout(afterSort, seg, layoutEntity, parentId);",
          "371:         }",
          "372:         ss.sparkContext().setJobDescription(null);",
          "373:         logger.info(\"Finished Build index :{}, in segment:{}\", cuboid.getId(), seg.id());",
          "374:         return layoutEntity;",
          "375:     }",
          "377:     private void saveAndUpdateLayout(Dataset<Row> dataset, SegmentInfo seg, LayoutEntity layout,",
          "378:                                      long parentId) throws IOException {",
          "379:         long layoutId = layout.getId();",
          "382:         String queryExecutionId = UUID.randomUUID().toString();",
          "383:         ss.sparkContext().setLocalProperty(QueryExecutionCache.N_EXECUTION_ID_KEY(), queryExecutionId);",
          "385:         NSparkCubingEngine.NSparkCubingStorage storage = StorageFactory.createEngineAdapter(layout,",
          "386:                 NSparkCubingEngine.NSparkCubingStorage.class);",
          "387:         String path = PathManager.getParquetStoragePath(config, getParam(MetadataConstants.P_CUBE_NAME), seg.name(), seg.identifier(),",
          "388:                 String.valueOf(layoutId));",
          "389:         String tempPath = path + TEMP_DIR_SUFFIX;",
          "391:         logger.info(\"Cuboids are saved to temp path : \" + tempPath);",
          "392:         storage.saveTo(tempPath, dataset, ss);",
          "394:         JobMetrics metrics = JobMetricsUtils.collectMetrics(queryExecutionId);",
          "395:         long rowCount = metrics.getMetrics(Metrics.CUBOID_ROWS_CNT());",
          "396:         if (rowCount == -1) {",
          "397:             infos.recordAbnormalLayouts(layoutId, \"'Job metrics seems null, use count() to collect cuboid rows.'\");",
          "398:             logger.debug(\"Can not get cuboid row cnt, use count() to collect cuboid rows.\");",
          "399:             long cuboidRowCnt = dataset.count();",
          "400:             layout.setRows(cuboidRowCnt);",
          "402:             cuboidsRowCount.putIfAbsent(layoutId, cuboidRowCnt);",
          "403:             layout.setSourceRows(cuboidsRowCount.get(parentId));",
          "404:         } else {",
          "405:             layout.setRows(rowCount);",
          "406:             layout.setSourceRows(metrics.getMetrics(Metrics.SOURCE_ROWS_CNT()));",
          "407:         }",
          "408:         int shardNum = BuildUtils.repartitionIfNeed(layout, storage, path, tempPath, cubeInstance.getConfig(), ss);",
          "409:         layout.setShardNum(shardNum);",
          "410:         cuboidShardNum.put(layoutId, (short) shardNum);",
          "411:         ss.sparkContext().setLocalProperty(QueryExecutionCache.N_EXECUTION_ID_KEY(), null);",
          "412:         QueryExecutionCache.removeQueryExecution(queryExecutionId);",
          "413:         BuildUtils.fillCuboidInfo(layout, path);",
          "414:     }",
          "416:     private void updateExistingLayout(LayoutEntity layout, long parentId) throws IOException {",
          "417:         FileSystem fs = HadoopUtil.getWorkingFileSystem();",
          "418:         long layoutId = layout.getId();",
          "419:         String path = PathManager.getParquetStoragePath(config, cubeInstance.getName(), optSegInfo.name(), optSegInfo.identifier(),",
          "420:                 String.valueOf(layoutId));",
          "421:         Dataset<Row> dataset = StorageFactory",
          "422:                 .createEngineAdapter(layout, NSparkCubingEngine.NSparkCubingStorage.class)",
          "423:                 .getFrom(path, ss);",
          "424:         logger.debug(\"Existing cuboid, use count() to collect cuboid rows.\");",
          "425:         long cuboidRowCnt = dataset.count();",
          "426:         ContentSummary cs = HadoopUtil.getContentSummary(fs, new Path(path));",
          "427:         layout.setRows(cuboidRowCnt);",
          "428:         layout.setFileCount(cs.getFileCount());",
          "429:         layout.setByteSize(cs.getLength());",
          "431:         cuboidsRowCount.putIfAbsent(layoutId, cuboidRowCnt);",
          "432:         layout.setSourceRows(cuboidsRowCount.get(parentId));",
          "433:         int shardNum = originalSeg.getCuboidShardNums().get(layoutId);",
          "434:         layout.setShardNum(shardNum);",
          "435:         optSegInfo.updateLayout(layout);",
          "436:     }",
          "438:     @Override",
          "439:     protected String generateInfo() {",
          "440:         return LogJobInfoUtils.dfOptimizeJobInfo();",
          "441:     }",
          "442: }",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/ResourceDetectBeforeOptimizingJob.java||kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/ResourceDetectBeforeOptimizingJob.java": [
          "File: kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/ResourceDetectBeforeOptimizingJob.java -> kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/ResourceDetectBeforeOptimizingJob.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "19: package org.apache.kylin.engine.spark.job;",
          "21: import org.apache.hadoop.fs.Path;",
          "22: import org.apache.kylin.cube.CubeInstance;",
          "23: import org.apache.kylin.cube.CubeManager;",
          "24: import org.apache.kylin.cube.CubeSegment;",
          "25: import org.apache.kylin.engine.mr.steps.CubingExecutableUtil;",
          "26: import org.apache.kylin.engine.spark.application.SparkApplication;",
          "27: import org.apache.kylin.engine.spark.builder.NBuildSourceInfo;",
          "28: import org.apache.kylin.engine.spark.metadata.SegmentInfo;",
          "29: import org.apache.kylin.engine.spark.metadata.cube.ManagerHub;",
          "30: import org.apache.kylin.engine.spark.metadata.cube.model.ForestSpanningTree;",
          "31: import org.apache.kylin.engine.spark.metadata.cube.model.SpanningTree;",
          "32: import org.apache.kylin.engine.spark.utils.SparkUtils;",
          "33: import org.apache.kylin.metadata.MetadataConstants;",
          "34: import org.apache.kylin.shaded.com.google.common.collect.Maps;",
          "35: import org.apache.spark.rdd.RDD;",
          "36: import org.apache.spark.sql.Dataset;",
          "37: import org.apache.spark.sql.Row;",
          "38: import org.apache.spark.sql.hive.utils.ResourceDetectUtils;",
          "39: import org.slf4j.Logger;",
          "40: import org.slf4j.LoggerFactory;",
          "41: import scala.collection.JavaConversions;",
          "43: import java.util.ArrayList;",
          "44: import java.util.List;",
          "45: import java.util.Map;",
          "46: import java.util.stream.Collectors;",
          "48: public class ResourceDetectBeforeOptimizingJob extends SparkApplication {",
          "49:     protected static final Logger logger = LoggerFactory.getLogger(ResourceDetectBeforeOptimizingJob.class);",
          "50:     protected volatile SpanningTree spanningTree;",
          "51:     protected volatile List<NBuildSourceInfo> sources = new ArrayList<>();",
          "53:     public static void main(String[] args) {",
          "54:         ResourceDetectBeforeOptimizingJob resourceDetectJob = new ResourceDetectBeforeOptimizingJob();",
          "55:         resourceDetectJob.execute(args);",
          "56:     }",
          "58:     @Override",
          "59:     protected void doExecute() throws Exception {",
          "60:         logger.info(\"Start detect resource before optimize.\");",
          "61:         String segId = getParam(CubingExecutableUtil.SEGMENT_ID);",
          "62:         String cubeId = getParam(MetadataConstants.P_CUBE_ID);",
          "63:         CubeManager cubeManager = CubeManager.getInstance(config);",
          "64:         CubeInstance cubeInstance = cubeManager.getCubeByUuid(cubeId);",
          "65:         SegmentInfo segInfo = ManagerHub.getSegmentInfo(config, cubeId, segId);",
          "66:         CubeSegment segment = cubeInstance.getSegmentById(segId);",
          "67:         infos.recordOptimizingSegment(segInfo);",
          "69:         spanningTree = new ForestSpanningTree(JavaConversions.asJavaCollection(segInfo.toBuildLayouts()));",
          "70:         segInfo.removeLayout(segment.getCuboidScheduler().getBaseCuboidId());",
          "71:         ResourceDetectUtils.write(new Path(config.getJobTmpShareDir(project, jobId), ResourceDetectUtils.countDistinctSuffix()), false);",
          "72:         ParentSourceChooser datasetChooser = new ParentSourceChooser(spanningTree, segInfo, segment, jobId, ss, config, false);",
          "74:         datasetChooser.decideSources();",
          "75:         NBuildSourceInfo buildFromFlatTable = datasetChooser.flatTableSource();",
          "76:         if (buildFromFlatTable != null) {",
          "77:             sources.add(buildFromFlatTable);",
          "78:         }",
          "79:         Map<Long, NBuildSourceInfo> buildFromLayouts = datasetChooser.reuseSources();",
          "80:         sources.addAll(buildFromLayouts.values());",
          "82:         Map<String, List<String>> resourcePaths = Maps.newHashMap();",
          "83:         Map<String, Integer> layoutLeafTaskNums = Maps.newHashMap();",
          "84:         infos.clearSparkPlans();",
          "85:         for (NBuildSourceInfo source : sources) {",
          "86:             Dataset<Row> dataset = source.getParentDS();",
          "87:             RDD actionRdd = dataset.queryExecution().toRdd();",
          "88:             logger.info(\"leaf nodes is: {} \", SparkUtils.leafNodes(actionRdd));",
          "89:             infos.recordSparkPlan(dataset.queryExecution().sparkPlan());",
          "90:             List<Path> paths = JavaConversions",
          "91:                     .seqAsJavaList(ResourceDetectUtils.getPaths(dataset.queryExecution().sparkPlan()));",
          "92:             List<String> pathList = paths.stream().map(Path::toString).collect(Collectors.toList());",
          "93:             resourcePaths.put(String.valueOf(source.getLayoutId()), pathList);",
          "94:             layoutLeafTaskNums.put(String.valueOf(source.getLayoutId()), SparkUtils.leafNodePartitionNums(actionRdd));",
          "95:         }",
          "96:         ResourceDetectUtils.write(",
          "97:                 new Path(config.getJobTmpShareDir(project, jobId), segId + \"_\" + ResourceDetectUtils.fileName()),",
          "98:                 resourcePaths);",
          "99:         ResourceDetectUtils.write(",
          "100:                 new Path(config.getJobTmpShareDir(project, jobId), segId + \"_\" + ResourceDetectUtils.cubingDetectItemFileSuffix()),",
          "101:                 layoutLeafTaskNums);",
          "102:     }",
          "104:     @Override",
          "105:     protected String generateInfo() {",
          "106:         return LogJobInfoUtils.resourceDetectBeforeOptimizeJobInfo();",
          "107:     }",
          "108: }",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/utils/UpdateMetadataUtil.java||kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/utils/UpdateMetadataUtil.java": [
          "File: kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/utils/UpdateMetadataUtil.java -> kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/utils/UpdateMetadataUtil.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "39: import org.apache.kylin.cube.CubeManager;",
          "40: import org.apache.kylin.cube.CubeSegment;",
          "41: import org.apache.kylin.cube.CubeUpdate;",
          "42: import org.apache.kylin.cube.model.CubeBuildTypeEnum;",
          "43: import org.apache.kylin.engine.mr.common.BatchConstants;",
          "44: import org.apache.kylin.engine.mr.steps.CubingExecutableUtil;",
          "45: import org.apache.kylin.engine.spark.job.NSparkExecutable;",
          "46: import org.apache.kylin.metadata.MetadataConstants;",
          "47: import org.apache.kylin.metadata.model.SegmentStatusEnum;",
          "48: import org.apache.kylin.metadata.realization.RealizationStatusEnum;",
          "49: import org.slf4j.Logger;",
          "50: import org.slf4j.LoggerFactory;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "42: import org.apache.kylin.cube.cuboid.CuboidModeEnum;",
          "45: import org.apache.kylin.engine.mr.common.CubeStatsReader;",
          "46: import org.apache.kylin.engine.mr.common.CubeStatsWriter;",
          "49: import org.apache.kylin.measure.hllc.HLLCounter;",
          "53: import org.apache.kylin.shaded.com.google.common.collect.Maps;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "54:     protected static final Logger logger = LoggerFactory.getLogger(UpdateMetadataUtil.class);",
          "58:         String cubeId = nsparkExecutable.getParam(MetadataConstants.P_CUBE_ID);",
          "61:         String segmentId = segmentIds.iterator().next();",
          "62:         String remoteResourceStore = nsparkExecutable.getDistMetaUrl();",
          "63:         String jobType = nsparkExecutable.getParam(MetadataConstants.P_JOB_TYPE);",
          "",
          "[Removed Lines]",
          "56:     public static void syncLocalMetadataToRemote(KylinConfig config,",
          "57:                                                  NSparkExecutable nsparkExecutable) throws IOException {",
          "59:         Set<String> segmentIds = Sets.newHashSet(StringUtils.split(",
          "60:                 nsparkExecutable.getParam(CubingExecutableUtil.SEGMENT_ID), \" \"));",
          "",
          "[Added Lines]",
          "61:     public static void syncLocalMetadataToRemote(KylinConfig config, NSparkExecutable nsparkExecutable)",
          "62:             throws IOException {",
          "64:         Set<String> segmentIds = Sets",
          "65:                 .newHashSet(StringUtils.split(nsparkExecutable.getParam(CubingExecutableUtil.SEGMENT_ID), \" \"));",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "77:                             currentInstanceCopy.toString(), toUpdateSeg.toString(), tobeSegments.toString()));",
          "79:         String resKey = toUpdateSeg.getStatisticsResourcePath();",
          "82:         FileSystem fs = HadoopUtil.getWorkingFileSystem();",
          "83:         if (fs.exists(statisticsFile)) {",
          "84:             FSDataInputStream is = fs.open(statisticsFile);",
          "",
          "[Removed Lines]",
          "80:         String jobTmpDir = config.getJobTmpDir(currentInstanceCopy.getProject()) + \"/\" + nsparkExecutable.getParam(MetadataConstants.P_JOB_ID);",
          "81:         Path statisticsFile = new Path(jobTmpDir + \"/\" + ResourceStore.CUBE_STATISTICS_ROOT + \"/\" + cubeId + \"/\" + segmentId + \"/\" + BatchConstants.CFG_STATISTICS_CUBOID_ESTIMATION_FILENAME);",
          "",
          "[Added Lines]",
          "85:         String statisticsDir = config.getJobTmpDir(currentInstanceCopy.getProject()) + \"/\"",
          "86:                 + nsparkExecutable.getParam(MetadataConstants.P_JOB_ID) + \"/\" + ResourceStore.CUBE_STATISTICS_ROOT + \"/\"",
          "87:                 + cubeId + \"/\" + segmentId + \"/\";",
          "88:         Path statisticsFile = new Path(statisticsDir, BatchConstants.CFG_STATISTICS_CUBOID_ESTIMATION_FILENAME);",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "92:         if (String.valueOf(CubeBuildTypeEnum.MERGE).equals(jobType)) {",
          "93:             toUpdateSeg.getSnapshots().clear();",
          "97:                 toUpdateSeg.putSnapshotResPath(entry.getKey(), entry.getValue());",
          "98:             }",
          "99:         } else {",
          "100:             toUpdateSeg.setStatus(SegmentStatusEnum.READY);",
          "101:             for (CubeSegment segment : currentInstanceCopy.getSegments()) {",
          "",
          "[Removed Lines]",
          "95:             for (Map.Entry<String, String> entry :",
          "96:                     currentInstanceCopy.getLatestReadySegment().getSnapshots().entrySet()) {",
          "",
          "[Added Lines]",
          "102:             for (Map.Entry<String, String> entry : currentInstanceCopy.getLatestReadySegment().getSnapshots()",
          "103:                     .entrySet()) {",
          "106:         } else if (String.valueOf(CubeBuildTypeEnum.OPTIMIZE).equals(jobType)) {",
          "107:             CubeSegment origSeg = currentInstanceCopy.getOriginalSegmentToOptimize(toUpdateSeg);",
          "108:             toUpdateSeg.getDictionaries().putAll(origSeg.getDictionaries());",
          "109:             toUpdateSeg.getSnapshots().putAll(origSeg.getSnapshots());",
          "110:             toUpdateSeg.getRowkeyStats().addAll(origSeg.getRowkeyStats());",
          "112:             CubeStatsReader optSegStatsReader = new CubeStatsReader(toUpdateSeg, config);",
          "113:             CubeStatsReader origSegStatsReader = new CubeStatsReader(origSeg, config);",
          "114:             Map<Long, HLLCounter> cuboidHLLMap = Maps.newHashMap();",
          "115:             if (origSegStatsReader.getCuboidRowHLLCounters() == null) {",
          "116:                 throw new IllegalArgumentException(",
          "117:                         \"Cuboid statistics of original segment do not exist. Please check the config of kylin.engine.segment-statistics-enabled.\");",
          "118:             }",
          "119:             addFromCubeStatsReader(origSegStatsReader, cuboidHLLMap);",
          "120:             addFromCubeStatsReader(optSegStatsReader, cuboidHLLMap);",
          "122:             Set<Long> recommendCuboids = currentInstanceCopy.getCuboidsByMode(CuboidModeEnum.RECOMMEND);",
          "123:             Map<Long, HLLCounter> resultCuboidHLLMap = Maps.newHashMapWithExpectedSize(recommendCuboids.size());",
          "124:             for (long cuboid : recommendCuboids) {",
          "125:                 HLLCounter hll = cuboidHLLMap.get(cuboid);",
          "126:                 if (hll == null) {",
          "127:                     logger.warn(\"Cannot get the row count stats for cuboid \" + cuboid);",
          "128:                 } else {",
          "129:                     resultCuboidHLLMap.put(cuboid, hll);",
          "130:                 }",
          "131:             }",
          "132:             if (fs.exists(statisticsFile)) {",
          "133:                 fs.delete(statisticsFile, false);",
          "134:             }",
          "135:             CubeStatsWriter.writeCuboidStatistics(HadoopUtil.getCurrentConfiguration(), new Path(statisticsDir),",
          "136:                     resultCuboidHLLMap, 1, origSegStatsReader.getSourceRowCount());",
          "137:             FSDataInputStream is = fs.open(statisticsFile);",
          "138:             ResourceStore.getStore(config).putBigResource(resKey, is, System.currentTimeMillis());",
          "140:             toUpdateSeg.setStatus(SegmentStatusEnum.READY_PENDING);",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "108:             }",
          "109:         }",
          "113:         toUpdateSeg.setLastBuildTime(System.currentTimeMillis());",
          "116:         cubeManager.updateCube(update);",
          "117:     }",
          "121:         CubeManager cubeManager = CubeManager.getInstance(config);",
          "122:         CubeInstance currentInstanceCopy = cubeManager.getCubeByUuid(cubeId).latestCopyForWrite();",
          "",
          "[Removed Lines]",
          "111:         logger.info(\"Promoting cube {}, new segment {}, to remove segments {}\", currentInstanceCopy, toUpdateSeg, toRemoveSegs);",
          "114:         update.setToRemoveSegs(toRemoveSegs.toArray(new CubeSegment[0]))",
          "115:                 .setToUpdateSegs(toUpdateSeg);",
          "119:     public static void updateMetadataAfterMerge(String cubeId, String segmentId,",
          "120:                                                 KylinConfig config) throws IOException {",
          "",
          "[Added Lines]",
          "153:         logger.info(\"Promoting cube {}, new segment {}, to remove segments {}\", currentInstanceCopy, toUpdateSeg,",
          "154:                 toRemoveSegs);",
          "157:         update.setToRemoveSegs(toRemoveSegs.toArray(new CubeSegment[0])).setToUpdateSegs(toUpdateSeg);",
          "161:     public static void updateMetadataAfterMerge(String cubeId, String segmentId, KylinConfig config)",
          "162:             throws IOException {",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "142:             update.setStatus(RealizationStatusEnum.READY);",
          "143:         }",
          "147:         toUpdateSegs.setLastBuildTime(System.currentTimeMillis());",
          "150:         cubeManager.updateCube(update);",
          "151:     }",
          "152: }",
          "",
          "[Removed Lines]",
          "145:         logger.info(\"Promoting cube {}, new segment {}, to remove segments {}\", currentInstanceCopy, toUpdateSegs, toRemoveSegs);",
          "148:         update.setToRemoveSegs(toRemoveSegs.toArray(new CubeSegment[0]))",
          "149:                 .setToUpdateSegs(toUpdateSegs);",
          "",
          "[Added Lines]",
          "187:         logger.info(\"Promoting cube {}, new segment {}, to remove segments {}\", currentInstanceCopy, toUpdateSegs,",
          "188:                 toRemoveSegs);",
          "191:         update.setToRemoveSegs(toRemoveSegs.toArray(new CubeSegment[0])).setToUpdateSegs(toUpdateSegs);",
          "195:     private static void addFromCubeStatsReader(CubeStatsReader cubeStatsReader, Map<Long, HLLCounter> cuboidHLLMap) {",
          "196:         for (Map.Entry<Long, HLLCounter> entry : cubeStatsReader.getCuboidRowHLLCounters().entrySet()) {",
          "197:             if (cuboidHLLMap.get(entry.getKey()) != null) {",
          "198:                 cuboidHLLMap.get(entry.getKey()).merge(entry.getValue());",
          "199:             } else {",
          "200:                 cuboidHLLMap.put(entry.getKey(), entry.getValue());",
          "201:             }",
          "202:         }",
          "203:     }",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/BuildJobInfos.scala||kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/BuildJobInfos.scala": [
          "File: kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/BuildJobInfos.scala -> kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/BuildJobInfos.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "39:   private val mergingSegments: java.util.List[SegmentInfo] = new util.LinkedList[SegmentInfo]",
          "42:   private val abnormalLayouts: util.Map[Long, util.List[String]] = new util.HashMap[Long, util.List[String]]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "42:   private var optimizingSegment: SegmentInfo = null",
          "44:   private val addCuboids: java.util.List[Long] = new util.LinkedList[Long]",
          "46:   private val reusedCuboids: java.util.Set[Long] = new util.HashSet[Long]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "82:     mergingSegments.addAll(segments)",
          "83:   }",
          "85:   def clearMergingSegments(): Unit = {",
          "86:     mergingSegments.clear()",
          "87:   }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "92:   def recordOptimizingSegment(segment: SegmentInfo): Unit = {",
          "93:     optimizingSegment = segment",
          "94:   }",
          "96:   def getOptimizingSegment(): SegmentInfo = {",
          "97:     optimizingSegment",
          "98:   }",
          "100:   def recordReusedCuboids(cuboids: util.Set[Long]): Unit = {",
          "101:     reusedCuboids.addAll(cuboids)",
          "102:   }",
          "104:   def getReusedCuboid(): util.Set[Long] = {",
          "105:     reusedCuboids",
          "106:   }",
          "108:   def clearReusedCuboids(): Unit = {",
          "109:     reusedCuboids.clear()",
          "110:   }",
          "112:   def clearAddCuboids(): Unit = {",
          "113:     addCuboids.clear()",
          "114:   }",
          "116:   def getAddCuboids: util.List[Long] = {",
          "117:     addCuboids",
          "118:   }",
          "120:   def recordAddCuboids(cuboidId: Long): Unit = {",
          "121:     addCuboids.add(cuboidId)",
          "122:   }",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CubeBuildJob.java||kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CubeBuildJob.java": [
          "File: kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CubeBuildJob.java -> kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CubeBuildJob.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "37: import org.apache.hadoop.fs.FSDataInputStream;",
          "38: import org.apache.kylin.common.persistence.ResourceStore;",
          "40: import org.apache.kylin.engine.mr.common.BatchConstants;",
          "41: import org.apache.kylin.engine.mr.common.CubeStatsWriter;",
          "42: import org.apache.kylin.engine.mr.common.StatisticsDecisionUtil;",
          "",
          "[Removed Lines]",
          "39: import org.apache.kylin.engine.mr.JobBuilderSupport;",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "111:         Preconditions.checkArgument(segmentIds.size() == 1, \"Build one segment in one time.\");",
          "113:         String firstSegmentId = segmentIds.iterator().next();",
          "116:         cubeManager = CubeManager.getInstance(config);",
          "118:         CubeSegment newSegment = cubeInstance.getSegmentById(firstSegmentId);",
          "119:         SpanningTree spanningTree;",
          "120:         ParentSourceChooser sourceChooser;",
          "",
          "[Removed Lines]",
          "114:         String cubeName = getParam(MetadataConstants.P_CUBE_ID);",
          "115:         SegmentInfo seg = ManagerHub.getSegmentInfo(config, cubeName, firstSegmentId);",
          "117:         cubeInstance = cubeManager.getCubeByUuid(cubeName);",
          "",
          "[Added Lines]",
          "113:         String cubeId = getParam(MetadataConstants.P_CUBE_ID);",
          "114:         SegmentInfo seg = ManagerHub.getSegmentInfo(config, cubeId, firstSegmentId);",
          "116:         cubeInstance = cubeManager.getCubeByUuid(cubeId);",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "128:             long startMills = System.currentTimeMillis();",
          "129:             spanningTree = new ForestSpanningTree(JavaConversions.asJavaCollection(seg.toBuildLayouts()));",
          "131:             sourceChooser.setNeedStatistics();",
          "132:             sourceChooser.decideFlatTableSource(null);",
          "133:             Map<Long, HLLCounter> hllMap = new HashMap<>();",
          "",
          "[Removed Lines]",
          "130:             sourceChooser = new ParentSourceChooser(spanningTree, seg, jobId, ss, config, false);",
          "",
          "[Added Lines]",
          "129:             sourceChooser = new ParentSourceChooser(spanningTree, seg, newSegment, jobId, ss, config, false);",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "140:             String jobTmpDir = config.getJobTmpDir(project) + \"/\" + jobId;",
          "142:             Optional<HLLCounter> hll = hllMap.values().stream().max(Comparator.comparingLong(HLLCounter::getCountEstimate));",
          "143:             long rc = hll.map(HLLCounter::getCountEstimate).orElse(1L);",
          "144:             CubeStatsWriter.writeCuboidStatistics(HadoopUtil.getCurrentConfiguration(), statisticsDir, hllMap, 1, rc);",
          "",
          "[Removed Lines]",
          "141:             Path statisticsDir = new Path(jobTmpDir + \"/\" + ResourceStore.CUBE_STATISTICS_ROOT + \"/\" + cubeName + \"/\" + firstSegmentId + \"/\");",
          "",
          "[Added Lines]",
          "140:             Path statisticsDir = new Path(jobTmpDir + \"/\" + ResourceStore.CUBE_STATISTICS_ROOT + \"/\" + cubeId + \"/\" + firstSegmentId + \"/\");",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "164:         try {",
          "166:             for (String segId : segmentIds) {",
          "168:                 spanningTree = new ForestSpanningTree(",
          "169:                         JavaConversions.asJavaCollection(seg.toBuildLayouts()));",
          "170:                 logger.info(\"There are {} cuboids to be built in segment {}.\",",
          "",
          "[Removed Lines]",
          "167:                 seg = ManagerHub.getSegmentInfo(config, cubeName, segId);",
          "",
          "[Added Lines]",
          "166:                 seg = ManagerHub.getSegmentInfo(config, cubeId, segId);",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "175:                 }",
          "179:                 sourceChooser.decideSources();",
          "180:                 NBuildSourceInfo buildFromFlatTable = sourceChooser.flatTableSource();",
          "181:                 Map<Long, NBuildSourceInfo> buildFromLayouts = sourceChooser.reuseSources();",
          "",
          "[Removed Lines]",
          "178:                 sourceChooser = new ParentSourceChooser(spanningTree, seg, jobId, ss, config, true);",
          "",
          "[Added Lines]",
          "177:                 sourceChooser = new ParentSourceChooser(spanningTree, seg, newSegment, jobId, ss, config, true);",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CuboidStatisticsJob.scala||kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CuboidStatisticsJob.scala": [
          "File: kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CuboidStatisticsJob.scala -> kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CuboidStatisticsJob.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "46:     l",
          "47:   }",
          "48: }",
          "50: class CuboidStatisticsJob(ids: Array[Long], rkc: Int) extends Serializable {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "49:   def statistics(inputDs: Dataset[Row], seg: SegmentInfo, layoutIds: Array[Long]): Array[(Long, AggInfo)] = {",
          "50:     val res = inputDs.rdd.repartition(inputDs.sparkSession.sparkContext.defaultParallelism)",
          "51:       .mapPartitions(new CuboidStatisticsJob(layoutIds, seg.allColumns.count(c => c.rowKey)).statisticsWithinPartition)",
          "52:     val l = res.map(a => (a.key, a)).reduceByKey((a, b) => a.merge(b)).collect()",
          "53:     l",
          "54:   }",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/LogJobInfoUtils.scala||kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/LogJobInfoUtils.scala": [
          "File: kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/LogJobInfoUtils.scala -> kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/LogJobInfoUtils.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "72:      \"\"\".stripMargin",
          "73:   }",
          "75:   def dfMergeJobInfo: String = {",
          "76:     s\"\"\"",
          "77:        |==========================[MERGE CUBE]===============================",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "75:   def resourceDetectBeforeOptimizeJobInfo: String = {",
          "76:     s\"\"\"",
          "77:        |==========================[RESOURCE DETECT BEFORE OPTIMIZE]===============================",
          "78:        |optimizing segment : ${infos.getOptimizingSegment()}",
          "79:        |spark plans : ${infos.getSparkPlans}",
          "80:        |==========================[RESOURCE DETECT BEFORE OPTIMIZE]===============================",
          "81:      \"\"\".stripMargin",
          "82:   }",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "86:        |==========================[MERGE CUBE]===============================",
          "87:      \"\"\".stripMargin",
          "88:   }",
          "89: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "99:   def filterRecommendCuboidJobInfo: String = {",
          "100:     s\"\"\"",
          "101:        |==========================[FILTER RECOMMEND CUBOID]===============================",
          "102:        |copy cuboids : ${infos.getReusedCuboid}",
          "103:        |==========================[[FILTER RECOMMEND CUBOID]===============================",
          "104:      \"\"\".stripMargin",
          "105:   }",
          "107:   def dfOptimizeJobInfo: String = {",
          "108:     s\"\"\"",
          "109:        |==========================[BUILD CUBE]===============================",
          "110:        |auto spark config :${infos.getAutoSparkConfs}",
          "111:        |wait time: ${infos.waitTime}",
          "112:        |build time: ${infos.buildTime}",
          "113:        |add cuboids: ${infos.getAddCuboids}",
          "114:        |abnormal layouts : ${infos.getAbnormalLayouts}",
          "115:        |retry times : ${infos.getRetryTimes}",
          "116:        |job retry infos :",
          "117:        |  ${infos.getJobRetryInfos.asScala.map(_.toString).mkString(\"\\n\")}",
          "118:        |==========================[BUILD CUBE]===============================",
          "119:      \"\"\".stripMargin",
          "120:   }",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/ParentSourceChooser.scala||kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/ParentSourceChooser.scala": [
          "File: kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/ParentSourceChooser.scala -> kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/ParentSourceChooser.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "21: import org.apache.kylin.shaded.com.google.common.collect.Maps",
          "22: import org.apache.kylin.engine.spark.builder._",
          "23: import org.apache.kylin.common.KylinConfig",
          "24: import org.apache.kylin.engine.spark.builder.NBuildSourceInfo",
          "25: import org.apache.kylin.engine.spark.metadata.cube.model.{LayoutEntity, SpanningTree}",
          "26: import org.apache.kylin.engine.spark.metadata.SegmentInfo",
          "27: import org.apache.spark.internal.Logging",
          "28: import org.apache.spark.sql.functions.col",
          "29: import org.apache.spark.sql.{Dataset, Row, SaveMode, SparkSession}",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "24: import org.apache.kylin.cube.CubeSegment",
          "28: import org.apache.kylin.engine.spark.metadata.cube.PathManager",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "34: class ParentSourceChooser(",
          "35:   toBuildTree: SpanningTree,",
          "37:   jobId: String,",
          "38:   ss: SparkSession,",
          "39:   config: KylinConfig,",
          "",
          "[Removed Lines]",
          "36:   var seg: SegmentInfo,",
          "",
          "[Added Lines]",
          "38:   var segInfo: SegmentInfo,",
          "39:   var segment: CubeSegment,",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "57:   def setNeedStatistics(): Unit =",
          "58:     needStatistics = true",
          "",
          "[Removed Lines]",
          "55:     MetadataConverter.getCubeDesc(seg.getCube),",
          "",
          "[Added Lines]",
          "58:     MetadataConverter.getCubeDesc(segInfo.getCube),",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "62:   def decideSources(): Unit = {",
          "63:     toBuildTree.getRootIndexEntities.asScala.foreach { entity =>",
          "65:       if (parentLayout != null) {",
          "66:         decideParentLayoutSource(entity, parentLayout)",
          "67:       } else {",
          "",
          "[Removed Lines]",
          "64:       val parentLayout = CuboidLayoutChooser.selectLayoutForBuild(seg, entity)",
          "",
          "[Added Lines]",
          "67:       val parentLayout = CuboidLayoutChooser.selectLayoutForBuild(segInfo, entity)",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "80:         builder.checkDupKey()",
          "82:       }",
          "83:       flatTableSource = getFlatTable",
          "86:       if (aggInfo == null && needStatistics) {",
          "87:         val startMs = System.currentTimeMillis()",
          "88:         logInfo(\"Sampling start ...\")",
          "89:         val coreDs = flatTableSource.getFlatTableDS.select(rowKeyColumns.head, rowKeyColumns.tail: _*)",
          "91:         logInfo(\"Sampling finished and cost \" + (System.currentTimeMillis() - startMs)/1000 + \" s .\")",
          "92:         val statisticsStr = aggInfo.sortBy(x => x._1).map(x => x._1 + \":\" + x._2.cuboid.counter.getCountEstimate).mkString(\", \")",
          "93:         logInfo(\"Cuboid Statistics results : \\t\" + statisticsStr)",
          "",
          "[Removed Lines]",
          "79:         val builder = new CubeSnapshotBuilder(seg, ss)",
          "81:         seg = builder.buildSnapshot",
          "85:       val rowKeyColumns: Seq[String] = seg.allColumns.filter(c => c.rowKey).map(c => c.id.toString)",
          "90:         aggInfo = CuboidStatisticsJob.statistics(coreDs, seg)",
          "",
          "[Added Lines]",
          "82:         val builder = new CubeSnapshotBuilder(segInfo, ss)",
          "84:         segInfo = builder.buildSnapshot",
          "88:       val rowKeyColumns: Seq[String] = segInfo.allColumns.filter(c => c.rowKey).map(c => c.id.toString)",
          "93:         aggInfo = CuboidStatisticsJob.statistics(coreDs, segInfo)",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "131:         }.toSeq",
          "133:         df.select(allUsedCols.map(col): _*)",
          "135:         ss.sparkContext.setJobDescription(\"Persist flat table.\")",
          "136:         df.write.mode(SaveMode.Overwrite).parquet(path)",
          "137:         logInfo(s\"Persist flat table into:$path. Selected cols in table are $allUsedCols.\")",
          "",
          "[Removed Lines]",
          "134:         path = s\"${config.getJobTmpFlatTableDir(seg.project, jobId)}\"",
          "",
          "[Added Lines]",
          "137:         path = s\"${config.getJobTmpFlatTableDir(segInfo.project, jobId)}\"",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "165:   private def getSourceFromLayout(layout: LayoutEntity): NBuildSourceInfo = {",
          "166:     val buildSource = new NBuildSourceInfo",
          "168:     buildSource.setSparkSession(ss)",
          "169:     buildSource.setLayoutId(layout.getId)",
          "170:     buildSource.setLayout(layout)",
          "",
          "[Removed Lines]",
          "167:     buildSource.setParentStoragePath(\"NSparkCubingUtil.getStoragePath(dataCuboid)\")",
          "",
          "[Added Lines]",
          "170:     buildSource.setParentStoragePath(NSparkCubingUtil.getStoragePath(segment, layout.getId))",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "181:     sourceInfo.setLayoutId(ParentSourceChooser.FLAT_TABLE_FLAG)",
          "186:     val afterJoin: Dataset[Row] = flatTable.generateDataset(needEncoding, true)",
          "187:     sourceInfo.setFlatTableDS(afterJoin)",
          "",
          "[Removed Lines]",
          "185:     val flatTable = new CreateFlatTable(seg, toBuildTree, ss, sourceInfo)",
          "",
          "[Added Lines]",
          "188:     val flatTable = new CreateFlatTable(segInfo, toBuildTree, ss, sourceInfo)",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/ResourceDetectBeforeCubingJob.java||kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/ResourceDetectBeforeCubingJob.java": [
          "File: kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/ResourceDetectBeforeCubingJob.java -> kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/ResourceDetectBeforeCubingJob.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "19: package org.apache.kylin.engine.spark.job;",
          "21: import org.apache.kylin.shaded.com.google.common.collect.Maps;",
          "22: import org.apache.kylin.shaded.com.google.common.collect.Sets;",
          "23: import org.apache.kylin.engine.spark.application.SparkApplication;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "21: import org.apache.kylin.cube.CubeInstance;",
          "22: import org.apache.kylin.cube.CubeManager;",
          "23: import org.apache.kylin.cube.CubeSegment;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "52:     protected void doExecute() throws Exception {",
          "53:         logger.info(\"Start detect resource before cube.\");",
          "54:         Set<String> segmentIds = Sets.newHashSet(StringUtils.split(getParam(MetadataConstants.P_SEGMENT_IDS)));",
          "55:         for (String segId : segmentIds) {",
          "56:             SegmentInfo seg = ManagerHub.getSegmentInfo(config, getParam(MetadataConstants.P_CUBE_ID), segId);",
          "57:             spanningTree = new ForestSpanningTree(JavaConversions.asJavaCollection(seg.toBuildLayouts()));",
          "58:             ResourceDetectUtils.write(new Path(config.getJobTmpShareDir(project, jobId), ResourceDetectUtils.countDistinctSuffix()),",
          "59:                     ResourceDetectUtils.findCountDistinctMeasure(JavaConversions.asJavaCollection(seg.toBuildLayouts())));",
          "61:             datasetChooser.decideSources();",
          "62:             NBuildSourceInfo buildFromFlatTable = datasetChooser.flatTableSource();",
          "63:             if (buildFromFlatTable != null) {",
          "",
          "[Removed Lines]",
          "60:             ParentSourceChooser datasetChooser = new ParentSourceChooser(spanningTree, seg, jobId, ss, config, false);",
          "",
          "[Added Lines]",
          "58:         CubeManager cubeManager = CubeManager.getInstance(config);",
          "59:         CubeInstance cubeInstance = cubeManager.getCubeByUuid(getParam(MetadataConstants.P_CUBE_ID));",
          "62:             CubeSegment segment = cubeInstance.getSegmentById(segId);",
          "66:             ParentSourceChooser datasetChooser = new ParentSourceChooser(spanningTree, seg, segment, jobId, ss, config, false);",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-metadata/src/main/java/org/apache/kylin/engine/spark/metadata/cube/ManagerHub.java||kylin-spark-project/kylin-spark-metadata/src/main/java/org/apache/kylin/engine/spark/metadata/cube/ManagerHub.java": [
          "File: kylin-spark-project/kylin-spark-metadata/src/main/java/org/apache/kylin/engine/spark/metadata/cube/ManagerHub.java -> kylin-spark-project/kylin-spark-metadata/src/main/java/org/apache/kylin/engine/spark/metadata/cube/ManagerHub.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "22: import org.apache.kylin.cube.CubeInstance;",
          "23: import org.apache.kylin.cube.CubeManager;",
          "24: import org.apache.kylin.cube.CubeSegment;",
          "25: import org.apache.kylin.engine.spark.metadata.MetadataConverter;",
          "26: import org.apache.kylin.engine.spark.metadata.SegmentInfo;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "25: import org.apache.kylin.cube.cuboid.CuboidModeEnum;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "32:     private ManagerHub() {",
          "33:     }",
          "37:         CubeSegment segment = cubeInstance.getSegmentById(segmentId);",
          "40:     }",
          "42:     public static CubeInstance updateSegment(KylinConfig kylinConfig, SegmentInfo segmentInfo) throws IOException {",
          "",
          "[Removed Lines]",
          "35:     public static SegmentInfo getSegmentInfo(KylinConfig kylinConfig, String cubeName, String segmentId) {",
          "36:         CubeInstance cubeInstance = CubeManager.getInstance(kylinConfig).getCubeByUuid(cubeName);",
          "38:         return MetadataConverter.getSegmentInfo(CubeManager.getInstance(kylinConfig).getCubeByUuid(cubeName),",
          "39:                 segment.getUuid(), segment.getName(), segment.getStorageLocationIdentifier());",
          "",
          "[Added Lines]",
          "36:     public static SegmentInfo getSegmentInfo(KylinConfig kylinConfig, String cubeId, String segmentId) {",
          "37:         return getSegmentInfo(kylinConfig, cubeId, segmentId, CuboidModeEnum.CURRENT);",
          "38:     }",
          "40:     public static SegmentInfo getSegmentInfo(KylinConfig kylinConfig, String cubeId, String segmentId, CuboidModeEnum cuboidMode) {",
          "41:         CubeInstance cubeInstance = CubeManager.getInstance(kylinConfig).getCubeByUuid(cubeId);",
          "43:         return MetadataConverter.getSegmentInfo(CubeManager.getInstance(kylinConfig).getCubeByUuid(cubeId),",
          "44:                 segment.getUuid(), segment.getName(), segment.getStorageLocationIdentifier(), cuboidMode);",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-metadata/src/main/scala/org/apache/kylin/engine/spark/metadata/MetaData.scala||kylin-spark-project/kylin-spark-metadata/src/main/scala/org/apache/kylin/engine/spark/metadata/MetaData.scala": [
          "File: kylin-spark-project/kylin-spark-metadata/src/main/scala/org/apache/kylin/engine/spark/metadata/MetaData.scala -> kylin-spark-project/kylin-spark-metadata/src/main/scala/org/apache/kylin/engine/spark/metadata/MetaData.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "118:     toBuildLayouts.remove(layoutEntity)",
          "119:   }",
          "121:   def updateSnapshot(tableInfo: Map[String, String]): Unit = {",
          "122:     snapshotInfo = tableInfo",
          "123:   }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "121:   def removeLayout(layoutId: Long): Unit = {",
          "122:     toBuildLayouts = toBuildLayouts.filter(layout => !layout.getId.equals(layoutId))",
          "123:   }",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-metadata/src/main/scala/org/apache/kylin/engine/spark/metadata/MetadataConverter.scala||kylin-spark-project/kylin-spark-metadata/src/main/scala/org/apache/kylin/engine/spark/metadata/MetadataConverter.scala": [
          "File: kylin-spark-project/kylin-spark-metadata/src/main/scala/org/apache/kylin/engine/spark/metadata/MetadataConverter.scala -> kylin-spark-project/kylin-spark-metadata/src/main/scala/org/apache/kylin/engine/spark/metadata/MetadataConverter.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "23: import java.{lang, util}",
          "25: import org.apache.commons.lang.StringUtils",
          "27: import org.apache.kylin.cube.{CubeInstance, CubeSegment, CubeUpdate}",
          "28: import org.apache.kylin.engine.spark.metadata.cube.BitUtils",
          "29: import org.apache.kylin.engine.spark.metadata.cube.model.LayoutEntity",
          "",
          "[Removed Lines]",
          "26: import org.apache.kylin.cube.cuboid.Cuboid",
          "",
          "[Added Lines]",
          "26: import org.apache.kylin.cube.cuboid.{Cuboid, CuboidModeEnum}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "38: object MetadataConverter {",
          "39:   def getSegmentInfo(cubeInstance: CubeInstance, segmentId: String, segmentName: String, identifier: String): SegmentInfo = {",
          "40:     val (allColumnDesc, allRowKeyCols) = extractAllColumnDesc(cubeInstance)",
          "42:     val dictColumn = measure.values.filter(_.returnType.dataType.equals(\"bitmap\"))",
          "43:       .map(_.pra.head).toSet",
          "44:     SegmentInfo(segmentId, segmentName, identifier, cubeInstance.getProject, cubeInstance.getConfig, extractFactTable(cubeInstance),",
          "",
          "[Removed Lines]",
          "41:     val (layoutEntities, measure) = extractEntityAndMeasures(cubeInstance)",
          "",
          "[Added Lines]",
          "40:     getSegmentInfo(cubeInstance, segmentId, segmentName, identifier, CuboidModeEnum.CURRENT)",
          "41:   }",
          "43:   def getSegmentInfo(cubeInstance: CubeInstance, segmentId: String, segmentName: String, identifier: String, cuboidMode: CuboidModeEnum): SegmentInfo = {",
          "45:     val (layoutEntities, measure) = extractEntityAndMeasures(cubeInstance, cuboidMode)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "122:   }",
          "124:   def extractEntityAndMeasures(cubeInstance: CubeInstance): (List[LayoutEntity], Map[Integer, FunctionDesc]) = {",
          "125:     val (columnIndexes, shardByColumnsId, idToColumnMap, measureId) = genIDToColumnMap(cubeInstance)",
          "128:       .asScala",
          "129:       .map { long =>",
          "130:         genLayoutEntity(columnIndexes, shardByColumnsId, idToColumnMap, measureId, long)",
          "",
          "[Removed Lines]",
          "126:     (cubeInstance.getCuboidScheduler",
          "127:       .getAllCuboidIds",
          "",
          "[Added Lines]",
          "129:     extractEntityAndMeasures(cubeInstance, CuboidModeEnum.CURRENT)",
          "130:   }",
          "132:   def extractEntityAndMeasures(cubeInstance: CubeInstance, cuboidMode: CuboidModeEnum): (List[LayoutEntity], Map[Integer, FunctionDesc]) = {",
          "134:     (cubeInstance.getCuboidsByMode(cuboidMode)",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "202:   }",
          "204:   def extractEntityList2JavaList(cubeInstance: CubeInstance): java.util.List[LayoutEntity] = {",
          "206:   }",
          "208:   private def toColumnDesc(ref: TblColRef, index: Int = -1, rowKey: Boolean = false) = {",
          "",
          "[Removed Lines]",
          "205:     extractEntityAndMeasures(cubeInstance)._1.asJava",
          "",
          "[Added Lines]",
          "212:     extractEntityList2JavaList(cubeInstance, CuboidModeEnum.CURRENT)",
          "213:   }",
          "215:   def extractEntityList2JavaList(cubeInstance: CubeInstance, cuboidMode: CuboidModeEnum): java.util.List[LayoutEntity] = {",
          "216:     extractEntityAndMeasures(cubeInstance, cuboidMode)._1.asJava",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-test/src/test/java/org/apache/kylin/engine/spark2/NOptimizeJobTest.java||kylin-spark-project/kylin-spark-test/src/test/java/org/apache/kylin/engine/spark2/NOptimizeJobTest.java": [
          "File: kylin-spark-project/kylin-spark-test/src/test/java/org/apache/kylin/engine/spark2/NOptimizeJobTest.java -> kylin-spark-project/kylin-spark-test/src/test/java/org/apache/kylin/engine/spark2/NOptimizeJobTest.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "19: package org.apache.kylin.engine.spark2;",
          "21: import org.apache.hadoop.fs.FileSystem;",
          "22: import org.apache.hadoop.fs.Path;",
          "23: import org.apache.kylin.common.KylinConfig;",
          "24: import org.apache.kylin.common.util.HadoopUtil;",
          "25: import org.apache.kylin.cube.CubeInstance;",
          "26: import org.apache.kylin.cube.CubeManager;",
          "27: import org.apache.kylin.cube.CubeSegment;",
          "28: import org.apache.kylin.engine.mr.common.CubeStatsReader;",
          "29: import org.apache.kylin.engine.spark.LocalWithSparkSessionTest;",
          "30: import org.apache.kylin.engine.spark.job.NSparkBatchOptimizeJobCheckpointBuilder;",
          "31: import org.apache.kylin.engine.spark.job.NSparkOptimizingJob;",
          "32: import org.apache.kylin.engine.spark.metadata.cube.PathManager;",
          "33: import org.apache.kylin.job.exception.SchedulerException;",
          "34: import org.apache.kylin.job.execution.CheckpointExecutable;",
          "35: import org.apache.kylin.job.execution.ExecutableManager;",
          "36: import org.apache.kylin.job.execution.ExecutableState;",
          "37: import org.apache.kylin.metadata.model.SegmentRange;",
          "38: import org.apache.kylin.metadata.model.SegmentStatusEnum;",
          "39: import org.apache.kylin.metadata.realization.RealizationType;",
          "40: import org.apache.kylin.query.routing.Candidate;",
          "41: import org.apache.kylin.shaded.com.google.common.collect.Maps;",
          "42: import org.junit.Assert;",
          "43: import org.junit.Test;",
          "45: import java.util.HashSet;",
          "46: import java.util.Map;",
          "47: import java.util.Set;",
          "49: public class NOptimizeJobTest extends LocalWithSparkSessionTest {",
          "50:     protected KylinConfig config;",
          "51:     protected CubeManager cubeMgr;",
          "52:     protected ExecutableManager execMgr;",
          "54:     private final String CUBE_NAME = \"ci_left_join_cube\";",
          "55:     private final long CUBOID_ADD = 1048575L;",
          "56:     private final long CUBOID_DELETE = 14336L;",
          "58:     @Override",
          "59:     public void setup() throws SchedulerException {",
          "60:         super.setup();",
          "61:         overwriteSystemProp(\"kylin.env\", \"UT\");",
          "62:         overwriteSystemProp(\"isDeveloperMode\", \"true\");",
          "63:         overwriteSystemProp(\"kylin.engine.segment-statistics-enabled\", \"true\");",
          "64:         Map<RealizationType, Integer> priorities = Maps.newHashMap();",
          "65:         priorities.put(RealizationType.HYBRID, 0);",
          "66:         priorities.put(RealizationType.CUBE, 0);",
          "67:         Candidate.setPriorities(priorities);",
          "68:         config = KylinConfig.getInstanceFromEnv();",
          "69:         cubeMgr = CubeManager.getInstance(config);",
          "70:         execMgr = ExecutableManager.getInstance(config);",
          "71:     }",
          "73:     @Override",
          "74:     public void after() {",
          "75:         super.after();",
          "76:     }",
          "78:     @Test",
          "79:     public void verifyOptimizeJob() throws Exception {",
          "80:         CubeInstance cube = cubeMgr.reloadCube(CUBE_NAME);",
          "81:         Set<Long> recommendCuboids = new HashSet<>();",
          "82:         recommendCuboids.addAll(cube.getCuboidScheduler().getAllCuboidIds());",
          "83:         recommendCuboids.add(CUBOID_ADD);",
          "84:         recommendCuboids.remove(CUBOID_DELETE);",
          "86:         buildSegments(CUBE_NAME, new SegmentRange.TSRange(dateToLong(\"2012-01-01\"), dateToLong(\"2012-02-01\")),",
          "87:                 new SegmentRange.TSRange(dateToLong(\"2012-02-01\"), dateToLong(\"2012-03-01\")));",
          "90:         CubeSegment[] optimizeSegments = cubeMgr.optimizeSegments(cube, recommendCuboids);",
          "91:         for (CubeSegment segment : optimizeSegments) {",
          "92:             ExecutableState result = optimizeSegment(segment);",
          "93:             Assert.assertEquals(ExecutableState.SUCCEED, result);",
          "94:         }",
          "96:         cube = cubeMgr.reloadCube(CUBE_NAME);",
          "98:         Assert.assertEquals(4, cube.getSegments().size());",
          "99:         Assert.assertEquals(2, cube.getSegments(SegmentStatusEnum.READY_PENDING).size());",
          "100:         Assert.assertEquals(2, cube.getSegments(SegmentStatusEnum.READY).size());",
          "103:         executeCheckPoint(cube);",
          "105:         cube = cubeMgr.reloadCube(CUBE_NAME);",
          "108:         Assert.assertEquals(2, cube.getSegments().size());",
          "109:         Assert.assertEquals(2, cube.getSegments(SegmentStatusEnum.READY).size());",
          "110:         FileSystem fs = HadoopUtil.getWorkingFileSystem();",
          "111:         for (CubeSegment segment : cube.getSegments()) {",
          "112:             Assert.assertEquals(SegmentStatusEnum.READY, segment.getStatus());",
          "113:             CubeStatsReader segStatsReader = new CubeStatsReader(segment, config);",
          "114:             Assert.assertEquals(recommendCuboids, segStatsReader.getCuboidRowHLLCounters().keySet());",
          "115:             String cuboidPath = PathManager.getSegmentParquetStoragePath(cube, segment.getName(), segment.getStorageLocationIdentifier());",
          "116:             Assert.assertTrue(fs.exists(new Path(cuboidPath)));",
          "117:             Assert.assertTrue(fs.exists(new Path(cuboidPath + \"/\" + CUBOID_ADD)));",
          "118:             Assert.assertFalse(fs.exists(new Path(cuboidPath + \"/\" + CUBOID_DELETE)));",
          "120:         }",
          "121:         Assert.assertEquals(recommendCuboids, cube.getCuboidScheduler().getAllCuboidIds());",
          "122:     }",
          "124:     public void buildSegments(String cubeName, SegmentRange.TSRange... toBuildRanges) throws Exception {",
          "125:         Assert.assertTrue(config.getHdfsWorkingDirectory().startsWith(\"file:\"));",
          "128:         cleanupSegments(cubeName);",
          "130:         ExecutableState state;",
          "131:         for (SegmentRange.TSRange toBuildRange : toBuildRanges) {",
          "132:             state = buildCuboid(cubeName, toBuildRange);",
          "133:             Assert.assertEquals(ExecutableState.SUCCEED, state);",
          "134:         }",
          "135:     }",
          "137:     protected ExecutableState optimizeSegment(CubeSegment segment) throws Exception {",
          "138:         NSparkOptimizingJob optimizeJob = NSparkOptimizingJob.optimize(segment, \"ADMIN\");",
          "139:         execMgr.addJob(optimizeJob);",
          "140:         ExecutableState result = wait(optimizeJob);",
          "141:         checkJobTmpPathDeleted(config, optimizeJob);",
          "142:         return result;",
          "143:     }",
          "145:     protected ExecutableState executeCheckPoint(CubeInstance cubeInstance) throws Exception {",
          "146:         CheckpointExecutable checkPointJob = new NSparkBatchOptimizeJobCheckpointBuilder(cubeInstance, \"ADMIN\").build();",
          "147:         execMgr.addJob(checkPointJob);",
          "148:         ExecutableState result = wait(checkPointJob);",
          "149:         return result;",
          "150:     }",
          "151: }",
          "",
          "---------------"
        ],
        "server-base/src/main/java/org/apache/kylin/rest/controller/CubeController.java||server-base/src/main/java/org/apache/kylin/rest/controller/CubeController.java": [
          "File: server-base/src/main/java/org/apache/kylin/rest/controller/CubeController.java -> server-base/src/main/java/org/apache/kylin/rest/controller/CubeController.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "27: import java.util.Locale;",
          "28: import java.util.Map;",
          "29: import java.util.Set;",
          "31: import javax.servlet.http.HttpServletResponse;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "30: import java.util.HashSet;",
          "",
          "---------------"
        ],
        "server-base/src/main/java/org/apache/kylin/rest/service/JobService.java||server-base/src/main/java/org/apache/kylin/rest/service/JobService.java": [
          "File: server-base/src/main/java/org/apache/kylin/rest/service/JobService.java -> server-base/src/main/java/org/apache/kylin/rest/service/JobService.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "44: import org.apache.kylin.cube.CubeSegment;",
          "45: import org.apache.kylin.cube.model.CubeBuildTypeEnum;",
          "46: import org.apache.kylin.engine.EngineFactory;",
          "48: import org.apache.kylin.engine.mr.CubingJob;",
          "49: import org.apache.kylin.engine.mr.LookupSnapshotBuildJob;",
          "50: import org.apache.kylin.engine.mr.common.CubeJobLockUtil;",
          "51: import org.apache.kylin.engine.mr.common.JobInfoConverter;",
          "52: import org.apache.kylin.engine.mr.steps.CubingExecutableUtil;",
          "53: import org.apache.kylin.engine.spark.job.NSparkCubingJob;",
          "54: import org.apache.kylin.engine.spark.metadata.cube.source.SourceFactory;",
          "55: import org.apache.kylin.job.JobInstance;",
          "",
          "[Removed Lines]",
          "47: import org.apache.kylin.engine.mr.BatchOptimizeJobCheckpointBuilder;",
          "",
          "[Added Lines]",
          "52: import org.apache.kylin.engine.spark.job.NSparkBatchOptimizeJobCheckpointBuilder;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "331:             }",
          "335:             checkpointJob.addTaskListForCheck(optimizeJobList);",
          "337:             getExecutableManager().addJob(checkpointJob);",
          "",
          "[Removed Lines]",
          "334:             CheckpointExecutable checkpointJob = new BatchOptimizeJobCheckpointBuilder(cube, submitter).build();",
          "",
          "[Added Lines]",
          "334:             CheckpointExecutable checkpointJob = new NSparkBatchOptimizeJobCheckpointBuilder(cube, submitter).build();",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "502:     public String getJobStepOutput(String jobId, String stepId) {",
          "503:         ExecutableManager executableManager = getExecutableManager();",
          "504:         return executableManager.getOutputFromHDFSByJobId(jobId, stepId).getVerboseMsg();",
          "505:     }",
          "507:     public String getAllJobStepOutput(String jobId, String stepId) {",
          "508:         ExecutableManager executableManager = getExecutableManager();",
          "509:         return executableManager.getOutputFromHDFSByJobId(jobId, stepId, Integer.MAX_VALUE).getVerboseMsg();",
          "510:     }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "504:         AbstractExecutable job = executableManager.getJob(jobId);",
          "505:         if (job instanceof CheckpointExecutable) {",
          "506:             return executableManager.getOutput(stepId).getVerboseMsg();",
          "507:         }",
          "513:         AbstractExecutable job = executableManager.getJob(jobId);",
          "514:         if (job instanceof CheckpointExecutable) {",
          "515:             return executableManager.getOutput(stepId).getVerboseMsg();",
          "516:         }",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "666:         if (null == job.getRelatedCube() || null == getCubeManager().getCube(job.getRelatedCube())",
          "667:                 || null == job.getRelatedSegment()) {",
          "668:             getExecutableManager().discardJob(job.getId());",
          "669:         }",
          "671:         logger.info(\"Cancel job [\" + job.getId() + \"] trigger by \"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "677:             return;",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "fd4a472e348589ea9c6e0c46edcc4b1a06b0ee38",
      "candidate_info": {
        "commit_hash": "fd4a472e348589ea9c6e0c46edcc4b1a06b0ee38",
        "repo": "apache/kylin",
        "commit_url": "https://github.com/apache/kylin/commit/fd4a472e348589ea9c6e0c46edcc4b1a06b0ee38",
        "files": [
          "core-cube/src/main/java/org/apache/kylin/gridtable/GTAggregateScanner.java",
          "server-base/src/main/java/org/apache/kylin/rest/util/ControllerSplitter.java",
          "source-hive/src/main/java/org/apache/kylin/source/hive/cardinality/HiveColumnCardinalityUpdateJob.java"
        ],
        "message": "minor, fix sonar reported bugs (#1875)\n\n* minor, fix sonar reported bugs\n\n* m",
        "before_after_code_files": [
          "core-cube/src/main/java/org/apache/kylin/gridtable/GTAggregateScanner.java||core-cube/src/main/java/org/apache/kylin/gridtable/GTAggregateScanner.java",
          "server-base/src/main/java/org/apache/kylin/rest/util/ControllerSplitter.java||server-base/src/main/java/org/apache/kylin/rest/util/ControllerSplitter.java",
          "source-hive/src/main/java/org/apache/kylin/source/hive/cardinality/HiveColumnCardinalityUpdateJob.java||source-hive/src/main/java/org/apache/kylin/source/hive/cardinality/HiveColumnCardinalityUpdateJob.java"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/kylin/pull/1893",
          "https://github.com/apache/kylin/pull/2018",
          "https://github.com/apache/kylin/pull/2125",
          "https://github.com/apache/kylin/pull/2033",
          "https://github.com/apache/kylin/pull/2112",
          "https://github.com/apache/kylin/pull/2115",
          "https://github.com/apache/kylin/pull/1913",
          "https://github.com/apache/kylin/pull/2135"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "core-cube/src/main/java/org/apache/kylin/gridtable/GTAggregateScanner.java||core-cube/src/main/java/org/apache/kylin/gridtable/GTAggregateScanner.java": [
          "File: core-cube/src/main/java/org/apache/kylin/gridtable/GTAggregateScanner.java -> core-cube/src/main/java/org/apache/kylin/gridtable/GTAggregateScanner.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "723:             public void spill() throws IOException {",
          "724:                 if (spillBuffer == null)",
          "725:                     return;",
          "733:                 logger.info(\"Spill buffer to disk, location: {}, size = {}.\", dumpedFile.getAbsolutePath(),",
          "734:                         dumpedFile.length());",
          "",
          "[Removed Lines]",
          "726:                 OutputStream ops = new FileOutputStream(dumpedFile);",
          "727:                 InputStream ips = new ByteArrayInputStream(spillBuffer);",
          "728:                 IOUtils.copy(ips, ops);",
          "729:                 spillBuffer = null;",
          "730:                 IOUtils.closeQuietly(ips);",
          "731:                 IOUtils.closeQuietly(ops);",
          "",
          "[Added Lines]",
          "726:                 try (OutputStream ops = new FileOutputStream(dumpedFile);",
          "727:                      InputStream ips = new ByteArrayInputStream(spillBuffer)) {",
          "728:                     IOUtils.copy(ips, ops);",
          "729:                     spillBuffer = null;",
          "730:                 }",
          "",
          "---------------"
        ],
        "server-base/src/main/java/org/apache/kylin/rest/util/ControllerSplitter.java||server-base/src/main/java/org/apache/kylin/rest/util/ControllerSplitter.java": [
          "File: server-base/src/main/java/org/apache/kylin/rest/util/ControllerSplitter.java -> server-base/src/main/java/org/apache/kylin/rest/util/ControllerSplitter.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "47:     private static void chopOff(File f, String annoPtn) throws IOException {",
          "49:         System.out.println(\"Processing \" + f);",
          "54:         List<String> outLines = new ArrayList<>(lines.size());",
          "56:         boolean del = false;",
          "",
          "[Removed Lines]",
          "51:         FileInputStream is = new FileInputStream(f);",
          "52:         List<String> lines = IOUtils.readLines(is, \"UTF-8\");",
          "53:         is.close();",
          "",
          "[Added Lines]",
          "50:         List<String> lines = new ArrayList<>(0);",
          "51:         try (FileInputStream is = new FileInputStream(f)) {",
          "52:            lines = IOUtils.readLines(is, \"UTF-8\");",
          "53:         }",
          "",
          "---------------"
        ],
        "source-hive/src/main/java/org/apache/kylin/source/hive/cardinality/HiveColumnCardinalityUpdateJob.java||source-hive/src/main/java/org/apache/kylin/source/hive/cardinality/HiveColumnCardinalityUpdateJob.java": [
          "File: source-hive/src/main/java/org/apache/kylin/source/hive/cardinality/HiveColumnCardinalityUpdateJob.java -> source-hive/src/main/java/org/apache/kylin/source/hive/cardinality/HiveColumnCardinalityUpdateJob.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "151:             CompressionCodec codec = factory.getCodec(item.getPath());",
          "152:             InputStream stream = null;",
          "161:             StringWriter writer = new StringWriter();",
          "163:             String raw = writer.toString();",
          "164:             for (String str : StringUtil.split(raw, \"\\n\")) {",
          "165:                 results.add(str);",
          "",
          "[Removed Lines]",
          "155:             if (codec != null) {",
          "156:                 stream = codec.createInputStream(fileSystem.open(item.getPath()));",
          "157:             } else {",
          "158:                 stream = fileSystem.open(item.getPath());",
          "159:             }",
          "162:             IOUtils.copy(stream, writer, \"UTF-8\");",
          "",
          "[Added Lines]",
          "154:             try {",
          "156:                 if (codec != null) {",
          "157:                     stream = codec.createInputStream(fileSystem.open(item.getPath()));",
          "158:                 } else {",
          "159:                     stream = fileSystem.open(item.getPath());",
          "160:                 }",
          "162:                 IOUtils.copy(stream, writer, \"UTF-8\");",
          "163:             } finally {",
          "164:                 if (stream != null) {",
          "165:                     stream.close();",
          "166:                 }",
          "167:             }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "588e08b465115a39810819102074c80161e6c790",
      "candidate_info": {
        "commit_hash": "588e08b465115a39810819102074c80161e6c790",
        "repo": "apache/kylin",
        "commit_url": "https://github.com/apache/kylin/commit/588e08b465115a39810819102074c80161e6c790",
        "files": [
          "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CubeBuildJob.java",
          "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CubeMergeJob.java",
          "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/utils/Repartitioner.java"
        ],
        "message": "KYLIN-4877 Use all dimension columns as sort columns when saving cuboid data",
        "before_after_code_files": [
          "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CubeBuildJob.java||kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CubeBuildJob.java",
          "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CubeMergeJob.java||kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CubeMergeJob.java",
          "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/utils/Repartitioner.java||kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/utils/Repartitioner.java"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/kylin/pull/1893",
          "https://github.com/apache/kylin/pull/2018",
          "https://github.com/apache/kylin/pull/2125",
          "https://github.com/apache/kylin/pull/2033",
          "https://github.com/apache/kylin/pull/2112",
          "https://github.com/apache/kylin/pull/2115",
          "https://github.com/apache/kylin/pull/1865",
          "https://github.com/apache/kylin/pull/1913",
          "https://github.com/apache/kylin/pull/2135"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CubeBuildJob.java||kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CubeBuildJob.java": [
          "File: kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CubeBuildJob.java -> kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CubeBuildJob.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "421:             ss.sparkContext().setJobDescription(\"build \" + layoutEntity.getId() + \" from parent \" + parentName);",
          "422:             Set<Integer> orderedDims = layoutEntity.getOrderedDimensions().keySet();",
          "423:             Dataset<Row> afterSort = afterPrj.select(NSparkCubingUtil.getColumns(orderedDims))",
          "425:             saveAndUpdateLayout(afterSort, seg, layoutEntity, parentId);",
          "426:         } else {",
          "427:             Dataset<Row> afterAgg = CuboidAggregator.agg(ss, parent, dimIndexes, cuboid.getOrderedMeasures(),",
          "",
          "[Removed Lines]",
          "424:                     .sortWithinPartitions(NSparkCubingUtil.getFirstColumn(orderedDims));",
          "",
          "[Added Lines]",
          "424:                     .sortWithinPartitions(NSparkCubingUtil.getColumns(orderedDims));",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "433:             Dataset<Row> afterSort = afterAgg",
          "434:                     .select(NSparkCubingUtil.getColumns(rowKeys, layoutEntity.getOrderedMeasures().keySet()))",
          "437:             saveAndUpdateLayout(afterSort, seg, layoutEntity, parentId);",
          "438:         }",
          "",
          "[Removed Lines]",
          "435:                     .sortWithinPartitions(NSparkCubingUtil.getFirstColumn(rowKeys));",
          "",
          "[Added Lines]",
          "435:                     .sortWithinPartitions(NSparkCubingUtil.getColumns(rowKeys));",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CubeMergeJob.java||kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CubeMergeJob.java": [
          "File: kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CubeMergeJob.java -> kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CubeMergeJob.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "99:             Dataset<Row> afterSort;",
          "100:             if (layout.isTableIndex()) {",
          "101:                 afterSort =",
          "103:             } else {",
          "104:                 Set<Integer> dimColumns = layout.getOrderedDimensions().keySet();",
          "105:                 Dataset<Row> afterAgg = CuboidAggregator.agg(ss, afterMerge, dimColumns,",
          "106:                         layout.getOrderedMeasures(), spanningTree, false);",
          "107:                 afterSort = afterAgg.sortWithinPartitions(",
          "109:             }",
          "110:             buildLayoutWithUpdate.submit(new BuildLayoutWithUpdate.JobEntity() {",
          "111:                 @Override",
          "",
          "[Removed Lines]",
          "102:                         afterMerge.sortWithinPartitions(NSparkCubingUtil.getFirstColumn(layout.getOrderedDimensions().keySet()));",
          "108:                         NSparkCubingUtil.getFirstColumn(dimColumns));",
          "",
          "[Added Lines]",
          "102:                         afterMerge.sortWithinPartitions(NSparkCubingUtil.getColumns(layout.getOrderedDimensions().keySet()));",
          "108:                         NSparkCubingUtil.getColumns(dimColumns));",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/utils/Repartitioner.java||kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/utils/Repartitioner.java": [
          "File: kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/utils/Repartitioner.java -> kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/utils/Repartitioner.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "158:                 data = storage.getFrom(tempPath, ss).repartition(repartitionNum,",
          "159:                         NSparkCubingUtil.getColumns(getShardByColumns()))",
          "161:             } else {",
          "163:                 logger.info(\"Cuboid[{}] repartition to {}\", cuboid, repartitionNum);",
          "164:                 data = storage.getFrom(tempPath, ss).repartition(repartitionNum)",
          "166:             }",
          "168:             storage.saveTo(path, data, ss);",
          "",
          "[Removed Lines]",
          "160:                         .sortWithinPartitions(sortCols[0]);",
          "165:                         .sortWithinPartitions(sortCols[0]);",
          "",
          "[Added Lines]",
          "160:                         .sortWithinPartitions(sortCols);",
          "165:                         .sortWithinPartitions(sortCols);",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "d8e5db8457bf5bbd1ae05b9360c5ae48038b57fb",
      "candidate_info": {
        "commit_hash": "d8e5db8457bf5bbd1ae05b9360c5ae48038b57fb",
        "repo": "apache/kylin",
        "commit_url": "https://github.com/apache/kylin/commit/d8e5db8457bf5bbd1ae05b9360c5ae48038b57fb",
        "files": [
          "build-engine/src/main/java/org/apache/kylin/engine/mr/common/CubeStatsReader.java",
          "build-engine/src/main/java/org/apache/kylin/engine/mr/common/CuboidRecommenderUtil.java",
          "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java",
          "core-common/src/main/resources/kylin-defaults.properties",
          "core-cube/src/main/java/org/apache/kylin/cube/cuboid/algorithm/CuboidRecommender.java",
          "core-cube/src/main/java/org/apache/kylin/cube/cuboid/algorithm/greedy/GreedyAlgorithm.java",
          "core-cube/src/main/java/org/apache/kylin/cube/kv/CubeDimEncMap.java",
          "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkExecutable.java",
          "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CubeBuildJob.java",
          "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CuboidStatisticsJob.scala"
        ],
        "message": "KYLIN-4818 Performance profile for CuboidStatisticsJob",
        "before_after_code_files": [
          "build-engine/src/main/java/org/apache/kylin/engine/mr/common/CubeStatsReader.java||build-engine/src/main/java/org/apache/kylin/engine/mr/common/CubeStatsReader.java",
          "build-engine/src/main/java/org/apache/kylin/engine/mr/common/CuboidRecommenderUtil.java||build-engine/src/main/java/org/apache/kylin/engine/mr/common/CuboidRecommenderUtil.java",
          "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java||core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java",
          "core-common/src/main/resources/kylin-defaults.properties||core-common/src/main/resources/kylin-defaults.properties",
          "core-cube/src/main/java/org/apache/kylin/cube/cuboid/algorithm/CuboidRecommender.java||core-cube/src/main/java/org/apache/kylin/cube/cuboid/algorithm/CuboidRecommender.java",
          "core-cube/src/main/java/org/apache/kylin/cube/cuboid/algorithm/greedy/GreedyAlgorithm.java||core-cube/src/main/java/org/apache/kylin/cube/cuboid/algorithm/greedy/GreedyAlgorithm.java",
          "core-cube/src/main/java/org/apache/kylin/cube/kv/CubeDimEncMap.java||core-cube/src/main/java/org/apache/kylin/cube/kv/CubeDimEncMap.java",
          "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkExecutable.java||kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkExecutable.java",
          "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CubeBuildJob.java||kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CubeBuildJob.java",
          "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CuboidStatisticsJob.scala||kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CuboidStatisticsJob.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/kylin/pull/1893",
          "https://github.com/apache/kylin/pull/2018",
          "https://github.com/apache/kylin/pull/2125",
          "https://github.com/apache/kylin/pull/2033",
          "https://github.com/apache/kylin/pull/2112",
          "https://github.com/apache/kylin/pull/2115",
          "https://github.com/apache/kylin/pull/1865",
          "https://github.com/apache/kylin/pull/1913",
          "https://github.com/apache/kylin/pull/2135"
        ],
        "olp_code_files": {
          "patch": [
            "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java||core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java"
          ],
          "candidate": [
            "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java||core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java"
          ]
        }
      },
      "candidate_diff": {
        "build-engine/src/main/java/org/apache/kylin/engine/mr/common/CubeStatsReader.java||build-engine/src/main/java/org/apache/kylin/engine/mr/common/CubeStatsReader.java": [
          "File: build-engine/src/main/java/org/apache/kylin/engine/mr/common/CubeStatsReader.java -> build-engine/src/main/java/org/apache/kylin/engine/mr/common/CubeStatsReader.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "218:         final Long baseCuboidRowCount = rowCountMap.get(baseCuboid.getId());",
          "220:         for (int i = 0; i < columnList.size(); i++) {",
          "222:         }",
          "224:         Map<Long, Double> sizeMap = Maps.newHashMap();",
          "",
          "[Removed Lines]",
          "221:             rowkeyColumnSize.add(dimEncMap.get(columnList.get(i)).getLengthOfEncoding());",
          "",
          "[Added Lines]",
          "225:             rowkeyColumnSize.add(4);",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "360:             }",
          "361:         }",
          "364:         double cuboidSizeMemHungryRatio = kylinConf.getJobCuboidSizeCountDistinctRatio();",
          "365:         double cuboidSizeTopNRatio = kylinConf.getJobCuboidSizeTopNRatio();",
          "368:                 + 1.0 * countDistinctSpace * rowCount * cuboidSizeMemHungryRatio + 1.0 * percentileSpace * rowCount",
          "369:                 + 1.0 * topNSpace * rowCount * cuboidSizeTopNRatio) / (1024L * 1024L);",
          "370:         return ret;",
          "",
          "[Removed Lines]",
          "363:         double cuboidSizeRatio = kylinConf.getJobCuboidSizeRatio();",
          "367:         double ret = (1.0 * normalSpace * rowCount * cuboidSizeRatio",
          "",
          "[Added Lines]",
          "370:         double ret = (1.0 * normalSpace * rowCount",
          "",
          "---------------"
        ],
        "build-engine/src/main/java/org/apache/kylin/engine/mr/common/CuboidRecommenderUtil.java||build-engine/src/main/java/org/apache/kylin/engine/mr/common/CuboidRecommenderUtil.java": [
          "File: build-engine/src/main/java/org/apache/kylin/engine/mr/common/CuboidRecommenderUtil.java -> build-engine/src/main/java/org/apache/kylin/engine/mr/common/CuboidRecommenderUtil.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "60:         Set<Long> mandatoryCuboids = segment.getCubeDesc().getMandatoryCuboids();",
          "62:         String key = cube.getName();",
          "65:         return CuboidRecommender.getInstance().getRecommendCuboidList(cuboidStats, segment.getConfig(),",
          "66:                 !mandatoryCuboids.isEmpty());",
          "67:     }",
          "",
          "[Removed Lines]",
          "63:         CuboidStats cuboidStats = new CuboidStats.Builder(key, baseCuboid, cubeStatsReader.getCuboidRowEstimatesHLL(),",
          "64:                 cubeStatsReader.getCuboidSizeMap()).setMandatoryCuboids(mandatoryCuboids).setBPUSMinBenefitRatio(segment.getConfig().getCubePlannerBPUSMinBenefitRatio()).build();",
          "",
          "[Added Lines]",
          "63:         CuboidStats cuboidStats = new CuboidStats.Builder(key, baseCuboid, cubeStatsReader.getCuboidRowEstimatesHLL(), cubeStatsReader.getCuboidSizeMap())",
          "64:                 .setMandatoryCuboids(mandatoryCuboids)",
          "65:                 .setBPUSMinBenefitRatio(segment.getConfig().getCubePlannerBPUSMinBenefitRatio())",
          "66:                 .build();",
          "",
          "---------------"
        ],
        "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java||core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java": [
          "File: core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java -> core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "715:         return Boolean.parseBoolean(getOptional(\"kylin.cube.size-estimate-enable-optimize\", \"false\"));",
          "716:     }",
          "718:     public double getJobCuboidSizeRatio() {",
          "719:         return Double.parseDouble(getOptional(\"kylin.cube.size-estimate-ratio\", \"0.25\"));",
          "720:     }",
          "723:     public double getJobCuboidSizeMemHungryRatio() {",
          "724:         return Double.parseDouble(getOptional(\"kylin.cube.size-estimate-memhungry-ratio\", \"0.05\"));",
          "725:     }",
          "",
          "[Removed Lines]",
          "722:     @Deprecated",
          "",
          "[Added Lines]",
          "718:     @ConfigTag({ConfigTag.Tag.DEPRECATED, ConfigTag.Tag.NOT_CLEAR})",
          "723:     @ConfigTag({ConfigTag.Tag.DEPRECATED, ConfigTag.Tag.NOT_CLEAR})",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "803:     public boolean isCubePlannerEnabled() {",
          "805:     }",
          "807:     public boolean isCubePlannerEnabledForExistingCube() {",
          "",
          "[Removed Lines]",
          "804:         return Boolean.parseBoolean(getOptional(\"kylin.cube.cubeplanner.enabled\", TRUE));",
          "",
          "[Added Lines]",
          "805:         return Boolean.parseBoolean(getOptional(\"kylin.cube.cubeplanner.enabled\", FALSE));",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "832:         return Integer.parseInt(getOptional(\"kylin.cube.cubeplanner.algorithm-threshold-genetic\", \"23\"));",
          "833:     }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "840:     public double getStorageCompressionRatio() {",
          "841:         return Double.parseDouble(getOptional(\"kylin.cube.cubeplanner.storage.compression.ratio\", \"0.2\"));",
          "842:     }",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "2638:         return getFileName(kylinHome + File.separator + \"lib\", PARQUET_JOB_JAR_NAME_PATTERN);",
          "2639:     }",
          "2641:     public void overrideKylinParquetJobJarPath(String path) {",
          "2642:         logger.info(\"override {} to {}\", KYLIN_ENGINE_PARQUET_JOB_JAR, path);",
          "2643:         System.setProperty(KYLIN_ENGINE_PARQUET_JOB_JAR, path);",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "2653:     public String getSparkSubmitCmd() {",
          "2654:         return getOptional(\"kylin.engine.spark-cmd\", null);",
          "2655:     }",
          "",
          "---------------"
        ],
        "core-common/src/main/resources/kylin-defaults.properties||core-common/src/main/resources/kylin-defaults.properties": [
          "File: core-common/src/main/resources/kylin-defaults.properties -> core-common/src/main/resources/kylin-defaults.properties",
          "--- Hunk 1 ---",
          "[Context before]",
          "154: kylin.cube.aggrgroup.max-combination=32768",
          "157: kylin.cube.cubeplanner.enabled-for-existing-cube=false",
          "158: kylin.cube.cubeplanner.expansion-threshold=15.0",
          "159: kylin.cube.cubeplanner.recommend-cache-max-size=200",
          "",
          "[Removed Lines]",
          "156: kylin.cube.cubeplanner.enabled=true",
          "",
          "[Added Lines]",
          "156: kylin.cube.cubeplanner.enabled=false",
          "",
          "---------------"
        ],
        "core-cube/src/main/java/org/apache/kylin/cube/cuboid/algorithm/CuboidRecommender.java||core-cube/src/main/java/org/apache/kylin/cube/cuboid/algorithm/CuboidRecommender.java": [
          "File: core-cube/src/main/java/org/apache/kylin/cube/cuboid/algorithm/CuboidRecommender.java -> core-cube/src/main/java/org/apache/kylin/cube/cuboid/algorithm/CuboidRecommender.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "144:         long startTime = System.currentTimeMillis();",
          "145:         logger.info(\"Cube Planner Algorithm started at {}\", startTime);",
          "147:         logger.info(\"Cube Planner Algorithm ended at {}\", System.currentTimeMillis() - startTime);",
          "149:         if (recommendCuboidList.size() < allCuboidCount) {",
          "",
          "[Removed Lines]",
          "146:         List<Long> recommendCuboidList = algorithm.recommend(kylinConf.getCubePlannerExpansionRateThreshold());",
          "",
          "[Added Lines]",
          "146:         List<Long> recommendCuboidList = algorithm.recommend(",
          "147:                 kylinConf.getCubePlannerExpansionRateThreshold() / kylinConf.getStorageCompressionRatio());",
          "",
          "---------------"
        ],
        "core-cube/src/main/java/org/apache/kylin/cube/cuboid/algorithm/greedy/GreedyAlgorithm.java||core-cube/src/main/java/org/apache/kylin/cube/cuboid/algorithm/greedy/GreedyAlgorithm.java": [
          "File: core-cube/src/main/java/org/apache/kylin/cube/cuboid/algorithm/greedy/GreedyAlgorithm.java -> core-cube/src/main/java/org/apache/kylin/cube/cuboid/algorithm/greedy/GreedyAlgorithm.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "92:                     remaining.remove(best.getCuboidId());",
          "93:                     benefitPolicy.propagateAggregationCost(best.getCuboidId(), selected);",
          "94:                     round++;",
          "97:                     }",
          "98:                 } else {",
          "99:                     doesRemainSpace = false;",
          "",
          "[Removed Lines]",
          "95:                     if (logger.isTraceEnabled()) {",
          "96:                         logger.trace(\"Recommend in round {} : {}\", round, best);",
          "",
          "[Added Lines]",
          "95:                     if (logger.isDebugEnabled()) {",
          "96:                         logger.debug(\"Recommend in round {} : {}\", round, best);",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "111:                 \"There should be no intersection between excluded list and selected list.\");",
          "112:         logger.info(\"Greedy Algorithm finished.\");",
          "117:             for (Long cuboid : excluded) {",
          "119:                         cuboidStats.getCuboidQueryCost(cuboid), cuboidStats.getCuboidSize(cuboid));",
          "120:             }",
          "123:         }",
          "124:         return Lists.newArrayList(selected);",
          "125:     }",
          "",
          "[Removed Lines]",
          "114:         if (logger.isTraceEnabled()) {",
          "115:             logger.trace(\"Excluded cuboidId size: {}\", excluded.size());",
          "116:             logger.trace(\"Excluded cuboidId detail:\");",
          "118:                 logger.trace(\"cuboidId {} and Cost: {} and Space: {}\", cuboid,",
          "121:             logger.trace(\"Total Space: {}\", spaceLimit - remainingSpace);",
          "122:             logger.trace(\"Space Expansion Rate: {}\", (spaceLimit - remainingSpace) / cuboidStats.getBaseCuboidSize());",
          "",
          "[Added Lines]",
          "114:         if (logger.isDebugEnabled()) {",
          "115:             logger.debug(\"Excluded cuboidId size: {}\", excluded.size());",
          "116:             logger.debug(\"Excluded cuboidId detail:\");",
          "118:                 logger.debug(\"cuboidId {} and Cost: {} and Space: {}\", cuboid,",
          "121:             logger.debug(\"Total Space: {}\", spaceLimit - remainingSpace);",
          "122:             logger.debug(\"Space Expansion Rate: {}\", (spaceLimit - remainingSpace) / cuboidStats.getBaseCuboidSize());",
          "",
          "---------------"
        ],
        "core-cube/src/main/java/org/apache/kylin/cube/kv/CubeDimEncMap.java||core-cube/src/main/java/org/apache/kylin/cube/kv/CubeDimEncMap.java": [
          "File: core-cube/src/main/java/org/apache/kylin/cube/kv/CubeDimEncMap.java -> core-cube/src/main/java/org/apache/kylin/cube/kv/CubeDimEncMap.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "21: import java.util.Map;",
          "23: import org.apache.kylin.common.util.Dictionary;",
          "24: import org.apache.kylin.cube.CubeSegment;",
          "25: import org.apache.kylin.cube.model.CubeDesc;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "23: import org.apache.kylin.common.annotation.Clarification;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "36: import org.apache.kylin.shaded.com.google.common.collect.Maps;",
          "38: public class CubeDimEncMap implements IDimensionEncodingMap, java.io.Serializable {",
          "40:     private static final Logger logger = LoggerFactory.getLogger(CubeDimEncMap.class);",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "39: @Clarification(deprecated = true, msg = \"Useless code in Kylin 4\")",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkExecutable.java||kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkExecutable.java": [
          "File: kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkExecutable.java -> kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkExecutable.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "347:     protected String generateSparkCmd(KylinConfig config, String hadoopConf, String jars, String kylinJobJar,",
          "348:                                       String appArgs) {",
          "349:         StringBuilder sb = new StringBuilder();",
          "352:         Map<String, String> sparkConfs = getSparkConfigOverride(config);",
          "353:         for (Entry<String, String> entry : sparkConfs.entrySet()) {",
          "",
          "[Removed Lines]",
          "350:         sb.append(\"export HADOOP_CONF_DIR=%s && %s/bin/spark-submit --class org.apache.kylin.engine.spark.application.SparkEntry \");",
          "",
          "[Added Lines]",
          "351:         String sparkSubmitCmd = config.getSparkSubmitCmd() != null ?",
          "352:                 config.getSparkSubmitCmd() : KylinConfig.getSparkHome() + \"/bin/spark-submit\";",
          "353:         sb.append(\"export HADOOP_CONF_DIR=%s && %s --class org.apache.kylin.engine.spark.application.SparkEntry \");",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "362:         sb.append(\"--files \").append(config.sparkUploadFiles()).append(\" \");",
          "363:         sb.append(\"--name job_step_%s \");",
          "364:         sb.append(\"--jars %s %s %s\");",
          "366:                 appArgs);",
          "368:         logger.info(\"spark submit cmd: {}\", cmd);",
          "",
          "[Removed Lines]",
          "365:         String cmd = String.format(Locale.ROOT, sb.toString(), hadoopConf, KylinConfig.getSparkHome(), getId(), jars, kylinJobJar,",
          "",
          "[Added Lines]",
          "368:         String cmd = String.format(Locale.ROOT, sb.toString(), hadoopConf, sparkSubmitCmd, getId(), jars, kylinJobJar,",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CubeBuildJob.java||kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CubeBuildJob.java": [
          "File: kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CubeBuildJob.java -> kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CubeBuildJob.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "22: import java.util.ArrayList;",
          "23: import java.util.Collection;",
          "24: import java.util.Collections;",
          "25: import java.util.HashMap;",
          "26: import java.util.LinkedList;",
          "27: import java.util.List;",
          "28: import java.util.Locale;",
          "29: import java.util.Map;",
          "30: import java.util.Set;",
          "31: import java.util.UUID;",
          "32: import java.util.concurrent.ConcurrentHashMap;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "25: import java.util.Comparator;",
          "31: import java.util.Optional;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "141:             String jobWorkingDirPath = JobBuilderSupport.getJobWorkingDir(cubeInstance.getConfig().getHdfsWorkingDirectory(), jobId);",
          "142:             Path statisticsDir = new Path(jobWorkingDirPath + \"/\" + firstSegmentId + \"/\" + CFG_OUTPUT_STATISTICS);",
          "145:             FileSystem fs = HadoopUtil.getWorkingFileSystem();",
          "146:             ResourceStore rs = ResourceStore.getStore(config);",
          "",
          "[Removed Lines]",
          "143:             CubeStatsWriter.writeCuboidStatistics(HadoopUtil.getCurrentConfiguration(), statisticsDir, hllMap, 1);",
          "",
          "[Added Lines]",
          "145:             Optional<HLLCounter> hll = hllMap.values().stream().max(Comparator.comparingLong(HLLCounter::getCountEstimate));",
          "146:             long rc = hll.map(HLLCounter::getCountEstimate).orElse(1L);",
          "147:             CubeStatsWriter.writeCuboidStatistics(HadoopUtil.getCurrentConfiguration(), statisticsDir, hllMap, 1, rc);",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CuboidStatisticsJob.scala||kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CuboidStatisticsJob.scala": [
          "File: kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CuboidStatisticsJob.scala -> kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CuboidStatisticsJob.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "50: class CuboidStatisticsJob(ids: List[Long], rkc: Int) extends Serializable {",
          "51:   private val info = mutable.Map[String, AggInfo]()",
          "53:   private val hf: HashFunction = Hashing.murmur3_128",
          "54:   private val rowHashCodesLong = new Array[Long](rkc)",
          "55:   private var idx = 0",
          "57:   def statisticsWithinPartition(rows: Iterator[Row]): Iterator[AggInfo] = {",
          "58:     init()",
          "59:     rows.foreach(update)",
          "60:     info.valuesIterator",
          "61:   }",
          "63:   def init(): Unit = {",
          "64:     ids.foreach(i => info.put(i.toString, AggInfo(i.toString)))",
          "65:   }",
          "67:   def update(r: Row): Unit = {",
          "68:     idx += 1",
          "70:       println(r)",
          "71:     updateCuboid(r)",
          "72:   }",
          "74:   def updateCuboid(r: Row): Unit = {",
          "77:     var idx = 0",
          "78:     while (idx < rkc) {",
          "79:       val hc = hf.newHasher",
          "80:       var colValue = r.get(idx).toString",
          "81:       if (colValue == null) colValue = \"0\"",
          "83:       rowHashCodesLong(idx) = hc.putUnencodedChars(colValue).hash().padToLong() + idx",
          "84:       idx += 1",
          "85:     }",
          "88:     val n = allCuboidsBitSet.length",
          "89:     idx = 0",
          "",
          "[Removed Lines]",
          "52:   val allCuboidsBitSet: Array[Array[Integer]] = getCuboidBitSet(ids, rkc)",
          "69:     if (idx <= 5 || idx % 300 == 0)",
          "",
          "[Added Lines]",
          "52:   private var allCuboidsBitSet: Array[Array[Integer]] = Array()",
          "56:   private var meter1 = 0L",
          "57:   private var meter2 = 0L",
          "58:   private var startMills = 0L",
          "59:   private var endMills = 0L",
          "64:     println(\"CuboidStatisticsJob-statisticsWithinPartition1-\" + System.currentTimeMillis())",
          "66:     printStat()",
          "67:     println(\"CuboidStatisticsJob-statisticsWithinPartition2-\" + System.currentTimeMillis())",
          "72:     println(\"CuboidStatisticsJob-Init1-\" + System.currentTimeMillis())",
          "73:     allCuboidsBitSet = getCuboidBitSet(ids, rkc)",
          "75:     println(\"CuboidStatisticsJob-Init2-\" + System.currentTimeMillis())",
          "80:     if (idx <= 5)",
          "87:     startMills = System.currentTimeMillis()",
          "97:     endMills = System.currentTimeMillis()",
          "98:     meter1 += (endMills - startMills)",
          "101:     startMills = System.currentTimeMillis()",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "97:       info(ids(idx).toString).cuboid.counter.addHashDirectly(value)",
          "98:       idx += 1",
          "99:     }",
          "100:   }",
          "102:   def getCuboidBitSet(cuboidIds: List[Long], nRowKey: Int): Array[Array[Integer]] = {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "115:     endMills = System.currentTimeMillis()",
          "116:     meter2 += (endMills - startMills)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "120:     }",
          "121:     allCuboidsBitSet",
          "122:   }",
          "123: }",
          "125: case class AggInfo(key: String,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "141:   def printStat(): Unit = {",
          "142:     println(\"    Stats\")",
          "143:     println(\"   i   :\" + idx)",
          "144:     println(\"meter1 :\" + meter1)",
          "145:     println(\"meter2 :\" + meter2)",
          "146:   }",
          "",
          "---------------"
        ]
      }
    }
  ]
}