{
  "cve_id": "CVE-2023-50944",
  "cve_desc": "Apache Airflow, versions before 2.8.1, have a vulnerability that allows an authenticated user to access the source code of a DAG to which they don't have access.\u00a0This vulnerability is considered low since it requires an authenticated user to exploit it. Users are recommended to upgrade to version 2.8.1, which fixes this issue.",
  "repo": "apache/airflow",
  "patch_hash": "8d76538d6e105947272b000581c6fabec20146b1",
  "patch_info": {
    "commit_hash": "8d76538d6e105947272b000581c6fabec20146b1",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/8d76538d6e105947272b000581c6fabec20146b1",
    "files": [
      "airflow/api_connexion/endpoints/dag_source_endpoint.py",
      "airflow/models/dagcode.py",
      "tests/api_connexion/endpoints/test_dag_source_endpoint.py"
    ],
    "message": "Check DAG read permission before accessing DAG code (#36257)\n\n(cherry picked from commit 30ea37e0d247ce54c2d25b115e807fdb0074d795)",
    "before_after_code_files": [
      "airflow/api_connexion/endpoints/dag_source_endpoint.py||airflow/api_connexion/endpoints/dag_source_endpoint.py",
      "airflow/models/dagcode.py||airflow/models/dagcode.py",
      "tests/api_connexion/endpoints/test_dag_source_endpoint.py||tests/api_connexion/endpoints/test_dag_source_endpoint.py"
    ]
  },
  "patch_diff": {
    "airflow/api_connexion/endpoints/dag_source_endpoint.py||airflow/api_connexion/endpoints/dag_source_endpoint.py": [
      "File: airflow/api_connexion/endpoints/dag_source_endpoint.py -> airflow/api_connexion/endpoints/dag_source_endpoint.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "17: from __future__ import annotations",
      "19: from http import HTTPStatus",
      "21: from flask import Response, current_app, request",
      "22: from itsdangerous import BadSignature, URLSafeSerializer",
      "24: from airflow.api_connexion import security",
      "26: from airflow.api_connexion.schemas.dag_source_schema import dag_source_schema",
      "27: from airflow.auth.managers.models.resource_details import DagAccessEntity",
      "28: from airflow.models.dagcode import DagCode",
      "31: @security.requires_access_dag(\"GET\", DagAccessEntity.CODE)",
      "33:     \"\"\"Get source code using file token.\"\"\"",
      "34:     secret_key = current_app.config[\"SECRET_KEY\"]",
      "35:     auth_s = URLSafeSerializer(secret_key)",
      "36:     try:",
      "37:         path = auth_s.loads(file_token)",
      "39:     except (BadSignature, FileNotFoundError):",
      "40:         raise NotFound(\"Dag source not found\")",
      "",
      "[Removed Lines]",
      "25: from airflow.api_connexion.exceptions import NotFound",
      "32: def get_dag_source(*, file_token: str) -> Response:",
      "38:         dag_source = DagCode.code(path)",
      "",
      "[Added Lines]",
      "20: from typing import TYPE_CHECKING",
      "26: from airflow.api_connexion.exceptions import NotFound, PermissionDenied",
      "28: from airflow.api_connexion.security import get_readable_dags",
      "30: from airflow.models.dag import DagModel",
      "32: from airflow.utils.session import NEW_SESSION, provide_session",
      "34: if TYPE_CHECKING:",
      "35:     from sqlalchemy.orm import Session",
      "39: @provide_session",
      "40: def get_dag_source(*, file_token: str, session: Session = NEW_SESSION) -> Response:",
      "46:         dag_ids = session.query(DagModel.dag_id).filter(DagModel.fileloc == path).all()",
      "47:         readable_dags = get_readable_dags()",
      "48:         # Check if user has read access to all the DAGs defined in the file",
      "49:         if any(dag_id[0] not in readable_dags for dag_id in dag_ids):",
      "50:             raise PermissionDenied()",
      "51:         dag_source = DagCode.code(path, session=session)",
      "",
      "---------------"
    ],
    "airflow/models/dagcode.py||airflow/models/dagcode.py": [
      "File: airflow/models/dagcode.py -> airflow/models/dagcode.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "177:         return cls.code(fileloc)",
      "179:     @classmethod",
      "181:         \"\"\"Return source code for this DagCode object.",
      "183:         :return: source code as string",
      "184:         \"\"\"",
      "187:     @staticmethod",
      "188:     def _get_code_from_file(fileloc):",
      "",
      "[Removed Lines]",
      "180:     def code(cls, fileloc) -> str:",
      "185:         return cls._get_code_from_db(fileloc)",
      "",
      "[Added Lines]",
      "180:     @provide_session",
      "181:     def code(cls, fileloc, session: Session = NEW_SESSION) -> str:",
      "186:         return cls._get_code_from_db(fileloc, session)",
      "",
      "---------------"
    ],
    "tests/api_connexion/endpoints/test_dag_source_endpoint.py||tests/api_connexion/endpoints/test_dag_source_endpoint.py": [
      "File: tests/api_connexion/endpoints/test_dag_source_endpoint.py -> tests/api_connexion/endpoints/test_dag_source_endpoint.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "35: ROOT_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), os.pardir, os.pardir))",
      "36: EXAMPLE_DAG_FILE = os.path.join(\"airflow\", \"example_dags\", \"example_bash_operator.py\")",
      "39: @pytest.fixture(scope=\"module\")",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "37: EXAMPLE_DAG_ID = \"example_bash_operator\"",
      "38: TEST_DAG_ID = \"latest_only\"",
      "39: NOT_READABLE_DAG_ID = \"latest_only_with_trigger\"",
      "40: TEST_MULTIPLE_DAGS_ID = \"dataset_produces_1\"",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "45:         role_name=\"Test\",",
      "46:         permissions=[(permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG_CODE)],  # type: ignore",
      "47:     )",
      "48:     create_user(app, username=\"test_no_permissions\", role_name=\"TestNoPermissions\")  # type: ignore",
      "50:     yield app",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "52:     app.appbuilder.sm.sync_perm_for_dag(  # type: ignore",
      "53:         TEST_DAG_ID,",
      "54:         access_control={\"Test\": [permissions.ACTION_CAN_READ]},",
      "55:     )",
      "56:     app.appbuilder.sm.sync_perm_for_dag(  # type: ignore",
      "57:         EXAMPLE_DAG_ID,",
      "58:         access_control={\"Test\": [permissions.ACTION_CAN_READ]},",
      "59:     )",
      "60:     app.appbuilder.sm.sync_perm_for_dag(  # type: ignore",
      "61:         TEST_MULTIPLE_DAGS_ID,",
      "62:         access_control={\"Test\": [permissions.ACTION_CAN_READ]},",
      "63:     )",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "80:     def test_should_respond_200_text(self, url_safe_serializer):",
      "81:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
      "82:         dagbag.sync_to_db()",
      "87:         response = self.client.get(",
      "88:             url, headers={\"Accept\": \"text/plain\"}, environ_overrides={\"REMOTE_USER\": \"test\"}",
      "89:         )",
      "",
      "[Removed Lines]",
      "83:         first_dag: DAG = next(iter(dagbag.dags.values()))",
      "84:         dag_docstring = self._get_dag_file_docstring(first_dag.fileloc)",
      "86:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(first_dag.fileloc)}\"",
      "",
      "[Added Lines]",
      "99:         test_dag: DAG = dagbag.dags[TEST_DAG_ID]",
      "100:         dag_docstring = self._get_dag_file_docstring(test_dag.fileloc)",
      "102:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(test_dag.fileloc)}\"",
      "",
      "---------------",
      "--- Hunk 4 ---",
      "[Context before]",
      "95:     def test_should_respond_200_json(self, url_safe_serializer):",
      "96:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
      "97:         dagbag.sync_to_db()",
      "102:         response = self.client.get(",
      "103:             url, headers={\"Accept\": \"application/json\"}, environ_overrides={\"REMOTE_USER\": \"test\"}",
      "104:         )",
      "",
      "[Removed Lines]",
      "98:         first_dag: DAG = next(iter(dagbag.dags.values()))",
      "99:         dag_docstring = self._get_dag_file_docstring(first_dag.fileloc)",
      "101:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(first_dag.fileloc)}\"",
      "",
      "[Added Lines]",
      "114:         test_dag: DAG = dagbag.dags[TEST_DAG_ID]",
      "115:         dag_docstring = self._get_dag_file_docstring(test_dag.fileloc)",
      "117:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(test_dag.fileloc)}\"",
      "",
      "---------------",
      "--- Hunk 5 ---",
      "[Context before]",
      "110:     def test_should_respond_406(self, url_safe_serializer):",
      "111:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
      "112:         dagbag.sync_to_db()",
      "116:         response = self.client.get(",
      "117:             url, headers={\"Accept\": \"image/webp\"}, environ_overrides={\"REMOTE_USER\": \"test\"}",
      "118:         )",
      "",
      "[Removed Lines]",
      "113:         first_dag: DAG = next(iter(dagbag.dags.values()))",
      "115:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(first_dag.fileloc)}\"",
      "",
      "[Added Lines]",
      "129:         test_dag: DAG = dagbag.dags[TEST_DAG_ID]",
      "131:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(test_dag.fileloc)}\"",
      "",
      "---------------",
      "--- Hunk 6 ---",
      "[Context before]",
      "151:             environ_overrides={\"REMOTE_USER\": \"test_no_permissions\"},",
      "152:         )",
      "153:         assert response.status_code == 403",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "171:     def test_should_respond_403_not_readable(self, url_safe_serializer):",
      "172:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
      "173:         dagbag.sync_to_db()",
      "174:         dag: DAG = dagbag.dags[NOT_READABLE_DAG_ID]",
      "176:         response = self.client.get(",
      "177:             f\"/api/v1/dagSources/{url_safe_serializer.dumps(dag.fileloc)}\",",
      "178:             headers={\"Accept\": \"text/plain\"},",
      "179:             environ_overrides={\"REMOTE_USER\": \"test\"},",
      "180:         )",
      "181:         read_dag = self.client.get(",
      "182:             f\"/api/v1/dags/{NOT_READABLE_DAG_ID}\",",
      "183:             environ_overrides={\"REMOTE_USER\": \"test\"},",
      "184:         )",
      "185:         assert response.status_code == 403",
      "186:         assert read_dag.status_code == 403",
      "188:     def test_should_respond_403_some_dags_not_readable_in_the_file(self, url_safe_serializer):",
      "189:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
      "190:         dagbag.sync_to_db()",
      "191:         dag: DAG = dagbag.dags[TEST_MULTIPLE_DAGS_ID]",
      "193:         response = self.client.get(",
      "194:             f\"/api/v1/dagSources/{url_safe_serializer.dumps(dag.fileloc)}\",",
      "195:             headers={\"Accept\": \"text/plain\"},",
      "196:             environ_overrides={\"REMOTE_USER\": \"test\"},",
      "197:         )",
      "199:         read_dag = self.client.get(",
      "200:             f\"/api/v1/dags/{TEST_MULTIPLE_DAGS_ID}\",",
      "201:             environ_overrides={\"REMOTE_USER\": \"test\"},",
      "202:         )",
      "203:         assert response.status_code == 403",
      "204:         assert read_dag.status_code == 200",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "e3316c36b4159d5bc6ae73e4b77ba62045064184",
      "candidate_info": {
        "commit_hash": "e3316c36b4159d5bc6ae73e4b77ba62045064184",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/e3316c36b4159d5bc6ae73e4b77ba62045064184",
        "files": [
          "dev/README_RELEASE_PROVIDER_PACKAGES.md",
          "dev/breeze/src/airflow_breeze/prepare_providers/provider_packages.py"
        ],
        "message": "Fix preparing rc candidates for providers (#36465)\n\nThe last auto-upgrade RC implementd in #36441 had a bug - it was bumping\nrc even for providers that have been already released. This change fixes\nit - it skips packages that already have \"final\" tag present in the\nrepo. It also explicitely calls \"Apply template update\" as optional\nstep - only needed in case we modify templates and want to update\nautomatically generated documentation with it.\n\n(cherry picked from commit a3e5a971edf9e4e86c86d595daa04dccd277aabe)",
        "before_after_code_files": [
          "dev/breeze/src/airflow_breeze/prepare_providers/provider_packages.py||dev/breeze/src/airflow_breeze/prepare_providers/provider_packages.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/breeze/src/airflow_breeze/prepare_providers/provider_packages.py||dev/breeze/src/airflow_breeze/prepare_providers/provider_packages.py": [
          "File: dev/breeze/src/airflow_breeze/prepare_providers/provider_packages.py -> dev/breeze/src/airflow_breeze/prepare_providers/provider_packages.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "172:         return False, version_suffix",
          "173:     # version_suffix starts with \"rc\"",
          "174:     current_version = int(version_suffix[2:])",
          "175:     while True:",
          "176:         current_tag = get_latest_provider_tag(provider_id, f\"rc{current_version}\")",
          "177:         if tag_exists_for_provider(provider_id, current_tag):",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "175:     release_tag = get_latest_provider_tag(provider_id, \"\")",
          "176:     if tag_exists_for_provider(provider_id, release_tag):",
          "177:         get_console().print(f\"[warning]The tag {release_tag} exists. Provider is released. Skipping it.[/]\")",
          "178:         return True, \"\"",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "81d1fec83f659c3421a24eeb4d6d8e75d9681f86",
      "candidate_info": {
        "commit_hash": "81d1fec83f659c3421a24eeb4d6d8e75d9681f86",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/81d1fec83f659c3421a24eeb4d6d8e75d9681f86",
        "files": [
          "airflow/www/static/js/components/RunTypeIcon.tsx"
        ],
        "message": "Fix run type icon alignment with run type text (#36616)\n\n(cherry picked from commit 70be78f4e7cea07c759802ed9af51408d36184cf)",
        "before_after_code_files": [
          "airflow/www/static/js/components/RunTypeIcon.tsx||airflow/www/static/js/components/RunTypeIcon.tsx"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/www/static/js/components/RunTypeIcon.tsx||airflow/www/static/js/components/RunTypeIcon.tsx": [
          "File: airflow/www/static/js/components/RunTypeIcon.tsx -> airflow/www/static/js/components/RunTypeIcon.tsx",
          "--- Hunk 1 ---",
          "[Context before]",
          "29:   runType: DagRun[\"runType\"];",
          "30: }",
          "32: const DagRunTypeIcon = ({ runType, ...rest }: Props) => {",
          "33:   switch (runType) {",
          "34:     case \"manual\":",
          "36:     case \"backfill\":",
          "38:     case \"scheduled\":",
          "40:     case \"dataset_triggered\":",
          "42:     default:",
          "43:       return null;",
          "44:   }",
          "",
          "[Removed Lines]",
          "35:       return <MdPlayArrow style={{ display: \"inline\" }} {...rest} />;",
          "37:       return <RiArrowGoBackFill style={{ display: \"inline\" }} {...rest} />;",
          "39:       return <MdOutlineSchedule style={{ display: \"inline\" }} {...rest} />;",
          "41:       return <HiDatabase style={{ display: \"inline\" }} {...rest} />;",
          "",
          "[Added Lines]",
          "32: const iconStyle = {",
          "33:   display: \"inline\",",
          "34:   verticalAlign: \"bottom\",",
          "35: };",
          "40:       return <MdPlayArrow style={iconStyle} {...rest} />;",
          "42:       return <RiArrowGoBackFill style={iconStyle} {...rest} />;",
          "44:       return <MdOutlineSchedule style={iconStyle} {...rest} />;",
          "46:       return <HiDatabase style={iconStyle} {...rest} />;",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "d8537857036a36dcfda171395092f932770d6dda",
      "candidate_info": {
        "commit_hash": "d8537857036a36dcfda171395092f932770d6dda",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/d8537857036a36dcfda171395092f932770d6dda",
        "files": [
          "dev/README_RELEASE_PROVIDER_PACKAGES.md",
          "dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "dev/breeze/src/airflow_breeze/prepare_providers/provider_packages.py"
        ],
        "message": "Automate rcN calculation when releasing provider packages (#36441)\n\nThis change will automatically generate the **right** rcN package\nwhen prepareing the packages for PyPI. This allows to have pretty\nmuch continuous release process for voting over the provider packages.\n\nSimply when an rcN candidate is not released, it will be automatically\nincluded in the next wave of packages with rcN+1 version - unless during\nprovider package generation the version will be bumped to MAJOR or MINOR\ndue to new changes.\n\nThis allows for the workflow where in every new wave we always generate\nall provider packages ready for release.\n\n(cherry picked from commit 6f5a50ea10842c2bb4b6bdc1e28dfaa680536d5a)",
        "before_after_code_files": [
          "dev/breeze/src/airflow_breeze/commands/release_management_commands.py||dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "dev/breeze/src/airflow_breeze/prepare_providers/provider_packages.py||dev/breeze/src/airflow_breeze/prepare_providers/provider_packages.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/breeze/src/airflow_breeze/commands/release_management_commands.py||dev/breeze/src/airflow_breeze/commands/release_management_commands.py": [
          "File: dev/breeze/src/airflow_breeze/commands/release_management_commands.py -> dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "563:         shutil.rmtree(DIST_DIR, ignore_errors=True)",
          "564:         DIST_DIR.mkdir(parents=True, exist_ok=True)",
          "565:     for provider_id in packages_list:",
          "566:         try:",
          "567:             basic_provider_checks(provider_id)",
          "570:             get_console().print()",
          "571:             with ci_group(f\"Preparing provider package [special]{provider_id}\"):",
          "572:                 get_console().print()",
          "573:                 target_provider_root_sources_path = copy_provider_sources_to_target(provider_id)",
          "574:                 generate_build_files(",
          "575:                     provider_id=provider_id,",
          "577:                     target_provider_root_sources_path=target_provider_root_sources_path,",
          "578:                 )",
          "579:                 cleanup_build_remnants(target_provider_root_sources_path)",
          "",
          "[Removed Lines]",
          "568:             if not skip_tag_check and should_skip_the_package(provider_id, version_suffix_for_pypi):",
          "569:                 continue",
          "576:                     version_suffix=version_suffix_for_pypi,",
          "",
          "[Added Lines]",
          "566:         package_version = version_suffix_for_pypi",
          "569:             if not skip_tag_check:",
          "570:                 should_skip, package_version = should_skip_the_package(provider_id, package_version)",
          "571:                 if should_skip:",
          "572:                     continue",
          "579:                     version_suffix=package_version,",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/prepare_providers/provider_packages.py||dev/breeze/src/airflow_breeze/prepare_providers/provider_packages.py": [
          "File: dev/breeze/src/airflow_breeze/prepare_providers/provider_packages.py -> dev/breeze/src/airflow_breeze/prepare_providers/provider_packages.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "155:     get_console().print(f\"\\n[info]Generated package build files for {provider_id}[/]\\n\")",
          "161:     For RC and official releases we check if the \"officially released\" version exists",
          "162:     and skip the released if it was. This allows to skip packages that have not been",
          "163:     marked for release in this wave. For \"dev\" suffixes, we always build all packages.",
          "164:     \"\"\"",
          "167:         if tag_exists_for_provider(provider_id, current_tag):",
          "173: def cleanup_build_remnants(target_provider_root_sources_path: Path):",
          "",
          "[Removed Lines]",
          "158: def should_skip_the_package(provider_id: str, version_suffix: str) -> bool:",
          "159:     \"\"\"Return True if the package should be skipped.",
          "165:     if version_suffix.startswith(\"rc\") or version_suffix == \"\":",
          "166:         current_tag = get_latest_provider_tag(provider_id, version_suffix)",
          "168:             get_console().print(f\"[warning]The tag {current_tag} exists. Skipping the package.[/]\")",
          "169:             return True",
          "170:     return False",
          "",
          "[Added Lines]",
          "158: def should_skip_the_package(provider_id: str, version_suffix: str) -> tuple[bool, str]:",
          "159:     \"\"\"Return True, version if the package should be skipped and False, good version suffix if not.",
          "165:     if version_suffix != \"\" and not version_suffix.startswith(\"rc\"):",
          "166:         return False, version_suffix",
          "167:     if version_suffix == \"\":",
          "168:         current_tag = get_latest_provider_tag(provider_id, \"\")",
          "170:             get_console().print(f\"[warning]The 'final' tag {current_tag} exists. Skipping the package.[/]\")",
          "171:             return True, version_suffix",
          "172:         return False, version_suffix",
          "173:     # version_suffix starts with \"rc\"",
          "174:     current_version = int(version_suffix[2:])",
          "175:     while True:",
          "176:         current_tag = get_latest_provider_tag(provider_id, f\"rc{current_version}\")",
          "177:         if tag_exists_for_provider(provider_id, current_tag):",
          "178:             current_version += 1",
          "179:             get_console().print(f\"[warning]The tag {current_tag} exists. Checking rc{current_version}.[/]\")",
          "180:         else:",
          "181:             return False, f\"rc{current_version}\"",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "42263f0bbb87c2833398ae70b05e864e22f042eb",
      "candidate_info": {
        "commit_hash": "42263f0bbb87c2833398ae70b05e864e22f042eb",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/42263f0bbb87c2833398ae70b05e864e22f042eb",
        "files": [
          ".github/workflows/ci.yml",
          ".pre-commit-config.yaml",
          ".rat-excludes",
          "BREEZE.rst",
          "STATIC_CODE_CHECKS.rst",
          "clients/README.md",
          "clients/gen/common.sh",
          "clients/gen/go.sh",
          "clients/gen/python.sh",
          "clients/python/.gitignore",
          "clients/python/.openapi-generator-ignore",
          "clients/python/CHANGELOG.md",
          "clients/python/INSTALL",
          "clients/python/LICENSE",
          "clients/python/NOTICE",
          "clients/python/README.md",
          "clients/python/pyproject.toml",
          "clients/python/test_python_client.py",
          "clients/python/version.txt",
          "dev/README_RELEASE_AIRFLOW.md",
          "dev/README_RELEASE_PYTHON_CLIENT.md",
          "dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py",
          "dev/breeze/src/airflow_breeze/utils/docker_command_utils.py",
          "images/breeze/output_release-management.svg",
          "images/breeze/output_release-management.txt",
          "images/breeze/output_release-management_prepare-airflow-package.svg",
          "images/breeze/output_release-management_prepare-airflow-package.txt",
          "images/breeze/output_release-management_prepare-python-client.svg",
          "images/breeze/output_release-management_prepare-python-client.txt",
          "images/breeze/output_setup_check-all-params-in-groups.svg",
          "images/breeze/output_setup_check-all-params-in-groups.txt",
          "images/breeze/output_setup_regenerate-command-images.svg",
          "images/breeze/output_setup_regenerate-command-images.txt",
          "scripts/ci/docker-compose/local.yml",
          "scripts/ci/openapi/client_codegen_diff.sh"
        ],
        "message": "Generate Python client in reproducible way (#36763)\n\nClient source code and package generation was done using the code\ngenerated and committed to `airflow-client-python` and while the\nrepository with such code is useful to have, it's just a convenience\nrepo, because all sources are (and should be) generated from the\nAPI specification which is present in the Airflow repository.\n\nThis also made the reproducible builds and package generation not really\npossible, because we never knew if the source generated in the\n`airflow-client-python` repository has been generated and not tampered\nwith.\n\nWhile implementing it, it turned out that there were some issues in\nthe past that nade our client generation somewhat broken..\n\n* In 2.7.0 python client, we added the same code twice\n  (See https://github.com/apache/airflow-client-python/pull/93) on\n  top of \"airflow_client.client\" package, we also added copy of the\n  API client generated in \"airflow_client.airflow_client\" - that was\n  likely due to bad bash scripts and tools that were used to generate\n  it and errors during generation the clients.\n\n* We used to generate the code for \"client\" package and then moved\n  the \"client\" package to \"airflow_client.client\" package, while\n  manually modifying imports with `sed` (!?). That was likely due to\n  limitations in some old version of the client generator. However the\n  client generator we use now is capable of generating code directly in\n  the \"airflow_client.client\" package.\n\n* We also manually (via pre-commit) added Apache Licence to the\n  generated files. Whieh was completely unnecessary, because ASF rules\n  do not require licence headers to be added to code automatically\n  generated from a code that already has ASF licence.\n\n* We also generated source tarball packages from such generated code,\n  which was completely unnecessary - because sdist packages are already\n  fulfilling all the reqirements of such source pacakges - the code\n  in the packages is enough to build the package from the sources and\n  it does not contain any binary code, moreover the code is generated\n  out of the API specificiation, which means that anyone can take\n  the code and genearate the pacakged software from just sources in\n  sdist. Similarly as in case of provider packages, we do not need\n  to produce separate -source.tar.gz files.\n\nThis PR fixes all of it.\n\nFirst of all the source that lands in the source repository\n`airflow-client-python` and sdist/wheel packages are generated directly\nfrom the openapi specification.\n\nThey are generated using breeze release_management command from airflow\nsource  tagged with specific tag in the Airflow repo (including the\nsource of reproducible build date that is updated together with airflow\nrelease notes. This means that any PMC member can regenerate packages\n(binary identical) straight from the Airflow repository - without\ngoing through \"airflow-client-python\" repository.\n\nNo source tarball is generated - it is not needed, sdist is enough.\n\nThe `test_python_client.py` has been also moved over to Airflow repo\nand updated with handling case when expose_config is not enabled and\nit is used to automatically test the API client after it has been\ngenerated.\n\n(cherry picked from commit 9787440593196881b481466aa6d3cca4408f99e5)",
        "before_after_code_files": [
          "clients/gen/common.sh||clients/gen/common.sh",
          "clients/gen/go.sh||clients/gen/go.sh",
          "clients/gen/python.sh||clients/gen/python.sh",
          "clients/python/test_python_client.py||clients/python/test_python_client.py",
          "dev/breeze/src/airflow_breeze/commands/release_management_commands.py||dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py||dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py",
          "dev/breeze/src/airflow_breeze/utils/docker_command_utils.py||dev/breeze/src/airflow_breeze/utils/docker_command_utils.py",
          "scripts/ci/openapi/client_codegen_diff.sh||scripts/ci/openapi/client_codegen_diff.sh"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "clients/gen/common.sh||clients/gen/common.sh": [
          "File: clients/gen/common.sh -> clients/gen/common.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "clients/gen/go.sh||clients/gen/go.sh": [
          "File: clients/gen/go.sh -> clients/gen/go.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "clients/gen/python.sh||clients/gen/python.sh": [
          "File: clients/gen/python.sh -> clients/gen/python.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "clients/python/test_python_client.py||clients/python/test_python_client.py": [
          "File: clients/python/test_python_client.py -> clients/python/test_python_client.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: # Licensed to the Apache Software Foundation (ASF) under one",
          "2: # or more contributor license agreements.  See the NOTICE file",
          "3: # distributed with this work for additional information",
          "4: # regarding copyright ownership.  The ASF licenses this file",
          "5: # to you under the Apache License, Version 2.0 (the",
          "6: # \"License\"); you may not use this file except in compliance",
          "7: # with the License.  You may obtain a copy of the License at",
          "8: #",
          "9: #   http://www.apache.org/licenses/LICENSE-2.0",
          "10: #",
          "11: # Unless required by applicable law or agreed to in writing,",
          "12: # software distributed under the License is distributed on an",
          "13: # \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
          "14: # KIND, either express or implied.  See the License for the",
          "15: # specific language governing permissions and limitations",
          "16: # under the License.",
          "17: #",
          "18: # PEP 723 compliant inline script metadata (not yet widely supported)",
          "19: # /// script",
          "20: # requires-python = \">=3.8\"",
          "21: # dependencies = [",
          "22: #   \"apache-airflow-client\",",
          "23: #   \"rich\",",
          "24: # ]",
          "25: # ///",
          "27: from __future__ import annotations",
          "29: import sys",
          "30: import uuid",
          "32: import airflow_client.client",
          "34: try:",
          "35:     # If you have rich installed, you will have nice colored output of the API responses",
          "36:     from rich import print",
          "37: except ImportError:",
          "38:     print(\"Output will not be colored. Please install rich to get colored output: `pip install rich`\")",
          "39:     pass",
          "40: from airflow_client.client.api import config_api, dag_api, dag_run_api",
          "41: from airflow_client.client.model.dag_run import DAGRun",
          "43: # The client must use the authentication and authorization parameters",
          "44: # in accordance with the API server security policy.",
          "45: # Examples for each auth method are provided below, use the example that",
          "46: # satisfies your auth use case.",
          "47: #",
          "48: # In case of the basic authentication below, make sure that Airflow is",
          "49: # configured also with the basic_auth as backend additionally to regular session backend needed",
          "50: # by the UI. In the `[api]` section of your `airflow.cfg` set:",
          "51: #",
          "52: # auth_backend = airflow.api.auth.backend.session,airflow.api.auth.backend.basic_auth",
          "53: #",
          "54: # Make sure that your user/name are configured properly - using the user/password that has admin",
          "55: # privileges in Airflow",
          "57: # Configure HTTP basic authorization: Basic",
          "58: configuration = airflow_client.client.Configuration(",
          "59:     host=\"http://localhost:8080/api/v1\", username=\"admin\", password=\"admin\"",
          "60: )",
          "62: # Make sure in the [core] section, the  `load_examples` config is set to True in your airflow.cfg",
          "63: # or AIRFLOW__CORE__LOAD_EXAMPLES environment variable set to True",
          "64: DAG_ID = \"example_bash_operator\"",
          "66: # Enter a context with an instance of the API client",
          "67: with airflow_client.client.ApiClient(configuration) as api_client:",
          "68:     errors = False",
          "70:     print(\"[blue]Getting DAG list\")",
          "71:     dag_api_instance = dag_api.DAGApi(api_client)",
          "72:     try:",
          "73:         api_response = dag_api_instance.get_dags()",
          "74:         print(api_response)",
          "75:     except airflow_client.client.OpenApiException as e:",
          "76:         print(f\"[red]Exception when calling DagAPI->get_dags: {e}\\n\")",
          "77:         errors = True",
          "78:     else:",
          "79:         print(\"[green]Getting DAG list successful\")",
          "81:     print(\"[blue]Getting Tasks for a DAG\")",
          "82:     try:",
          "83:         api_response = dag_api_instance.get_tasks(DAG_ID)",
          "84:         print(api_response)",
          "85:     except airflow_client.client.exceptions.OpenApiException as e:",
          "86:         print(f\"[red]Exception when calling DagAPI->get_tasks: {e}\\n\")",
          "87:         errors = True",
          "88:     else:",
          "89:         print(\"[green]Getting Tasks successful\")",
          "91:     print(\"[blue]Triggering a DAG run\")",
          "92:     dag_run_api_instance = dag_run_api.DAGRunApi(api_client)",
          "93:     try:",
          "94:         # Create a DAGRun object (no dag_id should be specified because it is read-only property of DAGRun)",
          "95:         # dag_run id is generated randomly to allow multiple executions of the script",
          "96:         dag_run = DAGRun(",
          "97:             dag_run_id=\"some_test_run_\" + uuid.uuid4().hex,",
          "98:         )",
          "99:         api_response = dag_run_api_instance.post_dag_run(DAG_ID, dag_run)",
          "100:         print(api_response)",
          "101:     except airflow_client.client.exceptions.OpenApiException as e:",
          "102:         print(f\"[red]Exception when calling DAGRunAPI->post_dag_run: {e}\\n\")",
          "103:         errors = True",
          "104:     else:",
          "105:         print(\"[green]Posting DAG Run successful\")",
          "107:     # Get current configuration. Note, this is disabled by default with most installation.",
          "108:     # You need to set `expose_config = True` in Airflow configuration in order to retrieve configuration.",
          "109:     conf_api_instance = config_api.ConfigApi(api_client)",
          "110:     try:",
          "111:         api_response = conf_api_instance.get_config()",
          "112:         print(api_response)",
          "113:     except airflow_client.client.OpenApiException as e:",
          "114:         if \"FORBIDDEN\" in str(e):",
          "115:             print(",
          "116:                 \"[yellow]You need to set `expose_config = True` in Airflow configuration\"",
          "117:                 \" in order to retrieve configuration.\"",
          "118:             )",
          "119:             print(\"[bright_blue]This is OK. Exposing config is disabled by default.\")",
          "120:         else:",
          "121:             print(f\"[red]Exception when calling DAGRunAPI->post_dag_run: {e}\\n\")",
          "122:             errors = True",
          "123:     else:",
          "124:         print(\"[green]Config retrieved successfully\")",
          "126:     if errors:",
          "127:         print(\"\\n[red]There were errors while running the script - see above for details\")",
          "128:         sys.exit(1)",
          "129:     else:",
          "130:         print(\"\\n[green]Everything went well\")",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/commands/release_management_commands.py||dev/breeze/src/airflow_breeze/commands/release_management_commands.py": [
          "File: dev/breeze/src/airflow_breeze/commands/release_management_commands.py -> dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "191:     show_default=True,",
          "192:     envvar=\"PACKAGE_FORMAT\",",
          "193: )",
          "195: if TYPE_CHECKING:",
          "196:     from packaging.version import Version",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "194: option_use_local_hatch = click.option(",
          "195:     \"--use-local-hatch\",",
          "196:     is_flag=True,",
          "197:     help=\"Use local hatch instead of docker to build the package. You need to have hatch installed.\",",
          "198: )",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "279: AIRFLOW_BUILD_DOCKERFILE_IGNORE_PATH = AIRFLOW_SOURCES_ROOT / \"airflow-build-dockerfile.dockerignore\"",
          "322:     # This is security feature.",
          "323:     #",
          "324:     # Building the image needed to build airflow package including .git directory",
          "",
          "[Removed Lines]",
          "282: @release_management.command(",
          "283:     name=\"prepare-airflow-package\",",
          "284:     help=\"Prepare sdist/whl package of Airflow.\",",
          "285: )",
          "286: @click.option(",
          "287:     \"--use-local-hatch\",",
          "288:     is_flag=True,",
          "289:     help=\"Use local hatch instead of docker to build the package. You need to have hatch installed.\",",
          "290: )",
          "291: @option_package_format",
          "292: @option_version_suffix_for_pypi",
          "293: @option_verbose",
          "294: @option_dry_run",
          "295: def prepare_airflow_packages(",
          "296:     package_format: str,",
          "297:     version_suffix_for_pypi: str,",
          "298:     use_local_hatch: bool,",
          "299: ):",
          "300:     perform_environment_checks()",
          "301:     fix_ownership_using_docker()",
          "302:     cleanup_python_generated_files()",
          "303:     source_date_epoch = get_source_date_epoch()",
          "304:     if use_local_hatch:",
          "305:         hatch_build_command = [\"hatch\", \"build\", \"-c\", \"-t\", \"custom\"]",
          "306:         if package_format in [\"sdist\", \"both\"]:",
          "307:             hatch_build_command.extend([\"-t\", \"sdist\"])",
          "308:         if package_format in [\"wheel\", \"both\"]:",
          "309:             hatch_build_command.extend([\"-t\", \"wheel\"])",
          "310:         env_copy = os.environ.copy()",
          "311:         env_copy[\"SOURCE_DATE_EPOCH\"] = str(source_date_epoch)",
          "312:         run_command(",
          "313:             hatch_build_command,",
          "314:             check=True,",
          "315:             env=env_copy,",
          "316:         )",
          "317:         get_console().print(\"[success]Successfully prepared Airflow packages:\")",
          "318:         for file in sorted(DIST_DIR.glob(\"apache_airflow*\")):",
          "319:             get_console().print(file.name)",
          "320:         get_console().print()",
          "321:         return",
          "",
          "[Added Lines]",
          "287: def _build_local_build_image():",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "344:         cwd=AIRFLOW_SOURCES_ROOT,",
          "345:         env={\"DOCKER_CLI_HINTS\": \"false\"},",
          "346:     )",
          "347:     container_id = f\"airflow-build-{random.getrandbits(64):08x}\"",
          "348:     result = run_command(",
          "349:         cmd=[",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "315: def _build_airflow_packages_with_docker(",
          "316:     package_format: str, source_date_epoch: int, version_suffix_for_pypi: str",
          "317: ):",
          "318:     _build_local_build_image()",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "355:             \"-e\",",
          "356:             f\"VERSION_SUFFIX_FOR_PYPI={version_suffix_for_pypi}\",",
          "357:             \"-e\",",
          "358:             \"HOME=/opt/airflow/files/home\",",
          "359:             \"-e\",",
          "360:             \"GITHUB_ACTIONS\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "330:             f\"SOURCE_DATE_EPOCH={source_date_epoch}\",",
          "331:             \"-e\",",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "375:     DIST_DIR.mkdir(parents=True, exist_ok=True)",
          "376:     # Copy all files in the dist directory in container to the host dist directory (note '/.' in SRC)",
          "377:     run_command([\"docker\", \"cp\", f\"{container_id}:/opt/airflow/dist/.\", \"./dist\"], check=True)",
          "379:     get_console().print(\"[success]Successfully prepared Airflow packages:\")",
          "380:     for file in sorted(DIST_DIR.glob(\"apache_airflow*\")):",
          "381:         get_console().print(file.name)",
          "",
          "[Removed Lines]",
          "378:     run_command([\"docker\", \"rm\", \"--force\", container_id], check=True)",
          "",
          "[Added Lines]",
          "352:     run_command([\"docker\", \"rm\", \"--force\", container_id], check=False, stderr=DEVNULL, stdout=DEVNULL)",
          "355: def _build_airflow_packages_with_hatch(",
          "356:     package_format: str, source_date_epoch: int, version_suffix_for_pypi: str",
          "357: ):",
          "358:     hatch_build_command = [\"hatch\", \"build\", \"-c\", \"-t\", \"custom\"]",
          "359:     if package_format in [\"sdist\", \"both\"]:",
          "360:         hatch_build_command.extend([\"-t\", \"sdist\"])",
          "361:     if package_format in [\"wheel\", \"both\"]:",
          "362:         hatch_build_command.extend([\"-t\", \"wheel\"])",
          "363:     env_copy = os.environ.copy()",
          "364:     env_copy[\"SOURCE_DATE_EPOCH\"] = str(source_date_epoch)",
          "365:     env_copy[\"VERSION_SUFFIX_FOR_PYPI\"] = version_suffix_for_pypi",
          "366:     run_command(",
          "367:         hatch_build_command,",
          "368:         check=True,",
          "369:         env=env_copy,",
          "370:     )",
          "373: @release_management.command(",
          "374:     name=\"prepare-airflow-package\",",
          "375:     help=\"Prepare sdist/whl package of Airflow.\",",
          "376: )",
          "377: @option_package_format",
          "378: @option_version_suffix_for_pypi",
          "379: @option_use_local_hatch",
          "380: @option_verbose",
          "381: @option_dry_run",
          "382: def prepare_airflow_packages(",
          "383:     package_format: str,",
          "384:     version_suffix_for_pypi: str,",
          "385:     use_local_hatch: bool,",
          "386: ):",
          "387:     perform_environment_checks()",
          "388:     fix_ownership_using_docker()",
          "389:     cleanup_python_generated_files()",
          "390:     source_date_epoch = get_source_date_epoch()",
          "391:     if use_local_hatch:",
          "392:         _build_airflow_packages_with_hatch(",
          "393:             package_format=package_format,",
          "394:             source_date_epoch=source_date_epoch,",
          "395:             version_suffix_for_pypi=version_suffix_for_pypi,",
          "396:         )",
          "397:     else:",
          "398:         _build_airflow_packages_with_docker(",
          "399:             package_format=package_format,",
          "400:             source_date_epoch=source_date_epoch,",
          "401:             version_suffix_for_pypi=version_suffix_for_pypi,",
          "402:         )",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "2078:         type=no_version_file + \"-\" + suffix,",
          "2079:         comparable_version=Version(version),",
          "2080:     )",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "2107: PYTHON_CLIENT_DIR_PATH = AIRFLOW_SOURCES_ROOT / \"clients\" / \"python\"",
          "2108: PYTHON_CLIENT_DIST_DIR_PATH = PYTHON_CLIENT_DIR_PATH / \"dist\"",
          "2109: PYTHON_CLIENT_TMP_DIR = PYTHON_CLIENT_DIR_PATH / \"tmp\"",
          "2111: REPRODUCIBLE_BUILD_YAML = AIRFLOW_SOURCES_ROOT / \"airflow\" / \"reproducible_build.yaml\"",
          "2113: VERSION_FILE = PYTHON_CLIENT_DIR_PATH / \"version.txt\"",
          "2114: SOURCE_API_YAML_PATH = AIRFLOW_SOURCES_ROOT / \"airflow\" / \"api_connexion\" / \"openapi\" / \"v1.yaml\"",
          "2115: TARGET_API_YAML_PATH = PYTHON_CLIENT_DIR_PATH / \"v1.yaml\"",
          "2116: OPENAPI_GENERATOR_CLI_VER = \"5.4.0\"",
          "2118: GENERATED_CLIENT_DIRECTORIES_TO_COPY = [\"airflow_client\", \"docs\", \"test\"]",
          "2119: FILES_TO_COPY_TO_CLIENT_REPO = [",
          "2120:     \".gitignore\",",
          "2121:     \".openapi-generator-ignore\",",
          "2122:     \"CHANGELOG.md\",",
          "2123:     \"README.md\",",
          "2124:     \"INSTALL\",",
          "2125:     \"LICENSE\",",
          "2126:     \"NOTICE\",",
          "2127:     \"pyproject.toml\",",
          "2128:     \"test_python_client.py\",",
          "2129:     \"version.txt\",",
          "2130: ]",
          "2133: def _get_python_client_version(version_suffix_for_pypi):",
          "2134:     from packaging.version import Version",
          "2136:     python_client_version = VERSION_FILE.read_text().strip()",
          "2137:     version = Version(python_client_version)",
          "2138:     if version_suffix_for_pypi:",
          "2139:         if version.pre:",
          "2140:             currrent_suffix = version.pre[0] + str(version.pre[1])",
          "2141:             if currrent_suffix != version_suffix_for_pypi:",
          "2142:                 get_console().print(",
          "2143:                     f\"[error]The version suffix for PyPI ({version_suffix_for_pypi}) does not match the \"",
          "2144:                     f\"suffix in the version ({version})[/]\"",
          "2145:                 )",
          "2146:                 sys.exit(1)",
          "2147:     return version.base_version + version_suffix_for_pypi",
          "2150: def _generate_python_client_sources(python_client_version: str) -> None:",
          "2151:     get_console().print(f\"\\n[info]Generating client code in {PYTHON_CLIENT_TMP_DIR}[/]\")",
          "2152:     result = run_command(",
          "2153:         [",
          "2154:             \"docker\",",
          "2155:             \"run\",",
          "2156:             \"--rm\",",
          "2157:             \"-u\",",
          "2158:             f\"{os.getuid()}:{os.getgid()}\",",
          "2159:             \"-v\",",
          "2160:             f\"{TARGET_API_YAML_PATH}:/spec.yaml\",",
          "2161:             \"-v\",",
          "2162:             f\"{PYTHON_CLIENT_TMP_DIR}:/output\",",
          "2163:             f\"openapitools/openapi-generator-cli:v{OPENAPI_GENERATOR_CLI_VER}\",",
          "2164:             \"generate\",",
          "2165:             \"--input-spec\",",
          "2166:             \"/spec.yaml\",",
          "2167:             \"--generator-name\",",
          "2168:             \"python\",",
          "2169:             \"--git-user-id\",",
          "2170:             f\"{os.environ.get('GIT_USER')}\",",
          "2171:             \"--output\",",
          "2172:             \"/output\",",
          "2173:             \"--package-name\",",
          "2174:             \"airflow_client.client\",",
          "2175:             \"--git-repo-id\",",
          "2176:             \"airflow-client-python\",",
          "2177:             \"--additional-properties\",",
          "2178:             f'packageVersion=\"{python_client_version}\"',",
          "2179:         ],",
          "2180:         capture_output=True,",
          "2181:         text=True,",
          "2182:     )",
          "2183:     if result.returncode != 0:",
          "2184:         get_console().print(\"[error]Failed to generate client code[/]\")",
          "2185:         get_console().print(result.stdout, markup=False)",
          "2186:         get_console().print(result.stderr, markup=False, style=\"error\")",
          "2187:         sys.exit(result.returncode)",
          "2188:     get_console().print(f\"[success]Generated client code in {PYTHON_CLIENT_TMP_DIR}:[/]\")",
          "2189:     get_console().print(f\"\\n[info]Content of {PYTHON_CLIENT_TMP_DIR}:[/]\")",
          "2190:     for file in sorted(PYTHON_CLIENT_TMP_DIR.glob(\"*\")):",
          "2191:         get_console().print(f\"[info]  {file.name}[/]\")",
          "2192:     get_console().print()",
          "2195: def _copy_selected_sources_from_tmp_directory_to_clients_python():",
          "2196:     get_console().print(",
          "2197:         f\"[info]Copying selected sources: {GENERATED_CLIENT_DIRECTORIES_TO_COPY} from \"",
          "2198:         f\"{PYTHON_CLIENT_TMP_DIR} to {PYTHON_CLIENT_DIR_PATH}[/]\"",
          "2199:     )",
          "2200:     for dir in GENERATED_CLIENT_DIRECTORIES_TO_COPY:",
          "2201:         source_dir = PYTHON_CLIENT_TMP_DIR / dir",
          "2202:         target_dir = PYTHON_CLIENT_DIR_PATH / dir",
          "2203:         get_console().print(f\"[info]  Copying generated sources from {source_dir} to {target_dir}[/]\")",
          "2204:         shutil.rmtree(target_dir, ignore_errors=True)",
          "2205:         shutil.copytree(source_dir, target_dir)",
          "2206:         get_console().print(f\"[success]  Copied generated sources from {source_dir} to {target_dir}[/]\")",
          "2207:     get_console().print(",
          "2208:         f\"[info]Copied selected sources {GENERATED_CLIENT_DIRECTORIES_TO_COPY} from \"",
          "2209:         f\"{PYTHON_CLIENT_TMP_DIR} to {PYTHON_CLIENT_DIR_PATH}[/]\\n\"",
          "2210:     )",
          "2211:     get_console().print(f\"\\n[info]Content of {PYTHON_CLIENT_DIR_PATH}:[/]\")",
          "2212:     for file in sorted(PYTHON_CLIENT_DIR_PATH.glob(\"*\")):",
          "2213:         get_console().print(f\"[info]  {file.name}[/]\")",
          "2214:     get_console().print()",
          "2217: def _build_client_packages_with_hatch(source_date_epoch: int, package_format: str):",
          "2218:     command = [",
          "2219:         \"hatch\",",
          "2220:         \"build\",",
          "2221:         \"-c\",",
          "2222:     ]",
          "2223:     if package_format == \"sdist\" or package_format == \"both\":",
          "2224:         command += [\"-t\", \"sdist\"]",
          "2225:     if package_format == \"wheel\" or package_format == \"both\":",
          "2226:         command += [\"-t\", \"wheel\"]",
          "2227:     env_copy = os.environ.copy()",
          "2228:     env_copy[\"SOURCE_DATE_EPOCH\"] = str(source_date_epoch)",
          "2229:     run_command(",
          "2230:         cmd=command,",
          "2231:         cwd=PYTHON_CLIENT_DIR_PATH,",
          "2232:         env=env_copy,",
          "2233:         check=True,",
          "2234:     )",
          "2235:     shutil.copytree(PYTHON_CLIENT_DIST_DIR_PATH, DIST_DIR, dirs_exist_ok=True)",
          "2238: def _build_client_packages_with_docker(source_date_epoch: int, package_format: str):",
          "2239:     _build_local_build_image()",
          "2240:     command = \"hatch build -c \"",
          "2241:     if package_format == \"sdist\" or package_format == \"both\":",
          "2242:         command += \"-t sdist \"",
          "2243:     if package_format == \"wheel\" or package_format == \"both\":",
          "2244:         command += \"-t wheel \"",
          "2245:     container_id = f\"airflow-build-{random.getrandbits(64):08x}\"",
          "2246:     result = run_command(",
          "2247:         cmd=[",
          "2248:             \"docker\",",
          "2249:             \"run\",",
          "2250:             \"--name\",",
          "2251:             container_id,",
          "2252:             \"-t\",",
          "2253:             \"-e\",",
          "2254:             f\"SOURCE_DATE_EPOCH={source_date_epoch}\",",
          "2255:             \"-e\",",
          "2256:             \"HOME=/opt/airflow/files/home\",",
          "2257:             \"-e\",",
          "2258:             \"GITHUB_ACTIONS\",",
          "2259:             \"-w\",",
          "2260:             \"/opt/airflow/clients/python\",",
          "2261:             AIRFLOW_BUILD_IMAGE_TAG,",
          "2262:             \"bash\",",
          "2263:             \"-c\",",
          "2264:             command,",
          "2265:         ],",
          "2266:         check=False,",
          "2267:     )",
          "2268:     if result.returncode != 0:",
          "2269:         get_console().print(\"[error]Error preparing Python client packages[/]\")",
          "2270:         fix_ownership_using_docker()",
          "2271:         sys.exit(result.returncode)",
          "2272:     DIST_DIR.mkdir(parents=True, exist_ok=True)",
          "2273:     get_console().print()",
          "2274:     # Copy all files in the dist directory in container to the host dist directory (note '/.' in SRC)",
          "2275:     run_command([\"docker\", \"cp\", f\"{container_id}:/opt/airflow/clients/python/dist/.\", \"./dist\"], check=True)",
          "2276:     run_command([\"docker\", \"rm\", \"--force\", container_id], check=False, stdout=DEVNULL, stderr=DEVNULL)",
          "2279: @release_management.command(name=\"prepare-python-client\", help=\"Prepares python client packages.\")",
          "2280: @option_package_format",
          "2281: @option_version_suffix_for_pypi",
          "2282: @option_use_local_hatch",
          "2283: @click.option(",
          "2284:     \"--python-client-repo\",",
          "2285:     envvar=\"PYTHON_CLIENT_REPO\",",
          "2286:     type=click.Path(file_okay=False, dir_okay=True, path_type=Path, exists=True),",
          "2287:     help=\"Directory where the python client repo is checked out\",",
          "2288: )",
          "2289: @click.option(",
          "2290:     \"--only-publish-build-scripts\",",
          "2291:     envvar=\"ONLY_PUBLISH_BUILD_SCRIPTS\",",
          "2292:     is_flag=True,",
          "2293:     help=\"Only publish updated build scripts to puthon client repo, not generated client code.\",",
          "2294: )",
          "2295: @click.option(",
          "2296:     \"--security-schemes\",",
          "2297:     default=\"Basic,GoogleOpenID,Kerberos\",",
          "2298:     envvar=\"SECURITY_SCHEMES\",",
          "2299:     show_default=True,",
          "2300:     help=\"Security schemes to be added to the API documentation (coma separated)\",",
          "2301: )",
          "2302: @option_dry_run",
          "2303: @option_verbose",
          "2304: def prepare_python_client(",
          "2305:     package_format: str,",
          "2306:     version_suffix_for_pypi: str,",
          "2307:     use_local_hatch: bool,",
          "2308:     python_client_repo: Path | None,",
          "2309:     only_publish_build_scripts: bool,",
          "2310:     security_schemes: str,",
          "2311: ):",
          "2312:     shutil.rmtree(PYTHON_CLIENT_TMP_DIR, ignore_errors=True)",
          "2313:     PYTHON_CLIENT_TMP_DIR.mkdir(parents=True, exist_ok=True)",
          "2314:     shutil.copy(src=SOURCE_API_YAML_PATH, dst=TARGET_API_YAML_PATH)",
          "2315:     import yaml",
          "2317:     openapi_yaml = yaml.safe_load(TARGET_API_YAML_PATH.read_text())",
          "2319:     # Add security schemes to documentation",
          "2320:     security: list[dict[str, Any]] = []",
          "2321:     for scheme in security_schemes.split(\",\"):",
          "2322:         security.append({scheme: []})",
          "2323:     openapi_yaml[\"security\"] = security",
          "2324:     python_client_version = _get_python_client_version(version_suffix_for_pypi)",
          "2325:     TARGET_API_YAML_PATH.write_text(yaml.dump(openapi_yaml))",
          "2327:     _generate_python_client_sources(python_client_version=python_client_version)",
          "2328:     _copy_selected_sources_from_tmp_directory_to_clients_python()",
          "2330:     reproducible_build_yaml = yaml.safe_load(REPRODUCIBLE_BUILD_YAML.read_text())",
          "2331:     source_date_epoch = reproducible_build_yaml[\"source-date-epoch\"]",
          "2333:     if python_client_repo:",
          "2334:         if not only_publish_build_scripts:",
          "2335:             get_console().print(",
          "2336:                 f\"[info]Copying generated client from {PYTHON_CLIENT_DIR_PATH} to {python_client_repo}[/]\"",
          "2337:             )",
          "2338:             for dir in GENERATED_CLIENT_DIRECTORIES_TO_COPY:",
          "2339:                 source_dir = PYTHON_CLIENT_DIR_PATH / dir",
          "2340:                 target_dir = python_client_repo / dir",
          "2341:                 get_console().print(f\"[info]  Copying {source_dir} to {target_dir}[/]\")",
          "2342:                 shutil.rmtree(target_dir, ignore_errors=True)",
          "2343:                 shutil.copytree(source_dir, target_dir)",
          "2344:                 get_console().print(f\"[success]  Copied {source_dir} to {target_dir}[/]\")",
          "2345:             get_console().print(",
          "2346:                 f\"[info]Copied generated client from {PYTHON_CLIENT_DIR_PATH} to {python_client_repo}[/]\"",
          "2347:             )",
          "2348:         get_console().print(",
          "2349:             f\"[info]Copying build scripts from {PYTHON_CLIENT_DIR_PATH} to {python_client_repo}[/]\"",
          "2350:         )",
          "2351:         for file in FILES_TO_COPY_TO_CLIENT_REPO:",
          "2352:             source_file = PYTHON_CLIENT_DIR_PATH / file",
          "2353:             target_file = python_client_repo / file",
          "2354:             get_console().print(f\"[info]  Copying {source_file} to {target_file}[/]\")",
          "2355:             shutil.copy(source_file, target_file)",
          "2356:             get_console().print(f\"[success]  Copied {source_file} to {target_file}[/]\")",
          "2357:         get_console().print(",
          "2358:             f\"[success]Copied build scripts from {PYTHON_CLIENT_DIR_PATH} to {python_client_repo}[/]\"",
          "2359:         )",
          "2360:         spec_dir = python_client_repo / \"spec\"",
          "2361:         spec_dir.mkdir(parents=True, exist_ok=True)",
          "2362:         source_spec_file = PYTHON_CLIENT_DIR_PATH / \"v1.yaml\"",
          "2363:         target_spec_file = spec_dir / \"v1.yaml\"",
          "2364:         get_console().print(f\"[info]  Copying {source_spec_file} to {target_spec_file}[/]\")",
          "2365:         shutil.copy(source_spec_file, target_spec_file)",
          "2366:         get_console().print(f\"[success]  Copied {source_spec_file} to {target_spec_file}[/]\")",
          "2367:         get_console().print(",
          "2368:             f\"[success]Copied client code from {PYTHON_CLIENT_DIR_PATH} to {python_client_repo}[/]\\n\"",
          "2369:         )",
          "2370:     else:",
          "2371:         get_console().print(",
          "2372:             \"\\n[warning]No python client repo directory provided - skipping copying the generated client[/]\\n\"",
          "2373:         )",
          "2374:     get_console().print(f\"\\n[info]Building packages in {PYTHON_CLIENT_DIST_DIR_PATH}[/]\\n\")",
          "2375:     shutil.rmtree(PYTHON_CLIENT_DIST_DIR_PATH, ignore_errors=True)",
          "2376:     PYTHON_CLIENT_DIST_DIR_PATH.mkdir(parents=True, exist_ok=True)",
          "2377:     version = _get_python_client_version(version_suffix_for_pypi)",
          "2378:     original_version = VERSION_FILE.read_text().strip()",
          "2379:     if version_suffix_for_pypi:",
          "2380:         VERSION_FILE.write_text(version)",
          "2381:     try:",
          "2382:         if use_local_hatch:",
          "2383:             _build_client_packages_with_hatch(",
          "2384:                 source_date_epoch=source_date_epoch, package_format=package_format",
          "2385:             )",
          "2386:         else:",
          "2387:             _build_client_packages_with_docker(",
          "2388:                 source_date_epoch=source_date_epoch, package_format=package_format",
          "2389:             )",
          "2390:         get_console().print(f\"\\n[success]Built packages in {DIST_DIR}[/]\\n\")",
          "2391:     finally:",
          "2392:         if version_suffix_for_pypi:",
          "2393:             VERSION_FILE.write_text(original_version)",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py||dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py": [
          "File: dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py -> dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "45:     \"name\": \"Other release commands\",",
          "46:     \"commands\": [",
          "47:         \"add-back-references\",",
          "48:         \"publish-docs\",",
          "49:         \"generate-constraints\",",
          "50:         \"update-constraints\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "48:         \"prepare-python-client\",",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "57:             \"name\": \"Package flags\",",
          "58:             \"options\": [",
          "59:                 \"--package-format\",",
          "61:                 \"--version-suffix-for-pypi\",",
          "62:             ],",
          "63:         }",
          "64:     ],",
          "",
          "[Removed Lines]",
          "60:                 \"--use-local-hatch\",",
          "",
          "[Added Lines]",
          "62:                 \"--use-local-hatch\",",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "167:             ],",
          "168:         }",
          "169:     ],",
          "170:     \"breeze release-management generate-constraints\": [",
          "171:         {",
          "172:             \"name\": \"Generate constraints flags\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "171:     \"breeze release-management prepare-python-client\": [",
          "172:         {",
          "173:             \"name\": \"Python client preparation flags\",",
          "174:             \"options\": [",
          "175:                 \"--package-format\",",
          "176:                 \"--version-suffix-for-pypi\",",
          "177:                 \"--use-local-hatch\",",
          "178:                 \"--python-client-repo\",",
          "179:                 \"--only-publish-build-scripts\",",
          "180:                 \"--security-schemes\",",
          "181:             ],",
          "182:         }",
          "183:     ],",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/utils/docker_command_utils.py||dev/breeze/src/airflow_breeze/utils/docker_command_utils.py": [
          "File: dev/breeze/src/airflow_breeze/utils/docker_command_utils.py -> dev/breeze/src/airflow_breeze/utils/docker_command_utils.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "84:     (\"RELEASE_NOTES.rst\", \"/opt/airflow/RELEASE_NOTES.rst\"),",
          "85:     (\"airflow\", \"/opt/airflow/airflow\"),",
          "86:     (\"constraints\", \"/opt/airflow/constraints\"),",
          "87:     (\"dags\", \"/opt/airflow/dags\"),",
          "88:     (\"dev\", \"/opt/airflow/dev\"),",
          "89:     (\"docs\", \"/opt/airflow/docs\"),",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "87:     (\"clients\", \"/opt/airflow/clients\"),",
          "",
          "---------------"
        ],
        "scripts/ci/openapi/client_codegen_diff.sh||scripts/ci/openapi/client_codegen_diff.sh": [
          "File: scripts/ci/openapi/client_codegen_diff.sh -> scripts/ci/openapi/client_codegen_diff.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "dd6572fb0350d0f8f5473a2d30163a550b7dff6a",
      "candidate_info": {
        "commit_hash": "dd6572fb0350d0f8f5473a2d30163a550b7dff6a",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/dd6572fb0350d0f8f5473a2d30163a550b7dff6a",
        "files": [
          "dev/breeze/src/airflow_breeze/utils/docker_command_utils.py",
          "dev/breeze/src/airflow_breeze/utils/path_utils.py"
        ],
        "message": "Check executable permission for entrypoints at breeze start (#36482)\n\n* Check executable permission for entrypoints at breeze start\n\nSometimes our contributors check out Airflow repository on filesystems\nthat are not POSIX compliant and do not have support for executable bits\n(for example when you check-out the repository in Windows and attempt to\nmap it to a Linux VM). Breeze and building CI images will not\nwork in this case, but the error that you see might be misleading.\n\nThis PR performs additional environment check and informs you that\nyou should not do it, if executable bits are missing from entrypoints.\n\n* Update dev/breeze/src/airflow_breeze/utils/docker_command_utils.py\n\nCo-authored-by: Jens Scheffler <95105677+jscheffl@users.noreply.github.com>\n\n---------\n\nCo-authored-by: Jens Scheffler <95105677+jscheffl@users.noreply.github.com>\n(cherry picked from commit 5551e14f6c0d8706bf8beaaab6a8fa5c80719a89)",
        "before_after_code_files": [
          "dev/breeze/src/airflow_breeze/utils/docker_command_utils.py||dev/breeze/src/airflow_breeze/utils/docker_command_utils.py",
          "dev/breeze/src/airflow_breeze/utils/path_utils.py||dev/breeze/src/airflow_breeze/utils/path_utils.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/breeze/src/airflow_breeze/utils/docker_command_utils.py||dev/breeze/src/airflow_breeze/utils/docker_command_utils.py": [
          "File: dev/breeze/src/airflow_breeze/utils/docker_command_utils.py -> dev/breeze/src/airflow_breeze/utils/docker_command_utils.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "30: from airflow_breeze.utils.host_info_utils import get_host_group_id, get_host_os, get_host_user_id",
          "31: from airflow_breeze.utils.path_utils import (",
          "32:     AIRFLOW_SOURCES_ROOT,",
          "33:     cleanup_python_generated_files,",
          "34:     create_mypy_volume_if_needed,",
          "35: )",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "33:     SCRIPTS_DOCKER_DIR,",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "481:         env_variables[\"AIRFLOW__CELERY__BROKER_URL\"] = url_map[params.celery_broker]",
          "484: def perform_environment_checks(quiet: bool = False):",
          "485:     check_docker_is_running()",
          "486:     check_docker_version(quiet)",
          "487:     check_docker_compose_version(quiet)",
          "490: def get_docker_syntax_version() -> str:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "485: def check_executable_entrypoint_permissions(quiet: bool = False):",
          "486:     \"\"\"",
          "487:     Checks if the user has executable permissions on the entrypoints in checked-out airflow repository..",
          "488:     \"\"\"",
          "489:     for entrypoint in SCRIPTS_DOCKER_DIR.glob(\"entrypoint*.sh\"):",
          "490:         if get_verbose() and not quiet:",
          "491:             get_console().print(f\"[info]Checking executable permissions on {entrypoint.as_posix()}[/]\")",
          "492:         if not os.access(entrypoint.as_posix(), os.X_OK):",
          "493:             get_console().print(",
          "494:                 f\"[error]You do not have executable permissions on {entrypoint}[/]\\n\"",
          "495:                 f\"You likely checked out airflow repo on a filesystem that does not support executable \"",
          "496:                 f\"permissions (for example on a Windows filesystem that is mapped to Linux VM). Airflow \"",
          "497:                 f\"repository should only be checked out on a filesystem that is POSIX compliant.\"",
          "498:             )",
          "499:             sys.exit(1)",
          "500:     if not quiet:",
          "501:         get_console().print(\"[success]Executable permissions on entrypoints are OK[/]\")",
          "508:     check_executable_entrypoint_permissions(quiet)",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/utils/path_utils.py||dev/breeze/src/airflow_breeze/utils/path_utils.py": [
          "File: dev/breeze/src/airflow_breeze/utils/path_utils.py -> dev/breeze/src/airflow_breeze/utils/path_utils.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "291: GENERATED_PROVIDER_PACKAGES_DIR = DIST_DIR / \"provider_packages\"",
          "292: DOCS_DIR = AIRFLOW_SOURCES_ROOT / \"docs\"",
          "293: SCRIPTS_CI_DIR = AIRFLOW_SOURCES_ROOT / \"scripts\" / \"ci\"",
          "294: SCRIPTS_CI_DOCKER_COMPOSE_DIR = SCRIPTS_CI_DIR / \"docker-compose\"",
          "295: SCRIPTS_CI_DOCKER_COMPOSE_LOCAL_YAML_FILE = SCRIPTS_CI_DOCKER_COMPOSE_DIR / \"local.yml\"",
          "296: GENERATED_DOCKER_COMPOSE_ENV_FILE = SCRIPTS_CI_DOCKER_COMPOSE_DIR / \"_generated_docker_compose.env\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "294: SCRIPTS_DOCKER_DIR = AIRFLOW_SOURCES_ROOT / \"scripts\" / \"docker\"",
          "",
          "---------------"
        ]
      }
    }
  ]
}