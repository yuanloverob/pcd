{
  "cve_id": "CVE-2022-40604",
  "cve_desc": "In Apache Airflow 2.3.0 through 2.3.4, part of a url was unnecessarily formatted, allowing for possible information extraction.",
  "repo": "apache/airflow",
  "patch_hash": "6f24836e5ee56c452947aa87f84a21dd4f8eb87c",
  "patch_info": {
    "commit_hash": "6f24836e5ee56c452947aa87f84a21dd4f8eb87c",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/6f24836e5ee56c452947aa87f84a21dd4f8eb87c",
    "files": [
      "airflow/utils/log/file_task_handler.py"
    ],
    "message": "Properly build URL to retrieve logs independently from system (#26337)\n\nThe previous way of building the path depended on the OS path\nbut it was really used to build the URL so we should use\nurllib instead of os.path.join.\n\n(cherry picked from commit 18386026c28939fa6d91d198c5489c295a05dcd2)",
    "before_after_code_files": [
      "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py"
    ]
  },
  "patch_diff": {
    "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py": [
      "File: airflow/utils/log/file_task_handler.py -> airflow/utils/log/file_task_handler.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "21: import warnings",
      "22: from pathlib import Path",
      "23: from typing import TYPE_CHECKING, Optional",
      "25: from airflow.configuration import AirflowConfigException, conf",
      "26: from airflow.exceptions import RemovedInAirflow3Warning",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "24: from urllib.parse import urljoin",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "194:         else:",
      "195:             import httpx",
      "199:             )",
      "200:             log += f\"*** Log file does not exist: {location}\\n\"",
      "201:             log += f\"*** Fetching from: {url}\\n\"",
      "",
      "[Removed Lines]",
      "197:             url = os.path.join(\"http://{ti.hostname}:{worker_log_server_port}/log\", log_relative_path).format(",
      "198:                 ti=ti, worker_log_server_port=conf.get('logging', 'WORKER_LOG_SERVER_PORT')",
      "",
      "[Added Lines]",
      "198:             url = urljoin(",
      "199:                 f\"http://{ti.hostname}:{conf.get('logging', 'WORKER_LOG_SERVER_PORT')}/log\", log_relative_path",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "3871f00230b7064e6234aa65b8a00ee2b6d33e4c",
      "candidate_info": {
        "commit_hash": "3871f00230b7064e6234aa65b8a00ee2b6d33e4c",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/3871f00230b7064e6234aa65b8a00ee2b6d33e4c",
        "files": [
          "airflow/www/views.py"
        ],
        "message": "Fix UI redirect (#26409)\n\nCo-authored-by: Konstantin Weddige <konstantin.weddige@lutrasecurity.com>\n(cherry picked from commit 56e7555c42f013f789a4b718676ff09b4a9d5135)",
        "before_after_code_files": [
          "airflow/www/views.py||airflow/www/views.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/26371"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/www/views.py||airflow/www/views.py": [
          "File: airflow/www/views.py -> airflow/www/views.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "2329:         task_id = args.get('task_id')",
          "2330:         dag_run_id = args.get('dag_run_id')",
          "2331:         state = args.get('state')",
          "2334:         if 'map_index' not in args:",
          "2335:             map_indexes: list[int] | None = None",
          "",
          "[Removed Lines]",
          "2332:         origin = args.get('origin')",
          "",
          "[Added Lines]",
          "2332:         origin = get_safe_url(args.get('origin'))",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "c06d8dbe9dddfb924041747cc84ae2d12799e0da",
      "candidate_info": {
        "commit_hash": "c06d8dbe9dddfb924041747cc84ae2d12799e0da",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/c06d8dbe9dddfb924041747cc84ae2d12799e0da",
        "files": [
          "dev/breeze/src/airflow_breeze/commands/release_management_commands.py"
        ],
        "message": "Add better progress in CI for constraints generation. (#26253)\n\nCurrently constraints generation is not really showing good progress\nwhile the packages are being removed/installed. This adds progress\nthat shows that something happens.\n\n(cherry picked from commit d6f473b31d9902290dc3a204657b4a7ed8d7843b)",
        "before_after_code_files": [
          "dev/breeze/src/airflow_breeze/commands/release_management_commands.py||dev/breeze/src/airflow_breeze/commands/release_management_commands.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/26371"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/breeze/src/airflow_breeze/commands/release_management_commands.py||dev/breeze/src/airflow_breeze/commands/release_management_commands.py": [
          "File: dev/breeze/src/airflow_breeze/commands/release_management_commands.py -> dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "65:     get_extra_docker_flags,",
          "66:     perform_environment_checks,",
          "67: )",
          "69: from airflow_breeze.utils.python_versions import get_python_version_list",
          "70: from airflow_breeze.utils.run_utils import (",
          "71:     RunCommandResult,",
          "",
          "[Removed Lines]",
          "68: from airflow_breeze.utils.parallel import check_async_run_results, run_with_pool",
          "",
          "[Added Lines]",
          "68: from airflow_breeze.utils.parallel import GenericRegexpProgressMatcher, check_async_run_results, run_with_pool",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "305:     )",
          "308: def run_generate_constraints_in_parallel(",
          "309:     shell_params_list: List[ShellParams],",
          "310:     python_version_list: List[str],",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "308: CONSTRAINT_PROGRESS_MATCHER = (",
          "309:     r'Found|Uninstalling|uninstalled|Collecting|Downloading|eta|Running|Installing|built|Attempting'",
          "310: )",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "320:             f\"Constraints {shell_params.airflow_constraints_mode}:{shell_params.python}\"",
          "321:             for shell_params in shell_params_list",
          "322:         ]",
          "324:             results = [",
          "325:                 pool.apply_async(",
          "326:                     run_generate_constraints,",
          "",
          "[Removed Lines]",
          "323:         with run_with_pool(parallelism=parallelism, all_params=all_params) as (pool, outputs):",
          "",
          "[Added Lines]",
          "328:         with run_with_pool(",
          "329:             parallelism=parallelism,",
          "330:             all_params=all_params,",
          "331:             progress_matcher=GenericRegexpProgressMatcher(",
          "332:                 regexp=CONSTRAINT_PROGRESS_MATCHER, lines_to_search=6",
          "333:             ),",
          "334:         ) as (pool, outputs):",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "e60d2b5b88c6a45bddc1e671c8495ae04e32ecfe",
      "candidate_info": {
        "commit_hash": "e60d2b5b88c6a45bddc1e671c8495ae04e32ecfe",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/e60d2b5b88c6a45bddc1e671c8495ae04e32ecfe",
        "files": [
          "Dockerfile",
          "Dockerfile.ci"
        ],
        "message": "Limit eager upgrade of protobuf library to < 4.21.0 (#26243)\n\n* Limit eager upgrade of protobuf library to < 4.21.0\n\nUntil all the Google client libraries get upgraded to >= 2.0.0, we need to\nlimit the protobuf version.\n\n(cherry picked from commit c6053f52908d6f1605b5e9a0566a653e90cc2d0e)",
        "before_after_code_files": [
          "Dockerfile.ci||Dockerfile.ci"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/26371"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "Dockerfile.ci||Dockerfile.ci": [
          "File: Dockerfile.ci -> Dockerfile.ci",
          "--- Hunk 1 ---",
          "[Context before]",
          "1133: # * dill<0.3.3 required by apache-beam",
          "1134: # * pyarrow>=6.0.0 is because pip resolver decides for Python 3.10 to downgrade pyarrow to 5 even if it is OK",
          "1135: #   for python 3.10 and other dependencies adding the limit helps resolver to make better decisions",
          "1137: ARG UPGRADE_TO_NEWER_DEPENDENCIES=\"false\"",
          "1138: ENV EAGER_UPGRADE_ADDITIONAL_REQUIREMENTS=${EAGER_UPGRADE_ADDITIONAL_REQUIREMENTS} \\",
          "1139:     UPGRADE_TO_NEWER_DEPENDENCIES=${UPGRADE_TO_NEWER_DEPENDENCIES}",
          "",
          "[Removed Lines]",
          "1136: ARG EAGER_UPGRADE_ADDITIONAL_REQUIREMENTS=\"dill<0.3.3 pyarrow>=6.0.0\"",
          "",
          "[Added Lines]",
          "1136: # We need to limit the protobuf library to < 4.21.0 because not all google libraries we use",
          "1137: # are compatible with the new protobuf version. All the google python client libraries need",
          "1138: # to be upgraded to >= 2.0.0 in order to able to lift that limitation",
          "1139: # https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates",
          "1140: ARG EAGER_UPGRADE_ADDITIONAL_REQUIREMENTS=\"dill<0.3.3 pyarrow>=6.0.0 protobuf<4.21.0\"",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "25f601168f990a1fb16bba3a8335ff8933bf41ed",
      "candidate_info": {
        "commit_hash": "25f601168f990a1fb16bba3a8335ff8933bf41ed",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/25f601168f990a1fb16bba3a8335ff8933bf41ed",
        "files": [
          "airflow/utils/dag_cycle_tester.py"
        ],
        "message": "Add the dag_id to AirflowDagCycleException message (#26204)\n\n(cherry picked from commit aada79e1dabe729362ddd8a3c7eb43efb8fc1c17)",
        "before_after_code_files": [
          "airflow/utils/dag_cycle_tester.py||airflow/utils/dag_cycle_tester.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/26371"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/utils/dag_cycle_tester.py||airflow/utils/dag_cycle_tester.py": [
          "File: airflow/utils/dag_cycle_tester.py -> airflow/utils/dag_cycle_tester.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "58:         \"\"\"Returns first untraversed child task, else None if all tasks traversed.\"\"\"",
          "59:         for adjacent_task in current_task.get_direct_relative_ids():",
          "60:             if visited[adjacent_task] == CYCLE_IN_PROGRESS:",
          "62:                 raise AirflowDagCycleException(msg)",
          "63:             elif visited[adjacent_task] == CYCLE_NEW:",
          "64:                 return adjacent_task",
          "",
          "[Removed Lines]",
          "61:                 msg = f\"Cycle detected in DAG. Faulty task: {task_id}\"",
          "",
          "[Added Lines]",
          "61:                 msg = f\"Cycle detected in DAG: {dag.dag_id}. Faulty task: {task_id}\"",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "c7ea01d67652dbc057ba2f6e52525ded0a4d4762",
      "candidate_info": {
        "commit_hash": "c7ea01d67652dbc057ba2f6e52525ded0a4d4762",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/c7ea01d67652dbc057ba2f6e52525ded0a4d4762",
        "files": [
          "airflow/datasets/__init__.py",
          "tests/datasets/test_dataset.py",
          "tests/models/test_dataset.py"
        ],
        "message": "Better validation of Dataset URI during dag parse (#26389)\n\nPreviously we had the validation on the Dataset model, but we since\nmoved the \"dag\" facing class to a separate one. This adds validation to\nthe public class, and extends the validation to not allow space-only\nstrings\n\n(cherry picked from commit bd181daced707680ed22f5fd74e1e13094f6b164)",
        "before_after_code_files": [
          "airflow/datasets/__init__.py||airflow/datasets/__init__.py",
          "tests/datasets/test_dataset.py||tests/datasets/test_dataset.py",
          "tests/models/test_dataset.py||tests/models/test_dataset.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/26371"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/datasets/__init__.py||airflow/datasets/__init__.py": [
          "File: airflow/datasets/__init__.py -> airflow/datasets/__init__.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "17: from __future__ import annotations",
          "19: from typing import Any",
          "21: import attr",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "20: from urllib.parse import urlparse",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "25: class Dataset:",
          "26:     \"\"\"A Dataset is used for marking data dependencies between workflows.\"\"\"",
          "29:     extra: dict[str, Any] | None = None",
          "",
          "[Removed Lines]",
          "28:     uri: str",
          "",
          "[Added Lines]",
          "29:     uri: str = attr.field(validator=[attr.validators.min_len(1), attr.validators.max_len(3000)])",
          "32:     @uri.validator",
          "33:     def _check_uri(self, attr, uri: str):",
          "34:         if uri.isspace():",
          "35:             raise ValueError(f'{attr.name} cannot be just whitespace')",
          "36:         try:",
          "37:             uri.encode('ascii')",
          "38:         except UnicodeEncodeError:",
          "39:             raise ValueError(f'{attr.name!r} must be ascii')",
          "40:         parsed = urlparse(uri)",
          "41:         if parsed.scheme and parsed.scheme.lower() == 'airflow':",
          "42:             raise ValueError(f'{attr.name!r} scheme `airflow` is reserved')",
          "",
          "---------------"
        ],
        "tests/datasets/test_dataset.py||tests/datasets/test_dataset.py": [
          "File: tests/datasets/test_dataset.py -> tests/datasets/test_dataset.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: # Licensed to the Apache Software Foundation (ASF) under one",
          "2: # or more contributor license agreements.  See the NOTICE file",
          "3: # distributed with this work for additional information",
          "4: # regarding copyright ownership.  The ASF licenses this file",
          "5: # to you under the Apache License, Version 2.0 (the",
          "6: # \"License\"); you may not use this file except in compliance",
          "7: # with the License.  You may obtain a copy of the License at",
          "8: #",
          "9: #   http://www.apache.org/licenses/LICENSE-2.0",
          "10: #",
          "11: # Unless required by applicable law or agreed to in writing,",
          "12: # software distributed under the License is distributed on an",
          "13: # \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
          "14: # KIND, either express or implied.  See the License for the",
          "15: # specific language governing permissions and limitations",
          "16: # under the License.",
          "18: from __future__ import annotations",
          "20: import pytest",
          "22: from airflow.datasets import Dataset",
          "23: from airflow.operators.empty import EmptyOperator",
          "26: @pytest.mark.parametrize(",
          "27:     [\"uri\"],",
          "28:     [",
          "29:         pytest.param(\"\", id=\"empty\"),",
          "30:         pytest.param(\"\\n\\t\", id=\"whitespace\"),",
          "31:         pytest.param(\"a\" * 3001, id=\"too_long\"),",
          "32:         pytest.param(\"airflow:\" * 3001, id=\"reserved_scheme\"),",
          "33:         pytest.param(\"\ud83d\ude0a\" * 3001, id=\"non-ascii\"),",
          "34:     ],",
          "35: )",
          "36: def test_invalid_uris(uri):",
          "37:     with pytest.raises(ValueError):",
          "38:         Dataset(uri=uri)",
          "41: def test_uri_with_scheme():",
          "42:     dataset = Dataset(uri=\"s3://example_dataset\")",
          "43:     EmptyOperator(task_id=\"task1\", outlets=[dataset])",
          "46: def test_uri_without_scheme():",
          "47:     dataset = Dataset(uri=\"example_dataset\")",
          "48:     EmptyOperator(task_id=\"task1\", outlets=[dataset])",
          "",
          "---------------"
        ],
        "tests/models/test_dataset.py||tests/models/test_dataset.py": [
          "File: tests/models/test_dataset.py -> tests/models/test_dataset.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ]
      }
    }
  ]
}