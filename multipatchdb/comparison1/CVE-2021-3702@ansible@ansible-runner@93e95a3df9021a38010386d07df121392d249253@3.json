{
  "cve_id": "CVE-2021-3702",
  "cve_desc": "A race condition flaw was found in ansible-runner, where an attacker could watch for rapid creation and deletion of a temporary directory, substitute their directory at that name, and then have access to ansible-runner's private_data_dir the next time ansible-runner made use of the private_data_dir. The highest Threat out of this flaw is to integrity and confidentiality.",
  "repo": "ansible/ansible-runner",
  "patch_hash": "93e95a3df9021a38010386d07df121392d249253",
  "patch_info": {
    "commit_hash": "93e95a3df9021a38010386d07df121392d249253",
    "repo": "ansible/ansible-runner",
    "commit_url": "https://github.com/ansible/ansible-runner/commit/93e95a3df9021a38010386d07df121392d249253",
    "files": [
      "ansible_runner/interface.py",
      "ansible_runner/runner.py",
      "ansible_runner/streaming.py"
    ],
    "message": "Successfully runs\n\nStreamController and StreamWorker are now fleshed out.",
    "before_after_code_files": [
      "ansible_runner/interface.py||ansible_runner/interface.py",
      "ansible_runner/runner.py||ansible_runner/runner.py",
      "ansible_runner/streaming.py||ansible_runner/streaming.py"
    ]
  },
  "patch_diff": {
    "ansible_runner/interface.py||ansible_runner/interface.py": [
      "File: ansible_runner/interface.py -> ansible_runner/interface.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "24: from ansible_runner import output",
      "25: from ansible_runner.runner_config import RunnerConfig",
      "26: from ansible_runner.runner import Runner",
      "28: from ansible_runner.utils import (",
      "29:     dump_artifacts,",
      "30:     check_isolation_executable_installed,",
      "",
      "[Removed Lines]",
      "27: from ansible_runner.streaming import StreamWorker",
      "",
      "[Added Lines]",
      "27: from ansible_runner.streaming import StreamController, StreamWorker",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "65:     event_callback_handler = kwargs.pop('event_handler', None)",
      "66:     status_callback_handler = kwargs.pop('status_handler', None)",
      "67:     cancel_callback = kwargs.pop('cancel_callback', None)",
      "69:     finished_callback = kwargs.pop('finished_callback', None)",
      "71:     control_out = kwargs.pop('control_out', None)",
      "78:     rc = RunnerConfig(**kwargs)",
      "79:     rc.prepare()",
      "",
      "[Removed Lines]",
      "68:     artifacts_callback = kwargs.pop('artifacts_callback', None)  # Currently not expected",
      "72:     if control_out is not None:",
      "73:         stream_worker = StreamWorker(control_out)",
      "74:         status_callback_handler = stream_worker.status_handler",
      "75:         event_callback_handler = stream_worker.event_handler",
      "76:         artifacts_callback = stream_worker.artifacts_callback",
      "",
      "[Added Lines]",
      "67:     artifacts_handler = kwargs.pop('artifacts_handler', None)",
      "71:     control_in = kwargs.pop('control_in', None)",
      "73:     worker_in = kwargs.pop('worker_in', None)",
      "74:     worker_out = kwargs.pop('worker_out', None)",
      "76:     if worker_in is not None and worker_out is not None:",
      "77:         stream_worker = StreamWorker(worker_in, worker_out, **kwargs)",
      "78:         return stream_worker",
      "80:     if control_in is not None and control_out is not None:",
      "81:         stream_controller = StreamController(control_in, control_out,",
      "82:                                              event_handler=event_callback_handler,",
      "83:                                              status_handler=status_callback_handler,",
      "84:                                              artifacts_handler=artifacts_handler,",
      "85:                                              cancel_callback=cancel_callback,",
      "86:                                              finished_callback=finished_callback,",
      "88:         return stream_controller",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "81:     return Runner(rc,",
      "82:                   event_handler=event_callback_handler,",
      "83:                   status_handler=status_callback_handler,",
      "84:                   cancel_callback=cancel_callback,",
      "86:                   finished_callback=finished_callback)",
      "",
      "[Removed Lines]",
      "85:                   artifacts_callback=artifacts_callback,",
      "",
      "[Added Lines]",
      "96:                   artifacts_handler=artifacts_handler,",
      "",
      "---------------",
      "--- Hunk 4 ---",
      "[Context before]",
      "124:     :param artifact_dir: The path to the directory where artifacts should live, this defaults to 'artifacts' under the private data dir",
      "125:     :param project_dir: The path to the playbook content, this defaults to 'project' within the private data dir",
      "126:     :param rotate_artifacts: Keep at most n artifact directories, disable with a value of 0 which is the default",
      "128:     :param event_handler: An optional callback that will be invoked any time an event is received by Runner itself, return True to keep the event",
      "129:     :param cancel_callback: An optional callback that can inform runner to cancel (returning True) or not (returning False)",
      "130:     :param finished_callback: An optional callback that will be invoked at shutdown after process cleanup.",
      "131:     :param status_handler: An optional callback that will be invoked any time the status changes (e.g...started, running, failed, successful, timeout)",
      "132:     :param process_isolation: Enable process isolation, using either a container engine (e.g. podman) or a sandbox (e.g. bwrap).",
      "133:     :param process_isolation_executable: Process isolation executable or container engine used to isolate execution. (default: podman)",
      "134:     :param process_isolation_path: Path that an isolated playbook run will use for staging. (default: /tmp)",
      "",
      "[Removed Lines]",
      "127:     :param control_out: A file-like object used for streaming information back to a control instance of Runner",
      "",
      "[Added Lines]",
      "139:     :param control_in: A file object used for receiving streamed data back from a worker instance of Runner",
      "140:     :param control_out: A file object used for streaming project data to a worker instance of Runner",
      "141:     :param worker_in: A file object used for streaming project data to a worker instance of Runner",
      "142:     :param worker_out: A file object used for streaming information back to a control instance of Runner",
      "147:     :param artifacts_handler: An optional callback that will be invoked at the end of the run to deal with the artifacts from the run.",
      "",
      "---------------",
      "--- Hunk 5 ---",
      "[Context before]",
      "170:     :type forks: int",
      "171:     :type quiet: bool",
      "172:     :type verbosity: int",
      "173:     :type control_out: file",
      "174:     :type event_handler: function",
      "175:     :type cancel_callback: function",
      "176:     :type finished_callback: function",
      "177:     :type status_handler: function",
      "178:     :type process_isolation: bool",
      "179:     :type process_isolation_executable: str",
      "180:     :type process_isolation_path: str",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "189:     :type control_in: file",
      "191:     :type worker_in: file",
      "192:     :type worker_out: file",
      "197:     :type artifacts_handler: function",
      "",
      "---------------"
    ],
    "ansible_runner/runner.py||ansible_runner/runner.py": [
      "File: ansible_runner/runner.py -> ansible_runner/runner.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "27: class Runner(object):",
      "29:     def __init__(self, config, cancel_callback=None, remove_partials=True, event_handler=None,",
      "31:         self.config = config",
      "32:         self.cancel_callback = cancel_callback",
      "33:         self.event_handler = event_handler",
      "35:         self.finished_callback = finished_callback",
      "36:         self.status_handler = status_handler",
      "37:         self.canceled = False",
      "",
      "[Removed Lines]",
      "30:                  artifacts_callback=None, finished_callback=None, status_handler=None):",
      "34:         self.artifacts_callback = artifacts_callback",
      "",
      "[Added Lines]",
      "30:                  artifacts_handler=None, finished_callback=None, status_handler=None):",
      "34:         self.artifacts_handler = artifacts_handler",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "284:                 logger.error('Failed to delete cgroup: {}'.format(stderr))",
      "285:                 raise RuntimeError('Failed to delete cgroup: {}'.format(stderr))",
      "288:             try:",
      "290:             except Exception as e:",
      "291:                 raise CallbackError(\"Exception in Artifact Callback: {}\".format(e))",
      "",
      "[Removed Lines]",
      "287:         if self.artifacts_callback is not None:",
      "289:                 self.artifacts_callback(self.config.artifact_dir)",
      "",
      "[Added Lines]",
      "287:         if self.artifacts_handler is not None:",
      "289:                 self.artifacts_handler(self.config.artifact_dir)",
      "",
      "---------------"
    ],
    "ansible_runner/streaming.py||ansible_runner/streaming.py": [
      "File: ansible_runner/streaming.py -> ansible_runner/streaming.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "1: import base64",
      "2: import io",
      "3: import json",
      "4: import os",
      "5: import zipfile",
      "10:         self.control_out = control_out",
      "14:         self.control_out.write(b'\\n')",
      "15:         self.control_out.flush()",
      "17:     def event_handler(self, event_data):",
      "23:         buf = io.BytesIO()",
      "24:         with zipfile.ZipFile(buf, 'w', compression=zipfile.ZIP_DEFLATED, allowZip64=True) as archive:",
      "25:             for dirpath, dirs, files in os.walk(artifact_dir):",
      "",
      "[Removed Lines]",
      "8: class StreamWorker(object):",
      "9:     def __init__(self, control_out):",
      "12:     def status_handler(self, status, runner_config):",
      "13:         self.control_out.write(json.dumps(status).encode('utf-8'))",
      "18:         self.control_out.write(json.dumps(event_data).encode('utf-8'))",
      "19:         self.control_out.write(b'\\n')",
      "20:         self.control_out.flush()",
      "22:     def artifacts_callback(self, artifact_dir):",
      "",
      "[Added Lines]",
      "2: import codecs",
      "6: import stat",
      "7: import tempfile",
      "8: import uuid",
      "11: import ansible_runner",
      "12: import ansible_runner.plugins",
      "15: class UUIDEncoder(json.JSONEncoder):",
      "16:     def default(self, obj):",
      "17:         if isinstance(obj, uuid.UUID):",
      "18:             return obj.hex",
      "19:         return json.JSONEncoder.default(self, obj)",
      "22: # List of kwargs options to the run method that should be sent to the remote executor.",
      "23: remote_run_options = (",
      "24:     'forks',",
      "25:     'host_pattern',",
      "26:     'ident',",
      "27:     'ignore_logging',",
      "28:     'inventory',",
      "29:     'limit',",
      "30:     'module',",
      "31:     'module_args',",
      "32:     'omit_event_data',",
      "33:     'only_failed_event_data',",
      "34:     'playbook',",
      "35:     'verbosity',",
      "36: )",
      "39: class StreamController(object):",
      "40:     def __init__(self, control_in, control_out, status_handler=None, event_handler=None,",
      "41:                  artifacts_handler=None, cancel_callback=None, finished_callback=None, **kwargs):",
      "42:         self.control_in = control_in",
      "45:         self.kwargs = kwargs",
      "46:         self.config = ansible_runner.RunnerConfig(**kwargs)",
      "47:         self.status_handler = status_handler",
      "48:         self.event_handler = event_handler",
      "49:         self.artifacts_handler = artifacts_handler",
      "51:         self.cancel_callback = cancel_callback",
      "52:         self.finished_callback = finished_callback",
      "54:         self.status = \"unstarted\"",
      "55:         self.rc = None",
      "57:     def run(self):",
      "58:         self.send_job()",
      "60:         job_events_path = os.path.join(self.config.artifact_dir, 'job_events')",
      "61:         if not os.path.exists(job_events_path):",
      "62:             os.mkdir(job_events_path, 0o700)",
      "64:         for line in self.control_in:",
      "65:             data = json.loads(line)",
      "66:             if 'status' in data:",
      "67:                 self.status_callback(data)",
      "68:             elif 'artifacts' in data:",
      "69:                 self.artifacts_callback(data)",
      "70:             elif 'eof' in data:",
      "71:                 break",
      "72:             else:",
      "73:                 self.event_callback(data)",
      "75:         if self.finished_callback is not None:",
      "76:             self.finished_callback(self)",
      "77:         return self.status, self.rc",
      "79:     def send_job(self):",
      "80:         self.config.prepare()",
      "81:         remote_options = {key: value for key, value in self.kwargs.items() if key in remote_run_options}",
      "83:         buf = io.BytesIO()",
      "84:         with zipfile.ZipFile(buf, 'w', compression=zipfile.ZIP_DEFLATED, allowZip64=True) as archive:",
      "85:             private_data_dir = self.kwargs.get('private_data_dir', None)",
      "86:             if private_data_dir:",
      "87:                 for dirpath, dirs, files in os.walk(private_data_dir):",
      "88:                     relpath = os.path.relpath(dirpath, private_data_dir)",
      "89:                     if relpath == \".\":",
      "90:                         relpath = \"\"",
      "91:                     for fname in files:",
      "92:                         archive.write(os.path.join(dirpath, fname), arcname=os.path.join(relpath, fname))",
      "94:             kwargs = json.dumps(remote_options, cls=UUIDEncoder)",
      "95:             archive.writestr('kwargs', kwargs)",
      "96:             archive.close()",
      "97:         buf.flush()",
      "99:         data = {",
      "100:             'private_data_dir': True,",
      "101:             'payload': base64.b64encode(buf.getvalue()).decode('ascii'),",
      "102:         }",
      "103:         self.control_out.write(json.dumps(data).encode('utf-8'))",
      "106:         self.control_out.close()",
      "108:     def status_callback(self, status_data):",
      "109:         self.status = status_data['status']",
      "111:         for plugin in ansible_runner.plugins:",
      "112:             ansible_runner.plugins[plugin].status_handler(self.config, status_data)",
      "113:         if self.status_handler is not None:",
      "114:             self.status_handler(status_data, runner_config=self.config)",
      "116:     def event_callback(self, event_data):",
      "117:         full_filename = os.path.join(self.config.artifact_dir,",
      "118:                                      'job_events',",
      "119:                                      '{}-{}.json'.format(event_data['counter'],",
      "120:                                                          event_data['uuid']))",
      "122:         if self.event_handler is not None:",
      "123:             should_write = self.event_handler(event_data)",
      "124:         else:",
      "125:             should_write = True",
      "126:         for plugin in ansible_runner.plugins:",
      "127:             ansible_runner.plugins[plugin].event_handler(self.config, event_data)",
      "128:         if should_write:",
      "129:             with codecs.open(full_filename, 'w', encoding='utf-8') as write_file:",
      "130:                 os.chmod(full_filename, stat.S_IRUSR | stat.S_IWUSR)",
      "131:                 json.dump(event_data, write_file)",
      "133:     def artifacts_callback(self, artifacts_data):  # FIXME",
      "134:         if self.artifacts_handler is not None:",
      "135:             self.artifacts_handler()",
      "138: class StreamWorker(object):",
      "139:     def __init__(self, worker_in, worker_out, **kwargs):",
      "140:         self.worker_in = worker_in",
      "141:         self.worker_out = worker_out",
      "143:         self.kwargs = kwargs",
      "145:         self.private_data_dir = tempfile.TemporaryDirectory().name",
      "147:     def run(self):",
      "148:         for line in self.worker_in:",
      "149:             data = json.loads(line)",
      "150:             if data.get('private_data_dir'):",
      "151:                 buf = io.BytesIO(base64.b64decode(data['payload']))",
      "152:                 with zipfile.ZipFile(buf, 'r') as archive:",
      "153:                     archive.extractall(path=self.private_data_dir)",
      "155:         kwargs_path = os.path.join(self.private_data_dir, 'kwargs')",
      "156:         if os.path.exists(kwargs_path):",
      "157:             with open(kwargs_path, \"r\") as kwf:",
      "158:                 kwargs = json.load(kwf)",
      "159:             if not isinstance(kwargs, dict):",
      "160:                 raise ValueError(\"Invalid kwargs data\")",
      "161:         else:",
      "162:             kwargs = {}",
      "164:         self.kwargs.update(kwargs)",
      "166:         self.kwargs['quiet'] = True",
      "167:         self.kwargs['private_data_dir'] = self.private_data_dir",
      "168:         self.kwargs['status_handler'] = self.status_handler",
      "169:         self.kwargs['event_handler'] = self.event_handler",
      "170:         self.kwargs['artifacts_handler'] = self.artifacts_handler",
      "171:         self.kwargs['finished_callback'] = self.finished_callback",
      "173:         ansible_runner.interface.run(**self.kwargs)",
      "175:         # FIXME: do cleanup on the tempdir",
      "177:     def status_handler(self, status, runner_config):",
      "178:         self.worker_out.write(json.dumps(status).encode('utf-8'))",
      "179:         self.worker_out.write(b'\\n')",
      "180:         self.worker_out.flush()",
      "183:         self.worker_out.write(json.dumps(event_data).encode('utf-8'))",
      "184:         self.worker_out.write(b'\\n')",
      "185:         self.worker_out.flush()",
      "187:     def artifacts_handler(self, artifact_dir):",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "34:             'artifacts': True,",
      "35:             'payload': base64.b64encode(buf.getvalue()).decode('ascii'),",
      "36:         }",
      "",
      "[Removed Lines]",
      "37:         self.control_out.write(json.dumps(data).encode('utf-8'))",
      "38:         self.control_out.write(b'\\n')",
      "39:         self.control_out.flush()",
      "40:         self.control_out.close()",
      "",
      "[Added Lines]",
      "202:         self.worker_out.write(json.dumps(data).encode('utf-8'))",
      "203:         self.worker_out.write(b'\\n')",
      "204:         self.worker_out.flush()",
      "206:     def finished_callback(self, runner_obj):",
      "207:         self.worker_out.write(json.dumps({'eof': True}).encode('utf-8'))",
      "208:         self.worker_out.write(b'\\n')",
      "209:         self.worker_out.flush()",
      "210:         self.worker_out.close()",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "581db2fc6d6339af3a28ab76818045db824bb097",
      "candidate_info": {
        "commit_hash": "581db2fc6d6339af3a28ab76818045db824bb097",
        "repo": "ansible/ansible-runner",
        "commit_url": "https://github.com/ansible/ansible-runner/commit/581db2fc6d6339af3a28ab76818045db824bb097",
        "files": [
          "ansible_runner/streaming.py",
          "ansible_runner/utils/__init__.py",
          "ansible_runner/utils/base64io.py",
          "ansible_runner/utils/streaming.py"
        ],
        "message": "Avoid loading zip data into memory\n\nLink: https://github.com/ansible/ansible-runner/issues/604\nReplaces: https://github.com/ansible/ansible-runner/pull/605",
        "before_after_code_files": [
          "ansible_runner/streaming.py||ansible_runner/streaming.py",
          "ansible_runner/utils.py||ansible_runner/utils/__init__.py",
          "ansible_runner/utils/base64io.py||ansible_runner/utils/base64io.py",
          "ansible_runner/utils/streaming.py||ansible_runner/utils/streaming.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "ansible_runner/streaming.py||ansible_runner/streaming.py"
          ],
          "candidate": [
            "ansible_runner/streaming.py||ansible_runner/streaming.py"
          ]
        }
      },
      "candidate_diff": {
        "ansible_runner/streaming.py||ansible_runner/streaming.py": [
          "File: ansible_runner/streaming.py -> ansible_runner/streaming.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "15: from ansible_runner.exceptions import ConfigurationError",
          "16: from ansible_runner.loader import ArtifactLoader",
          "17: import ansible_runner.plugins",
          "21: class UUIDEncoder(json.JSONEncoder):",
          "",
          "[Removed Lines]",
          "18: from ansible_runner import utils",
          "",
          "[Added Lines]",
          "18: from ansible_runner.utils.streaming import stream_dir, unstream_dir",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "50:         self._output.flush()",
          "52:         if not self.only_transmit_kwargs:",
          "55:         self._output.write(json.dumps({'eof': True}).encode('utf-8'))",
          "56:         self._output.write(b'\\n')",
          "",
          "[Removed Lines]",
          "53:             self._output.write(utils.stream_dir(self.private_data_dir))",
          "",
          "[Added Lines]",
          "53:             stream_dir(self.private_data_dir, self._output)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "103:             if 'kwargs' in data:",
          "104:                 self.job_kwargs = self.update_paths(data['kwargs'])",
          "105:             elif 'zipfile' in data:",
          "107:                 try:",
          "109:                 except Exception:",
          "110:                     self.status_handler({",
          "111:                         'status': 'error',",
          "",
          "[Removed Lines]",
          "106:                 zip_data = self._input.read(data['zipfile'])",
          "108:                     utils.unstream_dir(zip_data, self.private_data_dir)",
          "",
          "[Added Lines]",
          "107:                     unstream_dir(self._input, data['zipfile'], self.private_data_dir)",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "145:         self._output.flush()",
          "147:     def artifacts_handler(self, artifact_dir):",
          "149:         self._output.flush()",
          "151:     def finished_callback(self, runner_obj):",
          "",
          "[Removed Lines]",
          "148:         self._output.write(utils.stream_dir(artifact_dir))",
          "",
          "[Added Lines]",
          "147:         stream_dir(artifact_dir, self._output)",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "223:                 json.dump(event_data, write_file)",
          "225:     def artifacts_callback(self, artifacts_data):",
          "230:         if self.artifacts_handler is not None:",
          "231:             self.artifacts_handler(self.artifact_dir)",
          "",
          "[Removed Lines]",
          "226:         zip_data = self._input.read(artifacts_data['zipfile'])",
          "228:         utils.unstream_dir(zip_data, self.artifact_dir)",
          "",
          "[Added Lines]",
          "225:         length = artifacts_data['zipfile']",
          "226:         utils.unstream_dir(self._input, length, self.artifact_dir)",
          "",
          "---------------"
        ],
        "ansible_runner/utils.py||ansible_runner/utils/__init__.py": [
          "File: ansible_runner/utils.py -> ansible_runner/utils/__init__.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "15: import uuid",
          "16: import codecs",
          "17: import zipfile",
          "19: try:",
          "20:     from collections.abc import Iterable, Mapping",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "18: import math",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "82:             raise RuntimeError(f'{isolation_executable} unavailable for unexpected reason.')",
          "83:         return False",
          "116: def dump_artifact(obj, path, filename=None):",
          "117:     '''",
          "118:     Write the artifact to disk at the specified path",
          "",
          "[Removed Lines]",
          "86: def stream_dir(directory):",
          "87:     buf = BytesIO()",
          "88:     with zipfile.ZipFile(buf, 'w', compression=zipfile.ZIP_DEFLATED, allowZip64=True) as archive:",
          "89:         if directory:",
          "90:             for dirpath, dirs, files in os.walk(directory):",
          "91:                 relpath = os.path.relpath(dirpath, directory)",
          "92:                 if relpath == \".\":",
          "93:                     relpath = \"\"",
          "94:                 for fname in files:",
          "95:                     archive.write(os.path.join(dirpath, fname), arcname=os.path.join(relpath, fname))",
          "96:         archive.close()",
          "98:     payload = base64.b85encode(buf.getvalue())",
          "99:     return b'\\n'.join((json.dumps({'zipfile': len(payload)}).encode('utf-8'), payload))",
          "102: def unstream_dir(data, directory):",
          "103:     # NOTE: caller needs to process exceptions",
          "104:     data = base64.b85decode(data)",
          "105:     buf = BytesIO(data)",
          "106:     with zipfile.ZipFile(buf, 'r') as archive:",
          "107:         # Fancy extraction in order to preserve permissions",
          "108:         # https://www.burgundywall.com/post/preserving-file-perms-with-python-zipfile-module",
          "109:         for info in archive.infolist():",
          "110:             archive.extract(info.filename, path=directory)",
          "111:             out_path = os.path.join(directory, info.filename)",
          "112:             perm = info.external_attr >> 16",
          "113:             os.chmod(out_path, perm)",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "ansible_runner/utils/base64io.py||ansible_runner/utils/base64io.py": [
          "File: ansible_runner/utils/base64io.py -> ansible_runner/utils/base64io.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: # Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.",
          "2: #",
          "3: # Licensed under the Apache License, Version 2.0 (the \"License\"). You",
          "4: # may not use this file except in compliance with the License. A copy of",
          "5: # the License is located at",
          "6: #",
          "7: # http://aws.amazon.com/apache2.0/",
          "8: #",
          "9: # or in the \"license\" file accompanying this file. This file is",
          "10: # distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF",
          "11: # ANY KIND, either express or implied. See the License for the specific",
          "12: # language governing permissions and limitations under the License.",
          "13: \"\"\"Base64 stream with context manager support.\"\"\"",
          "14: from __future__ import division",
          "16: import base64",
          "17: import io",
          "18: import logging",
          "19: import string",
          "20: import sys",
          "22: LOGGER_NAME = \"base64io\"",
          "24: try:  # Python 3.5.0 and 3.5.1 have incompatible typing modules",
          "25:     from types import TracebackType  # noqa pylint: disable=unused-import",
          "26:     from typing import (",
          "27:         IO,",
          "28:         AnyStr,",
          "29:         Iterable,",
          "30:         List,",
          "31:         Literal,",
          "32:         Optional,",
          "33:         Type,",
          "34:         Union,",
          "35:     )  # noqa pylint: disable=unused-import",
          "36: except ImportError:  # pragma: no cover",
          "37:     # We only actually need these imports when running the mypy checks",
          "38:     pass",
          "40: __all__ = (\"Base64IO\",)",
          "41: __version__ = \"1.0.3\"",
          "42: _LOGGER = logging.getLogger(LOGGER_NAME)",
          "45: def _py2():",
          "46:     # type: () -> bool",
          "47:     \"\"\"Determine if runtime is Python 2.",
          "49:     :returns: decision:",
          "50:     :rtype: bool",
          "51:     \"\"\"",
          "52:     return sys.version_info[0] == 2",
          "55: if not _py2():",
          "56:     # The \"file\" object does not exist in Python 3, but we need to reference",
          "57:     # it in Python 2 code paths. Defining this here accomplishes two things:",
          "58:     # First, it allows linters to accept \"file\" as a defined object in Python 3.",
          "59:     # Second, it will serve as a canary to ensure that there are no references",
          "60:     # to \"file\" in Python 3 code paths.",
          "61:     file = NotImplemented  # pylint: disable=invalid-name",
          "64: def _to_bytes(data):",
          "65:     # type: (AnyStr) -> bytes",
          "66:     \"\"\"Convert input data from either string or bytes to bytes.",
          "68:     :param data: Data to convert",
          "69:     :returns: ``data`` converted to bytes",
          "70:     :rtype: bytes",
          "71:     \"\"\"",
          "72:     if isinstance(data, bytes):",
          "73:         return data",
          "74:     return data.encode(\"utf-8\")",
          "77: class Base64IO(io.IOBase):",
          "78:     \"\"\"Base64 stream with context manager support.",
          "80:     Wraps a stream, base64-decoding read results before returning them and base64-encoding",
          "81:     written bytes before writing them to the stream. Instances",
          "82:     of this class are not reusable in order maintain consistency with the :class:`io.IOBase`",
          "83:     behavior on ``close()``.",
          "85:     .. note::",
          "87:         Provides iterator and context manager interfaces.",
          "89:     .. warning::",
          "91:         Because up to two bytes of data must be buffered to ensure correct base64 encoding",
          "92:         of all data written, this object **must** be closed after you are done writing to",
          "93:         avoid data loss. If used as a context manager, we take care of that for you.",
          "95:     :param wrapped: Stream to wrap",
          "96:     \"\"\"",
          "98:     closed = False",
          "100:     def __init__(self, wrapped):",
          "101:         # type: (Base64IO, IO) -> None",
          "102:         \"\"\"Check for required methods on wrapped stream and set up read buffer.",
          "104:         :raises TypeError: if ``wrapped`` does not have attributes needed to determine the stream's state",
          "105:         \"\"\"",
          "106:         required_attrs = (\"read\", \"write\", \"close\", \"closed\", \"flush\")",
          "107:         if not all(hasattr(wrapped, attr) for attr in required_attrs):",
          "108:             raise TypeError(",
          "109:                 \"Base64IO wrapped object must have attributes: %s\" % (repr(sorted(required_attrs)),)",
          "110:             )",
          "111:         super(Base64IO, self).__init__()",
          "112:         self.__wrapped = wrapped",
          "113:         self.__read_buffer = b\"\"",
          "114:         self.__write_buffer = b\"\"",
          "116:     def __enter__(self):",
          "117:         # type: () -> Base64IO",
          "118:         \"\"\"Return self on enter.\"\"\"",
          "119:         return self",
          "121:     def __exit__(self, exc_type, exc_value, traceback):",
          "122:         # type: (Optional[Type[BaseException]], Optional[BaseException], Optional[TracebackType]) -> Literal[False]",
          "123:         \"\"\"Properly close self on exit.\"\"\"",
          "124:         self.close()",
          "125:         return False",
          "127:     def close(self):",
          "128:         # type: () -> None",
          "129:         \"\"\"Close this stream, encoding and writing any buffered bytes is present.",
          "131:         .. note::",
          "133:             This does **not** close the wrapped stream.",
          "134:         \"\"\"",
          "135:         if self.__write_buffer:",
          "136:             self.__wrapped.write(base64.b64encode(self.__write_buffer))",
          "137:             self.__write_buffer = b\"\"",
          "138:         self.closed = True",
          "140:     def _passthrough_interactive_check(self, method_name, mode):",
          "141:         # type: (str, str) -> bool",
          "142:         \"\"\"Attempt to call the specified method on the wrapped stream and return the result.",
          "144:         If the method is not found on the wrapped stream, return False.",
          "146:         .. note::",
          "148:             Special Case: If wrapped stream is a Python 2 file, inspect the file mode.",
          "150:         :param str method_name: Name of method to call",
          "151:         :param str mode: Python 2 mode character",
          "152:         :rtype: bool",
          "153:         \"\"\"",
          "154:         try:",
          "155:             method = getattr(self.__wrapped, method_name)",
          "156:         except AttributeError:",
          "157:             if (",
          "158:                 _py2()",
          "159:                 and isinstance(",
          "160:                     self.__wrapped, file",
          "161:                 )  # pylint: disable=isinstance-second-argument-not-valid-type",
          "162:                 and mode in self.__wrapped.mode",
          "163:             ):",
          "164:                 return True",
          "165:             return False",
          "166:         else:",
          "167:             return method()",
          "169:     def writable(self):",
          "170:         # type: () -> bool",
          "171:         \"\"\"Determine if the stream can be written to.",
          "173:         Delegates to wrapped stream when possible.",
          "174:         Otherwise returns False.",
          "176:         :rtype: bool",
          "177:         \"\"\"",
          "178:         return self._passthrough_interactive_check(\"writable\", \"w\")",
          "180:     def readable(self):",
          "181:         # type: () -> bool",
          "182:         \"\"\"Determine if the stream can be read from.",
          "184:         Delegates to wrapped stream when possible.",
          "185:         Otherwise returns False.",
          "187:         :rtype: bool",
          "188:         \"\"\"",
          "189:         return self._passthrough_interactive_check(\"readable\", \"r\")",
          "191:     def flush(self):",
          "192:         # type: () -> None",
          "193:         \"\"\"Flush the write buffer of the wrapped stream.\"\"\"",
          "194:         return self.__wrapped.flush()",
          "196:     def write(self, b):",
          "197:         # type: (bytes) -> int",
          "198:         \"\"\"Base64-encode the bytes and write them to the wrapped stream.",
          "200:         Any bytes that would require padding for the next write call are buffered until the",
          "201:         next write or close.",
          "203:         .. warning::",
          "205:             Because up to two bytes of data must be buffered to ensure correct base64 encoding",
          "206:             of all data written, this object **must** be closed after you are done writing to",
          "207:             avoid data loss. If used as a context manager, we take care of that for you.",
          "209:         :param bytes b: Bytes to write to wrapped stream",
          "210:         :raises ValueError: if called on closed Base64IO object",
          "211:         :raises IOError: if underlying stream is not writable",
          "212:         \"\"\"",
          "213:         if self.closed:",
          "214:             raise ValueError(\"I/O operation on closed file.\")",
          "216:         if not self.writable():",
          "217:             raise IOError(\"Stream is not writable\")",
          "219:         # Load any stashed bytes and clear the buffer",
          "220:         _bytes_to_write = self.__write_buffer + b",
          "221:         self.__write_buffer = b\"\"",
          "223:         # If an even base64 chunk or finalizing the stream, write through.",
          "224:         if len(_bytes_to_write) % 3 == 0:",
          "225:             return self.__wrapped.write(base64.b64encode(_bytes_to_write))",
          "227:         # We're not finalizing the stream, so stash the trailing bytes and encode the rest.",
          "228:         trailing_byte_pos = -1 * (len(_bytes_to_write) % 3)",
          "229:         self.__write_buffer = _bytes_to_write[trailing_byte_pos:]",
          "230:         return self.__wrapped.write(base64.b64encode(_bytes_to_write[:trailing_byte_pos]))",
          "232:     def writelines(self, lines):",
          "233:         # type: (Iterable[bytes]) -> None",
          "234:         \"\"\"Write a list of lines.",
          "236:         :param list lines: Lines to write",
          "237:         \"\"\"",
          "238:         for line in lines:",
          "239:             self.write(line)",
          "241:     def _read_additional_data_removing_whitespace(self, data, total_bytes_to_read):",
          "242:         # type: (bytes, int) -> bytes",
          "243:         \"\"\"Read additional data from wrapped stream until we reach the desired number of bytes.",
          "245:         .. note::",
          "247:             All whitespace is ignored.",
          "249:         :param bytes data: Data that has already been read from wrapped stream",
          "250:         :param int total_bytes_to_read: Number of total non-whitespace bytes to read from wrapped stream",
          "251:         :returns: ``total_bytes_to_read`` bytes from wrapped stream with no whitespace",
          "252:         :rtype: bytes",
          "253:         \"\"\"",
          "254:         if total_bytes_to_read is None:",
          "255:             # If the requested number of bytes is None, we read the entire message, in which",
          "256:             # case the base64 module happily removes any whitespace.",
          "257:             return data",
          "259:         _data_buffer = io.BytesIO()",
          "261:         _data_buffer.write(b\"\".join(data.split()))",
          "262:         _remaining_bytes_to_read = total_bytes_to_read - _data_buffer.tell()",
          "264:         while _remaining_bytes_to_read > 0:",
          "265:             _raw_additional_data = _to_bytes(self.__wrapped.read(_remaining_bytes_to_read))",
          "266:             if not _raw_additional_data:",
          "267:                 # No more data to read from wrapped stream.",
          "268:                 break",
          "270:             _data_buffer.write(b\"\".join(_raw_additional_data.split()))",
          "271:             _remaining_bytes_to_read = total_bytes_to_read - _data_buffer.tell()",
          "272:         return _data_buffer.getvalue()",
          "274:     def read(self, b=-1):",
          "275:         # type: (int) -> bytes",
          "276:         \"\"\"Read bytes from wrapped stream, base64-decoding before return.",
          "278:         .. note::",
          "280:             The number of bytes requested from the wrapped stream is adjusted to return the",
          "281:             requested number of bytes after decoding returned bytes.",
          "283:         :param int b: Number of bytes to read",
          "284:         :returns: Decoded bytes from wrapped stream",
          "285:         :rtype: bytes",
          "286:         \"\"\"",
          "287:         if self.closed:",
          "288:             raise ValueError(\"I/O operation on closed file.\")",
          "290:         if not self.readable():",
          "291:             raise IOError(\"Stream is not readable\")",
          "293:         if b is None or b < 0:",
          "294:             b = -1",
          "295:             _bytes_to_read = -1",
          "296:         elif b == 0:",
          "297:             _bytes_to_read = 0",
          "298:         elif b > 0:",
          "299:             # Calculate number of encoded bytes that must be read to get b raw bytes.",
          "300:             _bytes_to_read = int((b - len(self.__read_buffer)) * 4 / 3)",
          "301:             _bytes_to_read += 4 - _bytes_to_read % 4",
          "303:         # Read encoded bytes from wrapped stream.",
          "304:         data = _to_bytes(self.__wrapped.read(_bytes_to_read))",
          "305:         # Remove whitespace from read data and attempt to read more data to get the desired",
          "306:         # number of bytes.",
          "308:         if any([char in data for char in string.whitespace.encode(\"utf-8\")]):",
          "309:             data = self._read_additional_data_removing_whitespace(data, _bytes_to_read)",
          "311:         results = io.BytesIO()",
          "312:         # First, load any stashed bytes",
          "313:         results.write(self.__read_buffer)",
          "314:         # Decode encoded bytes.",
          "315:         results.write(base64.b64decode(data))",
          "317:         results.seek(0)",
          "318:         output_data = results.read(b)",
          "319:         # Stash any extra bytes for the next run.",
          "320:         self.__read_buffer = results.read()",
          "322:         return output_data",
          "324:     def __iter__(self):",
          "325:         # Until https://github.com/python/typing/issues/11",
          "326:         # there's no good way to tell mypy about custom",
          "327:         # iterators that subclass io.IOBase.",
          "328:         \"\"\"Let this class act as an iterator.\"\"\"",
          "329:         return self",
          "331:     def readline(self, limit=-1):",
          "332:         # type: (int) -> bytes",
          "333:         \"\"\"Read and return one line from the stream.",
          "335:         If limit is specified, at most limit bytes will be read.",
          "337:         .. note::",
          "339:             Because the source that this reads from may not contain any OEL characters, we",
          "340:             read \"lines\" in chunks of length ``io.DEFAULT_BUFFER_SIZE``.",
          "342:         :type limit: int",
          "343:         :rtype: bytes",
          "344:         \"\"\"",
          "345:         return self.read(limit if limit > 0 else io.DEFAULT_BUFFER_SIZE)",
          "347:     def readlines(self, hint=-1):",
          "348:         # type: (int) -> List[bytes]",
          "349:         \"\"\"Read and return a list of lines from the stream.",
          "351:         ``hint`` can be specified to control the number of lines read: no more lines will",
          "352:         be read if the total size (in bytes/characters) of all lines so far exceeds hint.",
          "354:         :type hint: int",
          "355:         :returns: Lines of data",
          "356:         :rtype: list of bytes",
          "357:         \"\"\"",
          "358:         lines = []",
          "359:         total_len = 0",
          "360:         hint_defined = hint > 0",
          "362:         for line in self:  # type: ignore",
          "363:             lines.append(line)",
          "364:             total_len += len(line)",
          "366:             hint_satisfied = total_len > hint",
          "367:             if hint_defined and hint_satisfied:",
          "368:                 break",
          "369:         return lines",
          "371:     def __next__(self):",
          "372:         # type: () -> bytes",
          "373:         \"\"\"Python 3 iterator hook.\"\"\"",
          "374:         line = self.readline()",
          "375:         if line:",
          "376:             return line",
          "377:         raise StopIteration()",
          "379:     def next(self):",
          "380:         # type: () -> bytes",
          "381:         \"\"\"Python 2 iterator hook.\"\"\"",
          "382:         return self.__next__()",
          "",
          "---------------"
        ],
        "ansible_runner/utils/streaming.py||ansible_runner/utils/streaming.py": [
          "File: ansible_runner/utils/streaming.py -> ansible_runner/utils/streaming.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: import tempfile",
          "2: import zipfile",
          "3: import os",
          "4: import json",
          "5: import sys",
          "7: from .base64io import Base64IO",
          "8: from pathlib import Path",
          "10: def stream_dir(source_directory, stream):",
          "11:     with tempfile.NamedTemporaryFile() as tmp:",
          "12:         with zipfile.ZipFile(tmp.name, 'w', compression=zipfile.ZIP_DEFLATED, allowZip64=True) as archive:",
          "13:             if source_directory:",
          "14:                 for dirpath, dirs, files in os.walk(source_directory):",
          "15:                     relpath = os.path.relpath(dirpath, source_directory)",
          "16:                     if relpath == \".\":",
          "17:                         relpath = \"\"",
          "18:                     for fname in files:",
          "19:                         archive.write(os.path.join(dirpath, fname), arcname=os.path.join(relpath, fname))",
          "20:             archive.close()",
          "22:         zip_size = Path(tmp.name).stat().st_size",
          "24:         with open(tmp.name, \"rb\") as source:",
          "25:             if stream.name == '<stdout>':",
          "26:                 target = sys.stdout.buffer",
          "27:             else:",
          "28:                 target = stream",
          "29:             target.write(json.dumps({'zipfile': zip_size}).encode('utf-8') + b'\\n')",
          "30:             with Base64IO(target) as encoded_target:",
          "31:                 for line in source:",
          "32:                     encoded_target.write(line)",
          "35: def unstream_dir(stream, length, target_directory):",
          "36:     # NOTE: caller needs to process exceptions",
          "37:     with tempfile.NamedTemporaryFile() as tmp:",
          "38:         with open(tmp.name, \"wb\") as target:",
          "39:             with Base64IO(stream) as source:",
          "40:                 while True:",
          "41:                     remaining = length",
          "42:                     chunk_size = 1024 * 1000  # 1 MB",
          "43:                     if chunk_size >= remaining:",
          "44:                         chunk_size = remaining",
          "45:                     data = source.read(chunk_size)",
          "46:                     target.write(data)",
          "47:                     remaining -= chunk_size",
          "49:                     if remaining == 0:",
          "50:                         break",
          "52:         with zipfile.ZipFile(tmp.name, 'r') as archive:",
          "53:             # Fancy extraction in order to preserve permissions",
          "54:             # https://www.burgundywall.com/post/preserving-file-perms-with-python-zipfile-module",
          "55:             for info in archive.infolist():",
          "56:                 archive.extract(info.filename, path=target_directory)",
          "57:                 out_path = os.path.join(target_directory, info.filename)",
          "58:                 perm = info.external_attr >> 16",
          "59:                 os.chmod(out_path, perm)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "19188fc632cbd29e5116cb2850bf56cfb70d0252",
      "candidate_info": {
        "commit_hash": "19188fc632cbd29e5116cb2850bf56cfb70d0252",
        "repo": "ansible/ansible-runner",
        "commit_url": "https://github.com/ansible/ansible-runner/commit/19188fc632cbd29e5116cb2850bf56cfb70d0252",
        "files": [
          "ansible_runner/interface.py"
        ],
        "message": "Hook in the methods from StreamWorker when a control output file is passed",
        "before_after_code_files": [
          "ansible_runner/interface.py||ansible_runner/interface.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "ansible_runner/interface.py||ansible_runner/interface.py"
          ],
          "candidate": [
            "ansible_runner/interface.py||ansible_runner/interface.py"
          ]
        }
      },
      "candidate_diff": {
        "ansible_runner/interface.py||ansible_runner/interface.py": [
          "File: ansible_runner/interface.py -> ansible_runner/interface.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "24: from ansible_runner import output",
          "25: from ansible_runner.runner_config import RunnerConfig",
          "26: from ansible_runner.runner import Runner",
          "27: from ansible_runner.utils import (",
          "28:     dump_artifacts,",
          "29:     check_isolation_executable_installed,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "27: from ansible_runner.streaming import StreamWorker",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "64:     event_callback_handler = kwargs.pop('event_handler', None)",
          "65:     status_callback_handler = kwargs.pop('status_handler', None)",
          "66:     cancel_callback = kwargs.pop('cancel_callback', None)",
          "69:     rc = RunnerConfig(**kwargs)",
          "70:     rc.prepare()",
          "",
          "[Removed Lines]",
          "67:     finished_callback = kwargs.pop('finished_callback',  None)",
          "",
          "[Added Lines]",
          "68:     artifacts_callback = kwargs.pop('artifacts_callback', None)  # Currently not expected",
          "69:     finished_callback = kwargs.pop('finished_callback', None)",
          "71:     control_out = kwargs.pop('control_out', None)",
          "72:     if control_out is not None:",
          "73:         stream_worker = StreamWorker(control_out)",
          "74:         status_callback_handler = stream_worker.status_handler",
          "75:         event_callback_handler = stream_worker.event_handler",
          "76:         artifacts_callback = stream_worker.artifacts_callback",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "73:                   event_handler=event_callback_handler,",
          "74:                   status_handler=status_callback_handler,",
          "75:                   cancel_callback=cancel_callback,",
          "76:                   finished_callback=finished_callback)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "85:                   artifacts_callback=artifacts_callback,",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "114:     :param artifact_dir: The path to the directory where artifacts should live, this defaults to 'artifacts' under the private data dir",
          "115:     :param project_dir: The path to the playbook content, this defaults to 'project' within the private data dir",
          "116:     :param rotate_artifacts: Keep at most n artifact directories, disable with a value of 0 which is the default",
          "117:     :param event_handler: An optional callback that will be invoked any time an event is received by Runner itself, return True to keep the event",
          "118:     :param cancel_callback: An optional callback that can inform runner to cancel (returning True) or not (returning False)",
          "119:     :param finished_callback: An optional callback that will be invoked at shutdown after process cleanup.",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "127:     :param control_out: A file-like object used for streaming information back to a control instance of Runner",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "159:     :type forks: int",
          "160:     :type quiet: bool",
          "161:     :type verbosity: int",
          "162:     :type event_handler: function",
          "163:     :type cancel_callback: function",
          "164:     :type finished_callback: function",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "173:     :type control_out: file",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "cf8e8cb26fa9a4642ce269e9e35ebf8e1c547503",
      "candidate_info": {
        "commit_hash": "cf8e8cb26fa9a4642ce269e9e35ebf8e1c547503",
        "repo": "ansible/ansible-runner",
        "commit_url": "https://github.com/ansible/ansible-runner/commit/cf8e8cb26fa9a4642ce269e9e35ebf8e1c547503",
        "files": [
          "ansible_runner/__main__.py",
          "ansible_runner/streaming.py"
        ],
        "message": "Add the container command line flags to the transmit subcommand",
        "before_after_code_files": [
          "ansible_runner/__main__.py||ansible_runner/__main__.py",
          "ansible_runner/streaming.py||ansible_runner/streaming.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "ansible_runner/streaming.py||ansible_runner/streaming.py"
          ],
          "candidate": [
            "ansible_runner/streaming.py||ansible_runner/streaming.py"
          ]
        }
      },
      "candidate_diff": {
        "ansible_runner/__main__.py||ansible_runner/__main__.py": [
          "File: ansible_runner/__main__.py -> ansible_runner/__main__.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "785:     start_container_group = start_subparser.add_argument_group(*container_group_options)",
          "786:     stop_container_group = stop_subparser.add_argument_group(*container_group_options)",
          "787:     isalive_container_group = isalive_subparser.add_argument_group(*container_group_options)",
          "788:     adhoc_container_group = adhoc_subparser.add_argument_group(*container_group_options)",
          "789:     playbook_container_group = playbook_subparser.add_argument_group(*container_group_options)",
          "790:     add_args_to_parser(run_container_group, DEFAULT_CLI_ARGS['container_group'])",
          "791:     add_args_to_parser(start_container_group, DEFAULT_CLI_ARGS['container_group'])",
          "792:     add_args_to_parser(stop_container_group, DEFAULT_CLI_ARGS['container_group'])",
          "793:     add_args_to_parser(isalive_container_group, DEFAULT_CLI_ARGS['container_group'])",
          "794:     add_args_to_parser(adhoc_container_group, DEFAULT_CLI_ARGS['container_group'])",
          "795:     add_args_to_parser(playbook_container_group, DEFAULT_CLI_ARGS['container_group'])",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "788:     transmit_container_group = transmit_subparser.add_argument_group(*container_group_options)",
          "795:     add_args_to_parser(transmit_container_group, DEFAULT_CLI_ARGS['container_group'])",
          "",
          "---------------"
        ],
        "ansible_runner/streaming.py||ansible_runner/streaming.py": [
          "File: ansible_runner/streaming.py -> ansible_runner/streaming.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "198:                 json.dump(event_data, write_file)",
          "200:     def artifacts_callback(self, artifacts_data):",
          "202:         with zipfile.ZipFile(buf, 'r') as archive:",
          "203:             archive.extractall(path=self.config.artifact_dir)",
          "",
          "[Removed Lines]",
          "201:         buf = io.BytesIO(artifacts_data)",
          "",
          "[Added Lines]",
          "201:         buf = io.BytesIO(self._input.read(artifacts_data['zipfile']))",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "217:             if 'status' in data:",
          "218:                 self.status_callback(data)",
          "219:             elif 'zipfile' in data:",
          "221:             elif 'eof' in data:",
          "222:                 break",
          "223:             else:",
          "",
          "[Removed Lines]",
          "220:                 self.artifacts_callback(self._input.read(data['zipfile']))",
          "",
          "[Added Lines]",
          "220:                 self.artifacts_callback(data)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "aa194612a3dba93a2555e91e0eec21a7b7a7ac8d",
      "candidate_info": {
        "commit_hash": "aa194612a3dba93a2555e91e0eec21a7b7a7ac8d",
        "repo": "ansible/ansible-runner",
        "commit_url": "https://github.com/ansible/ansible-runner/commit/aa194612a3dba93a2555e91e0eec21a7b7a7ac8d",
        "files": [
          "ansible_runner/streaming.py"
        ],
        "message": "Preserve permissions when unzipping worker payload\n\nPulled from: https://www.burgundywall.com/post/preserving-file-perms-with-python-zipfile-module",
        "before_after_code_files": [
          "ansible_runner/streaming.py||ansible_runner/streaming.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "ansible_runner/streaming.py||ansible_runner/streaming.py"
          ],
          "candidate": [
            "ansible_runner/streaming.py||ansible_runner/streaming.py"
          ]
        }
      },
      "candidate_diff": {
        "ansible_runner/streaming.py||ansible_runner/streaming.py": [
          "File: ansible_runner/streaming.py -> ansible_runner/streaming.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "88:                 zip_data = self._input.read(data['zipfile'])",
          "89:                 buf = io.BytesIO(zip_data)",
          "90:                 with zipfile.ZipFile(buf, 'r') as archive:",
          "92:             elif 'eof' in data:",
          "93:                 break",
          "",
          "[Removed Lines]",
          "91:                     archive.extractall(path=self.private_data_dir)",
          "",
          "[Added Lines]",
          "91:                     # Fancy extraction in order to preserve permissions",
          "92:                     # https://www.burgundywall.com/post/preserving-file-perms-with-python-zipfile-module",
          "93:                     for info in archive.infolist():",
          "94:                         archive.extract(info.filename, path=self.private_data_dir)",
          "95:                         out_path = os.path.join(self.private_data_dir, info.filename)",
          "96:                         perm = info.external_attr >> 16",
          "97:                         os.chmod(out_path, perm)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "60d39348d9b3144640776bfa9a2ed200e479e48a",
      "candidate_info": {
        "commit_hash": "60d39348d9b3144640776bfa9a2ed200e479e48a",
        "repo": "ansible/ansible-runner",
        "commit_url": "https://github.com/ansible/ansible-runner/commit/60d39348d9b3144640776bfa9a2ed200e479e48a",
        "files": [
          "ansible_runner/streaming.py"
        ],
        "message": "Preserve permissions when unzipping artifacts",
        "before_after_code_files": [
          "ansible_runner/streaming.py||ansible_runner/streaming.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "ansible_runner/streaming.py||ansible_runner/streaming.py"
          ],
          "candidate": [
            "ansible_runner/streaming.py||ansible_runner/streaming.py"
          ]
        }
      },
      "candidate_diff": {
        "ansible_runner/streaming.py||ansible_runner/streaming.py": [
          "File: ansible_runner/streaming.py -> ansible_runner/streaming.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "206:     def artifacts_callback(self, artifacts_data):",
          "207:         buf = io.BytesIO(self._input.read(artifacts_data['zipfile']))",
          "208:         with zipfile.ZipFile(buf, 'r') as archive:",
          "211:         if self.artifacts_handler is not None:",
          "212:             self.artifacts_handler(self.artifact_dir)",
          "",
          "[Removed Lines]",
          "209:             archive.extractall(path=self.artifact_dir)",
          "",
          "[Added Lines]",
          "209:             # Fancy extraction in order to preserve permissions",
          "210:             # https://www.burgundywall.com/post/preserving-file-perms-with-python-zipfile-module",
          "211:             for info in archive.infolist():",
          "212:                 archive.extract(info.filename, path=self.private_data_dir)",
          "213:                 out_path = os.path.join(self.private_data_dir, info.filename)",
          "214:                 perm = info.external_attr >> 16",
          "215:                 os.chmod(out_path, perm)",
          "",
          "---------------"
        ]
      }
    }
  ]
}