{
  "cve_id": "CVE-2022-31777",
  "cve_desc": "A stored cross-site scripting (XSS) vulnerability in Apache Spark 3.2.1 and earlier, and 3.3.0, allows remote attackers to execute arbitrary JavaScript in the web browser of a user, by including a malicious payload into the logs which would be returned in logs rendered in the UI.",
  "repo": "apache/spark",
  "patch_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
  "patch_info": {
    "commit_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
    "repo": "apache/spark",
    "commit_url": "https://github.com/apache/spark/commit/ad90195de56688ce0898691eb9d04297ab0871ad",
    "files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ],
    "message": "[SPARK-39505][UI] Escape log content rendered in UI\n\n### What changes were proposed in this pull request?\n\nEscape log content rendered to the UI.\n\n### Why are the changes needed?\n\nLog content may contain reserved characters or other code in the log and be misinterpreted in the UI as HTML.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nExisting tests\n\nCloses #36902 from srowen/LogViewEscape.\n\nAuthored-by: Sean Owen <srowen@gmail.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
    "before_after_code_files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ]
  },
  "patch_diff": {
    "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js": [
      "File: core/src/main/resources/org/apache/spark/ui/static/log-view.js -> core/src/main/resources/org/apache/spark/ui/static/log-view.js",
      "--- Hunk 1 ---",
      "[Context before]",
      "85:       if (retStartByte == 0) {",
      "86:         disableMoreButton();",
      "87:       }",
      "90:       curLogLength = curLogLength + (startByte - retStartByte);",
      "91:       startByte = retStartByte;",
      "",
      "[Removed Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(cleanData);",
      "",
      "[Added Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(document.createTextNode(cleanData));",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "115:             var retLogLength = dataInfo[2];",
      "117:             var cleanData = data.substring(newlineIndex + 1);",
      "120:             curLogLength = curLogLength + (retEndByte - retStartByte);",
      "121:             endByte = retEndByte;",
      "",
      "[Removed Lines]",
      "118:             $(\"pre\", \".log-content\").append(cleanData);",
      "",
      "[Added Lines]",
      "118:             $(\"pre\", \".log-content\").append(document.createTextNode(cleanData));",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "49830c68f0dcc55419df46f4de4c65479db1eb95",
      "candidate_info": {
        "commit_hash": "49830c68f0dcc55419df46f4de4c65479db1eb95",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/49830c68f0dcc55419df46f4de4c65479db1eb95",
        "files": [
          "sql/catalyst/src/main/antlr4/org/apache/spark/sql/catalyst/parser/SqlBaseParser.g4",
          "sql/catalyst/src/main/java/org/apache/spark/sql/connector/catalog/TableChange.java",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/v2AlterTableCommands.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/connector/catalog/CatalogV2Util.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/DDLParserSuite.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/connector/catalog/CatalogSuite.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveSessionCatalog.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/connector/AlterTableTests.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/connector/V2CommandsCaseSensitivitySuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/v2/V2SessionCatalogSuite.scala"
        ],
        "message": "[SPARK-38939][SQL] Support DROP COLUMN [IF EXISTS] syntax\n\n### What changes were proposed in this pull request?\nThis PR introduces the following:\n- Parser changes to have an `IF EXISTS` clause for `DROP COLUMN`.\n- Logic to silence the errors within parser and analyzer when encountering missing columns while using `IF EXISTS`\n- Ensure only resolving and dropping existing columns inside table schema\n\n### Why are the changes needed?\nCurrently `ALTER TABLE ... DROP COLUMN(s) ...` syntax will always throw error if the column doesn't exist. This PR would like to provide an (IF EXISTS) syntax to provide better user experience for downstream handlers (such as Delta with incoming column dropping support) that support it, and make consistent with some other DMLs such as `DROP TABLE IF EXISTS`.\n\n### Does this PR introduce _any_ user-facing change?\nUser may now specify `ALTER TABLE xxx DROP COLUMN(S) IF EXISTS a, a.b, c.d`.\n\n### How was this patch tested?\nModified existing unit tests and new unit tests.\n\ncloud-fan gengliangwang MaxGekk\n\nCloses #36252 from jackierwzhang/SPARK-38939.\n\nAuthored-by: jackierwzhang <ruowang.zhang@databricks.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/antlr4/org/apache/spark/sql/catalyst/parser/SqlBaseParser.g4||sql/catalyst/src/main/antlr4/org/apache/spark/sql/catalyst/parser/SqlBaseParser.g4",
          "sql/catalyst/src/main/java/org/apache/spark/sql/connector/catalog/TableChange.java||sql/catalyst/src/main/java/org/apache/spark/sql/connector/catalog/TableChange.java",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/v2AlterTableCommands.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/v2AlterTableCommands.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/connector/catalog/CatalogV2Util.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/connector/catalog/CatalogV2Util.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/DDLParserSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/DDLParserSuite.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/connector/catalog/CatalogSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/connector/catalog/CatalogSuite.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveSessionCatalog.scala||sql/core/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveSessionCatalog.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/connector/AlterTableTests.scala||sql/core/src/test/scala/org/apache/spark/sql/connector/AlterTableTests.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/connector/V2CommandsCaseSensitivitySuite.scala||sql/core/src/test/scala/org/apache/spark/sql/connector/V2CommandsCaseSensitivitySuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/v2/V2SessionCatalogSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/v2/V2SessionCatalogSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/antlr4/org/apache/spark/sql/catalyst/parser/SqlBaseParser.g4||sql/catalyst/src/main/antlr4/org/apache/spark/sql/catalyst/parser/SqlBaseParser.g4": [
          "File: sql/catalyst/src/main/antlr4/org/apache/spark/sql/catalyst/parser/SqlBaseParser.g4 -> sql/catalyst/src/main/antlr4/org/apache/spark/sql/catalyst/parser/SqlBaseParser.g4",
          "--- Hunk 1 ---",
          "[Context before]",
          "110:         RENAME COLUMN",
          "111:         from=multipartIdentifier TO to=errorCapturingIdentifier        #renameTableColumn",
          "112:     | ALTER TABLE multipartIdentifier",
          "114:         LEFT_PAREN columns=multipartIdentifierList RIGHT_PAREN         #dropTableColumns",
          "115:     | ALTER TABLE multipartIdentifier",
          "117:     | ALTER (TABLE | VIEW) from=multipartIdentifier",
          "118:         RENAME TO to=multipartIdentifier                               #renameTable",
          "119:     | ALTER (TABLE | VIEW) multipartIdentifier",
          "",
          "[Removed Lines]",
          "113:         DROP (COLUMN | COLUMNS)",
          "116:         DROP (COLUMN | COLUMNS) columns=multipartIdentifierList        #dropTableColumns",
          "",
          "[Added Lines]",
          "113:         DROP (COLUMN | COLUMNS) (IF EXISTS)?",
          "116:         DROP (COLUMN | COLUMNS) (IF EXISTS)?",
          "117:         columns=multipartIdentifierList                                #dropTableColumns",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/java/org/apache/spark/sql/connector/catalog/TableChange.java||sql/catalyst/src/main/java/org/apache/spark/sql/connector/catalog/TableChange.java": [
          "File: sql/catalyst/src/main/java/org/apache/spark/sql/connector/catalog/TableChange.java -> sql/catalyst/src/main/java/org/apache/spark/sql/connector/catalog/TableChange.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "231:   }",
          "",
          "[Removed Lines]",
          "229:   static TableChange deleteColumn(String[] fieldNames) {",
          "230:     return new DeleteColumn(fieldNames);",
          "",
          "[Added Lines]",
          "230:   static TableChange deleteColumn(String[] fieldNames, Boolean ifExists) {",
          "231:     return new DeleteColumn(fieldNames, ifExists);",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "652:   final class DeleteColumn implements ColumnChange {",
          "653:     private final String[] fieldNames;",
          "656:       this.fieldNames = fieldNames;",
          "657:     }",
          "659:     @Override",
          "",
          "[Removed Lines]",
          "655:     private DeleteColumn(String[] fieldNames) {",
          "",
          "[Added Lines]",
          "655:     private final Boolean ifExists;",
          "657:     private DeleteColumn(String[] fieldNames, Boolean ifExists) {",
          "659:       this.ifExists = ifExists;",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "661:       return fieldNames;",
          "662:     }",
          "664:     @Override",
          "665:     public boolean equals(Object o) {",
          "666:       if (this == o) return true;",
          "667:       if (o == null || getClass() != o.getClass()) return false;",
          "668:       DeleteColumn that = (DeleteColumn) o;",
          "670:     }",
          "672:     @Override",
          "",
          "[Removed Lines]",
          "669:       return Arrays.equals(fieldNames, that.fieldNames);",
          "",
          "[Added Lines]",
          "667:     public Boolean ifExists() { return ifExists; }",
          "674:       return Arrays.equals(fieldNames, that.fieldNames) && that.ifExists() == this.ifExists();",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "3669:           case other => other",
          "3670:         })",
          "3672:       case a: AlterTableCommand if a.table.resolved && hasUnresolvedFieldName(a) =>",
          "3673:         val table = a.table.asInstanceOf[ResolvedTable]",
          "3674:         a.transformExpressions {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "3672:       case a: DropColumns if a.table.resolved && hasUnresolvedFieldName(a) && a.ifExists =>",
          "3674:         val table = a.table.asInstanceOf[ResolvedTable]",
          "3675:         val columnsToDrop = a.columnsToDrop",
          "3676:         a.copy(columnsToDrop = columnsToDrop.flatMap(c => resolveFieldNamesOpt(table, c.name, c)))",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "3758:         table: ResolvedTable,",
          "3759:         fieldName: Seq[String],",
          "3760:         context: Expression): ResolvedFieldName = {",
          "3761:       table.schema.findNestedField(",
          "3762:         fieldName, includeCollections = true, conf.resolver, context.origin",
          "3763:       ).map {",
          "3764:         case (path, field) => ResolvedFieldName(path, field)",
          "3766:     }",
          "3768:     private def hasUnresolvedFieldName(a: AlterTableCommand): Boolean = {",
          "",
          "[Removed Lines]",
          "3765:       }.getOrElse(throw QueryCompilationErrors.missingFieldError(fieldName, table, context.origin))",
          "",
          "[Added Lines]",
          "3767:       resolveFieldNamesOpt(table, fieldName, context)",
          "3768:         .getOrElse(throw QueryCompilationErrors.missingFieldError(fieldName, table, context.origin))",
          "3769:     }",
          "3771:     private def resolveFieldNamesOpt(",
          "3772:         table: ResolvedTable,",
          "3773:         fieldName: Seq[String],",
          "3774:         context: Expression): Option[ResolvedFieldName] = {",
          "3779:       }",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "3836:   override def visitDropTableColumns(",
          "3837:       ctx: DropTableColumnsContext): LogicalPlan = withOrigin(ctx) {",
          "3838:     val columnsToDrop = ctx.columns.multipartIdentifier.asScala.map(typedVisit[Seq[String]])",
          "3839:     DropColumns(",
          "3840:       createUnresolvedTable(",
          "3841:         ctx.multipartIdentifier,",
          "3842:         \"ALTER TABLE ... DROP COLUMNS\"),",
          "3844:   }",
          "",
          "[Removed Lines]",
          "3843:       columnsToDrop.map(UnresolvedFieldName(_)).toSeq)",
          "",
          "[Added Lines]",
          "3838:     val ifExists = ctx.EXISTS() != null",
          "3844:       columnsToDrop.map(UnresolvedFieldName(_)).toSeq,",
          "3845:       ifExists)",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/v2AlterTableCommands.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/v2AlterTableCommands.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/v2AlterTableCommands.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/v2AlterTableCommands.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "144:     require(table.resolved)",
          "145:     val deleteChanges = table.schema.fieldNames.map { name =>",
          "147:     }",
          "148:     val addChanges = columnsToAdd.map { col =>",
          "149:       assert(col.path.isEmpty)",
          "",
          "[Removed Lines]",
          "146:       TableChange.deleteColumn(Array(name))",
          "",
          "[Added Lines]",
          "147:       TableChange.deleteColumn(Array(name), ifExists = false)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "168: case class DropColumns(",
          "169:     table: LogicalPlan,",
          "171:   override def changes: Seq[TableChange] = {",
          "172:     columnsToDrop.map { col =>",
          "173:       require(col.resolved, \"FieldName should be resolved before it's converted to TableChange.\")",
          "175:     }",
          "176:   }",
          "",
          "[Removed Lines]",
          "170:     columnsToDrop: Seq[FieldName]) extends AlterTableCommand {",
          "174:       TableChange.deleteColumn(col.name.toArray)",
          "",
          "[Added Lines]",
          "171:     columnsToDrop: Seq[FieldName],",
          "172:     ifExists: Boolean) extends AlterTableCommand {",
          "176:       TableChange.deleteColumn(col.name.toArray, ifExists)",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/connector/catalog/CatalogV2Util.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/connector/catalog/CatalogV2Util.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/connector/catalog/CatalogV2Util.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/connector/catalog/CatalogV2Util.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "191:           }",
          "193:         case delete: DeleteColumn =>",
          "196:         case _ =>",
          "",
          "[Removed Lines]",
          "194:           replace(schema, delete.fieldNames, _ => None)",
          "",
          "[Added Lines]",
          "194:           replace(schema, delete.fieldNames, _ => None, delete.ifExists)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "222:   private def replace(",
          "223:       struct: StructType,",
          "224:       fieldNames: Seq[String],",
          "229:     val field = struct.fields(pos)",
          "230:     val replacement: Option[StructField] = (fieldNames.tail, field.dataType) match {",
          "231:       case (Seq(), _) =>",
          "232:         update(field)",
          "234:       case (names, struct: StructType) =>",
          "236:         Some(StructField(field.name, updatedType, field.nullable, field.metadata))",
          "238:       case (Seq(\"key\"), map @ MapType(keyType, _, _)) =>",
          "",
          "[Removed Lines]",
          "225:       update: StructField => Option[StructField]): StructType = {",
          "227:     val pos = struct.getFieldIndex(fieldNames.head)",
          "228:         .getOrElse(throw new IllegalArgumentException(s\"Cannot find field: ${fieldNames.head}\"))",
          "235:         val updatedType: StructType = replace(struct, names, update)",
          "",
          "[Added Lines]",
          "225:       update: StructField => Option[StructField],",
          "226:       ifExists: Boolean = false): StructType = {",
          "228:     val posOpt = struct.getFieldIndex(fieldNames.head)",
          "229:     if (posOpt.isEmpty) {",
          "230:       if (ifExists) {",
          "233:         return struct",
          "234:       } else {",
          "235:         throw new IllegalArgumentException(s\"Cannot find field: ${fieldNames.head}\")",
          "236:       }",
          "237:     }",
          "239:     val pos = posOpt.get",
          "246:         val updatedType: StructType = replace(struct, names, update, ifExists)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "241:         Some(field.copy(dataType = map.copy(keyType = updated.dataType)))",
          "243:       case (Seq(\"key\", names @ _*), map @ MapType(keyStruct: StructType, _, _)) =>",
          "246:       case (Seq(\"value\"), map @ MapType(_, mapValueType, isNullable)) =>",
          "247:         val updated = update(StructField(\"value\", mapValueType, nullable = isNullable))",
          "",
          "[Removed Lines]",
          "244:         Some(field.copy(dataType = map.copy(keyType = replace(keyStruct, names, update))))",
          "",
          "[Added Lines]",
          "255:         Some(field.copy(dataType = map.copy(keyType = replace(keyStruct, names, update, ifExists))))",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "251:           valueContainsNull = updated.nullable)))",
          "253:       case (Seq(\"value\", names @ _*), map @ MapType(_, valueStruct: StructType, _)) =>",
          "256:       case (Seq(\"element\"), array @ ArrayType(elementType, isNullable)) =>",
          "257:         val updated = update(StructField(\"element\", elementType, nullable = isNullable))",
          "",
          "[Removed Lines]",
          "254:         Some(field.copy(dataType = map.copy(valueType = replace(valueStruct, names, update))))",
          "",
          "[Added Lines]",
          "265:         Some(field.copy(dataType = map.copy(valueType =",
          "266:           replace(valueStruct, names, update, ifExists))))",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "261:           containsNull = updated.nullable)))",
          "263:       case (Seq(\"element\", names @ _*), array @ ArrayType(elementStruct: StructType, _)) =>",
          "266:       case (names, dataType) =>",
          "269:     }",
          "271:     val newFields = struct.fields.zipWithIndex.flatMap {",
          "",
          "[Removed Lines]",
          "264:         Some(field.copy(dataType = array.copy(elementType = replace(elementStruct, names, update))))",
          "267:         throw new IllegalArgumentException(",
          "268:           s\"Cannot find field: ${names.head} in ${dataType.simpleString}\")",
          "",
          "[Added Lines]",
          "276:         Some(field.copy(dataType = array.copy(elementType =",
          "277:           replace(elementStruct, names, update, ifExists))))",
          "280:         if (!ifExists) {",
          "281:           throw new IllegalArgumentException(",
          "282:             s\"Cannot find field: ${names.head} in ${dataType.simpleString}\")",
          "283:         }",
          "284:         None",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/DDLParserSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/DDLParserSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/DDLParserSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/DDLParserSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "1017:       parsePlan(\"ALTER TABLE table_name DROP COLUMN a.b.c\"),",
          "1018:       DropColumns(",
          "1019:         UnresolvedTable(Seq(\"table_name\"), \"ALTER TABLE ... DROP COLUMNS\", None),",
          "1021:   }",
          "1023:   test(\"alter table: drop multiple columns\") {",
          "",
          "[Removed Lines]",
          "1020:         Seq(UnresolvedFieldName(Seq(\"a\", \"b\", \"c\")))))",
          "",
          "[Added Lines]",
          "1020:         Seq(UnresolvedFieldName(Seq(\"a\", \"b\", \"c\"))),",
          "1021:         ifExists = false))",
          "1023:     comparePlans(",
          "1024:       parsePlan(\"ALTER TABLE table_name DROP COLUMN IF EXISTS a.b.c\"),",
          "1025:       DropColumns(",
          "1026:         UnresolvedTable(Seq(\"table_name\"), \"ALTER TABLE ... DROP COLUMNS\", None),",
          "1027:         Seq(UnresolvedFieldName(Seq(\"a\", \"b\", \"c\"))),",
          "1028:         ifExists = true))",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1029:           UnresolvedTable(Seq(\"table_name\"), \"ALTER TABLE ... DROP COLUMNS\", None),",
          "1030:           Seq(UnresolvedFieldName(Seq(\"x\")),",
          "1031:             UnresolvedFieldName(Seq(\"y\")),",
          "1033:     }",
          "1034:   }",
          "",
          "[Removed Lines]",
          "1032:             UnresolvedFieldName(Seq(\"a\", \"b\", \"c\")))))",
          "",
          "[Added Lines]",
          "1040:             UnresolvedFieldName(Seq(\"a\", \"b\", \"c\"))),",
          "1041:           ifExists = false))",
          "1042:     }",
          "1044:     val sqlIfExists = \"ALTER TABLE table_name DROP COLUMN IF EXISTS x, y, a.b.c\"",
          "1045:     Seq(sqlIfExists, sqlIfExists.replace(\"COLUMN\", \"COLUMNS\")).foreach { drop =>",
          "1046:       comparePlans(",
          "1047:         parsePlan(drop),",
          "1048:         DropColumns(",
          "1049:           UnresolvedTable(Seq(\"table_name\"), \"ALTER TABLE ... DROP COLUMNS\", None),",
          "1050:           Seq(UnresolvedFieldName(Seq(\"x\")),",
          "1051:             UnresolvedFieldName(Seq(\"y\")),",
          "1052:             UnresolvedFieldName(Seq(\"a\", \"b\", \"c\"))),",
          "1053:           ifExists = true))",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/connector/catalog/CatalogSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/connector/catalog/CatalogSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/connector/catalog/CatalogSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/connector/catalog/CatalogSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "550:     assert(table.schema == schema)",
          "552:     val updated = catalog.alterTable(testIdent,",
          "555:     val expectedSchema = new StructType().add(\"data\", StringType)",
          "556:     assert(updated.schema == expectedSchema)",
          "",
          "[Removed Lines]",
          "553:       TableChange.deleteColumn(Array(\"id\")))",
          "",
          "[Added Lines]",
          "553:       TableChange.deleteColumn(Array(\"id\"), false))",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "567:     assert(table.schema == tableSchema)",
          "569:     val updated = catalog.alterTable(testIdent,",
          "572:     val newPointStruct = new StructType().add(\"x\", DoubleType)",
          "573:     val expectedSchema = schema.add(\"point\", newPointStruct)",
          "",
          "[Removed Lines]",
          "570:       TableChange.deleteColumn(Array(\"point\", \"y\")))",
          "",
          "[Added Lines]",
          "570:       TableChange.deleteColumn(Array(\"point\", \"y\"), false))",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "583:     assert(table.schema == schema)",
          "585:     val exc = intercept[IllegalArgumentException] {",
          "587:     }",
          "589:     assert(exc.getMessage.contains(\"missing_col\"))",
          "590:     assert(exc.getMessage.contains(\"Cannot find\"))",
          "591:   }",
          "593:   test(\"alterTable: delete missing nested column fails\") {",
          "",
          "[Removed Lines]",
          "586:       catalog.alterTable(testIdent, TableChange.deleteColumn(Array(\"missing_col\")))",
          "",
          "[Added Lines]",
          "586:       catalog.alterTable(testIdent, TableChange.deleteColumn(Array(\"missing_col\"), false))",
          "593:     catalog.alterTable(testIdent, TableChange.deleteColumn(Array(\"missing_col\"), true))",
          "594:     assert(table.schema == schema)",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "601:     assert(table.schema == tableSchema)",
          "603:     val exc = intercept[IllegalArgumentException] {",
          "605:     }",
          "607:     assert(exc.getMessage.contains(\"z\"))",
          "608:     assert(exc.getMessage.contains(\"Cannot find\"))",
          "609:   }",
          "611:   test(\"alterTable: table does not exist\") {",
          "",
          "[Removed Lines]",
          "604:       catalog.alterTable(testIdent, TableChange.deleteColumn(Array(\"point\", \"z\")))",
          "",
          "[Added Lines]",
          "608:       catalog.alterTable(testIdent, TableChange.deleteColumn(Array(\"point\", \"z\"), false))",
          "615:     catalog.alterTable(testIdent, TableChange.deleteColumn(Array(\"point\", \"z\"), true))",
          "616:     assert(table.schema == tableSchema)",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveSessionCatalog.scala||sql/core/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveSessionCatalog.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveSessionCatalog.scala -> sql/core/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveSessionCatalog.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "93:     case RenameColumn(ResolvedV1TableIdentifier(_), _, _) =>",
          "94:       throw QueryCompilationErrors.operationOnlySupportedWithV2TableError(\"RENAME COLUMN\")",
          "97:       throw QueryCompilationErrors.operationOnlySupportedWithV2TableError(\"DROP COLUMN\")",
          "99:     case SetTableProperties(ResolvedV1TableIdentifier(ident), props) =>",
          "",
          "[Removed Lines]",
          "96:     case DropColumns(ResolvedV1TableIdentifier(_), _) =>",
          "",
          "[Added Lines]",
          "96:     case DropColumns(ResolvedV1TableIdentifier(_), _, _) =>",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/connector/AlterTableTests.scala||sql/core/src/test/scala/org/apache/spark/sql/connector/AlterTableTests.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/connector/AlterTableTests.scala -> sql/core/src/test/scala/org/apache/spark/sql/connector/AlterTableTests.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "1070:     }",
          "1071:   }",
          "1074:     val t = s\"${catalogAndNamespace}table_name\"",
          "1075:     withTable(t) {",
          "1076:       sql(s\"CREATE TABLE $t (id int) USING $v2Format\")",
          "",
          "[Removed Lines]",
          "1073:   test(\"AlterTable: drop column must exist\") {",
          "",
          "[Added Lines]",
          "1073:   test(\"AlterTable: drop column must exist if required\") {",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1080:       }",
          "1082:       assert(exc.getMessage.contains(\"Missing field data\"))",
          "1083:     }",
          "1084:   }",
          "1087:     val t = s\"${catalogAndNamespace}table_name\"",
          "1088:     withTable(t) {",
          "1089:       sql(s\"CREATE TABLE $t (id int) USING $v2Format\")",
          "",
          "[Removed Lines]",
          "1086:   test(\"AlterTable: nested drop column must exist\") {",
          "",
          "[Added Lines]",
          "1085:       sql(s\"ALTER TABLE $t DROP COLUMN IF EXISTS data\")",
          "1086:       val table = getTableMetadata(fullTableName(t))",
          "1087:       assert(table.schema == new StructType().add(\"id\", IntegerType))",
          "1091:   test(\"AlterTable: nested drop column must exist if required\") {",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "1093:       }",
          "1095:       assert(exc.getMessage.contains(\"Missing field point.x\"))",
          "1096:     }",
          "1097:   }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1103:       sql(s\"ALTER TABLE $t DROP COLUMN IF EXISTS point.x\")",
          "1104:       val table = getTableMetadata(fullTableName(t))",
          "1105:       assert(table.schema == new StructType().add(\"id\", IntegerType))",
          "1107:     }",
          "1108:   }",
          "1110:   test(\"AlterTable: drop mixed existing/non-existing columns using IF EXISTS\") {",
          "1111:     val t = s\"${catalogAndNamespace}table_name\"",
          "1112:     withTable(t) {",
          "1113:       sql(s\"CREATE TABLE $t (id int, name string, points array<struct<x: double, y: double>>) \" +",
          "1114:         s\"USING $v2Format\")",
          "1117:       sql(s\"ALTER TABLE $t DROP COLUMNS IF EXISTS \" +",
          "1118:         s\"names, name, points.element.z, id, points.element.x\")",
          "1119:       val table = getTableMetadata(fullTableName(t))",
          "1120:       assert(table.schema == new StructType()",
          "1121:         .add(\"points\", ArrayType(StructType(Seq(StructField(\"y\", DoubleType))))))",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/connector/V2CommandsCaseSensitivitySuite.scala||sql/core/src/test/scala/org/apache/spark/sql/connector/V2CommandsCaseSensitivitySuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/connector/V2CommandsCaseSensitivitySuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/connector/V2CommandsCaseSensitivitySuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "278:   test(\"AlterTable: drop column resolution\") {",
          "279:     Seq(Array(\"ID\"), Array(\"point\", \"X\"), Array(\"POINT\", \"X\"), Array(\"POINT\", \"x\")).foreach { ref =>",
          "284:     }",
          "285:   }",
          "",
          "[Removed Lines]",
          "280:       alterTableTest(",
          "281:         DropColumns(table, Seq(UnresolvedFieldName(ref))),",
          "282:         Seq(\"Missing field \" + ref.quoted)",
          "283:       )",
          "",
          "[Added Lines]",
          "280:       Seq(true, false).foreach { ifExists =>",
          "281:         val expectedErrors = if (ifExists) {",
          "282:           Seq.empty[String]",
          "283:         } else {",
          "284:           Seq(\"Missing field \" + ref.quoted)",
          "285:         }",
          "286:         val alter = DropColumns(table, Seq(UnresolvedFieldName(ref)), ifExists)",
          "287:         if (ifExists) {",
          "289:           assertAnalysisSuccess(alter, caseSensitive = true)",
          "290:           assertAnalysisSuccess(alter, caseSensitive = false)",
          "291:         } else {",
          "292:           alterTableTest(alter, expectedErrors, expectErrorOnCaseSensitive = true)",
          "293:         }",
          "294:       }",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/v2/V2SessionCatalogSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/v2/V2SessionCatalogSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/v2/V2SessionCatalogSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/v2/V2SessionCatalogSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "610:     assert(table.schema == schema)",
          "612:     val updated = catalog.alterTable(testIdent,",
          "615:     val expectedSchema = new StructType().add(\"data\", StringType)",
          "616:     assert(updated.schema == expectedSchema)",
          "",
          "[Removed Lines]",
          "613:       TableChange.deleteColumn(Array(\"id\")))",
          "",
          "[Added Lines]",
          "613:       TableChange.deleteColumn(Array(\"id\"), false))",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "627:     assert(table.schema == tableSchema)",
          "629:     val updated = catalog.alterTable(testIdent,",
          "632:     val newPointStruct = new StructType().add(\"x\", DoubleType)",
          "633:     val expectedSchema = schema.add(\"point\", newPointStruct)",
          "",
          "[Removed Lines]",
          "630:       TableChange.deleteColumn(Array(\"point\", \"y\")))",
          "",
          "[Added Lines]",
          "630:       TableChange.deleteColumn(Array(\"point\", \"y\"), false))",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "643:     assert(table.schema == schema)",
          "645:     val exc = intercept[IllegalArgumentException] {",
          "647:     }",
          "649:     assert(exc.getMessage.contains(\"missing_col\"))",
          "650:     assert(exc.getMessage.contains(\"Cannot find\"))",
          "651:   }",
          "653:   test(\"alterTable: delete missing nested column fails\") {",
          "",
          "[Removed Lines]",
          "646:       catalog.alterTable(testIdent, TableChange.deleteColumn(Array(\"missing_col\")))",
          "",
          "[Added Lines]",
          "646:       catalog.alterTable(testIdent, TableChange.deleteColumn(Array(\"missing_col\"), false))",
          "653:     catalog.alterTable(testIdent, TableChange.deleteColumn(Array(\"missing_col\"), true))",
          "654:     assert(table.schema == schema)",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "661:     assert(table.schema == tableSchema)",
          "663:     val exc = intercept[IllegalArgumentException] {",
          "665:     }",
          "667:     assert(exc.getMessage.contains(\"z\"))",
          "668:     assert(exc.getMessage.contains(\"Cannot find\"))",
          "669:   }",
          "671:   test(\"alterTable: table does not exist\") {",
          "",
          "[Removed Lines]",
          "664:       catalog.alterTable(testIdent, TableChange.deleteColumn(Array(\"point\", \"z\")))",
          "",
          "[Added Lines]",
          "668:       catalog.alterTable(testIdent, TableChange.deleteColumn(Array(\"point\", \"z\"), false))",
          "675:     catalog.alterTable(testIdent, TableChange.deleteColumn(Array(\"point\", \"z\"), true))",
          "676:     assert(table.schema == tableSchema)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "463a24d9afdaefabfa9f1129350b38e69ccd062d",
      "candidate_info": {
        "commit_hash": "463a24d9afdaefabfa9f1129350b38e69ccd062d",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/463a24d9afdaefabfa9f1129350b38e69ccd062d",
        "files": [
          "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateSchemaCompatibilityChecker.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStore.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStoreConf.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/statefulOperators.scala",
          "sql/core/src/test/resources/structured-streaming/checkpoint-version-3.3.0-streaming-deduplication/commits/.0.crc",
          "sql/core/src/test/resources/structured-streaming/checkpoint-version-3.3.0-streaming-deduplication/commits/.1.crc",
          "sql/core/src/test/resources/structured-streaming/checkpoint-version-3.3.0-streaming-deduplication/commits/0",
          "sql/core/src/test/resources/structured-streaming/checkpoint-version-3.3.0-streaming-deduplication/commits/1",
          "sql/core/src/test/resources/structured-streaming/checkpoint-version-3.3.0-streaming-deduplication/metadata",
          "sql/core/src/test/resources/structured-streaming/checkpoint-version-3.3.0-streaming-deduplication/offsets/.0.crc",
          "sql/core/src/test/resources/structured-streaming/checkpoint-version-3.3.0-streaming-deduplication/offsets/.1.crc",
          "sql/core/src/test/resources/structured-streaming/checkpoint-version-3.3.0-streaming-deduplication/offsets/0",
          "sql/core/src/test/resources/structured-streaming/checkpoint-version-3.3.0-streaming-deduplication/offsets/1",
          "sql/core/src/test/resources/structured-streaming/checkpoint-version-3.3.0-streaming-deduplication/state/0/0/.1.delta.crc",
          "sql/core/src/test/resources/structured-streaming/checkpoint-version-3.3.0-streaming-deduplication/state/0/0/.2.delta.crc",
          "sql/core/src/test/resources/structured-streaming/checkpoint-version-3.3.0-streaming-deduplication/state/0/0/1.delta",
          "sql/core/src/test/resources/structured-streaming/checkpoint-version-3.3.0-streaming-deduplication/state/0/0/2.delta",
          "sql/core/src/test/resources/structured-streaming/checkpoint-version-3.3.0-streaming-deduplication/state/0/0/_metadata/.schema.crc",
          "sql/core/src/test/resources/structured-streaming/checkpoint-version-3.3.0-streaming-deduplication/state/0/0/_metadata/schema",
          "sql/core/src/test/resources/structured-streaming/checkpoint-version-3.3.0-streaming-deduplication/state/0/1/.1.delta.crc",
          "sql/core/src/test/resources/structured-streaming/checkpoint-version-3.3.0-streaming-deduplication/state/0/1/.2.delta.crc",
          "sql/core/src/test/resources/structured-streaming/checkpoint-version-3.3.0-streaming-deduplication/state/0/1/1.delta",
          "sql/core/src/test/resources/structured-streaming/checkpoint-version-3.3.0-streaming-deduplication/state/0/1/2.delta",
          "sql/core/src/test/resources/structured-streaming/checkpoint-version-3.3.0-streaming-deduplication/state/0/2/.1.delta.crc",
          "sql/core/src/test/resources/structured-streaming/checkpoint-version-3.3.0-streaming-deduplication/state/0/2/.2.delta.crc",
          "sql/core/src/test/resources/structured-streaming/checkpoint-version-3.3.0-streaming-deduplication/state/0/2/1.delta",
          "sql/core/src/test/resources/structured-streaming/checkpoint-version-3.3.0-streaming-deduplication/state/0/2/2.delta",
          "sql/core/src/test/resources/structured-streaming/checkpoint-version-3.3.0-streaming-deduplication/state/0/3/.1.delta.crc",
          "sql/core/src/test/resources/structured-streaming/checkpoint-version-3.3.0-streaming-deduplication/state/0/3/.2.delta.crc",
          "sql/core/src/test/resources/structured-streaming/checkpoint-version-3.3.0-streaming-deduplication/state/0/3/1.delta",
          "sql/core/src/test/resources/structured-streaming/checkpoint-version-3.3.0-streaming-deduplication/state/0/3/2.delta",
          "sql/core/src/test/resources/structured-streaming/checkpoint-version-3.3.0-streaming-deduplication/state/0/4/.1.delta.crc",
          "sql/core/src/test/resources/structured-streaming/checkpoint-version-3.3.0-streaming-deduplication/state/0/4/.2.delta.crc",
          "sql/core/src/test/resources/structured-streaming/checkpoint-version-3.3.0-streaming-deduplication/state/0/4/1.delta",
          "sql/core/src/test/resources/structured-streaming/checkpoint-version-3.3.0-streaming-deduplication/state/0/4/2.delta",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/state/StateSchemaCompatibilityCheckerSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/streaming/StreamingDeduplicationSuite.scala"
        ],
        "message": "[SPARK-39650][SS] Fix incorrect value schema in streaming deduplication with backward compatibility\n\n### What changes were proposed in this pull request?\n\nThis PR proposes to fix the incorrect value schema in streaming deduplication. It stores the empty row having a single column with null (using NullType), but the value schema is specified as all columns, which leads incorrect behavior from state store schema compatibility checker.\n\nThis PR proposes to set the schema of value as `StructType(Array(StructField(\"__dummy__\", NullType)))` to fit with the empty row. With this change, the streaming queries creating the checkpoint after this fix would work smoothly.\n\nTo not break the existing streaming queries having incorrect value schema, this PR proposes to disable the check for value schema on streaming deduplication. Disabling the value check was there for the format validation (we have two different checkers for state store), but it has been missing for state store schema compatibility check. To avoid adding more config, this PR leverages the existing config \"format validation\" is using.\n\n### Why are the changes needed?\n\nThis is a bug fix. Suppose the streaming query below:\n\n```\n# df has the columns `a`, `b`, `c`\nval df = spark.readStream.format(\"...\").load()\nval query = df.dropDuplicate(\"a\").writeStream.format(\"...\").start()\n```\n\nwhile the query is running, df can produce a different set of columns (e.g. `a`, `b`, `c`, `d`) from the same source due to schema evolution. Since we only deduplicate the rows with column `a`, the change of schema should not matter for streaming deduplication, but state store schema checker throws error saying \"value schema is not compatible\" before this fix.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo, this is basically a bug fix which end users wouldn't notice unless they encountered a bug.\n\n### How was this patch tested?\n\nNew tests.\n\nCloses #37041 from HeartSaVioR/SPARK-39650.\n\nAuthored-by: Jungtaek Lim <kabhwan.opensource@gmail.com>\nSigned-off-by: Jungtaek Lim <kabhwan.opensource@gmail.com>\n(cherry picked from commit fe536033bdd00d921b3c86af329246ca55a4f46a)\nSigned-off-by: Jungtaek Lim <kabhwan.opensource@gmail.com>",
        "before_after_code_files": [
          "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateSchemaCompatibilityChecker.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateSchemaCompatibilityChecker.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStore.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStore.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStoreConf.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStoreConf.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/statefulOperators.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/statefulOperators.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/state/StateSchemaCompatibilityCheckerSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/state/StateSchemaCompatibilityCheckerSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/streaming/StreamingDeduplicationSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/streaming/StreamingDeduplicationSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateSchemaCompatibilityChecker.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateSchemaCompatibilityChecker.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateSchemaCompatibilityChecker.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateSchemaCompatibilityChecker.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "41:   fm.mkdirs(schemaFileLocation.getParent)",
          "43:   def check(keySchema: StructType, valueSchema: StructType): Unit = {",
          "44:     if (fm.exists(schemaFileLocation)) {",
          "45:       logDebug(s\"Schema file for provider $providerId exists. Comparing with provided schema.\")",
          "46:       val (storedKeySchema, storedValueSchema) = readSchemaFile()",
          "49:       } else if (!schemasCompatible(storedKeySchema, keySchema) ||",
          "51:         val errorMsg = \"Provided schema doesn't match to the schema for existing state! \" +",
          "52:           \"Please note that Spark allow difference of field name: check count of fields \" +",
          "53:           \"and data type of each field.\\n\" +",
          "58:           s\"If you want to force running query without schema validation, please set \" +",
          "59:           s\"${SQLConf.STATE_SCHEMA_CHECK_ENABLED.key} to false.\\n\" +",
          "60:           \"Please note running query with incompatible schema could cause indeterministic\" +",
          "",
          "[Removed Lines]",
          "47:       if (storedKeySchema.equals(keySchema) && storedValueSchema.equals(valueSchema)) {",
          "50:         !schemasCompatible(storedValueSchema, valueSchema)) {",
          "54:           s\"- Provided key schema: $keySchema\\n\" +",
          "55:           s\"- Provided value schema: $valueSchema\\n\" +",
          "56:           s\"- Existing key schema: $storedKeySchema\\n\" +",
          "57:           s\"- Existing value schema: $storedValueSchema\\n\" +",
          "",
          "[Added Lines]",
          "44:     check(keySchema, valueSchema, ignoreValueSchema = false)",
          "45:   }",
          "47:   def check(keySchema: StructType, valueSchema: StructType, ignoreValueSchema: Boolean): Unit = {",
          "51:       if (storedKeySchema.equals(keySchema) &&",
          "52:         (ignoreValueSchema || storedValueSchema.equals(valueSchema))) {",
          "55:         (!ignoreValueSchema && !schemasCompatible(storedValueSchema, valueSchema))) {",
          "56:         val errorMsgForKeySchema = s\"- Provided key schema: $keySchema\\n\" +",
          "57:           s\"- Existing key schema: $storedKeySchema\\n\"",
          "61:         val errorMsgForValueSchema = if (!ignoreValueSchema) {",
          "62:           s\"- Provided value schema: $valueSchema\\n\" +",
          "63:             s\"- Existing value schema: $storedValueSchema\\n\"",
          "64:         } else {",
          "65:           \"\"",
          "66:         }",
          "70:           errorMsgForKeySchema +",
          "71:           errorMsgForValueSchema +",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStore.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStore.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStore.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStore.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "511:           val checker = new StateSchemaCompatibilityChecker(storeProviderId, hadoopConf)",
          "515:           if (storeConf.stateSchemaCheckEnabled) {",
          "516:             ret",
          "517:           } else {",
          "",
          "[Removed Lines]",
          "514:           val ret = Try(checker.check(keySchema, valueSchema)).toEither.fold(Some(_), _ => None)",
          "",
          "[Added Lines]",
          "516:           val ret = Try(",
          "517:             checker.check(keySchema, valueSchema,",
          "518:               ignoreValueSchema = !storeConf.formatValidationCheckValue)",
          "519:           ).toEither.fold(Some(_), _ => None)",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStoreConf.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStoreConf.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStoreConf.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStoreConf.scala"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/statefulOperators.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/statefulOperators.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/statefulOperators.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/statefulOperators.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "783:       keyExpressions, getStateInfo, conf) :: Nil",
          "784:   }",
          "786:   override protected def doExecute(): RDD[InternalRow] = {",
          "787:     metrics // force lazy init at driver",
          "789:     child.execute().mapPartitionsWithStateStore(",
          "790:       getStateInfo,",
          "791:       keyExpressions.toStructType,",
          "793:       numColsPrefixKey = 0,",
          "794:       session.sessionState,",
          "795:       Some(session.streams.stateStoreCoordinator),",
          "",
          "[Removed Lines]",
          "792:       child.output.toStructType,",
          "",
          "[Added Lines]",
          "786:   private val schemaForEmptyRow: StructType = StructType(Array(StructField(\"__dummy__\", NullType)))",
          "794:       schemaForEmptyRow,",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/state/StateSchemaCompatibilityCheckerSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/state/StateSchemaCompatibilityCheckerSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/state/StateSchemaCompatibilityCheckerSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/state/StateSchemaCompatibilityCheckerSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "231:     assert((resultKeySchema, resultValueSchema) === (keySchema, valueSchema))",
          "232:   }",
          "234:   private def applyNewSchemaToNestedFieldInKey(newNestedSchema: StructType): StructType = {",
          "235:     applyNewSchemaToNestedField(keySchema, newNestedSchema, \"key3\")",
          "236:   }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "234:   test(\"SPARK-39650: ignore value schema on compatibility check\") {",
          "235:     val typeChangedValueSchema = StructType(valueSchema.map(_.copy(dataType = TimestampType)))",
          "236:     verifySuccess(keySchema, valueSchema, keySchema, typeChangedValueSchema,",
          "237:       ignoreValueSchema = true)",
          "239:     val typeChangedKeySchema = StructType(keySchema.map(_.copy(dataType = TimestampType)))",
          "240:     verifyException(keySchema, valueSchema, typeChangedKeySchema, valueSchema,",
          "241:       ignoreValueSchema = true)",
          "242:   }",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "257:       dir: String,",
          "258:       queryId: UUID,",
          "259:       newKeySchema: StructType,",
          "263:     val providerId = StateStoreProviderId(",
          "264:       StateStoreId(dir, opId, partitionId), queryId)",
          "266:     new StateSchemaCompatibilityChecker(providerId, hadoopConf)",
          "268:   }",
          "270:   private def verifyException(",
          "271:       oldKeySchema: StructType,",
          "272:       oldValueSchema: StructType,",
          "273:       newKeySchema: StructType,",
          "275:     val dir = newDir()",
          "276:     val queryId = UUID.randomUUID()",
          "279:     val e = intercept[StateSchemaNotCompatible] {",
          "281:     }",
          "288:   }",
          "290:   private def verifySuccess(",
          "291:       oldKeySchema: StructType,",
          "292:       oldValueSchema: StructType,",
          "293:       newKeySchema: StructType,",
          "295:     val dir = newDir()",
          "296:     val queryId = UUID.randomUUID()",
          "299:   }",
          "300: }",
          "",
          "[Removed Lines]",
          "260:       newValueSchema: StructType): Unit = {",
          "267:       .check(newKeySchema, newValueSchema)",
          "274:       newValueSchema: StructType): Unit = {",
          "277:     runSchemaChecker(dir, queryId, oldKeySchema, oldValueSchema)",
          "280:       runSchemaChecker(dir, queryId, newKeySchema, newValueSchema)",
          "283:     e.getMessage.contains(\"Provided schema doesn't match to the schema for existing state!\")",
          "284:     e.getMessage.contains(newKeySchema.json)",
          "285:     e.getMessage.contains(newValueSchema.json)",
          "286:     e.getMessage.contains(oldKeySchema.json)",
          "287:     e.getMessage.contains(oldValueSchema.json)",
          "294:       newValueSchema: StructType): Unit = {",
          "297:     runSchemaChecker(dir, queryId, oldKeySchema, oldValueSchema)",
          "298:     runSchemaChecker(dir, queryId, newKeySchema, newValueSchema)",
          "",
          "[Added Lines]",
          "270:       newValueSchema: StructType,",
          "271:       ignoreValueSchema: Boolean): Unit = {",
          "278:       .check(newKeySchema, newValueSchema, ignoreValueSchema = ignoreValueSchema)",
          "285:       newValueSchema: StructType,",
          "286:       ignoreValueSchema: Boolean = false): Unit = {",
          "289:     runSchemaChecker(dir, queryId, oldKeySchema, oldValueSchema,",
          "290:       ignoreValueSchema = ignoreValueSchema)",
          "293:       runSchemaChecker(dir, queryId, newKeySchema, newValueSchema,",
          "294:         ignoreValueSchema = ignoreValueSchema)",
          "297:     assert(e.getMessage.contains(\"Provided schema doesn't match to the schema for existing state!\"))",
          "298:     assert(e.getMessage.contains(newKeySchema.toString()))",
          "299:     assert(e.getMessage.contains(oldKeySchema.toString()))",
          "301:     if (ignoreValueSchema) {",
          "302:       assert(!e.getMessage.contains(newValueSchema.toString()))",
          "303:       assert(!e.getMessage.contains(oldValueSchema.toString()))",
          "304:     } else {",
          "305:       assert(e.getMessage.contains(newValueSchema.toString()))",
          "306:       assert(e.getMessage.contains(oldValueSchema.toString()))",
          "307:     }",
          "314:       newValueSchema: StructType,",
          "315:       ignoreValueSchema: Boolean = false): Unit = {",
          "318:     runSchemaChecker(dir, queryId, oldKeySchema, oldValueSchema,",
          "319:       ignoreValueSchema = ignoreValueSchema)",
          "320:     runSchemaChecker(dir, queryId, newKeySchema, newValueSchema,",
          "321:       ignoreValueSchema = ignoreValueSchema)",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/streaming/StreamingDeduplicationSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/streaming/StreamingDeduplicationSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/streaming/StreamingDeduplicationSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/streaming/StreamingDeduplicationSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: package org.apache.spark.sql.streaming",
          "20: import org.apache.spark.sql.DataFrame",
          "21: import org.apache.spark.sql.catalyst.streaming.InternalOutputModes._",
          "22: import org.apache.spark.sql.execution.streaming.MemoryStream",
          "23: import org.apache.spark.sql.functions._",
          "24: import org.apache.spark.sql.internal.SQLConf",
          "26: class StreamingDeduplicationSuite extends StateStoreMetricsTest {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "20: import java.io.File",
          "22: import org.apache.commons.io.FileUtils",
          "29: import org.apache.spark.util.Utils",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "413:       assertStateOperatorCustomMetric(\"numDroppedDuplicateRows\", expected = 1)",
          "414:     )",
          "415:   }",
          "416: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "422:   test(\"SPARK-39650: duplicate with specific keys should allow input to change schema\") {",
          "423:     withTempDir { checkpoint =>",
          "424:       val dedupeInputData = MemoryStream[(String, Int)]",
          "425:       val dedupe = dedupeInputData.toDS().dropDuplicates(\"_1\")",
          "427:       testStream(dedupe, Append)(",
          "428:         StartStream(checkpointLocation = checkpoint.getCanonicalPath),",
          "430:         AddData(dedupeInputData, \"a\" -> 1),",
          "431:         CheckLastBatch(\"a\" -> 1),",
          "433:         AddData(dedupeInputData, \"a\" -> 2, \"b\" -> 3),",
          "434:         CheckLastBatch(\"b\" -> 3)",
          "435:       )",
          "437:       val dedupeInputData2 = MemoryStream[(String, Int, String)]",
          "438:       val dedupe2 = dedupeInputData2.toDS().dropDuplicates(\"_1\")",
          "441:       dedupeInputData2.addData((\"a\", 1, \"dummy\"))",
          "442:       dedupeInputData2.addData(Seq((\"a\", 2, \"dummy\"), (\"b\", 3, \"dummy\")))",
          "444:       testStream(dedupe2, Append)(",
          "445:         StartStream(checkpointLocation = checkpoint.getCanonicalPath),",
          "447:         AddData(dedupeInputData2, (\"a\", 5, \"a\"), (\"b\", 2, \"b\"), (\"c\", 9, \"c\")),",
          "448:         CheckLastBatch((\"c\", 9, \"c\"))",
          "449:       )",
          "450:     }",
          "451:   }",
          "453:   test(\"SPARK-39650: recovery from checkpoint having all columns as value schema\") {",
          "456:     val inputData = MemoryStream[(String, Int, String)]",
          "457:     val dedupe = inputData.toDS().dropDuplicates(\"_1\")",
          "461:     val resourceUri = this.getClass.getResource(",
          "462:       \"/structured-streaming/checkpoint-version-3.3.0-streaming-deduplication/\").toURI",
          "464:     val checkpointDir = Utils.createTempDir().getCanonicalFile",
          "467:     FileUtils.copyDirectory(new File(resourceUri), checkpointDir)",
          "469:     inputData.addData((\"a\", 1, \"dummy\"))",
          "470:     inputData.addData((\"a\", 2, \"dummy\"), (\"b\", 3, \"dummy\"))",
          "472:     testStream(dedupe, Append)(",
          "473:       StartStream(checkpointLocation = checkpointDir.getAbsolutePath),",
          "475:         Note: The checkpoint was generated using the following input in Spark version 3.3.0",
          "476:         AddData(inputData, (\"a\", 1)),",
          "477:         CheckLastBatch((\"a\", 1)),",
          "478:         AddData(inputData, (\"a\", 2), (\"b\", 3)),",
          "479:         CheckLastBatch((\"b\", 3))",
          "482:       AddData(inputData, (\"a\", 5, \"a\"), (\"b\", 2, \"b\"), (\"c\", 9, \"c\")),",
          "483:       CheckLastBatch((\"c\", 9, \"c\"))",
          "484:     )",
          "485:   }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "047c108378facf4a6bdddac058ba13fad1aca014",
      "candidate_info": {
        "commit_hash": "047c108378facf4a6bdddac058ba13fad1aca014",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/047c108378facf4a6bdddac058ba13fad1aca014",
        "files": [
          "dev/deps/spark-deps-hadoop-2-hive-2.3",
          "dev/deps/spark-deps-hadoop-3-hive-2.3",
          "pom.xml"
        ],
        "message": "[SPARK-39250][BUILD] Upgrade Jackson to 2.13.3\n\n### What changes were proposed in this pull request?\n\nThis PR aims to upgrade Jackson to 2.13.3.\n\n### Why are the changes needed?\n\nAlthough Spark is not affected, Jackson 2.13.0~2.13.2 has the following regression which affects the user apps.\n- https://github.com/FasterXML/jackson-databind/issues/3446\n\nHere is a full release note.\n- https://github.com/FasterXML/jackson/wiki/Jackson-Release-2.13.3\n\n### Does this PR introduce _any_ user-facing change?\n\nNo. The previous version is not released yet.\n\n### How was this patch tested?\n\nPass the CIs.\n\nCloses #36627 from dongjoon-hyun/SPARK-39250.\n\nAuthored-by: Dongjoon Hyun <dongjoon@apache.org>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>\n(cherry picked from commit 73438c048fc646f944415ba2e99cb08cc57d856b)\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
        "before_after_code_files": [
          "dev/deps/spark-deps-hadoop-2-hive-2.3||dev/deps/spark-deps-hadoop-2-hive-2.3",
          "dev/deps/spark-deps-hadoop-3-hive-2.3||dev/deps/spark-deps-hadoop-3-hive-2.3"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/deps/spark-deps-hadoop-2-hive-2.3||dev/deps/spark-deps-hadoop-2-hive-2.3": [
          "File: dev/deps/spark-deps-hadoop-2-hive-2.3 -> dev/deps/spark-deps-hadoop-2-hive-2.3",
          "--- Hunk 1 ---",
          "[Context before]",
          "112: httpcore/4.4.14//httpcore-4.4.14.jar",
          "113: istack-commons-runtime/3.0.8//istack-commons-runtime-3.0.8.jar",
          "114: ivy/2.5.0//ivy-2.5.0.jar",
          "116: jackson-core-asl/1.9.13//jackson-core-asl-1.9.13.jar",
          "122: jackson-jaxrs/1.9.13//jackson-jaxrs-1.9.13.jar",
          "123: jackson-mapper-asl/1.9.13//jackson-mapper-asl-1.9.13.jar",
          "125: jackson-xc/1.9.13//jackson-xc-1.9.13.jar",
          "126: jakarta.annotation-api/1.3.5//jakarta.annotation-api-1.3.5.jar",
          "127: jakarta.inject/2.6.1//jakarta.inject-2.6.1.jar",
          "",
          "[Removed Lines]",
          "115: jackson-annotations/2.13.2//jackson-annotations-2.13.2.jar",
          "117: jackson-core/2.13.2//jackson-core-2.13.2.jar",
          "118: jackson-databind/2.13.2.1//jackson-databind-2.13.2.1.jar",
          "119: jackson-dataformat-cbor/2.13.2//jackson-dataformat-cbor-2.13.2.jar",
          "120: jackson-dataformat-yaml/2.13.2//jackson-dataformat-yaml-2.13.2.jar",
          "121: jackson-datatype-jsr310/2.13.2//jackson-datatype-jsr310-2.13.2.jar",
          "124: jackson-module-scala_2.12/2.13.2//jackson-module-scala_2.12-2.13.2.jar",
          "",
          "[Added Lines]",
          "115: jackson-annotations/2.13.3//jackson-annotations-2.13.3.jar",
          "117: jackson-core/2.13.3//jackson-core-2.13.3.jar",
          "118: jackson-databind/2.13.3//jackson-databind-2.13.3.jar",
          "119: jackson-dataformat-cbor/2.13.3//jackson-dataformat-cbor-2.13.3.jar",
          "120: jackson-dataformat-yaml/2.13.3//jackson-dataformat-yaml-2.13.3.jar",
          "121: jackson-datatype-jsr310/2.13.3//jackson-datatype-jsr310-2.13.3.jar",
          "124: jackson-module-scala_2.12/2.13.3//jackson-module-scala_2.12-2.13.3.jar",
          "",
          "---------------"
        ],
        "dev/deps/spark-deps-hadoop-3-hive-2.3||dev/deps/spark-deps-hadoop-3-hive-2.3": [
          "File: dev/deps/spark-deps-hadoop-3-hive-2.3 -> dev/deps/spark-deps-hadoop-3-hive-2.3",
          "--- Hunk 1 ---",
          "[Context before]",
          "102: ini4j/0.5.4//ini4j-0.5.4.jar",
          "103: istack-commons-runtime/3.0.8//istack-commons-runtime-3.0.8.jar",
          "104: ivy/2.5.0//ivy-2.5.0.jar",
          "106: jackson-core-asl/1.9.13//jackson-core-asl-1.9.13.jar",
          "112: jackson-mapper-asl/1.9.13//jackson-mapper-asl-1.9.13.jar",
          "114: jakarta.annotation-api/1.3.5//jakarta.annotation-api-1.3.5.jar",
          "115: jakarta.inject/2.6.1//jakarta.inject-2.6.1.jar",
          "116: jakarta.servlet-api/4.0.3//jakarta.servlet-api-4.0.3.jar",
          "",
          "[Removed Lines]",
          "105: jackson-annotations/2.13.2//jackson-annotations-2.13.2.jar",
          "107: jackson-core/2.13.2//jackson-core-2.13.2.jar",
          "108: jackson-databind/2.13.2.1//jackson-databind-2.13.2.1.jar",
          "109: jackson-dataformat-cbor/2.13.2//jackson-dataformat-cbor-2.13.2.jar",
          "110: jackson-dataformat-yaml/2.13.2//jackson-dataformat-yaml-2.13.2.jar",
          "111: jackson-datatype-jsr310/2.13.2//jackson-datatype-jsr310-2.13.2.jar",
          "113: jackson-module-scala_2.12/2.13.2//jackson-module-scala_2.12-2.13.2.jar",
          "",
          "[Added Lines]",
          "105: jackson-annotations/2.13.3//jackson-annotations-2.13.3.jar",
          "107: jackson-core/2.13.3//jackson-core-2.13.3.jar",
          "108: jackson-databind/2.13.3//jackson-databind-2.13.3.jar",
          "109: jackson-dataformat-cbor/2.13.3//jackson-dataformat-cbor-2.13.3.jar",
          "110: jackson-dataformat-yaml/2.13.3//jackson-dataformat-yaml-2.13.3.jar",
          "111: jackson-datatype-jsr310/2.13.3//jackson-datatype-jsr310-2.13.3.jar",
          "113: jackson-module-scala_2.12/2.13.3//jackson-module-scala_2.12-2.13.3.jar",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "fb9f85ed3b2391fae3349a34cbda951eee224fd1",
      "candidate_info": {
        "commit_hash": "fb9f85ed3b2391fae3349a34cbda951eee224fd1",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/fb9f85ed3b2391fae3349a34cbda951eee224fd1",
        "files": [
          "sql/core/src/main/scala/org/apache/spark/sql/execution/python/WindowInPandasExec.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/window/WindowExec.scala"
        ],
        "message": "[SPARK-39932][SQL] WindowExec should clear the final partition buffer\n\n### What changes were proposed in this pull request?\n\nExplicitly clear final partition buffer if can not find next in `WindowExec`. The same fix in `WindowInPandasExec`\n\n### Why are the changes needed?\n\nWe do a repartition after a window, then we need do a local sort after window due to RoundRobinPartitioning shuffle.\n\nThe error stack:\n```java\nExternalAppendOnlyUnsafeRowArray INFO - Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter\n\norg.apache.spark.memory.SparkOutOfMemoryError: Unable to acquire 65536 bytes of memory, got 0\n\tat org.apache.spark.memory.MemoryConsumer.throwOom(MemoryConsumer.java:157)\n\tat org.apache.spark.memory.MemoryConsumer.allocateArray(MemoryConsumer.java:97)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.growPointerArrayIfNecessary(UnsafeExternalSorter.java:352)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:435)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:455)\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:355)\n```\n\n`WindowExec` only clear buffer in `fetchNextPartition` so the final partition buffer miss to clear.\n\nIt is not a big problem since we have task completion listener.\n```scala\ntaskContext.addTaskCompletionListener(context -> {\n  cleanupResources();\n});\n```\n\nThis bug only affects if the window is not the last operator for this task and the follow operator like sort.\n\n### Does this PR introduce _any_ user-facing change?\n\nyes, bug fix\n\n### How was this patch tested?\n\nN/A\n\nCloses #37358 from ulysses-you/window.\n\nAuthored-by: ulysses-you <ulyssesyou18@gmail.com>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>\n(cherry picked from commit 1fac870126c289a7ec75f45b6b61c93b9a4965d4)\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>",
        "before_after_code_files": [
          "sql/core/src/main/scala/org/apache/spark/sql/execution/python/WindowInPandasExec.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/python/WindowInPandasExec.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/window/WindowExec.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/window/WindowExec.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/core/src/main/scala/org/apache/spark/sql/execution/python/WindowInPandasExec.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/python/WindowInPandasExec.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/python/WindowInPandasExec.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/python/WindowInPandasExec.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "333:         var rowIndex = 0",
          "338:         override final def next(): Iterator[UnsafeRow] = {",
          "",
          "[Removed Lines]",
          "335:         override final def hasNext: Boolean =",
          "336:           (bufferIterator != null && bufferIterator.hasNext) || nextRowAvailable",
          "",
          "[Added Lines]",
          "335:         override final def hasNext: Boolean = {",
          "336:           val found = (bufferIterator != null && bufferIterator.hasNext) || nextRowAvailable",
          "337:           if (!found) {",
          "339:             buffer.clear()",
          "340:           }",
          "341:           found",
          "342:         }",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/execution/window/WindowExec.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/window/WindowExec.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/window/WindowExec.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/window/WindowExec.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "159:         var rowIndex = 0",
          "164:         val join = new JoinedRow",
          "165:         override final def next(): InternalRow = {",
          "",
          "[Removed Lines]",
          "161:         override final def hasNext: Boolean =",
          "162:           (bufferIterator != null && bufferIterator.hasNext) || nextRowAvailable",
          "",
          "[Added Lines]",
          "161:         override final def hasNext: Boolean = {",
          "162:           val found = (bufferIterator != null && bufferIterator.hasNext) || nextRowAvailable",
          "163:           if (!found) {",
          "165:             buffer.clear()",
          "166:           }",
          "167:           found",
          "168:         }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "7adb6e2102232447fde0cea010c2b68a602613b5",
      "candidate_info": {
        "commit_hash": "7adb6e2102232447fde0cea010c2b68a602613b5",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/7adb6e2102232447fde0cea010c2b68a602613b5",
        "files": [
          "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala"
        ],
        "message": "[SPARK-37753][FOLLOWUP][SQL] Add comments to unit test\n\n### What changes were proposed in this pull request?\nadd comments to unit test.\n\n### Why are the changes needed?\ncode can be hard to understand without comments\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\nexisting test\n\nCloses #37018 from mcdull-zhang/add_reason.\n\nAuthored-by: mcdull-zhang <work4dong@163.com>\nSigned-off-by: Sean Owen <srowen@gmail.com>\n(cherry picked from commit 9fd010be24fcd6d81e05bd08133fd80ba81b97ac)\nSigned-off-by: Sean Owen <srowen@gmail.com>",
        "before_after_code_files": [
          "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala"
        ]
      }
    }
  ]
}