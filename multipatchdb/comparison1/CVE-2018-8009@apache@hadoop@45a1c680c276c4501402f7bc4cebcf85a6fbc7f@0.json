{
  "cve_id": "CVE-2018-8009",
  "cve_desc": "Apache Hadoop 3.1.0, 3.0.0-alpha to 3.0.2, 2.9.0 to 2.9.1, 2.8.0 to 2.8.4, 2.0.0-alpha to 2.7.6, 0.23.0 to 0.23.11 is exploitable via the zip slip vulnerability in places that accept a zip file.",
  "repo": "apache/hadoop",
  "patch_hash": "45a1c680c276c4501402f7bc4cebcf85a6fbc7f5",
  "patch_info": {
    "commit_hash": "45a1c680c276c4501402f7bc4cebcf85a6fbc7f5",
    "repo": "apache/hadoop",
    "commit_url": "https://github.com/apache/hadoop/commit/45a1c680c276c4501402f7bc4cebcf85a6fbc7f5",
    "files": [
      "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileUtil.java",
      "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestFileUtil.java"
    ],
    "message": "Additional check when unpacking archives. Contributed by Jason Lowe and Akira Ajisaka.",
    "before_after_code_files": [
      "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileUtil.java||hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileUtil.java",
      "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestFileUtil.java||hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestFileUtil.java"
    ]
  },
  "patch_diff": {
    "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileUtil.java||hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileUtil.java": [
      "File: hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileUtil.java -> hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileUtil.java",
      "--- Hunk 1 ---",
      "[Context before]",
      "587:   public static void unZip(File inFile, File unzipDir) throws IOException {",
      "588:     Enumeration<? extends ZipEntry> entries;",
      "589:     ZipFile zipFile = new ZipFile(inFile);",
      "591:     try {",
      "592:       entries = zipFile.entries();",
      "593:       while (entries.hasMoreElements()) {",
      "594:         ZipEntry entry = entries.nextElement();",
      "595:         if (!entry.isDirectory()) {",
      "596:           InputStream in = zipFile.getInputStream(entry);",
      "597:           try {",
      "600:               if (!file.getParentFile().isDirectory()) {",
      "601:                 throw new IOException(\"Mkdirs failed to create \" +",
      "602:                                       file.getParentFile().toString());",
      "",
      "[Removed Lines]",
      "598:             File file = new File(unzipDir, entry.getName());",
      "599:             if (!file.getParentFile().mkdirs()) {",
      "",
      "[Added Lines]",
      "590:     String targetDirPath = unzipDir.getCanonicalPath() + File.separator;",
      "597:           File file = new File(unzipDir, entry.getName());",
      "598:           if (!file.getCanonicalPath().startsWith(targetDirPath)) {",
      "599:             throw new IOException(\"expanding \" + entry.getName()",
      "600:                 + \" would create file outside of \" + unzipDir);",
      "601:           }",
      "604:             if (!file.getParentFile().mkdirs()) {",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "706:   private static void unpackEntries(TarArchiveInputStream tis,",
      "707:       TarArchiveEntry entry, File outputDir) throws IOException {",
      "708:     if (entry.isDirectory()) {",
      "709:       File subDir = new File(outputDir, entry.getName());",
      "710:       if (!subDir.mkdirs() && !subDir.isDirectory()) {",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "713:     String targetDirPath = outputDir.getCanonicalPath() + File.separator;",
      "714:     File outputFile = new File(outputDir, entry.getName());",
      "715:     if (!outputFile.getCanonicalPath().startsWith(targetDirPath)) {",
      "716:       throw new IOException(\"expanding \" + entry.getName()",
      "717:           + \" would create entry outside of \" + outputDir);",
      "718:     }",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "719:       return;",
      "720:     }",
      "723:     if (!outputFile.getParentFile().exists()) {",
      "724:       if (!outputFile.getParentFile().mkdirs()) {",
      "725:         throw new IOException(\"Mkdirs failed to create tar internal dir \"",
      "",
      "[Removed Lines]",
      "722:     File outputFile = new File(outputDir, entry.getName());",
      "",
      "[Added Lines]",
      "[None]",
      "",
      "---------------"
    ],
    "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestFileUtil.java||hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestFileUtil.java": [
      "File: hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestFileUtil.java -> hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestFileUtil.java",
      "--- Hunk 1 ---",
      "[Context before]",
      "25: import java.io.FileReader;",
      "26: import java.io.IOException;",
      "27: import java.io.OutputStream;",
      "29: import java.io.PrintWriter;",
      "30: import java.util.ArrayList;",
      "31: import java.util.Arrays;",
      "32: import java.util.Collections;",
      "",
      "[Removed Lines]",
      "28: import java.net.URI;",
      "",
      "[Added Lines]",
      "29: import java.net.URI;",
      "30: import java.nio.charset.StandardCharsets;",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "40: import org.apache.commons.logging.Log;",
      "41: import org.apache.commons.logging.LogFactory;",
      "42: import org.apache.hadoop.conf.Configuration;",
      "43: import org.apache.hadoop.util.Shell;",
      "44: import org.apache.hadoop.util.StringUtils;",
      "45: import org.apache.tools.tar.TarEntry;",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "44: import org.apache.hadoop.test.GenericTestUtils;",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "743:     } catch (IOException ioe) {",
      "745:     }",
      "748:   @Test (timeout = 30000)",
      "",
      "[Removed Lines]",
      "746:   }",
      "",
      "[Added Lines]",
      "746:   }",
      "748:   @Test (timeout = 30000)",
      "749:   public void testUnZip2() throws IOException {",
      "750:     setupDirs();",
      "752:     final File simpleZip = new File(del, FILE);",
      "753:     OutputStream os = new FileOutputStream(simpleZip);",
      "754:     try (ZipOutputStream tos = new ZipOutputStream(os)) {",
      "756:       ZipEntry ze = new ZipEntry(\"../foo\");",
      "757:       byte[] data = \"some-content\".getBytes(StandardCharsets.UTF_8);",
      "758:       ze.setSize(data.length);",
      "759:       tos.putNextEntry(ze);",
      "760:       tos.write(data);",
      "761:       tos.closeEntry();",
      "762:       tos.flush();",
      "763:       tos.finish();",
      "764:     }",
      "767:     try {",
      "768:       FileUtil.unZip(simpleZip, tmp);",
      "769:       Assert.fail(\"unZip should throw IOException.\");",
      "770:     } catch (IOException e) {",
      "771:       GenericTestUtils.assertExceptionContains(",
      "772:           \"would create file outside of\", e);",
      "773:     }",
      "774:   }",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "e8777342d3d916276a06bbd8cb2257159402bd58",
      "candidate_info": {
        "commit_hash": "e8777342d3d916276a06bbd8cb2257159402bd58",
        "repo": "apache/hadoop",
        "commit_url": "https://github.com/apache/hadoop/commit/e8777342d3d916276a06bbd8cb2257159402bd58",
        "files": [
          "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockInfoContiguousUnderConstruction.java",
          "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
          "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
          "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/LeaseManager.java",
          "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestBlockInfoUnderConstruction.java",
          "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestBlockUnderConstruction.java",
          "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCommitBlockSynchronization.java"
        ],
        "message": "HDFS-13486. Backport HDFS-11817 (A faulty node can cause a lease leak and NPE on accessing data) to branch-2.7. Contributed by Kihwal Lee.",
        "before_after_code_files": [
          "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockInfoContiguousUnderConstruction.java||hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockInfoContiguousUnderConstruction.java",
          "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java||hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
          "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java||hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
          "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/LeaseManager.java||hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/LeaseManager.java",
          "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestBlockInfoUnderConstruction.java||hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestBlockInfoUnderConstruction.java",
          "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestBlockUnderConstruction.java||hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestBlockUnderConstruction.java",
          "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCommitBlockSynchronization.java||hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCommitBlockSynchronization.java"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/bcdev/hadoop/pull/2"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockInfoContiguousUnderConstruction.java||hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockInfoContiguousUnderConstruction.java": [
          "File: hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockInfoContiguousUnderConstruction.java -> hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockInfoContiguousUnderConstruction.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "192:   public void setExpectedLocations(DatanodeStorageInfo[] targets) {",
          "194:     this.replicas = new ArrayList<ReplicaUnderConstruction>(numLocations);",
          "196:       if (targets[i] != null) {",
          "197:         replicas.add(new ReplicaUnderConstruction(this, targets[i], ReplicaState.RBW));",
          "198:       }",
          "199:   }",
          "",
          "[Removed Lines]",
          "193:     int numLocations = targets == null ? 0 : targets.length;",
          "195:     for(int i = 0; i < numLocations; i++)",
          "",
          "[Added Lines]",
          "193:     if (targets == null) {",
          "194:       return;",
          "195:     }",
          "196:     int numLocations = 0;",
          "197:     for (DatanodeStorageInfo target : targets) {",
          "198:       if (target != null) {",
          "199:         numLocations++;",
          "200:       }",
          "201:     }",
          "204:     for(int i = 0; i < targets.length; i++) {",
          "209:     }",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "291:     setBlockUCState(BlockUCState.UNDER_RECOVERY);",
          "292:     blockRecoveryId = recoveryId;",
          "293:     if (replicas.size() == 0) {",
          "294:       NameNode.blockStateChangeLog.warn(\"BLOCK*\"",
          "295:         + \" BlockInfoUnderConstruction.initLeaseRecovery:\"",
          "",
          "[Removed Lines]",
          "290:   public void initializeBlockRecovery(long recoveryId) {",
          "",
          "[Added Lines]",
          "303:   public void initializeBlockRecovery(long recoveryId, boolean startRecovery) {",
          "306:     if (!startRecovery) {",
          "307:       return;",
          "308:     }",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "337:   void addReplicaIfNotPresent(DatanodeStorageInfo storage,",
          "338:                      Block block,",
          "339:                      ReplicaState rState) {",
          "358:       }",
          "359:     }",
          "361:   }",
          "363:   @Override // BlockInfo",
          "",
          "[Removed Lines]",
          "340:     Iterator<ReplicaUnderConstruction> it = replicas.iterator();",
          "341:     while (it.hasNext()) {",
          "342:       ReplicaUnderConstruction r = it.next();",
          "343:       DatanodeStorageInfo expectedLocation = r.getExpectedStorageLocation();",
          "344:       if(expectedLocation == storage) {",
          "346:         r.setGenerationStamp(block.getGenerationStamp());",
          "347:         return;",
          "348:       } else if (expectedLocation != null &&",
          "349:                  expectedLocation.getDatanodeDescriptor() ==",
          "350:                      storage.getDatanodeDescriptor()) {",
          "356:         it.remove();",
          "357:         break;",
          "360:     replicas.add(new ReplicaUnderConstruction(block, storage, rState));",
          "",
          "[Added Lines]",
          "356:     if (replicas == null) {",
          "357:       replicas = new ArrayList<ReplicaUnderConstruction>(1);",
          "358:       replicas.add(new ReplicaUnderConstruction(block, storage,",
          "359:           rState));",
          "360:     } else {",
          "361:       Iterator<ReplicaUnderConstruction> it = replicas.iterator();",
          "362:       while (it.hasNext()) {",
          "363:         ReplicaUnderConstruction r = it.next();",
          "364:         DatanodeStorageInfo expectedLocation = r.getExpectedStorageLocation();",
          "365:         if (expectedLocation == storage) {",
          "367:           r.setGenerationStamp(block.getGenerationStamp());",
          "368:           return;",
          "369:         } else if (expectedLocation != null",
          "370:             && expectedLocation.getDatanodeDescriptor() ==",
          "371:             storage.getDatanodeDescriptor()) {",
          "377:           it.remove();",
          "378:           break;",
          "379:         }",
          "381:       replicas.add(new ReplicaUnderConstruction(block, storage, rState));",
          "",
          "---------------"
        ],
        "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java||hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java": [
          "File: hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java -> hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "505:       DatanodeID[] datanodeID, String[] storageIDs,",
          "506:       String format, Object... args) throws UnregisteredNodeException {",
          "507:     if (datanodeID.length != storageIDs.length) {",
          "508:       final String err = (storageIDs.length == 0?",
          "509:           \"Missing storageIDs: It is likely that the HDFS client,\"",
          "510:           + \" who made this call, is running in an older version of Hadoop\"",
          "512:           : \"Length mismatched: storageIDs.length=\" + storageIDs.length + \" != \"",
          "513:           ) + \" datanodeID.length=\" + datanodeID.length;",
          "514:       throw new HadoopIllegalArgumentException(",
          "",
          "[Removed Lines]",
          "511:           + \" which does not support storageIDs.\"",
          "",
          "[Added Lines]",
          "512:           + \"(pre-2.0.0-alpha)  which does not support storageIDs.\"",
          "",
          "---------------"
        ],
        "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java||hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java": [
          "File: hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java -> hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "2205:           truncatedBlockUC.getTruncateBlock().getNumBytes(), truncatedBlockUC);",
          "2206:     }",
          "2207:     if (shouldRecoverNow) {",
          "2209:     }",
          "2211:     return newBlock;",
          "",
          "[Removed Lines]",
          "2208:       truncatedBlockUC.initializeBlockRecovery(newBlock.getGenerationStamp());",
          "",
          "[Added Lines]",
          "2208:       truncatedBlockUC.initializeBlockRecovery(newBlock.getGenerationStamp(),",
          "2209:           true);",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "4105:       } else if(truncateRecovery) {",
          "4106:         recoveryBlock.setGenerationStamp(blockRecoveryId);",
          "4107:       }",
          "4109:       leaseManager.renewLease(lease);",
          "",
          "[Removed Lines]",
          "4108:       uc.initializeBlockRecovery(blockRecoveryId);",
          "",
          "[Added Lines]",
          "4109:       uc.initializeBlockRecovery(blockRecoveryId, true);",
          "",
          "---------------"
        ],
        "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/LeaseManager.java||hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/LeaseManager.java": [
          "File: hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/LeaseManager.java -> hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/LeaseManager.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "491:         try {",
          "492:           INodesInPath iip = fsnamesystem.getFSDirectory().getINodesInPath(p,",
          "493:               true);",
          "496:           if (LOG.isDebugEnabled()) {",
          "497:             if (completed) {",
          "498:               LOG.debug(\"Lease recovery for \" + p + \" is complete. File closed.\");",
          "",
          "[Removed Lines]",
          "494:           boolean completed = fsnamesystem.internalReleaseLease(leaseToCheck, p,",
          "495:               iip, HdfsServerConstants.NAMENODE_LEASE_HOLDER);",
          "",
          "[Added Lines]",
          "495:           if (!p.startsWith(\"/\")) {",
          "496:             throw new IOException(\"Invalid path in the lease \" + p);",
          "497:           }",
          "498:           boolean completed = false;",
          "499:           try {",
          "500:             completed = fsnamesystem.internalReleaseLease(",
          "501:                 leaseToCheck, p, iip,",
          "502:                 HdfsServerConstants.NAMENODE_LEASE_HOLDER);",
          "503:           } catch (IOException e) {",
          "504:             LOG.warn(\"Cannot release the path \" + p + \" in the lease \"",
          "505:                 + leaseToCheck + \". It will be retried.\", e);",
          "506:             continue;",
          "507:           }",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "505:             needSync = true;",
          "506:           }",
          "507:         } catch (IOException e) {",
          "509:               + leaseToCheck, e);",
          "510:           removing.add(p);",
          "511:         }",
          "",
          "[Removed Lines]",
          "508:           LOG.error(\"Cannot release the path \" + p + \" in the lease \"",
          "",
          "[Added Lines]",
          "520:           LOG.warn(\"Removing lease with an invalid path: \" + p + \",\"",
          "",
          "---------------"
        ],
        "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestBlockInfoUnderConstruction.java||hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestBlockInfoUnderConstruction.java": [
          "File: hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestBlockInfoUnderConstruction.java -> hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestBlockInfoUnderConstruction.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "50:     DFSTestUtil.resetLastUpdatesWithOffset(dd1, -3 * 1000);",
          "51:     DFSTestUtil.resetLastUpdatesWithOffset(dd2, -1 * 1000);",
          "52:     DFSTestUtil.resetLastUpdatesWithOffset(dd3, -2 * 1000);",
          "54:     BlockInfoContiguousUnderConstruction[] blockInfoRecovery = dd2.getLeaseRecoveryCommand(1);",
          "55:     assertEquals(blockInfoRecovery[0], blockInfo);",
          "",
          "[Removed Lines]",
          "53:     blockInfo.initializeBlockRecovery(1);",
          "",
          "[Added Lines]",
          "53:     blockInfo.initializeBlockRecovery(1, true);",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "58:     DFSTestUtil.resetLastUpdatesWithOffset(dd1, -2 * 1000);",
          "59:     DFSTestUtil.resetLastUpdatesWithOffset(dd2, -1 * 1000);",
          "60:     DFSTestUtil.resetLastUpdatesWithOffset(dd3, -3 * 1000);",
          "62:     blockInfoRecovery = dd1.getLeaseRecoveryCommand(1);",
          "63:     assertEquals(blockInfoRecovery[0], blockInfo);",
          "",
          "[Removed Lines]",
          "61:     blockInfo.initializeBlockRecovery(2);",
          "",
          "[Added Lines]",
          "61:     blockInfo.initializeBlockRecovery(2, true);",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "66:     DFSTestUtil.resetLastUpdatesWithOffset(dd1, -2 * 1000);",
          "67:     DFSTestUtil.resetLastUpdatesWithOffset(dd2, -1 * 1000);",
          "68:     DFSTestUtil.resetLastUpdatesWithOffset(dd3, -3 * 1000);",
          "70:     blockInfoRecovery = dd3.getLeaseRecoveryCommand(1);",
          "71:     assertEquals(blockInfoRecovery[0], blockInfo);",
          "",
          "[Removed Lines]",
          "69:     blockInfo.initializeBlockRecovery(3);",
          "",
          "[Added Lines]",
          "69:     blockInfo.initializeBlockRecovery(3, true);",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "75:     DFSTestUtil.resetLastUpdatesWithOffset(dd1, -2 * 1000);",
          "76:     DFSTestUtil.resetLastUpdatesWithOffset(dd2, -1 * 1000);",
          "77:     DFSTestUtil.resetLastUpdatesWithOffset(dd3, 0);",
          "79:     blockInfoRecovery = dd3.getLeaseRecoveryCommand(1);",
          "80:     assertEquals(blockInfoRecovery[0], blockInfo);",
          "81:   }",
          "",
          "[Removed Lines]",
          "78:     blockInfo.initializeBlockRecovery(3);",
          "",
          "[Added Lines]",
          "78:     blockInfo.initializeBlockRecovery(3, true);",
          "",
          "---------------"
        ],
        "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestBlockUnderConstruction.java||hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestBlockUnderConstruction.java": [
          "File: hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestBlockUnderConstruction.java -> hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestBlockUnderConstruction.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "37: import org.apache.hadoop.hdfs.protocol.LocatedBlocks;",
          "38: import org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoContiguous;",
          "39: import org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoContiguousUnderConstruction;",
          "40: import org.apache.hadoop.hdfs.server.common.HdfsServerConstants.BlockUCState;",
          "41: import org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols;",
          "42: import org.junit.AfterClass;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "40: import org.apache.hadoop.hdfs.server.blockmanagement.BlockManager;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "182:     out.close();",
          "183:   }",
          "184: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "192:   @Test",
          "193:   public void testEmptyExpectedLocations() throws Exception {",
          "194:     final NamenodeProtocols namenode = cluster.getNameNodeRpc();",
          "195:     final FSNamesystem fsn = cluster.getNamesystem();",
          "196:     final BlockManager bm = fsn.getBlockManager();",
          "197:     final Path p = new Path(BASE_DIR, \"file2.dat\");",
          "198:     final String src = p.toString();",
          "199:     final FSDataOutputStream out = TestFileCreation.createFile(hdfs, p, 1);",
          "200:     writeFile(p, out, 256);",
          "201:     out.hflush();",
          "204:     LocatedBlocks lbs = namenode.getBlockLocations(src, 0, 256);",
          "205:     LocatedBlock lastLB = lbs.getLocatedBlocks().get(0);",
          "206:     final Block b = lastLB.getBlock().getLocalBlock();",
          "209:     long blockRecoveryId = fsn.getBlockIdManager().nextGenerationStamp(false);",
          "210:     BlockInfoContiguousUnderConstruction uc = bm.getStoredBlock(b).",
          "211:         convertToBlockUnderConstruction(BlockUCState.UNDER_CONSTRUCTION, null);",
          "212:     uc.initializeBlockRecovery(blockRecoveryId, false);",
          "214:     try {",
          "215:       String[] storages = { \"invalid-storage-id1\" };",
          "216:       fsn.commitBlockSynchronization(lastLB.getBlock(), blockRecoveryId, 256L,",
          "217:           true, false, lastLB.getLocations(), storages);",
          "218:     } catch (java.lang.IllegalStateException ise) {",
          "221:     }",
          "224:     lbs = namenode.getBlockLocations(src, 0, 256);",
          "225:   }",
          "",
          "---------------"
        ],
        "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCommitBlockSynchronization.java||hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCommitBlockSynchronization.java": [
          "File: hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCommitBlockSynchronization.java -> hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCommitBlockSynchronization.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "68:         block, (short) 1, HdfsServerConstants.BlockUCState.UNDER_CONSTRUCTION, targets);",
          "69:     blockInfo.setBlockCollection(file);",
          "70:     blockInfo.setGenerationStamp(genStamp);",
          "72:     doReturn(true).when(file).removeLastBlock(any(Block.class));",
          "73:     doReturn(true).when(file).isUnderConstruction();",
          "74:     doReturn(new BlockInfoContiguous[1]).when(file).getBlocks();",
          "",
          "[Removed Lines]",
          "71:     blockInfo.initializeBlockRecovery(genStamp);",
          "",
          "[Added Lines]",
          "71:     blockInfo.initializeBlockRecovery(genStamp, true);",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "5b57f9cae452039385e0c6d4c371fc01539f37d7",
      "candidate_info": {
        "commit_hash": "5b57f9cae452039385e0c6d4c371fc01539f37d7",
        "repo": "apache/hadoop",
        "commit_url": "https://github.com/apache/hadoop/commit/5b57f9cae452039385e0c6d4c371fc01539f37d7",
        "files": [
          "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyProvider.java",
          "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/CommonConfigurationKeysPublic.java",
          "hadoop-common-project/hadoop-common/src/main/resources/core-default.xml"
        ],
        "message": "HADOOP-15473. Configure serialFilter in KeyProvider to avoid UnrecoverableKeyException caused by JDK-8189997. Contributed by Gabor Bota.\n\n(cherry picked from commit 02322de3f95ba78a22c057037ef61aa3ab1d3824)\n\n Conflicts:\n\thadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyProvider.java\n\n(cherry picked from commit 51eaa0ab5b93af524f44bf2f72b5e09aad1f2e4e)\n\n Conflicts:\n\thadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/CommonConfigurationKeysPublic.java",
        "before_after_code_files": [
          "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyProvider.java||hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyProvider.java",
          "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/CommonConfigurationKeysPublic.java||hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/CommonConfigurationKeysPublic.java"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/bcdev/hadoop/pull/2"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyProvider.java||hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyProvider.java": [
          "File: hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyProvider.java -> hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyProvider.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "40: import javax.crypto.KeyGenerator;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "42: import static org.apache.hadoop.fs.CommonConfigurationKeysPublic.HADOOP_SECURITY_CRYPTO_JCEKS_KEY_SERIALFILTER;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "56:   public static final String DEFAULT_BITLENGTH_NAME =",
          "57:       \"hadoop.security.key.default.bitlength\";",
          "58:   public static final int DEFAULT_BITLENGTH = 128;",
          "60:   private final Configuration conf;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "61:   public static final String JCEKS_KEY_SERIALFILTER_DEFAULT =",
          "62:       \"java.lang.Enum;\"",
          "63:           + \"java.security.KeyRep;\"",
          "64:           + \"java.security.KeyRep$Type;\"",
          "65:           + \"javax.crypto.spec.SecretKeySpec;\"",
          "66:           + \"org.apache.hadoop.crypto.key.JavaKeyStoreProvider$KeyMetadata;\"",
          "67:           + \"!*\";",
          "68:   public static final String JCEKS_KEY_SERIAL_FILTER = \"jceks.key.serialFilter\";",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "365:   public KeyProvider(Configuration conf) {",
          "366:     this.conf = new Configuration(conf);",
          "367:   }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "379:     if(System.getProperty(JCEKS_KEY_SERIAL_FILTER) == null) {",
          "380:       String serialFilter =",
          "381:           conf.get(HADOOP_SECURITY_CRYPTO_JCEKS_KEY_SERIALFILTER,",
          "382:               JCEKS_KEY_SERIALFILTER_DEFAULT);",
          "383:       System.setProperty(JCEKS_KEY_SERIAL_FILTER, serialFilter);",
          "384:     }",
          "",
          "---------------"
        ],
        "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/CommonConfigurationKeysPublic.java||hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/CommonConfigurationKeysPublic.java": [
          "File: hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/CommonConfigurationKeysPublic.java -> hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/CommonConfigurationKeysPublic.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "340:   public static final String HADOOP_SECURITY_CRYPTO_JCE_PROVIDER_KEY =",
          "341:     \"hadoop.security.crypto.jce.provider\";",
          "343:   public static final String HADOOP_SECURITY_CRYPTO_BUFFER_SIZE_KEY =",
          "344:     \"hadoop.security.crypto.buffer.size\";",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "347:   public static final String HADOOP_SECURITY_CRYPTO_JCEKS_KEY_SERIALFILTER =",
          "348:       \"hadoop.security.crypto.jceks.key.serialfilter\";",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "be22e242ac66322958f47dec14db8424f908e7d3",
      "candidate_info": {
        "commit_hash": "be22e242ac66322958f47dec14db8424f908e7d3",
        "repo": "apache/hadoop",
        "commit_url": "https://github.com/apache/hadoop/commit/be22e242ac66322958f47dec14db8424f908e7d3",
        "files": [
          "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/net/NetworkTopology.java"
        ],
        "message": "HADOOP-15486. Make NetworkTopology#netLock fair. Contributed by Nanda kumar.",
        "before_after_code_files": [
          "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/net/NetworkTopology.java||hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/net/NetworkTopology.java"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/bcdev/hadoop/pull/2"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/net/NetworkTopology.java||hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/net/NetworkTopology.java": [
          "File: hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/net/NetworkTopology.java -> hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/net/NetworkTopology.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "389:   private boolean clusterEverBeenMultiRack = false;",
          "394:   public NetworkTopology() {",
          "395:     clusterMap = new InnerNode(InnerNode.ROOT);",
          "",
          "[Removed Lines]",
          "392:   protected ReadWriteLock netlock = new ReentrantReadWriteLock();",
          "",
          "[Added Lines]",
          "392:   protected ReadWriteLock netlock = new ReentrantReadWriteLock(true);",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "7be1bc52bfb95768dd97374ccbe0769c1a732b6b",
      "candidate_info": {
        "commit_hash": "7be1bc52bfb95768dd97374ccbe0769c1a732b6b",
        "repo": "apache/hadoop",
        "commit_url": "https://github.com/apache/hadoop/commit/7be1bc52bfb95768dd97374ccbe0769c1a732b6b",
        "files": [
          "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl/main.c"
        ],
        "message": "Addendum fix in container executor. Contributed by Wilfred Spiegelenburg.",
        "before_after_code_files": [
          "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl/main.c||hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl/main.c"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/bcdev/hadoop/pull/2"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl/main.c||hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl/main.c": [
          "File: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl/main.c -> hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl/main.c",
          "--- Hunk 1 ---",
          "[Context before]",
          "129:   }",
          "131:   char *orig_conf_file = HADOOP_CONF_DIR \"/\" CONF_FILENAME;",
          "133:   char *local_dirs, *log_dirs;",
          "134:   char *resources, *resources_key, *resources_value;",
          "",
          "[Removed Lines]",
          "132:   char *conf_file = resolve_config_path(orig_conf_file, argv[0]);",
          "",
          "[Added Lines]",
          "132:   char *conf_file = resolve_config_path(orig_conf_file, executable_file);",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "20c6b448ac34841c24cdb2ca5e7d7414148ceedd",
      "candidate_info": {
        "commit_hash": "20c6b448ac34841c24cdb2ca5e7d7414148ceedd",
        "repo": "apache/hadoop",
        "commit_url": "https://github.com/apache/hadoop/commit/20c6b448ac34841c24cdb2ca5e7d7414148ceedd",
        "files": [
          "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFSImage.java"
        ],
        "message": "HDFS-12156. TestFSImage fails without -Pnative\n\n(cherry picked from commit 319defafc105c0d0b69b83828b578d9c453036f5)\n(cherry picked from commit b0c80f1c814827abf95316dc9c23701f8ef09d9a)\n(cherry picked from commit 9699bc2e11663469ed1425fb42d09dba3fbeb0d6)\n\nConflicts:\n\thadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFSImage.java",
        "before_after_code_files": [
          "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFSImage.java||hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFSImage.java"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/bcdev/hadoop/pull/2"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFSImage.java||hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFSImage.java": [
          "File: hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFSImage.java -> hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFSImage.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "25: import java.util.EnumSet;",
          "27: import org.junit.Assert;",
          "29: import org.apache.hadoop.conf.Configuration;",
          "30: import org.apache.hadoop.fs.FSDataOutputStream;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "28: import org.junit.Assume;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "46: import org.apache.hadoop.hdfs.util.MD5FileUtils;",
          "47: import org.apache.hadoop.test.GenericTestUtils;",
          "48: import org.apache.hadoop.test.PathUtils;",
          "49: import org.junit.Test;",
          "51: public class TestFSImage {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "50: import org.apache.hadoop.util.NativeCodeLoader;",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "65:     setCompressCodec(conf, \"org.apache.hadoop.io.compress.DefaultCodec\");",
          "66:     setCompressCodec(conf, \"org.apache.hadoop.io.compress.GzipCodec\");",
          "67:     setCompressCodec(conf, \"org.apache.hadoop.io.compress.BZip2Codec\");",
          "68:     setCompressCodec(conf, \"org.apache.hadoop.io.compress.Lz4Codec\");",
          "69:   }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "70:   }",
          "72:   @Test",
          "73:   public void testNativeCompression() throws IOException {",
          "74:     Assume.assumeTrue(NativeCodeLoader.isNativeCodeLoaded());",
          "75:     Configuration conf = new Configuration();",
          "76:     conf.setBoolean(DFSConfigKeys.DFS_IMAGE_COMPRESS_KEY, true);",
          "",
          "---------------"
        ]
      }
    }
  ]
}