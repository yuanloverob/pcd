{
  "cve_id": "CVE-2023-50943",
  "cve_desc": "Apache Airflow, versions before 2.8.1, have a vulnerability that allows a potential attacker to poison the XCom data by bypassing the protection of \"enable_xcom_pickling=False\" configuration setting resulting in poisoned data after XCom deserialization. This vulnerability is considered low since it requires a DAG author to exploit it. Users are recommended to upgrade to version 2.8.1 or later, which fixes this issue.",
  "repo": "apache/airflow",
  "patch_hash": "2c4c5bc604e9ab0cc1e98f7bee7d31d566579462",
  "patch_info": {
    "commit_hash": "2c4c5bc604e9ab0cc1e98f7bee7d31d566579462",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/2c4c5bc604e9ab0cc1e98f7bee7d31d566579462",
    "files": [
      "airflow/models/xcom.py",
      "tests/api_connexion/schemas/test_xcom_schema.py",
      "tests/models/test_xcom.py"
    ],
    "message": "Stop deserializing pickle when enable_xcom_pickling is False (#36255)\n\n* Stop deserializing pickle when enable_xcom_pickling is False\n\n* Fix unit tests\n\n(cherry picked from commit 63e97abec5d56bc62a293c93f5227f364561e51c)",
    "before_after_code_files": [
      "airflow/models/xcom.py||airflow/models/xcom.py",
      "tests/api_connexion/schemas/test_xcom_schema.py||tests/api_connexion/schemas/test_xcom_schema.py",
      "tests/models/test_xcom.py||tests/models/test_xcom.py"
    ]
  },
  "patch_diff": {
    "airflow/models/xcom.py||airflow/models/xcom.py": [
      "File: airflow/models/xcom.py -> airflow/models/xcom.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "685:             except pickle.UnpicklingError:",
      "686:                 return json.loads(result.value.decode(\"UTF-8\"), cls=XComDecoder, object_hook=object_hook)",
      "687:         else:",
      "693:     @staticmethod",
      "694:     def deserialize_value(result: XCom) -> Any:",
      "",
      "[Removed Lines]",
      "688:             try:",
      "689:                 return json.loads(result.value.decode(\"UTF-8\"), cls=XComDecoder, object_hook=object_hook)",
      "690:             except (json.JSONDecodeError, UnicodeDecodeError):",
      "691:                 return pickle.loads(result.value)",
      "",
      "[Added Lines]",
      "688:             # Since xcom_pickling is disabled, we should only try to deserialize with JSON",
      "689:             return json.loads(result.value.decode(\"UTF-8\"), cls=XComDecoder, object_hook=object_hook)",
      "",
      "---------------"
    ],
    "tests/api_connexion/schemas/test_xcom_schema.py||tests/api_connexion/schemas/test_xcom_schema.py": [
      "File: tests/api_connexion/schemas/test_xcom_schema.py -> tests/api_connexion/schemas/test_xcom_schema.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "30: from airflow.models import DagRun, XCom",
      "31: from airflow.utils.dates import parse_execution_date",
      "32: from airflow.utils.session import create_session",
      "34: pytestmark = pytest.mark.db_test",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "33: from tests.test_utils.config import conf_vars",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "188:     default_time = \"2016-04-02T21:00:00+00:00\"",
      "189:     default_time_parsed = parse_execution_date(default_time)",
      "191:     def test_serialize(self, create_xcom, session):",
      "192:         create_xcom(",
      "193:             dag_id=\"test_dag\",",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "192:     @conf_vars({(\"core\", \"enable_xcom_pickling\"): \"True\"})",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "208:             \"map_index\": -1,",
      "209:         }",
      "211:     def test_deserialize(self):",
      "212:         xcom_dump = {",
      "213:             \"key\": \"test_key\",",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "213:     @conf_vars({(\"core\", \"enable_xcom_pickling\"): \"True\"})",
      "",
      "---------------"
    ],
    "tests/models/test_xcom.py||tests/models/test_xcom.py": [
      "File: tests/models/test_xcom.py -> tests/models/test_xcom.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "140:             ret_value = XCom.get_value(key=\"xcom_test3\", ti_key=ti_key, session=session)",
      "141:         assert ret_value == {\"key\": \"value\"}",
      "144:         with conf_vars({(\"core\", \"enable_xcom_pickling\"): \"True\"}):",
      "145:             XCom.set(",
      "146:                 key=\"xcom_test3\",",
      "",
      "[Removed Lines]",
      "143:     def test_xcom_deserialize_with_pickle_to_json_switch(self, task_instance, session):",
      "",
      "[Added Lines]",
      "143:     def test_xcom_deserialize_pickle_when_xcom_pickling_is_disabled(self, task_instance, session):",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "151:                 session=session,",
      "152:             )",
      "153:         with conf_vars({(\"core\", \"enable_xcom_pickling\"): \"False\"}):",
      "163:     @conf_vars({(\"core\", \"xcom_enable_pickling\"): \"False\"})",
      "164:     def test_xcom_disable_pickle_type_fail_on_non_json(self, task_instance, session):",
      "",
      "[Removed Lines]",
      "154:             ret_value = XCom.get_one(",
      "155:                 key=\"xcom_test3\",",
      "156:                 dag_id=task_instance.dag_id,",
      "157:                 task_id=task_instance.task_id,",
      "158:                 run_id=task_instance.run_id,",
      "159:                 session=session,",
      "160:             )",
      "161:         assert ret_value == {\"key\": \"value\"}",
      "",
      "[Added Lines]",
      "154:             with pytest.raises(UnicodeDecodeError):",
      "155:                 XCom.get_one(",
      "156:                     key=\"xcom_test3\",",
      "157:                     dag_id=task_instance.dag_id,",
      "158:                     task_id=task_instance.task_id,",
      "159:                     run_id=task_instance.run_id,",
      "160:                     session=session,",
      "161:                 )",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "e49c573d9ce03066c61cce0624576700de1de07a",
      "candidate_info": {
        "commit_hash": "e49c573d9ce03066c61cce0624576700de1de07a",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/e49c573d9ce03066c61cce0624576700de1de07a",
        "files": [
          "dev/README_RELEASE_PROVIDER_PACKAGES.md",
          "dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "dev/breeze/src/airflow_breeze/provider_issue_TEMPLATE.md.jinja2"
        ],
        "message": "Update release process for providers to include mixed RC versions (#36385)\n\nThis PR updates released process for providers to enable releasing\nproviders in more regular batches. Sometimes when we exclude a\nprovider from previous voting, we want to release RCN (2,3 etc.)\ncandidate.\n\nHowever, especially when time between previous RC and the new one\nis long (for example because fixing took a long time) we might\nwant to release the RCN release for that cancelled providers and\nRC1 for all the providers that have been changed in the meantime.\n\nThis cchange makes it possible (and easy):\n\n1) release RC1 for all providers (the RCN provider should be skipped,\n   because tag for this provider already exists.\n\n2) release the RCN providers with `--version-suffix-for-pypi rcN`.\n\nThe release process and tools were updated to account for that - where\nrc candidate number is retrieved from packages prepared in `dist`.\n\nFixed a few small missing things in the process.\n\n(cherry picked from commit 4deed641322343cb18eabd9ad199f5ac83f29ff0)",
        "before_after_code_files": [
          "dev/breeze/src/airflow_breeze/commands/release_management_commands.py||dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "dev/breeze/src/airflow_breeze/provider_issue_TEMPLATE.md.jinja2||dev/breeze/src/airflow_breeze/provider_issue_TEMPLATE.md.jinja2"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/breeze/src/airflow_breeze/commands/release_management_commands.py||dev/breeze/src/airflow_breeze/commands/release_management_commands.py": [
          "File: dev/breeze/src/airflow_breeze/commands/release_management_commands.py -> dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1495:     )",
          "1498: def get_prs_for_package(provider_id: str) -> list[int]:",
          "1499:     pr_matcher = re.compile(r\".*\\(#([0-9]*)\\)``$\")",
          "1500:     prs = []",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1498: VERSION_MATCH = re.compile(r\"([0-9]+)\\.([0-9]+)\\.([0-9]+)(.*)\")",
          "1501: def get_suffix_from_package_in_dist(dist_files: list[str], package: str) -> str | None:",
          "1502:     \"\"\"Get suffix from package prepared in dist folder.\"\"\"",
          "1503:     for file in dist_files:",
          "1504:         if file.startswith(f'apache_airflow_providers_{package.replace(\".\", \"_\")}') and file.endswith(",
          "1505:             \".tar.gz\"",
          "1506:         ):",
          "1507:             file = file[: -len(\".tar.gz\")]",
          "1508:             version = file.split(\"-\")[-1]",
          "1509:             match = VERSION_MATCH.match(version)",
          "1510:             if match:",
          "1511:                 return match.group(4)",
          "1512:     return None",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1566:         pypi_package_name: str",
          "1567:         version: str",
          "1568:         pr_list: list[PullRequest.PullRequest | Issue.Issue]",
          "1570:     if not provider_packages:",
          "1571:         provider_packages = list(DEPENDENCIES.keys())",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1586:         suffix: str",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "1622:                 ).read_text()",
          "1623:             )",
          "1624:             if pull_request_list:",
          "1625:                 providers[provider_id] = ProviderPRInfo(",
          "1626:                     version=provider_yaml_dict[\"versions\"][0],",
          "1627:                     provider_package_id=provider_id,",
          "1628:                     pypi_package_name=provider_yaml_dict[\"package-name\"],",
          "1629:                     pr_list=pull_request_list,",
          "1630:                 )",
          "1631:         template = jinja2.Template(",
          "1632:             (Path(__file__).parents[1] / \"provider_issue_TEMPLATE.md.jinja2\").read_text()",
          "1633:         )",
          "1635:         get_console().print()",
          "1636:         get_console().print(",
          "1637:             \"[green]Below you can find the issue content that you can use \"",
          "",
          "[Removed Lines]",
          "1634:         issue_content = template.render(providers=providers, date=datetime.now(), suffix=suffix)",
          "",
          "[Added Lines]",
          "1643:                 package_suffix = get_suffix_from_package_in_dist(files_in_dist, provider_id)",
          "1649:                     suffix=package_suffix if package_suffix else suffix,",
          "1654:         issue_content = template.render(providers=providers, date=datetime.now())",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/provider_issue_TEMPLATE.md.jinja2||dev/breeze/src/airflow_breeze/provider_issue_TEMPLATE.md.jinja2": [
          "File: dev/breeze/src/airflow_breeze/provider_issue_TEMPLATE.md.jinja2 -> dev/breeze/src/airflow_breeze/provider_issue_TEMPLATE.md.jinja2",
          "--- Hunk 1 ---",
          "[Context before]",
          "10: Those are providers that require testing as there were some substantial changes introduced:",
          "12: {% for provider_id, provider_info in providers.items()  %}",
          "14: {%- for pr in provider_info.pr_list %}",
          "15:    - [ ] [{{ pr.title }} (#{{ pr.number }})]({{ pr.html_url }}): @{{ pr.user.login }}",
          "16: {%- endfor %}",
          "",
          "[Removed Lines]",
          "13: ## Provider [{{ provider_id }}: {{ provider_info.version }}{{ suffix }}](https://pypi.org/project/{{ provider_info.pypi_package_name }}/{{ provider_info.version }}{{ suffix }})",
          "",
          "[Added Lines]",
          "13: ## Provider [{{ provider_id }}: {{ provider_info.version }}{{ provider_info.suffix }}](https://pypi.org/project/{{ provider_info.pypi_package_name }}/{{ provider_info.version }}{{ provider_info.suffix }})",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "c284ece1454bc64afdb2686464e91e42a4519b55",
      "candidate_info": {
        "commit_hash": "c284ece1454bc64afdb2686464e91e42a4519b55",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/c284ece1454bc64afdb2686464e91e42a4519b55",
        "files": [
          "dev/breeze/src/airflow_breeze/utils/md5_build_check.py"
        ],
        "message": "Less verbose information about changed provider.yaml files (#36307)\n\nWhen we attempt to see if provider.yaml files make changes in\ndependencies, we print verbose information on what provider.yaml\nfiles changeed, but this is not necessary or needed. This change\nmakes the output less verbose by detail - just a number of changed\nfiles rather than full list of them - the full list is only printed\nwhen `--verbose` flag is used.\n\n(cherry picked from commit 7212301b2200cb968cd38cdaddb30d7ed7360bda)",
        "before_after_code_files": [
          "dev/breeze/src/airflow_breeze/utils/md5_build_check.py||dev/breeze/src/airflow_breeze/utils/md5_build_check.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/breeze/src/airflow_breeze/utils/md5_build_check.py||dev/breeze/src/airflow_breeze/utils/md5_build_check.py": [
          "File: dev/breeze/src/airflow_breeze/utils/md5_build_check.py -> dev/breeze/src/airflow_breeze/utils/md5_build_check.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "29: from airflow_breeze.utils.console import get_console",
          "30: from airflow_breeze.utils.path_utils import AIRFLOW_SOURCES_ROOT",
          "31: from airflow_breeze.utils.run_utils import run_command",
          "33: if TYPE_CHECKING:",
          "34:     from airflow_breeze.params.build_ci_params import BuildCiParams",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "32: from airflow_breeze.utils.shared_options import get_verbose",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "102:         if modified_provider_yaml_files:",
          "103:             get_console().print(",
          "104:                 \"[info]Attempting to generate provider dependencies. \"",
          "109:             )",
          "110:             # Regenerate provider_dependencies.json",
          "111:             run_command(",
          "112:                 [",
          "",
          "[Removed Lines]",
          "105:                 \"Provider yaml files changed since last check:[/]\"",
          "106:             )",
          "107:             get_console().print(",
          "108:                 [os.fspath(file.relative_to(AIRFLOW_SOURCES_ROOT)) for file in modified_provider_yaml_files]",
          "",
          "[Added Lines]",
          "106:                 f\"{len(modified_provider_yaml_files)} provider.yaml file(s) changed since last check.\"",
          "108:             if get_verbose():",
          "109:                 get_console().print(",
          "110:                     [",
          "111:                         os.fspath(file.relative_to(AIRFLOW_SOURCES_ROOT))",
          "112:                         for file in modified_provider_yaml_files",
          "113:                     ]",
          "114:                 )",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "8317580ccf671d344bc45cea151f641800826a23",
      "candidate_info": {
        "commit_hash": "8317580ccf671d344bc45cea151f641800826a23",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/8317580ccf671d344bc45cea151f641800826a23",
        "files": [
          "airflow/operators/python.py"
        ],
        "message": "Fix PythonVirtualenvOperator tests (#36367)\n\n(cherry picked from commit af4b51cdfc8f98bec9922facd165ea8a6440c12b)",
        "before_after_code_files": [
          "airflow/operators/python.py||airflow/operators/python.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/operators/python.py||airflow/operators/python.py": [
          "File: airflow/operators/python.py -> airflow/operators/python.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "472:                 else:",
          "473:                     raise",
          "475:             return self._read_result(output_path)",
          "477:     def determine_kwargs(self, context: Mapping[str, Any]) -> Mapping[str, Any]:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "475:             if 0 in self.skip_on_exit_code:",
          "476:                 raise AirflowSkipException(\"Process exited with code 0. Skipping.\")",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "dbc4b1fd9b6f53d6c8bc9abbde591e7df3c90536",
      "candidate_info": {
        "commit_hash": "dbc4b1fd9b6f53d6c8bc9abbde591e7df3c90536",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/dbc4b1fd9b6f53d6c8bc9abbde591e7df3c90536",
        "files": [
          ".github/workflows/ci.yml",
          "Dockerfile.ci",
          "airflow/models/dag.py",
          "airflow/providers/cncf/kubernetes/pod_launcher_deprecated.py",
          "airflow/serialization/serialized_objects.py",
          "airflow/serialization/serializers/datetime.py",
          "airflow/serialization/serializers/timezone.py",
          "airflow/settings.py",
          "airflow/timetables/_cron.py",
          "airflow/timetables/trigger.py",
          "airflow/utils/sqlalchemy.py",
          "airflow/utils/timezone.py",
          "dev/breeze/src/airflow_breeze/commands/common_options.py",
          "dev/breeze/src/airflow_breeze/commands/developer_commands.py",
          "dev/breeze/src/airflow_breeze/commands/developer_commands_config.py",
          "dev/breeze/src/airflow_breeze/commands/testing_commands.py",
          "dev/breeze/src/airflow_breeze/commands/testing_commands_config.py",
          "dev/breeze/src/airflow_breeze/params/shell_params.py",
          "images/breeze/output_shell.svg",
          "images/breeze/output_shell.txt",
          "images/breeze/output_testing_db-tests.svg",
          "images/breeze/output_testing_db-tests.txt",
          "images/breeze/output_testing_non-db-tests.svg",
          "images/breeze/output_testing_non-db-tests.txt",
          "images/breeze/output_testing_tests.svg",
          "images/breeze/output_testing_tests.txt",
          "kubernetes_tests/test_kubernetes_pod_operator.py",
          "newsfragments/36281.significant.rst",
          "pyproject.toml",
          "scripts/ci/docker-compose/devcontainer.env",
          "scripts/docker/entrypoint_ci.sh",
          "tests/api_connexion/endpoints/test_dag_endpoint.py",
          "tests/api_connexion/schemas/test_dag_schema.py",
          "tests/cli/commands/test_dag_command.py",
          "tests/models/test_dag.py",
          "tests/providers/openlineage/plugins/test_utils.py",
          "tests/sensors/test_time_sensor.py",
          "tests/serialization/serializers/test_serializers.py",
          "tests/serialization/test_serialized_objects.py",
          "tests/triggers/test_temporal.py",
          "tests/utils/test_timezone.py"
        ],
        "message": "Add support of Pendulum 3 (#36281)\n\n* Add support of Pendulum 3\n\n* Add backcompat to pendulum 2\n\n* Update airflow/serialization/serialized_objects.py\n\nCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>\n\n* Add newsfragments\n\n---------\n\nCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>\n(cherry picked from commit 2ffa6e4c4c9dc129daa54491d5af8f535cd0d479)",
        "before_after_code_files": [
          "Dockerfile.ci||Dockerfile.ci",
          "airflow/models/dag.py||airflow/models/dag.py",
          "airflow/providers/cncf/kubernetes/pod_launcher_deprecated.py||airflow/providers/cncf/kubernetes/pod_launcher_deprecated.py",
          "airflow/serialization/serialized_objects.py||airflow/serialization/serialized_objects.py",
          "airflow/serialization/serializers/datetime.py||airflow/serialization/serializers/datetime.py",
          "airflow/serialization/serializers/timezone.py||airflow/serialization/serializers/timezone.py",
          "airflow/settings.py||airflow/settings.py",
          "airflow/timetables/_cron.py||airflow/timetables/_cron.py",
          "airflow/timetables/trigger.py||airflow/timetables/trigger.py",
          "airflow/utils/sqlalchemy.py||airflow/utils/sqlalchemy.py",
          "airflow/utils/timezone.py||airflow/utils/timezone.py",
          "dev/breeze/src/airflow_breeze/commands/common_options.py||dev/breeze/src/airflow_breeze/commands/common_options.py",
          "dev/breeze/src/airflow_breeze/commands/developer_commands.py||dev/breeze/src/airflow_breeze/commands/developer_commands.py",
          "dev/breeze/src/airflow_breeze/commands/developer_commands_config.py||dev/breeze/src/airflow_breeze/commands/developer_commands_config.py",
          "dev/breeze/src/airflow_breeze/commands/testing_commands.py||dev/breeze/src/airflow_breeze/commands/testing_commands.py",
          "dev/breeze/src/airflow_breeze/commands/testing_commands_config.py||dev/breeze/src/airflow_breeze/commands/testing_commands_config.py",
          "dev/breeze/src/airflow_breeze/params/shell_params.py||dev/breeze/src/airflow_breeze/params/shell_params.py",
          "kubernetes_tests/test_kubernetes_pod_operator.py||kubernetes_tests/test_kubernetes_pod_operator.py",
          "scripts/ci/docker-compose/devcontainer.env||scripts/ci/docker-compose/devcontainer.env",
          "scripts/docker/entrypoint_ci.sh||scripts/docker/entrypoint_ci.sh",
          "tests/api_connexion/endpoints/test_dag_endpoint.py||tests/api_connexion/endpoints/test_dag_endpoint.py",
          "tests/api_connexion/schemas/test_dag_schema.py||tests/api_connexion/schemas/test_dag_schema.py",
          "tests/cli/commands/test_dag_command.py||tests/cli/commands/test_dag_command.py",
          "tests/models/test_dag.py||tests/models/test_dag.py",
          "tests/providers/openlineage/plugins/test_utils.py||tests/providers/openlineage/plugins/test_utils.py",
          "tests/sensors/test_time_sensor.py||tests/sensors/test_time_sensor.py",
          "tests/serialization/serializers/test_serializers.py||tests/serialization/serializers/test_serializers.py",
          "tests/serialization/test_serialized_objects.py||tests/serialization/test_serialized_objects.py",
          "tests/triggers/test_temporal.py||tests/triggers/test_temporal.py",
          "tests/utils/test_timezone.py||tests/utils/test_timezone.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "Dockerfile.ci||Dockerfile.ci": [
          "File: Dockerfile.ci -> Dockerfile.ci",
          "--- Hunk 1 ---",
          "[Context before]",
          "908:     pip check",
          "909: }",
          "911: function check_run_tests() {",
          "912:     if [[ ${RUN_TESTS=} != \"true\" ]]; then",
          "913:         return",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "911: function check_download_pendulum() {",
          "912:     if [[ ${DOWNGRADE_PENDULUM=} != \"true\" ]]; then",
          "913:         return",
          "914:     fi",
          "915:     min_pendulum_version=$(grep \"\\\"pendulum>=\" pyproject.toml | sed \"s/.*>=\\([0-9\\.]*\\).*/\\1/\" | xargs)",
          "916:     echo",
          "917:     echo \"${COLOR_BLUE}Downgrading pendulum to minimum supported version: ${min_pendulum_version}${COLOR_RESET}\"",
          "918:     echo",
          "919:     pip install --root-user-action ignore \"pendulum==${min_pendulum_version}\"",
          "920:     pip check",
          "921: }",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "937: environment_initialization",
          "938: check_boto_upgrade",
          "939: check_download_sqlalchemy",
          "940: check_run_tests \"${@}\"",
          "942: exec /bin/bash \"${@}\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "952: check_download_pendulum",
          "",
          "---------------"
        ],
        "airflow/models/dag.py||airflow/models/dag.py": [
          "File: airflow/models/dag.py -> airflow/models/dag.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "138: if TYPE_CHECKING:",
          "139:     from types import ModuleType",
          "142:     from sqlalchemy.orm.query import Query",
          "143:     from sqlalchemy.orm.session import Session",
          "",
          "[Removed Lines]",
          "141:     from pendulum.tz.timezone import Timezone",
          "",
          "[Added Lines]",
          "141:     from pendulum.tz.timezone import FixedTimezone, Timezone",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "213:     return DataInterval(start, end)",
          "217:     \"\"\"Create a Timetable instance from a ``schedule_interval`` argument.\"\"\"",
          "218:     if interval is NOTSET:",
          "219:         return DeltaDataIntervalTimetable(DEFAULT_SCHEDULE_INTERVAL)",
          "",
          "[Removed Lines]",
          "216: def create_timetable(interval: ScheduleIntervalArg, timezone: Timezone) -> Timetable:",
          "",
          "[Added Lines]",
          "216: def create_timetable(interval: ScheduleIntervalArg, timezone: Timezone | FixedTimezone) -> Timetable:",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "530:             tzinfo = None if date.tzinfo else settings.TIMEZONE",
          "531:             tz = pendulum.instance(date, tz=tzinfo).timezone",
          "534:         # Apply the timezone we settled on to end_date if it wasn't supplied",
          "535:         if \"end_date\" in self.default_args and self.default_args[\"end_date\"]:",
          "",
          "[Removed Lines]",
          "532:         self.timezone: Timezone = tz or settings.TIMEZONE",
          "",
          "[Added Lines]",
          "532:         self.timezone: Timezone | FixedTimezone = tz or settings.TIMEZONE",
          "",
          "---------------"
        ],
        "airflow/providers/cncf/kubernetes/pod_launcher_deprecated.py||airflow/providers/cncf/kubernetes/pod_launcher_deprecated.py": [
          "File: airflow/providers/cncf/kubernetes/pod_launcher_deprecated.py -> airflow/providers/cncf/kubernetes/pod_launcher_deprecated.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "21: import math",
          "22: import time",
          "23: import warnings",
          "26: import pendulum",
          "27: import tenacity",
          "",
          "[Removed Lines]",
          "24: from typing import TYPE_CHECKING",
          "",
          "[Added Lines]",
          "24: from typing import TYPE_CHECKING, cast",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "148:         \"\"\"",
          "149:         if get_logs:",
          "150:             read_logs_since_sec = None",
          "152:             while True:",
          "153:                 logs = self.read_pod_logs(pod, timestamps=True, since_seconds=read_logs_since_sec)",
          "154:                 for line in logs:",
          "155:                     timestamp, message = self.parse_log_line(line.decode(\"utf-8\"))",
          "156:                     if timestamp:",
          "158:                     self.log.info(message)",
          "159:                 time.sleep(1)",
          "",
          "[Removed Lines]",
          "151:             last_log_time = None",
          "157:                         last_log_time = pendulum.parse(timestamp)",
          "",
          "[Added Lines]",
          "151:             last_log_time: pendulum.DateTime | None = None",
          "157:                         last_log_time = cast(pendulum.DateTime, pendulum.parse(timestamp))",
          "",
          "---------------"
        ],
        "airflow/serialization/serialized_objects.py||airflow/serialization/serialized_objects.py": [
          "File: airflow/serialization/serialized_objects.py -> airflow/serialization/serialized_objects.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "65: from airflow.utils.module_loading import import_string, qualname",
          "66: from airflow.utils.operator_resources import Resources",
          "67: from airflow.utils.task_group import MappedTaskGroup, TaskGroup",
          "68: from airflow.utils.types import NOTSET, ArgNotSet",
          "70: if TYPE_CHECKING:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "68: from airflow.utils.timezone import parse_timezone",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "144:     return relativedelta.relativedelta(**var)",
          "148:     \"\"\"",
          "149:     Encode a Pendulum Timezone for serialization.",
          "",
          "[Removed Lines]",
          "147: def encode_timezone(var: Timezone) -> str | int:",
          "",
          "[Added Lines]",
          "148: def encode_timezone(var: Timezone | FixedTimezone) -> str | int:",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "167:     )",
          "171:     \"\"\"Decode a previously serialized Pendulum Timezone.\"\"\"",
          "175: def _get_registered_timetable(importable_string: str) -> type[Timetable] | None:",
          "",
          "[Removed Lines]",
          "170: def decode_timezone(var: str | int) -> Timezone:",
          "172:     return pendulum.tz.timezone(var)",
          "",
          "[Added Lines]",
          "171: def decode_timezone(var: str | int) -> Timezone | FixedTimezone:",
          "173:     return parse_timezone(var)",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "607:             raise TypeError(f\"Invalid type {type_!s} in deserialization.\")",
          "609:     _deserialize_datetime = pendulum.from_timestamp",
          "612:     @classmethod",
          "613:     def _deserialize_timedelta(cls, seconds: int) -> datetime.timedelta:",
          "",
          "[Removed Lines]",
          "610:     _deserialize_timezone = pendulum.tz.timezone",
          "",
          "[Added Lines]",
          "611:     _deserialize_timezone = parse_timezone",
          "",
          "---------------"
        ],
        "airflow/serialization/serializers/datetime.py||airflow/serialization/serializers/datetime.py": [
          "File: airflow/serialization/serializers/datetime.py -> airflow/serialization/serializers/datetime.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "24:     serialize as serialize_timezone,",
          "25: )",
          "26: from airflow.utils.module_loading import qualname",
          "28: if TYPE_CHECKING:",
          "29:     import datetime",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "27: from airflow.utils.timezone import parse_timezone",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "62:     import datetime",
          "64:     from pendulum import DateTime",
          "67:     tz: datetime.tzinfo | None = None",
          "68:     if isinstance(data, dict) and TIMEZONE in data:",
          "69:         if version == 1:",
          "70:             # try to deserialize unsupported timezones",
          "71:             timezone_mapping = {",
          "77:             }",
          "78:             if data[TIMEZONE] in timezone_mapping:",
          "79:                 tz = timezone_mapping[data[TIMEZONE]]",
          "80:             else:",
          "82:         else:",
          "83:             tz = (",
          "84:                 deserialize_timezone(data[TIMEZONE][1], data[TIMEZONE][2], data[TIMEZONE][0])",
          "",
          "[Removed Lines]",
          "65:     from pendulum.tz import fixed_timezone, timezone",
          "72:                 \"EDT\": fixed_timezone(-4 * 3600),",
          "73:                 \"CDT\": fixed_timezone(-5 * 3600),",
          "74:                 \"MDT\": fixed_timezone(-6 * 3600),",
          "75:                 \"PDT\": fixed_timezone(-7 * 3600),",
          "76:                 \"CEST\": timezone(\"CET\"),",
          "81:                 tz = timezone(data[TIMEZONE])",
          "",
          "[Added Lines]",
          "72:                 \"EDT\": parse_timezone(-4 * 3600),",
          "73:                 \"CDT\": parse_timezone(-5 * 3600),",
          "74:                 \"MDT\": parse_timezone(-6 * 3600),",
          "75:                 \"PDT\": parse_timezone(-7 * 3600),",
          "76:                 \"CEST\": parse_timezone(\"CET\"),",
          "81:                 tz = parse_timezone(data[TIMEZONE])",
          "",
          "---------------"
        ],
        "airflow/serialization/serializers/timezone.py||airflow/serialization/serializers/timezone.py": [
          "File: airflow/serialization/serializers/timezone.py -> airflow/serialization/serializers/timezone.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "76: def deserialize(classname: str, version: int, data: object) -> Any:",
          "79:     if not isinstance(data, (str, int)):",
          "80:         raise TypeError(f\"{data} is not of type int or str but of {type(data)}\")",
          "",
          "[Removed Lines]",
          "77:     from pendulum.tz import fixed_timezone, timezone",
          "",
          "[Added Lines]",
          "77:     from airflow.utils.timezone import parse_timezone",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "82:     if version > __version__:",
          "83:         raise TypeError(f\"serialized {version} of {classname} > {__version__}\")",
          "88:     if \"zoneinfo.ZoneInfo\" in classname:",
          "89:         try:",
          "90:             from zoneinfo import ZoneInfo",
          "",
          "[Removed Lines]",
          "85:     if isinstance(data, int):",
          "86:         return fixed_timezone(data)",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "94:         return ZoneInfo(data)",
          "99: # ported from pendulum.tz.timezone._get_tzinfo_name",
          "",
          "[Removed Lines]",
          "96:     return timezone(data)",
          "",
          "[Added Lines]",
          "93:     return parse_timezone(data)",
          "",
          "---------------"
        ],
        "airflow/settings.py||airflow/settings.py": [
          "File: airflow/settings.py -> airflow/settings.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "26: import warnings",
          "27: from typing import TYPE_CHECKING, Any, Callable",
          "30: import pluggy",
          "31: import sqlalchemy",
          "32: from sqlalchemy import create_engine, exc, text",
          "",
          "[Removed Lines]",
          "29: import pendulum",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "40: from airflow.logging_config import configure_logging",
          "41: from airflow.utils.orm_event_handlers import setup_event_handlers",
          "42: from airflow.utils.state import State",
          "44: if TYPE_CHECKING:",
          "45:     from sqlalchemy.engine import Engine",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "42: from airflow.utils.timezone import local_timezone, parse_timezone, utc",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "50: log = logging.getLogger(__name__)",
          "52: try:",
          "56:     else:",
          "58: except Exception:",
          "61: log.info(\"Configured default timezone %s\", TIMEZONE)",
          "",
          "[Removed Lines]",
          "53:     tz = conf.get_mandatory_value(\"core\", \"default_timezone\")",
          "54:     if tz == \"system\":",
          "55:         TIMEZONE = pendulum.tz.local_timezone()",
          "57:         TIMEZONE = pendulum.tz.timezone(tz)",
          "59:     TIMEZONE = pendulum.tz.timezone(\"UTC\")",
          "",
          "[Added Lines]",
          "53:     if (tz := conf.get_mandatory_value(\"core\", \"default_timezone\")) != \"system\":",
          "54:         TIMEZONE = parse_timezone(tz)",
          "56:         TIMEZONE = local_timezone()",
          "58:     TIMEZONE = utc",
          "",
          "---------------"
        ],
        "airflow/timetables/_cron.py||airflow/timetables/_cron.py": [
          "File: airflow/timetables/_cron.py -> airflow/timetables/_cron.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "19: import datetime",
          "20: from typing import TYPE_CHECKING, Any",
          "23: from cron_descriptor import CasingTypeEnum, ExpressionDescriptor, FormatException, MissingFieldException",
          "24: from croniter import CroniterBadCronError, CroniterBadDateError, croniter",
          "26: from airflow.exceptions import AirflowTimetableInvalid",
          "27: from airflow.utils.dates import cron_presets",
          "30: if TYPE_CHECKING:",
          "31:     from pendulum import DateTime",
          "35: def _covers_every_hour(cron: croniter) -> bool:",
          "",
          "[Removed Lines]",
          "22: import pendulum",
          "28: from airflow.utils.timezone import convert_to_utc, make_aware, make_naive",
          "32:     from pendulum.tz.timezone import Timezone",
          "",
          "[Added Lines]",
          "27: from airflow.utils.timezone import convert_to_utc, make_aware, make_naive, parse_timezone",
          "31:     from pendulum.tz.timezone import FixedTimezone, Timezone",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "63: class CronMixin:",
          "64:     \"\"\"Mixin to provide interface to work with croniter.\"\"\"",
          "67:         self._expression = cron_presets.get(cron, cron)",
          "69:         if isinstance(timezone, str):",
          "71:         self._timezone = timezone",
          "73:         try:",
          "",
          "[Removed Lines]",
          "66:     def __init__(self, cron: str, timezone: str | Timezone) -> None:",
          "70:             timezone = pendulum.tz.timezone(timezone)",
          "",
          "[Added Lines]",
          "65:     def __init__(self, cron: str, timezone: str | Timezone | FixedTimezone) -> None:",
          "69:             timezone = parse_timezone(timezone)",
          "",
          "---------------"
        ],
        "airflow/timetables/trigger.py||airflow/timetables/trigger.py": [
          "File: airflow/timetables/trigger.py -> airflow/timetables/trigger.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "27: if TYPE_CHECKING:",
          "28:     from dateutil.relativedelta import relativedelta",
          "31:     from airflow.timetables.base import TimeRestriction",
          "",
          "[Removed Lines]",
          "29:     from pendulum.tz.timezone import Timezone",
          "",
          "[Added Lines]",
          "29:     from pendulum.tz.timezone import FixedTimezone, Timezone",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "48:         self,",
          "49:         cron: str,",
          "52:         interval: datetime.timedelta | relativedelta = datetime.timedelta(),",
          "53:     ) -> None:",
          "54:         super().__init__(cron, timezone)",
          "",
          "[Removed Lines]",
          "51:         timezone: str | Timezone,",
          "",
          "[Added Lines]",
          "51:         timezone: str | Timezone | FixedTimezone,",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "77:         return {\"expression\": self._expression, \"timezone\": timezone, \"interval\": interval}",
          "79:     def infer_manual_data_interval(self, *, run_after: DateTime) -> DataInterval:",
          "82:     def next_dagrun_info(",
          "83:         self,",
          "",
          "[Removed Lines]",
          "80:         return DataInterval(run_after - self._interval, run_after)",
          "",
          "[Added Lines]",
          "80:         return DataInterval(",
          "81:             # pendulum.Datetime \u00b1 timedelta should return pendulum.Datetime",
          "82:             # however mypy decide that output would be datetime.datetime",
          "83:             run_after - self._interval,  # type: ignore[arg-type]",
          "84:             run_after,",
          "85:         )",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "101:             next_start_time = max(start_time_candidates)",
          "102:         if restriction.latest is not None and restriction.latest < next_start_time:",
          "103:             return None",
          "",
          "[Removed Lines]",
          "104:         return DagRunInfo.interval(next_start_time - self._interval, next_start_time)",
          "",
          "[Added Lines]",
          "109:         return DagRunInfo.interval(",
          "110:             # pendulum.Datetime \u00b1 timedelta should return pendulum.Datetime",
          "111:             # however mypy decide that output would be datetime.datetime",
          "112:             next_start_time - self._interval,  # type: ignore[arg-type]",
          "113:             next_start_time,",
          "114:         )",
          "",
          "---------------"
        ],
        "airflow/utils/sqlalchemy.py||airflow/utils/sqlalchemy.py": [
          "File: airflow/utils/sqlalchemy.py -> airflow/utils/sqlalchemy.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "24: import logging",
          "25: from typing import TYPE_CHECKING, Any, Generator, Iterable, overload",
          "28: from dateutil import relativedelta",
          "29: from sqlalchemy import TIMESTAMP, PickleType, and_, event, false, nullsfirst, or_, true, tuple_",
          "30: from sqlalchemy.dialects import mssql, mysql",
          "",
          "[Removed Lines]",
          "27: import pendulum",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "34: from airflow import settings",
          "35: from airflow.configuration import conf",
          "36: from airflow.serialization.enums import Encoding",
          "39: if TYPE_CHECKING:",
          "40:     from kubernetes.client.models.v1_pod import V1Pod",
          "",
          "[Removed Lines]",
          "37: from airflow.utils.timezone import make_naive",
          "",
          "[Added Lines]",
          "36: from airflow.utils.timezone import make_naive, utc",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "47: log = logging.getLogger(__name__)",
          "52: class UtcDateTime(TypeDecorator):",
          "53:     \"\"\"",
          "",
          "[Removed Lines]",
          "49: utc = pendulum.tz.timezone(\"UTC\")",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "airflow/utils/timezone.py||airflow/utils/timezone.py": [
          "File: airflow/utils/timezone.py -> airflow/utils/timezone.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: from __future__ import annotations",
          "20: import datetime as dt",
          "23: import pendulum",
          "24: from dateutil.relativedelta import relativedelta",
          "25: from pendulum.datetime import DateTime",
          "31: def is_localized(value):",
          "",
          "[Removed Lines]",
          "21: from typing import overload",
          "27: # UTC time zone as a tzinfo instance.",
          "28: utc = pendulum.tz.timezone(\"UTC\")",
          "",
          "[Added Lines]",
          "21: from typing import TYPE_CHECKING, overload",
          "27: if TYPE_CHECKING:",
          "28:     from pendulum.tz.timezone import FixedTimezone, Timezone",
          "30: _PENDULUM3 = pendulum.__version__.startswith(\"3\")",
          "31: # UTC Timezone as a tzinfo instance. Actual value depends on pendulum version:",
          "32: # - Timezone(\"UTC\") in pendulum 3",
          "33: # - FixedTimezone(0, \"UTC\") in pendulum 2",
          "34: utc = pendulum.UTC",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "135:     # Check that we won't overwrite the timezone of an aware datetime.",
          "136:     if is_localized(value):",
          "137:         raise ValueError(f\"make_aware expects a naive datetime, got {value}\")",
          "144:     localized = getattr(timezone, \"localize\", None)",
          "145:     if localized is not None:",
          "146:         # This method is available for pytz time zones",
          "",
          "[Removed Lines]",
          "138:     if hasattr(value, \"fold\"):",
          "139:         # In case of python 3.6 we want to do the same that pendulum does for python3.5",
          "140:         # i.e in case we move clock back we want to schedule the run at the time of the second",
          "141:         # instance of the same clock time rather than the first one.",
          "142:         # Fold parameter has no impact in other cases so we can safely set it to 1 here",
          "143:         value = value.replace(fold=1)",
          "",
          "[Added Lines]",
          "144:     # In case we move clock back we want to schedule the run at the time of the second",
          "145:     # instance of the same clock time rather than the first one.",
          "146:     # Fold parameter has no impact in other cases, so we can safely set it to 1 here",
          "147:     value = value.replace(fold=1)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "273:     if not joined:",
          "274:         return \"<1s\"",
          "275:     return joined",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "282: def parse_timezone(name: str | int) -> FixedTimezone | Timezone:",
          "283:     \"\"\"",
          "284:     Parse timezone and return one of the pendulum Timezone.",
          "286:     Provide the same interface as ``pendulum.timezone(name)``",
          "288:     :param name: Either IANA timezone or offset to UTC in seconds.",
          "290:     :meta private:",
          "291:     \"\"\"",
          "292:     if _PENDULUM3:",
          "293:         # This only presented in pendulum 3 and code do not reached into the pendulum 2",
          "294:         return pendulum.timezone(name)  # type: ignore[operator]",
          "295:     # In pendulum 2 this refers to the function, in pendulum 3 refers to the module",
          "296:     return pendulum.tz.timezone(name)  # type: ignore[operator]",
          "299: def local_timezone() -> FixedTimezone | Timezone:",
          "300:     \"\"\"",
          "301:     Return local timezone.",
          "303:     Provide the same interface as ``pendulum.tz.local_timezone()``",
          "305:     :meta private:",
          "306:     \"\"\"",
          "307:     return pendulum.tz.local_timezone()",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/commands/common_options.py||dev/breeze/src/airflow_breeze/commands/common_options.py": [
          "File: dev/breeze/src/airflow_breeze/commands/common_options.py -> dev/breeze/src/airflow_breeze/commands/common_options.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "151:     is_flag=True,",
          "152:     envvar=\"DOWNGRADE_SQLALCHEMY\",",
          "153: )",
          "154: option_dry_run = click.option(",
          "155:     \"-D\",",
          "156:     \"--dry-run\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "154: option_downgrade_pendulum = click.option(",
          "155:     \"--downgrade-pendulum\",",
          "156:     help=\"Downgrade Pendulum to minimum supported version.\",",
          "157:     is_flag=True,",
          "158:     envvar=\"DOWNGRADE_PENDULUM\",",
          "159: )",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/commands/developer_commands.py||dev/breeze/src/airflow_breeze/commands/developer_commands.py": [
          "File: dev/breeze/src/airflow_breeze/commands/developer_commands.py -> dev/breeze/src/airflow_breeze/commands/developer_commands.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "39:     option_database_isolation,",
          "40:     option_db_reset,",
          "41:     option_docker_host,",
          "42:     option_downgrade_sqlalchemy,",
          "43:     option_dry_run,",
          "44:     option_forward_credentials,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "42:     option_downgrade_pendulum,",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "248: @option_db_reset",
          "249: @option_docker_host",
          "250: @option_downgrade_sqlalchemy",
          "251: @option_dry_run",
          "252: @option_executor_shell",
          "253: @option_force_build",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "252: @option_downgrade_pendulum",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "294:     database_isolation: bool,",
          "295:     db_reset: bool,",
          "296:     downgrade_sqlalchemy: bool,",
          "297:     docker_host: str | None,",
          "298:     executor: str,",
          "299:     extra_args: tuple,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "299:     downgrade_pendulum: bool,",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "354:         database_isolation=database_isolation,",
          "355:         db_reset=db_reset,",
          "356:         downgrade_sqlalchemy=downgrade_sqlalchemy,",
          "357:         docker_host=docker_host,",
          "358:         executor=executor,",
          "359:         extra_args=extra_args if not max_time else [\"exit\"],",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "360:         downgrade_pendulum=downgrade_pendulum,",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/commands/developer_commands_config.py||dev/breeze/src/airflow_breeze/commands/developer_commands_config.py": [
          "File: dev/breeze/src/airflow_breeze/commands/developer_commands_config.py -> dev/breeze/src/airflow_breeze/commands/developer_commands_config.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "159:             \"options\": [",
          "160:                 \"--upgrade-boto\",",
          "161:                 \"--downgrade-sqlalchemy\",",
          "162:             ],",
          "163:         },",
          "164:         {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "162:                 \"--downgrade-pendulum\",",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/commands/testing_commands.py||dev/breeze/src/airflow_breeze/commands/testing_commands.py": [
          "File: dev/breeze/src/airflow_breeze/commands/testing_commands.py -> dev/breeze/src/airflow_breeze/commands/testing_commands.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "29:     option_backend,",
          "30:     option_db_reset,",
          "31:     option_debug_resources,",
          "32:     option_downgrade_sqlalchemy,",
          "33:     option_dry_run,",
          "34:     option_forward_credentials,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "32:     option_downgrade_pendulum,",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "471: @option_excluded_parallel_test_types",
          "472: @option_upgrade_boto",
          "473: @option_downgrade_sqlalchemy",
          "474: @option_collect_only",
          "475: @option_remove_arm_packages",
          "476: @option_skip_docker_compose_down",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "475: @option_downgrade_pendulum",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "513: @option_excluded_parallel_test_types",
          "514: @option_upgrade_boto",
          "515: @option_downgrade_sqlalchemy",
          "516: @option_collect_only",
          "517: @option_remove_arm_packages",
          "518: @option_skip_docker_compose_down",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "518: @option_downgrade_pendulum",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "548: @option_collect_only",
          "549: @option_debug_resources",
          "550: @option_downgrade_sqlalchemy",
          "551: @option_dry_run",
          "552: @option_enable_coverage",
          "553: @option_excluded_parallel_test_types",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "554: @option_downgrade_pendulum",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "589:     db_reset: bool,",
          "590:     debug_resources: bool,",
          "591:     downgrade_sqlalchemy: bool,",
          "592:     enable_coverage: bool,",
          "593:     excluded_parallel_test_types: str,",
          "594:     extra_pytest_args: tuple,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "596:     downgrade_pendulum: bool,",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "632:         backend=backend,",
          "633:         collect_only=collect_only,",
          "634:         downgrade_sqlalchemy=downgrade_sqlalchemy,",
          "635:         enable_coverage=enable_coverage,",
          "636:         forward_credentials=forward_credentials,",
          "637:         forward_ports=False,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "640:         downgrade_pendulum=downgrade_pendulum,",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/commands/testing_commands_config.py||dev/breeze/src/airflow_breeze/commands/testing_commands_config.py": [
          "File: dev/breeze/src/airflow_breeze/commands/testing_commands_config.py -> dev/breeze/src/airflow_breeze/commands/testing_commands_config.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "79:                 \"--mount-sources\",",
          "80:                 \"--upgrade-boto\",",
          "81:                 \"--downgrade-sqlalchemy\",",
          "82:                 \"--remove-arm-packages\",",
          "83:                 \"--skip-docker-compose-down\",",
          "84:             ],",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "82:                 \"--downgrade-pendulum\",",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "126:                 \"--mount-sources\",",
          "127:                 \"--upgrade-boto\",",
          "128:                 \"--downgrade-sqlalchemy\",",
          "129:                 \"--remove-arm-packages\",",
          "130:                 \"--skip-docker-compose-down\",",
          "131:             ],",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "130:                 \"--downgrade-pendulum\",",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "177:                 \"--mount-sources\",",
          "178:                 \"--upgrade-boto\",",
          "179:                 \"--downgrade-sqlalchemy\",",
          "180:                 \"--remove-arm-packages\",",
          "181:                 \"--skip-docker-compose-down\",",
          "182:             ],",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "182:                 \"--downgrade-pendulum\",",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/params/shell_params.py||dev/breeze/src/airflow_breeze/params/shell_params.py": [
          "File: dev/breeze/src/airflow_breeze/params/shell_params.py -> dev/breeze/src/airflow_breeze/params/shell_params.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "148:     dev_mode: bool = False",
          "149:     docker_host: str | None = os.environ.get(\"DOCKER_HOST\")",
          "150:     downgrade_sqlalchemy: bool = False",
          "151:     dry_run: bool = False",
          "152:     enable_coverage: bool = False",
          "153:     executor: str = START_AIRFLOW_DEFAULT_ALLOWED_EXECUTOR",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "151:     downgrade_pendulum: bool = False",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "516:         _set_var(_env, \"DEV_MODE\", self.dev_mode)",
          "517:         _set_var(_env, \"DOCKER_IS_ROOTLESS\", self.rootless_docker)",
          "518:         _set_var(_env, \"DOWNGRADE_SQLALCHEMY\", self.downgrade_sqlalchemy)",
          "519:         _set_var(_env, \"ENABLED_SYSTEMS\", None, \"\")",
          "520:         _set_var(_env, \"FLOWER_HOST_PORT\", None, FLOWER_HOST_PORT)",
          "521:         _set_var(_env, \"GITHUB_ACTIONS\", self.github_actions)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "520:         _set_var(_env, \"DOWNGRADE_PENDULUM\", self.downgrade_pendulum)",
          "",
          "---------------"
        ],
        "kubernetes_tests/test_kubernetes_pod_operator.py||kubernetes_tests/test_kubernetes_pod_operator.py": [
          "File: kubernetes_tests/test_kubernetes_pod_operator.py -> kubernetes_tests/test_kubernetes_pod_operator.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "26: from unittest.mock import ANY, MagicMock",
          "27: from uuid import uuid4",
          "30: import pytest",
          "31: from kubernetes import client",
          "32: from kubernetes.client import V1EnvVar, V1PodSecurityContext, V1SecurityContext, models as k8s",
          "",
          "[Removed Lines]",
          "29: import pendulum",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "54: def create_context(task) -> Context:",
          "55:     dag = DAG(dag_id=\"dag\")",
          "57:     dag_run = DagRun(",
          "58:         dag_id=dag.dag_id,",
          "59:         execution_date=execution_date,",
          "",
          "[Removed Lines]",
          "56:     execution_date = timezone.datetime(2016, 1, 1, 1, 0, 0, tzinfo=pendulum.tz.timezone(\"Europe/Amsterdam\"))",
          "",
          "[Added Lines]",
          "55:     execution_date = timezone.datetime(",
          "56:         2016, 1, 1, 1, 0, 0, tzinfo=timezone.parse_timezone(\"Europe/Amsterdam\")",
          "57:     )",
          "",
          "---------------"
        ],
        "scripts/ci/docker-compose/devcontainer.env||scripts/ci/docker-compose/devcontainer.env": [
          "File: scripts/ci/docker-compose/devcontainer.env -> scripts/ci/docker-compose/devcontainer.env",
          "--- Hunk 1 ---",
          "[Context before]",
          "68: SUSPENDED_PROVIDERS_FOLDERS=\"\"",
          "69: TEST_TYPE=",
          "70: UPGRADE_BOTO=\"false\"",
          "71: DOWNGRADE_SQLALCHEMY=\"false\"",
          "72: UPGRADE_TO_NEWER_DEPENDENCIES=\"false\"",
          "73: VERBOSE=\"false\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "71: DOWNGRADE_PENDULUM=\"false\"",
          "",
          "---------------"
        ],
        "scripts/docker/entrypoint_ci.sh||scripts/docker/entrypoint_ci.sh": [
          "File: scripts/docker/entrypoint_ci.sh -> scripts/docker/entrypoint_ci.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "239:     pip check",
          "240: }",
          "242: # Check if we should run tests and run them if needed",
          "243: function check_run_tests() {",
          "244:     if [[ ${RUN_TESTS=} != \"true\" ]]; then",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "242: # Download minimum supported version of pendulum to run tests with it",
          "243: function check_download_pendulum() {",
          "244:     if [[ ${DOWNGRADE_PENDULUM=} != \"true\" ]]; then",
          "245:         return",
          "246:     fi",
          "247:     min_pendulum_version=$(grep \"\\\"pendulum>=\" pyproject.toml | sed \"s/.*>=\\([0-9\\.]*\\).*/\\1/\" | xargs)",
          "248:     echo",
          "249:     echo \"${COLOR_BLUE}Downgrading pendulum to minimum supported version: ${min_pendulum_version}${COLOR_RESET}\"",
          "250:     echo",
          "251:     pip install --root-user-action ignore \"pendulum==${min_pendulum_version}\"",
          "252:     pip check",
          "253: }",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "269: environment_initialization",
          "270: check_boto_upgrade",
          "271: check_download_sqlalchemy",
          "272: check_run_tests \"${@}\"",
          "274: # If we are not running tests - just exec to bash shell",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "285: check_download_pendulum",
          "",
          "---------------"
        ],
        "tests/api_connexion/endpoints/test_dag_endpoint.py||tests/api_connexion/endpoints/test_dag_endpoint.py": [
          "File: tests/api_connexion/endpoints/test_dag_endpoint.py -> tests/api_connexion/endpoints/test_dag_endpoint.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: import unittest.mock",
          "21: from datetime import datetime",
          "23: import pytest",
          "25: from airflow.api_connexion.exceptions import EXCEPTIONS_LINK_MAP",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "23: import pendulum",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "46: TASK_ID = \"op1\"",
          "47: DAG2_ID = \"test_dag2\"",
          "48: DAG3_ID = \"test_dag3\"",
          "51: @pytest.fixture(scope=\"module\")",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "50: UTC_JSON_REPR = \"UTC\" if pendulum.__version__.startswith(\"3\") else \"Timezone('UTC')\"",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "316:             \"tags\": [],",
          "317:             \"template_searchpath\": None,",
          "318:             \"timetable_description\": None,",
          "320:         }",
          "321:         assert response.json == expected",
          "",
          "[Removed Lines]",
          "319:             \"timezone\": \"Timezone('UTC')\",",
          "",
          "[Added Lines]",
          "321:             \"timezone\": UTC_JSON_REPR,",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "367:             \"tags\": [],",
          "368:             \"template_searchpath\": None,",
          "369:             \"timetable_description\": None,",
          "371:         }",
          "372:         assert response.json == expected",
          "",
          "[Removed Lines]",
          "370:             \"timezone\": \"Timezone('UTC')\",",
          "",
          "[Added Lines]",
          "372:             \"timezone\": UTC_JSON_REPR,",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "418:             \"tags\": [],",
          "419:             \"template_searchpath\": None,",
          "420:             \"timetable_description\": None,",
          "422:         }",
          "423:         assert response.json == expected",
          "",
          "[Removed Lines]",
          "421:             \"timezone\": \"Timezone('UTC')\",",
          "",
          "[Added Lines]",
          "423:             \"timezone\": UTC_JSON_REPR,",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "478:             \"tags\": [],",
          "479:             \"template_searchpath\": None,",
          "480:             \"timetable_description\": None,",
          "482:         }",
          "483:         response = self.client.get(",
          "484:             f\"/api/v1/dags/{self.dag_id}/details\", environ_overrides={\"REMOTE_USER\": \"test\"}",
          "",
          "[Removed Lines]",
          "481:             \"timezone\": \"Timezone('UTC')\",",
          "",
          "[Added Lines]",
          "483:             \"timezone\": UTC_JSON_REPR,",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "539:             \"tags\": [],",
          "540:             \"template_searchpath\": None,",
          "541:             \"timetable_description\": None,",
          "543:         }",
          "544:         expected.update({\"last_parsed\": response.json[\"last_parsed\"]})",
          "545:         assert response.json == expected",
          "",
          "[Removed Lines]",
          "542:             \"timezone\": \"Timezone('UTC')\",",
          "",
          "[Added Lines]",
          "544:             \"timezone\": UTC_JSON_REPR,",
          "",
          "---------------"
        ],
        "tests/api_connexion/schemas/test_dag_schema.py||tests/api_connexion/schemas/test_dag_schema.py": [
          "File: tests/api_connexion/schemas/test_dag_schema.py -> tests/api_connexion/schemas/test_dag_schema.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "19: from datetime import datetime",
          "21: import pytest",
          "23: from airflow.api_connexion.schemas.dag_schema import (",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "21: import pendulum",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "29: from airflow.models import DagModel, DagTag",
          "30: from airflow.models.dag import DAG",
          "33: def test_serialize_test_dag_schema(url_safe_serializer):",
          "34:     dag_model = DagModel(",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "33: UTC_JSON_REPR = \"UTC\" if pendulum.__version__.startswith(\"3\") else \"Timezone('UTC')\"",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "184:         \"start_date\": \"2020-06-19T00:00:00+00:00\",",
          "185:         \"tags\": [{\"name\": \"example1\"}, {\"name\": \"example2\"}],",
          "186:         \"template_searchpath\": None,",
          "188:         \"max_active_runs\": 16,",
          "189:         \"pickle_id\": None,",
          "190:         \"end_date\": None,",
          "",
          "[Removed Lines]",
          "187:         \"timezone\": \"Timezone('UTC')\",",
          "",
          "[Added Lines]",
          "190:         \"timezone\": UTC_JSON_REPR,",
          "",
          "---------------"
        ],
        "tests/cli/commands/test_dag_command.py||tests/cli/commands/test_dag_command.py": [
          "File: tests/cli/commands/test_dag_command.py -> tests/cli/commands/test_dag_command.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "50: from tests.test_utils.db import clear_db_dags, clear_db_runs",
          "52: DEFAULT_DATE = timezone.make_aware(datetime(2015, 1, 1), timezone=timezone.utc)",
          "54: # TODO: Check if tests needs side effects - locally there's missing DAG",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "53: if pendulum.__version__.startswith(\"3\"):",
          "54:     DEFAULT_DATE_REPR = DEFAULT_DATE.isoformat(sep=\" \")",
          "55: else:",
          "56:     DEFAULT_DATE_REPR = DEFAULT_DATE.isoformat()",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "162:             )",
          "164:         output = stdout.getvalue()",
          "166:         assert \"Task runme_0 located in DAG example_bash_operator\\n\" in output",
          "168:         mock_run.assert_not_called()  # Dry run shouldn't run the backfill",
          "",
          "[Removed Lines]",
          "165:         assert f\"Dry run of DAG example_bash_operator on {DEFAULT_DATE.isoformat()}\\n\" in output",
          "",
          "[Added Lines]",
          "169:         assert f\"Dry run of DAG example_bash_operator on {DEFAULT_DATE_REPR}\\n\" in output",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "236:         output = stdout.getvalue()",
          "242:         assert \"Task run_this_first located in DAG example_branch_python_operator_decorator\\n\" in output",
          "244:         assert \"Task run_this_first located in DAG example_branch_operator\\n\" in output",
          "246:     @mock.patch(\"airflow.cli.commands.dag_command.get_dag\")",
          "",
          "[Removed Lines]",
          "238:         assert (",
          "239:             f\"Dry run of DAG example_branch_python_operator_decorator on \"",
          "240:             f\"{DEFAULT_DATE.isoformat()}\\n\" in output",
          "241:         )",
          "243:         assert f\"Dry run of DAG example_branch_operator on {DEFAULT_DATE.isoformat()}\\n\" in output",
          "",
          "[Added Lines]",
          "242:         assert f\"Dry run of DAG example_branch_python_operator_decorator on {DEFAULT_DATE_REPR}\\n\" in output",
          "244:         assert f\"Dry run of DAG example_branch_operator on {DEFAULT_DATE_REPR}\\n\" in output",
          "",
          "---------------"
        ],
        "tests/models/test_dag.py||tests/models/test_dag.py": [
          "File: tests/models/test_dag.py -> tests/models/test_dag.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "37: import pytest",
          "38: import time_machine",
          "39: from dateutil.relativedelta import relativedelta",
          "40: from sqlalchemy import inspect",
          "42: from airflow import settings",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "40: from pendulum.tz.timezone import Timezone",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "676:         \"\"\"",
          "677:         Make sure DST transitions are properly observed",
          "678:         \"\"\"",
          "681:         assert start.isoformat() == \"2018-10-28T02:55:00+02:00\", \"Pre-condition: start date is in DST\"",
          "683:         utc = timezone.convert_to_utc(start)",
          "",
          "[Removed Lines]",
          "679:         local_tz = pendulum.timezone(\"Europe/Zurich\")",
          "680:         start = local_tz.convert(datetime.datetime(2018, 10, 28, 2, 55), dst_rule=pendulum.PRE_TRANSITION)",
          "",
          "[Added Lines]",
          "680:         local_tz = Timezone(\"Europe/Zurich\")",
          "681:         start = local_tz.convert(datetime.datetime(2018, 10, 28, 2, 55, fold=0))",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "706:         Make sure DST transitions are properly observed",
          "707:         \"\"\"",
          "708:         local_tz = pendulum.timezone(\"Europe/Zurich\")",
          "711:         utc = timezone.convert_to_utc(start)",
          "",
          "[Removed Lines]",
          "709:         start = local_tz.convert(datetime.datetime(2018, 10, 27, 3), dst_rule=pendulum.PRE_TRANSITION)",
          "",
          "[Added Lines]",
          "710:         start = local_tz.convert(datetime.datetime(2018, 10, 27, 3, fold=0))",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "735:         Make sure DST transitions are properly observed",
          "736:         \"\"\"",
          "737:         local_tz = pendulum.timezone(\"Europe/Zurich\")",
          "740:         utc = timezone.convert_to_utc(start)",
          "",
          "[Removed Lines]",
          "738:         start = local_tz.convert(datetime.datetime(2018, 3, 25, 2), dst_rule=pendulum.PRE_TRANSITION)",
          "",
          "[Added Lines]",
          "739:         start = local_tz.convert(datetime.datetime(2018, 3, 25, 2, fold=0))",
          "",
          "---------------"
        ],
        "tests/providers/openlineage/plugins/test_utils.py||tests/providers/openlineage/plugins/test_utils.py": [
          "File: tests/providers/openlineage/plugins/test_utils.py -> tests/providers/openlineage/plugins/test_utils.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "23: from json import JSONEncoder",
          "24: from typing import Any",
          "27: import pytest",
          "28: from attrs import define",
          "29: from openlineage.client.utils import RedactMixin",
          "",
          "[Removed Lines]",
          "26: import pendulum",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "39:     to_json_encodable,",
          "40:     url_to_https,",
          "41: )",
          "42: from airflow.utils.log.secrets_masker import _secrets_masker",
          "43: from airflow.utils.state import State",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "41: from airflow.utils import timezone",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "86:         state=State.NONE, run_id=run_id, data_interval=dag.get_next_data_interval(dag_model)",
          "87:     )",
          "88:     assert dagrun.data_interval_start is not None",
          "91:     assert dagrun.data_interval_start, dagrun.data_interval_end == (start_date_tz, end_date_tz)",
          "",
          "[Removed Lines]",
          "89:     start_date_tz = datetime.datetime(2022, 1, 1, tzinfo=pendulum.tz.timezone(\"UTC\"))",
          "90:     end_date_tz = datetime.datetime(2022, 1, 1, hour=2, tzinfo=pendulum.tz.timezone(\"UTC\"))",
          "",
          "[Added Lines]",
          "89:     start_date_tz = datetime.datetime(2022, 1, 1, tzinfo=timezone.utc)",
          "90:     end_date_tz = datetime.datetime(2022, 1, 1, hour=2, tzinfo=timezone.utc)",
          "",
          "---------------"
        ],
        "tests/sensors/test_time_sensor.py||tests/sensors/test_time_sensor.py": [
          "File: tests/sensors/test_time_sensor.py -> tests/sensors/test_time_sensor.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: from __future__ import annotations",
          "20: from datetime import datetime, time",
          "23: import pendulum",
          "24: import pytest",
          "25: import time_machine",
          "28: from airflow.exceptions import TaskDeferred",
          "29: from airflow.models.dag import DAG",
          "",
          "[Removed Lines]",
          "21: from unittest.mock import patch",
          "26: from pendulum.tz.timezone import UTC",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "34: DEFAULT_TIMEZONE = \"Asia/Singapore\"  # UTC+08:00",
          "35: DEFAULT_DATE_WO_TZ = datetime(2015, 1, 1)",
          "39: class TestTimeSensor:",
          "",
          "[Removed Lines]",
          "36: DEFAULT_DATE_WITH_TZ = datetime(2015, 1, 1, tzinfo=pendulum.tz.timezone(DEFAULT_TIMEZONE))",
          "",
          "[Added Lines]",
          "34: DEFAULT_DATE_WITH_TZ = datetime(2015, 1, 1, tzinfo=timezone.parse_timezone(DEFAULT_TIMEZONE))",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "46:         ],",
          "47:     )",
          "48:     @time_machine.travel(timezone.datetime(2020, 1, 1, 23, 0).replace(tzinfo=timezone.utc))",
          "56: class TestTimeSensorAsync:",
          "",
          "[Removed Lines]",
          "49:     def test_timezone(self, default_timezone, start_date, expected):",
          "50:         with patch(\"airflow.settings.TIMEZONE\", pendulum.timezone(default_timezone)):",
          "51:             dag = DAG(\"test\", default_args={\"start_date\": start_date})",
          "52:             op = TimeSensor(task_id=\"test\", target_time=time(10, 0), dag=dag)",
          "53:             assert op.poke(None) == expected",
          "",
          "[Added Lines]",
          "47:     def test_timezone(self, default_timezone, start_date, expected, monkeypatch):",
          "48:         monkeypatch.setattr(\"airflow.settings.TIMEZONE\", timezone.parse_timezone(default_timezone))",
          "49:         dag = DAG(\"test\", default_args={\"start_date\": start_date})",
          "50:         op = TimeSensor(task_id=\"test\", target_time=time(10, 0), dag=dag)",
          "51:         assert op.poke(None) == expected",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "72:         with DAG(\"test_target_time_aware\", start_date=timezone.datetime(2020, 1, 1, 23, 0)):",
          "73:             aware_time = time(0, 1).replace(tzinfo=pendulum.local_timezone())",
          "74:             op = TimeSensorAsync(task_id=\"test\", target_time=aware_time)",
          "78:     def test_target_time_naive_dag_timezone(self):",
          "79:         \"\"\"",
          "",
          "[Removed Lines]",
          "75:             assert hasattr(op.target_datetime.tzinfo, \"offset\")",
          "76:             assert op.target_datetime.tzinfo.offset == 0",
          "",
          "[Added Lines]",
          "73:             assert op.target_datetime.tzinfo == timezone.utc",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "85:         ):",
          "86:             op = TimeSensorAsync(task_id=\"test\", target_time=pendulum.time(9, 0))",
          "87:             assert op.target_datetime.time() == pendulum.time(1, 0)",
          "",
          "[Removed Lines]",
          "88:             assert op.target_datetime.tzinfo == UTC",
          "",
          "[Added Lines]",
          "85:             assert op.target_datetime.tzinfo == timezone.utc",
          "",
          "---------------"
        ],
        "tests/serialization/serializers/test_serializers.py||tests/serialization/serializers/test_serializers.py": [
          "File: tests/serialization/serializers/test_serializers.py -> tests/serialization/serializers/test_serializers.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "21: from unittest.mock import patch",
          "23: import numpy as np",
          "24: import pendulum.tz",
          "25: import pytest",
          "26: from dateutil.tz import tzutc",
          "27: from deltalake import DeltaTable",
          "28: from pendulum import DateTime",
          "29: from pyiceberg.catalog import Catalog",
          "30: from pyiceberg.io import FileIO",
          "31: from pyiceberg.table import Table",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "24: import pendulum",
          "30: from pendulum.tz.timezone import FixedTimezone, Timezone",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "39: else:",
          "40:     from backports.zoneinfo import ZoneInfo",
          "43: class TestSerializers:",
          "44:     def test_datetime(self):",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "44: PENDULUM3 = pendulum.__version__.startswith(\"3\")",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "227:         assert i.version() == d.version()",
          "228:         assert i._storage_options == d._storage_options",
          "229:         assert d._storage_options is None",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "235:     @pytest.mark.skipif(not PENDULUM3, reason=\"Test case for pendulum~=3\")",
          "236:     @pytest.mark.parametrize(",
          "237:         \"ser_value, expected\",",
          "238:         [",
          "239:             pytest.param(",
          "240:                 {",
          "241:                     \"__classname__\": \"pendulum.datetime.DateTime\",",
          "242:                     \"__version__\": 2,",
          "243:                     \"__data__\": {",
          "244:                         \"timestamp\": 1680307200.0,",
          "245:                         \"tz\": {",
          "246:                             \"__classname__\": \"builtins.tuple\",",
          "247:                             \"__version__\": 1,",
          "248:                             \"__data__\": [\"UTC\", \"pendulum.tz.timezone.FixedTimezone\", 1, True],",
          "249:                         },",
          "250:                     },",
          "251:                 },",
          "252:                 pendulum.datetime(2023, 4, 1, tz=Timezone(\"UTC\")),",
          "253:                 id=\"in-utc-timezone\",",
          "254:             ),",
          "255:             pytest.param(",
          "256:                 {",
          "257:                     \"__classname__\": \"pendulum.datetime.DateTime\",",
          "258:                     \"__version__\": 2,",
          "259:                     \"__data__\": {",
          "260:                         \"timestamp\": 1680292800.0,",
          "261:                         \"tz\": {",
          "262:                             \"__classname__\": \"builtins.tuple\",",
          "263:                             \"__version__\": 1,",
          "264:                             \"__data__\": [\"Asia/Tbilisi\", \"pendulum.tz.timezone.Timezone\", 1, True],",
          "265:                         },",
          "266:                     },",
          "267:                 },",
          "268:                 pendulum.datetime(2023, 4, 1, tz=Timezone(\"Asia/Tbilisi\")),",
          "269:                 id=\"non-dts-timezone\",",
          "270:             ),",
          "271:             pytest.param(",
          "272:                 {",
          "273:                     \"__classname__\": \"pendulum.datetime.DateTime\",",
          "274:                     \"__version__\": 2,",
          "275:                     \"__data__\": {",
          "276:                         \"timestamp\": 1680303600.0,",
          "277:                         \"tz\": {",
          "278:                             \"__classname__\": \"builtins.tuple\",",
          "279:                             \"__version__\": 1,",
          "280:                             \"__data__\": [\"Europe/London\", \"pendulum.tz.timezone.Timezone\", 1, True],",
          "281:                         },",
          "282:                     },",
          "283:                 },",
          "284:                 pendulum.datetime(2023, 4, 1, tz=Timezone(\"Europe/London\")),",
          "285:                 id=\"dts-timezone\",",
          "286:             ),",
          "287:             pytest.param(",
          "288:                 {",
          "289:                     \"__classname__\": \"pendulum.datetime.DateTime\",",
          "290:                     \"__version__\": 2,",
          "291:                     \"__data__\": {",
          "292:                         \"timestamp\": 1680310800.0,",
          "293:                         \"tz\": {",
          "294:                             \"__classname__\": \"builtins.tuple\",",
          "295:                             \"__version__\": 1,",
          "296:                             \"__data__\": [-3600, \"pendulum.tz.timezone.FixedTimezone\", 1, True],",
          "297:                         },",
          "298:                     },",
          "299:                 },",
          "300:                 pendulum.datetime(2023, 4, 1, tz=FixedTimezone(-3600)),",
          "301:                 id=\"offset-timezone\",",
          "302:             ),",
          "303:         ],",
          "304:     )",
          "305:     def test_pendulum_2_to_3(self, ser_value, expected):",
          "306:         \"\"\"Test deserialize objects in pendulum 3 which serialised in pendulum 2.\"\"\"",
          "307:         assert deserialize(ser_value) == expected",
          "309:     @pytest.mark.skipif(PENDULUM3, reason=\"Test case for pendulum~=2\")",
          "310:     @pytest.mark.parametrize(",
          "311:         \"ser_value, expected\",",
          "312:         [",
          "313:             pytest.param(",
          "314:                 {",
          "315:                     \"__classname__\": \"pendulum.datetime.DateTime\",",
          "316:                     \"__version__\": 2,",
          "317:                     \"__data__\": {",
          "318:                         \"timestamp\": 1680307200.0,",
          "319:                         \"tz\": {",
          "320:                             \"__classname__\": \"builtins.tuple\",",
          "321:                             \"__version__\": 1,",
          "322:                             \"__data__\": [\"UTC\", \"pendulum.tz.timezone.Timezone\", 1, True],",
          "323:                         },",
          "324:                     },",
          "325:                 },",
          "326:                 pendulum.datetime(2023, 4, 1, tz=Timezone(\"UTC\")),",
          "327:                 id=\"in-utc-timezone\",",
          "328:             ),",
          "329:             pytest.param(",
          "330:                 {",
          "331:                     \"__classname__\": \"pendulum.datetime.DateTime\",",
          "332:                     \"__version__\": 2,",
          "333:                     \"__data__\": {",
          "334:                         \"timestamp\": 1680292800.0,",
          "335:                         \"tz\": {",
          "336:                             \"__classname__\": \"builtins.tuple\",",
          "337:                             \"__version__\": 1,",
          "338:                             \"__data__\": [\"Asia/Tbilisi\", \"pendulum.tz.timezone.Timezone\", 1, True],",
          "339:                         },",
          "340:                     },",
          "341:                 },",
          "342:                 pendulum.datetime(2023, 4, 1, tz=Timezone(\"Asia/Tbilisi\")),",
          "343:                 id=\"non-dts-timezone\",",
          "344:             ),",
          "345:             pytest.param(",
          "346:                 {",
          "347:                     \"__classname__\": \"pendulum.datetime.DateTime\",",
          "348:                     \"__version__\": 2,",
          "349:                     \"__data__\": {",
          "350:                         \"timestamp\": 1680303600.0,",
          "351:                         \"tz\": {",
          "352:                             \"__classname__\": \"builtins.tuple\",",
          "353:                             \"__version__\": 1,",
          "354:                             \"__data__\": [\"Europe/London\", \"pendulum.tz.timezone.Timezone\", 1, True],",
          "355:                         },",
          "356:                     },",
          "357:                 },",
          "358:                 pendulum.datetime(2023, 4, 1, tz=Timezone(\"Europe/London\")),",
          "359:                 id=\"dts-timezone\",",
          "360:             ),",
          "361:             pytest.param(",
          "362:                 {",
          "363:                     \"__classname__\": \"pendulum.datetime.DateTime\",",
          "364:                     \"__version__\": 2,",
          "365:                     \"__data__\": {",
          "366:                         \"timestamp\": 1680310800.0,",
          "367:                         \"tz\": {",
          "368:                             \"__classname__\": \"builtins.tuple\",",
          "369:                             \"__version__\": 1,",
          "370:                             \"__data__\": [-3600, \"pendulum.tz.timezone.FixedTimezone\", 1, True],",
          "371:                         },",
          "372:                     },",
          "373:                 },",
          "374:                 pendulum.datetime(2023, 4, 1, tz=FixedTimezone(-3600)),",
          "375:                 id=\"offset-timezone\",",
          "376:             ),",
          "377:         ],",
          "378:     )",
          "379:     def test_pendulum_3_to_2(self, ser_value, expected):",
          "380:         \"\"\"Test deserialize objects in pendulum 2 which serialised in pendulum 3.\"\"\"",
          "381:         assert deserialize(ser_value) == expected",
          "",
          "---------------"
        ],
        "tests/serialization/test_serialized_objects.py||tests/serialization/test_serialized_objects.py": [
          "File: tests/serialization/test_serialized_objects.py -> tests/serialization/test_serialized_objects.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: import json",
          "21: from datetime import datetime, timedelta",
          "24: import pytest",
          "25: from dateutil import relativedelta",
          "26: from kubernetes.client import models as k8s",
          "28: from airflow.datasets import Dataset",
          "29: from airflow.exceptions import SerializationError",
          "",
          "[Removed Lines]",
          "23: import pendulum",
          "",
          "[Added Lines]",
          "26: from pendulum.tz.timezone import Timezone",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "142:         (1, None, equals),",
          "143:         (datetime.utcnow(), DAT.DATETIME, equal_time),",
          "144:         (timedelta(minutes=2), DAT.TIMEDELTA, equals),",
          "146:         (relativedelta.relativedelta(hours=+1), DAT.RELATIVEDELTA, lambda a, b: a.hours == b.hours),",
          "147:         ({\"test\": \"dict\", \"test-1\": 1}, None, equals),",
          "148:         ([\"array_item\", 2], None, equals),",
          "",
          "[Removed Lines]",
          "145:         (pendulum.tz.timezone(\"UTC\"), DAT.TIMEZONE, lambda a, b: a.name == b.name),",
          "",
          "[Added Lines]",
          "145:         (Timezone(\"UTC\"), DAT.TIMEZONE, lambda a, b: a.name == b.name),",
          "",
          "---------------"
        ],
        "tests/triggers/test_temporal.py||tests/triggers/test_temporal.py": [
          "File: tests/triggers/test_temporal.py -> tests/triggers/test_temporal.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "64: @pytest.mark.parametrize(",
          "65:     \"tz\",",
          "66:     [",
          "70:     ],",
          "71: )",
          "72: @pytest.mark.asyncio",
          "",
          "[Removed Lines]",
          "67:         pendulum.tz.timezone(\"UTC\"),",
          "68:         pendulum.tz.timezone(\"Europe/Paris\"),",
          "69:         pendulum.tz.timezone(\"America/Toronto\"),",
          "",
          "[Added Lines]",
          "67:         timezone.parse_timezone(\"UTC\"),",
          "68:         timezone.parse_timezone(\"Europe/Paris\"),",
          "69:         timezone.parse_timezone(\"America/Toronto\"),",
          "",
          "---------------"
        ],
        "tests/utils/test_timezone.py||tests/utils/test_timezone.py": [
          "File: tests/utils/test_timezone.py -> tests/utils/test_timezone.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "22: import pendulum",
          "23: import pytest",
          "25: from airflow.utils import timezone",
          "31: UTC = timezone.utc",
          "",
          "[Removed Lines]",
          "26: from airflow.utils.timezone import coerce_datetime",
          "28: CET = pendulum.tz.timezone(\"Europe/Paris\")",
          "29: EAT = pendulum.tz.timezone(\"Africa/Nairobi\")  # Africa/Nairobi",
          "30: ICT = pendulum.tz.timezone(\"Asia/Bangkok\")  # Asia/Bangkok",
          "",
          "[Added Lines]",
          "24: from pendulum.tz.timezone import Timezone",
          "27: from airflow.utils.timezone import coerce_datetime, parse_timezone",
          "29: CET = Timezone(\"Europe/Paris\")",
          "30: EAT = Timezone(\"Africa/Nairobi\")  # Africa/Nairobi",
          "31: ICT = Timezone(\"Asia/Bangkok\")  # Asia/Bangkok",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "117: )",
          "118: def test_coerce_datetime(input_datetime, output_datetime):",
          "119:     assert output_datetime == coerce_datetime(input_datetime)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "123: @pytest.mark.parametrize(",
          "124:     \"tz_name\",",
          "125:     [",
          "126:         pytest.param(\"Europe/Paris\", id=\"CET\"),",
          "127:         pytest.param(\"Africa/Nairobi\", id=\"EAT\"),",
          "128:         pytest.param(\"Asia/Bangkok\", id=\"ICT\"),",
          "129:     ],",
          "130: )",
          "131: def test_parse_timezone_iana(tz_name: str):",
          "132:     tz = parse_timezone(tz_name)",
          "133:     assert tz.name == tz_name",
          "134:     assert parse_timezone(tz_name) is tz",
          "137: @pytest.mark.parametrize(\"tz_name\", [\"utc\", \"UTC\", \"uTc\"])",
          "138: def test_parse_timezone_utc(tz_name):",
          "139:     tz = parse_timezone(tz_name)",
          "140:     assert tz.name == \"UTC\"",
          "141:     assert parse_timezone(tz_name) is tz",
          "142:     assert tz is timezone.utc, \"Expected that UTC timezone is same object as `airflow.utils.timezone.utc`\"",
          "145: @pytest.mark.parametrize(",
          "146:     \"tz_offset, expected_offset, expected_name\",",
          "147:     [",
          "148:         pytest.param(0, 0, \"+00:00\", id=\"zero-offset\"),",
          "149:         pytest.param(-3600, -3600, \"-01:00\", id=\"1-hour-behind\"),",
          "150:         pytest.param(19800, 19800, \"+05:30\", id=\"5.5-hours-ahead\"),",
          "151:     ],",
          "152: )",
          "153: def test_parse_timezone_offset(tz_offset: int, expected_offset, expected_name):",
          "154:     tz = parse_timezone(tz_offset)",
          "155:     assert hasattr(tz, \"offset\")",
          "156:     assert tz.offset == expected_offset",
          "157:     assert tz.name == expected_name",
          "158:     assert parse_timezone(tz_offset) is tz",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "812794a15a902790ca8700a0ad2027f51caa748f",
      "candidate_info": {
        "commit_hash": "812794a15a902790ca8700a0ad2027f51caa748f",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/812794a15a902790ca8700a0ad2027f51caa748f",
        "files": [
          ".github/workflows/ci.yml",
          "airflow/providers/google/marketing_platform/hooks/analytics.py",
          "airflow/providers/google/marketing_platform/hooks/analytics_admin.py",
          "airflow/providers/google/marketing_platform/links/__init__.py",
          "airflow/providers/google/marketing_platform/links/analytics_admin.py",
          "airflow/providers/google/marketing_platform/operators/analytics.py",
          "airflow/providers/google/marketing_platform/operators/analytics_admin.py",
          "airflow/providers/google/provider.yaml",
          "docs/apache-airflow-providers-google/operators/marketing_platform/analytics.rst",
          "docs/apache-airflow-providers-google/operators/marketing_platform/analytics_admin.rst",
          "generated/provider_dependencies.json",
          "tests/providers/google/marketing_platform/hooks/test_analytics_admin.py",
          "tests/providers/google/marketing_platform/links/__init__.py",
          "tests/providers/google/marketing_platform/links/test_analytics_admin.py",
          "tests/providers/google/marketing_platform/operators/test_analytics_admin.py",
          "tests/system/providers/google/marketing_platform/example_analytics_admin.py"
        ],
        "message": "Implement Google Analytics Admin (GA4) operators (#36276)\n\n(cherry picked from commit f28643b7bdc90a61ec5bd12f8505772cd8c3bf7f)",
        "before_after_code_files": [
          "airflow/providers/google/marketing_platform/hooks/analytics.py||airflow/providers/google/marketing_platform/hooks/analytics.py",
          "airflow/providers/google/marketing_platform/hooks/analytics_admin.py||airflow/providers/google/marketing_platform/hooks/analytics_admin.py",
          "airflow/providers/google/marketing_platform/links/__init__.py||airflow/providers/google/marketing_platform/links/__init__.py",
          "airflow/providers/google/marketing_platform/links/analytics_admin.py||airflow/providers/google/marketing_platform/links/analytics_admin.py",
          "airflow/providers/google/marketing_platform/operators/analytics.py||airflow/providers/google/marketing_platform/operators/analytics.py",
          "airflow/providers/google/marketing_platform/operators/analytics_admin.py||airflow/providers/google/marketing_platform/operators/analytics_admin.py",
          "tests/providers/google/marketing_platform/hooks/test_analytics_admin.py||tests/providers/google/marketing_platform/hooks/test_analytics_admin.py",
          "tests/providers/google/marketing_platform/links/__init__.py||tests/providers/google/marketing_platform/links/__init__.py",
          "tests/providers/google/marketing_platform/links/test_analytics_admin.py||tests/providers/google/marketing_platform/links/test_analytics_admin.py",
          "tests/providers/google/marketing_platform/operators/test_analytics_admin.py||tests/providers/google/marketing_platform/operators/test_analytics_admin.py",
          "tests/system/providers/google/marketing_platform/example_analytics_admin.py||tests/system/providers/google/marketing_platform/example_analytics_admin.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/providers/google/marketing_platform/hooks/analytics.py||airflow/providers/google/marketing_platform/hooks/analytics.py": [
          "File: airflow/providers/google/marketing_platform/hooks/analytics.py -> airflow/providers/google/marketing_platform/hooks/analytics.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "17: # under the License.",
          "18: from __future__ import annotations",
          "20: from typing import Any",
          "22: from googleapiclient.discovery import Resource, build",
          "23: from googleapiclient.http import MediaFileUpload",
          "25: from airflow.providers.google.common.hooks.base_google import GoogleBaseHook",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "20: import warnings",
          "26: from airflow.exceptions import AirflowProviderDeprecationWarning",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "31:     def __init__(self, api_version: str = \"v3\", *args, **kwargs):",
          "32:         super().__init__(*args, **kwargs)",
          "33:         self.api_version = api_version",
          "34:         self._conn = None",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "35:         warnings.warn(",
          "36:             f\"The `{type(self).__name__}` class is deprecated, please use \"",
          "37:             f\"`GoogleAnalyticsAdminHook` instead.\",",
          "38:             AirflowProviderDeprecationWarning,",
          "39:             stacklevel=1,",
          "40:         )",
          "",
          "---------------"
        ],
        "airflow/providers/google/marketing_platform/hooks/analytics_admin.py||airflow/providers/google/marketing_platform/hooks/analytics_admin.py": [
          "File: airflow/providers/google/marketing_platform/hooks/analytics_admin.py -> airflow/providers/google/marketing_platform/hooks/analytics_admin.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: #",
          "2: # Licensed to the Apache Software Foundation (ASF) under one",
          "3: # or more contributor license agreements.  See the NOTICE file",
          "4: # distributed with this work for additional information",
          "5: # regarding copyright ownership.  The ASF licenses this file",
          "6: # to you under the Apache License, Version 2.0 (the",
          "7: # \"License\"); you may not use this file except in compliance",
          "8: # with the License.  You may obtain a copy of the License at",
          "9: #",
          "10: #   http://www.apache.org/licenses/LICENSE-2.0",
          "11: #",
          "12: # Unless required by applicable law or agreed to in writing,",
          "13: # software distributed under the License is distributed on an",
          "14: # \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
          "15: # KIND, either express or implied.  See the License for the",
          "16: # specific language governing permissions and limitations",
          "17: # under the License.",
          "18: \"\"\"",
          "19: Hooks for Google Analytics (GA4) Admin service.",
          "21: .. spelling:word-list::",
          "23:     DataStream",
          "24:     ListAccountsPager",
          "25:     ListGoogleAdsLinksPager",
          "26: \"\"\"",
          "27: from __future__ import annotations",
          "29: from typing import TYPE_CHECKING, Sequence",
          "31: from google.analytics.admin_v1beta import (",
          "32:     AnalyticsAdminServiceClient,",
          "33:     DataStream,",
          "34:     Property,",
          "35: )",
          "36: from google.api_core.gapic_v1.method import DEFAULT, _MethodDefault",
          "38: from airflow.providers.google.common.consts import CLIENT_INFO",
          "39: from airflow.providers.google.common.hooks.base_google import GoogleBaseHook",
          "41: if TYPE_CHECKING:",
          "42:     from google.analytics.admin_v1beta.services.analytics_admin_service.pagers import (",
          "43:         ListAccountsPager,",
          "44:         ListGoogleAdsLinksPager,",
          "45:     )",
          "46:     from google.api_core.retry import Retry",
          "49: class GoogleAnalyticsAdminHook(GoogleBaseHook):",
          "50:     \"\"\"Hook for Google Analytics 4 (GA4) Admin API.\"\"\"",
          "52:     def __init__(self, *args, **kwargs) -> None:",
          "53:         super().__init__(*args, **kwargs)",
          "54:         self._conn: AnalyticsAdminServiceClient | None = None",
          "56:     def get_conn(self) -> AnalyticsAdminServiceClient:",
          "57:         if not self._conn:",
          "58:             self._conn = AnalyticsAdminServiceClient(",
          "59:                 credentials=self.get_credentials(), client_info=CLIENT_INFO",
          "60:             )",
          "61:         return self._conn",
          "63:     def list_accounts(",
          "64:         self,",
          "65:         page_size: int | None = None,",
          "66:         page_token: str | None = None,",
          "67:         show_deleted: bool | None = None,",
          "68:         retry: Retry | _MethodDefault = DEFAULT,",
          "69:         timeout: float | None = None,",
          "70:         metadata: Sequence[tuple[str, str]] = (),",
          "71:     ) -> ListAccountsPager:",
          "72:         \"\"\"Get list of accounts in Google Analytics.",
          "74:         .. seealso::",
          "75:             For more details please check the client library documentation:",
          "76:             https://developers.google.com/analytics/devguides/config/admin/v1/rest/v1beta/accounts/list",
          "78:         :param page_size: Optional, number of results to return in the list.",
          "79:         :param page_token: Optional. The next_page_token value returned from a previous List request, if any.",
          "80:         :param show_deleted: Optional. Whether to include soft-deleted (ie: \"trashed\") Accounts in the results.",
          "81:         :param retry: Optional, a retry object used  to retry requests. If `None` is specified, requests",
          "82:             will not be retried.",
          "83:         :param timeout: Optional. The timeout for this request.",
          "84:         :param metadata: Optional. Strings which should be sent along with the request as metadata.",
          "86:         :returns: List of Google Analytics accounts.",
          "87:         \"\"\"",
          "88:         request = {\"page_size\": page_size, \"page_token\": page_token, \"show_deleted\": show_deleted}",
          "89:         client = self.get_conn()",
          "90:         return client.list_accounts(request=request, retry=retry, timeout=timeout, metadata=metadata)",
          "92:     def create_property(",
          "93:         self,",
          "94:         analytics_property: Property | dict,",
          "95:         retry: Retry | _MethodDefault = DEFAULT,",
          "96:         timeout: float | None = None,",
          "97:         metadata: Sequence[tuple[str, str]] = (),",
          "98:     ) -> Property:",
          "99:         \"\"\"Create Google Analytics property.",
          "101:         .. seealso::",
          "102:             For more details please check the client library documentation:",
          "103:             https://developers.google.com/analytics/devguides/config/admin/v1/rest/v1beta/properties/create",
          "105:         :param analytics_property: The property to create. Note: the supplied property must specify its",
          "106:             parent.",
          "107:         :param retry: Optional, a retry object used  to retry requests. If `None` is specified, requests",
          "108:             will not be retried.",
          "109:         :param timeout: Optional. The timeout for this request.",
          "110:         :param metadata: Optional. Strings which should be sent along with the request as metadata.",
          "112:         :returns: Created Google Analytics property.",
          "113:         \"\"\"",
          "114:         client = self.get_conn()",
          "115:         return client.create_property(",
          "116:             request={\"property\": analytics_property},",
          "117:             retry=retry,",
          "118:             timeout=timeout,",
          "119:             metadata=metadata,",
          "120:         )",
          "122:     def delete_property(",
          "123:         self,",
          "124:         property_id: str,",
          "125:         retry: Retry | _MethodDefault = DEFAULT,",
          "126:         timeout: float | None = None,",
          "127:         metadata: Sequence[tuple[str, str]] = (),",
          "128:     ) -> Property:",
          "129:         \"\"\"Soft delete Google Analytics property.",
          "131:         .. seealso::",
          "132:             For more details please check the client library documentation:",
          "133:             https://developers.google.com/analytics/devguides/config/admin/v1/rest/v1beta/properties/delete",
          "135:         :param property_id: ID of the Property to soft-delete. Format: properties/{property_id}.",
          "136:         :param retry: Optional, a retry object used  to retry requests. If `None` is specified, requests",
          "137:             will not be retried.",
          "138:         :param timeout: Optional. The timeout for this request.",
          "139:         :param metadata: Optional. Strings which should be sent along with the request as metadata.",
          "141:         :returns: Resource message representing Google Analytics property.",
          "142:         \"\"\"",
          "143:         client = self.get_conn()",
          "144:         request = {\"name\": f\"properties/{property_id}\"}",
          "145:         return client.delete_property(request=request, retry=retry, timeout=timeout, metadata=metadata)",
          "147:     def create_data_stream(",
          "148:         self,",
          "149:         property_id: str,",
          "150:         data_stream: DataStream | dict,",
          "151:         retry: Retry | _MethodDefault = DEFAULT,",
          "152:         timeout: float | None = None,",
          "153:         metadata: Sequence[tuple[str, str]] = (),",
          "154:     ) -> DataStream:",
          "155:         \"\"\"Create Google Analytics data stream.",
          "157:         .. seealso::",
          "158:             For more details please check the client library documentation:",
          "159:             https://developers.google.com/analytics/devguides/config/admin/v1/rest/v1beta/properties.dataStreams/create",
          "161:         :param property_id: ID of the parent property for the data stream.",
          "162:         :param data_stream: The data stream to create.",
          "163:         :param retry: Optional, a retry object used  to retry requests. If `None` is specified, requests",
          "164:             will not be retried.",
          "165:         :param timeout: Optional. The timeout for this request.",
          "166:         :param metadata: Optional. Strings which should be sent along with the request as metadata.",
          "168:         :returns: Created Google Analytics data stream.",
          "169:         \"\"\"",
          "170:         client = self.get_conn()",
          "171:         return client.create_data_stream(",
          "172:             request={\"parent\": f\"properties/{property_id}\", \"data_stream\": data_stream},",
          "173:             retry=retry,",
          "174:             timeout=timeout,",
          "175:             metadata=metadata,",
          "176:         )",
          "178:     def delete_data_stream(",
          "179:         self,",
          "180:         property_id: str,",
          "181:         data_stream_id: str,",
          "182:         retry: Retry | _MethodDefault = DEFAULT,",
          "183:         timeout: float | None = None,",
          "184:         metadata: Sequence[tuple[str, str]] = (),",
          "185:     ) -> None:",
          "186:         \"\"\"Delete Google Analytics data stream.",
          "188:         .. seealso::",
          "189:             For more details please check the client library documentation:",
          "190:             https://developers.google.com/analytics/devguides/config/admin/v1/rest/v1beta/properties.dataStreams/delete",
          "192:         :param property_id: ID of the parent property for the data stream.",
          "193:         :param data_stream_id: The data stream id to delete.",
          "194:         :param retry: Optional, a retry object used  to retry requests. If `None` is specified, requests",
          "195:             will not be retried.",
          "196:         :param timeout: Optional. The timeout for this request.",
          "197:         :param metadata: Optional. Strings which should be sent along with the request as metadata.",
          "198:         \"\"\"",
          "199:         client = self.get_conn()",
          "200:         return client.delete_data_stream(",
          "201:             request={\"name\": f\"properties/{property_id}/dataStreams/{data_stream_id}\"},",
          "202:             retry=retry,",
          "203:             timeout=timeout,",
          "204:             metadata=metadata,",
          "205:         )",
          "207:     def list_google_ads_links(",
          "208:         self,",
          "209:         property_id: str,",
          "210:         page_size: int | None = None,",
          "211:         page_token: str | None = None,",
          "212:         retry: Retry | _MethodDefault = DEFAULT,",
          "213:         timeout: float | None = None,",
          "214:         metadata: Sequence[tuple[str, str]] = (),",
          "215:     ) -> ListGoogleAdsLinksPager:",
          "216:         \"\"\"Get list of Google Ads links.",
          "218:         .. seealso::",
          "219:             For more details please check the client library documentation:",
          "220:             https://googleapis.dev/python/analyticsadmin/latest/admin_v1beta/analytics_admin_service.html#google.analytics.admin_v1beta.services.analytics_admin_service.AnalyticsAdminServiceAsyncClient.list_google_ads_links",
          "222:         :param property_id: ID of the parent property.",
          "223:         :param page_size: Optional, number of results to return in the list.",
          "224:         :param page_token: Optional. The next_page_token value returned from a previous List request, if any.",
          "225:         :param retry: Optional, a retry object used  to retry requests. If `None` is specified, requests",
          "226:             will not be retried.",
          "227:         :param timeout: Optional. The timeout for this request.",
          "228:         :param metadata: Optional. Strings which should be sent along with the request as metadata.",
          "230:         :returns: List of Google Analytics accounts.",
          "231:         \"\"\"",
          "232:         client = self.get_conn()",
          "233:         request = {\"parent\": f\"properties/{property_id}\", \"page_size\": page_size, \"page_token\": page_token}",
          "234:         return client.list_google_ads_links(request=request, retry=retry, timeout=timeout, metadata=metadata)",
          "",
          "---------------"
        ],
        "airflow/providers/google/marketing_platform/links/__init__.py||airflow/providers/google/marketing_platform/links/__init__.py": [
          "File: airflow/providers/google/marketing_platform/links/__init__.py -> airflow/providers/google/marketing_platform/links/__init__.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: # Licensed to the Apache Software Foundation (ASF) under one",
          "2: # or more contributor license agreements.  See the NOTICE file",
          "3: # distributed with this work for additional information",
          "4: # regarding copyright ownership.  The ASF licenses this file",
          "5: # to you under the Apache License, Version 2.0 (the",
          "6: # \"License\"); you may not use this file except in compliance",
          "7: # with the License.  You may obtain a copy of the License at",
          "8: #",
          "9: #   http://www.apache.org/licenses/LICENSE-2.0",
          "10: #",
          "11: # Unless required by applicable law or agreed to in writing,",
          "12: # software distributed under the License is distributed on an",
          "13: # \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
          "14: # KIND, either express or implied.  See the License for the",
          "15: # specific language governing permissions and limitations",
          "16: # under the License.",
          "",
          "---------------"
        ],
        "airflow/providers/google/marketing_platform/links/analytics_admin.py||airflow/providers/google/marketing_platform/links/analytics_admin.py": [
          "File: airflow/providers/google/marketing_platform/links/analytics_admin.py -> airflow/providers/google/marketing_platform/links/analytics_admin.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: # Licensed to the Apache Software Foundation (ASF) under one",
          "2: # or more contributor license agreements.  See the NOTICE file",
          "3: # distributed with this work for additional information",
          "4: # regarding copyright ownership.  The ASF licenses this file",
          "5: # to you under the Apache License, Version 2.0 (the",
          "6: # \"License\"); you may not use this file except in compliance",
          "7: # with the License.  You may obtain a copy of the License at",
          "8: #",
          "9: #   http://www.apache.org/licenses/LICENSE-2.0",
          "10: #",
          "11: # Unless required by applicable law or agreed to in writing,",
          "12: # software distributed under the License is distributed on an",
          "13: # \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
          "14: # KIND, either express or implied.  See the License for the",
          "15: # specific language governing permissions and limitations",
          "16: # under the License.",
          "17: from __future__ import annotations",
          "19: from typing import TYPE_CHECKING, ClassVar",
          "21: from airflow.models import BaseOperator, BaseOperatorLink, XCom",
          "23: if TYPE_CHECKING:",
          "24:     from airflow.models.taskinstancekey import TaskInstanceKey",
          "25:     from airflow.utils.context import Context",
          "28: BASE_LINK = \"https://analytics.google.com/analytics/web/\"",
          "31: class GoogleAnalyticsBaseLink(BaseOperatorLink):",
          "32:     \"\"\"Base class for Google Analytics links.",
          "34:     :meta private:",
          "35:     \"\"\"",
          "37:     name: ClassVar[str]",
          "38:     key: ClassVar[str]",
          "39:     format_str: ClassVar[str]",
          "41:     def get_link(self, operator: BaseOperator, *, ti_key: TaskInstanceKey) -> str:",
          "42:         if conf := XCom.get_value(key=self.key, ti_key=ti_key):",
          "43:             res = BASE_LINK + \"#/\" + self.format_str.format(**conf)",
          "44:             return res",
          "45:         return \"\"",
          "48: class GoogleAnalyticsPropertyLink(GoogleAnalyticsBaseLink):",
          "49:     \"\"\"Helper class for constructing Google Analytics Property Link.\"\"\"",
          "51:     name = \"Data Analytics Property\"",
          "52:     key = \"data_analytics_property\"",
          "53:     format_str = \"p{property_id}/\"",
          "55:     @staticmethod",
          "56:     def persist(",
          "57:         context: Context,",
          "58:         task_instance: BaseOperator,",
          "59:         property_id: str,",
          "60:     ):",
          "61:         task_instance.xcom_push(",
          "62:             context,",
          "63:             key=GoogleAnalyticsPropertyLink.key,",
          "64:             value={\"property_id\": property_id},",
          "65:         )",
          "",
          "---------------"
        ],
        "airflow/providers/google/marketing_platform/operators/analytics.py||airflow/providers/google/marketing_platform/operators/analytics.py": [
          "File: airflow/providers/google/marketing_platform/operators/analytics.py -> airflow/providers/google/marketing_platform/operators/analytics.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "19: from __future__ import annotations",
          "21: import csv",
          "22: from tempfile import NamedTemporaryFile",
          "23: from typing import TYPE_CHECKING, Any, Sequence",
          "25: from airflow.models import BaseOperator",
          "26: from airflow.providers.google.cloud.hooks.gcs import GCSHook",
          "27: from airflow.providers.google.marketing_platform.hooks.analytics import GoogleAnalyticsHook",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "22: import warnings",
          "26: from airflow.exceptions import AirflowProviderDeprecationWarning",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "34:     \"\"\"",
          "35:     Lists all accounts to which the user has access.",
          "37:     .. seealso::",
          "38:         Check official API docs:",
          "39:         https://developers.google.com/analytics/devguides/config/mgmt/v3/mgmtReference/management/accounts/list",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "39:     .. seealso::",
          "40:         This operator is deprecated, please use",
          "41:         :class:`~airflow.providers.google.marketing_platform.operators.analytics_admin.GoogleAnalyticsAdminListAccountsOperator`:",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "70:         impersonation_chain: str | Sequence[str] | None = None,",
          "72:     ) -> None:",
          "73:         super().__init__(**kwargs)",
          "75:         self.api_version = api_version",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "79:         warnings.warn(",
          "80:             f\"The `{type(self).__name__}` operator is deprecated, please use \"",
          "81:             f\"`GoogleAnalyticsAdminListAccountsOperator` instead.\",",
          "82:             AirflowProviderDeprecationWarning,",
          "83:             stacklevel=1,",
          "84:         )",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "90:     \"\"\"",
          "91:     Returns a web property-Google Ads link to which the user has access.",
          "93:     .. seealso::",
          "94:         Check official API docs:",
          "95:         https://developers.google.com/analytics/devguides/config/mgmt/v3/mgmtReference/management/webPropertyAdWordsLinks/get",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "106:     .. seealso::",
          "107:         This operator is deprecated, please use",
          "108:         :class:`~airflow.providers.google.marketing_platform.operators.analytics_admin.GoogleAnalyticsAdminGetGoogleAdsLinkOperator`:",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "133:     ):",
          "134:         super().__init__(**kwargs)",
          "136:         self.account_id = account_id",
          "137:         self.web_property_ad_words_link_id = web_property_ad_words_link_id",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "152:         warnings.warn(",
          "153:             f\"The `{type(self).__name__}` operator is deprecated, please use \"",
          "154:             f\"`GoogleAnalyticsAdminGetGoogleAdsLinkOperator` instead.\",",
          "155:             AirflowProviderDeprecationWarning,",
          "156:             stacklevel=1,",
          "157:         )",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "158:     \"\"\"",
          "159:     Lists webProperty-Google Ads links for a given web property.",
          "161:     .. seealso::",
          "162:         Check official API docs:",
          "163:         https://developers.google.com/analytics/devguides/config/mgmt/v3/mgmtReference/management/webPropertyAdWordsLinks/list#http-request",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "184:     .. seealso::",
          "185:         This operator is deprecated, please use",
          "186:         :class:`~airflow.providers.google.marketing_platform.operators.analytics_admin.GoogleAnalyticsAdminListGoogleAdsLinksOperator`:",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "198:     ) -> None:",
          "199:         super().__init__(**kwargs)",
          "201:         self.account_id = account_id",
          "202:         self.web_property_id = web_property_id",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "227:         warnings.warn(",
          "228:             f\"The `{type(self).__name__}` operator is deprecated, please use \"",
          "229:             f\"`GoogleAnalyticsAdminListGoogleAdsLinksOperator` instead.\",",
          "230:             AirflowProviderDeprecationWarning,",
          "231:             stacklevel=1,",
          "232:         )",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "221:     \"\"\"",
          "222:     Take a file from Cloud Storage and uploads it to GA via data import API.",
          "224:     :param storage_bucket: The Google cloud storage bucket where the file is stored.",
          "225:     :param storage_name_object: The name of the object in the desired Google cloud",
          "226:           storage bucket. (templated) If the destination points to an existing",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "257:     .. seealso::",
          "258:         This operator is deprecated, please use",
          "259:         :class:`~airflow.providers.google.marketing_platform.operators.analytics_admin.GoogleAnalyticsAdminCreateDataStreamOperator`:",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "266:         impersonation_chain: str | Sequence[str] | None = None,",
          "268:     ) -> None:",
          "269:         super().__init__(**kwargs)",
          "270:         self.storage_bucket = storage_bucket",
          "271:         self.storage_name_object = storage_name_object",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "306:         warnings.warn(",
          "307:             f\"The `{type(self).__name__}` operator is deprecated, please use \"",
          "308:             f\"`GoogleAnalyticsAdminCreateDataStreamOperator` instead.\",",
          "309:             AirflowProviderDeprecationWarning,",
          "310:             stacklevel=1,",
          "311:         )",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "317:     \"\"\"",
          "318:     Deletes previous GA uploads to leave the latest file to control the size of the Data Set Quota.",
          "320:     :param account_id: The GA account Id (long) to which the data upload belongs.",
          "321:     :param web_property_id: The web property UA-string associated with the upload.",
          "322:     :param custom_data_source_id: The id to which the data import belongs.",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "363:     .. seealso::",
          "364:         This operator is deprecated, please use",
          "365:         :class:`~airflow.providers.google.marketing_platform.operators.analytics_admin.GoogleAnalyticsAdminDeleteDataStreamOperator`:",
          "",
          "---------------",
          "--- Hunk 11 ---",
          "[Context before]",
          "348:         impersonation_chain: str | Sequence[str] | None = None,",
          "350:     ) -> None:",
          "351:         super().__init__(**kwargs)",
          "353:         self.account_id = account_id",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "398:         warnings.warn(",
          "399:             f\"The `{type(self).__name__}` operator is deprecated, please use \"",
          "400:             f\"`GoogleAnalyticsAdminDeleteDataStreamOperator` instead.\",",
          "401:             AirflowProviderDeprecationWarning,",
          "402:             stacklevel=1,",
          "403:         )",
          "",
          "---------------"
        ],
        "airflow/providers/google/marketing_platform/operators/analytics_admin.py||airflow/providers/google/marketing_platform/operators/analytics_admin.py": [
          "File: airflow/providers/google/marketing_platform/operators/analytics_admin.py -> airflow/providers/google/marketing_platform/operators/analytics_admin.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: #",
          "2: # Licensed to the Apache Software Foundation (ASF) under one",
          "3: # or more contributor license agreements.  See the NOTICE file",
          "4: # distributed with this work for additional information",
          "5: # regarding copyright ownership.  The ASF licenses this file",
          "6: # to you under the Apache License, Version 2.0 (the",
          "7: # \"License\"); you may not use this file except in compliance",
          "8: # with the License.  You may obtain a copy of the License at",
          "9: #",
          "10: #   http://www.apache.org/licenses/LICENSE-2.0",
          "11: #",
          "12: # Unless required by applicable law or agreed to in writing,",
          "13: # software distributed under the License is distributed on an",
          "14: # \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
          "15: # KIND, either express or implied.  See the License for the",
          "16: # specific language governing permissions and limitations",
          "17: # under the License.",
          "18: \"\"\"This module contains Google Analytics 4 (GA4) operators.\"\"\"",
          "19: from __future__ import annotations",
          "21: from typing import TYPE_CHECKING, Any, Sequence",
          "23: from google.analytics.admin_v1beta import (",
          "24:     Account,",
          "25:     DataStream,",
          "26:     GoogleAdsLink,",
          "27:     Property,",
          "28: )",
          "29: from google.api_core.gapic_v1.method import DEFAULT, _MethodDefault",
          "31: from airflow.exceptions import AirflowNotFoundException",
          "32: from airflow.providers.google.cloud.operators.cloud_base import GoogleCloudBaseOperator",
          "33: from airflow.providers.google.marketing_platform.hooks.analytics_admin import GoogleAnalyticsAdminHook",
          "34: from airflow.providers.google.marketing_platform.links.analytics_admin import GoogleAnalyticsPropertyLink",
          "36: if TYPE_CHECKING:",
          "37:     from google.api_core.retry import Retry",
          "38:     from google.protobuf.message import Message",
          "40:     from airflow.utils.context import Context",
          "43: class GoogleAnalyticsAdminListAccountsOperator(GoogleCloudBaseOperator):",
          "44:     \"\"\"",
          "45:     Lists all accounts to which the user has access.",
          "47:     .. seealso::",
          "48:         For more information on how to use this operator, take a look at the guide:",
          "49:         :ref:`howto/operator:GoogleAnalyticsAdminListAccountsOperator`",
          "51:     :param page_size: Optional, number of results to return in the list.",
          "52:     :param page_token: Optional. The next_page_token value returned from a previous List request, if any.",
          "53:     :param show_deleted: Optional. Whether to include soft-deleted (ie: \"trashed\") Accounts in the results.",
          "54:     :param retry: Optional, a retry object used  to retry requests. If `None` is specified, requests",
          "55:             will not be retried.",
          "56:     :param timeout: Optional. The timeout for this request.",
          "57:     :param metadata: Optional. Strings which should be sent along with the request as metadata.",
          "58:     :param gcp_conn_id: The connection ID to use when fetching connection info.",
          "59:     :param impersonation_chain: Optional. Service account to impersonate using short-term",
          "60:         credentials, or chained list of accounts required to get the access_token",
          "61:         of the last account in the list, which will be impersonated in the request.",
          "62:         If set as a string, the account must grant the originating account",
          "63:         the Service Account Token Creator IAM role.",
          "64:         If set as a sequence, the identities from the list must grant",
          "65:         Service Account Token Creator IAM role to the directly preceding identity, with first",
          "66:         account from the list granting this role to the originating account (templated).",
          "67:     \"\"\"",
          "69:     template_fields: Sequence[str] = (",
          "70:         \"gcp_conn_id\",",
          "71:         \"impersonation_chain\",",
          "72:         \"page_size\",",
          "73:         \"page_token\",",
          "74:     )",
          "76:     def __init__(",
          "77:         self,",
          "79:         page_size: int | None = None,",
          "80:         page_token: str | None = None,",
          "81:         show_deleted: bool | None = None,",
          "82:         retry: Retry | _MethodDefault = DEFAULT,",
          "83:         timeout: float | None = None,",
          "84:         metadata: Sequence[tuple[str, str]] = (),",
          "85:         gcp_conn_id: str = \"google_cloud_default\",",
          "86:         impersonation_chain: str | Sequence[str] | None = None,",
          "88:     ) -> None:",
          "89:         super().__init__(**kwargs)",
          "90:         self.page_size = page_size",
          "91:         self.page_token = page_token",
          "92:         self.show_deleted = show_deleted",
          "93:         self.retry = retry",
          "94:         self.timeout = timeout",
          "95:         self.metadata = metadata",
          "96:         self.gcp_conn_id = gcp_conn_id",
          "97:         self.impersonation_chain = impersonation_chain",
          "99:     def execute(",
          "100:         self,",
          "101:         context: Context,",
          "102:     ) -> Sequence[Message]:",
          "103:         hook = GoogleAnalyticsAdminHook(",
          "104:             gcp_conn_id=self.gcp_conn_id,",
          "105:             impersonation_chain=self.impersonation_chain,",
          "106:         )",
          "107:         self.log.info(",
          "108:             \"Requesting list of Google Analytics accounts. \"",
          "109:             f\"Page size: {self.page_size}, page token: {self.page_token}\"",
          "110:         )",
          "111:         accounts = hook.list_accounts(",
          "112:             page_size=self.page_size,",
          "113:             page_token=self.page_token,",
          "114:             show_deleted=self.show_deleted,",
          "115:             retry=self.retry,",
          "116:             timeout=self.timeout,",
          "117:             metadata=self.metadata,",
          "118:         )",
          "119:         accounts_list: Sequence[Message] = [Account.to_dict(item) for item in accounts]",
          "120:         n = len(accounts_list)",
          "121:         self.log.info(\"Successful request. Retrieved %s item%s.\", n, \"s\" if n > 1 else \"\")",
          "122:         return accounts_list",
          "125: class GoogleAnalyticsAdminCreatePropertyOperator(GoogleCloudBaseOperator):",
          "126:     \"\"\"",
          "127:     Creates property.",
          "129:     .. seealso::",
          "130:         For more information on how to use this operator, take a look at the guide:",
          "131:         :ref:`howto/operator:GoogleAnalyticsAdminCreatePropertyOperator`",
          "133:     :param analytics_property: The property to create. Note: the supplied property must specify its parent.",
          "134:         For more details see: https://developers.google.com/analytics/devguides/config/admin/v1/rest/v1beta/properties#Property",
          "135:     :param retry: Optional, a retry object used  to retry requests. If `None` is specified, requests",
          "136:         will not be retried.",
          "137:     :param timeout: Optional. The timeout for this request.",
          "138:     :param metadata: Optional. Strings which should be sent along with the request as metadata.",
          "139:     :param gcp_conn_id: The connection ID to use when fetching connection info.",
          "140:     :param impersonation_chain: Optional. Service account to impersonate using short-term",
          "141:         credentials, or chained list of accounts required to get the access_token",
          "142:         of the last account in the list, which will be impersonated in the request.",
          "143:         If set as a string, the account must grant the originating account",
          "144:         the Service Account Token Creator IAM role.",
          "145:         If set as a sequence, the identities from the list must grant",
          "146:         Service Account Token Creator IAM role to the directly preceding identity, with first",
          "147:         account from the list granting this role to the originating account (templated).",
          "148:     \"\"\"",
          "150:     template_fields: Sequence[str] = (",
          "151:         \"gcp_conn_id\",",
          "152:         \"impersonation_chain\",",
          "153:         \"analytics_property\",",
          "154:     )",
          "155:     operator_extra_links = (GoogleAnalyticsPropertyLink(),)",
          "157:     def __init__(",
          "158:         self,",
          "160:         analytics_property: Property | dict[str, Any],",
          "161:         retry: Retry | _MethodDefault = DEFAULT,",
          "162:         timeout: float | None = None,",
          "163:         metadata: Sequence[tuple[str, str]] = (),",
          "164:         gcp_conn_id: str = \"google_cloud_default\",",
          "165:         impersonation_chain: str | Sequence[str] | None = None,",
          "167:     ) -> None:",
          "168:         super().__init__(**kwargs)",
          "169:         self.analytics_property = analytics_property",
          "170:         self.retry = retry",
          "171:         self.timeout = timeout",
          "172:         self.metadata = metadata",
          "173:         self.gcp_conn_id = gcp_conn_id",
          "174:         self.impersonation_chain = impersonation_chain",
          "176:     def execute(",
          "177:         self,",
          "178:         context: Context,",
          "179:     ) -> Message:",
          "180:         hook = GoogleAnalyticsAdminHook(",
          "181:             gcp_conn_id=self.gcp_conn_id,",
          "182:             impersonation_chain=self.impersonation_chain,",
          "183:         )",
          "184:         self.log.info(\"Creating a Google Analytics property.\")",
          "185:         prop = hook.create_property(",
          "186:             analytics_property=self.analytics_property,",
          "187:             retry=self.retry,",
          "188:             timeout=self.timeout,",
          "189:             metadata=self.metadata,",
          "190:         )",
          "191:         self.log.info(\"The Google Analytics property %s was created successfully.\", prop.name)",
          "192:         GoogleAnalyticsPropertyLink.persist(",
          "193:             context=context,",
          "194:             task_instance=self,",
          "195:             property_id=prop.name.lstrip(\"properties/\"),",
          "196:         )",
          "198:         return Property.to_dict(prop)",
          "201: class GoogleAnalyticsAdminDeletePropertyOperator(GoogleCloudBaseOperator):",
          "202:     \"\"\"",
          "203:     Soft-delete property.",
          "205:     .. seealso::",
          "206:         For more information on how to use this operator, take a look at the guide:",
          "207:         :ref:`howto/operator:GoogleAnalyticsAdminDeletePropertyOperator`",
          "209:     :param property_id: The id of the Property to soft-delete.",
          "210:     :param retry: Optional, a retry object used  to retry requests. If `None` is specified, requests",
          "211:         will not be retried.",
          "212:     :param timeout: Optional. The timeout for this request.",
          "213:     :param metadata: Optional. Strings which should be sent along with the request as metadata.",
          "214:     :param gcp_conn_id: The connection ID to use when fetching connection info.",
          "215:     :param impersonation_chain: Optional. Service account to impersonate using short-term",
          "216:         credentials, or chained list of accounts required to get the access_token",
          "217:         of the last account in the list, which will be impersonated in the request.",
          "218:         If set as a string, the account must grant the originating account",
          "219:         the Service Account Token Creator IAM role.",
          "220:         If set as a sequence, the identities from the list must grant",
          "221:         Service Account Token Creator IAM role to the directly preceding identity, with first",
          "222:         account from the list granting this role to the originating account (templated).",
          "223:     \"\"\"",
          "225:     template_fields: Sequence[str] = (",
          "226:         \"gcp_conn_id\",",
          "227:         \"impersonation_chain\",",
          "228:         \"property_id\",",
          "229:     )",
          "231:     def __init__(",
          "232:         self,",
          "234:         property_id: str,",
          "235:         retry: Retry | _MethodDefault = DEFAULT,",
          "236:         timeout: float | None = None,",
          "237:         metadata: Sequence[tuple[str, str]] = (),",
          "238:         gcp_conn_id: str = \"google_cloud_default\",",
          "239:         impersonation_chain: str | Sequence[str] | None = None,",
          "241:     ) -> None:",
          "242:         super().__init__(**kwargs)",
          "243:         self.property_id = property_id",
          "244:         self.retry = retry",
          "245:         self.timeout = timeout",
          "246:         self.metadata = metadata",
          "247:         self.gcp_conn_id = gcp_conn_id",
          "248:         self.impersonation_chain = impersonation_chain",
          "250:     def execute(",
          "251:         self,",
          "252:         context: Context,",
          "253:     ) -> Message:",
          "254:         hook = GoogleAnalyticsAdminHook(",
          "255:             gcp_conn_id=self.gcp_conn_id,",
          "256:             impersonation_chain=self.impersonation_chain,",
          "257:         )",
          "258:         self.log.info(\"Deleting a Google Analytics property.\")",
          "259:         prop = hook.delete_property(",
          "260:             property_id=self.property_id,",
          "261:             retry=self.retry,",
          "262:             timeout=self.timeout,",
          "263:             metadata=self.metadata,",
          "264:         )",
          "265:         self.log.info(\"The Google Analytics property %s was soft-deleted successfully.\", prop.name)",
          "266:         return Property.to_dict(prop)",
          "269: class GoogleAnalyticsAdminCreateDataStreamOperator(GoogleCloudBaseOperator):",
          "270:     \"\"\"",
          "271:     Creates Data stream.",
          "273:     .. seealso::",
          "274:         For more information on how to use this operator, take a look at the guide:",
          "275:         :ref:`howto/operator:GoogleAnalyticsAdminCreateDataStreamOperator`",
          "277:     :param property_id: ID of the parent property for the data stream.",
          "278:     :param data_stream: The data stream to create.",
          "279:         For more details see: https://developers.google.com/analytics/devguides/config/admin/v1/rest/v1beta/properties.dataStreams#DataStream",
          "280:     :param retry: Optional, a retry object used  to retry requests. If `None` is specified, requests",
          "281:         will not be retried.",
          "282:     :param timeout: Optional. The timeout for this request.",
          "283:     :param metadata: Optional. Strings which should be sent along with the request as metadata.",
          "284:     :param gcp_conn_id: The connection ID to use when fetching connection info.",
          "285:     :param impersonation_chain: Optional. Service account to impersonate using short-term",
          "286:         credentials, or chained list of accounts required to get the access_token",
          "287:         of the last account in the list, which will be impersonated in the request.",
          "288:         If set as a string, the account must grant the originating account",
          "289:         the Service Account Token Creator IAM role.",
          "290:         If set as a sequence, the identities from the list must grant",
          "291:         Service Account Token Creator IAM role to the directly preceding identity, with first",
          "292:         account from the list granting this role to the originating account (templated).",
          "293:     \"\"\"",
          "295:     template_fields: Sequence[str] = (",
          "296:         \"gcp_conn_id\",",
          "297:         \"impersonation_chain\",",
          "298:         \"property_id\",",
          "299:         \"data_stream\",",
          "300:     )",
          "302:     def __init__(",
          "303:         self,",
          "305:         property_id: str,",
          "306:         data_stream: DataStream | dict,",
          "307:         retry: Retry | _MethodDefault = DEFAULT,",
          "308:         timeout: float | None = None,",
          "309:         metadata: Sequence[tuple[str, str]] = (),",
          "310:         gcp_conn_id: str = \"google_cloud_default\",",
          "311:         impersonation_chain: str | Sequence[str] | None = None,",
          "313:     ) -> None:",
          "314:         super().__init__(**kwargs)",
          "315:         self.property_id = property_id",
          "316:         self.data_stream = data_stream",
          "317:         self.retry = retry",
          "318:         self.timeout = timeout",
          "319:         self.metadata = metadata",
          "320:         self.gcp_conn_id = gcp_conn_id",
          "321:         self.impersonation_chain = impersonation_chain",
          "323:     def execute(",
          "324:         self,",
          "325:         context: Context,",
          "326:     ) -> Message:",
          "327:         hook = GoogleAnalyticsAdminHook(",
          "328:             gcp_conn_id=self.gcp_conn_id,",
          "329:             impersonation_chain=self.impersonation_chain,",
          "330:         )",
          "331:         self.log.info(\"Creating a Google Analytics data stream.\")",
          "332:         data_stream = hook.create_data_stream(",
          "333:             property_id=self.property_id,",
          "334:             data_stream=self.data_stream,",
          "335:             retry=self.retry,",
          "336:             timeout=self.timeout,",
          "337:             metadata=self.metadata,",
          "338:         )",
          "339:         self.log.info(\"The Google Analytics data stream %s was created successfully.\", data_stream.name)",
          "340:         return DataStream.to_dict(data_stream)",
          "343: class GoogleAnalyticsAdminDeleteDataStreamOperator(GoogleCloudBaseOperator):",
          "344:     \"\"\"",
          "345:     Deletes Data stream.",
          "347:     .. seealso::",
          "348:         For more information on how to use this operator, take a look at the guide:",
          "349:         :ref:`howto/operator:GoogleAnalyticsAdminDeleteDataStreamOperator`",
          "351:     :param property_id: ID of the property which is parent for the data stream.",
          "352:     :param data_stream_id: ID of the data stream to delete.",
          "353:     :param retry: Optional, a retry object used  to retry requests. If `None` is specified, requests",
          "354:         will not be retried.",
          "355:     :param timeout: Optional. The timeout for this request.",
          "356:     :param metadata: Optional. Strings which should be sent along with the request as metadata.",
          "357:     :param gcp_conn_id: The connection ID to use when fetching connection info.",
          "358:     :param impersonation_chain: Optional. Service account to impersonate using short-term",
          "359:         credentials, or chained list of accounts required to get the access_token",
          "360:         of the last account in the list, which will be impersonated in the request.",
          "361:         If set as a string, the account must grant the originating account",
          "362:         the Service Account Token Creator IAM role.",
          "363:         If set as a sequence, the identities from the list must grant",
          "364:         Service Account Token Creator IAM role to the directly preceding identity, with first",
          "365:         account from the list granting this role to the originating account (templated).",
          "366:     \"\"\"",
          "368:     template_fields: Sequence[str] = (",
          "369:         \"gcp_conn_id\",",
          "370:         \"impersonation_chain\",",
          "371:         \"property_id\",",
          "372:         \"data_stream_id\",",
          "373:     )",
          "375:     def __init__(",
          "376:         self,",
          "378:         property_id: str,",
          "379:         data_stream_id: str,",
          "380:         retry: Retry | _MethodDefault = DEFAULT,",
          "381:         timeout: float | None = None,",
          "382:         metadata: Sequence[tuple[str, str]] = (),",
          "383:         gcp_conn_id: str = \"google_cloud_default\",",
          "384:         impersonation_chain: str | Sequence[str] | None = None,",
          "386:     ) -> None:",
          "387:         super().__init__(**kwargs)",
          "388:         self.property_id = property_id",
          "389:         self.data_stream_id = data_stream_id",
          "390:         self.retry = retry",
          "391:         self.timeout = timeout",
          "392:         self.metadata = metadata",
          "393:         self.gcp_conn_id = gcp_conn_id",
          "394:         self.impersonation_chain = impersonation_chain",
          "396:     def execute(",
          "397:         self,",
          "398:         context: Context,",
          "399:     ) -> None:",
          "400:         hook = GoogleAnalyticsAdminHook(",
          "401:             gcp_conn_id=self.gcp_conn_id,",
          "402:             impersonation_chain=self.impersonation_chain,",
          "403:         )",
          "404:         self.log.info(\"Deleting a Google Analytics data stream (id %s).\", self.data_stream_id)",
          "405:         hook.delete_data_stream(",
          "406:             property_id=self.property_id,",
          "407:             data_stream_id=self.data_stream_id,",
          "408:             retry=self.retry,",
          "409:             timeout=self.timeout,",
          "410:             metadata=self.metadata,",
          "411:         )",
          "412:         self.log.info(\"The Google Analytics data stream was deleted successfully.\")",
          "413:         return None",
          "416: class GoogleAnalyticsAdminListGoogleAdsLinksOperator(GoogleCloudBaseOperator):",
          "417:     \"\"\"",
          "418:     Lists all Google Ads links associated with a given property.",
          "420:     .. seealso::",
          "421:         For more information on how to use this operator, take a look at the guide:",
          "422:         :ref:`howto/operator:GoogleAnalyticsAdminListGoogleAdsLinksOperator`",
          "424:     :param property_id: ID of the parent property.",
          "425:     :param page_size: Optional, number of results to return in the list.",
          "426:     :param page_token: Optional. The next_page_token value returned from a previous List request, if any.",
          "427:     :param retry: Optional, a retry object used  to retry requests. If `None` is specified, requests",
          "428:         will not be retried.",
          "429:     :param timeout: Optional. The timeout for this request.",
          "430:     :param metadata: Optional. Strings which should be sent along with the request as metadata.",
          "431:     :param gcp_conn_id: The connection ID to use when fetching connection info.",
          "432:     :param impersonation_chain: Optional. Service account to impersonate using short-term",
          "433:         credentials, or chained list of accounts required to get the access_token",
          "434:         of the last account in the list, which will be impersonated in the request.",
          "435:         If set as a string, the account must grant the originating account",
          "436:         the Service Account Token Creator IAM role.",
          "437:         If set as a sequence, the identities from the list must grant",
          "438:         Service Account Token Creator IAM role to the directly preceding identity, with first",
          "439:         account from the list granting this role to the originating account (templated).",
          "440:     \"\"\"",
          "442:     template_fields: Sequence[str] = (",
          "443:         \"gcp_conn_id\",",
          "444:         \"impersonation_chain\",",
          "445:         \"property_id\",",
          "446:         \"page_size\",",
          "447:         \"page_token\",",
          "448:     )",
          "450:     def __init__(",
          "451:         self,",
          "453:         property_id: str,",
          "454:         page_size: int | None = None,",
          "455:         page_token: str | None = None,",
          "456:         retry: Retry | _MethodDefault = DEFAULT,",
          "457:         timeout: float | None = None,",
          "458:         metadata: Sequence[tuple[str, str]] = (),",
          "459:         gcp_conn_id: str = \"google_cloud_default\",",
          "460:         impersonation_chain: str | Sequence[str] | None = None,",
          "462:     ) -> None:",
          "463:         super().__init__(**kwargs)",
          "464:         self.property_id = property_id",
          "465:         self.page_size = page_size",
          "466:         self.page_token = page_token",
          "467:         self.retry = retry",
          "468:         self.timeout = timeout",
          "469:         self.metadata = metadata",
          "470:         self.gcp_conn_id = gcp_conn_id",
          "471:         self.impersonation_chain = impersonation_chain",
          "473:     def execute(",
          "474:         self,",
          "475:         context: Context,",
          "476:     ) -> Sequence[Message]:",
          "477:         hook = GoogleAnalyticsAdminHook(",
          "478:             gcp_conn_id=self.gcp_conn_id,",
          "479:             impersonation_chain=self.impersonation_chain,",
          "480:         )",
          "481:         self.log.info(",
          "482:             \"Requesting list of Google Ads links accounts for the property_id %s, \"",
          "483:             \"page size %s, page token %s\",",
          "484:             self.property_id,",
          "485:             self.page_size,",
          "486:             self.page_token,",
          "487:         )",
          "488:         google_ads_links = hook.list_google_ads_links(",
          "489:             property_id=self.property_id,",
          "490:             page_size=self.page_size,",
          "491:             page_token=self.page_token,",
          "492:             retry=self.retry,",
          "493:             timeout=self.timeout,",
          "494:             metadata=self.metadata,",
          "495:         )",
          "496:         ads_links_list: Sequence[Message] = [GoogleAdsLink.to_dict(item) for item in google_ads_links]",
          "497:         n = len(ads_links_list)",
          "498:         self.log.info(\"Successful request. Retrieved %s item%s.\", n, \"s\" if n > 1 else \"\")",
          "499:         return ads_links_list",
          "502: class GoogleAnalyticsAdminGetGoogleAdsLinkOperator(GoogleCloudBaseOperator):",
          "503:     \"\"\"",
          "504:     Gets a Google Ads link associated with a given property.",
          "506:     .. seealso::",
          "507:         For more information on how to use this operator, take a look at the guide:",
          "508:         :ref:`howto/operator:GoogleAnalyticsAdminGetGoogleAdsLinkOperator`",
          "510:     :param property_id: Parent property id.",
          "511:     :param google_ads_link_id: Google Ads link id.",
          "512:     :param retry: Optional, a retry object used  to retry requests. If `None` is specified, requests",
          "513:         will not be retried.",
          "514:     :param timeout: Optional. The timeout for this request.",
          "515:     :param metadata: Optional. Strings which should be sent along with the request as metadata.",
          "516:     :param gcp_conn_id: The connection ID to use when fetching connection info.",
          "517:     :param impersonation_chain: Optional. Service account to impersonate using short-term",
          "518:         credentials, or chained list of accounts required to get the access_token",
          "519:         of the last account in the list, which will be impersonated in the request.",
          "520:         If set as a string, the account must grant the originating account",
          "521:         the Service Account Token Creator IAM role.",
          "522:         If set as a sequence, the identities from the list must grant",
          "523:         Service Account Token Creator IAM role to the directly preceding identity, with first",
          "524:         account from the list granting this role to the originating account (templated).",
          "525:     \"\"\"",
          "527:     template_fields: Sequence[str] = (",
          "528:         \"gcp_conn_id\",",
          "529:         \"impersonation_chain\",",
          "530:         \"google_ads_link_id\",",
          "531:     )",
          "533:     def __init__(",
          "534:         self,",
          "536:         property_id: str,",
          "537:         google_ads_link_id: str,",
          "538:         retry: Retry | _MethodDefault = DEFAULT,",
          "539:         timeout: float | None = None,",
          "540:         metadata: Sequence[tuple[str, str]] = (),",
          "541:         gcp_conn_id: str = \"google_cloud_default\",",
          "542:         impersonation_chain: str | Sequence[str] | None = None,",
          "544:     ) -> None:",
          "545:         super().__init__(**kwargs)",
          "546:         self.property_id = property_id",
          "547:         self.google_ads_link_id = google_ads_link_id",
          "548:         self.retry = retry",
          "549:         self.timeout = timeout",
          "550:         self.metadata = metadata",
          "551:         self.gcp_conn_id = gcp_conn_id",
          "552:         self.impersonation_chain = impersonation_chain",
          "554:     def execute(",
          "555:         self,",
          "556:         context: Context,",
          "557:     ) -> Message:",
          "558:         hook = GoogleAnalyticsAdminHook(",
          "559:             gcp_conn_id=self.gcp_conn_id,",
          "560:             impersonation_chain=self.impersonation_chain,",
          "561:         )",
          "562:         self.log.info(",
          "563:             \"Requesting the Google Ads link with id %s for the property_id %s\",",
          "564:             self.google_ads_link_id,",
          "565:             self.property_id,",
          "566:         )",
          "567:         ads_links = hook.list_google_ads_links(",
          "568:             property_id=self.property_id,",
          "569:             retry=self.retry,",
          "570:             timeout=self.timeout,",
          "571:             metadata=self.metadata,",
          "572:         )",
          "573:         find_link = (item for item in ads_links if item.name.split(\"/\")[-1] == self.google_ads_link_id)",
          "574:         if ads_link := next(find_link, None):",
          "575:             self.log.info(\"Successful request.\")",
          "576:             return GoogleAdsLink.to_dict(ads_link)",
          "577:         raise AirflowNotFoundException(",
          "578:             f\"Google Ads Link with id {self.google_ads_link_id} and property id {self.property_id} not found\"",
          "579:         )",
          "",
          "---------------"
        ],
        "tests/providers/google/marketing_platform/hooks/test_analytics_admin.py||tests/providers/google/marketing_platform/hooks/test_analytics_admin.py": [
          "File: tests/providers/google/marketing_platform/hooks/test_analytics_admin.py -> tests/providers/google/marketing_platform/hooks/test_analytics_admin.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: #",
          "2: # Licensed to the Apache Software Foundation (ASF) under one",
          "3: # or more contributor license agreements.  See the NOTICE file",
          "4: # distributed with this work for additional information",
          "5: # regarding copyright ownership.  The ASF licenses this file",
          "6: # to you under the Apache License, Version 2.0 (the",
          "7: # \"License\"); you may not use this file except in compliance",
          "8: # with the License.  You may obtain a copy of the License at",
          "9: #",
          "10: #   http://www.apache.org/licenses/LICENSE-2.0",
          "11: #",
          "12: # Unless required by applicable law or agreed to in writing,",
          "13: # software distributed under the License is distributed on an",
          "14: # \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
          "15: # KIND, either express or implied.  See the License for the",
          "16: # specific language governing permissions and limitations",
          "17: # under the License.",
          "18: from __future__ import annotations",
          "20: from unittest import mock",
          "22: from airflow.providers.google.marketing_platform.hooks.analytics_admin import GoogleAnalyticsAdminHook",
          "23: from tests.providers.google.cloud.utils.base_gcp_mock import mock_base_gcp_hook_default_project_id",
          "25: GCP_CONN_ID = \"test_gcp_conn_id\"",
          "26: IMPERSONATION_CHAIN = [\"ACCOUNT_1\", \"ACCOUNT_2\", \"ACCOUNT_3\"]",
          "27: TEST_PROPERTY_ID = \"123456789\"",
          "28: TEST_PROPERTY_NAME = f\"properties/{TEST_PROPERTY_ID}\"",
          "29: TEST_DATASTREAM_ID = \"987654321\"",
          "30: TEST_DATASTREAM_NAME = f\"properties/{TEST_PROPERTY_ID}/dataStreams/{TEST_DATASTREAM_ID}\"",
          "31: ANALYTICS_HOOK_PATH = \"airflow.providers.google.marketing_platform.hooks.analytics_admin\"",
          "34: class TestGoogleAnalyticsAdminHook:",
          "35:     def setup_method(self):",
          "36:         with mock.patch(",
          "37:             \"airflow.providers.google.common.hooks.base_google.GoogleBaseHook.__init__\",",
          "38:             new=mock_base_gcp_hook_default_project_id,",
          "39:         ):",
          "40:             self.hook = GoogleAnalyticsAdminHook(GCP_CONN_ID)",
          "42:     @mock.patch(\"airflow.providers.google.common.hooks.base_google.GoogleBaseHook.__init__\")",
          "43:     def test_init(self, mock_base_init):",
          "44:         GoogleAnalyticsAdminHook(",
          "45:             GCP_CONN_ID,",
          "46:             impersonation_chain=IMPERSONATION_CHAIN,",
          "47:         )",
          "48:         mock_base_init.assert_called_once_with(",
          "49:             GCP_CONN_ID,",
          "50:             impersonation_chain=IMPERSONATION_CHAIN,",
          "51:         )",
          "53:     @mock.patch(f\"{ANALYTICS_HOOK_PATH}.CLIENT_INFO\")",
          "54:     @mock.patch(f\"{ANALYTICS_HOOK_PATH}.GoogleAnalyticsAdminHook.get_credentials\")",
          "55:     @mock.patch(f\"{ANALYTICS_HOOK_PATH}.AnalyticsAdminServiceClient\")",
          "56:     def test_get_conn(self, mock_client, get_credentials, mock_client_info):",
          "57:         mock_credentials = mock.MagicMock()",
          "58:         get_credentials.return_value = mock_credentials",
          "60:         result = self.hook.get_conn()",
          "62:         mock_client.assert_called_once_with(credentials=mock_credentials, client_info=mock_client_info)",
          "63:         assert self.hook._conn == result",
          "65:     @mock.patch(f\"{ANALYTICS_HOOK_PATH}.GoogleAnalyticsAdminHook.get_conn\")",
          "66:     def test_list_accounts(self, mock_get_conn):",
          "67:         list_accounts_expected = mock.MagicMock()",
          "68:         mock_list_accounts = mock_get_conn.return_value.list_accounts",
          "69:         mock_list_accounts.return_value = list_accounts_expected",
          "70:         mock_page_size, mock_page_token, mock_show_deleted, mock_retry, mock_timeout, mock_metadata = (",
          "71:             mock.MagicMock() for _ in range(6)",
          "72:         )",
          "74:         request = {",
          "75:             \"page_size\": mock_page_size,",
          "76:             \"page_token\": mock_page_token,",
          "77:             \"show_deleted\": mock_show_deleted,",
          "78:         }",
          "80:         list_accounts_received = self.hook.list_accounts(",
          "81:             page_size=mock_page_size,",
          "82:             page_token=mock_page_token,",
          "83:             show_deleted=mock_show_deleted,",
          "84:             retry=mock_retry,",
          "85:             timeout=mock_timeout,",
          "86:             metadata=mock_metadata,",
          "87:         )",
          "88:         mock_list_accounts.assert_called_once_with(",
          "89:             request=request, retry=mock_retry, timeout=mock_timeout, metadata=mock_metadata",
          "90:         )",
          "91:         assert list_accounts_received == list_accounts_expected",
          "93:     @mock.patch(f\"{ANALYTICS_HOOK_PATH}.GoogleAnalyticsAdminHook.get_conn\")",
          "94:     def test_create_property(self, mock_get_conn):",
          "95:         property_expected = mock.MagicMock()",
          "97:         mock_create_property = mock_get_conn.return_value.create_property",
          "98:         mock_create_property.return_value = property_expected",
          "99:         mock_property, mock_retry, mock_timeout, mock_metadata = (mock.MagicMock() for _ in range(4))",
          "101:         property_created = self.hook.create_property(",
          "102:             analytics_property=mock_property, retry=mock_retry, timeout=mock_timeout, metadata=mock_metadata",
          "103:         )",
          "105:         request = {\"property\": mock_property}",
          "106:         mock_create_property.assert_called_once_with(",
          "107:             request=request, retry=mock_retry, timeout=mock_timeout, metadata=mock_metadata",
          "108:         )",
          "109:         assert property_created == property_expected",
          "111:     @mock.patch(f\"{ANALYTICS_HOOK_PATH}.GoogleAnalyticsAdminHook.get_conn\")",
          "112:     def test_delete_property(self, mock_get_conn):",
          "113:         property_expected = mock.MagicMock()",
          "114:         mock_delete_property = mock_get_conn.return_value.delete_property",
          "115:         mock_delete_property.return_value = property_expected",
          "116:         mock_retry, mock_timeout, mock_metadata = (mock.MagicMock() for _ in range(3))",
          "118:         property_deleted = self.hook.delete_property(",
          "119:             property_id=TEST_PROPERTY_ID,",
          "120:             retry=mock_retry,",
          "121:             timeout=mock_timeout,",
          "122:             metadata=mock_metadata,",
          "123:         )",
          "124:         request = {\"name\": TEST_PROPERTY_NAME}",
          "125:         mock_delete_property.assert_called_once_with(",
          "126:             request=request,",
          "127:             retry=mock_retry,",
          "128:             timeout=mock_timeout,",
          "129:             metadata=mock_metadata,",
          "130:         )",
          "131:         assert property_deleted == property_expected",
          "133:     @mock.patch(f\"{ANALYTICS_HOOK_PATH}.GoogleAnalyticsAdminHook.get_conn\")",
          "134:     def test_create_data_stream(self, mock_get_conn):",
          "135:         data_stream_expected = mock.MagicMock()",
          "136:         mock_create_data_stream = mock_get_conn.return_value.create_data_stream",
          "137:         mock_create_data_stream.return_value = data_stream_expected",
          "138:         mock_data_stream, mock_retry, mock_timeout, mock_metadata = (mock.MagicMock() for _ in range(4))",
          "140:         data_stream_created = self.hook.create_data_stream(",
          "141:             property_id=TEST_PROPERTY_ID,",
          "142:             data_stream=mock_data_stream,",
          "143:             retry=mock_retry,",
          "144:             timeout=mock_timeout,",
          "145:             metadata=mock_metadata,",
          "146:         )",
          "148:         request = {\"parent\": TEST_PROPERTY_NAME, \"data_stream\": mock_data_stream}",
          "149:         mock_create_data_stream.assert_called_once_with(",
          "150:             request=request, retry=mock_retry, timeout=mock_timeout, metadata=mock_metadata",
          "151:         )",
          "152:         assert data_stream_created == data_stream_expected",
          "154:     @mock.patch(f\"{ANALYTICS_HOOK_PATH}.GoogleAnalyticsAdminHook.get_conn\")",
          "155:     def test_delete_data_stream(self, mock_get_conn):",
          "156:         mock_retry, mock_timeout, mock_metadata = (mock.MagicMock() for _ in range(3))",
          "158:         self.hook.delete_data_stream(",
          "159:             property_id=TEST_PROPERTY_ID,",
          "160:             data_stream_id=TEST_DATASTREAM_ID,",
          "161:             retry=mock_retry,",
          "162:             timeout=mock_timeout,",
          "163:             metadata=mock_metadata,",
          "164:         )",
          "166:         request = {\"name\": TEST_DATASTREAM_NAME}",
          "167:         mock_get_conn.return_value.delete_data_stream.assert_called_once_with(",
          "168:             request=request,",
          "169:             retry=mock_retry,",
          "170:             timeout=mock_timeout,",
          "171:             metadata=mock_metadata,",
          "172:         )",
          "174:     @mock.patch(f\"{ANALYTICS_HOOK_PATH}.GoogleAnalyticsAdminHook.get_conn\")",
          "175:     def test_list_ads_links(self, mock_get_conn):",
          "176:         mock_page_size, mock_page_token, mock_retry, mock_timeout, mock_metadata = (",
          "177:             mock.MagicMock() for _ in range(5)",
          "178:         )",
          "180:         self.hook.list_google_ads_links(",
          "181:             property_id=TEST_PROPERTY_ID,",
          "182:             page_size=mock_page_size,",
          "183:             page_token=mock_page_token,",
          "184:             retry=mock_retry,",
          "185:             timeout=mock_timeout,",
          "186:             metadata=mock_metadata,",
          "187:         )",
          "189:         request = {\"parent\": TEST_PROPERTY_NAME, \"page_size\": mock_page_size, \"page_token\": mock_page_token}",
          "190:         mock_get_conn.return_value.list_google_ads_links.assert_called_once_with(",
          "191:             request=request, retry=mock_retry, timeout=mock_timeout, metadata=mock_metadata",
          "192:         )",
          "",
          "---------------"
        ],
        "tests/providers/google/marketing_platform/links/__init__.py||tests/providers/google/marketing_platform/links/__init__.py": [
          "File: tests/providers/google/marketing_platform/links/__init__.py -> tests/providers/google/marketing_platform/links/__init__.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: #",
          "2: # Licensed to the Apache Software Foundation (ASF) under one",
          "3: # or more contributor license agreements.  See the NOTICE file",
          "4: # distributed with this work for additional information",
          "5: # regarding copyright ownership.  The ASF licenses this file",
          "6: # to you under the Apache License, Version 2.0 (the",
          "7: # \"License\"); you may not use this file except in compliance",
          "8: # with the License.  You may obtain a copy of the License at",
          "9: #",
          "10: #   http://www.apache.org/licenses/LICENSE-2.0",
          "11: #",
          "12: # Unless required by applicable law or agreed to in writing,",
          "13: # software distributed under the License is distributed on an",
          "14: # \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
          "15: # KIND, either express or implied.  See the License for the",
          "16: # specific language governing permissions and limitations",
          "17: # under the License.",
          "",
          "---------------"
        ],
        "tests/providers/google/marketing_platform/links/test_analytics_admin.py||tests/providers/google/marketing_platform/links/test_analytics_admin.py": [
          "File: tests/providers/google/marketing_platform/links/test_analytics_admin.py -> tests/providers/google/marketing_platform/links/test_analytics_admin.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: #",
          "2: # Licensed to the Apache Software Foundation (ASF) under one",
          "3: # or more contributor license agreements.  See the NOTICE file",
          "4: # distributed with this work for additional information",
          "5: # regarding copyright ownership.  The ASF licenses this file",
          "6: # to you under the Apache License, Version 2.0 (the",
          "7: # \"License\"); you may not use this file except in compliance",
          "8: # with the License.  You may obtain a copy of the License at",
          "9: #",
          "10: #   http://www.apache.org/licenses/LICENSE-2.0",
          "11: #",
          "12: # Unless required by applicable law or agreed to in writing,",
          "13: # software distributed under the License is distributed on an",
          "14: # \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
          "15: # KIND, either express or implied.  See the License for the",
          "16: # specific language governing permissions and limitations",
          "17: # under the License.",
          "18: from __future__ import annotations",
          "20: from unittest import mock",
          "22: from airflow.providers.google.marketing_platform.links.analytics_admin import (",
          "23:     BASE_LINK,",
          "24:     GoogleAnalyticsPropertyLink,",
          "25: )",
          "27: TEST_PROPERTY_ID = \"123456789\"",
          "28: TEST_PROJECT_ID = \"test_project\"",
          "29: TEST_CONF_GOOGLE_ADS_LINK = {\"property_id\": TEST_PROJECT_ID}",
          "30: ANALYTICS_LINKS_PATH = \"airflow.providers.google.marketing_platform.links.analytics_admin\"",
          "33: class TestGoogleAnalyticsPropertyLink:",
          "34:     @mock.patch(f\"{ANALYTICS_LINKS_PATH}.XCom\")",
          "35:     def test_get_link(self, mock_xcom):",
          "36:         mock_ti_key = mock.MagicMock()",
          "37:         mock_xcom.get_value.return_value = TEST_CONF_GOOGLE_ADS_LINK",
          "38:         url_expected = f\"{BASE_LINK}#/p{TEST_PROJECT_ID}/\"",
          "40:         link = GoogleAnalyticsPropertyLink()",
          "41:         url = link.get_link(operator=mock.MagicMock(), ti_key=mock_ti_key)",
          "43:         mock_xcom.get_value.assert_called_once_with(key=link.key, ti_key=mock_ti_key)",
          "44:         assert url == url_expected",
          "46:     @mock.patch(f\"{ANALYTICS_LINKS_PATH}.XCom\")",
          "47:     def test_get_link_not_found(self, mock_xcom):",
          "48:         mock_ti_key = mock.MagicMock()",
          "49:         mock_xcom.get_value.return_value = None",
          "51:         link = GoogleAnalyticsPropertyLink()",
          "52:         url = link.get_link(operator=mock.MagicMock(), ti_key=mock_ti_key)",
          "54:         mock_xcom.get_value.assert_called_once_with(key=link.key, ti_key=mock_ti_key)",
          "55:         assert url == \"\"",
          "57:     def test_persist(self):",
          "58:         mock_context = mock.MagicMock()",
          "59:         mock_task_instance = mock.MagicMock()",
          "61:         GoogleAnalyticsPropertyLink.persist(",
          "62:             context=mock_context,",
          "63:             task_instance=mock_task_instance,",
          "64:             property_id=TEST_PROPERTY_ID,",
          "65:         )",
          "67:         mock_task_instance.xcom_push.assert_called_once_with(",
          "68:             mock_context,",
          "69:             key=GoogleAnalyticsPropertyLink.key,",
          "70:             value={\"property_id\": TEST_PROPERTY_ID},",
          "71:         )",
          "",
          "---------------"
        ],
        "tests/providers/google/marketing_platform/operators/test_analytics_admin.py||tests/providers/google/marketing_platform/operators/test_analytics_admin.py": [
          "File: tests/providers/google/marketing_platform/operators/test_analytics_admin.py -> tests/providers/google/marketing_platform/operators/test_analytics_admin.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: # Licensed to the Apache Software Foundation (ASF) under one",
          "2: # or more contributor license agreements.  See the NOTICE file",
          "3: # distributed with this work for additional information",
          "4: # regarding copyright ownership.  The ASF licenses this file",
          "5: # to you under the Apache License, Version 2.0 (the",
          "6: # \"License\"); you may not use this file except in compliance",
          "7: # with the License.  You may obtain a copy of the License at",
          "8: #",
          "9: #   http://www.apache.org/licenses/LICENSE-2.0",
          "10: #",
          "11: # Unless required by applicable law or agreed to in writing,",
          "12: # software distributed under the License is distributed on an",
          "13: # \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
          "14: # KIND, either express or implied.  See the License for the",
          "15: # specific language governing permissions and limitations",
          "16: # under the License.",
          "17: from __future__ import annotations",
          "19: from unittest import mock",
          "21: import pytest",
          "23: from airflow.exceptions import AirflowNotFoundException",
          "24: from airflow.providers.google.marketing_platform.operators.analytics_admin import (",
          "25:     GoogleAnalyticsAdminCreateDataStreamOperator,",
          "26:     GoogleAnalyticsAdminCreatePropertyOperator,",
          "27:     GoogleAnalyticsAdminDeleteDataStreamOperator,",
          "28:     GoogleAnalyticsAdminDeletePropertyOperator,",
          "29:     GoogleAnalyticsAdminGetGoogleAdsLinkOperator,",
          "30:     GoogleAnalyticsAdminListAccountsOperator,",
          "31:     GoogleAnalyticsAdminListGoogleAdsLinksOperator,",
          "32: )",
          "34: GCP_CONN_ID = \"google_cloud_default\"",
          "35: IMPERSONATION_CHAIN = [\"ACCOUNT_1\", \"ACCOUNT_2\", \"ACCOUNT_3\"]",
          "36: TEST_GA_GOOGLE_ADS_PROPERTY_ID = \"123456789\"",
          "37: TEST_GA_GOOGLE_ADS_LINK_ID = \"987654321\"",
          "38: TEST_GA_GOOGLE_ADS_LINK_NAME = (",
          "39:     f\"properties/{TEST_GA_GOOGLE_ADS_PROPERTY_ID}/googleAdsLinks/{TEST_GA_GOOGLE_ADS_LINK_ID}\"",
          "40: )",
          "41: TEST_PROPERTY_ID = \"123456789\"",
          "42: TEST_PROPERTY_NAME = f\"properties/{TEST_PROPERTY_ID}\"",
          "43: TEST_DATASTREAM_ID = \"987654321\"",
          "44: TEST_DATASTREAM_NAME = f\"properties/{TEST_PROPERTY_ID}/dataStreams/{TEST_DATASTREAM_ID}\"",
          "45: ANALYTICS_PATH = \"airflow.providers.google.marketing_platform.operators.analytics_admin\"",
          "48: class TestGoogleAnalyticsAdminListAccountsOperator:",
          "49:     @mock.patch(f\"{ANALYTICS_PATH}.GoogleAnalyticsAdminHook\")",
          "50:     @mock.patch(f\"{ANALYTICS_PATH}.Account.to_dict\")",
          "51:     def test_execute(self, account_to_dict_mock, hook_mock):",
          "52:         list_accounts_returned = (mock.MagicMock(), mock.MagicMock(), mock.MagicMock())",
          "53:         hook_mock.return_value.list_accounts.return_value = list_accounts_returned",
          "55:         list_accounts_serialized = [mock.MagicMock(), mock.MagicMock(), mock.MagicMock()]",
          "56:         account_to_dict_mock.side_effect = list_accounts_serialized",
          "58:         mock_page_size, mock_page_token, mock_show_deleted, mock_retry, mock_timeout, mock_metadata = (",
          "59:             mock.MagicMock() for _ in range(6)",
          "60:         )",
          "62:         retrieved_accounts_list = GoogleAnalyticsAdminListAccountsOperator(",
          "63:             task_id=\"test_task\",",
          "64:             page_size=mock_page_size,",
          "65:             page_token=mock_page_token,",
          "66:             show_deleted=mock_show_deleted,",
          "67:             retry=mock_retry,",
          "68:             timeout=mock_timeout,",
          "69:             metadata=mock_metadata,",
          "70:             gcp_conn_id=GCP_CONN_ID,",
          "71:             impersonation_chain=IMPERSONATION_CHAIN,",
          "72:         ).execute(context=None)",
          "74:         hook_mock.assert_called_once()",
          "75:         hook_mock.return_value.list_accounts.assert_called_once_with(",
          "76:             page_size=mock_page_size,",
          "77:             page_token=mock_page_token,",
          "78:             show_deleted=mock_show_deleted,",
          "79:             retry=mock_retry,",
          "80:             timeout=mock_timeout,",
          "81:             metadata=mock_metadata,",
          "82:         )",
          "83:         account_to_dict_mock.assert_has_calls([mock.call(item) for item in list_accounts_returned])",
          "84:         assert retrieved_accounts_list == list_accounts_serialized",
          "87: class TestGoogleAnalyticsAdminCreatePropertyOperator:",
          "88:     @mock.patch(f\"{ANALYTICS_PATH}.GoogleAnalyticsPropertyLink\")",
          "89:     @mock.patch(f\"{ANALYTICS_PATH}.GoogleAnalyticsAdminHook\")",
          "90:     @mock.patch(f\"{ANALYTICS_PATH}.Property.to_dict\")",
          "91:     def test_execute(self, property_to_dict_mock, hook_mock, _):",
          "92:         property_returned = mock.MagicMock()",
          "93:         hook_mock.return_value.create_property.return_value = property_returned",
          "95:         property_serialized = mock.MagicMock()",
          "96:         property_to_dict_mock.return_value = property_serialized",
          "98:         mock_property, mock_retry, mock_timeout, mock_metadata = (mock.MagicMock() for _ in range(4))",
          "100:         property_created = GoogleAnalyticsAdminCreatePropertyOperator(",
          "101:             task_id=\"test_task\",",
          "102:             analytics_property=mock_property,",
          "103:             retry=mock_retry,",
          "104:             timeout=mock_timeout,",
          "105:             metadata=mock_metadata,",
          "106:             gcp_conn_id=GCP_CONN_ID,",
          "107:             impersonation_chain=IMPERSONATION_CHAIN,",
          "108:         ).execute(context=None)",
          "110:         hook_mock.assert_called_once()",
          "111:         hook_mock.return_value.create_property.assert_called_once_with(",
          "112:             analytics_property=mock_property,",
          "113:             retry=mock_retry,",
          "114:             timeout=mock_timeout,",
          "115:             metadata=mock_metadata,",
          "116:         )",
          "117:         property_to_dict_mock.assert_called_once_with(property_returned)",
          "118:         assert property_created == property_serialized",
          "121: class TestGoogleAnalyticsAdminDeletePropertyOperator:",
          "122:     @mock.patch(f\"{ANALYTICS_PATH}.GoogleAnalyticsAdminHook\")",
          "123:     @mock.patch(f\"{ANALYTICS_PATH}.Property.to_dict\")",
          "124:     def test_execute(self, property_to_dict_mock, hook_mock):",
          "125:         property_returned = mock.MagicMock()",
          "126:         hook_mock.return_value.delete_property.return_value = property_returned",
          "128:         property_serialized = mock.MagicMock()",
          "129:         property_to_dict_mock.return_value = property_serialized",
          "131:         mock_retry, mock_timeout, mock_metadata = (mock.MagicMock() for _ in range(3))",
          "133:         property_deleted = GoogleAnalyticsAdminDeletePropertyOperator(",
          "134:             task_id=\"test_task\",",
          "135:             gcp_conn_id=GCP_CONN_ID,",
          "136:             impersonation_chain=IMPERSONATION_CHAIN,",
          "137:             property_id=TEST_PROPERTY_ID,",
          "138:             retry=mock_retry,",
          "139:             timeout=mock_timeout,",
          "140:             metadata=mock_metadata,",
          "141:         ).execute(context=None)",
          "143:         hook_mock.assert_called_once()",
          "144:         hook_mock.return_value.delete_property.assert_called_once_with(",
          "145:             property_id=TEST_PROPERTY_ID,",
          "146:             retry=mock_retry,",
          "147:             timeout=mock_timeout,",
          "148:             metadata=mock_metadata,",
          "149:         )",
          "150:         property_to_dict_mock.assert_called_once_with(property_returned)",
          "151:         assert property_deleted == property_serialized",
          "154: class TestGoogleAnalyticsAdminCreateDataStreamOperator:",
          "155:     @mock.patch(f\"{ANALYTICS_PATH}.GoogleAnalyticsAdminHook\")",
          "156:     @mock.patch(f\"{ANALYTICS_PATH}.DataStream.to_dict\")",
          "157:     def test_execute(self, data_stream_to_dict_mock, hook_mock):",
          "158:         data_stream_returned = mock.MagicMock()",
          "159:         hook_mock.return_value.create_data_stream.return_value = data_stream_returned",
          "161:         data_stream_serialized = mock.MagicMock()",
          "162:         data_stream_to_dict_mock.return_value = data_stream_serialized",
          "164:         mock_parent, mock_data_stream, mock_retry, mock_timeout, mock_metadata = (",
          "165:             mock.MagicMock() for _ in range(5)",
          "166:         )",
          "168:         data_stream_created = GoogleAnalyticsAdminCreateDataStreamOperator(",
          "169:             task_id=\"test_task\",",
          "170:             property_id=TEST_PROPERTY_ID,",
          "171:             data_stream=mock_data_stream,",
          "172:             retry=mock_retry,",
          "173:             timeout=mock_timeout,",
          "174:             metadata=mock_metadata,",
          "175:             gcp_conn_id=GCP_CONN_ID,",
          "176:             impersonation_chain=IMPERSONATION_CHAIN,",
          "177:         ).execute(context=None)",
          "179:         hook_mock.assert_called_once()",
          "180:         hook_mock.return_value.create_data_stream.assert_called_once_with(",
          "181:             property_id=TEST_PROPERTY_ID,",
          "182:             data_stream=mock_data_stream,",
          "183:             retry=mock_retry,",
          "184:             timeout=mock_timeout,",
          "185:             metadata=mock_metadata,",
          "186:         )",
          "187:         data_stream_to_dict_mock.assert_called_once_with(data_stream_returned)",
          "188:         assert data_stream_created == data_stream_serialized",
          "191: class TestGoogleAnalyticsAdminDeleteDataStreamOperator:",
          "192:     @mock.patch(f\"{ANALYTICS_PATH}.GoogleAnalyticsAdminHook\")",
          "193:     def test_execute(self, hook_mock):",
          "194:         mock_retry, mock_timeout, mock_metadata = (mock.MagicMock() for _ in range(3))",
          "196:         GoogleAnalyticsAdminDeleteDataStreamOperator(",
          "197:             task_id=\"test_task\",",
          "198:             gcp_conn_id=GCP_CONN_ID,",
          "199:             impersonation_chain=IMPERSONATION_CHAIN,",
          "200:             property_id=TEST_PROPERTY_ID,",
          "201:             data_stream_id=TEST_DATASTREAM_ID,",
          "202:             retry=mock_retry,",
          "203:             timeout=mock_timeout,",
          "204:             metadata=mock_metadata,",
          "205:         ).execute(context=None)",
          "207:         hook_mock.assert_called_once()",
          "208:         hook_mock.return_value.delete_data_stream.assert_called_once_with(",
          "209:             property_id=TEST_PROPERTY_ID,",
          "210:             data_stream_id=TEST_DATASTREAM_ID,",
          "211:             retry=mock_retry,",
          "212:             timeout=mock_timeout,",
          "213:             metadata=mock_metadata,",
          "214:         )",
          "217: class TestGoogleAnalyticsAdminListGoogleAdsLinksOperator:",
          "218:     @mock.patch(f\"{ANALYTICS_PATH}.GoogleAnalyticsAdminHook\")",
          "219:     @mock.patch(f\"{ANALYTICS_PATH}.GoogleAdsLink.to_dict\")",
          "220:     def test_execute(self, ads_link_to_dict_mock, hook_mock):",
          "221:         list_ads_links_returned = (mock.MagicMock(), mock.MagicMock(), mock.MagicMock())",
          "222:         hook_mock.return_value.list_google_ads_links.return_value = list_ads_links_returned",
          "224:         list_ads_links_serialized = [mock.MagicMock(), mock.MagicMock(), mock.MagicMock()]",
          "225:         ads_link_to_dict_mock.side_effect = list_ads_links_serialized",
          "227:         mock_page_size, mock_page_token, mock_show_deleted, mock_retry, mock_timeout, mock_metadata = (",
          "228:             mock.MagicMock() for _ in range(6)",
          "229:         )",
          "231:         retrieved_ads_links = GoogleAnalyticsAdminListGoogleAdsLinksOperator(",
          "232:             task_id=\"test_task\",",
          "233:             property_id=TEST_PROPERTY_ID,",
          "234:             page_size=mock_page_size,",
          "235:             page_token=mock_page_token,",
          "236:             retry=mock_retry,",
          "237:             timeout=mock_timeout,",
          "238:             metadata=mock_metadata,",
          "239:             gcp_conn_id=GCP_CONN_ID,",
          "240:             impersonation_chain=IMPERSONATION_CHAIN,",
          "241:         ).execute(context=None)",
          "243:         hook_mock.assert_called_once()",
          "244:         hook_mock.return_value.list_google_ads_links.assert_called_once_with(",
          "245:             property_id=TEST_PROPERTY_ID,",
          "246:             page_size=mock_page_size,",
          "247:             page_token=mock_page_token,",
          "248:             retry=mock_retry,",
          "249:             timeout=mock_timeout,",
          "250:             metadata=mock_metadata,",
          "251:         )",
          "252:         ads_link_to_dict_mock.assert_has_calls([mock.call(item) for item in list_ads_links_returned])",
          "253:         assert retrieved_ads_links == list_ads_links_serialized",
          "256: class TestGoogleAnalyticsAdminGetGoogleAdsLinkOperator:",
          "257:     @mock.patch(f\"{ANALYTICS_PATH}.GoogleAnalyticsAdminHook\")",
          "258:     @mock.patch(f\"{ANALYTICS_PATH}.GoogleAdsLink\")",
          "259:     def test_execute(self, mock_google_ads_link, hook_mock):",
          "260:         mock_ad_link = mock.MagicMock()",
          "261:         mock_ad_link.name = TEST_GA_GOOGLE_ADS_LINK_NAME",
          "262:         list_ads_links = hook_mock.return_value.list_google_ads_links",
          "263:         list_ads_links.return_value = [mock_ad_link]",
          "264:         mock_retry, mock_timeout, mock_metadata = (mock.MagicMock() for _ in range(3))",
          "266:         GoogleAnalyticsAdminGetGoogleAdsLinkOperator(",
          "267:             task_id=\"test_task\",",
          "268:             property_id=TEST_GA_GOOGLE_ADS_PROPERTY_ID,",
          "269:             google_ads_link_id=TEST_GA_GOOGLE_ADS_LINK_ID,",
          "270:             gcp_conn_id=GCP_CONN_ID,",
          "271:             impersonation_chain=IMPERSONATION_CHAIN,",
          "272:             retry=mock_retry,",
          "273:             timeout=mock_timeout,",
          "274:             metadata=mock_metadata,",
          "275:         ).execute(context=None)",
          "277:         hook_mock.assert_called_once()",
          "278:         hook_mock.return_value.list_google_ads_links.assert_called_once_with(",
          "279:             property_id=TEST_PROPERTY_ID,",
          "280:             retry=mock_retry,",
          "281:             timeout=mock_timeout,",
          "282:             metadata=mock_metadata,",
          "283:         )",
          "284:         mock_google_ads_link.to_dict.assert_called_once_with(mock_ad_link)",
          "286:     @mock.patch(f\"{ANALYTICS_PATH}.GoogleAnalyticsAdminHook\")",
          "287:     def test_execute_not_found(self, hook_mock):",
          "288:         list_ads_links = hook_mock.return_value.list_google_ads_links",
          "289:         list_ads_links.return_value = []",
          "290:         mock_retry, mock_timeout, mock_metadata = (mock.MagicMock() for _ in range(3))",
          "292:         with pytest.raises(AirflowNotFoundException):",
          "293:             GoogleAnalyticsAdminGetGoogleAdsLinkOperator(",
          "294:                 task_id=\"test_task\",",
          "295:                 gcp_conn_id=GCP_CONN_ID,",
          "296:                 impersonation_chain=IMPERSONATION_CHAIN,",
          "297:                 property_id=TEST_GA_GOOGLE_ADS_PROPERTY_ID,",
          "298:                 google_ads_link_id=TEST_GA_GOOGLE_ADS_LINK_ID,",
          "299:                 retry=mock_retry,",
          "300:                 timeout=mock_timeout,",
          "301:                 metadata=mock_metadata,",
          "302:             ).execute(context=None)",
          "304:         hook_mock.assert_called_once()",
          "305:         hook_mock.return_value.list_google_ads_links.assert_called_once_with(",
          "306:             property_id=TEST_PROPERTY_ID,",
          "307:             retry=mock_retry,",
          "308:             timeout=mock_timeout,",
          "309:             metadata=mock_metadata,",
          "310:         )",
          "",
          "---------------"
        ],
        "tests/system/providers/google/marketing_platform/example_analytics_admin.py||tests/system/providers/google/marketing_platform/example_analytics_admin.py": [
          "File: tests/system/providers/google/marketing_platform/example_analytics_admin.py -> tests/system/providers/google/marketing_platform/example_analytics_admin.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: # Licensed to the Apache Software Foundation (ASF) under one",
          "2: # or more contributor license agreements.  See the NOTICE file",
          "3: # distributed with this work for additional information",
          "4: # regarding copyright ownership.  The ASF licenses this file",
          "5: # to you under the Apache License, Version 2.0 (the",
          "6: # \"License\"); you may not use this file except in compliance",
          "7: # with the License.  You may obtain a copy of the License at",
          "8: #",
          "9: #   http://www.apache.org/licenses/LICENSE-2.0",
          "10: #",
          "11: # Unless required by applicable law or agreed to in writing,",
          "12: # software distributed under the License is distributed on an",
          "13: # \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
          "14: # KIND, either express or implied.  See the License for the",
          "15: # specific language governing permissions and limitations",
          "16: # under the License.",
          "17: \"\"\"",
          "18: Example Airflow DAG that shows how to use Google Analytics (GA4) Admin Operators.",
          "20: This DAG relies on the following OS environment variables",
          "25: In order to run this test, make sure you followed steps:",
          "26: 1. Login to https://analytics.google.com",
          "27: 2. In the settings section create an account and save its ID in the variable GA_ACCOUNT_ID.",
          "28: 3. In the settings section go to the Property access management page and add your service account email with",
          "29: Editor permissions. This service account should be created on behalf of the account from the step 1.",
          "30: 4. Make sure Google Analytics Admin API is enabled in your GCP project.",
          "31: 5. Create Google Ads account and link it to your Google Analytics account in the GA admin panel.",
          "32: 6. Associate the Google Ads account with a property, and save this property's id in the variable",
          "33: GA_GOOGLE_ADS_PROPERTY_ID.",
          "34: \"\"\"",
          "35: from __future__ import annotations",
          "37: import json",
          "38: import logging",
          "39: import os",
          "40: from datetime import datetime",
          "42: from google.analytics import admin_v1beta as google_analytics",
          "44: from airflow.decorators import task",
          "45: from airflow.models import Connection",
          "46: from airflow.models.dag import DAG",
          "47: from airflow.operators.bash import BashOperator",
          "48: from airflow.providers.google.marketing_platform.operators.analytics_admin import (",
          "49:     GoogleAnalyticsAdminCreateDataStreamOperator,",
          "50:     GoogleAnalyticsAdminCreatePropertyOperator,",
          "51:     GoogleAnalyticsAdminDeleteDataStreamOperator,",
          "52:     GoogleAnalyticsAdminDeletePropertyOperator,",
          "53:     GoogleAnalyticsAdminGetGoogleAdsLinkOperator,",
          "54:     GoogleAnalyticsAdminListAccountsOperator,",
          "55:     GoogleAnalyticsAdminListGoogleAdsLinksOperator,",
          "56: )",
          "57: from airflow.settings import Session",
          "58: from airflow.utils.trigger_rule import TriggerRule",
          "60: ENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")",
          "61: DAG_ID = \"example_google_analytics_admin\"",
          "63: CONNECTION_ID = f\"connection_{DAG_ID}_{ENV_ID}\"",
          "64: ACCOUNT_ID = os.environ.get(\"GA_ACCOUNT_ID\", \"123456789\")",
          "65: PROPERTY_ID = \"{{ task_instance.xcom_pull('create_property')['name'].split('/')[-1] }}\"",
          "66: DATA_STREAM_ID = \"{{ task_instance.xcom_pull('create_data_stream')['name'].split('/')[-1] }}\"",
          "67: GA_GOOGLE_ADS_PROPERTY_ID = os.environ.get(\"GA_GOOGLE_ADS_PROPERTY_ID\", \"123456789\")",
          "68: GA_ADS_LINK_ID = \"{{ task_instance.xcom_pull('list_google_ads_links')[0]['name'].split('/')[-1] }}\"",
          "70: log = logging.getLogger(__name__)",
          "72: with DAG(",
          "73:     DAG_ID,",
          "74:     schedule=\"@once\",  # Override to match your needs,",
          "75:     start_date=datetime(2021, 1, 1),",
          "76:     catchup=False,",
          "77:     tags=[\"example\", \"analytics\"],",
          "78: ) as dag:",
          "80:     @task",
          "81:     def setup_connection(**kwargs) -> None:",
          "82:         connection = Connection(",
          "83:             conn_id=CONNECTION_ID,",
          "84:             conn_type=\"google_cloud_platform\",",
          "85:         )",
          "86:         conn_extra_json = json.dumps(",
          "87:             {",
          "88:                 \"scope\": \"https://www.googleapis.com/auth/analytics.edit,\"",
          "89:                 \"https://www.googleapis.com/auth/analytics.readonly\",",
          "90:             }",
          "91:         )",
          "92:         connection.set_extra(conn_extra_json)",
          "94:         session = Session()",
          "95:         if session.query(Connection).filter(Connection.conn_id == CONNECTION_ID).first():",
          "96:             log.warning(\"Connection %s already exists\", CONNECTION_ID)",
          "97:             return None",
          "99:         session.add(connection)",
          "100:         session.commit()",
          "102:     setup_connection_task = setup_connection()",
          "104:     # [START howto_marketing_platform_list_accounts_operator]",
          "105:     list_accounts = GoogleAnalyticsAdminListAccountsOperator(",
          "106:         task_id=\"list_account\",",
          "107:         gcp_conn_id=CONNECTION_ID,",
          "108:         show_deleted=True,",
          "109:     )",
          "110:     # [END howto_marketing_platform_list_accounts_operator]",
          "112:     # [START howto_marketing_platform_create_property_operator]",
          "113:     create_property = GoogleAnalyticsAdminCreatePropertyOperator(",
          "114:         task_id=\"create_property\",",
          "115:         analytics_property={",
          "116:             \"parent\": f\"accounts/{ACCOUNT_ID}\",",
          "117:             \"display_name\": \"Test display name\",",
          "118:             \"time_zone\": \"America/Los_Angeles\",",
          "119:         },",
          "120:         gcp_conn_id=CONNECTION_ID,",
          "121:     )",
          "122:     # [END howto_marketing_platform_create_property_operator]",
          "124:     # [START howto_marketing_platform_create_data_stream_operator]",
          "125:     create_data_stream = GoogleAnalyticsAdminCreateDataStreamOperator(",
          "126:         task_id=\"create_data_stream\",",
          "127:         property_id=PROPERTY_ID,",
          "128:         data_stream={",
          "129:             \"display_name\": \"Test data stream\",",
          "130:             \"web_stream_data\": {",
          "131:                 \"default_uri\": \"www.example.com\",",
          "132:             },",
          "133:             \"type_\": google_analytics.DataStream.DataStreamType.WEB_DATA_STREAM,",
          "134:         },",
          "135:         gcp_conn_id=CONNECTION_ID,",
          "136:     )",
          "137:     # [END howto_marketing_platform_create_data_stream_operator]",
          "139:     # [START howto_marketing_platform_delete_data_stream_operator]",
          "140:     delete_data_stream = GoogleAnalyticsAdminDeleteDataStreamOperator(",
          "141:         task_id=\"delete_datastream\",",
          "142:         property_id=PROPERTY_ID,",
          "143:         data_stream_id=DATA_STREAM_ID,",
          "144:         gcp_conn_id=CONNECTION_ID,",
          "145:     )",
          "146:     # [END howto_marketing_platform_delete_data_stream_operator]",
          "148:     # [START howto_marketing_platform_delete_property_operator]",
          "149:     delete_property = GoogleAnalyticsAdminDeletePropertyOperator(",
          "150:         task_id=\"delete_property\",",
          "151:         property_id=PROPERTY_ID,",
          "152:         gcp_conn_id=CONNECTION_ID,",
          "153:     )",
          "154:     # [END howto_marketing_platform_delete_property_operator]",
          "155:     delete_property.trigger_rule = TriggerRule.ALL_DONE",
          "157:     # [START howto_marketing_platform_list_google_ads_links]",
          "158:     list_google_ads_links = GoogleAnalyticsAdminListGoogleAdsLinksOperator(",
          "159:         task_id=\"list_google_ads_links\",",
          "160:         property_id=GA_GOOGLE_ADS_PROPERTY_ID,",
          "161:         gcp_conn_id=CONNECTION_ID,",
          "162:     )",
          "163:     # [END howto_marketing_platform_list_google_ads_links]",
          "165:     # [START howto_marketing_platform_get_google_ad_link]",
          "166:     get_ad_link = GoogleAnalyticsAdminGetGoogleAdsLinkOperator(",
          "167:         task_id=\"get_ad_link\",",
          "168:         property_id=GA_GOOGLE_ADS_PROPERTY_ID,",
          "169:         google_ads_link_id=GA_ADS_LINK_ID,",
          "170:         gcp_conn_id=CONNECTION_ID,",
          "171:     )",
          "172:     # [END howto_marketing_platform_get_google_ad_link]",
          "174:     delete_connection = BashOperator(",
          "175:         task_id=\"delete_connection\",",
          "176:         bash_command=f\"airflow connections delete {CONNECTION_ID}\",",
          "177:         trigger_rule=TriggerRule.ALL_DONE,",
          "178:     )",
          "180:     (",
          "181:         # TEST SETUP",
          "182:         setup_connection_task",
          "183:         # TEST BODY",
          "184:         >> list_accounts",
          "185:         >> create_property",
          "186:         >> create_data_stream",
          "187:         >> delete_data_stream",
          "188:         >> delete_property",
          "189:         >> list_google_ads_links",
          "190:         >> get_ad_link",
          "191:         # TEST TEARDOWN",
          "192:         >> delete_connection",
          "193:     )",
          "194:     from tests.system.utils.watcher import watcher",
          "196:     # This test needs watcher in order to properly mark success/failure",
          "197:     # when \"tearDown\" task with trigger rule is part of the DAG",
          "198:     list(dag.tasks) >> watcher()",
          "200: from tests.system.utils import get_test_run  # noqa: E402",
          "202: # Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)",
          "203: test_run = get_test_run(dag)",
          "",
          "---------------"
        ]
      }
    }
  ]
}