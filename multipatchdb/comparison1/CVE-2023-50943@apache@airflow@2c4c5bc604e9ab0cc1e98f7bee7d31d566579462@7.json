{
  "cve_id": "CVE-2023-50943",
  "cve_desc": "Apache Airflow, versions before 2.8.1, have a vulnerability that allows a potential attacker to poison the XCom data by bypassing the protection of \"enable_xcom_pickling=False\" configuration setting resulting in poisoned data after XCom deserialization. This vulnerability is considered low since it requires a DAG author to exploit it. Users are recommended to upgrade to version 2.8.1 or later, which fixes this issue.",
  "repo": "apache/airflow",
  "patch_hash": "2c4c5bc604e9ab0cc1e98f7bee7d31d566579462",
  "patch_info": {
    "commit_hash": "2c4c5bc604e9ab0cc1e98f7bee7d31d566579462",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/2c4c5bc604e9ab0cc1e98f7bee7d31d566579462",
    "files": [
      "airflow/models/xcom.py",
      "tests/api_connexion/schemas/test_xcom_schema.py",
      "tests/models/test_xcom.py"
    ],
    "message": "Stop deserializing pickle when enable_xcom_pickling is False (#36255)\n\n* Stop deserializing pickle when enable_xcom_pickling is False\n\n* Fix unit tests\n\n(cherry picked from commit 63e97abec5d56bc62a293c93f5227f364561e51c)",
    "before_after_code_files": [
      "airflow/models/xcom.py||airflow/models/xcom.py",
      "tests/api_connexion/schemas/test_xcom_schema.py||tests/api_connexion/schemas/test_xcom_schema.py",
      "tests/models/test_xcom.py||tests/models/test_xcom.py"
    ]
  },
  "patch_diff": {
    "airflow/models/xcom.py||airflow/models/xcom.py": [
      "File: airflow/models/xcom.py -> airflow/models/xcom.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "685:             except pickle.UnpicklingError:",
      "686:                 return json.loads(result.value.decode(\"UTF-8\"), cls=XComDecoder, object_hook=object_hook)",
      "687:         else:",
      "693:     @staticmethod",
      "694:     def deserialize_value(result: XCom) -> Any:",
      "",
      "[Removed Lines]",
      "688:             try:",
      "689:                 return json.loads(result.value.decode(\"UTF-8\"), cls=XComDecoder, object_hook=object_hook)",
      "690:             except (json.JSONDecodeError, UnicodeDecodeError):",
      "691:                 return pickle.loads(result.value)",
      "",
      "[Added Lines]",
      "688:             # Since xcom_pickling is disabled, we should only try to deserialize with JSON",
      "689:             return json.loads(result.value.decode(\"UTF-8\"), cls=XComDecoder, object_hook=object_hook)",
      "",
      "---------------"
    ],
    "tests/api_connexion/schemas/test_xcom_schema.py||tests/api_connexion/schemas/test_xcom_schema.py": [
      "File: tests/api_connexion/schemas/test_xcom_schema.py -> tests/api_connexion/schemas/test_xcom_schema.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "30: from airflow.models import DagRun, XCom",
      "31: from airflow.utils.dates import parse_execution_date",
      "32: from airflow.utils.session import create_session",
      "34: pytestmark = pytest.mark.db_test",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "33: from tests.test_utils.config import conf_vars",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "188:     default_time = \"2016-04-02T21:00:00+00:00\"",
      "189:     default_time_parsed = parse_execution_date(default_time)",
      "191:     def test_serialize(self, create_xcom, session):",
      "192:         create_xcom(",
      "193:             dag_id=\"test_dag\",",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "192:     @conf_vars({(\"core\", \"enable_xcom_pickling\"): \"True\"})",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "208:             \"map_index\": -1,",
      "209:         }",
      "211:     def test_deserialize(self):",
      "212:         xcom_dump = {",
      "213:             \"key\": \"test_key\",",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "213:     @conf_vars({(\"core\", \"enable_xcom_pickling\"): \"True\"})",
      "",
      "---------------"
    ],
    "tests/models/test_xcom.py||tests/models/test_xcom.py": [
      "File: tests/models/test_xcom.py -> tests/models/test_xcom.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "140:             ret_value = XCom.get_value(key=\"xcom_test3\", ti_key=ti_key, session=session)",
      "141:         assert ret_value == {\"key\": \"value\"}",
      "144:         with conf_vars({(\"core\", \"enable_xcom_pickling\"): \"True\"}):",
      "145:             XCom.set(",
      "146:                 key=\"xcom_test3\",",
      "",
      "[Removed Lines]",
      "143:     def test_xcom_deserialize_with_pickle_to_json_switch(self, task_instance, session):",
      "",
      "[Added Lines]",
      "143:     def test_xcom_deserialize_pickle_when_xcom_pickling_is_disabled(self, task_instance, session):",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "151:                 session=session,",
      "152:             )",
      "153:         with conf_vars({(\"core\", \"enable_xcom_pickling\"): \"False\"}):",
      "163:     @conf_vars({(\"core\", \"xcom_enable_pickling\"): \"False\"})",
      "164:     def test_xcom_disable_pickle_type_fail_on_non_json(self, task_instance, session):",
      "",
      "[Removed Lines]",
      "154:             ret_value = XCom.get_one(",
      "155:                 key=\"xcom_test3\",",
      "156:                 dag_id=task_instance.dag_id,",
      "157:                 task_id=task_instance.task_id,",
      "158:                 run_id=task_instance.run_id,",
      "159:                 session=session,",
      "160:             )",
      "161:         assert ret_value == {\"key\": \"value\"}",
      "",
      "[Added Lines]",
      "154:             with pytest.raises(UnicodeDecodeError):",
      "155:                 XCom.get_one(",
      "156:                     key=\"xcom_test3\",",
      "157:                     dag_id=task_instance.dag_id,",
      "158:                     task_id=task_instance.task_id,",
      "159:                     run_id=task_instance.run_id,",
      "160:                     session=session,",
      "161:                 )",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "4b4fd21037640c10ef99e635ef6d8eec98839e74",
      "candidate_info": {
        "commit_hash": "4b4fd21037640c10ef99e635ef6d8eec98839e74",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/4b4fd21037640c10ef99e635ef6d8eec98839e74",
        "files": [
          "airflow/utils/log/file_task_handler.py"
        ],
        "message": "Restore function scoped httpx import in file_task_handler for perf (#36753)\n\n(cherry picked from commit c792b259699550d06054984f1643bb62df5ea37d)",
        "before_after_code_files": [
          "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py": [
          "File: airflow/utils/log/file_task_handler.py -> airflow/utils/log/file_task_handler.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "29: from typing import TYPE_CHECKING, Any, Callable, Iterable",
          "30: from urllib.parse import urljoin",
          "33: import pendulum",
          "35: from airflow.configuration import conf",
          "",
          "[Removed Lines]",
          "32: import httpx",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "81: def _fetch_logs_from_service(url, log_relative_path):",
          "82:     from airflow.utils.jwt_signer import JWTSigner",
          "84:     timeout = conf.getint(\"webserver\", \"log_fetch_timeout_sec\", fallback=None)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "81:     # Import occurs in function scope for perf. Ref: https://github.com/apache/airflow/pull/21438",
          "82:     import httpx",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "557:                 messages.append(f\"Found logs served from host {url}\")",
          "558:                 logs.append(response.text)",
          "559:         except Exception as e:",
          "561:                 messages.append(self.inherits_from_empty_operator_log_message)",
          "562:             else:",
          "563:                 messages.append(f\"Could not read served logs: {e}\")",
          "",
          "[Removed Lines]",
          "560:             if isinstance(e, httpx.UnsupportedProtocol) and ti.task.inherits_from_empty_operator is True:",
          "",
          "[Added Lines]",
          "562:             from httpx import UnsupportedProtocol",
          "564:             if isinstance(e, UnsupportedProtocol) and ti.task.inherits_from_empty_operator is True:",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "48afea83e6c692258a266930cc76df8950e80f4e",
      "candidate_info": {
        "commit_hash": "48afea83e6c692258a266930cc76df8950e80f4e",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/48afea83e6c692258a266930cc76df8950e80f4e",
        "files": [
          "airflow/example_dags/plugins/workday.py",
          "tests/plugins/workday.py"
        ],
        "message": "Straighten typing in workday timetable (#36296)\n\n(cherry picked from commit 954bb60e876b7cbb491ec7542ecdbb6bb9b8ab03)",
        "before_after_code_files": [
          "airflow/example_dags/plugins/workday.py||airflow/example_dags/plugins/workday.py",
          "tests/plugins/workday.py||tests/plugins/workday.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/example_dags/plugins/workday.py||airflow/example_dags/plugins/workday.py": [
          "File: airflow/example_dags/plugins/workday.py -> airflow/example_dags/plugins/workday.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "73:     ) -> DagRunInfo | None:",
          "74:         if last_automated_data_interval is not None:  # There was a previous run on the regular schedule.",
          "75:             last_start = last_automated_data_interval.start",
          "91:         # Skip weekends and holidays",
          "94:         if restriction.latest is not None and next_start > restriction.latest:",
          "95:             return None  # Over the DAG's scheduled end; don't schedule.",
          "",
          "[Removed Lines]",
          "76:             next_start = DateTime.combine((last_start + timedelta(days=1)).date(), Time.min).replace(",
          "77:                 tzinfo=UTC",
          "78:             )",
          "79:         else:  # This is the first ever run on the regular schedule.",
          "80:             next_start = restriction.earliest",
          "81:             if next_start is None:  # No start_date. Don't schedule.",
          "82:                 return None",
          "83:             if not restriction.catchup:",
          "84:                 # If the DAG has catchup=False, today is the earliest to consider.",
          "85:                 next_start = max(next_start, DateTime.combine(Date.today(), Time.min).replace(tzinfo=UTC))",
          "86:             elif next_start.time() != Time.min:",
          "87:                 # If earliest does not fall on midnight, skip to the next day.",
          "88:                 next_start = DateTime.combine(next_start.date() + timedelta(days=1), Time.min).replace(",
          "89:                     tzinfo=UTC",
          "90:                 )",
          "92:         next_start = self.get_next_workday(next_start)",
          "",
          "[Added Lines]",
          "76:             next_start = DateTime.combine((last_start + timedelta(days=1)).date(), Time.min)",
          "77:         # Otherwise this is the first ever run on the regular schedule...",
          "78:         elif (earliest := restriction.earliest) is None:",
          "79:             return None  # No start_date. Don't schedule.",
          "80:         elif not restriction.catchup:",
          "81:             # If the DAG has catchup=False, today is the earliest to consider.",
          "82:             next_start = max(earliest, DateTime.combine(Date.today(), Time.min))",
          "83:         elif earliest.time() != Time.min:",
          "84:             # If earliest does not fall on midnight, skip to the next day.",
          "85:             next_start = DateTime.combine(earliest.date() + timedelta(days=1), Time.min)",
          "86:         else:",
          "87:             next_start = earliest",
          "89:         next_start = self.get_next_workday(next_start.replace(tzinfo=UTC))",
          "",
          "---------------"
        ],
        "tests/plugins/workday.py||tests/plugins/workday.py": [
          "File: tests/plugins/workday.py -> tests/plugins/workday.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "15: # KIND, either express or implied.  See the License for the",
          "16: # specific language governing permissions and limitations",
          "17: # under the License.",
          "",
          "[Removed Lines]",
          "18: \"\"\"Plugin to demonstrate timetable registration and accommodate example DAGs.\"\"\"",
          "19: from __future__ import annotations",
          "21: import logging",
          "22: from datetime import timedelta",
          "24: # [START howto_timetable]",
          "25: from pendulum import UTC, Date, DateTime, Time",
          "27: from airflow.plugins_manager import AirflowPlugin",
          "28: from airflow.timetables.base import DagRunInfo, DataInterval, TimeRestriction, Timetable",
          "30: log = logging.getLogger(__name__)",
          "31: try:",
          "32:     from pandas.tseries.holiday import USFederalHolidayCalendar",
          "34:     holiday_calendar = USFederalHolidayCalendar()",
          "35: except ImportError:",
          "36:     log.warning(\"Could not import pandas. Holidays will not be considered.\")",
          "37:     holiday_calendar = None  # type: ignore[assignment]",
          "40: class AfterWorkdayTimetable(Timetable):",
          "41:     def get_next_workday(self, d: DateTime, incr=1) -> DateTime:",
          "42:         next_start = d",
          "43:         while True:",
          "44:             if next_start.weekday() in (5, 6):  # If next start is in the weekend go to next day",
          "45:                 next_start = next_start + incr * timedelta(days=1)",
          "46:                 continue",
          "47:             if holiday_calendar is not None:",
          "48:                 holidays = holiday_calendar.holidays(start=next_start, end=next_start).to_pydatetime()",
          "49:                 if next_start in holidays:  # If next start is a holiday go to next day",
          "50:                     next_start = next_start + incr * timedelta(days=1)",
          "51:                     continue",
          "52:             break",
          "53:         return next_start",
          "55:     # [START howto_timetable_infer_manual_data_interval]",
          "56:     def infer_manual_data_interval(self, run_after: DateTime) -> DataInterval:",
          "57:         start = DateTime.combine((run_after - timedelta(days=1)).date(), Time.min).replace(tzinfo=UTC)",
          "58:         # Skip backwards over weekends and holidays to find last run",
          "59:         start = self.get_next_workday(start, incr=-1)",
          "60:         return DataInterval(start=start, end=(start + timedelta(days=1)))",
          "62:     # [END howto_timetable_infer_manual_data_interval]",
          "64:     # [START howto_timetable_next_dagrun_info]",
          "65:     def next_dagrun_info(",
          "66:         self,",
          "68:         last_automated_data_interval: DataInterval | None,",
          "69:         restriction: TimeRestriction,",
          "70:     ) -> DagRunInfo | None:",
          "71:         if last_automated_data_interval is not None:  # There was a previous run on the regular schedule.",
          "72:             last_start = last_automated_data_interval.start",
          "73:             next_start = DateTime.combine((last_start + timedelta(days=1)).date(), Time.min).replace(",
          "74:                 tzinfo=UTC",
          "75:             )",
          "76:         else:  # This is the first ever run on the regular schedule.",
          "77:             next_start = restriction.earliest",
          "78:             if next_start is None:  # No start_date. Don't schedule.",
          "79:                 return None",
          "80:             if not restriction.catchup:",
          "81:                 # If the DAG has catchup=False, today is the earliest to consider.",
          "82:                 next_start = max(next_start, DateTime.combine(Date.today(), Time.min).replace(tzinfo=UTC))",
          "83:             elif next_start.time() != Time.min:",
          "84:                 # If earliest does not fall on midnight, skip to the next day.",
          "85:                 next_start = DateTime.combine(next_start.date() + timedelta(days=1), Time.min).replace(",
          "86:                     tzinfo=UTC",
          "87:                 )",
          "88:         # Skip weekends and holidays",
          "89:         next_start = self.get_next_workday(next_start)",
          "91:         if restriction.latest is not None and next_start > restriction.latest:",
          "92:             return None  # Over the DAG's scheduled end; don't schedule.",
          "93:         return DagRunInfo.interval(start=next_start, end=(next_start + timedelta(days=1)))",
          "95:     # [END howto_timetable_next_dagrun_info]",
          "98: class WorkdayTimetablePlugin(AirflowPlugin):",
          "99:     name = \"workday_timetable_plugin\"",
          "100:     timetables = [AfterWorkdayTimetable]",
          "103: # [END howto_timetable]",
          "",
          "[Added Lines]",
          "18: \"\"\"Plugin to demonstrate timetable registration and accommodate example DAGs.",
          "20: This simply forwards the timetable from ``airflow.example_dags``, so we can make",
          "21: it discoverable to unit tests without exposing the entire subpackage.",
          "22: \"\"\"",
          "23: from __future__ import annotations",
          "25: from airflow.example_dags.plugins.workday import AfterWorkdayTimetable, WorkdayTimetablePlugin",
          "27: __all__ = [\"AfterWorkdayTimetable\", \"WorkdayTimetablePlugin\"]",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "dd344fd961bdc81efb7698652a3c148fb918eafb",
      "candidate_info": {
        "commit_hash": "dd344fd961bdc81efb7698652a3c148fb918eafb",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/dd344fd961bdc81efb7698652a3c148fb918eafb",
        "files": [
          "README.md",
          "dev/breeze/src/airflow_breeze/global_constants.py",
          "generated/PYPI_README.md",
          "images/breeze/output_k8s_configure-cluster.svg",
          "images/breeze/output_k8s_configure-cluster.txt",
          "images/breeze/output_k8s_create-cluster.svg",
          "images/breeze/output_k8s_create-cluster.txt",
          "images/breeze/output_k8s_delete-cluster.svg",
          "images/breeze/output_k8s_delete-cluster.txt",
          "images/breeze/output_k8s_deploy-airflow.svg",
          "images/breeze/output_k8s_deploy-airflow.txt",
          "images/breeze/output_k8s_k9s.svg",
          "images/breeze/output_k8s_k9s.txt",
          "images/breeze/output_k8s_logs.svg",
          "images/breeze/output_k8s_logs.txt",
          "images/breeze/output_k8s_run-complete-tests.svg",
          "images/breeze/output_k8s_run-complete-tests.txt",
          "images/breeze/output_k8s_shell.svg",
          "images/breeze/output_k8s_shell.txt",
          "images/breeze/output_k8s_status.svg",
          "images/breeze/output_k8s_status.txt",
          "images/breeze/output_k8s_tests.svg",
          "images/breeze/output_k8s_tests.txt",
          "images/breeze/output_k8s_upload-k8s-image.svg",
          "images/breeze/output_k8s_upload-k8s-image.txt"
        ],
        "message": "feat: K8S 1.29 support (#36527)\n\n(cherry picked from commit 7e26f79d4b9f0dccf0d39db3d40efbf08aa8d083)",
        "before_after_code_files": [
          "dev/breeze/src/airflow_breeze/global_constants.py||dev/breeze/src/airflow_breeze/global_constants.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/breeze/src/airflow_breeze/global_constants.py||dev/breeze/src/airflow_breeze/global_constants.py": [
          "File: dev/breeze/src/airflow_breeze/global_constants.py -> dev/breeze/src/airflow_breeze/global_constants.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "74: #   - https://endoflife.date/amazon-eks",
          "75: #   - https://endoflife.date/azure-kubernetes-service",
          "76: #   - https://endoflife.date/google-kubernetes-engine",
          "78: ALLOWED_EXECUTORS = [",
          "79:     \"LocalExecutor\",",
          "80:     \"KubernetesExecutor\",",
          "",
          "[Removed Lines]",
          "77: ALLOWED_KUBERNETES_VERSIONS = [\"v1.25.11\", \"v1.26.6\", \"v1.27.3\", \"v1.28.0\"]",
          "",
          "[Added Lines]",
          "77: ALLOWED_KUBERNETES_VERSIONS = [\"v1.25.11\", \"v1.26.6\", \"v1.27.3\", \"v1.28.0\", \"v1.29.0\"]",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "998dd5c84c2f22a12e255557fa80774d8b8d1086",
      "candidate_info": {
        "commit_hash": "998dd5c84c2f22a12e255557fa80774d8b8d1086",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/998dd5c84c2f22a12e255557fa80774d8b8d1086",
        "files": [
          "airflow/www/yarn.lock"
        ],
        "message": "Bump follow-redirects from 1.15.3 to 1.15.4 in /airflow/www (#36700)\n\nBumps [follow-redirects](https://github.com/follow-redirects/follow-redirects) from 1.15.3 to 1.15.4.\n- [Release notes](https://github.com/follow-redirects/follow-redirects/releases)\n- [Commits](https://github.com/follow-redirects/follow-redirects/compare/v1.15.3...v1.15.4)\n\n---\nupdated-dependencies:\n- dependency-name: follow-redirects\n  dependency-type: indirect\n...\n\nSigned-off-by: dependabot[bot] <support@github.com>\nCo-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com>\n(cherry picked from commit 0d7bad20d4398b55bafcd6fa41db89fc2e509d69)",
        "before_after_code_files": [
          "airflow/www/yarn.lock||airflow/www/yarn.lock"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/www/yarn.lock||airflow/www/yarn.lock": [
          "File: airflow/www/yarn.lock -> airflow/www/yarn.lock",
          "--- Hunk 1 ---",
          "[Context before]",
          "6441:     tslib \"^2.0.3\"",
          "6443: follow-redirects@^1.15.0:",
          "6448: for-each@^0.3.3:",
          "6449:   version \"0.3.3\"",
          "",
          "[Removed Lines]",
          "6444:   version \"1.15.3\"",
          "6445:   resolved \"https://registry.yarnpkg.com/follow-redirects/-/follow-redirects-1.15.3.tgz#fe2f3ef2690afce7e82ed0b44db08165b207123a\"",
          "6446:   integrity sha512-1VzOtuEM8pC9SFU1E+8KfTjZyMztRsgEfwQl44z8A25uy13jSzTj6dyK2Df52iV0vgHCfBwLhDWevLn95w5v6Q==",
          "",
          "[Added Lines]",
          "6444:   version \"1.15.4\"",
          "6445:   resolved \"https://registry.yarnpkg.com/follow-redirects/-/follow-redirects-1.15.4.tgz#cdc7d308bf6493126b17ea2191ea0ccf3e535adf\"",
          "6446:   integrity sha512-Cr4D/5wlrb0z9dgERpUL3LrmPKVDsETIJhaCMeDfuFYcqa5bldGV6wBsAN6X/vxlXQtFBMrXdXxdL8CbDTGniw==",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "185c9ff1b8db124e0b644b85da6499519a3a06d5",
      "candidate_info": {
        "commit_hash": "185c9ff1b8db124e0b644b85da6499519a3a06d5",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/185c9ff1b8db124e0b644b85da6499519a3a06d5",
        "files": [
          "airflow/www/static/css/main.css"
        ],
        "message": "Increase width of execution_date input in trigger.html (#36278)\n\n(cherry picked from commit aed3c922402121c64264654f8dd77dbfc0168cbb)",
        "before_after_code_files": [
          "airflow/www/static/css/main.css||airflow/www/static/css/main.css"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/www/static/css/main.css||airflow/www/static/css/main.css": [
          "File: airflow/www/static/css/main.css -> airflow/www/static/css/main.css",
          "--- Hunk 1 ---",
          "[Context before]",
          "146: }",
          "148: input#execution_date {",
          "149:   margin-bottom: 0;",
          "150: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "149:   width: 220px;",
          "",
          "---------------"
        ]
      }
    }
  ]
}