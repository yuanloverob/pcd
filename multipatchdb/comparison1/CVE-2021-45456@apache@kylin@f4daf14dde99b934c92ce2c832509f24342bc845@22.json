{
  "cve_id": "CVE-2021-45456",
  "cve_desc": "Apache kylin checks the legitimacy of the project before executing some commands with the project name passed in by the user. There is a mismatch between what is being checked and what is being used as the shell command argument in DiagnosisService. This may cause an illegal project name to pass the check and perform the following steps, resulting in a command injection vulnerability. This issue affects Apache Kylin 4.0.0.",
  "repo": "apache/kylin",
  "patch_hash": "f4daf14dde99b934c92ce2c832509f24342bc845",
  "patch_info": {
    "commit_hash": "f4daf14dde99b934c92ce2c832509f24342bc845",
    "repo": "apache/kylin",
    "commit_url": "https://github.com/apache/kylin/commit/f4daf14dde99b934c92ce2c832509f24342bc845",
    "files": [
      "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java",
      "core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java",
      "core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java",
      "server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java",
      "server/src/main/webapp/WEB-INF/web.xml"
    ],
    "message": "test fix",
    "before_after_code_files": [
      "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java||core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java",
      "core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java||core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java",
      "core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java||core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java",
      "server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java||server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java"
    ]
  },
  "patch_diff": {
    "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java||core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java": [
      "File: core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java -> core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java",
      "--- Hunk 1 ---",
      "[Context before]",
      "3403:     public String getKerberosPrincipal() {",
      "3404:         return getOptional(\"kylin.kerberos.principal\");",
      "3405:     }",
      "3406: }",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "3407:     public String getEncryptCipherIvSpec() {",
      "3408:         return getOptional(\"kylin.security.encrypt.cipher.ivSpec\", \"AAAAAAAAAAAAAAAA\");",
      "3409:     }",
      "",
      "---------------"
    ],
    "core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java||core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java": [
      "File: core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java -> core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java",
      "--- Hunk 1 ---",
      "[Context before]",
      "25: import java.security.NoSuchAlgorithmException;",
      "27: import org.apache.commons.codec.binary.Base64;",
      "29: import javax.crypto.Cipher;",
      "30: import javax.crypto.NoSuchPaddingException;",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "28: import org.apache.kylin.common.KylinConfig;",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "42:             InvalidKeyException, NoSuchPaddingException, NoSuchAlgorithmException, UnsupportedEncodingException {",
      "43:         Cipher cipher = Cipher.getInstance(\"AES/CFB/PKCS5Padding\");",
      "44:         final SecretKeySpec secretKey = new SecretKeySpec(key, \"AES\");",
      "46:         cipher.init(cipherMode, secretKey, ivSpec);",
      "47:         return cipher;",
      "48:     }",
      "",
      "[Removed Lines]",
      "45:         IvParameterSpec ivSpec = new IvParameterSpec(\"AAAAAAAAAAAAAAAA\".getBytes(\"UTF-8\"));",
      "",
      "[Added Lines]",
      "46:         IvParameterSpec ivSpec = new IvParameterSpec(KylinConfig.getInstanceFromEnv().getEncryptCipherIvSpec().getBytes(\"UTF-8\"));",
      "",
      "---------------"
    ],
    "core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java||core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java": [
      "File: core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java -> core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java",
      "--- Hunk 1 ---",
      "[Context before]",
      "19: package org.apache.kylin.common.util;",
      "21: import org.junit.Assert;",
      "22: import org.junit.Test;",
      "26:     @Test",
      "27:     public void testAESEncrypt(){",
      "",
      "[Removed Lines]",
      "24: public class EncryptUtilTest {",
      "",
      "[Added Lines]",
      "21: import org.junit.After;",
      "23: import org.junit.Before;",
      "26: public class EncryptUtilTest extends LocalFileMetadataTestCase {",
      "27:     @Before",
      "28:     public void setUp() throws Exception {",
      "29:         this.createTestMetadata();",
      "30:     }",
      "32:     @After",
      "33:     public void after() throws Exception {",
      "34:         this.cleanupTestMetadata();",
      "35:     }",
      "",
      "---------------"
    ],
    "server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java||server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java": [
      "File: server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java -> server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java",
      "--- Hunk 1 ---",
      "[Context before]",
      "87:     public String dumpProjectDiagnosisInfo(String project, File exportPath) throws IOException {",
      "88:         Message msg = MsgPicker.getMsg();",
      "89:         ProjectInstance projectInstance =",
      "90:                 ProjectManager.getInstance(KylinConfig.getInstanceFromEnv())",
      "92:         if (null == projectInstance) {",
      "93:             throw new BadRequestException(",
      "95:         }",
      "96:         aclEvaluate.checkProjectOperationPermission(projectInstance);",
      "98:         runDiagnosisCLI(args);",
      "99:         return getDiagnosisPackageName(exportPath);",
      "100:     }",
      "",
      "[Removed Lines]",
      "91:                         .getProject(ValidateUtil.convertStringToBeAlphanumericUnderscore(project));",
      "94:                     String.format(Locale.ROOT, msg.getDIAG_PROJECT_NOT_FOUND(), project));",
      "97:         String[] args = { project, exportPath.getAbsolutePath() };",
      "",
      "[Added Lines]",
      "89:         String projectName = ValidateUtil.convertStringToBeAlphanumericUnderscore(project);",
      "92:                         .getProject(projectName);",
      "95:                     String.format(Locale.ROOT, msg.getDIAG_PROJECT_NOT_FOUND(), projectName));",
      "98:         String[] args = { projectName, exportPath.getAbsolutePath() };",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "9f7e5316effee916605cb883f9accfa46b849071",
      "candidate_info": {
        "commit_hash": "9f7e5316effee916605cb883f9accfa46b849071",
        "repo": "apache/kylin",
        "commit_url": "https://github.com/apache/kylin/commit/9f7e5316effee916605cb883f9accfa46b849071",
        "files": [
          "kylin-spark-project/kylin-spark-common/src/main/spark24/org/apache/spark/sql/execution/KylinFileSourceScanExec.scala",
          "kylin-spark-project/kylin-spark-common/src/main/spark31/org/apache/spark/sql/execution/KylinFileSourceScanExec.scala",
          "kylin-spark-project/kylin-spark-common/src/main/spark31/org/apache/spark/sql/hive/utils/QueryMetricUtils.scala",
          "kylin-spark-project/kylin-spark-engine/src/test/scala/org/apache/spark/deploy/StandaloneAppClientTest.scala",
          "kylin-spark-project/kylin-spark-metadata/src/main/spark31/org/apache/kylin/engine/spark/cross/CrossDateTimeUtils.scala",
          "kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/kylin/query/runtime/plans/AggregatePlan.scala",
          "kylin-spark-project/kylin-spark-test/pom.xml",
          "kylin-spark-project/kylin-spark-test/src/test/java/org/apache/kylin/engine/spark2/NBadQueryAndPushDownTest.java",
          "kylin-spark-project/kylin-spark-test/src/test/java/org/apache/kylin/engine/spark2/NBuildAndQueryTest.java",
          "kylin-spark-project/kylin-spark-test/src/test/java/org/apache/kylin/engine/spark2/file_pruning/NFilePruningTest.java"
        ],
        "message": "Fix IT (#1699)\n\n* Fix IT\n\n* fix\n\n* fix\n\n* fix exactlyMatchCuboidMultiSegmentTest()",
        "before_after_code_files": [
          "kylin-spark-project/kylin-spark-common/src/main/spark24/org/apache/spark/sql/execution/KylinFileSourceScanExec.scala||kylin-spark-project/kylin-spark-common/src/main/spark24/org/apache/spark/sql/execution/KylinFileSourceScanExec.scala",
          "kylin-spark-project/kylin-spark-common/src/main/spark31/org/apache/spark/sql/execution/KylinFileSourceScanExec.scala||kylin-spark-project/kylin-spark-common/src/main/spark31/org/apache/spark/sql/execution/KylinFileSourceScanExec.scala",
          "kylin-spark-project/kylin-spark-common/src/main/spark31/org/apache/spark/sql/hive/utils/QueryMetricUtils.scala||kylin-spark-project/kylin-spark-common/src/main/spark31/org/apache/spark/sql/hive/utils/QueryMetricUtils.scala",
          "kylin-spark-project/kylin-spark-engine/src/test/scala/org/apache/spark/deploy/StandaloneAppClientTest.scala||kylin-spark-project/kylin-spark-engine/src/test/scala/org/apache/spark/deploy/StandaloneAppClientTest.scala",
          "kylin-spark-project/kylin-spark-metadata/src/main/spark31/org/apache/kylin/engine/spark/cross/CrossDateTimeUtils.scala||kylin-spark-project/kylin-spark-metadata/src/main/spark31/org/apache/kylin/engine/spark/cross/CrossDateTimeUtils.scala",
          "kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/kylin/query/runtime/plans/AggregatePlan.scala||kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/kylin/query/runtime/plans/AggregatePlan.scala",
          "kylin-spark-project/kylin-spark-test/src/test/java/org/apache/kylin/engine/spark2/NBadQueryAndPushDownTest.java||kylin-spark-project/kylin-spark-test/src/test/java/org/apache/kylin/engine/spark2/NBadQueryAndPushDownTest.java",
          "kylin-spark-project/kylin-spark-test/src/test/java/org/apache/kylin/engine/spark2/NBuildAndQueryTest.java||kylin-spark-project/kylin-spark-test/src/test/java/org/apache/kylin/engine/spark2/NBuildAndQueryTest.java",
          "kylin-spark-project/kylin-spark-test/src/test/java/org/apache/kylin/engine/spark2/file_pruning/NFilePruningTest.java||kylin-spark-project/kylin-spark-test/src/test/java/org/apache/kylin/engine/spark2/file_pruning/NFilePruningTest.java"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/kylin/pull/1893",
          "https://github.com/apache/kylin/pull/2018",
          "https://github.com/apache/kylin/pull/2125",
          "https://github.com/apache/kylin/pull/2033",
          "https://github.com/apache/kylin/pull/2112",
          "https://github.com/apache/kylin/pull/2115",
          "https://github.com/apache/kylin/pull/1865",
          "https://github.com/apache/kylin/pull/1913",
          "https://github.com/apache/kylin/pull/2135"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "kylin-spark-project/kylin-spark-common/src/main/spark24/org/apache/spark/sql/execution/KylinFileSourceScanExec.scala||kylin-spark-project/kylin-spark-common/src/main/spark24/org/apache/spark/sql/execution/KylinFileSourceScanExec.scala": [
          "File: kylin-spark-project/kylin-spark-common/src/main/spark24/org/apache/spark/sql/execution/KylinFileSourceScanExec.scala -> kylin-spark-project/kylin-spark-common/src/main/spark24/org/apache/spark/sql/execution/KylinFileSourceScanExec.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "57:     ret",
          "58:   }",
          "61:     val readFile: (PartitionedFile) => Iterator[InternalRow] =",
          "62:       relation.fileFormat.buildReaderWithPartitionValues(",
          "63:         sparkSession = relation.sparkSession,",
          "",
          "[Removed Lines]",
          "60:   private lazy val _inputRDD: RDD[InternalRow] = {",
          "",
          "[Added Lines]",
          "60:   private lazy val inputRDD: RDD[InternalRow] = {",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "77:   }",
          "79:   override def inputRDDs(): Seq[RDD[InternalRow]] = {",
          "81:   }",
          "83:   @transient",
          "",
          "[Removed Lines]",
          "80:     _inputRDD :: Nil",
          "",
          "[Added Lines]",
          "80:     inputRDD :: Nil",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-common/src/main/spark31/org/apache/spark/sql/execution/KylinFileSourceScanExec.scala||kylin-spark-project/kylin-spark-common/src/main/spark31/org/apache/spark/sql/execution/KylinFileSourceScanExec.scala": [
          "File: kylin-spark-project/kylin-spark-common/src/main/spark31/org/apache/spark/sql/execution/KylinFileSourceScanExec.scala -> kylin-spark-project/kylin-spark-common/src/main/spark31/org/apache/spark/sql/execution/KylinFileSourceScanExec.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "53:       metrics.filter(e => driverMetrics.contains(e._1)).values.toSeq)",
          "54:   }",
          "57:     val optimizerMetadataTimeNs = relation.location.metadataOpsTimeNs.getOrElse(0L)",
          "58:     val startTime = System.nanoTime()",
          "59:     val ret = relation.location.listFiles(partitionFilters, dataFilters)",
          "61:     driverMetrics(\"numFiles\") = ret.map(_.files.size.toLong).sum",
          "62:     driverMetrics(\"filesSize\") = ret.map(_.files.map(_.getLen).sum).sum",
          "63:     if (relation.partitionSchemaOption.isDefined) {",
          "",
          "[Removed Lines]",
          "56:   @transient lazy val _selectedPartitions: Seq[PartitionDirectory] = {",
          "",
          "[Added Lines]",
          "56:   @transient override lazy val selectedPartitions: Array[PartitionDirectory] = {",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "67:     val timeTakenMs = NANOSECONDS.toMillis((System.nanoTime() - startTime) + optimizerMetadataTimeNs)",
          "68:     driverMetrics(\"metadataTime\") = timeTakenMs",
          "69:     ret",
          "73:     val readFile: (PartitionedFile) => Iterator[InternalRow] =",
          "74:       relation.fileFormat.buildReaderWithPartitionValues(",
          "75:         sparkSession = relation.sparkSession,",
          "",
          "[Removed Lines]",
          "70:   }",
          "72:   private lazy val _inputRDD: RDD[InternalRow] = {",
          "",
          "[Added Lines]",
          "69:   }.toArray",
          "71:   override lazy val inputRDD: RDD[InternalRow] = {",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "83:     val readRDD = optionalShardSpec match {",
          "84:       case Some(spec) if KylinConfig.getInstanceFromEnv.isShardingJoinOptEnabled =>",
          "86:       case _ =>",
          "88:     }",
          "89:     sendDriverMetrics()",
          "90:     readRDD",
          "91:   }",
          "93:   override def inputRDDs(): Seq[RDD[InternalRow]] = {",
          "95:   }",
          "97:   @transient",
          "",
          "[Removed Lines]",
          "85:         createShardingReadRDD(spec, readFile, _selectedPartitions, relation)",
          "87:         createNonShardingReadRDD(readFile, _selectedPartitions, relation)",
          "94:     _inputRDD :: Nil",
          "",
          "[Added Lines]",
          "84:         createShardingReadRDD(spec, readFile, selectedPartitions, relation)",
          "86:         createNonShardingReadRDD(readFile, selectedPartitions, relation)",
          "93:     inputRDD :: Nil",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-common/src/main/spark31/org/apache/spark/sql/hive/utils/QueryMetricUtils.scala||kylin-spark-project/kylin-spark-common/src/main/spark31/org/apache/spark/sql/hive/utils/QueryMetricUtils.scala": [
          "File: kylin-spark-project/kylin-spark-common/src/main/spark31/org/apache/spark/sql/hive/utils/QueryMetricUtils.scala -> kylin-spark-project/kylin-spark-common/src/main/spark31/org/apache/spark/sql/hive/utils/QueryMetricUtils.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "32:     try {",
          "33:       val metrics = plan.collect {",
          "34:         case exec: AdaptiveSparkPlanExec => metricLine(recursiveGetSparkPlan(exec.executedPlan))",
          "36:       }",
          "38:       val scanRows = metrics.map(metric => java.lang.Long.valueOf(metric._1))",
          "",
          "[Removed Lines]",
          "35:         case exec: SparkPlan => metricLine(exec)",
          "",
          "[Added Lines]",
          "35:         case exec: KylinFileSourceScanExec => metricLine(exec)",
          "36:         case exec: FileSourceScanExec => metricLine(exec)",
          "37:         case exec: HiveTableScanExec => metricLine(exec)",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-engine/src/test/scala/org/apache/spark/deploy/StandaloneAppClientTest.scala||kylin-spark-project/kylin-spark-engine/src/test/scala/org/apache/spark/deploy/StandaloneAppClientTest.scala": [
          "File: kylin-spark-project/kylin-spark-engine/src/test/scala/org/apache/spark/deploy/StandaloneAppClientTest.scala -> kylin-spark-project/kylin-spark-engine/src/test/scala/org/apache/spark/deploy/StandaloneAppClientTest.scala"
        ],
        "kylin-spark-project/kylin-spark-metadata/src/main/spark31/org/apache/kylin/engine/spark/cross/CrossDateTimeUtils.scala||kylin-spark-project/kylin-spark-metadata/src/main/spark31/org/apache/kylin/engine/spark/cross/CrossDateTimeUtils.scala": [
          "File: kylin-spark-project/kylin-spark-metadata/src/main/spark31/org/apache/kylin/engine/spark/cross/CrossDateTimeUtils.scala -> kylin-spark-project/kylin-spark-metadata/src/main/spark31/org/apache/kylin/engine/spark/cross/CrossDateTimeUtils.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "40:   }",
          "42:   def millisToDays(millis: Long): Int = {",
          "44:   }",
          "46:   def daysToMillis(days: Int): Long = {",
          "48:   }",
          "50:   def dateToString(): String = {",
          "",
          "[Removed Lines]",
          "43:     DateTimeUtils.microsToDays(millis * 1000, DEFAULT_TZ_ID)",
          "47:     DateTimeUtils.daysToMicros(days, DEFAULT_TZ_ID)",
          "",
          "[Added Lines]",
          "43:     DateTimeUtils.microsToDays(DateTimeUtils.millisToMicros(millis), DEFAULT_TZ_ID)",
          "47:     DateTimeUtils.microsToMillis(DateTimeUtils.daysToMicros(days, DEFAULT_TZ_ID))",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/kylin/query/runtime/plans/AggregatePlan.scala||kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/kylin/query/runtime/plans/AggregatePlan.scala": [
          "File: kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/kylin/query/runtime/plans/AggregatePlan.scala -> kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/kylin/query/runtime/plans/AggregatePlan.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "245:     }",
          "246:     val groupByCols = rel.getGroups.asScala.map(_.getIdentity).toSet",
          "247:     if (groupByCols.isEmpty) return false",
          "249:     if (!groupByContainsPartition(groupByCols, cuboid.getCubeDesc.getModel.getPartitionDesc) &&",
          "250:       olapContext.realization.asInstanceOf[CubeInstance].getSegments(SegmentStatusEnum.READY).size() != 1) {",
          "251:       return false",
          "",
          "[Removed Lines]",
          "248:     val f = olapContext.realization.asInstanceOf[CubeInstance].getSegments(SegmentStatusEnum.READY).size()",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-test/src/test/java/org/apache/kylin/engine/spark2/NBadQueryAndPushDownTest.java||kylin-spark-project/kylin-spark-test/src/test/java/org/apache/kylin/engine/spark2/NBadQueryAndPushDownTest.java": [
          "File: kylin-spark-project/kylin-spark-test/src/test/java/org/apache/kylin/engine/spark2/NBadQueryAndPushDownTest.java -> kylin-spark-project/kylin-spark-test/src/test/java/org/apache/kylin/engine/spark2/NBadQueryAndPushDownTest.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "30: import org.apache.kylin.query.util.QueryUtil;",
          "31: import org.apache.spark.sql.KylinSparkEnv;",
          "32: import org.apache.spark.sql.SparderContext;",
          "34: import org.junit.Assert;",
          "35: import org.junit.Test;",
          "37: import java.io.File;",
          "38: import java.nio.charset.StandardCharsets;",
          "",
          "[Removed Lines]",
          "33: import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;",
          "",
          "[Added Lines]",
          "35: import org.junit.Ignore;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "74:         }",
          "75:     }",
          "78:     public void testPushDownToNonExistentDB() {",
          "80:         try {",
          "",
          "[Removed Lines]",
          "77:     @Test",
          "",
          "[Added Lines]",
          "77:     @Ignore",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "84:             pushDownSql(getProject(), sql, 0, 0,",
          "85:                     new SQLException(new NoRealizationFoundException(\"testPushDownToNonExistentDB\")));",
          "86:         } catch (Exception e) {",
          "88:             Assert.assertTrue(ExceptionUtils.getRootCauseMessage(e)",
          "90:         }",
          "91:     }",
          "",
          "[Removed Lines]",
          "87:             Assert.assertTrue(ExceptionUtils.getRootCause(e) instanceof NoSuchTableException);",
          "89:                     .contains(\"Table or view 'lineitem' not found in database 'default'\"));",
          "",
          "[Added Lines]",
          "88:                     .contains(\"Table or view not found: lineitem\"));",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "109:     public void testPushDownNonEquiSql() throws Exception {",
          "110:         File sqlFile = new File(\"src/test/resources/query/sql_pushdown/query11.sql\");",
          "111:         String sql = new String(Files.readAllBytes(sqlFile.toPath()), StandardCharsets.UTF_8);",
          "113:         try {",
          "114:             NExecAndComp.queryCubeAndSkipCompute(DEFAULT_PROJECT_NAME, sql);",
          "115:         } catch (Exception e) {",
          "116:             if (e instanceof SQLException)",
          "120:         }",
          "121:     }",
          "",
          "[Removed Lines]",
          "112:         KylinConfig.getInstanceFromEnv().setProperty(PUSHDOWN_RUNNER_KEY, \"\");",
          "117:             KylinConfig.getInstanceFromEnv().setProperty(PUSHDOWN_RUNNER_KEY,",
          "118:                     \"org.apache.kylin.query.pushdown.PushDownRunnerSparkImpl\");",
          "119:             pushDownSql(getProject(), sql, 0, 0, (SQLException) e);",
          "",
          "[Added Lines]",
          "111:         KylinConfig.getInstanceFromEnv().setProperty(PUSHDOWN_RUNNER_KEY,",
          "112:                 \"org.apache.kylin.query.pushdown.PushDownRunnerSparkImpl\");",
          "117:                 pushDownSql(getProject(), sql, 0, 0, (SQLException) e);",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-test/src/test/java/org/apache/kylin/engine/spark2/NBuildAndQueryTest.java||kylin-spark-project/kylin-spark-test/src/test/java/org/apache/kylin/engine/spark2/NBuildAndQueryTest.java": [
          "File: kylin-spark-project/kylin-spark-test/src/test/java/org/apache/kylin/engine/spark2/NBuildAndQueryTest.java -> kylin-spark-project/kylin-spark-test/src/test/java/org/apache/kylin/engine/spark2/NBuildAndQueryTest.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "114:         populateSSWithCSVData(config, getProject(), KylinSparkEnv.getSparkSession());",
          "116:         List<QueryCallable> tasks = new ArrayList<>();",
          "118:         List<Pair<String, Throwable>> results = execAndGetResults(tasks);",
          "119:         Assert.assertEquals(results.size(), tasks.size());",
          "120:         report(results);",
          "",
          "[Removed Lines]",
          "117:         tasks.add(new QueryCallable(CompareLevel.SAME, \"left\", \"sql_exactly_agg\"));",
          "",
          "[Added Lines]",
          "117:         tasks.add(new QueryCallable(CompareLevel.SAME, \"left\", \"sql_exactly_agg_multi_segment\"));",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-test/src/test/java/org/apache/kylin/engine/spark2/file_pruning/NFilePruningTest.java||kylin-spark-project/kylin-spark-test/src/test/java/org/apache/kylin/engine/spark2/file_pruning/NFilePruningTest.java": [
          "File: kylin-spark-project/kylin-spark-test/src/test/java/org/apache/kylin/engine/spark2/file_pruning/NFilePruningTest.java -> kylin-spark-project/kylin-spark-test/src/test/java/org/apache/kylin/engine/spark2/file_pruning/NFilePruningTest.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "208:     private long assertResultsAndScanFiles(String sql, long numScanFiles) throws Exception {",
          "209:         Dataset<Row> dataset = queryCubeAndSkipCompute(getProject(), sql);",
          "210:         dataset.collect();",
          "212:         Assert.assertEquals(numScanFiles, actualNum);",
          "213:         return actualNum;",
          "214:     }",
          "",
          "[Removed Lines]",
          "211:         long actualNum = findFileSourceScanExec(dataset.queryExecution().sparkPlan()).metrics().get(\"numFiles\").get().value();",
          "",
          "[Added Lines]",
          "211:         long actualNum = findFileSourceScanExec(dataset.queryExecution().executedPlan())",
          "212:                 .metrics().get(\"numFiles\").get().value();",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "914b97f5cf2347030525140038d060178b93f955",
      "candidate_info": {
        "commit_hash": "914b97f5cf2347030525140038d060178b93f955",
        "repo": "apache/kylin",
        "commit_url": "https://github.com/apache/kylin/commit/914b97f5cf2347030525140038d060178b93f955",
        "files": [
          "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java",
          "examples/test_case_data/localmeta/kylin.properties",
          "kylin-spark-project/kylin-spark-common/src/main/scala/org/apache/spark/dict/NBucketDictionary.java",
          "kylin-spark-project/kylin-spark-common/src/main/scala/org/apache/spark/dict/NGlobalDictionary.java",
          "kylin-spark-project/kylin-spark-common/src/main/scala/org/apache/spark/sql/KylinFunctions.scala",
          "kylin-spark-project/kylin-spark-common/src/main/scala/org/apache/spark/sql/catalyst/expressions/KylinExpresssions.scala",
          "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/builder/CreateFlatTable.scala",
          "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/builder/CubeTableEncoder.scala",
          "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/ParentSourceChooser.scala",
          "kylin-spark-project/kylin-spark-engine/src/test/java/org/apache/kylin/engine/spark/LocalWithSparkSessionTest.java",
          "kylin-spark-project/kylin-spark-engine/src/test/scala/org/apache/kylin/engine/spark/builder/TestCreateFlatTable.scala"
        ],
        "message": "KYLIN-5011 Detect and scatter skewed data in dict encoding step (#1662)\n\nCo-authored-by: Xiaoxiang Yu <xxyu@apache.org>",
        "before_after_code_files": [
          "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java||core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java",
          "examples/test_case_data/localmeta/kylin.properties||examples/test_case_data/localmeta/kylin.properties",
          "kylin-spark-project/kylin-spark-common/src/main/scala/org/apache/spark/dict/NBucketDictionary.java||kylin-spark-project/kylin-spark-common/src/main/scala/org/apache/spark/dict/NBucketDictionary.java",
          "kylin-spark-project/kylin-spark-common/src/main/scala/org/apache/spark/dict/NGlobalDictionary.java||kylin-spark-project/kylin-spark-common/src/main/scala/org/apache/spark/dict/NGlobalDictionary.java",
          "kylin-spark-project/kylin-spark-common/src/main/scala/org/apache/spark/sql/KylinFunctions.scala||kylin-spark-project/kylin-spark-common/src/main/scala/org/apache/spark/sql/KylinFunctions.scala",
          "kylin-spark-project/kylin-spark-common/src/main/scala/org/apache/spark/sql/catalyst/expressions/KylinExpresssions.scala||kylin-spark-project/kylin-spark-common/src/main/scala/org/apache/spark/sql/catalyst/expressions/KylinExpresssions.scala",
          "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/builder/CreateFlatTable.scala||kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/builder/CreateFlatTable.scala",
          "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/builder/CubeTableEncoder.scala||kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/builder/CubeTableEncoder.scala",
          "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/ParentSourceChooser.scala||kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/ParentSourceChooser.scala",
          "kylin-spark-project/kylin-spark-engine/src/test/java/org/apache/kylin/engine/spark/LocalWithSparkSessionTest.java||kylin-spark-project/kylin-spark-engine/src/test/java/org/apache/kylin/engine/spark/LocalWithSparkSessionTest.java",
          "kylin-spark-project/kylin-spark-engine/src/test/scala/org/apache/kylin/engine/spark/builder/TestCreateFlatTable.scala||kylin-spark-project/kylin-spark-engine/src/test/scala/org/apache/kylin/engine/spark/builder/TestCreateFlatTable.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/kylin/pull/1893",
          "https://github.com/apache/kylin/pull/2018",
          "https://github.com/apache/kylin/pull/2125",
          "https://github.com/apache/kylin/pull/2033",
          "https://github.com/apache/kylin/pull/2112",
          "https://github.com/apache/kylin/pull/2115",
          "https://github.com/apache/kylin/pull/1865",
          "https://github.com/apache/kylin/pull/1913",
          "https://github.com/apache/kylin/pull/2135"
        ],
        "olp_code_files": {
          "patch": [
            "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java||core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java"
          ],
          "candidate": [
            "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java||core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java"
          ]
        }
      },
      "candidate_diff": {
        "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java||core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java": [
          "File: core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java -> core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "3168:     public boolean rePartitionEncodedDatasetWithRowKey() {",
          "3169:         return Boolean.valueOf(getOptional(\"kylin.engine.spark.repartition.encoded.dataset\", \"false\"));",
          "3170:     }",
          "3171: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "3175:     public boolean detectDataSkewInDictEncodingEnabled() {",
          "3176:         return Boolean.valueOf(getOptional(\"kylin.dictionary.detect.data.skew.in.encoding\", \"false\"));",
          "3177:     }",
          "3183:     public double sampleRateInEncodingSkewDetection() {",
          "3184:         return Double.valueOf(getOptional(\"kylin.dictionary.detect.data.skew.sample.rate\", \"0.1\"));",
          "3185:     }",
          "3196:     public double skewPercentageThreshHold() {",
          "3197:         return Double.valueOf(getOptional(\"kylin.dictionary.data.skew.percentage.threshhold\", \"0.05\"));",
          "3198:     }",
          "",
          "---------------"
        ],
        "examples/test_case_data/localmeta/kylin.properties||examples/test_case_data/localmeta/kylin.properties": [
          "File: examples/test_case_data/localmeta/kylin.properties -> examples/test_case_data/localmeta/kylin.properties",
          "--- Hunk 1 ---",
          "[Context before]",
          "162: kylin.query.auto-sparder-context=false",
          "164: kylin.metrics.query-cache.expire-seconds=5",
          "",
          "[Removed Lines]",
          "165: kylin.metrics.query-cache.max-entries=2",
          "",
          "[Added Lines]",
          "166: kylin.dictionary.detect.data.skew.in.encoding=true",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-common/src/main/scala/org/apache/spark/dict/NBucketDictionary.java||kylin-spark-project/kylin-spark-common/src/main/scala/org/apache/spark/dict/NBucketDictionary.java": [
          "File: kylin-spark-project/kylin-spark-common/src/main/scala/org/apache/spark/dict/NBucketDictionary.java -> kylin-spark-project/kylin-spark-common/src/main/scala/org/apache/spark/dict/NBucketDictionary.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: import java.io.IOException;",
          "22: import org.slf4j.Logger;",
          "23: import org.slf4j.LoggerFactory;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "22: import com.esotericsoftware.kryo.Kryo;",
          "23: import com.esotericsoftware.kryo.io.Input;",
          "24: import org.apache.commons.lang.StringUtils;",
          "25: import org.apache.hadoop.conf.Configuration;",
          "26: import org.apache.hadoop.fs.FileSystem;",
          "27: import org.apache.hadoop.fs.Path;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "36:     private Object2LongMap<String> absoluteDictMap;",
          "38:     private Object2LongMap<String> relativeDictMap;",
          "41:             throws IOException {",
          "42:         this.workingDir = workingDir;",
          "43:         this.bucketId = bucketId;",
          "",
          "[Removed Lines]",
          "40:     NBucketDictionary(String baseDir, String workingDir, int bucketId, NGlobalDictMetaInfo metainfo)",
          "",
          "[Added Lines]",
          "45:     private Object2LongMap<String> skewedDictMap;",
          "47:     NBucketDictionary(String baseDir, String workingDir, int bucketId, NGlobalDictMetaInfo metainfo, String skewDictStorageFile)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "49:             this.absoluteDictMap = globalDictStore.getBucketDict(versions[versions.length - 1], metainfo, bucketId);",
          "50:         }",
          "51:         this.relativeDictMap = new Object2LongOpenHashMap<>();",
          "52:     }",
          "54:     NBucketDictionary(String workingDir) {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "59:         if (!StringUtils.isEmpty(skewDictStorageFile)) {",
          "60:             FileSystem fs = FileSystem.get(new Configuration());",
          "61:             if (fs.exists(new Path(skewDictStorageFile))) {",
          "62:                 Kryo kryo = new Kryo();",
          "63:                 Input input = new Input(fs.open(new Path(skewDictStorageFile)));",
          "64:                 skewedDictMap = (Object2LongMap<String>) kryo.readClassAndObject(input);",
          "65:                 input.close();",
          "66:             }",
          "67:         }",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "72:     }",
          "74:     public long encode(Object value) {",
          "75:         return absoluteDictMap.getLong(value.toString());",
          "76:     }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "91:         if (null != skewedDictMap && skewedDictMap.containsKey(value.toString())) {",
          "92:             return skewedDictMap.getLong(value.toString());",
          "93:         }",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-common/src/main/scala/org/apache/spark/dict/NGlobalDictionary.java||kylin-spark-project/kylin-spark-common/src/main/scala/org/apache/spark/dict/NGlobalDictionary.java": [
          "File: kylin-spark-project/kylin-spark-common/src/main/scala/org/apache/spark/dict/NGlobalDictionary.java -> kylin-spark-project/kylin-spark-common/src/main/scala/org/apache/spark/dict/NGlobalDictionary.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "39:     private String sourceTable;",
          "40:     private String sourceColumn;",
          "41:     private boolean isFirst = true;",
          "43:     public String getResourceDir() {",
          "44:         return \"/\" + project + HadoopUtil.GLOBAL_DICT_STORAGE_ROOT + \"/\" + sourceTable + \"/\" + sourceColumn + \"/\";",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "42:     private String skewDictStorageFile;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "71:         if (metadata != null) {",
          "72:             isFirst = false;",
          "73:         }",
          "74:     }",
          "76:     public NBucketDictionary loadBucketDictionary(int bucketId) throws IOException {",
          "77:         if (null == metadata) {",
          "78:             metadata = getMetaInfo();",
          "79:         }",
          "81:     }",
          "83:     public NBucketDictionary createNewBucketDictionary() {",
          "",
          "[Removed Lines]",
          "80:         return new NBucketDictionary(baseDir, getWorkingDir(), bucketId, metadata);",
          "",
          "[Added Lines]",
          "75:         if (dictInfo.length >= 5) {",
          "76:             skewDictStorageFile = dictInfo[4];",
          "77:         }",
          "84:         return new NBucketDictionary(baseDir, getWorkingDir(), bucketId, metadata, skewDictStorageFile);",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-common/src/main/scala/org/apache/spark/sql/KylinFunctions.scala||kylin-spark-project/kylin-spark-common/src/main/scala/org/apache/spark/sql/KylinFunctions.scala": [
          "File: kylin-spark-project/kylin-spark-common/src/main/scala/org/apache/spark/sql/KylinFunctions.scala -> kylin-spark-project/kylin-spark-common/src/main/scala/org/apache/spark/sql/KylinFunctions.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "24: import org.apache.spark.sql.types._",
          "25: import org.apache.spark.sql.catalyst.expressions.{ApproxCountDistinctDecode, BinaryExpression,",
          "26:   DictEncode, Expression, ExpressionInfo, ExpressionUtils, ImplicitCastInputTypes, In,",
          "28:   TimestampAdd, TimestampDiff, Truncate, UnaryExpression}",
          "29: import org.apache.spark.sql.catalyst.expressions.aggregate.AggregateFunction",
          "30: import org.apache.spark.sql.udaf.{ApproxCountDistinct, IntersectCount, PreciseCountDistinct}",
          "",
          "[Removed Lines]",
          "27:   KylinAddMonths, Like, Literal, PreciseCountDistinctDecode, RoundBase, SplitPart, Sum0,",
          "",
          "[Added Lines]",
          "27:   KylinAddMonths, Like, Literal, PreciseCountDistinctDecode, RoundBase, ScatterSkewData, SplitPart, Sum0,",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "44:     Column(DictEncode(column.expr, dictParams.expr, bucketSize.expr))",
          "45:   }",
          "48:   def k_lit(literal: Any): Column = literal match {",
          "49:     case c: Column => c",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "47:   def scatter_skew_data(column: Column, skewDataStorage: Column): Column = {",
          "48:     Column(ScatterSkewData(column.expr, skewDataStorage.expr))",
          "49:   }",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-common/src/main/scala/org/apache/spark/sql/catalyst/expressions/KylinExpresssions.scala||kylin-spark-project/kylin-spark-common/src/main/scala/org/apache/spark/sql/catalyst/expressions/KylinExpresssions.scala": [
          "File: kylin-spark-project/kylin-spark-common/src/main/scala/org/apache/spark/sql/catalyst/expressions/KylinExpresssions.scala -> kylin-spark-project/kylin-spark-common/src/main/scala/org/apache/spark/sql/catalyst/expressions/KylinExpresssions.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "24: import org.apache.spark.sql.catalyst.InternalRow",
          "25: import org.apache.spark.sql.catalyst.expressions.aggregate.DeclarativeAggregate",
          "26: import org.apache.spark.sql.catalyst.expressions.codegen.Block._",
          "28: import org.apache.spark.sql.types._",
          "29: import org.roaringbitmap.longlong.Roaring64NavigableMap",
          "",
          "[Removed Lines]",
          "27: import org.apache.spark.sql.catalyst.expressions.codegen.{CodeGenerator, CodegenContext, ExprCode}",
          "",
          "[Added Lines]",
          "27: import org.apache.spark.sql.catalyst.expressions.codegen.{CodeGenerator, CodegenContext, ExprCode, FalseLiteral}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "484:   override def dataType: DataType = LongType",
          "486:   override def prettyName: String = \"approx_count_distinct_decode\"",
          "487: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "487: }",
          "489: case class ScatterSkewData(left: Expression, right: Expression) extends BinaryExpression with ExpectsInputTypes {",
          "490:   override protected def doGenCode(ctx: CodegenContext, ev: ExprCode) : ExprCode = {",
          "492:     val rand = ctx.addMutableState(\"java.util.Random\", \"rand\")",
          "493:     val skewData = ctx.addMutableState(\"it.unimi.dsi.fastutil.objects.Object2LongOpenHashMap\",",
          "494:       \"skewData\")",
          "495:     val skewDataStorage = right.simpleString",
          "497:     val initParamsFuncName = ctx.addNewFunction(s\"initParams\",",
          "498:       s\"\"\"",
          "499:          | private void initParams() {",
          "500:          |   ${rand} = new java.util.Random();",
          "501:          |   com.esotericsoftware.kryo.Kryo kryo = new com.esotericsoftware.kryo.Kryo();",
          "502:          |   try {",
          "503:          |       org.apache.hadoop.fs.FileSystem fs = org.apache.hadoop.fs.FileSystem.get(new org.apache.hadoop.conf.Configuration());",
          "504:          |       if (fs.exists(new org.apache.hadoop.fs.Path(\"${skewDataStorage}\"))) {",
          "505:          |           com.esotericsoftware.kryo.io.Input input = new com.esotericsoftware.kryo.io.Input(",
          "506:          |               fs.open(new org.apache.hadoop.fs.Path(\"${skewDataStorage}\")));",
          "507:          |           ${skewData} = (it.unimi.dsi.fastutil.objects.Object2LongOpenHashMap<String>) kryo.readClassAndObject(input);",
          "508:          |           input.close();",
          "509:          |       }",
          "510:          |   } catch (java.io.IOException e) {",
          "511:          |       throw new java.lang.RuntimeException(e);",
          "512:          |   }",
          "513:          | }",
          "514:         \"\"\".stripMargin)",
          "516:     val addSalt = ctx.addNewFunction(s\"addSalt\",",
          "517:       s\"\"\"",
          "518:          | private org.apache.spark.unsafe.types.UTF8String addSalt(org.apache.spark.unsafe.types.UTF8String val) {",
          "519:          |   if (null != ${skewData} && (null == val || ${skewData}.containsKey(val.toString()))) {",
          "520:          |      return org.apache.spark.unsafe.types.UTF8String.fromString(",
          "521:          |          java.lang.Integer.toString(${rand}.nextInt()));",
          "522:          |   } else {",
          "523:          |      return val;",
          "524:          |   }",
          "525:          | }",
          "526:         \"\"\".stripMargin)",
          "528:     ctx.addPartitionInitializationStatement(s\"$initParamsFuncName();\");",
          "530:     val leftGen = left.genCode(ctx)",
          "531:     val rightGen = right.genCode(ctx)",
          "532:     val resultCode = s\"\"\"${ev.value} = $addSalt(${leftGen.value});\"\"\"",
          "534:     ev.copy(code = code\"\"\"",
          "535:         ${leftGen.code}",
          "536:         ${rightGen.code}",
          "537:         ${CodeGenerator.javaType(dataType)} ${ev.value} = ${CodeGenerator.defaultValue(dataType)};",
          "538:         $resultCode\"\"\", isNull = FalseLiteral)",
          "540:   }",
          "542:   override def dataType: DataType = StringType",
          "544:   override def inputTypes: Seq[AbstractDataType] = Seq(AnyDataType, AnyDataType)",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/builder/CreateFlatTable.scala||kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/builder/CreateFlatTable.scala": [
          "File: kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/builder/CreateFlatTable.scala -> kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/builder/CreateFlatTable.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "36: class CreateFlatTable(val seg: SegmentInfo,",
          "37:                       val toBuildTree: SpanningTree,",
          "38:                       val ss: SparkSession,",
          "41:   import org.apache.kylin.engine.spark.builder.CreateFlatTable._",
          "",
          "[Removed Lines]",
          "39:                       val sourceInfo: NBuildSourceInfo) extends Logging {",
          "",
          "[Added Lines]",
          "39:                       val sourceInfo: NBuildSourceInfo,",
          "40:                       val jobId: String) extends Logging {",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "108:     val matchedCols = filterCols(ds, encodeCols)",
          "109:     var encodeDs = ds",
          "110:     if (!matchedCols.isEmpty) {",
          "112:     }",
          "113:     encodeDs",
          "114:   }",
          "",
          "[Removed Lines]",
          "111:       encodeDs = CubeTableEncoder.encodeTable(ds, seg, matchedCols.asJava)",
          "",
          "[Added Lines]",
          "112:       encodeDs = CubeTableEncoder.encodeTable(ds, seg, matchedCols.asJava, jobId)",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/builder/CubeTableEncoder.scala||kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/builder/CubeTableEncoder.scala": [
          "File: kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/builder/CubeTableEncoder.scala -> kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/builder/CubeTableEncoder.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: package org.apache.kylin.engine.spark.builder",
          "22: import org.apache.kylin.engine.spark.builder.CubeBuilderHelper.ENCODE_SUFFIX",
          "23: import org.apache.kylin.engine.spark.job.NSparkCubingUtil._",
          "24: import org.apache.kylin.engine.spark.metadata.{ColumnDesc, SegmentInfo}",
          "",
          "[Removed Lines]",
          "20: import java.util",
          "",
          "[Added Lines]",
          "20: import com.esotericsoftware.kryo.Kryo",
          "21: import com.esotericsoftware.kryo.io.Output",
          "22: import it.unimi.dsi.fastutil.objects.Object2LongOpenHashMap",
          "23: import org.apache.hadoop.conf.Configuration",
          "24: import org.apache.hadoop.fs.{Path}",
          "26: import java.util",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "27: import org.apache.spark.sql.KylinFunctions._",
          "28: import org.apache.spark.sql.functions.{col, _}",
          "29: import org.apache.spark.sql.types.StringType",
          "31: import org.apache.spark.utils.SparkVersionUtils",
          "33: import scala.collection.JavaConverters._",
          "",
          "[Removed Lines]",
          "30: import org.apache.spark.sql.{Dataset, Row}",
          "",
          "[Added Lines]",
          "35: import org.apache.spark.sql.{Dataset, Row, functions}",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "36: object CubeTableEncoder extends Logging {",
          "39:     if (SparkVersionUtils.isLessThanSparkVersion(\"2.4\", true)) {",
          "40:       assert(!ds.sparkSession.conf.get(\"spark.sql.adaptive.enabled\", \"false\").toBoolean,",
          "41:         \"Parameter 'spark.sql.adaptive.enabled' must be false when encode tables.\")",
          "",
          "[Removed Lines]",
          "38:   def encodeTable(ds: Dataset[Row], seg: SegmentInfo, cols: util.Set[ColumnDesc]): Dataset[Row] = {",
          "",
          "[Added Lines]",
          "43:   def encodeTable(ds: Dataset[Row], seg: SegmentInfo, cols: util.Set[ColumnDesc], jobId: String): Dataset[Row] = {",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "61:         val encodeColRef = convertFromDot(ref.identity)",
          "62:         val columnIndex = structType.fieldIndex(encodeColRef)",
          "65:           .mkString(SEPARATOR)",
          "66:         val aliasName = structType.apply(columnIndex).name.concat(ENCODE_SUFFIX)",
          "70:         partitionedDs = partitionedDs",
          "71:           .repartition(enlargedBucketSize, col(encodeColRef).cast(StringType))",
          "73:       }",
          "74:     )",
          "",
          "[Removed Lines]",
          "64:         val dictParams = Array(seg.project, ref.tableAliasName, ref.columnName, seg.kylinconf.getHdfsWorkingDirectory)",
          "67:         val encodeCol = dict_encode(col(encodeColRef).cast(StringType), lit(dictParams), lit(bucketSize).cast(StringType)).as(aliasName)",
          "68:         val columns = partitionedDs.schema.map(ty => col(ty.name)) ++ Seq(encodeCol)",
          "72:           .select(columns: _*)",
          "",
          "[Added Lines]",
          "69:         var dictParams = Array(seg.project, ref.tableAliasName, ref.columnName, seg.kylinconf.getHdfsWorkingDirectory)",
          "72:         var encodeCol = dict_encode(col(encodeColRef).cast(StringType), lit(dictParams), lit(bucketSize).cast(StringType)).as(aliasName)",
          "73:         val columns = partitionedDs.schema.map(ty => col(ty.name))",
          "75:         if (seg.kylinconf.detectDataSkewInDictEncodingEnabled()) {",
          "77:           val castEncodeColRef = col(encodeColRef).cast(StringType)",
          "78:           val sampleData = ds.select(castEncodeColRef).sample(seg.kylinconf.sampleRateInEncodingSkewDetection()).cache()",
          "79:           val totalCount = sampleData.count();",
          "80:           val skewDictStorage = new Path(seg.kylinconf.getJobTmpDir(seg.project) +",
          "81:             \"/\" + jobId + \"/skewed_data/\" + ref.identity)",
          "82:           val skewedDict = new Object2LongOpenHashMap[String]()",
          "83:           sampleData.groupBy(encodeColRef)",
          "84:             .agg(functions.count(lit(1)).alias(\"count_value\"))",
          "85:             .filter(col(\"count_value\") > totalCount * seg.kylinconf.skewPercentageThreshHold())",
          "86:             .repartition(enlargedBucketSize, castEncodeColRef)",
          "87:             .select(Seq(castEncodeColRef, encodeCol): _*)",
          "88:             .collect().foreach(row => skewedDict.put(row.getString(0), row.getLong(1)));",
          "89:           sampleData.unpersist()",
          "92:           if (skewedDict.size() > 0) {",
          "93:             val kryo = new Kryo()",
          "94:             val fs = skewDictStorage.getFileSystem(new Configuration())",
          "95:             if (fs.exists(skewDictStorage)) {",
          "96:               fs.delete(skewDictStorage, true)",
          "97:             }",
          "98:             val output = new Output(fs.create(skewDictStorage))",
          "99:             kryo.writeClassAndObject(output, skewedDict)",
          "100:             output.close()",
          "103:             val scatterColumn = scatter_skew_data(castEncodeColRef, lit(skewDictStorage.toString))",
          "104:               .alias(\"scatter_skew_data_\" + ref.columnName)",
          "107:             dictParams = Array(seg.project, ref.tableAliasName, ref.columnName, seg.kylinconf.getHdfsWorkingDirectory, skewDictStorage.toString)",
          "108:               .mkString(SEPARATOR)",
          "109:             encodeCol = dict_encode(col(encodeColRef).cast(StringType), lit(dictParams), lit(bucketSize).cast(StringType)).alias(aliasName)",
          "111:             partitionedDs = partitionedDs.select(columns ++ Seq(scatterColumn): _*)",
          "112:               .repartition(enlargedBucketSize, col(\"scatter_skew_data_\" + ref.columnName))",
          "113:               .select(columns ++ Seq(encodeCol): _*)",
          "114:             return partitionedDs;",
          "115:           }",
          "116:         }",
          "119:           .select(columns ++ Seq(encodeCol): _*)",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/ParentSourceChooser.scala||kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/ParentSourceChooser.scala": [
          "File: kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/ParentSourceChooser.scala -> kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/ParentSourceChooser.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "189:     val afterJoin: Dataset[Row] = flatTable.generateDataset(needEncoding, true)",
          "190:     sourceInfo.setFlatTableDS(afterJoin)",
          "",
          "[Removed Lines]",
          "188:     val flatTable = new CreateFlatTable(segInfo, toBuildTree, ss, sourceInfo)",
          "",
          "[Added Lines]",
          "188:     val flatTable = new CreateFlatTable(segInfo, toBuildTree, ss, sourceInfo, jobId)",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-engine/src/test/java/org/apache/kylin/engine/spark/LocalWithSparkSessionTest.java||kylin-spark-project/kylin-spark-engine/src/test/java/org/apache/kylin/engine/spark/LocalWithSparkSessionTest.java": [
          "File: kylin-spark-project/kylin-spark-engine/src/test/java/org/apache/kylin/engine/spark/LocalWithSparkSessionTest.java -> kylin-spark-project/kylin-spark-engine/src/test/java/org/apache/kylin/engine/spark/LocalWithSparkSessionTest.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "313:         CreateFlatTable flatTable = new CreateFlatTable(",
          "314:                 MetadataConverter.getSegmentInfo(segment.getCubeInstance(), segment.getUuid(),",
          "316:         Dataset<Row> ds = flatTable.generateDataset(false, true);",
          "317:         return ds;",
          "318:     }",
          "",
          "[Removed Lines]",
          "315:                         segment.getName(), segment.getStorageLocationIdentifier()), null, ss, null);",
          "",
          "[Added Lines]",
          "315:                         segment.getName(), segment.getStorageLocationIdentifier()), null, ss, null, ss.sparkContext().applicationId());",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-engine/src/test/scala/org/apache/kylin/engine/spark/builder/TestCreateFlatTable.scala||kylin-spark-project/kylin-spark-engine/src/test/scala/org/apache/kylin/engine/spark/builder/TestCreateFlatTable.scala": [
          "File: kylin-spark-project/kylin-spark-engine/src/test/scala/org/apache/kylin/engine/spark/builder/TestCreateFlatTable.scala -> kylin-spark-project/kylin-spark-engine/src/test/scala/org/apache/kylin/engine/spark/builder/TestCreateFlatTable.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "96:     val afterJoin1 = generateFlatTable(seg1, cube, true)",
          "97:     afterJoin1.collect()",
          "99:     if (SPARK_VERSION.startsWith(\"2.4\")) {",
          "100:       val jobs = helper.getJobsByGroupId(groupId)",
          "102:     } else if (SPARK_VERSION.startsWith(\"3.1\")) {",
          "104:       val jobs = helper.getJobsByGroupId(null)",
          "",
          "[Removed Lines]",
          "101:       Assert.assertEquals(jobs.length, 15)",
          "",
          "[Added Lines]",
          "102:       if (seg1.getConfig.detectDataSkewInDictEncodingEnabled()) {",
          "103:         Assert.assertEquals(jobs.length, 18)",
          "104:       } else {",
          "105:         Assert.assertEquals(jobs.length, 15)",
          "106:       }",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "140:   private def generateFlatTable(segment: CubeSegment, cube: CubeInstance, needEncode: Boolean): Dataset[Row] = {",
          "141:     val seg = MetadataConverter.getSegmentInfo(segment.getCubeInstance, segment.getUuid, segment.getName, segment.getStorageLocationIdentifier)",
          "142:     val spanningTree = new ForestSpanningTree(JavaConversions.asJavaCollection(seg.toBuildLayouts))",
          "144:     val afterJoin = flatTable.generateDataset(needEncode)",
          "145:     afterJoin",
          "146:   }",
          "",
          "[Removed Lines]",
          "143:     val flatTable = new CreateFlatTable(seg, spanningTree, spark, null)",
          "",
          "[Added Lines]",
          "149:     val flatTable = new CreateFlatTable(seg, spanningTree, spark, null, spark.sparkContext.applicationId)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "367fa70b6582123bd10f919813fdf543c87e19d9",
      "candidate_info": {
        "commit_hash": "367fa70b6582123bd10f919813fdf543c87e19d9",
        "repo": "apache/kylin",
        "commit_url": "https://github.com/apache/kylin/commit/367fa70b6582123bd10f919813fdf543c87e19d9",
        "files": [
          "server-base/src/main/java/org/apache/kylin/rest/controller/CubeController.java",
          "server-base/src/main/java/org/apache/kylin/rest/service/CubeService.java"
        ],
        "message": "KYLIN-4608 add deletecubefast api for delete 300 cubes fast\n\n(cherry picked from commit 99317f4a3d8fb8cd479b9bb265767f4880236052)",
        "before_after_code_files": [
          "server-base/src/main/java/org/apache/kylin/rest/controller/CubeController.java||server-base/src/main/java/org/apache/kylin/rest/controller/CubeController.java",
          "server-base/src/main/java/org/apache/kylin/rest/service/CubeService.java||server-base/src/main/java/org/apache/kylin/rest/service/CubeService.java"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/kylin/pull/1893",
          "https://github.com/apache/kylin/pull/2018",
          "https://github.com/apache/kylin/pull/2125",
          "https://github.com/apache/kylin/pull/2033",
          "https://github.com/apache/kylin/pull/2112",
          "https://github.com/apache/kylin/pull/2115",
          "https://github.com/apache/kylin/pull/1865",
          "https://github.com/apache/kylin/pull/1913",
          "https://github.com/apache/kylin/pull/2135"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "server-base/src/main/java/org/apache/kylin/rest/controller/CubeController.java||server-base/src/main/java/org/apache/kylin/rest/controller/CubeController.java": [
          "File: server-base/src/main/java/org/apache/kylin/rest/controller/CubeController.java -> server-base/src/main/java/org/apache/kylin/rest/controller/CubeController.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "632:     }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "634:     @RequestMapping(value = \"/{cubeName}/fast\", method = {RequestMethod.DELETE})",
          "635:     @ResponseBody",
          "636:     public void deleteCubeFast(@PathVariable String cubeName) {",
          "637:         checkCubeExists(cubeName);",
          "638:         CubeInstance cube = cubeService.getCubeManager().getCube(cubeName);",
          "641:         try {",
          "642:             cubeService.deleteCubeFast(cube);",
          "643:         } catch (Exception e) {",
          "644:             logger.error(e.getLocalizedMessage(), e);",
          "645:             throw new InternalErrorException(\"Failed to delete cube. \" + \" Caused by: \" + e.getMessage(), e);",
          "646:         }",
          "648:     }",
          "",
          "---------------"
        ],
        "server-base/src/main/java/org/apache/kylin/rest/service/CubeService.java||server-base/src/main/java/org/apache/kylin/rest/service/CubeService.java": [
          "File: server-base/src/main/java/org/apache/kylin/rest/service/CubeService.java -> server-base/src/main/java/org/apache/kylin/rest/service/CubeService.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "363:         cleanSegmentStorage(cube, toRemoveSegs);",
          "364:     }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "366:     public void deleteCubeFast(CubeInstance cube) throws IOException {",
          "367:         aclEvaluate.checkProjectWritePermission(cube);",
          "369:         int cubeNum = getCubeManager().getCubesByDesc(cube.getDescriptor().getName()).size();",
          "370:         getCubeManager().dropCube(cube.getName(), cubeNum == 1);//only delete cube desc when no other cube is using it",
          "372:     }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "dc3fd9ab25ecd03ec53f8855f5366db8de542ce5",
      "candidate_info": {
        "commit_hash": "dc3fd9ab25ecd03ec53f8855f5366db8de542ce5",
        "repo": "apache/kylin",
        "commit_url": "https://github.com/apache/kylin/commit/dc3fd9ab25ecd03ec53f8855f5366db8de542ce5",
        "files": [
          "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/OptimizeBuildJob.java",
          "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CubeBuildJob.java",
          "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CubeMergeJob.java",
          "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/utils/JobMetricsUtils.scala",
          "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/utils/QueryExecutionInterceptListener.scala"
        ],
        "message": "[KYLIN-5121] Make JobMetricsUtils.collectMetrics be working again",
        "before_after_code_files": [
          "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/OptimizeBuildJob.java||kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/OptimizeBuildJob.java",
          "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CubeBuildJob.java||kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CubeBuildJob.java",
          "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CubeMergeJob.java||kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CubeMergeJob.java",
          "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/utils/JobMetricsUtils.scala||kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/utils/JobMetricsUtils.scala",
          "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/utils/QueryExecutionInterceptListener.scala||kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/utils/QueryExecutionInterceptListener.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/kylin/pull/1893",
          "https://github.com/apache/kylin/pull/2018",
          "https://github.com/apache/kylin/pull/2125",
          "https://github.com/apache/kylin/pull/2033",
          "https://github.com/apache/kylin/pull/2112",
          "https://github.com/apache/kylin/pull/2115",
          "https://github.com/apache/kylin/pull/1865",
          "https://github.com/apache/kylin/pull/1913",
          "https://github.com/apache/kylin/pull/2135"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/OptimizeBuildJob.java||kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/OptimizeBuildJob.java": [
          "File: kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/OptimizeBuildJob.java -> kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/OptimizeBuildJob.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "41: import org.apache.kylin.engine.spark.metadata.cube.model.ForestSpanningTree;",
          "42: import org.apache.kylin.engine.spark.metadata.cube.model.LayoutEntity;",
          "43: import org.apache.kylin.engine.spark.metadata.cube.model.SpanningTree;",
          "44: import org.apache.kylin.engine.spark.utils.JobMetrics;",
          "45: import org.apache.kylin.engine.spark.utils.JobMetricsUtils;",
          "46: import org.apache.kylin.engine.spark.utils.Metrics;",
          "51: import org.apache.kylin.measure.hllc.HLLCounter;",
          "52: import org.apache.kylin.metadata.MetadataConstants;",
          "53: import org.apache.kylin.shaded.com.google.common.base.Joiner;",
          "54: import org.apache.kylin.shaded.com.google.common.collect.Lists;",
          "55: import org.apache.kylin.shaded.com.google.common.collect.Maps;",
          "56: import org.apache.kylin.storage.StorageFactory;",
          "",
          "[Removed Lines]",
          "47: import org.apache.kylin.engine.spark.utils.QueryExecutionCache;",
          "48: import org.apache.kylin.engine.spark.utils.BuildUtils;",
          "49: import org.apache.kylin.metadata.model.IStorageAware;",
          "50: import org.apache.kylin.shaded.com.google.common.base.Preconditions;",
          "",
          "[Added Lines]",
          "44: import org.apache.kylin.engine.spark.utils.BuildUtils;",
          "50: import org.apache.kylin.metadata.model.IStorageAware;",
          "52: import org.apache.kylin.shaded.com.google.common.base.Preconditions;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "63: import scala.collection.JavaConversions;",
          "65: import java.io.IOException;",
          "67: import java.util.HashMap;",
          "68: import java.util.List;",
          "69: import java.util.Map;",
          "70: import java.util.Set;",
          "76: import java.util.stream.Collectors;",
          "78: public class OptimizeBuildJob extends SparkApplication {",
          "",
          "[Removed Lines]",
          "71: import java.util.Collection;",
          "72: import java.util.LinkedList;",
          "73: import java.util.ArrayList;",
          "74: import java.util.UUID;",
          "",
          "[Added Lines]",
          "65: import java.util.ArrayList;",
          "66: import java.util.Collection;",
          "68: import java.util.LinkedList;",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "378:                                      long parentId) throws IOException {",
          "379:         long layoutId = layout.getId();",
          "385:         NSparkCubingEngine.NSparkCubingStorage storage = StorageFactory.createEngineAdapter(layout,",
          "386:                 NSparkCubingEngine.NSparkCubingStorage.class);",
          "387:         String path = PathManager.getParquetStoragePath(config, getParam(MetadataConstants.P_CUBE_NAME), seg.name(), seg.identifier(),",
          "388:                 String.valueOf(layoutId));",
          "389:         String tempPath = path + TEMP_DIR_SUFFIX;",
          "391:         logger.info(\"Cuboids are saved to temp path : \" + tempPath);",
          "392:         storage.saveTo(tempPath, dataset, ss);",
          "",
          "[Removed Lines]",
          "382:         String queryExecutionId = UUID.randomUUID().toString();",
          "383:         ss.sparkContext().setLocalProperty(QueryExecutionCache.N_EXECUTION_ID_KEY(), queryExecutionId);",
          "",
          "[Added Lines]",
          "384:         String queryExecutionId = tempPath;",
          "385:         JobMetricsUtils.registerQueryExecutionListener(ss, queryExecutionId);",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "402:             cuboidsRowCount.putIfAbsent(layoutId, cuboidRowCnt);",
          "403:             layout.setSourceRows(cuboidsRowCount.get(parentId));",
          "404:         } else {",
          "405:             layout.setRows(rowCount);",
          "406:             layout.setSourceRows(metrics.getMetrics(Metrics.SOURCE_ROWS_CNT()));",
          "407:         }",
          "408:         int shardNum = BuildUtils.repartitionIfNeed(layout, storage, path, tempPath, cubeInstance.getConfig(), ss);",
          "409:         layout.setShardNum(shardNum);",
          "410:         cuboidShardNum.put(layoutId, (short) shardNum);",
          "413:         BuildUtils.fillCuboidInfo(layout, path);",
          "414:     }",
          "",
          "[Removed Lines]",
          "411:         ss.sparkContext().setLocalProperty(QueryExecutionCache.N_EXECUTION_ID_KEY(), null);",
          "412:         QueryExecutionCache.removeQueryExecution(queryExecutionId);",
          "",
          "[Added Lines]",
          "401:             cuboidsRowCount.putIfAbsent(layoutId, rowCount);",
          "408:         JobMetricsUtils.unRegisterQueryExecutionListener(ss, queryExecutionId);",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CubeBuildJob.java||kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CubeBuildJob.java": [
          "File: kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CubeBuildJob.java -> kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CubeBuildJob.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "19: package org.apache.kylin.engine.spark.job;",
          "37: import org.apache.hadoop.fs.FSDataInputStream;",
          "47: import org.apache.hadoop.fs.FileStatus;",
          "48: import org.apache.hadoop.fs.FileSystem;",
          "49: import org.apache.hadoop.fs.Path;",
          "50: import org.apache.hadoop.util.StringUtils;",
          "51: import org.apache.kylin.common.util.HadoopUtil;",
          "52: import org.apache.kylin.cube.CubeInstance;",
          "53: import org.apache.kylin.cube.CubeManager;",
          "54: import org.apache.kylin.cube.CubeSegment;",
          "55: import org.apache.kylin.cube.CubeUpdate;",
          "56: import org.apache.kylin.engine.spark.NSparkCubingEngine;",
          "57: import org.apache.kylin.engine.spark.application.SparkApplication;",
          "58: import org.apache.kylin.engine.spark.builder.NBuildSourceInfo;",
          "",
          "[Removed Lines]",
          "21: import java.io.IOException;",
          "22: import java.util.ArrayList;",
          "23: import java.util.Collection;",
          "24: import java.util.Collections;",
          "25: import java.util.Comparator;",
          "26: import java.util.HashMap;",
          "27: import java.util.LinkedList;",
          "28: import java.util.List;",
          "29: import java.util.Locale;",
          "30: import java.util.Map;",
          "31: import java.util.Optional;",
          "32: import java.util.Set;",
          "33: import java.util.UUID;",
          "34: import java.util.concurrent.ConcurrentHashMap;",
          "35: import java.util.stream.Collectors;",
          "38: import org.apache.kylin.common.KylinConfig;",
          "39: import org.apache.kylin.common.persistence.ResourceStore;",
          "40: import org.apache.kylin.cube.cuboid.CuboidModeEnum;",
          "41: import org.apache.kylin.engine.mr.common.BatchConstants;",
          "42: import org.apache.kylin.engine.mr.common.CubeStatsWriter;",
          "43: import org.apache.kylin.engine.mr.common.StatisticsDecisionUtil;",
          "44: import org.apache.kylin.measure.hllc.HLLCounter;",
          "45: import org.apache.kylin.shaded.com.google.common.base.Joiner;",
          "46: import org.apache.kylin.shaded.com.google.common.collect.Maps;",
          "",
          "[Added Lines]",
          "26: import org.apache.kylin.common.persistence.ResourceStore;",
          "32: import org.apache.kylin.cube.cuboid.CuboidModeEnum;",
          "33: import org.apache.kylin.engine.mr.common.BatchConstants;",
          "34: import org.apache.kylin.engine.mr.common.CubeStatsWriter;",
          "35: import org.apache.kylin.engine.mr.common.StatisticsDecisionUtil;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "66: import org.apache.kylin.engine.spark.utils.JobMetrics;",
          "67: import org.apache.kylin.engine.spark.utils.JobMetricsUtils;",
          "68: import org.apache.kylin.engine.spark.utils.Metrics;",
          "70: import org.apache.kylin.metadata.MetadataConstants;",
          "71: import org.apache.kylin.metadata.model.IStorageAware;",
          "72: import org.apache.kylin.storage.StorageFactory;",
          "73: import org.apache.spark.api.java.JavaRDD;",
          "74: import org.apache.spark.api.java.JavaSparkContext;",
          "",
          "[Removed Lines]",
          "69: import org.apache.kylin.engine.spark.utils.QueryExecutionCache;",
          "",
          "[Added Lines]",
          "49: import org.apache.kylin.measure.hllc.HLLCounter;",
          "52: import org.apache.kylin.shaded.com.google.common.base.Joiner;",
          "53: import org.apache.kylin.shaded.com.google.common.base.Preconditions;",
          "54: import org.apache.kylin.shaded.com.google.common.collect.Lists;",
          "55: import org.apache.kylin.shaded.com.google.common.collect.Maps;",
          "56: import org.apache.kylin.shaded.com.google.common.collect.Sets;",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "77: import org.apache.spark.sql.hive.utils.ResourceDetectUtils;",
          "78: import org.slf4j.Logger;",
          "79: import org.slf4j.LoggerFactory;",
          "85: import scala.Tuple2;",
          "86: import scala.collection.JavaConversions;",
          "88: public class CubeBuildJob extends SparkApplication {",
          "89:     protected static final Logger logger = LoggerFactory.getLogger(CubeBuildJob.class);",
          "90:     protected static String TEMP_DIR_SUFFIX = \"_temp\";",
          "",
          "[Removed Lines]",
          "81: import org.apache.kylin.shaded.com.google.common.base.Preconditions;",
          "82: import org.apache.kylin.shaded.com.google.common.collect.Lists;",
          "83: import org.apache.kylin.shaded.com.google.common.collect.Sets;",
          "",
          "[Added Lines]",
          "68: import java.io.IOException;",
          "69: import java.util.ArrayList;",
          "70: import java.util.Collection;",
          "71: import java.util.Collections;",
          "72: import java.util.Comparator;",
          "73: import java.util.HashMap;",
          "74: import java.util.LinkedList;",
          "75: import java.util.List;",
          "76: import java.util.Locale;",
          "77: import java.util.Map;",
          "78: import java.util.Optional;",
          "79: import java.util.Set;",
          "80: import java.util.concurrent.ConcurrentHashMap;",
          "81: import java.util.stream.Collectors;",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "457:                                      long parentId) throws IOException {",
          "458:         long layoutId = layout.getId();",
          "464:         NSparkCubingEngine.NSparkCubingStorage storage = StorageFactory.createEngineAdapter(layout,",
          "465:                 NSparkCubingEngine.NSparkCubingStorage.class);",
          "466:         String path = PathManager.getParquetStoragePath(config, getParam(MetadataConstants.P_CUBE_NAME), seg.name(), seg.identifier(),",
          "467:                 String.valueOf(layoutId));",
          "468:         String tempPath = path + TEMP_DIR_SUFFIX;",
          "470:         logger.info(\"Cuboids are saved to temp path : \" + tempPath);",
          "471:         storage.saveTo(tempPath, dataset, ss);",
          "",
          "[Removed Lines]",
          "461:         String queryExecutionId = UUID.randomUUID().toString();",
          "462:         ss.sparkContext().setLocalProperty(QueryExecutionCache.N_EXECUTION_ID_KEY(), queryExecutionId);",
          "",
          "[Added Lines]",
          "461:         String queryExecutionId = tempPath;",
          "462:         JobMetricsUtils.registerQueryExecutionListener(ss, queryExecutionId);",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "479:             cuboidsRowCount.putIfAbsent(layoutId, cuboidRowCnt);",
          "480:             layout.setSourceRows(cuboidsRowCount.get(parentId));",
          "481:         } else {",
          "482:             layout.setRows(rowCount);",
          "483:             layout.setSourceRows(metrics.getMetrics(Metrics.SOURCE_ROWS_CNT()));",
          "484:         }",
          "485:         int shardNum = BuildUtils.repartitionIfNeed(layout, storage, path, tempPath, cubeInstance.getConfig(), ss);",
          "486:         layout.setShardNum(shardNum);",
          "487:         cuboidShardNum.put(layoutId, (short) shardNum);",
          "490:         BuildUtils.fillCuboidInfo(layout, path);",
          "491:     }",
          "",
          "[Removed Lines]",
          "488:         ss.sparkContext().setLocalProperty(QueryExecutionCache.N_EXECUTION_ID_KEY(), null);",
          "489:         QueryExecutionCache.removeQueryExecution(queryExecutionId);",
          "",
          "[Added Lines]",
          "476:             cuboidsRowCount.putIfAbsent(layoutId, rowCount);",
          "483:         JobMetricsUtils.unRegisterQueryExecutionListener(ss, queryExecutionId);",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CubeMergeJob.java||kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CubeMergeJob.java": [
          "File: kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CubeMergeJob.java -> kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CubeMergeJob.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "19: package org.apache.kylin.engine.spark.job;",
          "27: import org.apache.kylin.cube.CubeInstance;",
          "28: import org.apache.kylin.cube.CubeManager;",
          "29: import org.apache.kylin.cube.CubeSegment;",
          "30: import org.apache.kylin.cube.CubeUpdate;",
          "31: import org.apache.kylin.engine.spark.builder.NBuildSourceInfo;",
          "32: import org.apache.kylin.engine.spark.metadata.SegmentInfo;",
          "33: import org.apache.kylin.engine.spark.metadata.cube.ManagerHub;",
          "",
          "[Removed Lines]",
          "21: import java.io.IOException;",
          "22: import java.util.List;",
          "23: import java.util.Map;",
          "24: import java.util.Set;",
          "25: import java.util.UUID;",
          "",
          "[Added Lines]",
          "25: import org.apache.kylin.engine.spark.NSparkCubingEngine;",
          "26: import org.apache.kylin.engine.spark.application.SparkApplication;",
          "27: import org.apache.kylin.engine.spark.builder.CubeMergeAssist;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "35: import org.apache.kylin.engine.spark.metadata.cube.model.ForestSpanningTree;",
          "36: import org.apache.kylin.engine.spark.metadata.cube.model.LayoutEntity;",
          "37: import org.apache.kylin.engine.spark.metadata.cube.model.SpanningTree;",
          "38: import org.apache.kylin.metadata.MetadataConstants;",
          "39: import org.apache.kylin.metadata.model.IStorageAware;",
          "40: import org.apache.kylin.storage.StorageFactory;",
          "41: import org.apache.spark.sql.Dataset;",
          "42: import org.apache.spark.sql.Row;",
          "43: import org.apache.spark.sql.SparkSession;",
          "44: import org.slf4j.Logger;",
          "45: import org.slf4j.LoggerFactory;",
          "58: import scala.collection.JavaConversions;",
          "60: public class CubeMergeJob extends SparkApplication {",
          "61:     protected static final Logger logger = LoggerFactory.getLogger(CubeMergeJob.class);",
          "",
          "[Removed Lines]",
          "47: import org.apache.kylin.shaded.com.google.common.collect.Lists;",
          "48: import org.apache.kylin.shaded.com.google.common.collect.Maps;",
          "50: import org.apache.kylin.engine.spark.NSparkCubingEngine;",
          "51: import org.apache.kylin.engine.spark.application.SparkApplication;",
          "52: import org.apache.kylin.engine.spark.builder.CubeMergeAssist;",
          "53: import org.apache.kylin.engine.spark.utils.BuildUtils;",
          "54: import org.apache.kylin.engine.spark.utils.JobMetrics;",
          "55: import org.apache.kylin.engine.spark.utils.JobMetricsUtils;",
          "56: import org.apache.kylin.engine.spark.utils.Metrics;",
          "57: import org.apache.kylin.engine.spark.utils.QueryExecutionCache;",
          "",
          "[Added Lines]",
          "35: import org.apache.kylin.engine.spark.utils.BuildUtils;",
          "36: import org.apache.kylin.engine.spark.utils.JobMetrics;",
          "37: import org.apache.kylin.engine.spark.utils.JobMetricsUtils;",
          "38: import org.apache.kylin.engine.spark.utils.Metrics;",
          "41: import org.apache.kylin.shaded.com.google.common.collect.Lists;",
          "42: import org.apache.kylin.shaded.com.google.common.collect.Maps;",
          "51: import java.io.IOException;",
          "52: import java.util.List;",
          "53: import java.util.Map;",
          "54: import java.util.Set;",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "165:             sourceCount += cuboid.getSourceRows();",
          "166:         }",
          "171:         ss.sparkContext().setJobDescription(\"merge layout \" + layoutId);",
          "172:         NSparkCubingEngine.NSparkCubingStorage storage = StorageFactory.createEngineAdapter(layout,",
          "173:                 NSparkCubingEngine.NSparkCubingStorage.class);",
          "174:         String path = PathManager.getParquetStoragePath(config, getParam(MetadataConstants.P_CUBE_NAME), seg.name(), seg.identifier(), String.valueOf(layoutId));",
          "175:         String tempPath = path + CubeBuildJob.TEMP_DIR_SUFFIX;",
          "177:         storage.saveTo(tempPath, dataset, ss);",
          "",
          "[Removed Lines]",
          "169:         String queryExecutionId = UUID.randomUUID().toString();",
          "170:         ss.sparkContext().setLocalProperty(QueryExecutionCache.N_EXECUTION_ID_KEY(), queryExecutionId);",
          "",
          "[Added Lines]",
          "170:         String queryExecutionId = tempPath;",
          "171:         JobMetricsUtils.registerQueryExecutionListener(ss, queryExecutionId);",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "191:         int partitionNum = BuildUtils.repartitionIfNeed(layout, storage, path, tempPath, config, ss);",
          "192:         layout.setShardNum(partitionNum);",
          "193:         cuboidShardNum.put(layoutId, (short)partitionNum);",
          "195:         ss.sparkContext().setJobDescription(null);",
          "198:         BuildUtils.fillCuboidInfo(layout, path);",
          "",
          "[Removed Lines]",
          "194:         ss.sparkContext().setLocalProperty(QueryExecutionCache.N_EXECUTION_ID_KEY(), null);",
          "196:         QueryExecutionCache.removeQueryExecution(queryExecutionId);",
          "",
          "[Added Lines]",
          "191:         JobMetricsUtils.registerQueryExecutionListener(ss, queryExecutionId);",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/utils/JobMetricsUtils.scala||kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/utils/JobMetricsUtils.scala": [
          "File: kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/utils/JobMetricsUtils.scala -> kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/utils/JobMetricsUtils.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "21: import java.util.concurrent.ConcurrentHashMap",
          "23: import org.apache.spark.internal.Logging",
          "25: import org.apache.spark.sql.SparkSession",
          "26: import org.apache.spark.sql.execution._",
          "27: import org.apache.spark.sql.execution.aggregate.{HashAggregateExec, ObjectHashAggregateExec, SortAggregateExec}",
          "",
          "[Removed Lines]",
          "24: import org.apache.spark.scheduler.{SparkListener, SparkListenerEvent}",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "33:   private val aggs = List(classOf[HashAggregateExec], classOf[SortAggregateExec], classOf[ObjectHashAggregateExec])",
          "34:   private val joins = List(classOf[BroadcastHashJoinExec], classOf[ShuffledHashJoinExec], classOf[SortMergeJoinExec],",
          "35:     classOf[BroadcastNestedLoopJoinExec], classOf[StreamingSymmetricHashJoinExec])",
          "37:   def collectMetrics(executionId: String): JobMetrics = {",
          "38:     var metrics = new JobMetrics",
          "42:       logInfo(s\"Collect output rows successfully. $metrics\")",
          "43:     }",
          "50:     metrics",
          "51:   }",
          "",
          "[Removed Lines]",
          "36:   var sparkListener : SparkListener = _",
          "39:     val execution = QueryExecutionCache.getQueryExecution(executionId)",
          "40:     if (execution != null) {",
          "41:       metrics = collectOutputRows(execution.executedPlan)",
          "",
          "[Added Lines]",
          "36:   private val executionIdToListener = new ConcurrentHashMap[String, QueryExecutionInterceptListener]()",
          "40:     val listener = executionIdToListener.getOrDefault(executionId,null)",
          "41:     if (listener != null && listener.queryExecution.isDefined) {",
          "42:       metrics = collectOutputRows(listener.queryExecution.get.executedPlan)",
          "44:     } else {",
          "45:       logInfo(s\"Collect output rows failed.\")",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "90:     rowMetrics",
          "91:   }",
          "116:   }",
          "121:     }",
          "122:   }",
          "123: }",
          "",
          "[Removed Lines]",
          "97:      override def onOtherEvent(event: SparkListenerEvent): Unit = event match {",
          "98:         case e: PostQueryExecutionForKylin =>",
          "99:           val nExecutionId = e.localProperties.getProperty(QueryExecutionCache.N_EXECUTION_ID_KEY, \"\")",
          "100:           if (nExecutionId != \"\" && e.queryExecution != null) {",
          "101:             QueryExecutionCache.setQueryExecution(nExecutionId, e.queryExecution)",
          "102:           } else {",
          "103:             logWarning(\"executionIdStr is null, can't get QueryExecution from SQLExecution.\")",
          "104:           }",
          "105:         case _ => // Ignore",
          "106:       }",
          "108:   def registerListener(ss: SparkSession): Unit = {",
          "109:     sparkListener = new SparkListener {",
          "111:       override def onOtherEvent(event: SparkListenerEvent): Unit = event match {",
          "112:         case _ => // Ignore",
          "113:       }",
          "114:     }",
          "115:     ss.sparkContext.addSparkListener(sparkListener)",
          "118:   def unRegisterListener(ss: SparkSession) : Unit = {",
          "119:     if (sparkListener != null) {",
          "120:       ss.sparkContext.removeSparkListener(sparkListener)",
          "125: object QueryExecutionCache extends Logging {",
          "126:   val N_EXECUTION_ID_KEY = \"kylin.query.execution.id\"",
          "128:   private val executionIdToQueryExecution = new ConcurrentHashMap[String, QueryExecution]()",
          "130:   def getQueryExecution(executionId: String): QueryExecution = {",
          "131:     if (executionId != null) {",
          "132:       executionIdToQueryExecution.get(executionId)",
          "133:     } else {",
          "134:       null",
          "135:     }",
          "136:   }",
          "138:   def setQueryExecution(executionId: String, queryExecution: QueryExecution): Unit = {",
          "139:     if (executionId != null) {",
          "140:       executionIdToQueryExecution.put(executionId, queryExecution)",
          "141:     } else {",
          "142:       logWarning(\"kylin.query.execution.id is null, don't put QueryExecution into QueryExecutionCache.\")",
          "143:     }",
          "144:   }",
          "146:   def removeQueryExecution(executionId: String): Unit = {",
          "147:     executionIdToQueryExecution.remove(executionId)",
          "148:   }",
          "150: }",
          "",
          "[Added Lines]",
          "90:   def registerQueryExecutionListener(ss: SparkSession, executionId: String): Unit = {",
          "91:     val listener = new QueryExecutionInterceptListener(executionId)",
          "92:     executionIdToListener.put(executionId, listener)",
          "93:     ss.listenerManager.register(listener)",
          "96:   def unRegisterQueryExecutionListener(ss: SparkSession, executionId: String) : Unit = {",
          "97:     val listener =  executionIdToListener.remove(executionId)",
          "98:     if (listener != null) {",
          "99:       ss.listenerManager.unregister(listener)",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/utils/QueryExecutionInterceptListener.scala||kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/utils/QueryExecutionInterceptListener.scala": [
          "File: kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/utils/QueryExecutionInterceptListener.scala -> kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/utils/QueryExecutionInterceptListener.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: package org.apache.kylin.engine.spark.utils",
          "3: import org.apache.spark.sql.execution.QueryExecution",
          "4: import org.apache.spark.sql.execution.command.DataWritingCommandExec",
          "5: import org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand",
          "6: import org.apache.spark.sql.util.QueryExecutionListener",
          "8: import java.net.URI",
          "13: class QueryExecutionInterceptListener(outputPath: String) extends QueryExecutionListener{",
          "15:   var queryExecution : Option[QueryExecution] = None",
          "17:   override def onSuccess(funcName: String, qe: QueryExecution, durationNs: Long): Unit = {",
          "18:     qe.sparkPlan foreach {",
          "19:       case plan: DataWritingCommandExec =>{",
          "21:         if (plan.cmd.isInstanceOf[InsertIntoHadoopFsRelationCommand]",
          "22:           && plan.cmd.asInstanceOf[InsertIntoHadoopFsRelationCommand].outputPath.toUri.equals(new URI(outputPath))) {",
          "23:           queryExecution = Some(qe)",
          "24:         }",
          "25:       }",
          "26:       case _ =>",
          "27:     }",
          "28:   }",
          "30:   override def onFailure(funcName: String, qe: QueryExecution, exception: Exception): Unit = {",
          "32:   }",
          "33: }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "3400338d7870c843e5ab2490c6767aa37e8092e4",
      "candidate_info": {
        "commit_hash": "3400338d7870c843e5ab2490c6767aa37e8092e4",
        "repo": "apache/kylin",
        "commit_url": "https://github.com/apache/kylin/commit/3400338d7870c843e5ab2490c6767aa37e8092e4",
        "files": [
          "metrics-reporter-hive/src/main/java/org/apache/kylin/metrics/lib/impl/hive/HiveProducer.java",
          "server/src/main/resources/kylinMetrics.xml"
        ],
        "message": "KYLIN-4573 Add option to indicate whether to close file for every append for Hive Producer\n\n(cherry picked from commit 616e06675278a6857f3cbb353a4f9c2243eeccc1)",
        "before_after_code_files": [
          "metrics-reporter-hive/src/main/java/org/apache/kylin/metrics/lib/impl/hive/HiveProducer.java||metrics-reporter-hive/src/main/java/org/apache/kylin/metrics/lib/impl/hive/HiveProducer.java"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/kylin/pull/1893",
          "https://github.com/apache/kylin/pull/2018",
          "https://github.com/apache/kylin/pull/2125",
          "https://github.com/apache/kylin/pull/2033",
          "https://github.com/apache/kylin/pull/2112",
          "https://github.com/apache/kylin/pull/2115",
          "https://github.com/apache/kylin/pull/1865",
          "https://github.com/apache/kylin/pull/1913",
          "https://github.com/apache/kylin/pull/2135"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "metrics-reporter-hive/src/main/java/org/apache/kylin/metrics/lib/impl/hive/HiveProducer.java||metrics-reporter-hive/src/main/java/org/apache/kylin/metrics/lib/impl/hive/HiveProducer.java": [
          "File: metrics-reporter-hive/src/main/java/org/apache/kylin/metrics/lib/impl/hive/HiveProducer.java -> metrics-reporter-hive/src/main/java/org/apache/kylin/metrics/lib/impl/hive/HiveProducer.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "48: import java.io.IOException;",
          "49: import java.net.InetAddress;",
          "50: import java.net.UnknownHostException;",
          "51: import java.util.List;",
          "52: import java.util.Locale;",
          "53: import java.util.Map;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "51: import java.util.HashMap;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "66:     private Path curPartitionContentPath;",
          "67:     private int id = 0;",
          "68:     private FSDataOutputStream fout;",
          "70:     public HiveProducer(String metricType, Properties props) throws Exception {",
          "71:         this(metricType, props, new HiveConf());",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "74:     private final boolean supportAppend;",
          "76:     private final boolean closeFileEveryAppend;",
          "78:     private final Map<String, String> kylinSpecifiedConfig = new HashMap<>();",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "75:         this.metricType = metricType;",
          "76:         hiveConf = hiveConfig;",
          "77:         for (Map.Entry<Object, Object> e : props.entrySet()) {",
          "79:         }",
          "81:         fs = FileSystem.get(hiveConf);",
          "",
          "[Removed Lines]",
          "78:             hiveConf.set(e.getKey().toString(), e.getValue().toString());",
          "",
          "[Added Lines]",
          "88:             String key = e.getKey().toString();",
          "89:             String value = e.getValue().toString();",
          "90:             if (key.startsWith(\"kylin.\")) {",
          "91:                 kylinSpecifiedConfig.put(key, value);",
          "92:             } else {",
          "93:                 hiveConf.set(key, value);",
          "94:             }",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "96:                         IMetaStoreClient metaStoreClient = HiveMetaStoreClientFactory.getHiveMetaStoreClient(hiveConf);",
          "97:                         String tableLocation = metaStoreClient.getTable(tableName.getFirst(), tableName.getSecond())",
          "98:                                 .getSd().getLocation();",
          "99:                         List<FieldSchema> fields = metaStoreClient.getFields(tableName.getFirst(),",
          "100:                                 tableName.getSecond());",
          "101:                         metaStoreClient.close();",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "115:                         logger.debug(\"Find table location for {} at {}\", tableName.getSecond(), tableLocation);",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "110:             hostName = \"UNKNOWN\";",
          "111:         }",
          "112:         contentFilePrefix = hostName + \"-\" + System.currentTimeMillis() + \"-part-\";",
          "113:     }",
          "115:     public void close() {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "130:         String fsUri = fs.getUri().toString();",
          "131:         supportAppend = fsUri.startsWith(\"hdfs\") ; // Only HDFS is appendable",
          "132:         logger.info(\"For {}, supportAppend was set to {}\", fsUri, supportAppend);",
          "134:         closeFileEveryAppend = !supportAppend",
          "135:                 || Boolean.parseBoolean(kylinSpecifiedConfig.get(\"kylin.hive.producer.close-file-every-append\"));",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "127:         for (Record record : recordList) {",
          "128:             HiveProducerRecord hiveRecord = convertTo(record);",
          "129:             if (recordMap.get(hiveRecord.key()) == null) {",
          "131:             }",
          "132:             recordMap.get(hiveRecord.key()).add(hiveRecord);",
          "133:         }",
          "",
          "[Removed Lines]",
          "130:                 recordMap.put(hiveRecord.key(), Lists.<HiveProducerRecord> newLinkedList());",
          "",
          "[Added Lines]",
          "153:                 recordMap.put(hiveRecord.key(), Lists.<HiveProducerRecord>newLinkedList());",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "175:             }",
          "176:             hql.append(\")\");",
          "177:             logger.debug(\"create partition by {}.\", hql);",
          "186:             }",
          "189:         }",
          "",
          "[Removed Lines]",
          "178:             Driver driver = new Driver(hiveConf);",
          "179:             CliSessionState session = new CliSessionState(hiveConf);",
          "180:             SessionState.start(session);",
          "181:             CommandProcessorResponse res = driver.run(hql.toString());",
          "182:             if (res.getResponseCode() != 0) {",
          "183:                 logger.warn(\"Fail to add partition. HQL: {}; Cause by: {}\",",
          "184:                         hql.toString(),",
          "185:                         res.toString());",
          "187:             session.close();",
          "188:             driver.close();",
          "",
          "[Added Lines]",
          "201:             Driver driver = null;",
          "202:             CliSessionState session = null;",
          "203:             try {",
          "204:                 driver = new Driver(hiveConf);",
          "205:                 session = new CliSessionState(hiveConf);",
          "206:                 SessionState.start(session);",
          "207:                 CommandProcessorResponse res = driver.run(hql.toString());",
          "208:                 if (res.getResponseCode() != 0) {",
          "209:                     logger.warn(\"Fail to add partition. HQL: {}; Cause by: {}\",",
          "210:                             hql.toString(),",
          "211:                             res.toString());",
          "212:                 }",
          "213:                 session.close();",
          "214:                 driver.close();",
          "215:             } catch (Exception ex) {",
          "217:                 logger.error(\"create partition failed, please create it manually : \" + hql, ex);",
          "218:             } finally {",
          "219:                 if (session != null) {",
          "220:                     session.close();",
          "221:                 }",
          "222:                 if (driver != null) {",
          "223:                     driver.close();",
          "224:                 }",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "195:                 closeFout();",
          "196:             }",
          "199:             logger.info(\"Try to use new partition content path: {} for metric: {}\", partitionContentPath, metricType);",
          "200:             if (!fs.exists(partitionContentPath)) {",
          "201:                 int nRetry = 0;",
          "",
          "[Removed Lines]",
          "198:             Path partitionContentPath = new Path(partitionPath, contentFilePrefix + String.format(Locale.ROOT, \"%04d\", id));",
          "",
          "[Added Lines]",
          "235:             Path partitionContentPath = new Path(partitionPath, contentFilePrefix + String.format(Locale.ROOT, \"%05d\", id));",
          "238:             int nCheck = 0;",
          "239:             while (!supportAppend && fs.exists(partitionContentPath)) {",
          "240:                 id++;",
          "241:                 nCheck++;",
          "242:                 partitionContentPath = new Path(partitionPath, contentFilePrefix + String.format(Locale.ROOT, \"%05d\", id));",
          "243:                 logger.debug(\"{} exists, skip it.\", partitionContentPath);",
          "244:                 if (nCheck > 100000) {",
          "245:                     logger.warn(\"Exceed max check times.\");",
          "246:                     break;",
          "247:                 }",
          "248:             }",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "210:                             \"Fail to create HDFS file: \" + partitionContentPath + \" after \" + nRetry + \" retries\");",
          "211:                 }",
          "212:             }",
          "214:             prePartitionPath = partitionPath.toString();",
          "215:             curPartitionContentPath = partitionContentPath;",
          "217:         }",
          "220:         try {",
          "221:             int count = 0;",
          "222:             for (HiveProducerRecord elem : recordItr) {",
          "223:                 fout.writeBytes(elem.valueToString() + \"\\n\");",
          "224:                 count++;",
          "225:             }",
          "227:         } catch (IOException e) {",
          "228:             logger.error(\"Fails to write metrics(\" + metricType + \") to file \" + curPartitionContentPath.toString()",
          "229:                     + \" due to \", e);",
          "230:             closeFout();",
          "231:         }",
          "232:     }",
          "234:     private void closeFout() {",
          "235:         if (fout != null) {",
          "236:             try {",
          "237:                 fout.close();",
          "238:             } catch (Exception e) {",
          "239:                 logger.error(\"Close the path: \" + curPartitionContentPath + \" failed\", e);",
          "",
          "[Removed Lines]",
          "213:             fout = fs.append(partitionContentPath);",
          "216:             id = (id + 1) % 10;",
          "226:             logger.info(\"Success to write {} metrics ({}) to file {}\", count, metricType, curPartitionContentPath);",
          "",
          "[Added Lines]",
          "264:             if (supportAppend) {",
          "265:                 fout = fs.append(partitionContentPath);",
          "266:             } else {",
          "267:                 fout = fs.create(partitionContentPath);",
          "268:             }",
          "271:             id = (id + 1) % (supportAppend ? 10 : 100000);",
          "281:             logger.debug(\"Success to write {} metrics ({}) to file {}\", count, metricType, curPartitionContentPath);",
          "287:         if (closeFileEveryAppend) {",
          "288:             closeFout();",
          "289:         }",
          "295:                 logger.debug(\"Flush output stream {}.\", curPartitionContentPath);",
          "",
          "---------------"
        ]
      }
    }
  ]
}