{
  "cve_id": "CVE-2022-31777",
  "cve_desc": "A stored cross-site scripting (XSS) vulnerability in Apache Spark 3.2.1 and earlier, and 3.3.0, allows remote attackers to execute arbitrary JavaScript in the web browser of a user, by including a malicious payload into the logs which would be returned in logs rendered in the UI.",
  "repo": "apache/spark",
  "patch_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
  "patch_info": {
    "commit_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
    "repo": "apache/spark",
    "commit_url": "https://github.com/apache/spark/commit/ad90195de56688ce0898691eb9d04297ab0871ad",
    "files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ],
    "message": "[SPARK-39505][UI] Escape log content rendered in UI\n\n### What changes were proposed in this pull request?\n\nEscape log content rendered to the UI.\n\n### Why are the changes needed?\n\nLog content may contain reserved characters or other code in the log and be misinterpreted in the UI as HTML.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nExisting tests\n\nCloses #36902 from srowen/LogViewEscape.\n\nAuthored-by: Sean Owen <srowen@gmail.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
    "before_after_code_files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ]
  },
  "patch_diff": {
    "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js": [
      "File: core/src/main/resources/org/apache/spark/ui/static/log-view.js -> core/src/main/resources/org/apache/spark/ui/static/log-view.js",
      "--- Hunk 1 ---",
      "[Context before]",
      "85:       if (retStartByte == 0) {",
      "86:         disableMoreButton();",
      "87:       }",
      "90:       curLogLength = curLogLength + (startByte - retStartByte);",
      "91:       startByte = retStartByte;",
      "",
      "[Removed Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(cleanData);",
      "",
      "[Added Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(document.createTextNode(cleanData));",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "115:             var retLogLength = dataInfo[2];",
      "117:             var cleanData = data.substring(newlineIndex + 1);",
      "120:             curLogLength = curLogLength + (retEndByte - retStartByte);",
      "121:             endByte = retEndByte;",
      "",
      "[Removed Lines]",
      "118:             $(\"pre\", \".log-content\").append(cleanData);",
      "",
      "[Added Lines]",
      "118:             $(\"pre\", \".log-content\").append(document.createTextNode(cleanData));",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "9473840bac8da0e92587a8d0edb5ac86757637c7",
      "candidate_info": {
        "commit_hash": "9473840bac8da0e92587a8d0edb5ac86757637c7",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/9473840bac8da0e92587a8d0edb5ac86757637c7",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CTESubstitution.scala",
          "sql/core/src/test/resources/sql-tests/inputs/cte-nested.sql",
          "sql/core/src/test/resources/sql-tests/results/cte-legacy.sql.out",
          "sql/core/src/test/resources/sql-tests/results/cte-nested.sql.out",
          "sql/core/src/test/resources/sql-tests/results/cte-nonlegacy.sql.out"
        ],
        "message": "[SPARK-38404][SQL][3.3] Improve CTE resolution when a nested CTE references an outer CTE\n\n### What changes were proposed in this pull request?\nPlease note that the bug in the [SPARK-38404](https://issues.apache.org/jira/browse/SPARK-38404) is fixed already with https://github.com/apache/spark/pull/34929.\nThis PR is a minor improvement to the current implementation by collecting already resolved outer CTEs to avoid re-substituting already collected CTE definitions.\n\n### Why are the changes needed?\nSmall improvement + additional tests.\n\n### Does this PR introduce _any_ user-facing change?\nNo.\n\n### How was this patch tested?\nAdded new test case.\n\nCloses #37760 from peter-toth/SPARK-38404-nested-cte-references-outer-cte-3.3.\n\nAuthored-by: Peter Toth <peter.toth@gmail.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CTESubstitution.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CTESubstitution.scala",
          "sql/core/src/test/resources/sql-tests/inputs/cte-nested.sql||sql/core/src/test/resources/sql-tests/inputs/cte-nested.sql"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CTESubstitution.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CTESubstitution.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CTESubstitution.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CTESubstitution.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: package org.apache.spark.sql.catalyst.analysis",
          "22: import org.apache.spark.sql.catalyst.expressions.SubqueryExpression",
          "23: import org.apache.spark.sql.catalyst.plans.logical.{Command, CTERelationDef, CTERelationRef, InsertIntoDir, LogicalPlan, ParsedStatement, SubqueryAlias, UnresolvedWith, WithCTE}",
          "",
          "[Removed Lines]",
          "20: import scala.collection.mutable",
          "",
          "[Added Lines]",
          "20: import scala.collection.mutable.ArrayBuffer",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "55:       case _: Command | _: ParsedStatement | _: InsertIntoDir => true",
          "56:       case _ => false",
          "57:     }",
          "59:     val (substituted, lastSubstituted) =",
          "60:       LegacyBehaviorPolicy.withName(conf.getConf(LEGACY_CTE_PRECEDENCE_POLICY)) match {",
          "61:         case LegacyBehaviorPolicy.EXCEPTION =>",
          "62:           assertNoNameConflictsInCTE(plan)",
          "64:         case LegacyBehaviorPolicy.LEGACY =>",
          "65:           (legacyTraverseAndSubstituteCTE(plan, cteDefs), None)",
          "66:         case LegacyBehaviorPolicy.CORRECTED =>",
          "68:     }",
          "69:     if (cteDefs.isEmpty) {",
          "70:       substituted",
          "71:     } else if (substituted eq lastSubstituted.get) {",
          "73:     } else {",
          "74:       var done = false",
          "75:       substituted.resolveOperatorsWithPruning(_ => !done) {",
          "76:         case p if p eq lastSubstituted.get =>",
          "77:           done = true",
          "79:       }",
          "80:     }",
          "81:   }",
          "",
          "[Removed Lines]",
          "58:     val cteDefs = mutable.ArrayBuffer.empty[CTERelationDef]",
          "63:           traverseAndSubstituteCTE(plan, isCommand, cteDefs)",
          "67:           traverseAndSubstituteCTE(plan, isCommand, cteDefs)",
          "72:       WithCTE(substituted, cteDefs.sortBy(_.id).toSeq)",
          "78:           WithCTE(p, cteDefs.sortBy(_.id).toSeq)",
          "",
          "[Added Lines]",
          "58:     val cteDefs = ArrayBuffer.empty[CTERelationDef]",
          "63:           traverseAndSubstituteCTE(plan, isCommand, Seq.empty, cteDefs)",
          "67:           traverseAndSubstituteCTE(plan, isCommand, Seq.empty, cteDefs)",
          "72:       WithCTE(substituted, cteDefs.toSeq)",
          "78:           WithCTE(p, cteDefs.toSeq)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "98:     val resolver = conf.resolver",
          "99:     plan match {",
          "100:       case UnresolvedWith(child, relations) =>",
          "102:         newNames ++= outerCTERelationNames",
          "103:         relations.foreach {",
          "104:           case (name, relation) =>",
          "",
          "[Removed Lines]",
          "101:         val newNames = mutable.ArrayBuffer.empty[String]",
          "",
          "[Added Lines]",
          "101:         val newNames = ArrayBuffer.empty[String]",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "122:   private def legacyTraverseAndSubstituteCTE(",
          "123:       plan: LogicalPlan,",
          "125:     plan.resolveOperatorsUp {",
          "126:       case UnresolvedWith(child, relations) =>",
          "127:         val resolvedCTERelations =",
          "129:         substituteCTE(child, alwaysInline = true, resolvedCTERelations)",
          "130:     }",
          "131:   }",
          "",
          "[Removed Lines]",
          "124:       cteDefs: mutable.ArrayBuffer[CTERelationDef]): LogicalPlan = {",
          "128:           resolveCTERelations(relations, isLegacy = true, isCommand = false, cteDefs)",
          "",
          "[Added Lines]",
          "124:       cteDefs: ArrayBuffer[CTERelationDef]): LogicalPlan = {",
          "128:           resolveCTERelations(relations, isLegacy = true, isCommand = false, Seq.empty, cteDefs)",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "175:   private def traverseAndSubstituteCTE(",
          "176:       plan: LogicalPlan,",
          "177:       isCommand: Boolean,",
          "179:     var lastSubstituted: Option[LogicalPlan] = None",
          "180:     val newPlan = plan.resolveOperatorsUpWithPruning(",
          "181:         _.containsAnyPattern(UNRESOLVED_WITH, PLAN_EXPRESSION)) {",
          "182:       case UnresolvedWith(child: LogicalPlan, relations) =>",
          "183:         val resolvedCTERelations =",
          "188:         lastSubstituted = Some(substituteCTE(child, isCommand, resolvedCTERelations))",
          "189:         lastSubstituted.get",
          "",
          "[Removed Lines]",
          "178:       cteDefs: mutable.ArrayBuffer[CTERelationDef]): (LogicalPlan, Option[LogicalPlan]) = {",
          "184:           resolveCTERelations(relations, isLegacy = false, isCommand, cteDefs)",
          "185:         if (!isCommand) {",
          "186:           cteDefs ++= resolvedCTERelations.map(_._2)",
          "187:         }",
          "",
          "[Added Lines]",
          "182:       outerCTEDefs: Seq[(String, CTERelationDef)],",
          "183:       cteDefs: ArrayBuffer[CTERelationDef]): (LogicalPlan, Option[LogicalPlan]) = {",
          "189:           resolveCTERelations(relations, isLegacy = false, isCommand, outerCTEDefs, cteDefs)",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "200:       relations: Seq[(String, SubqueryAlias)],",
          "201:       isLegacy: Boolean,",
          "202:       isCommand: Boolean,",
          "205:     for ((name, relation) <- relations) {",
          "207:       val innerCTEResolved = if (isLegacy) {",
          "",
          "[Removed Lines]",
          "203:       cteDefs: mutable.ArrayBuffer[CTERelationDef]): Seq[(String, CTERelationDef)] = {",
          "204:     val resolvedCTERelations = new mutable.ArrayBuffer[(String, CTERelationDef)](relations.size)",
          "206:       val lastCTEDefCount = cteDefs.length",
          "",
          "[Added Lines]",
          "205:       outerCTEDefs: Seq[(String, CTERelationDef)],",
          "206:       cteDefs: ArrayBuffer[CTERelationDef]): Seq[(String, CTERelationDef)] = {",
          "207:     var resolvedCTERelations = if (isLegacy || isCommand) {",
          "208:       Seq.empty",
          "209:     } else {",
          "210:       outerCTEDefs",
          "211:     }",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "240:       }",
          "245:       val cteRelation = CTERelationDef(substituted)",
          "247:     }",
          "249:   }",
          "251:   private def substituteCTE(",
          "",
          "[Removed Lines]",
          "224:         traverseAndSubstituteCTE(relation, isCommand, cteDefs)._1",
          "225:       }",
          "227:       if (cteDefs.length > lastCTEDefCount) {",
          "235:         for (i <- lastCTEDefCount until cteDefs.length) {",
          "236:           val substituted =",
          "237:             substituteCTE(cteDefs(i).child, isLegacy || isCommand, resolvedCTERelations.toSeq)",
          "238:           cteDefs(i) = cteDefs(i).copy(child = substituted)",
          "239:         }",
          "243:       val substituted =",
          "244:         substituteCTE(innerCTEResolved, isLegacy || isCommand, resolvedCTERelations.toSeq)",
          "246:       resolvedCTERelations += (name -> cteRelation)",
          "248:     resolvedCTERelations.toSeq",
          "",
          "[Added Lines]",
          "230:         traverseAndSubstituteCTE(relation, isCommand, resolvedCTERelations, cteDefs)._1",
          "233:       val substituted = substituteCTE(innerCTEResolved, isLegacy || isCommand, resolvedCTERelations)",
          "235:       if (!(isLegacy || isCommand)) {",
          "236:         cteDefs += cteRelation",
          "237:       }",
          "239:       resolvedCTERelations +:= (name -> cteRelation)",
          "241:     resolvedCTERelations",
          "",
          "---------------"
        ],
        "sql/core/src/test/resources/sql-tests/inputs/cte-nested.sql||sql/core/src/test/resources/sql-tests/inputs/cte-nested.sql": [
          "File: sql/core/src/test/resources/sql-tests/inputs/cte-nested.sql -> sql/core/src/test/resources/sql-tests/inputs/cte-nested.sql",
          "--- Hunk 1 ---",
          "[Context before]",
          "135: SELECT (",
          "136:   WITH aBc AS (SELECT 2)",
          "137:   SELECT * FROM aBC",
          "",
          "[Removed Lines]",
          "138: );",
          "",
          "[Added Lines]",
          "140: -- SPARK-38404: CTE in CTE definition references outer",
          "141: WITH",
          "142:   t1 AS (SELECT 1),",
          "143:   t2 AS (",
          "144:     WITH t3 AS (",
          "145:       SELECT * FROM t1",
          "146:     )",
          "147:     SELECT * FROM t3",
          "148:   )",
          "149: SELECT * FROM t2;",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "4ccd530f639e3652b7aad7c8bcfa379847dc2b68",
      "candidate_info": {
        "commit_hash": "4ccd530f639e3652b7aad7c8bcfa379847dc2b68",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/4ccd530f639e3652b7aad7c8bcfa379847dc2b68",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/RewriteDeleteFromTable.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/RewriteRowLevelCommand.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/ReplaceNullWithFalseInPredicate.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/SimplifyConditionalsInPredicate.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/planning/patterns.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/v2Commands.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/connector/write/RowLevelOperationInfoImpl.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/connector/write/RowLevelOperationTable.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Implicits.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/connector/catalog/InMemoryRowLevelOperationTable.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/connector/catalog/InMemoryRowLevelOperationTableCatalog.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/connector/catalog/InMemoryTable.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkOptimizer.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/GroupBasedRowLevelOperationScanPlanning.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/OptimizeMetadataOnlyDeleteFromTable.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/PushDownUtils.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2Writes.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/WriteToDataSourceV2Exec.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/connector/DeleteFromTableSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/command/PlanResolutionSuite.scala"
        ],
        "message": "[SPARK-38085][SQL] DataSource V2: Handle DELETE commands for group-based sources\n\nThis PR contains changes to rewrite DELETE operations for V2 data sources that can replace groups of data (e.g. files, partitions).\n\nThese changes are needed to support row-level operations in Spark per SPIP SPARK-35801.\n\nNo.\n\nThis PR comes with tests.\n\nCloses #35395 from aokolnychyi/spark-38085.\n\nAuthored-by: Anton Okolnychyi <aokolnychyi@apple.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\n(cherry picked from commit 5a92eccd514b7bc0513feaecb041aee2f8cd5a24)\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/RewriteDeleteFromTable.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/RewriteDeleteFromTable.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/RewriteRowLevelCommand.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/RewriteRowLevelCommand.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/ReplaceNullWithFalseInPredicate.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/ReplaceNullWithFalseInPredicate.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/SimplifyConditionalsInPredicate.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/SimplifyConditionalsInPredicate.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/planning/patterns.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/planning/patterns.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/v2Commands.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/v2Commands.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/connector/write/RowLevelOperationInfoImpl.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/connector/write/RowLevelOperationInfoImpl.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/connector/write/RowLevelOperationTable.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/connector/write/RowLevelOperationTable.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Implicits.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Implicits.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/connector/catalog/InMemoryRowLevelOperationTable.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/connector/catalog/InMemoryRowLevelOperationTable.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/connector/catalog/InMemoryRowLevelOperationTableCatalog.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/connector/catalog/InMemoryRowLevelOperationTableCatalog.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/connector/catalog/InMemoryTable.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/connector/catalog/InMemoryTable.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkOptimizer.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/SparkOptimizer.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/GroupBasedRowLevelOperationScanPlanning.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/GroupBasedRowLevelOperationScanPlanning.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/OptimizeMetadataOnlyDeleteFromTable.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/OptimizeMetadataOnlyDeleteFromTable.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/PushDownUtils.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/PushDownUtils.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2Writes.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2Writes.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/WriteToDataSourceV2Exec.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/WriteToDataSourceV2Exec.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/connector/DeleteFromTableSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/connector/DeleteFromTableSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/command/PlanResolutionSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/command/PlanResolutionSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "318:       ResolveRandomSeed ::",
          "319:       ResolveBinaryArithmetic ::",
          "320:       ResolveUnion ::",
          "321:       typeCoercionRules ++",
          "322:       Seq(ResolveWithCTE) ++",
          "323:       extendedResolutionRules : _*),",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "321:       RewriteDeleteFromTable ::",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/RewriteDeleteFromTable.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/RewriteDeleteFromTable.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/RewriteDeleteFromTable.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/RewriteDeleteFromTable.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "18: package org.apache.spark.sql.catalyst.analysis",
          "20: import org.apache.spark.sql.catalyst.expressions.{EqualNullSafe, Expression, Not}",
          "21: import org.apache.spark.sql.catalyst.expressions.Literal.TrueLiteral",
          "22: import org.apache.spark.sql.catalyst.plans.logical.{DeleteFromTable, Filter, LogicalPlan, ReplaceData}",
          "23: import org.apache.spark.sql.connector.catalog.{SupportsDelete, SupportsRowLevelOperations, TruncatableTable}",
          "24: import org.apache.spark.sql.connector.write.RowLevelOperation.Command.DELETE",
          "25: import org.apache.spark.sql.connector.write.RowLevelOperationTable",
          "26: import org.apache.spark.sql.errors.QueryCompilationErrors",
          "27: import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation",
          "28: import org.apache.spark.sql.util.CaseInsensitiveStringMap",
          "38: object RewriteDeleteFromTable extends RewriteRowLevelCommand {",
          "40:   override def apply(plan: LogicalPlan): LogicalPlan = plan resolveOperators {",
          "41:     case d @ DeleteFromTable(aliasedTable, cond) if d.resolved =>",
          "42:       EliminateSubqueryAliases(aliasedTable) match {",
          "43:         case DataSourceV2Relation(_: TruncatableTable, _, _, _, _) if cond == TrueLiteral =>",
          "45:           d",
          "47:         case r @ DataSourceV2Relation(t: SupportsRowLevelOperations, _, _, _, _) =>",
          "48:           val table = buildOperationTable(t, DELETE, CaseInsensitiveStringMap.empty())",
          "49:           buildReplaceDataPlan(r, table, cond)",
          "51:         case DataSourceV2Relation(_: SupportsDelete, _, _, _, _) =>",
          "53:           d",
          "55:         case DataSourceV2Relation(t, _, _, _, _) =>",
          "56:           throw QueryCompilationErrors.tableDoesNotSupportDeletesError(t)",
          "58:         case _ =>",
          "59:           d",
          "60:       }",
          "61:   }",
          "64:   private def buildReplaceDataPlan(",
          "65:       relation: DataSourceV2Relation,",
          "66:       operationTable: RowLevelOperationTable,",
          "67:       cond: Expression): ReplaceData = {",
          "71:     val metadataAttrs = resolveRequiredMetadataAttrs(relation, operationTable.operation)",
          "74:     val readRelation = buildRelationWithAttrs(relation, operationTable, metadataAttrs)",
          "82:     val remainingRowsFilter = Not(EqualNullSafe(cond, TrueLiteral))",
          "83:     val remainingRowsPlan = Filter(remainingRowsFilter, readRelation)",
          "86:     val writeRelation = relation.copy(table = operationTable)",
          "87:     ReplaceData(writeRelation, cond, remainingRowsPlan, relation)",
          "88:   }",
          "89: }",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/RewriteRowLevelCommand.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/RewriteRowLevelCommand.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/RewriteRowLevelCommand.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/RewriteRowLevelCommand.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "18: package org.apache.spark.sql.catalyst.analysis",
          "20: import scala.collection.mutable",
          "22: import org.apache.spark.sql.catalyst.expressions.{AttributeReference, ExprId, V2ExpressionUtils}",
          "23: import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan",
          "24: import org.apache.spark.sql.catalyst.rules.Rule",
          "25: import org.apache.spark.sql.connector.catalog.SupportsRowLevelOperations",
          "26: import org.apache.spark.sql.connector.write.{RowLevelOperation, RowLevelOperationInfoImpl, RowLevelOperationTable}",
          "27: import org.apache.spark.sql.connector.write.RowLevelOperation.Command",
          "28: import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation",
          "29: import org.apache.spark.sql.util.CaseInsensitiveStringMap",
          "31: trait RewriteRowLevelCommand extends Rule[LogicalPlan] {",
          "33:   protected def buildOperationTable(",
          "34:       table: SupportsRowLevelOperations,",
          "35:       command: Command,",
          "36:       options: CaseInsensitiveStringMap): RowLevelOperationTable = {",
          "37:     val info = RowLevelOperationInfoImpl(command, options)",
          "38:     val operation = table.newRowLevelOperationBuilder(info).build()",
          "39:     RowLevelOperationTable(table, operation)",
          "40:   }",
          "42:   protected def buildRelationWithAttrs(",
          "43:       relation: DataSourceV2Relation,",
          "44:       table: RowLevelOperationTable,",
          "45:       metadataAttrs: Seq[AttributeReference]): DataSourceV2Relation = {",
          "47:     val attrs = dedupAttrs(relation.output ++ metadataAttrs)",
          "48:     relation.copy(table = table, output = attrs)",
          "49:   }",
          "51:   protected def dedupAttrs(attrs: Seq[AttributeReference]): Seq[AttributeReference] = {",
          "52:     val exprIds = mutable.Set.empty[ExprId]",
          "53:     attrs.flatMap { attr =>",
          "54:       if (exprIds.contains(attr.exprId)) {",
          "55:         None",
          "56:       } else {",
          "57:         exprIds += attr.exprId",
          "58:         Some(attr)",
          "59:       }",
          "60:     }",
          "61:   }",
          "63:   protected def resolveRequiredMetadataAttrs(",
          "64:       relation: DataSourceV2Relation,",
          "65:       operation: RowLevelOperation): Seq[AttributeReference] = {",
          "67:     V2ExpressionUtils.resolveRefs[AttributeReference](",
          "68:       operation.requiredMetadataAttributes,",
          "69:       relation)",
          "70:   }",
          "71: }",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/ReplaceNullWithFalseInPredicate.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/ReplaceNullWithFalseInPredicate.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/ReplaceNullWithFalseInPredicate.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/ReplaceNullWithFalseInPredicate.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: import org.apache.spark.sql.catalyst.expressions.{And, ArrayExists, ArrayFilter, CaseWhen, EqualNullSafe, Expression, If, In, InSet, LambdaFunction, Literal, MapFilter, Not, Or}",
          "21: import org.apache.spark.sql.catalyst.expressions.Literal.{FalseLiteral, TrueLiteral}",
          "23: import org.apache.spark.sql.catalyst.rules.Rule",
          "24: import org.apache.spark.sql.catalyst.trees.TreePattern.{INSET, NULL_LITERAL, TRUE_OR_FALSE_LITERAL}",
          "25: import org.apache.spark.sql.types.BooleanType",
          "",
          "[Removed Lines]",
          "22: import org.apache.spark.sql.catalyst.plans.logical.{DeleteAction, DeleteFromTable, Filter, InsertAction, InsertStarAction, Join, LogicalPlan, MergeAction, MergeIntoTable, UpdateAction, UpdateStarAction, UpdateTable}",
          "",
          "[Added Lines]",
          "22: import org.apache.spark.sql.catalyst.plans.logical.{DeleteAction, DeleteFromTable, Filter, InsertAction, InsertStarAction, Join, LogicalPlan, MergeAction, MergeIntoTable, ReplaceData, UpdateAction, UpdateStarAction, UpdateTable}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "54:     _.containsAnyPattern(NULL_LITERAL, TRUE_OR_FALSE_LITERAL, INSET), ruleId) {",
          "55:     case f @ Filter(cond, _) => f.copy(condition = replaceNullWithFalse(cond))",
          "56:     case j @ Join(_, _, _, Some(cond), _) => j.copy(condition = Some(replaceNullWithFalse(cond)))",
          "57:     case d @ DeleteFromTable(_, cond) => d.copy(condition = replaceNullWithFalse(cond))",
          "58:     case u @ UpdateTable(_, _, Some(cond)) => u.copy(condition = Some(replaceNullWithFalse(cond)))",
          "59:     case m @ MergeIntoTable(_, _, mergeCond, matchedActions, notMatchedActions) =>",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "57:     case rd @ ReplaceData(_, cond, _, _, _) => rd.copy(condition = replaceNullWithFalse(cond))",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/SimplifyConditionalsInPredicate.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/SimplifyConditionalsInPredicate.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/SimplifyConditionalsInPredicate.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/SimplifyConditionalsInPredicate.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "48:     _.containsAnyPattern(CASE_WHEN, IF), ruleId) {",
          "49:     case f @ Filter(cond, _) => f.copy(condition = simplifyConditional(cond))",
          "50:     case j @ Join(_, _, _, Some(cond), _) => j.copy(condition = Some(simplifyConditional(cond)))",
          "51:     case d @ DeleteFromTable(_, cond) => d.copy(condition = simplifyConditional(cond))",
          "52:     case u @ UpdateTable(_, _, Some(cond)) => u.copy(condition = Some(simplifyConditional(cond)))",
          "53:   }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "51:     case rd @ ReplaceData(_, cond, _, _, _) => rd.copy(condition = simplifyConditional(cond))",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/planning/patterns.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/planning/patterns.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/planning/patterns.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/planning/patterns.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: package org.apache.spark.sql.catalyst.planning",
          "20: import org.apache.spark.internal.Logging",
          "21: import org.apache.spark.sql.catalyst.expressions._",
          "22: import org.apache.spark.sql.catalyst.expressions.aggregate.AggregateExpression",
          "23: import org.apache.spark.sql.catalyst.optimizer.JoinSelectionHelper",
          "24: import org.apache.spark.sql.catalyst.plans._",
          "25: import org.apache.spark.sql.catalyst.plans.logical._",
          "26: import org.apache.spark.sql.errors.QueryCompilationErrors",
          "27: import org.apache.spark.sql.internal.SQLConf",
          "29: trait OperationHelper extends AliasHelper with PredicateHelper {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "21: import org.apache.spark.sql.AnalysisException",
          "27: import org.apache.spark.sql.connector.catalog.Table",
          "29: import org.apache.spark.sql.execution.datasources.v2.{DataSourceV2Relation, DataSourceV2ScanRelation}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "388:     case _ => None",
          "389:   }",
          "390: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "405: object GroupBasedRowLevelOperation {",
          "406:   type ReturnType = (ReplaceData, Expression, LogicalPlan)",
          "408:   def unapply(plan: LogicalPlan): Option[ReturnType] = plan match {",
          "409:     case rd @ ReplaceData(DataSourceV2Relation(table, _, _, _, _), cond, query, _, _) =>",
          "410:       val readRelation = findReadRelation(table, query)",
          "411:       readRelation.map((rd, cond, _))",
          "413:     case _ =>",
          "414:       None",
          "415:   }",
          "417:   private def findReadRelation(",
          "418:       table: Table,",
          "419:       plan: LogicalPlan): Option[LogicalPlan] = {",
          "421:     val readRelations = plan.collect {",
          "422:       case r: DataSourceV2Relation if r.table eq table => r",
          "423:       case r: DataSourceV2ScanRelation if r.relation.table eq table => r",
          "424:     }",
          "430:     readRelations match {",
          "431:       case relations if relations.isEmpty =>",
          "432:         None",
          "434:       case Seq(relation) =>",
          "435:         Some(relation)",
          "437:       case relations =>",
          "438:         throw new AnalysisException(s\"Expected only one row-level read relation: $relations\")",
          "439:     }",
          "440:   }",
          "441: }",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/v2Commands.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/v2Commands.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/v2Commands.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/v2Commands.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: package org.apache.spark.sql.catalyst.plans.logical",
          "21: import org.apache.spark.sql.catalyst.catalog.CatalogTypes.TablePartitionSpec",
          "22: import org.apache.spark.sql.catalyst.catalog.FunctionResource",
          "24: import org.apache.spark.sql.catalyst.plans.DescribeCommandSchema",
          "25: import org.apache.spark.sql.catalyst.trees.BinaryLike",
          "26: import org.apache.spark.sql.catalyst.util.CharVarcharUtils",
          "27: import org.apache.spark.sql.connector.catalog._",
          "28: import org.apache.spark.sql.connector.expressions.Transform",
          "30: import org.apache.spark.sql.types.{BooleanType, DataType, MetadataBuilder, StringType, StructType}",
          "",
          "[Removed Lines]",
          "20: import org.apache.spark.sql.catalyst.analysis.{AnalysisContext, FieldName, NamedRelation, PartitionSpec, ResolvedDBObjectName, UnresolvedException}",
          "23: import org.apache.spark.sql.catalyst.expressions.{Attribute, AttributeReference, AttributeSet, Expression, Unevaluable}",
          "29: import org.apache.spark.sql.connector.write.Write",
          "",
          "[Added Lines]",
          "20: import org.apache.spark.sql.{sources, AnalysisException}",
          "21: import org.apache.spark.sql.catalyst.analysis.{AnalysisContext, EliminateSubqueryAliases, FieldName, NamedRelation, PartitionSpec, ResolvedDBObjectName, UnresolvedException}",
          "24: import org.apache.spark.sql.catalyst.expressions.{Attribute, AttributeReference, AttributeSet, Expression, MetadataAttribute, Unevaluable}",
          "30: import org.apache.spark.sql.connector.write.{RowLevelOperation, RowLevelOperationTable, Write}",
          "31: import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "176:   }",
          "177: }",
          "181: trait V2CreateTablePlan extends LogicalPlan {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "181: trait RowLevelWrite extends V2WriteCommand with SupportsSubquery {",
          "182:   def operation: RowLevelOperation",
          "183:   def condition: Expression",
          "184:   def originalTable: NamedRelation",
          "185: }",
          "199: case class ReplaceData(",
          "200:     table: NamedRelation,",
          "201:     condition: Expression,",
          "202:     query: LogicalPlan,",
          "203:     originalTable: NamedRelation,",
          "204:     write: Option[Write] = None) extends RowLevelWrite {",
          "206:   override val isByName: Boolean = false",
          "207:   override val stringArgs: Iterator[Any] = Iterator(table, query, write)",
          "209:   override lazy val references: AttributeSet = query.outputSet",
          "211:   lazy val operation: RowLevelOperation = {",
          "212:     EliminateSubqueryAliases(table) match {",
          "213:       case DataSourceV2Relation(RowLevelOperationTable(_, operation), _, _, _, _) =>",
          "214:         operation",
          "215:       case _ =>",
          "216:         throw new AnalysisException(s\"Cannot retrieve row-level operation from $table\")",
          "217:     }",
          "218:   }",
          "221:   lazy val dataInput: Seq[Attribute] = {",
          "222:     query.output.filter {",
          "223:       case MetadataAttribute(_) => false",
          "224:       case _ => true",
          "225:     }",
          "226:   }",
          "228:   override def outputResolved: Boolean = {",
          "229:     assert(table.resolved && query.resolved,",
          "230:       \"`outputResolved` can only be called when `table` and `query` are both resolved.\")",
          "237:     table.skipSchemaResolution || (dataInput.size == table.output.size &&",
          "238:       dataInput.zip(table.output).forall { case (inAttr, outAttr) =>",
          "239:         val outType = CharVarcharUtils.getRawType(outAttr.metadata).getOrElse(outAttr.dataType)",
          "241:         inAttr.name == outAttr.name &&",
          "242:           DataType.equalsIgnoreCompatibleNullability(inAttr.dataType, outType) &&",
          "243:           (outAttr.nullable || !inAttr.nullable)",
          "244:       })",
          "245:   }",
          "247:   override def withNewQuery(newQuery: LogicalPlan): ReplaceData = copy(query = newQuery)",
          "249:   override def withNewTable(newTable: NamedRelation): ReplaceData = copy(table = newTable)",
          "251:   override protected def withNewChildInternal(newChild: LogicalPlan): ReplaceData = {",
          "252:     copy(query = newChild)",
          "253:   }",
          "254: }",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "457:     copy(table = newChild)",
          "458: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "542: case class DeleteFromTableWithFilters(",
          "543:     table: LogicalPlan,",
          "544:     condition: Seq[sources.Filter]) extends LeafCommand",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/connector/write/RowLevelOperationInfoImpl.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/connector/write/RowLevelOperationInfoImpl.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/connector/write/RowLevelOperationInfoImpl.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/connector/write/RowLevelOperationInfoImpl.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "18: package org.apache.spark.sql.connector.write",
          "20: import org.apache.spark.sql.connector.write.RowLevelOperation.Command",
          "21: import org.apache.spark.sql.util.CaseInsensitiveStringMap",
          "23: private[sql] case class RowLevelOperationInfoImpl(",
          "24:     command: Command,",
          "25:     options: CaseInsensitiveStringMap) extends RowLevelOperationInfo",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/connector/write/RowLevelOperationTable.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/connector/write/RowLevelOperationTable.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/connector/write/RowLevelOperationTable.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/connector/write/RowLevelOperationTable.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "18: package org.apache.spark.sql.connector.write",
          "20: import java.util",
          "22: import org.apache.spark.sql.connector.catalog.{SupportsRead, SupportsRowLevelOperations, SupportsWrite, Table, TableCapability}",
          "23: import org.apache.spark.sql.connector.read.ScanBuilder",
          "24: import org.apache.spark.sql.types.StructType",
          "25: import org.apache.spark.sql.util.CaseInsensitiveStringMap",
          "35: private[sql] case class RowLevelOperationTable(",
          "36:     table: Table with SupportsRowLevelOperations,",
          "37:     operation: RowLevelOperation) extends Table with SupportsRead with SupportsWrite {",
          "39:   override def name: String = table.name",
          "40:   override def schema: StructType = table.schema",
          "41:   override def capabilities: util.Set[TableCapability] = table.capabilities",
          "42:   override def toString: String = table.toString",
          "44:   override def newScanBuilder(options: CaseInsensitiveStringMap): ScanBuilder = {",
          "45:     operation.newScanBuilder(options)",
          "46:   }",
          "48:   override def newWriteBuilder(info: LogicalWriteInfo): WriteBuilder = {",
          "49:     operation.newWriteBuilder(info)",
          "50:   }",
          "51: }",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "926:     tableDoesNotSupportError(\"atomic partition management\", table)",
          "927:   }",
          "929:   def cannotRenameTableWithAlterViewError(): Throwable = {",
          "930:     new AnalysisException(",
          "931:       \"Cannot rename a table with ALTER VIEW. Please use ALTER TABLE instead.\")",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "929:   def tableIsNotRowLevelOperationTableError(table: Table): Throwable = {",
          "930:     throw new AnalysisException(s\"Table ${table.name} is not a row-level operation table\")",
          "931:   }",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Implicits.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Implicits.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Implicits.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Implicits.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "23: import org.apache.spark.sql.catalyst.expressions.AttributeReference",
          "24: import org.apache.spark.sql.catalyst.util.METADATA_COL_ATTR_KEY",
          "25: import org.apache.spark.sql.connector.catalog.{MetadataColumn, SupportsAtomicPartitionManagement, SupportsDelete, SupportsPartitionManagement, SupportsRead, SupportsWrite, Table, TableCapability, TruncatableTable}",
          "26: import org.apache.spark.sql.errors.QueryCompilationErrors",
          "27: import org.apache.spark.sql.types.{MetadataBuilder, StructField, StructType}",
          "28: import org.apache.spark.sql.util.CaseInsensitiveStringMap",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "26: import org.apache.spark.sql.connector.write.RowLevelOperationTable",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "82:       }",
          "83:     }",
          "85:     def supports(capability: TableCapability): Boolean = table.capabilities.contains(capability)",
          "87:     def supportsAny(capabilities: TableCapability*): Boolean = capabilities.exists(supports)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "86:     def asRowLevelOperationTable: RowLevelOperationTable = {",
          "87:       table match {",
          "88:         case rowLevelOperationTable: RowLevelOperationTable =>",
          "89:           rowLevelOperationTable",
          "90:         case _ =>",
          "91:           throw QueryCompilationErrors.tableIsNotRowLevelOperationTableError(table)",
          "92:       }",
          "93:     }",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/connector/catalog/InMemoryRowLevelOperationTable.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/connector/catalog/InMemoryRowLevelOperationTable.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/connector/catalog/InMemoryRowLevelOperationTable.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/connector/catalog/InMemoryRowLevelOperationTable.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "18: package org.apache.spark.sql.connector.catalog",
          "20: import java.util",
          "22: import org.apache.spark.sql.connector.distributions.{Distribution, Distributions}",
          "23: import org.apache.spark.sql.connector.expressions.{FieldReference, LogicalExpressions, NamedReference, SortDirection, SortOrder, Transform}",
          "24: import org.apache.spark.sql.connector.read.{Scan, ScanBuilder}",
          "25: import org.apache.spark.sql.connector.write.{BatchWrite, LogicalWriteInfo, RequiresDistributionAndOrdering, RowLevelOperation, RowLevelOperationBuilder, RowLevelOperationInfo, Write, WriteBuilder, WriterCommitMessage}",
          "26: import org.apache.spark.sql.connector.write.RowLevelOperation.Command",
          "27: import org.apache.spark.sql.types.StructType",
          "28: import org.apache.spark.sql.util.CaseInsensitiveStringMap",
          "30: class InMemoryRowLevelOperationTable(",
          "31:     name: String,",
          "32:     schema: StructType,",
          "33:     partitioning: Array[Transform],",
          "34:     properties: util.Map[String, String])",
          "35:   extends InMemoryTable(name, schema, partitioning, properties) with SupportsRowLevelOperations {",
          "37:   override def newRowLevelOperationBuilder(",
          "38:       info: RowLevelOperationInfo): RowLevelOperationBuilder = {",
          "39:     () => PartitionBasedOperation(info.command)",
          "40:   }",
          "42:   case class PartitionBasedOperation(command: Command) extends RowLevelOperation {",
          "43:     private final val PARTITION_COLUMN_REF = FieldReference(PartitionKeyColumn.name)",
          "45:     var configuredScan: InMemoryBatchScan = _",
          "47:     override def requiredMetadataAttributes(): Array[NamedReference] = {",
          "48:       Array(PARTITION_COLUMN_REF)",
          "49:     }",
          "51:     override def newScanBuilder(options: CaseInsensitiveStringMap): ScanBuilder = {",
          "52:       new InMemoryScanBuilder(schema) {",
          "53:         override def build: Scan = {",
          "54:           val scan = super.build()",
          "55:           configuredScan = scan.asInstanceOf[InMemoryBatchScan]",
          "56:           scan",
          "57:         }",
          "58:       }",
          "59:     }",
          "61:     override def newWriteBuilder(info: LogicalWriteInfo): WriteBuilder = new WriteBuilder {",
          "63:       override def build(): Write = new Write with RequiresDistributionAndOrdering {",
          "64:         override def requiredDistribution(): Distribution = {",
          "65:           Distributions.clustered(Array(PARTITION_COLUMN_REF))",
          "66:         }",
          "68:         override def requiredOrdering(): Array[SortOrder] = {",
          "69:           Array[SortOrder](",
          "70:             LogicalExpressions.sort(",
          "71:               PARTITION_COLUMN_REF,",
          "72:               SortDirection.ASCENDING,",
          "73:               SortDirection.ASCENDING.defaultNullOrdering())",
          "74:           )",
          "75:         }",
          "77:         override def toBatch: BatchWrite = PartitionBasedReplaceData(configuredScan)",
          "79:         override def description(): String = \"InMemoryWrite\"",
          "80:       }",
          "81:     }",
          "83:     override def description(): String = \"InMemoryPartitionReplaceOperation\"",
          "84:   }",
          "86:   private case class PartitionBasedReplaceData(scan: InMemoryBatchScan) extends TestBatchWrite {",
          "88:     override def commit(messages: Array[WriterCommitMessage]): Unit = dataMap.synchronized {",
          "89:       val newData = messages.map(_.asInstanceOf[BufferedRows])",
          "90:       val readRows = scan.data.flatMap(_.asInstanceOf[BufferedRows].rows)",
          "91:       val readPartitions = readRows.map(r => getKey(r, schema))",
          "92:       dataMap --= readPartitions",
          "93:       withData(newData, schema)",
          "94:     }",
          "95:   }",
          "96: }",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/connector/catalog/InMemoryRowLevelOperationTableCatalog.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/connector/catalog/InMemoryRowLevelOperationTableCatalog.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/connector/catalog/InMemoryRowLevelOperationTableCatalog.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/connector/catalog/InMemoryRowLevelOperationTableCatalog.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "18: package org.apache.spark.sql.connector.catalog",
          "20: import java.util",
          "22: import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException",
          "23: import org.apache.spark.sql.connector.expressions.Transform",
          "24: import org.apache.spark.sql.types.StructType",
          "26: class InMemoryRowLevelOperationTableCatalog extends InMemoryTableCatalog {",
          "27:   import CatalogV2Implicits._",
          "29:   override def createTable(",
          "30:       ident: Identifier,",
          "31:       schema: StructType,",
          "32:       partitions: Array[Transform],",
          "33:       properties: util.Map[String, String]): Table = {",
          "34:     if (tables.containsKey(ident)) {",
          "35:       throw new TableAlreadyExistsException(ident)",
          "36:     }",
          "38:     InMemoryTableCatalog.maybeSimulateFailedTableCreation(properties)",
          "40:     val tableName = s\"$name.${ident.quoted}\"",
          "41:     val table = new InMemoryRowLevelOperationTable(tableName, schema, partitions, properties)",
          "42:     tables.put(ident, table)",
          "43:     namespaces.putIfAbsent(ident.namespace.toList, Map())",
          "44:     table",
          "45:   }",
          "46: }",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/connector/catalog/InMemoryTable.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/connector/catalog/InMemoryTable.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/connector/catalog/InMemoryTable.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/connector/catalog/InMemoryTable.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "56:   extends Table with SupportsRead with SupportsWrite with SupportsDelete",
          "57:       with SupportsMetadataColumns {",
          "60:     override def name: String = \"_partition\"",
          "61:     override def dataType: DataType = StringType",
          "62:     override def comment: String = \"Partition key used to store the row\"",
          "",
          "[Removed Lines]",
          "59:   private object PartitionKeyColumn extends MetadataColumn {",
          "",
          "[Added Lines]",
          "59:   protected object PartitionKeyColumn extends MetadataColumn {",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "104:   private val UTC = ZoneId.of(\"UTC\")",
          "105:   private val EPOCH_LOCAL_DATE = Instant.EPOCH.atZone(UTC).toLocalDate",
          "108:     @scala.annotation.tailrec",
          "109:     def extractor(",
          "110:         fieldNames: Array[String],",
          "",
          "[Removed Lines]",
          "107:   private def getKey(row: InternalRow): Seq[Any] = {",
          "",
          "[Added Lines]",
          "107:   protected def getKey(row: InternalRow): Seq[Any] = {",
          "108:     getKey(row, schema)",
          "109:   }",
          "111:   protected def getKey(row: InternalRow, rowSchema: StructType): Seq[Any] = {",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "124:       }",
          "125:     }",
          "128:     partitioning.map {",
          "129:       case IdentityTransform(ref) =>",
          "130:         extractor(ref.fieldNames, cleanedSchema, row)._1",
          "",
          "[Removed Lines]",
          "127:     val cleanedSchema = CharVarcharUtils.replaceCharVarcharWithStringInSchema(schema)",
          "",
          "[Added Lines]",
          "131:     val cleanedSchema = CharVarcharUtils.replaceCharVarcharWithStringInSchema(rowSchema)",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "219:     dataMap(key).clear()",
          "220:   }",
          "223:     data.foreach(_.rows.foreach { row =>",
          "225:       dataMap += dataMap.get(key)",
          "226:         .map(key -> _.withRow(row))",
          "227:         .getOrElse(key -> new BufferedRows(key).withRow(row))",
          "",
          "[Removed Lines]",
          "222:   def withData(data: Array[BufferedRows]): InMemoryTable = dataMap.synchronized {",
          "224:       val key = getKey(row)",
          "",
          "[Added Lines]",
          "226:   def withData(data: Array[BufferedRows]): InMemoryTable = {",
          "227:     withData(data, schema)",
          "228:   }",
          "230:   def withData(",
          "231:       data: Array[BufferedRows],",
          "232:       writeSchema: StructType): InMemoryTable = dataMap.synchronized {",
          "234:       val key = getKey(row, writeSchema)",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "372:     }",
          "373:   }",
          "376:     override def createBatchWriterFactory(info: PhysicalWriteInfo): DataWriterFactory = {",
          "377:       BufferedRowsWriterFactory",
          "378:     }",
          "",
          "[Removed Lines]",
          "375:   private abstract class TestBatchWrite extends BatchWrite {",
          "",
          "[Added Lines]",
          "385:   protected abstract class TestBatchWrite extends BatchWrite {",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkOptimizer.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/SparkOptimizer.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/SparkOptimizer.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/SparkOptimizer.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "25: import org.apache.spark.sql.connector.catalog.CatalogManager",
          "26: import org.apache.spark.sql.execution.datasources.PruneFileSourcePartitions",
          "27: import org.apache.spark.sql.execution.datasources.SchemaPruning",
          "29: import org.apache.spark.sql.execution.dynamicpruning.{CleanupDynamicPruningFilters, PartitionPruning}",
          "30: import org.apache.spark.sql.execution.python.{ExtractGroupingPythonUDFFromAggregate, ExtractPythonUDFFromAggregate, ExtractPythonUDFs}",
          "",
          "[Removed Lines]",
          "28: import org.apache.spark.sql.execution.datasources.v2.{V2ScanPartitioning, V2ScanRelationPushDown, V2Writes}",
          "",
          "[Added Lines]",
          "28: import org.apache.spark.sql.execution.datasources.v2.{GroupBasedRowLevelOperationScanPlanning, OptimizeMetadataOnlyDeleteFromTable, V2ScanPartitioning, V2ScanRelationPushDown, V2Writes}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "38:   override def earlyScanPushDownRules: Seq[Rule[LogicalPlan]] =",
          "40:     Seq(SchemaPruning) :+",
          "41:       V2ScanRelationPushDown :+",
          "42:       V2ScanPartitioning :+",
          "43:       V2Writes :+",
          "44:       PruneFileSourcePartitions",
          "46:   override def defaultBatches: Seq[Batch] = (preOptimizationBatches ++ super.defaultBatches :+",
          "47:     Batch(\"Optimize Metadata Only Query\", Once, OptimizeMetadataOnlyQuery(catalog)) :+",
          "48:     Batch(\"PartitionPruning\", Once,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "41:       GroupBasedRowLevelOperationScanPlanning :+",
          "47:   override def preCBORules: Seq[Rule[LogicalPlan]] =",
          "48:     OptimizeMetadataOnlyDeleteFromTable :: Nil",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "78:     ExtractPythonUDFFromJoinCondition.ruleName :+",
          "79:     ExtractPythonUDFFromAggregate.ruleName :+ ExtractGroupingPythonUDFFromAggregate.ruleName :+",
          "80:     ExtractPythonUDFs.ruleName :+",
          "81:     V2ScanRelationPushDown.ruleName :+",
          "82:     V2ScanPartitioning.ruleName :+",
          "83:     V2Writes.ruleName",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "85:     GroupBasedRowLevelOperationScanPlanning.ruleName :+",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "25: import org.apache.spark.sql.catalyst.catalog.CatalogUtils",
          "26: import org.apache.spark.sql.catalyst.expressions",
          "27: import org.apache.spark.sql.catalyst.expressions.{And, Attribute, DynamicPruning, Expression, NamedExpression, Not, Or, PredicateHelper, SubqueryExpression}",
          "28: import org.apache.spark.sql.catalyst.planning.PhysicalOperation",
          "29: import org.apache.spark.sql.catalyst.plans.logical._",
          "30: import org.apache.spark.sql.catalyst.util.{toPrettySQL, V2ExpressionBuilder}",
          "32: import org.apache.spark.sql.connector.catalog.index.SupportsIndex",
          "33: import org.apache.spark.sql.connector.expressions.{FieldReference}",
          "34: import org.apache.spark.sql.connector.expressions.filter.{And => V2And, Not => V2Not, Or => V2Or, Predicate}",
          "",
          "[Removed Lines]",
          "31: import org.apache.spark.sql.connector.catalog.{Identifier, StagingTableCatalog, SupportsNamespaces, SupportsPartitionManagement, SupportsWrite, Table, TableCapability, TableCatalog}",
          "",
          "[Added Lines]",
          "28: import org.apache.spark.sql.catalyst.expressions.Literal.TrueLiteral",
          "32: import org.apache.spark.sql.connector.catalog.{Identifier, StagingTableCatalog, SupportsDelete, SupportsNamespaces, SupportsPartitionManagement, SupportsWrite, Table, TableCapability, TableCatalog, TruncatableTable}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "254:     case OverwritePartitionsDynamic(r: DataSourceV2Relation, query, _, _, Some(write)) =>",
          "255:       OverwritePartitionsDynamicExec(planLater(query), refreshCache(r), write) :: Nil",
          "257:     case DeleteFromTable(relation, condition) =>",
          "258:       relation match {",
          "259:         case DataSourceV2ScanRelation(r, _, output, _) =>",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "258:     case DeleteFromTableWithFilters(r: DataSourceV2Relation, filters) =>",
          "259:       DeleteFromTableExec(r.table.asDeletable, filters.toArray, refreshCache(r)) :: Nil",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "269:                   throw QueryCompilationErrors.cannotTranslateExpressionToSourceFilterError(f))",
          "270:               }).toArray",
          "274:           }",
          "277:         case _ =>",
          "278:           throw QueryCompilationErrors.deleteOnlySupportedWithV2TablesError()",
          "279:       }",
          "281:     case WriteToContinuousDataSource(writer, query, customMetrics) =>",
          "282:       WriteToContinuousDataSourceExec(writer, planLater(query), customMetrics) :: Nil",
          "",
          "[Removed Lines]",
          "272:           if (!table.asDeletable.canDeleteWhere(filters)) {",
          "273:             throw QueryCompilationErrors.cannotDeleteTableWhereFiltersError(table, filters)",
          "276:           DeleteFromTableExec(table.asDeletable, filters, refreshCache(r)) :: Nil",
          "",
          "[Added Lines]",
          "276:           table match {",
          "277:             case t: SupportsDelete if t.canDeleteWhere(filters) =>",
          "278:               DeleteFromTableExec(t, filters, refreshCache(r)) :: Nil",
          "279:             case t: SupportsDelete =>",
          "280:               throw QueryCompilationErrors.cannotDeleteTableWhereFiltersError(t, filters)",
          "281:             case t: TruncatableTable if condition == TrueLiteral =>",
          "282:               TruncateTableExec(t, refreshCache(r)) :: Nil",
          "283:             case _ =>",
          "284:               throw QueryCompilationErrors.tableDoesNotSupportDeletesError(table)",
          "291:     case ReplaceData(_: DataSourceV2Relation, _, query, r: DataSourceV2Relation, Some(write)) =>",
          "293:       ReplaceDataExec(planLater(query), refreshCache(r), write) :: Nil",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/GroupBasedRowLevelOperationScanPlanning.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/GroupBasedRowLevelOperationScanPlanning.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/GroupBasedRowLevelOperationScanPlanning.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/GroupBasedRowLevelOperationScanPlanning.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "18: package org.apache.spark.sql.execution.datasources.v2",
          "20: import org.apache.spark.sql.catalyst.expressions.{AttributeReference, AttributeSet, Expression, PredicateHelper, SubqueryExpression}",
          "21: import org.apache.spark.sql.catalyst.planning.GroupBasedRowLevelOperation",
          "22: import org.apache.spark.sql.catalyst.plans.logical.{LogicalPlan, ReplaceData}",
          "23: import org.apache.spark.sql.catalyst.rules.Rule",
          "24: import org.apache.spark.sql.connector.expressions.filter.{Predicate => V2Filter}",
          "25: import org.apache.spark.sql.connector.read.ScanBuilder",
          "26: import org.apache.spark.sql.execution.datasources.DataSourceStrategy",
          "27: import org.apache.spark.sql.sources.Filter",
          "35: object GroupBasedRowLevelOperationScanPlanning extends Rule[LogicalPlan] with PredicateHelper {",
          "37:   import DataSourceV2Implicits._",
          "39:   override def apply(plan: LogicalPlan): LogicalPlan = plan transformDown {",
          "42:     case GroupBasedRowLevelOperation(rd: ReplaceData, cond, relation: DataSourceV2Relation) =>",
          "43:       val table = relation.table.asRowLevelOperationTable",
          "44:       val scanBuilder = table.newScanBuilder(relation.options)",
          "46:       val (pushedFilters, remainingFilters) = pushFilters(cond, relation.output, scanBuilder)",
          "47:       val pushedFiltersStr = if (pushedFilters.isLeft) {",
          "48:         pushedFilters.left.get.mkString(\", \")",
          "49:       } else {",
          "50:         pushedFilters.right.get.mkString(\", \")",
          "51:       }",
          "53:       val (scan, output) = PushDownUtils.pruneColumns(scanBuilder, relation, relation.output, Nil)",
          "55:       logInfo(",
          "56:         s\"\"\"",
          "57:            |Pushing operators to ${relation.name}",
          "58:            |Pushed filters: $pushedFiltersStr",
          "59:            |Filters that were not pushed: ${remainingFilters.mkString(\", \")}",
          "60:            |Output: ${output.mkString(\", \")}",
          "61:          \"\"\".stripMargin)",
          "64:       rd transform {",
          "65:         case r: DataSourceV2Relation if r eq relation =>",
          "66:           DataSourceV2ScanRelation(r, scan, PushDownUtils.toOutputAttrs(scan.readSchema(), r))",
          "67:       }",
          "68:   }",
          "70:   private def pushFilters(",
          "71:       cond: Expression,",
          "72:       tableAttrs: Seq[AttributeReference],",
          "73:       scanBuilder: ScanBuilder): (Either[Seq[Filter], Seq[V2Filter]], Seq[Expression]) = {",
          "75:     val tableAttrSet = AttributeSet(tableAttrs)",
          "76:     val filters = splitConjunctivePredicates(cond).filter(_.references.subsetOf(tableAttrSet))",
          "77:     val normalizedFilters = DataSourceStrategy.normalizeExprs(filters, tableAttrs)",
          "78:     val (_, normalizedFiltersWithoutSubquery) =",
          "79:       normalizedFilters.partition(SubqueryExpression.hasSubquery)",
          "81:     PushDownUtils.pushFilters(scanBuilder, normalizedFiltersWithoutSubquery)",
          "82:   }",
          "83: }",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/OptimizeMetadataOnlyDeleteFromTable.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/OptimizeMetadataOnlyDeleteFromTable.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/OptimizeMetadataOnlyDeleteFromTable.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/OptimizeMetadataOnlyDeleteFromTable.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "18: package org.apache.spark.sql.execution.datasources.v2",
          "20: import org.apache.spark.sql.catalyst.expressions.{Expression, PredicateHelper, SubqueryExpression}",
          "21: import org.apache.spark.sql.catalyst.expressions.Literal.TrueLiteral",
          "22: import org.apache.spark.sql.catalyst.plans.logical.{DeleteFromTable, DeleteFromTableWithFilters, LogicalPlan, ReplaceData, RowLevelWrite}",
          "23: import org.apache.spark.sql.catalyst.rules.Rule",
          "24: import org.apache.spark.sql.connector.catalog.{SupportsDelete, TruncatableTable}",
          "25: import org.apache.spark.sql.connector.write.RowLevelOperation",
          "26: import org.apache.spark.sql.connector.write.RowLevelOperation.Command.DELETE",
          "27: import org.apache.spark.sql.execution.datasources.DataSourceStrategy",
          "28: import org.apache.spark.sql.sources",
          "37: object OptimizeMetadataOnlyDeleteFromTable extends Rule[LogicalPlan] with PredicateHelper {",
          "39:   override def apply(plan: LogicalPlan): LogicalPlan = plan transform {",
          "40:     case RewrittenRowLevelCommand(rowLevelPlan, DELETE, cond, relation: DataSourceV2Relation) =>",
          "41:       relation.table match {",
          "42:         case table: SupportsDelete if !SubqueryExpression.hasSubquery(cond) =>",
          "43:           val predicates = splitConjunctivePredicates(cond)",
          "44:           val normalizedPredicates = DataSourceStrategy.normalizeExprs(predicates, relation.output)",
          "45:           val filters = toDataSourceFilters(normalizedPredicates)",
          "46:           val allPredicatesTranslated = normalizedPredicates.size == filters.length",
          "47:           if (allPredicatesTranslated && table.canDeleteWhere(filters)) {",
          "48:             logDebug(s\"Switching to delete with filters: ${filters.mkString(\"[\", \", \", \"]\")}\")",
          "49:             DeleteFromTableWithFilters(relation, filters)",
          "50:           } else {",
          "51:             rowLevelPlan",
          "52:           }",
          "54:         case _: TruncatableTable if cond == TrueLiteral =>",
          "55:           DeleteFromTable(relation, cond)",
          "57:         case _ =>",
          "58:           rowLevelPlan",
          "59:       }",
          "60:   }",
          "62:   private def toDataSourceFilters(predicates: Seq[Expression]): Array[sources.Filter] = {",
          "63:     predicates.flatMap { p =>",
          "64:       val filter = DataSourceStrategy.translateFilter(p, supportNestedPredicatePushdown = true)",
          "65:       if (filter.isEmpty) {",
          "66:         logDebug(s\"Cannot translate expression to data source filter: $p\")",
          "67:       }",
          "68:       filter",
          "69:     }.toArray",
          "70:   }",
          "72:   private object RewrittenRowLevelCommand {",
          "73:     type ReturnType = (RowLevelWrite, RowLevelOperation.Command, Expression, LogicalPlan)",
          "75:     def unapply(plan: LogicalPlan): Option[ReturnType] = plan match {",
          "76:       case rd @ ReplaceData(_, cond, _, originalTable, _) =>",
          "77:         val command = rd.operation.command",
          "78:         Some(rd, command, cond, originalTable)",
          "80:       case _ =>",
          "81:         None",
          "82:     }",
          "83:   }",
          "84: }",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/PushDownUtils.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/PushDownUtils.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/PushDownUtils.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/PushDownUtils.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "187:     }",
          "188:   }",
          "191:       schema: StructType,",
          "192:       relation: DataSourceV2Relation): Seq[AttributeReference] = {",
          "193:     val nameToAttr = relation.output.map(_.name).zip(relation.output).toMap",
          "",
          "[Removed Lines]",
          "190:   private def toOutputAttrs(",
          "",
          "[Added Lines]",
          "190:   def toOutputAttrs(",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2Writes.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2Writes.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2Writes.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2Writes.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: import java.util.UUID",
          "22: import org.apache.spark.sql.catalyst.expressions.PredicateHelper",
          "24: import org.apache.spark.sql.catalyst.rules.Rule",
          "25: import org.apache.spark.sql.catalyst.streaming.InternalOutputModes._",
          "26: import org.apache.spark.sql.connector.catalog.{SupportsWrite, Table}",
          "",
          "[Removed Lines]",
          "23: import org.apache.spark.sql.catalyst.plans.logical.{AppendData, LogicalPlan, OverwriteByExpression, OverwritePartitionsDynamic}",
          "",
          "[Added Lines]",
          "23: import org.apache.spark.sql.catalyst.plans.logical.{AppendData, LogicalPlan, OverwriteByExpression, OverwritePartitionsDynamic, Project, ReplaceData}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "31: import org.apache.spark.sql.internal.connector.SupportsStreamingUpdateAsAppend",
          "32: import org.apache.spark.sql.sources.{AlwaysTrue, Filter}",
          "33: import org.apache.spark.sql.streaming.OutputMode",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "34: import org.apache.spark.sql.types.StructType",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "42:   override def apply(plan: LogicalPlan): LogicalPlan = plan transformDown {",
          "43:     case a @ AppendData(r: DataSourceV2Relation, query, options, _, None) =>",
          "45:       val write = writeBuilder.build()",
          "46:       val newQuery = DistributionAndOrderingUtils.prepareQuery(write, query, conf)",
          "47:       a.copy(write = Some(write), query = newQuery)",
          "",
          "[Removed Lines]",
          "44:       val writeBuilder = newWriteBuilder(r.table, query, options)",
          "",
          "[Added Lines]",
          "45:       val writeBuilder = newWriteBuilder(r.table, options, query.schema)",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "57:       }.toArray",
          "59:       val table = r.table",
          "61:       val write = writeBuilder match {",
          "62:         case builder: SupportsTruncate if isTruncate(filters) =>",
          "63:           builder.truncate().build()",
          "",
          "[Removed Lines]",
          "60:       val writeBuilder = newWriteBuilder(table, query, options)",
          "",
          "[Added Lines]",
          "61:       val writeBuilder = newWriteBuilder(table, options, query.schema)",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "73:     case o @ OverwritePartitionsDynamic(r: DataSourceV2Relation, query, options, _, None) =>",
          "74:       val table = r.table",
          "76:       val write = writeBuilder match {",
          "77:         case builder: SupportsDynamicOverwrite =>",
          "78:           builder.overwriteDynamicPartitions().build()",
          "",
          "[Removed Lines]",
          "75:       val writeBuilder = newWriteBuilder(table, query, options)",
          "",
          "[Added Lines]",
          "76:       val writeBuilder = newWriteBuilder(table, options, query.schema)",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "85:     case WriteToMicroBatchDataSource(",
          "86:         relation, table, query, queryId, writeOptions, outputMode, Some(batchId)) =>",
          "89:       val write = buildWriteForMicroBatch(table, writeBuilder, outputMode)",
          "90:       val microBatchWrite = new MicroBatchWrite(batchId, write.toStreaming)",
          "91:       val customMetrics = write.supportedCustomMetrics.toSeq",
          "92:       val newQuery = DistributionAndOrderingUtils.prepareQuery(write, query, conf)",
          "93:       WriteToDataSourceV2(relation, microBatchWrite, newQuery, customMetrics)",
          "94:   }",
          "96:   private def buildWriteForMicroBatch(",
          "",
          "[Removed Lines]",
          "88:       val writeBuilder = newWriteBuilder(table, query, writeOptions, queryId)",
          "",
          "[Added Lines]",
          "89:       val writeBuilder = newWriteBuilder(table, writeOptions, query.schema, queryId)",
          "96:     case rd @ ReplaceData(r: DataSourceV2Relation, _, query, _, None) =>",
          "97:       val rowSchema = StructType.fromAttributes(rd.dataInput)",
          "98:       val writeBuilder = newWriteBuilder(r.table, Map.empty, rowSchema)",
          "99:       val write = writeBuilder.build()",
          "100:       val newQuery = DistributionAndOrderingUtils.prepareQuery(write, query, conf)",
          "102:       rd.copy(write = Some(write), query = Project(rd.dataInput, newQuery))",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "120:   private def newWriteBuilder(",
          "121:       table: Table,",
          "123:       writeOptions: Map[String, String],",
          "124:       queryId: String = UUID.randomUUID().toString): WriteBuilder = {",
          "127:     table.asWritable.newWriteBuilder(info)",
          "128:   }",
          "129: }",
          "",
          "[Removed Lines]",
          "122:       query: LogicalPlan,",
          "126:     val info = LogicalWriteInfoImpl(queryId, query.schema, writeOptions.asOptions)",
          "",
          "[Added Lines]",
          "133:       rowSchema: StructType,",
          "136:     val info = LogicalWriteInfoImpl(queryId, rowSchema, writeOptions.asOptions)",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/WriteToDataSourceV2Exec.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/WriteToDataSourceV2Exec.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/WriteToDataSourceV2Exec.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/WriteToDataSourceV2Exec.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "284:     copy(query = newChild)",
          "285: }",
          "287: case class WriteToDataSourceV2Exec(",
          "288:     batchWrite: BatchWrite,",
          "289:     refreshCache: () => Unit,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "290: case class ReplaceDataExec(",
          "291:     query: SparkPlan,",
          "292:     refreshCache: () => Unit,",
          "293:     write: Write) extends V2ExistingTableWriteExec {",
          "295:   override val stringArgs: Iterator[Any] = Iterator(query, write)",
          "297:   override protected def withNewChildInternal(newChild: SparkPlan): ReplaceDataExec = {",
          "298:     copy(query = newChild)",
          "299:   }",
          "300: }",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/connector/DeleteFromTableSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/connector/DeleteFromTableSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/connector/DeleteFromTableSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/connector/DeleteFromTableSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "18: package org.apache.spark.sql.connector",
          "20: import java.util.Collections",
          "22: import org.scalatest.BeforeAndAfter",
          "24: import org.apache.spark.sql.{AnalysisException, DataFrame, Encoders, QueryTest, Row}",
          "25: import org.apache.spark.sql.connector.catalog.{Identifier, InMemoryRowLevelOperationTableCatalog}",
          "26: import org.apache.spark.sql.connector.expressions.LogicalExpressions._",
          "27: import org.apache.spark.sql.execution.{QueryExecution, SparkPlan}",
          "28: import org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanHelper",
          "29: import org.apache.spark.sql.execution.datasources.v2.{DeleteFromTableExec, ReplaceDataExec}",
          "30: import org.apache.spark.sql.test.SharedSparkSession",
          "31: import org.apache.spark.sql.types.StructType",
          "32: import org.apache.spark.sql.util.QueryExecutionListener",
          "34: abstract class DeleteFromTableSuiteBase",
          "35:   extends QueryTest with SharedSparkSession with BeforeAndAfter with AdaptiveSparkPlanHelper {",
          "37:   import org.apache.spark.sql.connector.catalog.CatalogV2Implicits._",
          "38:   import testImplicits._",
          "40:   before {",
          "41:     spark.conf.set(\"spark.sql.catalog.cat\", classOf[InMemoryRowLevelOperationTableCatalog].getName)",
          "42:   }",
          "44:   after {",
          "45:     spark.sessionState.catalogManager.reset()",
          "46:     spark.sessionState.conf.unsetConf(\"spark.sql.catalog.cat\")",
          "47:   }",
          "49:   private val namespace = Array(\"ns1\")",
          "50:   private val ident = Identifier.of(namespace, \"test_table\")",
          "51:   private val tableNameAsString = \"cat.\" + ident.toString",
          "53:   private def catalog: InMemoryRowLevelOperationTableCatalog = {",
          "54:     val catalog = spark.sessionState.catalogManager.catalog(\"cat\")",
          "55:     catalog.asTableCatalog.asInstanceOf[InMemoryRowLevelOperationTableCatalog]",
          "56:   }",
          "58:   test(\"EXPLAIN only delete\") {",
          "59:     createAndInitTable(\"id INT, dep STRING\", \"\"\"{ \"id\": 1, \"dep\": \"hr\" }\"\"\")",
          "61:     sql(s\"EXPLAIN DELETE FROM $tableNameAsString WHERE id <= 10\")",
          "63:     checkAnswer(",
          "64:       sql(s\"SELECT * FROM $tableNameAsString\"),",
          "65:       Row(1, \"hr\") :: Nil)",
          "66:   }",
          "68:   test(\"delete from empty tables\") {",
          "69:     createTable(\"id INT, dep STRING\")",
          "71:     sql(s\"DELETE FROM $tableNameAsString WHERE id <= 1\")",
          "73:     checkAnswer(sql(s\"SELECT * FROM $tableNameAsString\"), Nil)",
          "74:   }",
          "76:   test(\"delete with basic filters\") {",
          "77:     createAndInitTable(\"id INT, dep STRING\",",
          "78:       \"\"\"{ \"id\": 1, \"dep\": \"hr\" }",
          "79:         |{ \"id\": 2, \"dep\": \"software\" }",
          "80:         |{ \"id\": 3, \"dep\": \"hr\" }",
          "81:         |\"\"\".stripMargin)",
          "83:     sql(s\"DELETE FROM $tableNameAsString WHERE id <= 1\")",
          "85:     checkAnswer(",
          "86:       sql(s\"SELECT * FROM $tableNameAsString\"),",
          "87:       Row(2, \"software\") :: Row(3, \"hr\") :: Nil)",
          "88:   }",
          "90:   test(\"delete with aliases\") {",
          "91:     createAndInitTable(\"id INT, dep STRING\",",
          "92:       \"\"\"{ \"id\": 1, \"dep\": \"hr\" }",
          "93:         |{ \"id\": 2, \"dep\": \"software\" }",
          "94:         |{ \"id\": 3, \"dep\": \"hr\" }",
          "95:         |\"\"\".stripMargin)",
          "97:     sql(s\"DELETE FROM $tableNameAsString AS t WHERE t.id <= 1 OR t.dep = 'hr'\")",
          "99:     checkAnswer(sql(s\"SELECT * FROM $tableNameAsString\"), Row(2, \"software\") :: Nil)",
          "100:   }",
          "102:   test(\"delete with IN predicates\") {",
          "103:     createAndInitTable(\"id INT, dep STRING\",",
          "104:       \"\"\"{ \"id\": 1, \"dep\": \"hr\" }",
          "105:         |{ \"id\": 2, \"dep\": \"software\" }",
          "106:         |{ \"id\": null, \"dep\": \"hr\" }",
          "107:         |\"\"\".stripMargin)",
          "109:     sql(s\"DELETE FROM $tableNameAsString WHERE id IN (1, null)\")",
          "111:     checkAnswer(",
          "112:       sql(s\"SELECT * FROM $tableNameAsString\"),",
          "113:       Row(2, \"software\") :: Row(null, \"hr\") :: Nil)",
          "114:   }",
          "116:   test(\"delete with NOT IN predicates\") {",
          "117:     createAndInitTable(\"id INT, dep STRING\",",
          "118:       \"\"\"{ \"id\": 1, \"dep\": \"hr\" }",
          "119:         |{ \"id\": 2, \"dep\": \"software\" }",
          "120:         |{ \"id\": null, \"dep\": \"hr\" }",
          "121:         |\"\"\".stripMargin)",
          "123:     sql(s\"DELETE FROM $tableNameAsString WHERE id NOT IN (null, 1)\")",
          "125:     checkAnswer(",
          "126:       sql(s\"SELECT * FROM $tableNameAsString\"),",
          "127:       Row(1, \"hr\") :: Row(2, \"software\") :: Row(null, \"hr\") :: Nil)",
          "129:     sql(s\"DELETE FROM $tableNameAsString WHERE id NOT IN (1, 10)\")",
          "131:     checkAnswer(",
          "132:       sql(s\"SELECT * FROM $tableNameAsString\"),",
          "133:       Row(1, \"hr\") :: Row(null, \"hr\") :: Nil)",
          "134:   }",
          "136:   test(\"delete with conditions on nested columns\") {",
          "137:     createAndInitTable(\"id INT, complex STRUCT<c1:INT,c2:STRING>, dep STRING\",",
          "138:       \"\"\"{ \"id\": 1, \"complex\": { \"c1\": 3, \"c2\": \"v1\" }, \"dep\": \"hr\" }",
          "139:         |{ \"id\": 2, \"complex\": { \"c1\": 2, \"c2\": \"v2\" }, \"dep\": \"software\" }",
          "140:         |\"\"\".stripMargin)",
          "142:     sql(s\"DELETE FROM $tableNameAsString WHERE complex.c1 = id + 2\")",
          "144:     checkAnswer(",
          "145:       sql(s\"SELECT * FROM $tableNameAsString\"),",
          "146:       Row(2, Row(2, \"v2\"), \"software\") :: Nil)",
          "148:     sql(s\"DELETE FROM $tableNameAsString t WHERE t.complex.c1 = id\")",
          "150:     checkAnswer(sql(s\"SELECT * FROM $tableNameAsString\"), Nil)",
          "151:   }",
          "153:   test(\"delete with IN subqueries\") {",
          "154:     withTempView(\"deleted_id\", \"deleted_dep\") {",
          "155:       createAndInitTable(\"id INT, dep STRING\",",
          "156:         \"\"\"{ \"id\": 1, \"dep\": \"hr\" }",
          "157:           |{ \"id\": 2, \"dep\": \"hardware\" }",
          "158:           |{ \"id\": null, \"dep\": \"hr\" }",
          "159:           |\"\"\".stripMargin)",
          "161:       val deletedIdDF = Seq(Some(0), Some(1), None).toDF()",
          "162:       deletedIdDF.createOrReplaceTempView(\"deleted_id\")",
          "164:       val deletedDepDF = Seq(\"software\", \"hr\").toDF()",
          "165:       deletedDepDF.createOrReplaceTempView(\"deleted_dep\")",
          "167:       sql(",
          "168:         s\"\"\"DELETE FROM $tableNameAsString",
          "169:            |WHERE",
          "170:            | id IN (SELECT * FROM deleted_id)",
          "171:            | AND",
          "172:            | dep IN (SELECT * FROM deleted_dep)",
          "173:            |\"\"\".stripMargin)",
          "175:       checkAnswer(",
          "176:         sql(s\"SELECT * FROM $tableNameAsString\"),",
          "177:         Row(2, \"hardware\") :: Row(null, \"hr\") :: Nil)",
          "179:       append(\"id INT, dep STRING\",",
          "180:         \"\"\"{ \"id\": 1, \"dep\": \"hr\" }",
          "181:           |{ \"id\": -1, \"dep\": \"hr\" }",
          "182:           |\"\"\".stripMargin)",
          "184:       checkAnswer(",
          "185:         sql(s\"SELECT * FROM $tableNameAsString\"),",
          "186:         Row(-1, \"hr\") :: Row(1, \"hr\") :: Row(2, \"hardware\") :: Row(null, \"hr\") :: Nil)",
          "188:       sql(",
          "189:         s\"\"\"DELETE FROM $tableNameAsString",
          "190:            |WHERE",
          "191:            | id IS NULL",
          "192:            | OR",
          "193:            | id IN (SELECT value + 2 FROM deleted_id)",
          "194:            |\"\"\".stripMargin)",
          "196:       checkAnswer(",
          "197:         sql(s\"SELECT * FROM $tableNameAsString\"),",
          "198:         Row(-1, \"hr\") :: Row(1, \"hr\") :: Nil)",
          "200:       append(\"id INT, dep STRING\",",
          "201:         \"\"\"{ \"id\": null, \"dep\": \"hr\" }",
          "202:           |{ \"id\": 2, \"dep\": \"hr\" }",
          "203:           |\"\"\".stripMargin)",
          "205:       checkAnswer(",
          "206:         sql(s\"SELECT * FROM $tableNameAsString\"),",
          "207:         Row(-1, \"hr\") :: Row(1, \"hr\") :: Row(2, \"hr\") :: Row(null, \"hr\") :: Nil)",
          "209:       sql(",
          "210:         s\"\"\"DELETE FROM $tableNameAsString",
          "211:            |WHERE",
          "212:            | id IN (SELECT value + 2 FROM deleted_id)",
          "213:            | AND",
          "214:            | dep = 'hr'",
          "215:            |\"\"\".stripMargin)",
          "217:       checkAnswer(",
          "218:         sql(s\"SELECT * FROM $tableNameAsString\"),",
          "219:         Row(-1, \"hr\") :: Row(1, \"hr\") :: Row(null, \"hr\") :: Nil)",
          "220:     }",
          "221:   }",
          "223:   test(\"delete with multi-column IN subqueries\") {",
          "224:     withTempView(\"deleted_employee\") {",
          "225:       createAndInitTable(\"id INT, dep STRING\",",
          "226:         \"\"\"{ \"id\": 1, \"dep\": \"hr\" }",
          "227:           |{ \"id\": 2, \"dep\": \"hardware\" }",
          "228:           |{ \"id\": null, \"dep\": \"hr\" }",
          "229:           |\"\"\".stripMargin)",
          "231:       val deletedEmployeeDF = Seq((None, \"hr\"), (Some(1), \"hr\")).toDF()",
          "232:       deletedEmployeeDF.createOrReplaceTempView(\"deleted_employee\")",
          "234:       sql(",
          "235:         s\"\"\"DELETE FROM $tableNameAsString",
          "236:            |WHERE",
          "237:            | (id, dep) IN (SELECT * FROM deleted_employee)",
          "238:            |\"\"\".stripMargin)",
          "240:       checkAnswer(",
          "241:         sql(s\"SELECT * FROM $tableNameAsString\"),",
          "242:         Row(2, \"hardware\") :: Row(null, \"hr\") :: Nil)",
          "243:     }",
          "244:   }",
          "246:   test(\"delete with NOT IN subqueries\") {",
          "247:     withTempView(\"deleted_id\", \"deleted_dep\") {",
          "248:       createAndInitTable(\"id INT, dep STRING\",",
          "249:         \"\"\"{ \"id\": 1, \"dep\": \"hr\" }",
          "250:           |{ \"id\": 2, \"dep\": \"hardware\" }",
          "251:           |{ \"id\": null, \"dep\": \"hr\" }",
          "252:           |\"\"\".stripMargin)",
          "254:       val deletedIdDF = Seq(Some(-1), Some(-2), None).toDF()",
          "255:       deletedIdDF.createOrReplaceTempView(\"deleted_id\")",
          "257:       val deletedDepDF = Seq(\"software\", \"hr\").toDF()",
          "258:       deletedDepDF.createOrReplaceTempView(\"deleted_dep\")",
          "260:       sql(",
          "261:         s\"\"\"DELETE FROM $tableNameAsString",
          "262:            |WHERE",
          "263:            | id NOT IN (SELECT * FROM deleted_id)",
          "264:            |\"\"\".stripMargin)",
          "266:       checkAnswer(",
          "267:         sql(s\"SELECT * FROM $tableNameAsString\"),",
          "268:         Row(1, \"hr\") :: Row(2, \"hardware\") :: Row(null, \"hr\") :: Nil)",
          "270:       sql(",
          "271:         s\"\"\"DELETE FROM $tableNameAsString",
          "272:            |WHERE",
          "273:            | id NOT IN (SELECT * FROM deleted_id WHERE value IS NOT NULL)",
          "274:            |\"\"\".stripMargin)",
          "276:       checkAnswer(sql(s\"SELECT * FROM $tableNameAsString\"), Row(null, \"hr\") :: Nil)",
          "278:       append(\"id INT, dep STRING\",",
          "279:         \"\"\"{ \"id\": 1, \"dep\": \"hr\" }",
          "280:           |{ \"id\": 2, \"dep\": \"hardware\" }",
          "281:           |{ \"id\": null, \"dep\": \"hr\" }",
          "282:           |\"\"\".stripMargin)",
          "284:       checkAnswer(",
          "285:         sql(s\"SELECT * FROM $tableNameAsString\"),",
          "286:         Row(1, \"hr\") :: Row(2, \"hardware\") :: Row(null, \"hr\") :: Row(null, \"hr\") :: Nil)",
          "288:       sql(",
          "289:         s\"\"\"DELETE FROM $tableNameAsString",
          "290:            |WHERE",
          "291:            | id NOT IN (SELECT * FROM deleted_id)",
          "292:            | OR",
          "293:            | dep IN ('software', 'hr')",
          "294:            |\"\"\".stripMargin)",
          "296:       checkAnswer(sql(s\"SELECT * FROM $tableNameAsString\"), Row(2, \"hardware\") :: Nil)",
          "298:       sql(",
          "299:         s\"\"\"DELETE FROM $tableNameAsString",
          "300:            |WHERE",
          "301:            | id NOT IN (SELECT * FROM deleted_id WHERE value IS NOT NULL)",
          "302:            | AND",
          "303:            | EXISTS (SELECT 1 FROM FROM deleted_dep WHERE dep = deleted_dep.value)",
          "304:            |\"\"\".stripMargin)",
          "306:       checkAnswer(sql(s\"SELECT * FROM $tableNameAsString\"), Row(2, \"hardware\") :: Nil)",
          "308:       sql(",
          "309:         s\"\"\"DELETE FROM $tableNameAsString t",
          "310:            |WHERE",
          "311:            | t.id NOT IN (SELECT * FROM deleted_id WHERE value IS NOT NULL)",
          "312:            | OR",
          "313:            | EXISTS (SELECT 1 FROM FROM deleted_dep WHERE t.dep = deleted_dep.value)",
          "314:            |\"\"\".stripMargin)",
          "316:       checkAnswer(sql(s\"SELECT * FROM $tableNameAsString\"), Nil)",
          "317:     }",
          "318:   }",
          "320:   test(\"delete with EXISTS subquery\") {",
          "321:     withTempView(\"deleted_id\", \"deleted_dep\") {",
          "322:       createAndInitTable(\"id INT, dep STRING\",",
          "323:         \"\"\"{ \"id\": 1, \"dep\": \"hr\" }",
          "324:           |{ \"id\": 2, \"dep\": \"hardware\" }",
          "325:           |{ \"id\": null, \"dep\": \"hr\" }",
          "326:           |\"\"\".stripMargin)",
          "328:       val deletedIdDF = Seq(Some(-1), Some(-2), None).toDF()",
          "329:       deletedIdDF.createOrReplaceTempView(\"deleted_id\")",
          "331:       val deletedDepDF = Seq(\"software\", \"hr\").toDF()",
          "332:       deletedDepDF.createOrReplaceTempView(\"deleted_dep\")",
          "334:       sql(",
          "335:         s\"\"\"DELETE FROM $tableNameAsString t",
          "336:            |WHERE",
          "337:            | EXISTS (SELECT 1 FROM deleted_id d WHERE t.id = d.value)",
          "338:            |\"\"\".stripMargin)",
          "340:       checkAnswer(",
          "341:         sql(s\"SELECT * FROM $tableNameAsString\"),",
          "342:         Row(1, \"hr\") :: Row(2, \"hardware\") :: Row(null, \"hr\") :: Nil)",
          "344:       sql(",
          "345:         s\"\"\"DELETE FROM $tableNameAsString t",
          "346:            |WHERE",
          "347:            | EXISTS (SELECT 1 FROM deleted_id d WHERE t.id = d.value + 2)",
          "348:            |\"\"\".stripMargin)",
          "350:       checkAnswer(",
          "351:         sql(s\"SELECT * FROM $tableNameAsString\"),",
          "352:         Row(2, \"hardware\") :: Row(null, \"hr\") :: Nil)",
          "354:       sql(",
          "355:         s\"\"\"DELETE FROM $tableNameAsString t",
          "356:            |WHERE",
          "357:            | EXISTS (SELECT 1 FROM deleted_id d WHERE t.id = d.value) OR t.id IS NULL",
          "358:            |\"\"\".stripMargin)",
          "360:       checkAnswer(",
          "361:         sql(s\"SELECT * FROM $tableNameAsString\"),",
          "362:         Row(2, \"hardware\") :: Nil)",
          "364:       sql(",
          "365:         s\"\"\"DELETE FROM $tableNameAsString t",
          "366:            |WHERE",
          "367:            | EXISTS (SELECT 1 FROM deleted_id di WHERE t.id = di.value)",
          "368:            | AND",
          "369:            | EXISTS (SELECT 1 FROM deleted_dep dd WHERE t.dep = dd.value)",
          "370:            |\"\"\".stripMargin)",
          "372:       checkAnswer(",
          "373:         sql(s\"SELECT * FROM $tableNameAsString\"),",
          "374:         Row(2, \"hardware\") :: Nil)",
          "375:     }",
          "376:   }",
          "378:   test(\"delete with NOT EXISTS subquery\") {",
          "379:     withTempView(\"deleted_id\", \"deleted_dep\") {",
          "380:       createAndInitTable(\"id INT, dep STRING\",",
          "381:         \"\"\"{ \"id\": 1, \"dep\": \"hr\" }",
          "382:           |{ \"id\": 2, \"dep\": \"hardware\" }",
          "383:           |{ \"id\": null, \"dep\": \"hr\" }",
          "384:           |\"\"\".stripMargin)",
          "386:       val deletedIdDF = Seq(Some(-1), Some(-2), None).toDF()",
          "387:       deletedIdDF.createOrReplaceTempView(\"deleted_id\")",
          "389:       val deletedDepDF = Seq(\"software\", \"hr\").toDF()",
          "390:       deletedDepDF.createOrReplaceTempView(\"deleted_dep\")",
          "392:       sql(",
          "393:         s\"\"\"DELETE FROM $tableNameAsString t",
          "394:            |WHERE",
          "395:            | NOT EXISTS (SELECT 1 FROM deleted_id di WHERE t.id = di.value + 2)",
          "396:            | AND",
          "397:            | NOT EXISTS (SELECT 1 FROM deleted_dep dd WHERE t.dep = dd.value)",
          "398:            |\"\"\".stripMargin)",
          "400:       checkAnswer(",
          "401:         sql(s\"SELECT * FROM $tableNameAsString\"),",
          "402:         Row(1, \"hr\") :: Row(null, \"hr\") :: Nil)",
          "404:       sql(",
          "405:         s\"\"\"DELETE FROM $tableNameAsString t",
          "406:            |WHERE",
          "407:            | NOT EXISTS (SELECT 1 FROM deleted_id d WHERE t.id = d.value + 2)",
          "408:            |\"\"\".stripMargin)",
          "410:       checkAnswer(sql(s\"SELECT * FROM $tableNameAsString\"), Row(1, \"hr\") :: Nil)",
          "412:       sql(",
          "413:         s\"\"\"DELETE FROM $tableNameAsString t",
          "414:            |WHERE",
          "415:            | NOT EXISTS (SELECT 1 FROM deleted_id d WHERE t.id = d.value + 2)",
          "416:            | OR",
          "417:            | t.id = 1",
          "418:            |\"\"\".stripMargin)",
          "420:       checkAnswer(sql(s\"SELECT * FROM $tableNameAsString\"), Nil)",
          "421:     }",
          "422:   }",
          "424:   test(\"delete with a scalar subquery\") {",
          "425:     withTempView(\"deleted_id\") {",
          "426:       createAndInitTable(\"id INT, dep STRING\",",
          "427:         \"\"\"{ \"id\": 1, \"dep\": \"hr\" }",
          "428:           |{ \"id\": 2, \"dep\": \"hardware\" }",
          "429:           |{ \"id\": null, \"dep\": \"hr\" }",
          "430:           |\"\"\".stripMargin)",
          "432:       val deletedIdDF = Seq(Some(1), Some(100), None).toDF()",
          "433:       deletedIdDF.createOrReplaceTempView(\"deleted_id\")",
          "435:       sql(",
          "436:         s\"\"\"DELETE FROM $tableNameAsString t",
          "437:            |WHERE",
          "438:            | id <= (SELECT min(value) FROM deleted_id)",
          "439:            |\"\"\".stripMargin)",
          "441:       checkAnswer(",
          "442:         sql(s\"SELECT * FROM $tableNameAsString\"),",
          "443:         Row(2, \"hardware\") :: Row(null, \"hr\") :: Nil)",
          "444:     }",
          "445:   }",
          "447:   test(\"delete refreshes relation cache\") {",
          "448:     withTempView(\"temp\") {",
          "449:       withCache(\"temp\") {",
          "450:         createAndInitTable(\"id INT, dep STRING\",",
          "451:           \"\"\"{ \"id\": 1, \"dep\": \"hr\" }",
          "452:             |{ \"id\": 1, \"dep\": \"hardware\" }",
          "453:             |{ \"id\": 2, \"dep\": \"hardware\" }",
          "454:             |{ \"id\": 3, \"dep\": \"hr\" }",
          "455:             |\"\"\".stripMargin)",
          "458:         val query = sql(s\"SELECT * FROM $tableNameAsString WHERE id = 1\")",
          "459:         query.createOrReplaceTempView(\"temp\")",
          "462:         sql(\"CACHE TABLE temp\")",
          "465:         checkAnswer(",
          "466:           sql(\"SELECT * FROM temp\"),",
          "467:           Row(1, \"hr\") :: Row(1, \"hardware\") :: Nil)",
          "470:         sql(s\"DELETE FROM $tableNameAsString WHERE id <= 1\")",
          "473:         checkAnswer(",
          "474:           sql(s\"SELECT * FROM $tableNameAsString\"),",
          "475:           Row(2, \"hardware\") :: Row(3, \"hr\") :: Nil)",
          "478:         checkAnswer(sql(\"SELECT * FROM temp\"), Nil)",
          "479:       }",
          "480:     }",
          "481:   }",
          "483:   test(\"delete with nondeterministic conditions\") {",
          "484:     createAndInitTable(\"id INT, dep STRING\",",
          "485:       \"\"\"{ \"id\": 1, \"dep\": \"hr\" }",
          "486:         |{ \"id\": 2, \"dep\": \"software\" }",
          "487:         |{ \"id\": 3, \"dep\": \"hr\" }",
          "488:         |\"\"\".stripMargin)",
          "490:     val e = intercept[AnalysisException] {",
          "491:       sql(s\"DELETE FROM $tableNameAsString WHERE id <= 1 AND rand() > 0.5\")",
          "492:     }",
          "493:     assert(e.message.contains(\"nondeterministic expressions are only allowed\"))",
          "494:   }",
          "496:   test(\"delete without condition executed as delete with filters\") {",
          "497:     createAndInitTable(\"id INT, dep INT\",",
          "498:       \"\"\"{ \"id\": 1, \"dep\": 100 }",
          "499:         |{ \"id\": 2, \"dep\": 200 }",
          "500:         |{ \"id\": 3, \"dep\": 100 }",
          "501:         |\"\"\".stripMargin)",
          "503:     executeDeleteWithFilters(s\"DELETE FROM $tableNameAsString\")",
          "505:     checkAnswer(sql(s\"SELECT * FROM $tableNameAsString\"), Nil)",
          "506:   }",
          "508:   test(\"delete with supported predicates gets converted into delete with filters\") {",
          "509:     createAndInitTable(\"id INT, dep INT\",",
          "510:       \"\"\"{ \"id\": 1, \"dep\": 100 }",
          "511:         |{ \"id\": 2, \"dep\": 200 }",
          "512:         |{ \"id\": 3, \"dep\": 100 }",
          "513:         |\"\"\".stripMargin)",
          "515:     executeDeleteWithFilters(s\"DELETE FROM $tableNameAsString WHERE dep = 100\")",
          "517:     checkAnswer(",
          "518:       sql(s\"SELECT * FROM $tableNameAsString\"),",
          "519:       Row(2, 200) :: Nil)",
          "520:   }",
          "522:   test(\"delete with unsupported predicates cannot be converted into delete with filters\") {",
          "523:     createAndInitTable(\"id INT, dep INT\",",
          "524:       \"\"\"{ \"id\": 1, \"dep\": 100 }",
          "525:         |{ \"id\": 2, \"dep\": 200 }",
          "526:         |{ \"id\": 3, \"dep\": 100 }",
          "527:         |\"\"\".stripMargin)",
          "529:     executeDeleteWithRewrite(s\"DELETE FROM $tableNameAsString WHERE dep = 100 OR dep < 200\")",
          "531:     checkAnswer(",
          "532:       sql(s\"SELECT * FROM $tableNameAsString\"),",
          "533:       Row(2, 200) :: Nil)",
          "534:   }",
          "536:   test(\"delete with subquery cannot be converted into delete with filters\") {",
          "537:     withTempView(\"deleted_id\") {",
          "538:       createAndInitTable(\"id INT, dep INT\",",
          "539:         \"\"\"{ \"id\": 1, \"dep\": 100 }",
          "540:           |{ \"id\": 2, \"dep\": 200 }",
          "541:           |{ \"id\": 3, \"dep\": 100 }",
          "542:           |\"\"\".stripMargin)",
          "544:       val deletedIdDF = Seq(Some(1), Some(100), None).toDF()",
          "545:       deletedIdDF.createOrReplaceTempView(\"deleted_id\")",
          "547:       val q = s\"DELETE FROM $tableNameAsString WHERE dep = 100 AND id IN (SELECT * FROM deleted_id)\"",
          "548:       executeDeleteWithRewrite(q)",
          "550:       checkAnswer(",
          "551:         sql(s\"SELECT * FROM $tableNameAsString\"),",
          "552:         Row(2, 200) :: Row(3, 100) :: Nil)",
          "553:     }",
          "554:   }",
          "556:   private def createTable(schemaString: String): Unit = {",
          "557:     val schema = StructType.fromDDL(schemaString)",
          "558:     val tableProps = Collections.emptyMap[String, String]",
          "559:     catalog.createTable(ident, schema, Array(identity(reference(Seq(\"dep\")))), tableProps)",
          "560:   }",
          "562:   private def createAndInitTable(schemaString: String, jsonData: String): Unit = {",
          "563:     createTable(schemaString)",
          "564:     append(schemaString, jsonData)",
          "565:   }",
          "567:   private def append(schemaString: String, jsonData: String): Unit = {",
          "568:     val df = toDF(jsonData, schemaString)",
          "569:     df.coalesce(1).writeTo(tableNameAsString).append()",
          "570:   }",
          "572:   private def toDF(jsonData: String, schemaString: String = null): DataFrame = {",
          "573:     val jsonRows = jsonData.split(\"\\\\n\").filter(str => str.trim.nonEmpty)",
          "574:     val jsonDS = spark.createDataset(jsonRows)(Encoders.STRING)",
          "575:     if (schemaString == null) {",
          "576:       spark.read.json(jsonDS)",
          "577:     } else {",
          "578:       spark.read.schema(schemaString).json(jsonDS)",
          "579:     }",
          "580:   }",
          "582:   private def executeDeleteWithFilters(query: String): Unit = {",
          "583:     val executedPlan = executeAndKeepPlan {",
          "584:       sql(query)",
          "585:     }",
          "587:     executedPlan match {",
          "588:       case _: DeleteFromTableExec =>",
          "590:       case other =>",
          "591:         fail(\"unexpected executed plan: \" + other)",
          "592:     }",
          "593:   }",
          "595:   private def executeDeleteWithRewrite(query: String): Unit = {",
          "596:     val executedPlan = executeAndKeepPlan {",
          "597:       sql(query)",
          "598:     }",
          "600:     executedPlan match {",
          "601:       case _: ReplaceDataExec =>",
          "603:       case other =>",
          "604:         fail(\"unexpected executed plan: \" + other)",
          "605:     }",
          "606:   }",
          "609:   private def executeAndKeepPlan(func: => Unit): SparkPlan = {",
          "610:     var executedPlan: SparkPlan = null",
          "612:     val listener = new QueryExecutionListener {",
          "613:       override def onSuccess(funcName: String, qe: QueryExecution, durationNs: Long): Unit = {",
          "614:         executedPlan = qe.executedPlan",
          "615:       }",
          "616:       override def onFailure(funcName: String, qe: QueryExecution, exception: Exception): Unit = {",
          "617:       }",
          "618:     }",
          "619:     spark.listenerManager.register(listener)",
          "621:     func",
          "623:     sparkContext.listenerBus.waitUntilEmpty()",
          "625:     stripAQEPlan(executedPlan)",
          "626:   }",
          "627: }",
          "629: class GroupBasedDeleteFromTableSuite extends DeleteFromTableSuiteBase",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/execution/command/PlanResolutionSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/command/PlanResolutionSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/execution/command/PlanResolutionSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/execution/command/PlanResolutionSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "34: import org.apache.spark.sql.catalyst.plans.logical.{AlterColumn, AnalysisOnlyCommand, AppendData, Assignment, CreateTable, CreateTableAsSelect, DeleteAction, DeleteFromTable, DescribeRelation, DropTable, InsertAction, LocalRelation, LogicalPlan, MergeIntoTable, OneRowRelation, Project, SetTableLocation, SetTableProperties, ShowTableProperties, SubqueryAlias, UnsetTableProperties, UpdateAction, UpdateTable}",
          "35: import org.apache.spark.sql.catalyst.rules.Rule",
          "36: import org.apache.spark.sql.connector.FakeV2Provider",
          "38: import org.apache.spark.sql.connector.expressions.Transform",
          "39: import org.apache.spark.sql.execution.datasources.{CreateTable => CreateTableV1}",
          "40: import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation",
          "",
          "[Removed Lines]",
          "37: import org.apache.spark.sql.connector.catalog.{CatalogManager, CatalogNotFoundException, Identifier, Table, TableCapability, TableCatalog, V1Table}",
          "",
          "[Added Lines]",
          "37: import org.apache.spark.sql.connector.catalog.{CatalogManager, CatalogNotFoundException, Identifier, SupportsDelete, Table, TableCapability, TableCatalog, V1Table}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "49:   private val v2Format = classOf[FakeV2Provider].getName",
          "51:   private val table: Table = {",
          "53:     when(t.schema()).thenReturn(new StructType().add(\"i\", \"int\").add(\"s\", \"string\"))",
          "54:     when(t.partitioning()).thenReturn(Array.empty[Transform])",
          "55:     t",
          "",
          "[Removed Lines]",
          "52:     val t = mock(classOf[Table])",
          "",
          "[Added Lines]",
          "52:     val t = mock(classOf[SupportsDelete])",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "aa53fcad4fc1622c18b14d22fc909f7b349f7931",
      "candidate_info": {
        "commit_hash": "aa53fcad4fc1622c18b14d22fc909f7b349f7931",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/aa53fcad4fc1622c18b14d22fc909f7b349f7931",
        "files": [
          "sql/core/src/test/scala/org/apache/spark/sql/TPCDSQueryTestSuite.scala"
        ],
        "message": "[SPARK-39856][SQL][TESTS][FOLLOW-UP] Increase the number of partitions in TPC-DS build to avoid out-of-memory\n\n### What changes were proposed in this pull request?\n\nThis PR increases the number of partitions further more (see also https://github.com/apache/spark/pull/37270)\n\n### Why are the changes needed?\n\nTo make the build pass.\n\n### Does this PR introduce _any_ user-facing change?\nNo, test and dev-only.\n\n### How was this patch tested?\n\nIt's tested in https://github.com/LuciferYang/spark/runs/7497163716?check_suite_focus=true\n\nCloses #37273 from LuciferYang/SPARK-39856-FOLLOWUP.\n\nAuthored-by: yangjie01 <yangjie01@baidu.com>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>",
        "before_after_code_files": [
          "sql/core/src/test/scala/org/apache/spark/sql/TPCDSQueryTestSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/TPCDSQueryTestSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/core/src/test/scala/org/apache/spark/sql/TPCDSQueryTestSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/TPCDSQueryTestSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/TPCDSQueryTestSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/TPCDSQueryTestSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "64:   override protected def sparkConf: SparkConf = super.sparkConf",
          "67:   protected override def createSparkSession: TestSparkSession = {",
          "68:     new TestSparkSession(new SparkContext(\"local[1]\", this.getClass.getSimpleName, sparkConf))",
          "",
          "[Removed Lines]",
          "65:     .set(SQLConf.SHUFFLE_PARTITIONS.key, 4.toString)",
          "",
          "[Added Lines]",
          "65:     .set(SQLConf.SHUFFLE_PARTITIONS.key, 16.toString)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "21d9db39b7ac0aae07b83fb9dba2639a4daffadc",
      "candidate_info": {
        "commit_hash": "21d9db39b7ac0aae07b83fb9dba2639a4daffadc",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/21d9db39b7ac0aae07b83fb9dba2639a4daffadc",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/RemoveRedundantAliasAndProjectSuite.scala",
          "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14a.sf100/explain.txt",
          "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14a/explain.txt",
          "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/metric/SQLMetricsSuite.scala"
        ],
        "message": "[SPARK-39887][SQL][3.3] RemoveRedundantAliases should keep aliases that make the output of projection nodes unique\n\n### What changes were proposed in this pull request?\nKeep the output attributes of a `Union` node's first child in the `RemoveRedundantAliases` rule to avoid correctness issues.\n\n### Why are the changes needed?\nTo fix the result of the following query:\n```\nSELECT a, b AS a FROM (\n  SELECT a, a AS b FROM (SELECT a FROM VALUES (1) AS t(a))\n  UNION ALL\n  SELECT a, b FROM (SELECT a, b FROM VALUES (1, 2) AS t(a, b))\n)\n```\nBefore this PR the query returns the incorrect result:\n```\n+---+---+\n|  a|  a|\n+---+---+\n|  1|  1|\n|  2|  2|\n+---+---+\n```\nAfter this PR it returns the expected result:\n```\n+---+---+\n|  a|  a|\n+---+---+\n|  1|  1|\n|  1|  2|\n+---+---+\n```\n\n### Does this PR introduce _any_ user-facing change?\nYes, fixes a correctness issue.\n\n### How was this patch tested?\nAdded new UTs.\n\nCloses #37472 from peter-toth/SPARK-39887-keep-attributes-of-unions-first-child-3.3.\n\nAuthored-by: Peter Toth <ptoth@cloudera.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/RemoveRedundantAliasAndProjectSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/RemoveRedundantAliasAndProjectSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/metric/SQLMetricsSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/metric/SQLMetricsSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "542:         })",
          "543:         Join(newLeft, newRight, joinType, newCondition, hint)",
          "545:       case _ =>",
          "547:         val currentNextAttrPairs = mutable.Buffer.empty[(Attribute, Attribute)]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "547:       case _: Union =>",
          "548:         var first = true",
          "549:         plan.mapChildren { child =>",
          "550:           if (first) {",
          "551:             first = false",
          "556:             removeRedundantAliases(child, excluded ++ child.outputSet)",
          "557:           } else {",
          "558:             removeRedundantAliases(child, excluded)",
          "559:           }",
          "560:         }",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/RemoveRedundantAliasAndProjectSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/RemoveRedundantAliasAndProjectSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/RemoveRedundantAliasAndProjectSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/RemoveRedundantAliasAndProjectSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "97:     val r2 = LocalRelation('b.int)",
          "98:     val query = r1.select('a as 'a).union(r2.select('b as 'b)).select('a).analyze",
          "99:     val optimized = Optimize.execute(query)",
          "101:     comparePlans(optimized, expected)",
          "102:   }",
          "",
          "[Removed Lines]",
          "100:     val expected = r1.union(r2)",
          "",
          "[Added Lines]",
          "100:     val expected = r1.select($\"a\" as \"a\").union(r2).analyze",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "3220:     val d1 = Seq(\"a\").toDF",
          "3221:     assert(d1.exceptAll(d1).count() === 0)",
          "3222:   }",
          "3223: }",
          "3225: case class GroupByKey(a: Int, b: Int)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "3224:   test(\"SPARK-39887: RemoveRedundantAliases should keep attributes of a Union's first child\") {",
          "3225:     val df = sql(",
          "3226:       \"\"\"",
          "3227:         |SELECT a, b AS a FROM (",
          "3228:         |  SELECT a, a AS b FROM (SELECT a FROM VALUES (1) AS t(a))",
          "3229:         |  UNION ALL",
          "3230:         |  SELECT a, b FROM (SELECT a, b FROM VALUES (1, 2) AS t(a, b))",
          "3231:         |)",
          "3232:         |\"\"\".stripMargin)",
          "3233:     val stringCols = df.logicalPlan.output.map(Column(_).cast(StringType))",
          "3234:     val castedDf = df.select(stringCols: _*)",
          "3235:     checkAnswer(castedDf, Row(\"1\", \"1\") :: Row(\"1\", \"2\") :: Nil)",
          "3236:   }",
          "3238:   test(\"SPARK-39887: RemoveRedundantAliases should keep attributes of a Union's first child 2\") {",
          "3239:     val df = sql(",
          "3240:       \"\"\"",
          "3241:         |SELECT",
          "3242:         |  to_date(a) a,",
          "3243:         |  to_date(b) b",
          "3244:         |FROM",
          "3245:         |  (",
          "3246:         |    SELECT",
          "3247:         |      a,",
          "3248:         |      a AS b",
          "3249:         |    FROM",
          "3250:         |      (",
          "3251:         |        SELECT",
          "3252:         |          to_date(a) a",
          "3253:         |        FROM",
          "3254:         |        VALUES",
          "3255:         |          ('2020-02-01') AS t1(a)",
          "3256:         |        GROUP BY",
          "3257:         |          to_date(a)",
          "3258:         |      ) t3",
          "3259:         |    UNION ALL",
          "3260:         |    SELECT",
          "3261:         |      a,",
          "3262:         |      b",
          "3263:         |    FROM",
          "3264:         |      (",
          "3265:         |        SELECT",
          "3266:         |          to_date(a) a,",
          "3267:         |          to_date(b) b",
          "3268:         |        FROM",
          "3269:         |        VALUES",
          "3270:         |          ('2020-01-01', '2020-01-02') AS t1(a, b)",
          "3271:         |        GROUP BY",
          "3272:         |          to_date(a),",
          "3273:         |          to_date(b)",
          "3274:         |      ) t4",
          "3275:         |  ) t5",
          "3276:         |GROUP BY",
          "3277:         |  to_date(a),",
          "3278:         |  to_date(b);",
          "3279:         |\"\"\".stripMargin)",
          "3280:     checkAnswer(df,",
          "3281:       Row(java.sql.Date.valueOf(\"2020-02-01\"), java.sql.Date.valueOf(\"2020-02-01\")) ::",
          "3282:         Row(java.sql.Date.valueOf(\"2020-01-01\"), java.sql.Date.valueOf(\"2020-01-02\")) :: Nil)",
          "3283:   }",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/execution/metric/SQLMetricsSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/metric/SQLMetricsSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/execution/metric/SQLMetricsSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/execution/metric/SQLMetricsSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "639:       val union = view.union(view)",
          "640:       testSparkPlanMetrics(union, 1, Map(",
          "641:         0L -> (\"Union\" -> Map()),",
          "644:     }",
          "645:   }",
          "",
          "[Removed Lines]",
          "642:         1L -> (\"LocalTableScan\" -> Map(\"number of output rows\" -> 2L)),",
          "643:         2L -> (\"LocalTableScan\" -> Map(\"number of output rows\" -> 2L))))",
          "",
          "[Added Lines]",
          "642:         1L -> (\"Project\" -> Map()),",
          "643:         2L -> (\"LocalTableScan\" -> Map(\"number of output rows\" -> 2L)),",
          "644:         3L -> (\"LocalTableScan\" -> Map(\"number of output rows\" -> 2L))))",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "e46d2e2d476e85024f1c53fdaf446fdd2e293d28",
      "candidate_info": {
        "commit_hash": "e46d2e2d476e85024f1c53fdaf446fdd2e293d28",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/e46d2e2d476e85024f1c53fdaf446fdd2e293d28",
        "files": [
          "python/pyspark/pandas/frame.py",
          "python/pyspark/pandas/tests/test_dataframe.py"
        ],
        "message": "[SPARK-40270][PS] Make 'compute.max_rows' as None working in DataFrame.style\n\nThis PR make `compute.max_rows` option as `None` working in `DataFrame.style`, as expected instead of throwing an exception., by collecting it all to a pandas DataFrame.\n\nTo make the configuration working as expected.\n\nYes.\n\n```python\nimport pyspark.pandas as ps\nps.set_option(\"compute.max_rows\", None)\nps.get_option(\"compute.max_rows\")\nps.range(1).style\n```\n\n**Before:**\n\n```\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/.../spark/python/pyspark/pandas/frame.py\", line 3656, in style\n    pdf = self.head(max_results + 1)._to_internal_pandas()\nTypeError: unsupported operand type(s) for +: 'NoneType' and 'int'\n```\n\n**After:**\n\n```\n<pandas.io.formats.style.Styler object at 0x7fdf78250430>\n```\n\nManually tested, and unittest was added.\n\nCloses #37718 from HyukjinKwon/SPARK-40270.\n\nAuthored-by: Hyukjin Kwon <gurwls223@apache.org>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>\n(cherry picked from commit 0f0e8cc26b6c80cc179368e3009d4d6c88103a64)\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>",
        "before_after_code_files": [
          "python/pyspark/pandas/frame.py||python/pyspark/pandas/frame.py",
          "python/pyspark/pandas/tests/test_dataframe.py||python/pyspark/pandas/tests/test_dataframe.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "python/pyspark/pandas/frame.py||python/pyspark/pandas/frame.py": [
          "File: python/pyspark/pandas/frame.py -> python/pyspark/pandas/frame.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "3459:         Property returning a Styler object containing methods for",
          "3460:         building a styled HTML representation for the DataFrame.",
          "3465:         Examples",
          "3466:         --------",
          "3467:         >>> ps.range(1001).style  # doctest: +SKIP",
          "3468:         <pandas.io.formats.style.Styler object at ...>",
          "3469:         \"\"\"",
          "3470:         max_results = get_option(\"compute.max_rows\")",
          "3476:     def set_index(",
          "3477:         self,",
          "",
          "[Removed Lines]",
          "3462:         .. note:: currently it collects top 1000 rows and return its",
          "3463:             pandas `pandas.io.formats.style.Styler` instance.",
          "3471:         pdf = self.head(max_results + 1)._to_internal_pandas()",
          "3472:         if len(pdf) > max_results:",
          "3473:             warnings.warn(\"'style' property will only use top %s rows.\" % max_results, UserWarning)",
          "3474:         return pdf.head(max_results).style",
          "",
          "[Added Lines]",
          "3468:         if max_results is not None:",
          "3469:             pdf = self.head(max_results + 1)._to_internal_pandas()",
          "3470:             if len(pdf) > max_results:",
          "3471:                 warnings.warn(",
          "3472:                     \"'style' property will only use top %s rows.\" % max_results, UserWarning",
          "3473:                 )",
          "3474:             return pdf.head(max_results).style",
          "3475:         else:",
          "3476:             return self._to_internal_pandas().style",
          "",
          "---------------"
        ],
        "python/pyspark/pandas/tests/test_dataframe.py||python/pyspark/pandas/tests/test_dataframe.py": [
          "File: python/pyspark/pandas/tests/test_dataframe.py -> python/pyspark/pandas/tests/test_dataframe.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "6375:         psdf = ps.from_pandas(pdf)",
          "6376:         self.assert_eq(pdf.cov(), psdf.cov())",
          "6379: if __name__ == \"__main__\":",
          "6380:     from pyspark.pandas.tests.test_dataframe import *  # noqa: F401",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "6378:     def test_style(self):",
          "6379:         # Currently, the `style` function returns a pandas object `Styler` as it is,",
          "6380:         # processing only the number of rows declared in `compute.max_rows`.",
          "6381:         # So it's a bit vague to test, but we are doing minimal tests instead of not testing at all.",
          "6382:         pdf = pd.DataFrame(np.random.randn(10, 4), columns=[\"A\", \"B\", \"C\", \"D\"])",
          "6383:         psdf = ps.from_pandas(pdf)",
          "6385:         def style_negative(v, props=\"\"):",
          "6386:             return props if v < 0 else None",
          "6388:         def check_style():",
          "6389:             # If the value is negative, the text color will be displayed as red.",
          "6390:             pdf_style = pdf.style.applymap(style_negative, props=\"color:red;\")",
          "6391:             psdf_style = psdf.style.applymap(style_negative, props=\"color:red;\")",
          "6393:             # Test whether the same shape as pandas table is created including the color.",
          "6394:             self.assert_eq(pdf_style.to_latex(), psdf_style.to_latex())",
          "6396:         check_style()",
          "6398:         with ps.option_context(\"compute.max_rows\", None):",
          "6399:             check_style()",
          "",
          "---------------"
        ]
      }
    }
  ]
}