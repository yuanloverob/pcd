{
  "cve_id": "CVE-2023-45348",
  "cve_desc": "Apache Airflow, versions 2.7.0 and 2.7.1, is affected by a vulnerability that allows an authenticated user to retrieve sensitive configuration information when the \"expose_config\" option is set to \"non-sensitive-only\". The `expose_config` option is False by default.\nIt is recommended to upgrade to a version that is not affected.",
  "repo": "apache/airflow",
  "patch_hash": "a4a0b5dd3d0ce05311c70bb9a32b66a650dbc0b4",
  "patch_info": {
    "commit_hash": "a4a0b5dd3d0ce05311c70bb9a32b66a650dbc0b4",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/a4a0b5dd3d0ce05311c70bb9a32b66a650dbc0b4",
    "files": [
      "airflow/api_connexion/endpoints/config_endpoint.py",
      "airflow/configuration.py",
      "tests/api_connexion/endpoints/test_config_endpoint.py"
    ],
    "message": "Check if the lower of provided values are sensitives in config endpoint (#34712)\n\n* Check if the lower of provided values are sensitives in config endpoint\n\n* update unit test\n\n* ensure that all values in sensitive dict are in lower characters\n\n(cherry picked from commit f044589b685855a8fce8f5376bea2564c5a001f7)",
    "before_after_code_files": [
      "airflow/api_connexion/endpoints/config_endpoint.py||airflow/api_connexion/endpoints/config_endpoint.py",
      "airflow/configuration.py||airflow/configuration.py",
      "tests/api_connexion/endpoints/test_config_endpoint.py||tests/api_connexion/endpoints/test_config_endpoint.py"
    ]
  },
  "patch_diff": {
    "airflow/api_connexion/endpoints/config_endpoint.py||airflow/api_connexion/endpoints/config_endpoint.py": [
      "File: airflow/api_connexion/endpoints/config_endpoint.py -> airflow/api_connexion/endpoints/config_endpoint.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "123:                 \"Config not found.\", detail=f\"The option [{section}/{option}] is not found in config.\"",
      "124:             )",
      "127:             value = \"< hidden >\"",
      "128:         else:",
      "129:             value = conf.get(section, option)",
      "",
      "[Removed Lines]",
      "126:         if (section, option) in conf.sensitive_config_values:",
      "",
      "[Added Lines]",
      "126:         if (section.lower(), option.lower()) in conf.sensitive_config_values:",
      "",
      "---------------"
    ],
    "airflow/configuration.py||airflow/configuration.py": [
      "File: airflow/configuration.py -> airflow/configuration.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "311:             for s, s_c in self.configuration_description.items()",
      "312:             for k, item in s_c.get(\"options\").items()  # type: ignore[union-attr]",
      "313:         }",
      "315:         depr_option = {self.deprecated_options[x][:-1] for x in sensitive if x in self.deprecated_options}",
      "316:         depr_section = {",
      "317:             (self.deprecated_sections[s][0], k) for s, k in sensitive if s in self.deprecated_sections",
      "",
      "[Removed Lines]",
      "314:         sensitive = {(section, key) for (section, key), v in flattened.items() if v.get(\"sensitive\") is True}",
      "",
      "[Added Lines]",
      "314:         sensitive = {",
      "315:             (section.lower(), key.lower())",
      "316:             for (section, key), v in flattened.items()",
      "317:             if v.get(\"sensitive\") is True",
      "318:         }",
      "",
      "---------------"
    ],
    "tests/api_connexion/endpoints/test_config_endpoint.py||tests/api_connexion/endpoints/test_config_endpoint.py": [
      "File: tests/api_connexion/endpoints/test_config_endpoint.py -> tests/api_connexion/endpoints/test_config_endpoint.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "247:         return_value=MOCK_CONF_WITH_SENSITIVE_VALUE,",
      "248:     )",
      "249:     @conf_vars({(\"webserver\", \"expose_config\"): \"non-sensitive-only\"})",
      "251:         response = self.client.get(",
      "253:             headers={\"Accept\": \"text/plain\"},",
      "254:             environ_overrides={\"REMOTE_USER\": \"test\"},",
      "255:         )",
      "256:         assert response.status_code == 200",
      "257:         expected = textwrap.dedent(",
      "261:         \"\"\"",
      "262:         )",
      "263:         assert expected == response.data.decode()",
      "",
      "[Removed Lines]",
      "250:     def test_should_respond_200_text_plain_with_non_sensitive_only(self, mock_as_dict):",
      "252:             \"/api/v1/config/section/core/option/sql_alchemy_conn\",",
      "258:             \"\"\"\\",
      "259:         [core]",
      "260:         sql_alchemy_conn = < hidden >",
      "",
      "[Added Lines]",
      "250:     @pytest.mark.parametrize(",
      "251:         \"section, option\",",
      "252:         [",
      "253:             (\"core\", \"sql_alchemy_conn\"),",
      "254:             (\"core\", \"SQL_ALCHEMY_CONN\"),",
      "255:             (\"corE\", \"sql_alchemy_conn\"),",
      "256:             (\"CORE\", \"sql_alchemy_conn\"),",
      "257:         ],",
      "258:     )",
      "259:     def test_should_respond_200_text_plain_with_non_sensitive_only(self, mock_as_dict, section, option):",
      "261:             f\"/api/v1/config/section/{section}/option/{option}\",",
      "267:             f\"\"\"\\",
      "268:         [{section}]",
      "269:         {option} = < hidden >",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "cff99284e8f9bccad6e2e3be024d98996ac639c6",
      "candidate_info": {
        "commit_hash": "cff99284e8f9bccad6e2e3be024d98996ac639c6",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/cff99284e8f9bccad6e2e3be024d98996ac639c6",
        "files": [
          "airflow/utils/dag_edges.py",
          "airflow/utils/db.py",
          "airflow/utils/db_cleanup.py",
          "airflow/utils/email.py",
          "airflow/utils/file.py",
          "airflow/utils/log/file_task_handler.py",
          "airflow/utils/log/secrets_masker.py"
        ],
        "message": "Refactor unneeded 'continue' jumps in utils (#33836)\n\n(cherry picked from commit ca4cd3b2eceaaaa870dd3d3911217e0ed2060e2f)",
        "before_after_code_files": [
          "airflow/utils/dag_edges.py||airflow/utils/dag_edges.py",
          "airflow/utils/db.py||airflow/utils/db.py",
          "airflow/utils/db_cleanup.py||airflow/utils/db_cleanup.py",
          "airflow/utils/email.py||airflow/utils/email.py",
          "airflow/utils/file.py||airflow/utils/file.py",
          "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py",
          "airflow/utils/log/secrets_masker.py||airflow/utils/log/secrets_masker.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/utils/dag_edges.py||airflow/utils/dag_edges.py": [
          "File: airflow/utils/dag_edges.py -> airflow/utils/dag_edges.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "111:                 edge = (task.task_id, child.task_id)",
          "112:                 if task.is_setup and child.is_teardown:",
          "113:                     setup_teardown_edges.add(edge)",
          "118:         tasks_to_trace = tasks_to_trace_next",
          "120:     result = []",
          "",
          "[Removed Lines]",
          "114:                 if edge in edges:",
          "115:                     continue",
          "116:                 edges.add(edge)",
          "117:                 tasks_to_trace_next.append(child)",
          "",
          "[Added Lines]",
          "114:                 if edge not in edges:",
          "115:                     edges.add(edge)",
          "116:                     tasks_to_trace_next.append(child)",
          "",
          "---------------"
        ],
        "airflow/utils/db.py||airflow/utils/db.py": [
          "File: airflow/utils/db.py -> airflow/utils/db.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1447:         dangling_table_name = _format_airflow_moved_table_name(source_table.name, change_version, \"dangling\")",
          "1448:         if dangling_table_name in existing_table_names:",
          "1449:             invalid_row_count = bad_rows_query.count()",
          "1453:                 yield _format_dangling_error(",
          "1454:                     source_table=source_table.name,",
          "1455:                     target_table=dangling_table_name,",
          "",
          "[Removed Lines]",
          "1450:             if invalid_row_count <= 0:",
          "1451:                 continue",
          "1452:             else:",
          "",
          "[Added Lines]",
          "1450:             if invalid_row_count:",
          "",
          "---------------"
        ],
        "airflow/utils/db_cleanup.py||airflow/utils/db_cleanup.py": [
          "File: airflow/utils/db_cleanup.py -> airflow/utils/db_cleanup.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "431:         _confirm_delete(date=clean_before_timestamp, tables=sorted(effective_table_names))",
          "432:     existing_tables = reflect_tables(tables=None, session=session).tables",
          "433:     for table_name, table_config in effective_config_dict.items():",
          "435:             logger.warning(\"Table %s not found.  Skipping.\", table_name)",
          "449: @provide_session",
          "",
          "[Removed Lines]",
          "434:         if table_name not in existing_tables:",
          "436:             continue",
          "437:         with _suppress_with_logging(table_name, session):",
          "438:             _cleanup_table(",
          "439:                 clean_before_timestamp=clean_before_timestamp,",
          "440:                 dry_run=dry_run,",
          "441:                 verbose=verbose,",
          "443:                 skip_archive=skip_archive,",
          "444:                 session=session,",
          "445:             )",
          "446:             session.commit()",
          "",
          "[Added Lines]",
          "434:         if table_name in existing_tables:",
          "435:             with _suppress_with_logging(table_name, session):",
          "436:                 _cleanup_table(",
          "437:                     clean_before_timestamp=clean_before_timestamp,",
          "438:                     dry_run=dry_run,",
          "439:                     verbose=verbose,",
          "441:                     skip_archive=skip_archive,",
          "442:                     session=session,",
          "443:                 )",
          "444:                 session.commit()",
          "445:         else:",
          "",
          "---------------"
        ],
        "airflow/utils/email.py||airflow/utils/email.py": [
          "File: airflow/utils/email.py -> airflow/utils/email.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "271:             try:",
          "272:                 smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)",
          "273:             except smtplib.SMTPServerDisconnected:",
          "288: def get_email_address_list(addresses: str | Iterable[str]) -> list[str]:",
          "",
          "[Removed Lines]",
          "274:                 if attempt < smtp_retry_limit:",
          "275:                     continue",
          "276:                 raise",
          "278:             if smtp_starttls:",
          "279:                 smtp_conn.starttls()",
          "280:             if smtp_user and smtp_password:",
          "281:                 smtp_conn.login(smtp_user, smtp_password)",
          "282:             log.info(\"Sent an alert email to %s\", e_to)",
          "283:             smtp_conn.sendmail(e_from, e_to, mime_msg.as_string())",
          "284:             smtp_conn.quit()",
          "285:             break",
          "",
          "[Added Lines]",
          "274:                 if attempt == smtp_retry_limit:",
          "275:                     raise",
          "276:             else:",
          "277:                 if smtp_starttls:",
          "278:                     smtp_conn.starttls()",
          "279:                 if smtp_user and smtp_password:",
          "280:                     smtp_conn.login(smtp_user, smtp_password)",
          "281:                 log.info(\"Sent an alert email to %s\", e_to)",
          "282:                 smtp_conn.sendmail(e_from, e_to, mime_msg.as_string())",
          "283:                 smtp_conn.quit()",
          "284:                 break",
          "",
          "---------------"
        ],
        "airflow/utils/file.py||airflow/utils/file.py": [
          "File: airflow/utils/file.py -> airflow/utils/file.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "244:             patterns_by_dir.update({dirpath: patterns.copy()})",
          "246:         for file in files:",
          "255: def find_path_from_directory(",
          "",
          "[Removed Lines]",
          "247:             if file == ignore_file_name:",
          "248:                 continue",
          "249:             abs_file_path = Path(root) / file",
          "250:             if ignore_rule_type.match(abs_file_path, patterns):",
          "251:                 continue",
          "252:             yield str(abs_file_path)",
          "",
          "[Added Lines]",
          "247:             if file != ignore_file_name:",
          "248:                 abs_file_path = Path(root) / file",
          "249:                 if not ignore_rule_type.match(abs_file_path, patterns):",
          "250:                     yield str(abs_file_path)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "310:     file_paths = []",
          "312:     for file_path in find_path_from_directory(directory, \".airflowignore\"):",
          "313:         try:",
          "323:         except Exception:",
          "324:             log.exception(\"Error while examining %s\", file_path)",
          "",
          "[Removed Lines]",
          "314:             if not os.path.isfile(file_path):",
          "315:                 continue",
          "316:             _, file_ext = os.path.splitext(os.path.split(file_path)[-1])",
          "317:             if file_ext != \".py\" and not zipfile.is_zipfile(file_path):",
          "318:                 continue",
          "319:             if not might_contain_dag(file_path, safe_mode):",
          "320:                 continue",
          "322:             file_paths.append(file_path)",
          "",
          "[Added Lines]",
          "311:         path = Path(file_path)",
          "313:             if path.is_file() and (path.suffix == \".py\" or zipfile.is_zipfile(path)):",
          "314:                 if might_contain_dag(file_path, safe_mode):",
          "315:                     file_paths.append(file_path)",
          "",
          "---------------"
        ],
        "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py": [
          "File: airflow/utils/log/file_task_handler.py -> airflow/utils/log/file_task_handler.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "109:     timestamp = None",
          "110:     next_timestamp = None",
          "111:     for idx, line in enumerate(lines):",
          "122: def _interleave_logs(*logs):",
          "",
          "[Removed Lines]",
          "112:         if not line:",
          "113:             continue",
          "114:         with suppress(Exception):",
          "115:             # next_timestamp unchanged if line can't be parsed",
          "116:             next_timestamp = _parse_timestamp(line)",
          "117:         if next_timestamp:",
          "118:             timestamp = next_timestamp",
          "119:         yield timestamp, idx, line",
          "",
          "[Added Lines]",
          "112:         if line:",
          "113:             with suppress(Exception):",
          "114:                 # next_timestamp unchanged if line can't be parsed",
          "115:                 next_timestamp = _parse_timestamp(line)",
          "116:             if next_timestamp:",
          "117:                 timestamp = next_timestamp",
          "118:             yield timestamp, idx, line",
          "",
          "---------------"
        ],
        "airflow/utils/log/secrets_masker.py||airflow/utils/log/secrets_masker.py": [
          "File: airflow/utils/log/secrets_masker.py -> airflow/utils/log/secrets_masker.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "206:         if self.replacer:",
          "207:             for k, v in record.__dict__.items():",
          "211:             if record.exc_info and record.exc_info[1] is not None:",
          "212:                 exc = record.exc_info[1]",
          "213:                 self._redact_exception_with_context(exc)",
          "",
          "[Removed Lines]",
          "208:                 if k in self._record_attrs_to_ignore:",
          "209:                     continue",
          "210:                 record.__dict__[k] = self.redact(v)",
          "",
          "[Added Lines]",
          "208:                 if k not in self._record_attrs_to_ignore:",
          "209:                     record.__dict__[k] = self.redact(v)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "4595aaf97ab5bbdfac755fa4dcc7f73b6d4f7c99",
      "candidate_info": {
        "commit_hash": "4595aaf97ab5bbdfac755fa4dcc7f73b6d4f7c99",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/4595aaf97ab5bbdfac755fa4dcc7f73b6d4f7c99",
        "files": [
          "airflow/models/dag.py"
        ],
        "message": "Make param validation consistent for DAG validation and triggering (#34248)\n\n(cherry picked from commit 3e340797ab98a06b51b2930610b0abb0ad20a750)",
        "before_after_code_files": [
          "airflow/models/dag.py||airflow/models/dag.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/models/dag.py||airflow/models/dag.py": [
          "File: airflow/models/dag.py -> airflow/models/dag.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "87:     AirflowSkipException,",
          "88:     DuplicateTaskIdFound,",
          "89:     FailStopDagInvalidTriggerRule,",
          "90:     RemovedInAirflow3Warning,",
          "91:     TaskNotFound,",
          "92: )",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "90:     ParamValidationError,",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "3276:     def validate_schedule_and_params(self):",
          "3277:         \"\"\"",
          "3282:         \"\"\"",
          "3283:         if not self.timetable.can_be_scheduled:",
          "3284:             return",
          "3293:     def iter_invalid_owner_links(self) -> Iterator[tuple[str, str]]:",
          "3294:         \"\"\"",
          "",
          "[Removed Lines]",
          "3278:         Validate Param values when the schedule_interval is not None.",
          "3280:         Raise exception if there are any Params in the DAG which neither have a default value nor",
          "3281:         have the null in schema['type'] list, but the DAG have a schedule_interval which is not None.",
          "3286:         for v in self.params.values():",
          "3287:             # As type can be an array, we would check if `null` is an allowed type or not",
          "3288:             if not v.has_value and (\"type\" not in v.schema or \"null\" not in v.schema[\"type\"]):",
          "3289:                 raise AirflowException(",
          "3290:                     \"DAG Schedule must be None, if there are any required params without default values\"",
          "3291:                 )",
          "",
          "[Added Lines]",
          "3279:         Validate Param values when the DAG has schedule defined.",
          "3281:         Raise exception if there are any Params which can not be resolved by their schema definition.",
          "3286:         try:",
          "3287:             self.params.validate()",
          "3288:         except ParamValidationError as pverr:",
          "3289:             raise AirflowException(",
          "3290:                 \"DAG is not allowed to define a Schedule, \"",
          "3291:                 \"if there are any required params without default values or default values are not valid.\"",
          "3292:             ) from pverr",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "c20fc8a2a78ba8c4fd548973af800461b8fae7f8",
      "candidate_info": {
        "commit_hash": "c20fc8a2a78ba8c4fd548973af800461b8fae7f8",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/c20fc8a2a78ba8c4fd548973af800461b8fae7f8",
        "files": [
          "dev/breeze/src/airflow_breeze/utils/docker_command_utils.py",
          "dev/breeze/tests/test_docker_command_utils.py"
        ],
        "message": "Fix autodetect_docker_context for list of dict case (#34779)\n\n* Fix autodetect_docker_context for list of dict case\n\n* Handle the case of dict\n\n(cherry picked from commit 5c2dc53bcb17ae515f9565c41d15cc6d1693382c)",
        "before_after_code_files": [
          "dev/breeze/src/airflow_breeze/utils/docker_command_utils.py||dev/breeze/src/airflow_breeze/utils/docker_command_utils.py",
          "dev/breeze/tests/test_docker_command_utils.py||dev/breeze/tests/test_docker_command_utils.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/breeze/src/airflow_breeze/utils/docker_command_utils.py||dev/breeze/src/airflow_breeze/utils/docker_command_utils.py": [
          "File: dev/breeze/src/airflow_breeze/utils/docker_command_utils.py -> dev/breeze/src/airflow_breeze/utils/docker_command_utils.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "826:     if result.returncode != 0:",
          "827:         get_console().print(\"[warning]Could not detect docker builder. Using default.[/]\")",
          "828:         return \"default\"",
          "830:     known_contexts = {info[\"Name\"]: info for info in context_dicts}",
          "831:     if not known_contexts:",
          "832:         get_console().print(\"[warning]Could not detect docker builder. Using default.[/]\")",
          "",
          "[Removed Lines]",
          "829:     context_dicts = (json.loads(line) for line in result.stdout.splitlines() if line.strip())",
          "",
          "[Added Lines]",
          "829:     try:",
          "830:         context_dicts = json.loads(result.stdout)",
          "831:         if isinstance(context_dicts, dict):",
          "832:             context_dicts = [context_dicts]",
          "833:     except json.decoder.JSONDecodeError:",
          "834:         context_dicts = (json.loads(line) for line in result.stdout.splitlines() if line.strip())",
          "",
          "---------------"
        ],
        "dev/breeze/tests/test_docker_command_utils.py||dev/breeze/tests/test_docker_command_utils.py": [
          "File: dev/breeze/tests/test_docker_command_utils.py -> dev/breeze/tests/test_docker_command_utils.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "228:             \"desktop-linux\",",
          "229:             \"[info]Using desktop-linux as context\",",
          "230:         ),",
          "231:     ],",
          "232: )",
          "233: def test_autodetect_docker_context(context_output: str, selected_context: str, console_output: str):",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "231:         (",
          "232:             _fake_ctx_output(\"a\", \"default\", \"desktop-linux\"),",
          "233:             \"desktop-linux\",",
          "234:             \"[info]Using desktop-linux as context\",",
          "235:         ),",
          "236:         (",
          "237:             '[{\"Name\": \"desktop-linux\", \"DockerEndpoint\": \"unix://desktop-linux\"}]',",
          "238:             \"desktop-linux\",",
          "239:             \"[info]Using desktop-linux as context\",",
          "240:         ),",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "2b717a827477d83de16f6a1437a1682b0787e3ed",
      "candidate_info": {
        "commit_hash": "2b717a827477d83de16f6a1437a1682b0787e3ed",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/2b717a827477d83de16f6a1437a1682b0787e3ed",
        "files": [
          "airflow/metrics/validators.py",
          "tests/core/test_otel_logger.py"
        ],
        "message": "Add more exemptions to lengthy metric list (#34531)\n\nCo-authored-by: Saurabh Kumar <mail@sa1.me>\n(cherry picked from commit fa6ca5d5316a9bd759a702e1688a69b19e4e63bc)",
        "before_after_code_files": [
          "airflow/metrics/validators.py||airflow/metrics/validators.py",
          "tests/core/test_otel_logger.py||tests/core/test_otel_logger.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/metrics/validators.py||airflow/metrics/validators.py": [
          "File: airflow/metrics/validators.py -> airflow/metrics/validators.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "70:     r\"^pool\\.open_slots\\.(?P<pool_name>.*)$\",",
          "71:     r\"^pool\\.queued_slots\\.(?P<pool_name>.*)$\",",
          "72:     r\"^pool\\.running_slots\\.(?P<pool_name>.*)$\",",
          "73:     r\"^pool\\.starving_tasks\\.(?P<pool_name>.*)$\",",
          "74:     r\"^dagrun\\.dependency-check\\.(?P<dag_id>.*)$\",",
          "75:     r\"^dag\\.(?P<dag_id>.*)\\.(?P<task_id>.*)\\.duration$\",",
          "76:     r\"^dag_processing\\.last_duration\\.(?P<dag_file>.*)$\",",
          "77:     r\"^dagrun\\.duration\\.success\\.(?P<dag_id>.*)$\",",
          "78:     r\"^dagrun\\.duration\\.failed\\.(?P<dag_id>.*)$\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "73:     r\"^pool\\.deferred_slots\\.(?P<pool_name>.*)$\",",
          "77:     r\"^dag\\.(?P<dag_id>.*)\\.(?P<task_id>.*)\\.queued_duration$\",",
          "78:     r\"^dag\\.(?P<dag_id>.*)\\.(?P<task_id>.*)\\.scheduled_duration$\",",
          "",
          "---------------"
        ],
        "tests/core/test_otel_logger.py||tests/core/test_otel_logger.py": [
          "File: tests/core/test_otel_logger.py -> tests/core/test_otel_logger.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "66:         assert not _is_up_down_counter(\"this_is_not_a_udc\")",
          "68:     def test_exemption_list_has_not_grown(self):",
          "70:             \"This test exists solely to ensure that nobody is adding names to the exemption list. \"",
          "72:             \"only ever go down as these names are deprecated.  If this test is failing, please \"",
          "73:             \"adjust your new stat's name; do not add as exemption without a very good reason.\"",
          "74:         )",
          "",
          "[Removed Lines]",
          "69:         assert len(BACK_COMPAT_METRIC_NAMES) <= 23, (",
          "71:             \"There are 23 names which are potentially too long for OTel and that number should \"",
          "",
          "[Added Lines]",
          "69:         assert len(BACK_COMPAT_METRIC_NAMES) <= 26, (",
          "71:             \"There are 26 names which are potentially too long for OTel and that number should \"",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "f9a05d3695c3eb2b41f06cc73b57155d11981cd5",
      "candidate_info": {
        "commit_hash": "f9a05d3695c3eb2b41f06cc73b57155d11981cd5",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/f9a05d3695c3eb2b41f06cc73b57155d11981cd5",
        "files": [
          "airflow/models/dag.py",
          "airflow/serialization/serialized_objects.py",
          "airflow/www/security.py",
          "tests/serialization/test_dag_serialization.py"
        ],
        "message": "Fix issues related to access_control={} (#34114)\n\n* allow empty access control on dags\n\n* Add test which demonstrates loss of information during ser/de\n\n* Keep empty value when serializing access_control dict\n\n(cherry picked from commit a61b5e893af83d3022aa8265f230b52c5b2ad93d)",
        "before_after_code_files": [
          "airflow/models/dag.py||airflow/models/dag.py",
          "airflow/serialization/serialized_objects.py||airflow/serialization/serialized_objects.py",
          "airflow/www/security.py||airflow/www/security.py",
          "tests/serialization/test_dag_serialization.py||tests/serialization/test_dag_serialization.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/models/dag.py||airflow/models/dag.py": [
          "File: airflow/models/dag.py -> airflow/models/dag.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "777:         will be replaced with 'can_read', in {'role2': {'can_dag_read', 'can_dag_edit'}}",
          "778:         'can_dag_edit' will be replaced with 'can_edit', etc.",
          "779:         \"\"\"",
          "781:             return None",
          "782:         new_perm_mapping = {",
          "783:             permissions.DEPRECATED_ACTION_CAN_DAG_READ: permissions.ACTION_CAN_READ,",
          "",
          "[Removed Lines]",
          "780:         if not access_control:",
          "",
          "[Added Lines]",
          "780:         if access_control is None:",
          "",
          "---------------"
        ],
        "airflow/serialization/serialized_objects.py||airflow/serialization/serialized_objects.py": [
          "File: airflow/serialization/serialized_objects.py -> airflow/serialization/serialized_objects.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1386:         return dag",
          "1388:     @classmethod",
          "1389:     def to_dict(cls, var: Any) -> dict:",
          "1390:         \"\"\"Stringifies DAGs and operators contained by var and returns a dict of var.\"\"\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1388:     @classmethod",
          "1389:     def _is_excluded(cls, var: Any, attrname: str, op: DAGNode):",
          "1390:         # {} is explicitly different from None in the case of DAG-level access control",
          "1391:         # and as a result we need to preserve empty dicts through serialization for this field",
          "1392:         if attrname == \"_access_control\" and var is not None:",
          "1393:             return False",
          "1394:         return super()._is_excluded(var, attrname, op)",
          "",
          "---------------"
        ],
        "airflow/www/security.py||airflow/www/security.py": [
          "File: airflow/www/security.py -> airflow/www/security.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "626:                 if (action_name, dag_resource_name) not in perms:",
          "627:                     self._merge_perm(action_name, dag_resource_name)",
          "630:                 self.sync_perm_for_dag(dag_resource_name, dag.access_control)",
          "632:     def update_admin_permission(self) -> None:",
          "",
          "[Removed Lines]",
          "629:             if dag.access_control:",
          "",
          "[Added Lines]",
          "629:             if dag.access_control is not None:",
          "",
          "---------------"
        ],
        "tests/serialization/test_dag_serialization.py||tests/serialization/test_dag_serialization.py": [
          "File: tests/serialization/test_dag_serialization.py -> tests/serialization/test_dag_serialization.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "427:                 print(task[\"task_id\"], k, v)",
          "428:         assert actual == expected",
          "430:     def test_dag_serialization_unregistered_custom_timetable(self):",
          "431:         \"\"\"Verify serialization fails without timetable registration.\"\"\"",
          "432:         dag = get_timetable_based_simple_dag(CustomSerializationTimetable(\"bar\"))",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "430:     def test_dag_serialization_preserves_empty_access_roles(self):",
          "431:         \"\"\"Verify that an explicitly empty access_control dict is preserved.\"\"\"",
          "432:         dag = collect_dags([\"airflow/example_dags\"])[\"simple_dag\"]",
          "433:         dag.access_control = {}",
          "434:         serialized_dag = SerializedDAG.to_dict(dag)",
          "435:         SerializedDAG.validate_schema(serialized_dag)",
          "437:         assert serialized_dag[\"dag\"][\"_access_control\"] == {\"__type\": \"dict\", \"__var\": {}}",
          "",
          "---------------"
        ]
      }
    }
  ]
}