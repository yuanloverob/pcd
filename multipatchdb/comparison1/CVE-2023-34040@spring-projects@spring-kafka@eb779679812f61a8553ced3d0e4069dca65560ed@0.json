{
  "cve_id": "CVE-2023-34040",
  "cve_desc": "In Spring for Apache Kafka 3.0.9 and earlier and versions 2.9.10 and earlier, a possible deserialization attack vector existed, but only if unusual configuration was applied. An attacker would have to construct a malicious serialized object in one of the deserialization exception record headers.\n\nSpecifically, an application is vulnerable when all of the following are true:\n\n  *  The user does not\u00a0configure an ErrorHandlingDeserializer for the key and/or value of the record\n  *  The user explicitly sets container properties checkDeserExWhenKeyNull and/or checkDeserExWhenValueNull container properties to true.\n  *  The user allows untrusted sources to publish to a Kafka topic\n\n\nBy default, these properties are false, and the container only attempts to deserialize the headers if an ErrorHandlingDeserializer is configured. The ErrorHandlingDeserializer prevents the vulnerability by removing any such malicious headers before processing the record.\n\n\n",
  "repo": "spring-projects/spring-kafka",
  "patch_hash": "eb779679812f61a8553ced3d0e4069dca65560ed",
  "patch_info": {
    "commit_hash": "eb779679812f61a8553ced3d0e4069dca65560ed",
    "repo": "spring-projects/spring-kafka",
    "commit_url": "https://github.com/spring-projects/spring-kafka/commit/eb779679812f61a8553ced3d0e4069dca65560ed",
    "files": [
      "spring-kafka-docs/src/main/asciidoc/kafka.adoc",
      "spring-kafka/src/main/java/org/springframework/kafka/listener/DeadLetterPublishingRecoverer.java",
      "spring-kafka/src/main/java/org/springframework/kafka/listener/KafkaMessageListenerContainer.java",
      "spring-kafka/src/main/java/org/springframework/kafka/listener/ListenerUtils.java",
      "spring-kafka/src/main/java/org/springframework/kafka/requestreply/ReplyingKafkaTemplate.java",
      "spring-kafka/src/main/java/org/springframework/kafka/support/serializer/DeserializationExceptionHeader.java",
      "spring-kafka/src/main/java/org/springframework/kafka/support/serializer/SerializationUtils.java",
      "spring-kafka/src/test/java/org/springframework/kafka/listener/DeadLetterPublishingRecovererTests.java",
      "spring-kafka/src/test/java/org/springframework/kafka/listener/ErrorHandlingDeserializerTests.java",
      "spring-kafka/src/test/java/org/springframework/kafka/listener/adapter/BatchAdapterConversionErrorsTests.java",
      "spring-kafka/src/test/java/org/springframework/kafka/support/serializer/SerializationTestUtils.java",
      "spring-kafka/src/test/java/org/springframework/kafka/support/serializer/SerializationUtilsTests.java"
    ],
    "message": "Private Header Type for DeserializationExceptions\n\nUse a package-private header for deserialization exceptions.\n\n**cherry-pick to 2.9.x**",
    "before_after_code_files": [
      "spring-kafka/src/main/java/org/springframework/kafka/listener/DeadLetterPublishingRecoverer.java||spring-kafka/src/main/java/org/springframework/kafka/listener/DeadLetterPublishingRecoverer.java",
      "spring-kafka/src/main/java/org/springframework/kafka/listener/KafkaMessageListenerContainer.java||spring-kafka/src/main/java/org/springframework/kafka/listener/KafkaMessageListenerContainer.java",
      "spring-kafka/src/main/java/org/springframework/kafka/listener/ListenerUtils.java||spring-kafka/src/main/java/org/springframework/kafka/listener/ListenerUtils.java",
      "spring-kafka/src/main/java/org/springframework/kafka/requestreply/ReplyingKafkaTemplate.java||spring-kafka/src/main/java/org/springframework/kafka/requestreply/ReplyingKafkaTemplate.java",
      "spring-kafka/src/main/java/org/springframework/kafka/support/serializer/DeserializationExceptionHeader.java||spring-kafka/src/main/java/org/springframework/kafka/support/serializer/DeserializationExceptionHeader.java",
      "spring-kafka/src/main/java/org/springframework/kafka/support/serializer/SerializationUtils.java||spring-kafka/src/main/java/org/springframework/kafka/support/serializer/SerializationUtils.java",
      "spring-kafka/src/test/java/org/springframework/kafka/listener/DeadLetterPublishingRecovererTests.java||spring-kafka/src/test/java/org/springframework/kafka/listener/DeadLetterPublishingRecovererTests.java",
      "spring-kafka/src/test/java/org/springframework/kafka/listener/ErrorHandlingDeserializerTests.java||spring-kafka/src/test/java/org/springframework/kafka/listener/ErrorHandlingDeserializerTests.java",
      "spring-kafka/src/test/java/org/springframework/kafka/listener/adapter/BatchAdapterConversionErrorsTests.java||spring-kafka/src/test/java/org/springframework/kafka/listener/adapter/BatchAdapterConversionErrorsTests.java",
      "spring-kafka/src/test/java/org/springframework/kafka/support/serializer/SerializationTestUtils.java||spring-kafka/src/test/java/org/springframework/kafka/support/serializer/SerializationTestUtils.java",
      "spring-kafka/src/test/java/org/springframework/kafka/support/serializer/SerializationUtilsTests.java||spring-kafka/src/test/java/org/springframework/kafka/support/serializer/SerializationUtilsTests.java"
    ]
  },
  "patch_diff": {
    "spring-kafka/src/main/java/org/springframework/kafka/listener/DeadLetterPublishingRecoverer.java||spring-kafka/src/main/java/org/springframework/kafka/listener/DeadLetterPublishingRecoverer.java": [
      "File: spring-kafka/src/main/java/org/springframework/kafka/listener/DeadLetterPublishingRecoverer.java -> spring-kafka/src/main/java/org/springframework/kafka/listener/DeadLetterPublishingRecoverer.java",
      "--- Hunk 1 ---",
      "[Context before]",
      "506:   if (consumer != null && this.verifyPartition) {",
      "507:    tp = checkPartition(tp, consumer);",
      "508:   }",
      "510:     SerializationUtils.VALUE_DESERIALIZER_EXCEPTION_HEADER, this.logger);",
      "512:     SerializationUtils.KEY_DESERIALIZER_EXCEPTION_HEADER, this.logger);",
      "513:   Headers headers = new RecordHeaders(record.headers().toArray());",
      "514:   addAndEnhanceHeaders(record, exception, vDeserEx, kDeserEx, headers);",
      "",
      "[Removed Lines]",
      "509:   DeserializationException vDeserEx = ListenerUtils.getExceptionFromHeader(record,",
      "511:   DeserializationException kDeserEx = ListenerUtils.getExceptionFromHeader(record,",
      "",
      "[Added Lines]",
      "509:   DeserializationException vDeserEx = SerializationUtils.getExceptionFromHeader(record,",
      "511:   DeserializationException kDeserEx = SerializationUtils.getExceptionFromHeader(record,",
      "",
      "---------------"
    ],
    "spring-kafka/src/main/java/org/springframework/kafka/listener/KafkaMessageListenerContainer.java||spring-kafka/src/main/java/org/springframework/kafka/listener/KafkaMessageListenerContainer.java": [
      "File: spring-kafka/src/main/java/org/springframework/kafka/listener/KafkaMessageListenerContainer.java -> spring-kafka/src/main/java/org/springframework/kafka/listener/KafkaMessageListenerContainer.java",
      "--- Hunk 1 ---",
      "[Context before]",
      "2985:   }",
      "2987:   public void checkDeser(final ConsumerRecord<K, V> cRecord, String headerName) {",
      "2989:    if (exception != null) {",
      "",
      "[Removed Lines]",
      "2988:    DeserializationException exception = ListenerUtils.getExceptionFromHeader(cRecord, headerName, this.logger);",
      "",
      "[Added Lines]",
      "2988:    DeserializationException exception = SerializationUtils.getExceptionFromHeader(cRecord, headerName,",
      "2989:      this.logger);",
      "",
      "---------------"
    ],
    "spring-kafka/src/main/java/org/springframework/kafka/listener/ListenerUtils.java||spring-kafka/src/main/java/org/springframework/kafka/listener/ListenerUtils.java": [
      "File: spring-kafka/src/main/java/org/springframework/kafka/listener/ListenerUtils.java -> spring-kafka/src/main/java/org/springframework/kafka/listener/ListenerUtils.java",
      "--- Hunk 1 ---",
      "[Context before]",
      "25: import org.apache.kafka.clients.consumer.ConsumerRecord;",
      "26: import org.apache.kafka.clients.consumer.OffsetAndMetadata;",
      "31: import org.springframework.core.log.LogAccessor;",
      "32: import org.springframework.kafka.support.serializer.DeserializationException;",
      "33: import org.springframework.lang.Nullable;",
      "34: import org.springframework.util.Assert;",
      "35: import org.springframework.util.backoff.BackOff;",
      "",
      "[Removed Lines]",
      "27: import org.apache.kafka.common.header.Header;",
      "28: import org.apache.kafka.common.header.Headers;",
      "29: import org.apache.kafka.common.header.internals.RecordHeaders;",
      "",
      "[Added Lines]",
      "30: import org.springframework.kafka.support.serializer.SerializationUtils;",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "96:  @Nullable",
      "97:  public static DeserializationException getExceptionFromHeader(final ConsumerRecord<?, ?> record,",
      "98:    String headerName, LogAccessor logger) {",
      "112:  }",
      "",
      "[Removed Lines]",
      "100:   Header header = record.headers().lastHeader(headerName);",
      "101:   if (header != null) {",
      "102:    byte[] value = header.value();",
      "103:    DeserializationException exception = byteArrayToDeserializationException(logger, value);",
      "104:    if (exception != null) {",
      "105:     Headers headers = new RecordHeaders(record.headers().toArray());",
      "106:     headers.remove(headerName);",
      "107:     exception.setHeaders(headers);",
      "108:    }",
      "109:    return exception;",
      "110:   }",
      "111:   return null;",
      "",
      "[Added Lines]",
      "96:  @Deprecated",
      "101:   return SerializationUtils.getExceptionFromHeader(record, headerName, logger);",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "122:  @Nullable",
      "123:  public static DeserializationException byteArrayToDeserializationException(LogAccessor logger, byte[] value) {",
      "124:   try {",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "115:  @Deprecated",
      "",
      "---------------"
    ],
    "spring-kafka/src/main/java/org/springframework/kafka/requestreply/ReplyingKafkaTemplate.java||spring-kafka/src/main/java/org/springframework/kafka/requestreply/ReplyingKafkaTemplate.java": [
      "File: spring-kafka/src/main/java/org/springframework/kafka/requestreply/ReplyingKafkaTemplate.java -> spring-kafka/src/main/java/org/springframework/kafka/requestreply/ReplyingKafkaTemplate.java",
      "--- Hunk 1 ---",
      "[Context before]",
      "48: import org.springframework.kafka.listener.ConsumerSeekAware;",
      "49: import org.springframework.kafka.listener.ContainerProperties;",
      "50: import org.springframework.kafka.listener.GenericMessageListenerContainer;",
      "52: import org.springframework.kafka.support.KafkaHeaders;",
      "53: import org.springframework.kafka.support.KafkaUtils;",
      "54: import org.springframework.kafka.support.TopicPartitionOffset;",
      "",
      "[Removed Lines]",
      "51: import org.springframework.kafka.listener.ListenerUtils;",
      "",
      "[Added Lines]",
      "[None]",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "569:  @Nullable",
      "570:  public static DeserializationException checkDeserialization(ConsumerRecord<?, ?> record, LogAccessor logger) {",
      "572:     SerializationUtils.VALUE_DESERIALIZER_EXCEPTION_HEADER, logger);",
      "573:   if (exception != null) {",
      "574:    logger.error(exception, () -> \"Reply value deserialization failed for \" + record.topic() + \"-\"",
      "575:      + record.partition() + \"@\" + record.offset());",
      "576:    return exception;",
      "577:   }",
      "579:     SerializationUtils.KEY_DESERIALIZER_EXCEPTION_HEADER, logger);",
      "580:   if (exception != null) {",
      "581:    logger.error(exception, () -> \"Reply key deserialization failed for \" + record.topic() + \"-\"",
      "",
      "[Removed Lines]",
      "571:   DeserializationException exception = ListenerUtils.getExceptionFromHeader(record,",
      "578:   exception = ListenerUtils.getExceptionFromHeader(record,",
      "",
      "[Added Lines]",
      "570:   DeserializationException exception = SerializationUtils.getExceptionFromHeader(record,",
      "577:   exception = SerializationUtils.getExceptionFromHeader(record,",
      "",
      "---------------"
    ],
    "spring-kafka/src/main/java/org/springframework/kafka/support/serializer/DeserializationExceptionHeader.java||spring-kafka/src/main/java/org/springframework/kafka/support/serializer/DeserializationExceptionHeader.java": [
      "File: spring-kafka/src/main/java/org/springframework/kafka/support/serializer/DeserializationExceptionHeader.java -> spring-kafka/src/main/java/org/springframework/kafka/support/serializer/DeserializationExceptionHeader.java",
      "--- Hunk 1 ---",
      "[Context before]",
      "[No context available]",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "17: package org.springframework.kafka.support.serializer;",
      "19: import org.apache.kafka.common.header.internals.RecordHeader;",
      "29: class DeserializationExceptionHeader extends RecordHeader {",
      "36:  DeserializationExceptionHeader(String key, byte[] value) {",
      "37:   super(key, value);",
      "38:  }",
      "40: }",
      "",
      "---------------"
    ],
    "spring-kafka/src/main/java/org/springframework/kafka/support/serializer/SerializationUtils.java||spring-kafka/src/main/java/org/springframework/kafka/support/serializer/SerializationUtils.java": [
      "File: spring-kafka/src/main/java/org/springframework/kafka/support/serializer/SerializationUtils.java -> spring-kafka/src/main/java/org/springframework/kafka/support/serializer/SerializationUtils.java",
      "--- Hunk 1 ---",
      "[Context before]",
      "17: package org.springframework.kafka.support.serializer;",
      "19: import java.io.ByteArrayOutputStream;",
      "20: import java.io.IOException;",
      "21: import java.io.ObjectOutputStream;",
      "22: import java.lang.reflect.InvocationTargetException;",
      "23: import java.lang.reflect.Method;",
      "24: import java.util.function.BiFunction;",
      "26: import org.apache.kafka.common.header.Headers;",
      "29: import org.springframework.util.Assert;",
      "30: import org.springframework.util.ClassUtils;",
      "",
      "[Removed Lines]",
      "27: import org.apache.kafka.common.header.internals.RecordHeader;",
      "",
      "[Added Lines]",
      "19: import java.io.ByteArrayInputStream;",
      "22: import java.io.ObjectInputStream;",
      "24: import java.io.ObjectStreamClass;",
      "29: import org.apache.kafka.clients.consumer.ConsumerRecord;",
      "30: import org.apache.kafka.common.header.Header;",
      "32: import org.apache.kafka.common.header.internals.RecordHeaders;",
      "34: import org.springframework.core.log.LogAccessor;",
      "35: import org.springframework.kafka.support.KafkaUtils;",
      "36: import org.springframework.lang.Nullable;",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "166:    }",
      "167:   }",
      "168:   headers.add(",
      "170:       ? KEY_DESERIALIZER_EXCEPTION_HEADER",
      "171:       : VALUE_DESERIALIZER_EXCEPTION_HEADER,",
      "172:       stream.toByteArray()));",
      "173:  }",
      "175: }",
      "",
      "[Removed Lines]",
      "169:     new RecordHeader(isForKeyArg",
      "",
      "[Added Lines]",
      "177:     new DeserializationExceptionHeader(isForKeyArg",
      "192:  @Nullable",
      "193:  public static DeserializationException getExceptionFromHeader(final ConsumerRecord<?, ?> record,",
      "194:    String headerName, LogAccessor logger) {",
      "196:   Header header = record.headers().lastHeader(headerName);",
      "197:   if (!(header instanceof DeserializationExceptionHeader)) {",
      "198:    logger.warn(",
      "199:      () -> String.format(\"Foreign deserialization exception header in (%s) ignored; possible attack?\",",
      "200:        KafkaUtils.format(record)));",
      "201:    return null;",
      "202:   }",
      "203:   if (header != null) {",
      "204:    byte[] value = header.value();",
      "205:    DeserializationException exception = byteArrayToDeserializationException(logger, header);",
      "206:    if (exception != null) {",
      "207:     Headers headers = new RecordHeaders(record.headers().toArray());",
      "208:     headers.remove(headerName);",
      "209:     exception.setHeaders(headers);",
      "210:    }",
      "211:    return exception;",
      "212:   }",
      "213:   return null;",
      "214:  }",
      "224:  @Nullable",
      "225:  public static DeserializationException byteArrayToDeserializationException(LogAccessor logger, Header header) {",
      "227:   if (!(header instanceof DeserializationExceptionHeader)) {",
      "228:    throw new IllegalStateException(\"Foreign deserialization exception header ignored; possible attack?\");",
      "229:   }",
      "230:   try {",
      "231:    ObjectInputStream ois = new ObjectInputStream(new ByteArrayInputStream(header.value())) {",
      "233:     boolean first = true;",
      "235:     @Override",
      "236:     protected Class<?> resolveClass(ObjectStreamClass desc) throws IOException, ClassNotFoundException {",
      "237:      if (this.first) {",
      "238:       this.first = false;",
      "239:       Assert.state(desc.getName().equals(DeserializationException.class.getName()),",
      "240:         \"Header does not contain a DeserializationException\");",
      "241:      }",
      "242:      return super.resolveClass(desc);",
      "243:     }",
      "246:    };",
      "247:    return (DeserializationException) ois.readObject();",
      "248:   }",
      "249:   catch (IOException | ClassNotFoundException | ClassCastException e) {",
      "250:    logger.error(e, \"Failed to deserialize a deserialization exception\");",
      "251:    return null;",
      "252:   }",
      "253:  }",
      "",
      "---------------"
    ],
    "spring-kafka/src/test/java/org/springframework/kafka/listener/DeadLetterPublishingRecovererTests.java||spring-kafka/src/test/java/org/springframework/kafka/listener/DeadLetterPublishingRecovererTests.java": [
      "File: spring-kafka/src/test/java/org/springframework/kafka/listener/DeadLetterPublishingRecovererTests.java -> spring-kafka/src/test/java/org/springframework/kafka/listener/DeadLetterPublishingRecovererTests.java",
      "--- Hunk 1 ---",
      "[Context before]",
      "73: import org.springframework.kafka.support.SendResult;",
      "74: import org.springframework.kafka.support.converter.ConversionException;",
      "75: import org.springframework.kafka.support.serializer.DeserializationException;",
      "76: import org.springframework.kafka.support.serializer.SerializationUtils;",
      "77: import org.springframework.kafka.test.utils.KafkaTestUtils;",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "76: import org.springframework.kafka.support.serializer.SerializationTestUtils;",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "172:   KafkaOperations<?, ?> template = mock(KafkaOperations.class);",
      "173:   DeadLetterPublishingRecoverer recoverer = new DeadLetterPublishingRecoverer(template);",
      "174:   Headers headers = new RecordHeaders();",
      "177:   Headers custom = new RecordHeaders();",
      "178:   custom.add(new RecordHeader(\"foo\", \"bar\".getBytes()));",
      "179:   recoverer.setHeadersFunction((rec, ex) -> custom);",
      "",
      "[Removed Lines]",
      "175:   headers.add(new RecordHeader(SerializationUtils.VALUE_DESERIALIZER_EXCEPTION_HEADER, header(false)));",
      "176:   headers.add(new RecordHeader(SerializationUtils.KEY_DESERIALIZER_EXCEPTION_HEADER, header(true)));",
      "",
      "[Added Lines]",
      "176:   headers.add(SerializationTestUtils.deserializationHeader(SerializationUtils.VALUE_DESERIALIZER_EXCEPTION_HEADER,",
      "177:     header(false)));",
      "178:   headers.add(SerializationTestUtils.deserializationHeader(SerializationUtils.KEY_DESERIALIZER_EXCEPTION_HEADER,",
      "179:     header(true)));",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "202:   KafkaOperations<?, ?> template = mock(KafkaOperations.class);",
      "203:   DeadLetterPublishingRecoverer recoverer = new DeadLetterPublishingRecoverer(template);",
      "204:   Headers headers = new RecordHeaders();",
      "206:   CompletableFuture future = new CompletableFuture();",
      "207:   future.complete(new Object());",
      "208:   willReturn(future).given(template).send(any(ProducerRecord.class));",
      "",
      "[Removed Lines]",
      "205:   headers.add(new RecordHeader(SerializationUtils.KEY_DESERIALIZER_EXCEPTION_HEADER, header(true)));",
      "",
      "[Added Lines]",
      "208:   headers.add(SerializationTestUtils.deserializationHeader(SerializationUtils.KEY_DESERIALIZER_EXCEPTION_HEADER,",
      "209:     header(true)));",
      "",
      "---------------",
      "--- Hunk 4 ---",
      "[Context before]",
      "222:   DeadLetterPublishingRecoverer recoverer = new DeadLetterPublishingRecoverer(template);",
      "223:   Headers headers = new RecordHeaders();",
      "224:   DeserializationException deserEx = createDeserEx(true);",
      "227:   CompletableFuture future = new CompletableFuture();",
      "228:   future.complete(new Object());",
      "229:   willReturn(future).given(template).send(any(ProducerRecord.class));",
      "",
      "[Removed Lines]",
      "225:   headers.add(",
      "226:     new RecordHeader(SerializationUtils.KEY_DESERIALIZER_EXCEPTION_HEADER, header(true, deserEx)));",
      "",
      "[Added Lines]",
      "229:   headers.add(SerializationTestUtils.deserializationHeader(SerializationUtils.KEY_DESERIALIZER_EXCEPTION_HEADER,",
      "230:     header(true, deserEx)));",
      "",
      "---------------",
      "--- Hunk 5 ---",
      "[Context before]",
      "245:   DeadLetterPublishingRecoverer recoverer = new DeadLetterPublishingRecoverer(template);",
      "246:   recoverer.setRetainExceptionHeader(true);",
      "247:   Headers headers = new RecordHeaders();",
      "250:   CompletableFuture future = new CompletableFuture();",
      "251:   future.complete(new Object());",
      "252:   willReturn(future).given(template).send(any(ProducerRecord.class));",
      "",
      "[Removed Lines]",
      "248:   headers.add(new RecordHeader(SerializationUtils.VALUE_DESERIALIZER_EXCEPTION_HEADER, header(false)));",
      "249:   headers.add(new RecordHeader(SerializationUtils.KEY_DESERIALIZER_EXCEPTION_HEADER, header(true)));",
      "",
      "[Added Lines]",
      "252:   headers.add(SerializationTestUtils.deserializationHeader(SerializationUtils.VALUE_DESERIALIZER_EXCEPTION_HEADER,",
      "253:     header(false)));",
      "254:   headers.add(SerializationTestUtils.deserializationHeader(SerializationUtils.KEY_DESERIALIZER_EXCEPTION_HEADER,",
      "255:     header(true)));",
      "",
      "---------------"
    ],
    "spring-kafka/src/test/java/org/springframework/kafka/listener/ErrorHandlingDeserializerTests.java||spring-kafka/src/test/java/org/springframework/kafka/listener/ErrorHandlingDeserializerTests.java": [
      "File: spring-kafka/src/test/java/org/springframework/kafka/listener/ErrorHandlingDeserializerTests.java -> spring-kafka/src/test/java/org/springframework/kafka/listener/ErrorHandlingDeserializerTests.java",
      "--- Hunk 1 ---",
      "[Context before]",
      "129:   ErrorHandlingDeserializer<String> ehd = new ErrorHandlingDeserializer<>(new MyDes());",
      "130:   Headers headers = new RecordHeaders();",
      "131:   ehd.deserialize(\"foo\", headers, new byte[1]);",
      "134:   assertThat(dex.getCause().getMessage())",
      "135:     .contains(\"Could not serialize\")",
      "136:     .contains(\"original exception message\");",
      "",
      "[Removed Lines]",
      "132:   DeserializationException dex = ListenerUtils.byteArrayToDeserializationException(null,",
      "133:     headers.lastHeader(SerializationUtils.VALUE_DESERIALIZER_EXCEPTION_HEADER).value());",
      "",
      "[Added Lines]",
      "132:   DeserializationException dex = SerializationUtils.byteArrayToDeserializationException(null,",
      "133:     headers.lastHeader(SerializationUtils.VALUE_DESERIALIZER_EXCEPTION_HEADER));",
      "",
      "---------------"
    ],
    "spring-kafka/src/test/java/org/springframework/kafka/listener/adapter/BatchAdapterConversionErrorsTests.java||spring-kafka/src/test/java/org/springframework/kafka/listener/adapter/BatchAdapterConversionErrorsTests.java": [
      "File: spring-kafka/src/test/java/org/springframework/kafka/listener/adapter/BatchAdapterConversionErrorsTests.java -> spring-kafka/src/test/java/org/springframework/kafka/listener/adapter/BatchAdapterConversionErrorsTests.java",
      "--- Hunk 1 ---",
      "[Context before]",
      "37: import org.springframework.kafka.core.ConsumerFactory;",
      "38: import org.springframework.kafka.listener.BatchListenerFailedException;",
      "39: import org.springframework.kafka.listener.ListenerExecutionFailedException;",
      "41: import org.springframework.kafka.support.KafkaHeaders;",
      "42: import org.springframework.kafka.support.converter.BatchMessagingMessageConverter;",
      "43: import org.springframework.kafka.support.converter.ConversionException;",
      "",
      "[Removed Lines]",
      "40: import org.springframework.kafka.listener.ListenerUtils;",
      "",
      "[Added Lines]",
      "[None]",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "74:     .extracting(\"index\")",
      "75:     .isEqualTo(1);",
      "76:   assertThat(listener.values).containsExactly(new Foo(\"baz\"), null, new Foo(\"qux\"));",
      "78:     SerializationUtils.VALUE_DESERIALIZER_EXCEPTION_HEADER, null);",
      "79:   assertThat(vDeserEx).isNotNull();",
      "80:   assertThat(vDeserEx.getData()).isEqualTo(\"JUNK\".getBytes());",
      "",
      "[Removed Lines]",
      "77:   DeserializationException vDeserEx = ListenerUtils.getExceptionFromHeader(junkRecord,",
      "",
      "[Added Lines]",
      "76:   DeserializationException vDeserEx = SerializationUtils.getExceptionFromHeader(junkRecord,",
      "",
      "---------------"
    ],
    "spring-kafka/src/test/java/org/springframework/kafka/support/serializer/SerializationTestUtils.java||spring-kafka/src/test/java/org/springframework/kafka/support/serializer/SerializationTestUtils.java": [
      "File: spring-kafka/src/test/java/org/springframework/kafka/support/serializer/SerializationTestUtils.java -> spring-kafka/src/test/java/org/springframework/kafka/support/serializer/SerializationTestUtils.java",
      "--- Hunk 1 ---",
      "[Context before]",
      "[No context available]",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "17: package org.springframework.kafka.support.serializer;",
      "19: import org.apache.kafka.common.header.Header;",
      "26: public final class SerializationTestUtils {",
      "28:  private SerializationTestUtils() {",
      "29:  }",
      "31:  public static Header deserializationHeader(String key, byte[] value) {",
      "32:   return new DeserializationExceptionHeader(key, value);",
      "33:  }",
      "35: }",
      "",
      "---------------"
    ],
    "spring-kafka/src/test/java/org/springframework/kafka/support/serializer/SerializationUtilsTests.java||spring-kafka/src/test/java/org/springframework/kafka/support/serializer/SerializationUtilsTests.java": [
      "File: spring-kafka/src/test/java/org/springframework/kafka/support/serializer/SerializationUtilsTests.java -> spring-kafka/src/test/java/org/springframework/kafka/support/serializer/SerializationUtilsTests.java",
      "--- Hunk 1 ---",
      "[Context before]",
      "[No context available]",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "17: package org.springframework.kafka.support.serializer;",
      "19: import static org.assertj.core.api.Assertions.assertThat;",
      "20: import static org.mockito.BDDMockito.given;",
      "21: import static org.mockito.BDDMockito.willAnswer;",
      "22: import static org.mockito.BDDMockito.willReturn;",
      "23: import static org.mockito.Mockito.mock;",
      "24: import static org.mockito.Mockito.spy;",
      "26: import java.util.List;",
      "27: import java.util.function.Supplier;",
      "29: import org.apache.commons.logging.LogFactory;",
      "30: import org.apache.kafka.clients.consumer.ConsumerRecord;",
      "31: import org.apache.kafka.common.header.internals.RecordHeader;",
      "32: import org.apache.kafka.common.header.internals.RecordHeaders;",
      "33: import org.junit.jupiter.api.Test;",
      "34: import org.mockito.ArgumentCaptor;",
      "36: import org.springframework.core.log.LogAccessor;",
      "43: public class SerializationUtilsTests {",
      "45:  @Test",
      "46:  void foreignDeserEx() {",
      "47:   RecordHeaders headers = new RecordHeaders(",
      "48:     List.of(new RecordHeader(SerializationUtils.VALUE_DESERIALIZER_EXCEPTION_HEADER, \"junk\".getBytes())));",
      "49:   ConsumerRecord<String, String> rec = mock(ConsumerRecord.class);",
      "50:   willReturn(headers).given(rec).headers();",
      "51:   given(rec.topic()).willReturn(\"foo\");",
      "52:   given(rec.partition()).willReturn(1);",
      "53:   given(rec.offset()).willReturn(0L);",
      "54:   LogAccessor logger = spy(new LogAccessor(LogFactory.getLog(getClass())));",
      "55:   ArgumentCaptor<Supplier<String>> captor = ArgumentCaptor.forClass(Supplier.class);",
      "56:   willAnswer(inv -> null).given(logger).warn(captor.capture());",
      "57:   assertThat(SerializationUtils.getExceptionFromHeader(rec,",
      "58:     SerializationUtils.VALUE_DESERIALIZER_EXCEPTION_HEADER, logger)).isNull();",
      "59:   assertThat(captor.getValue().get())",
      "60:     .isEqualTo(\"Foreign deserialization exception header in (foo-1@0) ignored; possible attack?\");",
      "61:  }",
      "63: }",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "7cc7fc8deb68764cb066f0617321b5ea313d6c92",
      "candidate_info": {
        "commit_hash": "7cc7fc8deb68764cb066f0617321b5ea313d6c92",
        "repo": "spring-projects/spring-kafka",
        "commit_url": "https://github.com/spring-projects/spring-kafka/commit/7cc7fc8deb68764cb066f0617321b5ea313d6c92",
        "files": [
          "spring-kafka/src/main/java/org/springframework/kafka/support/DefaultKafkaHeaderMapper.java",
          "spring-kafka/src/main/java/org/springframework/kafka/support/KafkaUtils.java",
          "spring-kafka/src/main/java/org/springframework/kafka/support/serializer/SerializationUtils.java",
          "spring-kafka/src/test/java/org/springframework/kafka/listener/DeadLetterPublishingRecovererTests.java",
          "spring-kafka/src/test/java/org/springframework/kafka/support/DefaultKafkaHeaderMapperTests.java",
          "spring-kafka/src/test/java/org/springframework/kafka/support/serializer/SerializationTestUtils.java"
        ],
        "message": "GH-3114: DeserializationException propagation\n\nFixes: #3114\n\n* Since DeserializationExceptionHeader is currently porpagated as a `byte[]`,\n  it encounters some issues when processing the header especially in batch\n  listeners. Fixing this by providing the deserialization header without `byte[]` conversion\n* Adding test to verify\n* Refactoring in SerializationTestUtils\n\n**Auto-cherry-pick to `3.1.x` & `3.0.x`**",
        "before_after_code_files": [
          "spring-kafka/src/main/java/org/springframework/kafka/support/DefaultKafkaHeaderMapper.java||spring-kafka/src/main/java/org/springframework/kafka/support/DefaultKafkaHeaderMapper.java",
          "spring-kafka/src/main/java/org/springframework/kafka/support/KafkaUtils.java||spring-kafka/src/main/java/org/springframework/kafka/support/KafkaUtils.java",
          "spring-kafka/src/main/java/org/springframework/kafka/support/serializer/SerializationUtils.java||spring-kafka/src/main/java/org/springframework/kafka/support/serializer/SerializationUtils.java",
          "spring-kafka/src/test/java/org/springframework/kafka/listener/DeadLetterPublishingRecovererTests.java||spring-kafka/src/test/java/org/springframework/kafka/listener/DeadLetterPublishingRecovererTests.java",
          "spring-kafka/src/test/java/org/springframework/kafka/support/DefaultKafkaHeaderMapperTests.java||spring-kafka/src/test/java/org/springframework/kafka/support/DefaultKafkaHeaderMapperTests.java",
          "spring-kafka/src/test/java/org/springframework/kafka/support/serializer/SerializationTestUtils.java||spring-kafka/src/test/java/org/springframework/kafka/support/serializer/SerializationTestUtils.java"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "spring-kafka/src/main/java/org/springframework/kafka/support/serializer/SerializationUtils.java||spring-kafka/src/main/java/org/springframework/kafka/support/serializer/SerializationUtils.java",
            "spring-kafka/src/test/java/org/springframework/kafka/listener/DeadLetterPublishingRecovererTests.java||spring-kafka/src/test/java/org/springframework/kafka/listener/DeadLetterPublishingRecovererTests.java",
            "spring-kafka/src/test/java/org/springframework/kafka/support/serializer/SerializationTestUtils.java||spring-kafka/src/test/java/org/springframework/kafka/support/serializer/SerializationTestUtils.java"
          ],
          "candidate": [
            "spring-kafka/src/main/java/org/springframework/kafka/support/serializer/SerializationUtils.java||spring-kafka/src/main/java/org/springframework/kafka/support/serializer/SerializationUtils.java",
            "spring-kafka/src/test/java/org/springframework/kafka/listener/DeadLetterPublishingRecovererTests.java||spring-kafka/src/test/java/org/springframework/kafka/listener/DeadLetterPublishingRecovererTests.java",
            "spring-kafka/src/test/java/org/springframework/kafka/support/serializer/SerializationTestUtils.java||spring-kafka/src/test/java/org/springframework/kafka/support/serializer/SerializationTestUtils.java"
          ]
        }
      },
      "candidate_diff": {
        "spring-kafka/src/main/java/org/springframework/kafka/support/DefaultKafkaHeaderMapper.java||spring-kafka/src/main/java/org/springframework/kafka/support/DefaultKafkaHeaderMapper.java": [
          "File: spring-kafka/src/main/java/org/springframework/kafka/support/DefaultKafkaHeaderMapper.java -> spring-kafka/src/main/java/org/springframework/kafka/support/DefaultKafkaHeaderMapper.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "314:    else if (headerName.equals(KafkaHeaders.LISTENER_INFO) && matchesForInbound(headerName)) {",
          "315:     headers.put(headerName, new String(header.value(), getCharset()));",
          "316:    }",
          "317:    else if (!(headerName.equals(JSON_TYPES)) && matchesForInbound(headerName)) {",
          "318:     if (jsonTypes.containsKey(headerName)) {",
          "319:      String requestedType = jsonTypes.get(headerName);",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "318:    else if (headerName.equals(KafkaUtils.KEY_DESERIALIZER_EXCEPTION_HEADER) ||",
          "319:      headerName.equals(KafkaUtils.VALUE_DESERIALIZER_EXCEPTION_HEADER)) {",
          "320:     headers.put(headerName, header);",
          "321:    }",
          "",
          "---------------"
        ],
        "spring-kafka/src/main/java/org/springframework/kafka/support/KafkaUtils.java||spring-kafka/src/main/java/org/springframework/kafka/support/KafkaUtils.java": [
          "File: spring-kafka/src/main/java/org/springframework/kafka/support/KafkaUtils.java -> spring-kafka/src/main/java/org/springframework/kafka/support/KafkaUtils.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "45: public final class KafkaUtils {",
          "47:  private static Function<ProducerRecord<?, ?>, String> prFormatter = ProducerRecord::toString;",
          "49:  private static Function<ConsumerRecord<?, ?>, String> crFormatter =",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "52:  public static final String DESERIALIZER_EXCEPTION_HEADER_PREFIX = \"springDeserializerException\";",
          "58:  public static final String KEY_DESERIALIZER_EXCEPTION_HEADER = DESERIALIZER_EXCEPTION_HEADER_PREFIX + \"Key\";",
          "64:  public static final String VALUE_DESERIALIZER_EXCEPTION_HEADER = DESERIALIZER_EXCEPTION_HEADER_PREFIX + \"Value\";",
          "",
          "---------------"
        ],
        "spring-kafka/src/main/java/org/springframework/kafka/support/serializer/SerializationUtils.java||spring-kafka/src/main/java/org/springframework/kafka/support/serializer/SerializationUtils.java": [
          "File: spring-kafka/src/main/java/org/springframework/kafka/support/serializer/SerializationUtils.java -> spring-kafka/src/main/java/org/springframework/kafka/support/serializer/SerializationUtils.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "67:  private SerializationUtils() {",
          "68:  }",
          "",
          "[Removed Lines]",
          "53:  public static final String DESERIALIZER_EXCEPTION_HEADER_PREFIX = \"springDeserializerException\";",
          "59:  public static final String KEY_DESERIALIZER_EXCEPTION_HEADER = DESERIALIZER_EXCEPTION_HEADER_PREFIX + \"Key\";",
          "65:  public static final String VALUE_DESERIALIZER_EXCEPTION_HEADER = DESERIALIZER_EXCEPTION_HEADER_PREFIX + \"Value\";",
          "",
          "[Added Lines]",
          "53:  public static final String DESERIALIZER_EXCEPTION_HEADER_PREFIX = KafkaUtils.DESERIALIZER_EXCEPTION_HEADER_PREFIX;",
          "59:  public static final String KEY_DESERIALIZER_EXCEPTION_HEADER = KafkaUtils.KEY_DESERIALIZER_EXCEPTION_HEADER;",
          "65:  public static final String VALUE_DESERIALIZER_EXCEPTION_HEADER = KafkaUtils.VALUE_DESERIALIZER_EXCEPTION_HEADER;",
          "",
          "---------------"
        ],
        "spring-kafka/src/test/java/org/springframework/kafka/listener/DeadLetterPublishingRecovererTests.java||spring-kafka/src/test/java/org/springframework/kafka/listener/DeadLetterPublishingRecovererTests.java": [
          "File: spring-kafka/src/test/java/org/springframework/kafka/listener/DeadLetterPublishingRecovererTests.java -> spring-kafka/src/test/java/org/springframework/kafka/listener/DeadLetterPublishingRecovererTests.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "33: import static org.mockito.Mockito.times;",
          "34: import static org.mockito.Mockito.verify;",
          "40: import java.time.Duration;",
          "41: import java.util.Collections;",
          "42: import java.util.HashMap;",
          "",
          "[Removed Lines]",
          "36: import java.io.ByteArrayOutputStream;",
          "37: import java.io.IOException;",
          "38: import java.io.ObjectOutputStream;",
          "39: import java.io.UncheckedIOException;",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "174:   DeadLetterPublishingRecoverer recoverer = new DeadLetterPublishingRecoverer(template);",
          "175:   Headers headers = new RecordHeaders();",
          "176:   headers.add(SerializationTestUtils.deserializationHeader(SerializationUtils.VALUE_DESERIALIZER_EXCEPTION_HEADER,",
          "178:   headers.add(SerializationTestUtils.deserializationHeader(SerializationUtils.KEY_DESERIALIZER_EXCEPTION_HEADER,",
          "180:   Headers custom = new RecordHeaders();",
          "181:   custom.add(new RecordHeader(\"foo\", \"bar\".getBytes()));",
          "182:   recoverer.setHeadersFunction((rec, ex) -> custom);",
          "",
          "[Removed Lines]",
          "177:     header(false)));",
          "179:     header(true)));",
          "",
          "[Added Lines]",
          "174:     SerializationTestUtils.header(false)));",
          "176:     SerializationTestUtils.header(true)));",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "206:   DeadLetterPublishingRecoverer recoverer = new DeadLetterPublishingRecoverer(template);",
          "207:   Headers headers = new RecordHeaders();",
          "208:   headers.add(SerializationTestUtils.deserializationHeader(SerializationUtils.KEY_DESERIALIZER_EXCEPTION_HEADER,",
          "210:   CompletableFuture future = new CompletableFuture();",
          "211:   future.complete(new Object());",
          "212:   willReturn(future).given(template).send(any(ProducerRecord.class));",
          "",
          "[Removed Lines]",
          "209:     header(true)));",
          "",
          "[Added Lines]",
          "206:     SerializationTestUtils.header(true)));",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "225:   KafkaOperations<?, ?> template = mock(KafkaOperations.class);",
          "226:   DeadLetterPublishingRecoverer recoverer = new DeadLetterPublishingRecoverer(template);",
          "227:   Headers headers = new RecordHeaders();",
          "229:   headers.add(SerializationTestUtils.deserializationHeader(SerializationUtils.KEY_DESERIALIZER_EXCEPTION_HEADER,",
          "231:   CompletableFuture future = new CompletableFuture();",
          "232:   future.complete(new Object());",
          "233:   willReturn(future).given(template).send(any(ProducerRecord.class));",
          "",
          "[Removed Lines]",
          "228:   DeserializationException deserEx = createDeserEx(true);",
          "230:     header(true, deserEx)));",
          "",
          "[Added Lines]",
          "225:   DeserializationException deserEx = SerializationTestUtils.createDeserEx(true);",
          "227:     SerializationTestUtils.header(deserEx)));",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "250:   recoverer.setRetainExceptionHeader(true);",
          "251:   Headers headers = new RecordHeaders();",
          "252:   headers.add(SerializationTestUtils.deserializationHeader(SerializationUtils.VALUE_DESERIALIZER_EXCEPTION_HEADER,",
          "254:   headers.add(SerializationTestUtils.deserializationHeader(SerializationUtils.KEY_DESERIALIZER_EXCEPTION_HEADER,",
          "256:   CompletableFuture future = new CompletableFuture();",
          "257:   future.complete(new Object());",
          "258:   willReturn(future).given(template).send(any(ProducerRecord.class));",
          "",
          "[Removed Lines]",
          "253:     header(false)));",
          "255:     header(true)));",
          "",
          "[Added Lines]",
          "250:     SerializationTestUtils.header(false)));",
          "252:     SerializationTestUtils.header(true)));",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "302:   verify(template2).send(any(ProducerRecord.class));",
          "303:  }",
          "326:  @SuppressWarnings({\"unchecked\", \"rawtypes\"})",
          "327:  @Test",
          "328:  void allOriginalHeaders() {",
          "",
          "[Removed Lines]",
          "305:  private byte[] header(boolean isKey) {",
          "306:   return header(isKey, createDeserEx(isKey));",
          "307:  }",
          "309:  private DeserializationException createDeserEx(boolean isKey) {",
          "310:   return new DeserializationException(",
          "311:     isKey ? \"testK\" : \"testV\",",
          "312:     isKey ? \"key\".getBytes() : \"value\".getBytes(), isKey, null);",
          "313:  }",
          "315:  private byte[] header(boolean isKey, DeserializationException deserEx) {",
          "316:   ByteArrayOutputStream baos = new ByteArrayOutputStream();",
          "317:   try {",
          "318:    new ObjectOutputStream(baos).writeObject(deserEx);",
          "319:   }",
          "320:   catch (IOException e) {",
          "321:    throw new UncheckedIOException(e);",
          "322:   }",
          "323:   return baos.toByteArray();",
          "324:  }",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "spring-kafka/src/test/java/org/springframework/kafka/support/DefaultKafkaHeaderMapperTests.java||spring-kafka/src/test/java/org/springframework/kafka/support/DefaultKafkaHeaderMapperTests.java": [
          "File: spring-kafka/src/test/java/org/springframework/kafka/support/DefaultKafkaHeaderMapperTests.java -> spring-kafka/src/test/java/org/springframework/kafka/support/DefaultKafkaHeaderMapperTests.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "34: import org.apache.kafka.common.header.internals.RecordHeaders;",
          "35: import org.junit.jupiter.api.Test;",
          "37: import org.springframework.kafka.support.DefaultKafkaHeaderMapper.NonTrustedHeaderType;",
          "38: import org.springframework.messaging.Message;",
          "39: import org.springframework.messaging.MessageHeaders;",
          "40: import org.springframework.messaging.support.ExecutorSubscribableChannel;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "37: import org.springframework.core.log.LogAccessor;",
          "39: import org.springframework.kafka.support.serializer.DeserializationException;",
          "40: import org.springframework.kafka.support.serializer.SerializationTestUtils;",
          "41: import org.springframework.kafka.support.serializer.SerializationUtils;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "321:     .containsKey(\"baz\");",
          "322:  }",
          "324:  public static final class Foo {",
          "326:   private String bar = \"bar\";",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "329:  @Test",
          "330:  void deserializationExceptionHeadersAreMappedAsNonByteArray() {",
          "331:   DefaultKafkaHeaderMapper mapper = new DefaultKafkaHeaderMapper();",
          "333:   byte[] keyDeserExceptionBytes = SerializationTestUtils.header(true);",
          "334:   Header keyHeader = SerializationTestUtils.deserializationHeader(SerializationUtils.KEY_DESERIALIZER_EXCEPTION_HEADER,",
          "335:     keyDeserExceptionBytes);",
          "336:   byte[] valueDeserExceptionBytes = SerializationTestUtils.header(false);",
          "337:   Header valueHeader = SerializationTestUtils.deserializationHeader(SerializationUtils.VALUE_DESERIALIZER_EXCEPTION_HEADER,",
          "338:     valueDeserExceptionBytes);",
          "339:   Headers headers = new RecordHeaders(",
          "340:     new Header[] { keyHeader, valueHeader });",
          "341:   Map<String, Object> springHeaders = new HashMap<>();",
          "342:   mapper.toHeaders(headers, springHeaders);",
          "343:   assertThat(springHeaders.get(SerializationUtils.KEY_DESERIALIZER_EXCEPTION_HEADER)).isEqualTo(keyHeader);",
          "344:   assertThat(springHeaders.get(SerializationUtils.VALUE_DESERIALIZER_EXCEPTION_HEADER)).isEqualTo(valueHeader);",
          "346:   LogAccessor logger = new LogAccessor(this.getClass());",
          "348:   DeserializationException keyDeserializationException = SerializationUtils.byteArrayToDeserializationException(logger, keyHeader);",
          "349:   assertThat(keyDeserExceptionBytes).containsExactly(SerializationTestUtils.header(keyDeserializationException));",
          "351:   DeserializationException valueDeserializationException =",
          "352:     SerializationUtils.byteArrayToDeserializationException(logger, valueHeader);",
          "353:   assertThat(valueDeserExceptionBytes).containsExactly(SerializationTestUtils.header(valueDeserializationException));",
          "355:   headers = new RecordHeaders();",
          "356:   mapper.fromHeaders(new MessageHeaders(springHeaders), headers);",
          "357:   assertThat(headers.lastHeader(SerializationUtils.KEY_DESERIALIZER_EXCEPTION_HEADER)).isNull();",
          "358:   assertThat(headers.lastHeader(SerializationUtils.VALUE_DESERIALIZER_EXCEPTION_HEADER)).isNull();",
          "359:  }",
          "",
          "---------------"
        ],
        "spring-kafka/src/test/java/org/springframework/kafka/support/serializer/SerializationTestUtils.java||spring-kafka/src/test/java/org/springframework/kafka/support/serializer/SerializationTestUtils.java": [
          "File: spring-kafka/src/test/java/org/springframework/kafka/support/serializer/SerializationTestUtils.java -> spring-kafka/src/test/java/org/springframework/kafka/support/serializer/SerializationTestUtils.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "17: package org.springframework.kafka.support.serializer;",
          "19: import org.apache.kafka.common.header.Header;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "19: import java.io.ByteArrayOutputStream;",
          "20: import java.io.IOException;",
          "21: import java.io.ObjectOutputStream;",
          "22: import java.io.UncheckedIOException;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "32:   return new DeserializationExceptionHeader(key, value);",
          "33:  }",
          "35: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "41:  public static byte[] header(boolean isKey) {",
          "42:   return header(createDeserEx(isKey));",
          "43:  }",
          "45:  public static DeserializationException createDeserEx(boolean isKey) {",
          "46:   return new DeserializationException(",
          "47:     isKey ? \"testK\" : \"testV\",",
          "48:     isKey ? \"key\".getBytes() : \"value\".getBytes(), isKey, null);",
          "49:  }",
          "51:  public static byte[] header(DeserializationException deserEx) {",
          "52:   ByteArrayOutputStream baos = new ByteArrayOutputStream();",
          "53:   try {",
          "54:    new ObjectOutputStream(baos).writeObject(deserEx);",
          "55:   }",
          "56:   catch (IOException e) {",
          "57:    throw new UncheckedIOException(e);",
          "58:   }",
          "59:   return baos.toByteArray();",
          "60:  }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "d1b638f8ab3c9e5718d653f3c1919d9a6073472e",
      "candidate_info": {
        "commit_hash": "d1b638f8ab3c9e5718d653f3c1919d9a6073472e",
        "repo": "spring-projects/spring-kafka",
        "commit_url": "https://github.com/spring-projects/spring-kafka/commit/d1b638f8ab3c9e5718d653f3c1919d9a6073472e",
        "files": [
          "spring-kafka/src/main/java/org/springframework/kafka/support/DefaultKafkaHeaderMapper.java",
          "spring-kafka/src/main/java/org/springframework/kafka/support/KafkaUtils.java",
          "spring-kafka/src/main/java/org/springframework/kafka/support/serializer/SerializationUtils.java",
          "spring-kafka/src/test/java/org/springframework/kafka/listener/DeadLetterPublishingRecovererTests.java",
          "spring-kafka/src/test/java/org/springframework/kafka/support/DefaultKafkaHeaderMapperTests.java",
          "spring-kafka/src/test/java/org/springframework/kafka/support/serializer/SerializationTestUtils.java"
        ],
        "message": "GH-3114: DeserializationException propagation\n\nFixes: #3114\n\n* Since DeserializationExceptionHeader is currently porpagated as a `byte[]`,\n  it encounters some issues when processing the header especially in batch\n  listeners. Fixing this by providing the deserialization header without `byte[]` conversion\n* Adding test to verify\n* Refactoring in SerializationTestUtils\n\n(cherry picked from commit 7cc7fc8deb68764cb066f0617321b5ea313d6c92)\n\n# Conflicts:\n#\tspring-kafka/src/main/java/org/springframework/kafka/support/KafkaUtils.java",
        "before_after_code_files": [
          "spring-kafka/src/main/java/org/springframework/kafka/support/DefaultKafkaHeaderMapper.java||spring-kafka/src/main/java/org/springframework/kafka/support/DefaultKafkaHeaderMapper.java",
          "spring-kafka/src/main/java/org/springframework/kafka/support/KafkaUtils.java||spring-kafka/src/main/java/org/springframework/kafka/support/KafkaUtils.java",
          "spring-kafka/src/main/java/org/springframework/kafka/support/serializer/SerializationUtils.java||spring-kafka/src/main/java/org/springframework/kafka/support/serializer/SerializationUtils.java",
          "spring-kafka/src/test/java/org/springframework/kafka/listener/DeadLetterPublishingRecovererTests.java||spring-kafka/src/test/java/org/springframework/kafka/listener/DeadLetterPublishingRecovererTests.java",
          "spring-kafka/src/test/java/org/springframework/kafka/support/DefaultKafkaHeaderMapperTests.java||spring-kafka/src/test/java/org/springframework/kafka/support/DefaultKafkaHeaderMapperTests.java",
          "spring-kafka/src/test/java/org/springframework/kafka/support/serializer/SerializationTestUtils.java||spring-kafka/src/test/java/org/springframework/kafka/support/serializer/SerializationTestUtils.java"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "spring-kafka/src/main/java/org/springframework/kafka/support/serializer/SerializationUtils.java||spring-kafka/src/main/java/org/springframework/kafka/support/serializer/SerializationUtils.java",
            "spring-kafka/src/test/java/org/springframework/kafka/listener/DeadLetterPublishingRecovererTests.java||spring-kafka/src/test/java/org/springframework/kafka/listener/DeadLetterPublishingRecovererTests.java",
            "spring-kafka/src/test/java/org/springframework/kafka/support/serializer/SerializationTestUtils.java||spring-kafka/src/test/java/org/springframework/kafka/support/serializer/SerializationTestUtils.java"
          ],
          "candidate": [
            "spring-kafka/src/main/java/org/springframework/kafka/support/serializer/SerializationUtils.java||spring-kafka/src/main/java/org/springframework/kafka/support/serializer/SerializationUtils.java",
            "spring-kafka/src/test/java/org/springframework/kafka/listener/DeadLetterPublishingRecovererTests.java||spring-kafka/src/test/java/org/springframework/kafka/listener/DeadLetterPublishingRecovererTests.java",
            "spring-kafka/src/test/java/org/springframework/kafka/support/serializer/SerializationTestUtils.java||spring-kafka/src/test/java/org/springframework/kafka/support/serializer/SerializationTestUtils.java"
          ]
        }
      },
      "candidate_diff": {
        "spring-kafka/src/main/java/org/springframework/kafka/support/DefaultKafkaHeaderMapper.java||spring-kafka/src/main/java/org/springframework/kafka/support/DefaultKafkaHeaderMapper.java": [
          "File: spring-kafka/src/main/java/org/springframework/kafka/support/DefaultKafkaHeaderMapper.java -> spring-kafka/src/main/java/org/springframework/kafka/support/DefaultKafkaHeaderMapper.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "320:    else if (headerName.equals(KafkaHeaders.LISTENER_INFO) && matchesForInbound(headerName)) {",
          "321:     headers.put(headerName, new String(header.value(), getCharset()));",
          "322:    }",
          "323:    else if (!(headerName.equals(JSON_TYPES)) && matchesForInbound(headerName)) {",
          "324:     if (jsonTypes != null && jsonTypes.containsKey(headerName)) {",
          "325:      String requestedType = jsonTypes.get(headerName);",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "324:    else if (headerName.equals(KafkaUtils.KEY_DESERIALIZER_EXCEPTION_HEADER) ||",
          "325:      headerName.equals(KafkaUtils.VALUE_DESERIALIZER_EXCEPTION_HEADER)) {",
          "326:     headers.put(headerName, header);",
          "327:    }",
          "",
          "---------------"
        ],
        "spring-kafka/src/main/java/org/springframework/kafka/support/KafkaUtils.java||spring-kafka/src/main/java/org/springframework/kafka/support/KafkaUtils.java": [
          "File: spring-kafka/src/main/java/org/springframework/kafka/support/KafkaUtils.java -> spring-kafka/src/main/java/org/springframework/kafka/support/KafkaUtils.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "43: public final class KafkaUtils {",
          "45:  private static final ThreadLocal<Boolean> LOG_METADATA_ONLY = new ThreadLocal<>();",
          "47:  private static Function<ProducerRecord<?, ?>, String> prFormatter = ProducerRecord::toString;",
          "49:  private static Function<ConsumerRecord<?, ?>, String> crFormatter =",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "50:  public static final String DESERIALIZER_EXCEPTION_HEADER_PREFIX = \"springDeserializerException\";",
          "56:  public static final String KEY_DESERIALIZER_EXCEPTION_HEADER = DESERIALIZER_EXCEPTION_HEADER_PREFIX + \"Key\";",
          "62:  public static final String VALUE_DESERIALIZER_EXCEPTION_HEADER = DESERIALIZER_EXCEPTION_HEADER_PREFIX + \"Value\";",
          "",
          "---------------"
        ],
        "spring-kafka/src/main/java/org/springframework/kafka/support/serializer/SerializationUtils.java||spring-kafka/src/main/java/org/springframework/kafka/support/serializer/SerializationUtils.java": [
          "File: spring-kafka/src/main/java/org/springframework/kafka/support/serializer/SerializationUtils.java -> spring-kafka/src/main/java/org/springframework/kafka/support/serializer/SerializationUtils.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "67:  private SerializationUtils() {",
          "68:  }",
          "",
          "[Removed Lines]",
          "53:  public static final String DESERIALIZER_EXCEPTION_HEADER_PREFIX = \"springDeserializerException\";",
          "59:  public static final String KEY_DESERIALIZER_EXCEPTION_HEADER = DESERIALIZER_EXCEPTION_HEADER_PREFIX + \"Key\";",
          "65:  public static final String VALUE_DESERIALIZER_EXCEPTION_HEADER = DESERIALIZER_EXCEPTION_HEADER_PREFIX + \"Value\";",
          "",
          "[Added Lines]",
          "53:  public static final String DESERIALIZER_EXCEPTION_HEADER_PREFIX = KafkaUtils.DESERIALIZER_EXCEPTION_HEADER_PREFIX;",
          "59:  public static final String KEY_DESERIALIZER_EXCEPTION_HEADER = KafkaUtils.KEY_DESERIALIZER_EXCEPTION_HEADER;",
          "65:  public static final String VALUE_DESERIALIZER_EXCEPTION_HEADER = KafkaUtils.VALUE_DESERIALIZER_EXCEPTION_HEADER;",
          "",
          "---------------"
        ],
        "spring-kafka/src/test/java/org/springframework/kafka/listener/DeadLetterPublishingRecovererTests.java||spring-kafka/src/test/java/org/springframework/kafka/listener/DeadLetterPublishingRecovererTests.java": [
          "File: spring-kafka/src/test/java/org/springframework/kafka/listener/DeadLetterPublishingRecovererTests.java -> spring-kafka/src/test/java/org/springframework/kafka/listener/DeadLetterPublishingRecovererTests.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "33: import static org.mockito.Mockito.times;",
          "34: import static org.mockito.Mockito.verify;",
          "40: import java.time.Duration;",
          "41: import java.util.Collections;",
          "42: import java.util.HashMap;",
          "",
          "[Removed Lines]",
          "36: import java.io.ByteArrayOutputStream;",
          "37: import java.io.IOException;",
          "38: import java.io.ObjectOutputStream;",
          "39: import java.io.UncheckedIOException;",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "174:   DeadLetterPublishingRecoverer recoverer = new DeadLetterPublishingRecoverer(template);",
          "175:   Headers headers = new RecordHeaders();",
          "176:   headers.add(SerializationTestUtils.deserializationHeader(SerializationUtils.VALUE_DESERIALIZER_EXCEPTION_HEADER,",
          "178:   headers.add(SerializationTestUtils.deserializationHeader(SerializationUtils.KEY_DESERIALIZER_EXCEPTION_HEADER,",
          "180:   Headers custom = new RecordHeaders();",
          "181:   custom.add(new RecordHeader(\"foo\", \"bar\".getBytes()));",
          "182:   recoverer.setHeadersFunction((rec, ex) -> custom);",
          "",
          "[Removed Lines]",
          "177:     header(false)));",
          "179:     header(true)));",
          "",
          "[Added Lines]",
          "174:     SerializationTestUtils.header(false)));",
          "176:     SerializationTestUtils.header(true)));",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "206:   DeadLetterPublishingRecoverer recoverer = new DeadLetterPublishingRecoverer(template);",
          "207:   Headers headers = new RecordHeaders();",
          "208:   headers.add(SerializationTestUtils.deserializationHeader(SerializationUtils.KEY_DESERIALIZER_EXCEPTION_HEADER,",
          "210:   CompletableFuture future = new CompletableFuture();",
          "211:   future.complete(new Object());",
          "212:   willReturn(future).given(template).send(any(ProducerRecord.class));",
          "",
          "[Removed Lines]",
          "209:     header(true)));",
          "",
          "[Added Lines]",
          "206:     SerializationTestUtils.header(true)));",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "225:   KafkaOperations<?, ?> template = mock(KafkaOperations.class);",
          "226:   DeadLetterPublishingRecoverer recoverer = new DeadLetterPublishingRecoverer(template);",
          "227:   Headers headers = new RecordHeaders();",
          "229:   headers.add(SerializationTestUtils.deserializationHeader(SerializationUtils.KEY_DESERIALIZER_EXCEPTION_HEADER,",
          "231:   CompletableFuture future = new CompletableFuture();",
          "232:   future.complete(new Object());",
          "233:   willReturn(future).given(template).send(any(ProducerRecord.class));",
          "",
          "[Removed Lines]",
          "228:   DeserializationException deserEx = createDeserEx(true);",
          "230:     header(true, deserEx)));",
          "",
          "[Added Lines]",
          "225:   DeserializationException deserEx = SerializationTestUtils.createDeserEx(true);",
          "227:     SerializationTestUtils.header(deserEx)));",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "250:   recoverer.setRetainExceptionHeader(true);",
          "251:   Headers headers = new RecordHeaders();",
          "252:   headers.add(SerializationTestUtils.deserializationHeader(SerializationUtils.VALUE_DESERIALIZER_EXCEPTION_HEADER,",
          "254:   headers.add(SerializationTestUtils.deserializationHeader(SerializationUtils.KEY_DESERIALIZER_EXCEPTION_HEADER,",
          "256:   CompletableFuture future = new CompletableFuture();",
          "257:   future.complete(new Object());",
          "258:   willReturn(future).given(template).send(any(ProducerRecord.class));",
          "",
          "[Removed Lines]",
          "253:     header(false)));",
          "255:     header(true)));",
          "",
          "[Added Lines]",
          "250:     SerializationTestUtils.header(false)));",
          "252:     SerializationTestUtils.header(true)));",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "302:   verify(template2).send(any(ProducerRecord.class));",
          "303:  }",
          "326:  @SuppressWarnings({\"unchecked\", \"rawtypes\"})",
          "327:  @Test",
          "328:  void allOriginalHeaders() {",
          "",
          "[Removed Lines]",
          "305:  private byte[] header(boolean isKey) {",
          "306:   return header(isKey, createDeserEx(isKey));",
          "307:  }",
          "309:  private DeserializationException createDeserEx(boolean isKey) {",
          "310:   return new DeserializationException(",
          "311:     isKey ? \"testK\" : \"testV\",",
          "312:     isKey ? \"key\".getBytes() : \"value\".getBytes(), isKey, null);",
          "313:  }",
          "315:  private byte[] header(boolean isKey, DeserializationException deserEx) {",
          "316:   ByteArrayOutputStream baos = new ByteArrayOutputStream();",
          "317:   try {",
          "318:    new ObjectOutputStream(baos).writeObject(deserEx);",
          "319:   }",
          "320:   catch (IOException e) {",
          "321:    throw new UncheckedIOException(e);",
          "322:   }",
          "323:   return baos.toByteArray();",
          "324:  }",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "spring-kafka/src/test/java/org/springframework/kafka/support/DefaultKafkaHeaderMapperTests.java||spring-kafka/src/test/java/org/springframework/kafka/support/DefaultKafkaHeaderMapperTests.java": [
          "File: spring-kafka/src/test/java/org/springframework/kafka/support/DefaultKafkaHeaderMapperTests.java -> spring-kafka/src/test/java/org/springframework/kafka/support/DefaultKafkaHeaderMapperTests.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "34: import org.apache.kafka.common.header.internals.RecordHeaders;",
          "35: import org.junit.jupiter.api.Test;",
          "37: import org.springframework.kafka.support.DefaultKafkaHeaderMapper.NonTrustedHeaderType;",
          "38: import org.springframework.messaging.Message;",
          "39: import org.springframework.messaging.MessageHeaders;",
          "40: import org.springframework.messaging.support.ExecutorSubscribableChannel;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "37: import org.springframework.core.log.LogAccessor;",
          "39: import org.springframework.kafka.support.serializer.DeserializationException;",
          "40: import org.springframework.kafka.support.serializer.SerializationTestUtils;",
          "41: import org.springframework.kafka.support.serializer.SerializationUtils;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "321:     .containsKey(\"baz\");",
          "322:  }",
          "324:  public static final class Foo {",
          "326:   private String bar = \"bar\";",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "329:  @Test",
          "330:  void deserializationExceptionHeadersAreMappedAsNonByteArray() {",
          "331:   DefaultKafkaHeaderMapper mapper = new DefaultKafkaHeaderMapper();",
          "333:   byte[] keyDeserExceptionBytes = SerializationTestUtils.header(true);",
          "334:   Header keyHeader = SerializationTestUtils.deserializationHeader(SerializationUtils.KEY_DESERIALIZER_EXCEPTION_HEADER,",
          "335:     keyDeserExceptionBytes);",
          "336:   byte[] valueDeserExceptionBytes = SerializationTestUtils.header(false);",
          "337:   Header valueHeader = SerializationTestUtils.deserializationHeader(SerializationUtils.VALUE_DESERIALIZER_EXCEPTION_HEADER,",
          "338:     valueDeserExceptionBytes);",
          "339:   Headers headers = new RecordHeaders(",
          "340:     new Header[] { keyHeader, valueHeader });",
          "341:   Map<String, Object> springHeaders = new HashMap<>();",
          "342:   mapper.toHeaders(headers, springHeaders);",
          "343:   assertThat(springHeaders.get(SerializationUtils.KEY_DESERIALIZER_EXCEPTION_HEADER)).isEqualTo(keyHeader);",
          "344:   assertThat(springHeaders.get(SerializationUtils.VALUE_DESERIALIZER_EXCEPTION_HEADER)).isEqualTo(valueHeader);",
          "346:   LogAccessor logger = new LogAccessor(this.getClass());",
          "348:   DeserializationException keyDeserializationException = SerializationUtils.byteArrayToDeserializationException(logger, keyHeader);",
          "349:   assertThat(keyDeserExceptionBytes).containsExactly(SerializationTestUtils.header(keyDeserializationException));",
          "351:   DeserializationException valueDeserializationException =",
          "352:     SerializationUtils.byteArrayToDeserializationException(logger, valueHeader);",
          "353:   assertThat(valueDeserExceptionBytes).containsExactly(SerializationTestUtils.header(valueDeserializationException));",
          "355:   headers = new RecordHeaders();",
          "356:   mapper.fromHeaders(new MessageHeaders(springHeaders), headers);",
          "357:   assertThat(headers.lastHeader(SerializationUtils.KEY_DESERIALIZER_EXCEPTION_HEADER)).isNull();",
          "358:   assertThat(headers.lastHeader(SerializationUtils.VALUE_DESERIALIZER_EXCEPTION_HEADER)).isNull();",
          "359:  }",
          "",
          "---------------"
        ],
        "spring-kafka/src/test/java/org/springframework/kafka/support/serializer/SerializationTestUtils.java||spring-kafka/src/test/java/org/springframework/kafka/support/serializer/SerializationTestUtils.java": [
          "File: spring-kafka/src/test/java/org/springframework/kafka/support/serializer/SerializationTestUtils.java -> spring-kafka/src/test/java/org/springframework/kafka/support/serializer/SerializationTestUtils.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "17: package org.springframework.kafka.support.serializer;",
          "19: import org.apache.kafka.common.header.Header;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "19: import java.io.ByteArrayOutputStream;",
          "20: import java.io.IOException;",
          "21: import java.io.ObjectOutputStream;",
          "22: import java.io.UncheckedIOException;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "32:   return new DeserializationExceptionHeader(key, value);",
          "33:  }",
          "35: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "41:  public static byte[] header(boolean isKey) {",
          "42:   return header(createDeserEx(isKey));",
          "43:  }",
          "45:  public static DeserializationException createDeserEx(boolean isKey) {",
          "46:   return new DeserializationException(",
          "47:     isKey ? \"testK\" : \"testV\",",
          "48:     isKey ? \"key\".getBytes() : \"value\".getBytes(), isKey, null);",
          "49:  }",
          "51:  public static byte[] header(DeserializationException deserEx) {",
          "52:   ByteArrayOutputStream baos = new ByteArrayOutputStream();",
          "53:   try {",
          "54:    new ObjectOutputStream(baos).writeObject(deserEx);",
          "55:   }",
          "56:   catch (IOException e) {",
          "57:    throw new UncheckedIOException(e);",
          "58:   }",
          "59:   return baos.toByteArray();",
          "60:  }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "6bfdbdb0a08b2483a4a492edd834ed2a4e96643f",
      "candidate_info": {
        "commit_hash": "6bfdbdb0a08b2483a4a492edd834ed2a4e96643f",
        "repo": "spring-projects/spring-kafka",
        "commit_url": "https://github.com/spring-projects/spring-kafka/commit/6bfdbdb0a08b2483a4a492edd834ed2a4e96643f",
        "files": [
          "spring-kafka/src/main/java/org/springframework/kafka/support/serializer/SerializationUtils.java"
        ],
        "message": "GH-2784: Fix Invalid Warning Log Message\n\nResolves https://github.com/spring-projects/spring-kafka/issues/2784\n\n**cherry-pick to 2.9.x**",
        "before_after_code_files": [
          "spring-kafka/src/main/java/org/springframework/kafka/support/serializer/SerializationUtils.java||spring-kafka/src/main/java/org/springframework/kafka/support/serializer/SerializationUtils.java"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "spring-kafka/src/main/java/org/springframework/kafka/support/serializer/SerializationUtils.java||spring-kafka/src/main/java/org/springframework/kafka/support/serializer/SerializationUtils.java"
          ],
          "candidate": [
            "spring-kafka/src/main/java/org/springframework/kafka/support/serializer/SerializationUtils.java||spring-kafka/src/main/java/org/springframework/kafka/support/serializer/SerializationUtils.java"
          ]
        }
      },
      "candidate_diff": {
        "spring-kafka/src/main/java/org/springframework/kafka/support/serializer/SerializationUtils.java||spring-kafka/src/main/java/org/springframework/kafka/support/serializer/SerializationUtils.java": [
          "File: spring-kafka/src/main/java/org/springframework/kafka/support/serializer/SerializationUtils.java -> spring-kafka/src/main/java/org/springframework/kafka/support/serializer/SerializationUtils.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "194:    String headerName, LogAccessor logger) {",
          "196:   Header header = record.headers().lastHeader(headerName);",
          "198:    logger.warn(",
          "199:      () -> String.format(\"Foreign deserialization exception header in (%s) ignored; possible attack?\",",
          "200:        KafkaUtils.format(record)));",
          "",
          "[Removed Lines]",
          "197:   if (!(header instanceof DeserializationExceptionHeader)) {",
          "",
          "[Added Lines]",
          "197:   if (header != null && !(header instanceof DeserializationExceptionHeader)) {",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "224:  @Nullable",
          "225:  public static DeserializationException byteArrayToDeserializationException(LogAccessor logger, Header header) {",
          "228:    throw new IllegalStateException(\"Foreign deserialization exception header ignored; possible attack?\");",
          "229:   }",
          "230:   try {",
          "",
          "[Removed Lines]",
          "227:   if (!(header instanceof DeserializationExceptionHeader)) {",
          "",
          "[Added Lines]",
          "227:   if (header != null && !(header instanceof DeserializationExceptionHeader)) {",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "f0cba91a8f3a2e3c4818f8b0635772ed639ce7e9",
      "candidate_info": {
        "commit_hash": "f0cba91a8f3a2e3c4818f8b0635772ed639ce7e9",
        "repo": "spring-projects/spring-kafka",
        "commit_url": "https://github.com/spring-projects/spring-kafka/commit/f0cba91a8f3a2e3c4818f8b0635772ed639ce7e9",
        "files": [
          "spring-kafka/src/main/java/org/springframework/kafka/annotation/KafkaListenerAnnotationBeanPostProcessor.java",
          "spring-kafka/src/main/java/org/springframework/kafka/listener/KafkaMessageListenerContainer.java"
        ],
        "message": "KLABPP, KMLC - Java17 Improvements\n\n- instanceof\n- don't use reserved work `record`",
        "before_after_code_files": [
          "spring-kafka/src/main/java/org/springframework/kafka/annotation/KafkaListenerAnnotationBeanPostProcessor.java||spring-kafka/src/main/java/org/springframework/kafka/annotation/KafkaListenerAnnotationBeanPostProcessor.java",
          "spring-kafka/src/main/java/org/springframework/kafka/listener/KafkaMessageListenerContainer.java||spring-kafka/src/main/java/org/springframework/kafka/listener/KafkaMessageListenerContainer.java"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "spring-kafka/src/main/java/org/springframework/kafka/listener/KafkaMessageListenerContainer.java||spring-kafka/src/main/java/org/springframework/kafka/listener/KafkaMessageListenerContainer.java"
          ],
          "candidate": [
            "spring-kafka/src/main/java/org/springframework/kafka/listener/KafkaMessageListenerContainer.java||spring-kafka/src/main/java/org/springframework/kafka/listener/KafkaMessageListenerContainer.java"
          ]
        }
      },
      "candidate_diff": {
        "spring-kafka/src/main/java/org/springframework/kafka/annotation/KafkaListenerAnnotationBeanPostProcessor.java||spring-kafka/src/main/java/org/springframework/kafka/annotation/KafkaListenerAnnotationBeanPostProcessor.java": [
          "File: spring-kafka/src/main/java/org/springframework/kafka/annotation/KafkaListenerAnnotationBeanPostProcessor.java -> spring-kafka/src/main/java/org/springframework/kafka/annotation/KafkaListenerAnnotationBeanPostProcessor.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "269:  public void setBeanFactory(BeanFactory beanFactory) {",
          "270:   this.beanFactory = beanFactory;",
          "273:    this.expressionContext = new BeanExpressionContext((ConfigurableListableBeanFactory) beanFactory,",
          "274:      this.listenerScope);",
          "275:   }",
          "",
          "[Removed Lines]",
          "271:   if (beanFactory instanceof ConfigurableListableBeanFactory) {",
          "272:    this.resolver = ((ConfigurableListableBeanFactory) beanFactory).getBeanExpressionResolver();",
          "",
          "[Added Lines]",
          "271:   if (beanFactory instanceof ConfigurableListableBeanFactory clbf) {",
          "272:    this.resolver = clbf.getBeanExpressionResolver();",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "295:  public void afterSingletonsInstantiated() {",
          "296:   this.registrar.setBeanFactory(this.beanFactory);",
          "299:    Map<String, KafkaListenerConfigurer> instances =",
          "301:    for (KafkaListenerConfigurer configurer : instances.values()) {",
          "302:     configurer.configureKafkaListeners(this.registrar);",
          "303:    }",
          "",
          "[Removed Lines]",
          "298:   if (this.beanFactory instanceof ListableBeanFactory) {",
          "300:      ((ListableBeanFactory) this.beanFactory).getBeansOfType(KafkaListenerConfigurer.class);",
          "",
          "[Added Lines]",
          "298:   if (this.beanFactory instanceof ListableBeanFactory lbf) {",
          "300:      lbf.getBeansOfType(KafkaListenerConfigurer.class);",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "342:     List<AnnotationEnhancer> enhancers = enhancersMap.values()",
          "343:       .stream()",
          "344:       .sorted(new OrderComparator())",
          "346:     this.enhancer = (attrs, element) -> {",
          "347:      Map<String, Object> newAttrs = attrs;",
          "348:      for (AnnotationEnhancer enh : enhancers) {",
          "",
          "[Removed Lines]",
          "345:       .collect(Collectors.toList());",
          "",
          "[Added Lines]",
          "345:       .toList();",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "364:   if (!this.nonAnnotatedClasses.contains(bean.getClass())) {",
          "365:    Class<?> targetClass = AopUtils.getTargetClass(bean);",
          "366:    Collection<KafkaListener> classLevelListeners = findListenerAnnotations(targetClass);",
          "368:    final List<Method> multiMethods = new ArrayList<>();",
          "369:    Map<Method, Set<KafkaListener>> annotatedMethods = MethodIntrospector.selectMethods(targetClass,",
          "370:      (MethodIntrospector.MetadataLookup<Set<KafkaListener>>) method -> {",
          "",
          "[Removed Lines]",
          "367:    final boolean hasClassLevelListeners = classLevelListeners.size() > 0;",
          "",
          "[Added Lines]",
          "367:    final boolean hasClassLevelListeners = !classLevelListeners.isEmpty();",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "432:   if (anns != null) {",
          "433:    listeners.addAll(Arrays.stream(anns.value())",
          "434:      .map(anno -> enhance(method, anno))",
          "436:   }",
          "437:   return listeners;",
          "438:  }",
          "",
          "[Removed Lines]",
          "435:      .collect(Collectors.toList()));",
          "",
          "[Added Lines]",
          "435:      .toList());",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "500:    retryableCandidates = Arrays.stream(tps)",
          "501:      .map(tp -> tp.getTopic())",
          "502:      .distinct()",
          "504:      .toArray(new String[0]);",
          "505:   }",
          "",
          "[Removed Lines]",
          "503:      .collect(Collectors.toList())",
          "",
          "[Added Lines]",
          "503:      .toList()",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "541:  }",
          "543:  private RetryTopicConfigurer createDefaultConfigurer() {",
          "546:    gac.registerBean(",
          "547:      RetryTopicBeanNames.DEFAULT_RETRY_TOPIC_CONFIG_SUPPORT_BEAN_NAME,",
          "548:      RetryTopicConfigurationSupport.class,",
          "550:    RetryTopicConfigurationSupport rtcs = this.applicationContext.getBean(",
          "551:      RetryTopicBeanNames.DEFAULT_RETRY_TOPIC_CONFIG_SUPPORT_BEAN_NAME,",
          "552:      RetryTopicConfigurationSupport.class);",
          "",
          "[Removed Lines]",
          "544:   if (this.applicationContext instanceof GenericApplicationContext) {",
          "545:    GenericApplicationContext gac = (GenericApplicationContext) this.applicationContext;",
          "549:      () -> new RetryTopicConfigurationSupport());",
          "",
          "[Added Lines]",
          "544:   if (this.applicationContext instanceof GenericApplicationContext gac) {",
          "548:      RetryTopicConfigurationSupport::new);",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "638:   String group = kafkaListener.containerGroup();",
          "639:   if (StringUtils.hasText(group)) {",
          "640:    Object resolvedGroup = resolveExpression(group);",
          "643:    }",
          "644:   }",
          "645:   String concurrency = kafkaListener.concurrency();",
          "",
          "[Removed Lines]",
          "641:    if (resolvedGroup instanceof String) {",
          "642:     endpoint.setGroup((String) resolvedGroup);",
          "",
          "[Added Lines]",
          "640:    if (resolvedGroup instanceof String str) {",
          "641:     endpoint.setGroup(str);",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "664:  private void resolveErrorHandler(MethodKafkaListenerEndpoint<?, ?> endpoint, KafkaListener kafkaListener) {",
          "665:   Object errorHandler = resolveExpression(kafkaListener.errorHandler());",
          "668:   }",
          "669:   else {",
          "670:    String errorHandlerBeanName = resolveExpressionAsString(kafkaListener.errorHandler(), \"errorHandler\");",
          "",
          "[Removed Lines]",
          "666:   if (errorHandler instanceof KafkaListenerErrorHandler) {",
          "667:    endpoint.setErrorHandler((KafkaListenerErrorHandler) errorHandler);",
          "",
          "[Added Lines]",
          "665:   if (errorHandler instanceof KafkaListenerErrorHandler kleh) {",
          "666:    endpoint.setErrorHandler(kleh);",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "678:  private void resolveContentTypeConverter(MethodKafkaListenerEndpoint<?, ?> endpoint, KafkaListener kafkaListener) {",
          "679:   Object converter = resolveExpression(kafkaListener.contentTypeConverter());",
          "682:   }",
          "683:   else {",
          "684:    String converterBeanName = resolveExpressionAsString(kafkaListener.contentTypeConverter(),",
          "",
          "[Removed Lines]",
          "680:   if (converter instanceof SmartMessageConverter) {",
          "681:    endpoint.setMessagingConverter((SmartMessageConverter) converter);",
          "",
          "[Added Lines]",
          "679:   if (converter instanceof SmartMessageConverter smc) {",
          "680:    endpoint.setMessagingConverter(smc);",
          "",
          "---------------",
          "--- Hunk 11 ---",
          "[Context before]",
          "693:  @SuppressWarnings({ \"rawtypes\", UNCHECKED })",
          "694:  private void resolveFilter(MethodKafkaListenerEndpoint<?, ?> endpoint, KafkaListener kafkaListener) {",
          "695:   Object filter = resolveExpression(kafkaListener.filter());",
          "698:   }",
          "699:   else {",
          "700:    String filterBeanName = resolveExpressionAsString(kafkaListener.filter(), \"filter\");",
          "",
          "[Removed Lines]",
          "696:   if (filter instanceof RecordFilterStrategy) {",
          "697:    endpoint.setRecordFilterStrategy((RecordFilterStrategy) filter);",
          "",
          "[Added Lines]",
          "695:   if (filter instanceof RecordFilterStrategy rfs) {",
          "696:    endpoint.setRecordFilterStrategy(rfs);",
          "",
          "---------------",
          "--- Hunk 12 ---",
          "[Context before]",
          "757:     if (value instanceof String) {",
          "758:      loadProperty(properties, property, value);",
          "759:     }",
          "762:       loadProperty(properties, prop, prop);",
          "763:      }",
          "764:     }",
          "765:     else if (value instanceof Collection) {",
          "766:      Collection<?> values = (Collection<?>) value;",
          "768:       for (String prop : (Collection<String>) value) {",
          "769:        loadProperty(properties, prop, prop);",
          "770:       }",
          "",
          "[Removed Lines]",
          "760:     else if (value instanceof String[]) {",
          "761:      for (String prop : (String[]) value) {",
          "767:      if (values.size() > 0 && values.iterator().next() instanceof String) {",
          "",
          "[Added Lines]",
          "759:     else if (value instanceof String[] strArr) {",
          "760:      for (String prop : strArr) {",
          "766:      if (!values.isEmpty() && values.iterator().next() instanceof String) {",
          "",
          "---------------",
          "--- Hunk 13 ---",
          "[Context before]",
          "836:   String text = kafkaListener.topicPattern();",
          "837:   if (StringUtils.hasText(text)) {",
          "838:    Object resolved = resolveExpression(text);",
          "841:    }",
          "844:    }",
          "845:    else if (resolved != null) {",
          "846:     throw new IllegalStateException(",
          "",
          "[Removed Lines]",
          "839:    if (resolved instanceof Pattern) {",
          "840:     pattern = (Pattern) resolved;",
          "842:    else if (resolved instanceof String) {",
          "843:     pattern = Pattern.compile((String) resolved);",
          "",
          "[Added Lines]",
          "838:    if (resolved instanceof Pattern pat) {",
          "839:     pattern = pat;",
          "841:    else if (resolved instanceof String str) {",
          "842:     pattern = Pattern.compile(str);",
          "",
          "---------------",
          "--- Hunk 14 ---",
          "[Context before]",
          "877:       resolveInitialOffset(topic, partitionOffset), isRelative(topic, partitionOffset), true);",
          "878:    }",
          "879:   }",
          "881:   return result;",
          "882:  }",
          "884:  private Long resolveInitialOffset(Object topic, PartitionOffset partitionOffset) {",
          "885:   Object initialOffsetValue = resolveExpression(partitionOffset.initialOffset());",
          "886:   Long initialOffset;",
          "889:      () -> \"'initialOffset' in @PartitionOffset for topic '\" + topic + \"' cannot be empty\");",
          "891:   }",
          "894:   }",
          "895:   else {",
          "896:    throw new IllegalArgumentException(String.format(",
          "",
          "[Removed Lines]",
          "880:   Assert.isTrue(result.size() > 0, () -> \"At least one partition required for \" + topic);",
          "887:   if (initialOffsetValue instanceof String) {",
          "888:    Assert.state(StringUtils.hasText((String) initialOffsetValue),",
          "890:    initialOffset = Long.valueOf((String) initialOffsetValue);",
          "892:   else if (initialOffsetValue instanceof Long) {",
          "893:    initialOffset = (Long) initialOffsetValue;",
          "",
          "[Added Lines]",
          "879:   Assert.isTrue(!result.isEmpty(), () -> \"At least one partition required for \" + topic);",
          "886:   if (initialOffsetValue instanceof String str) {",
          "887:    Assert.state(StringUtils.hasText(str),",
          "889:    initialOffset = Long.valueOf(str);",
          "891:   else if (initialOffsetValue instanceof Long lng) {",
          "892:    initialOffset = lng;",
          "",
          "---------------",
          "--- Hunk 15 ---",
          "[Context before]",
          "903:  private boolean isRelative(Object topic, PartitionOffset partitionOffset) {",
          "904:   Object relativeToCurrentValue = resolveExpression(partitionOffset.relativeToCurrent());",
          "905:   Boolean relativeToCurrent;",
          "908:   }",
          "911:   }",
          "912:   else {",
          "913:    throw new IllegalArgumentException(String.format(",
          "",
          "[Removed Lines]",
          "906:   if (relativeToCurrentValue instanceof String) {",
          "907:    relativeToCurrent = Boolean.valueOf((String) relativeToCurrentValue);",
          "909:   else if (relativeToCurrentValue instanceof Boolean) {",
          "910:    relativeToCurrent = (Boolean) relativeToCurrentValue;",
          "",
          "[Added Lines]",
          "905:   if (relativeToCurrentValue instanceof String str) {",
          "906:    relativeToCurrent = Boolean.valueOf(str);",
          "908:   else if (relativeToCurrentValue instanceof Boolean bool) {",
          "909:    relativeToCurrent = bool;",
          "",
          "---------------",
          "--- Hunk 16 ---",
          "[Context before]",
          "920:  @SuppressWarnings(UNCHECKED)",
          "921:  private void resolveAsString(Object resolvedValue, List<String> result) {",
          "924:     resolveAsString(object, result);",
          "925:    }",
          "926:   }",
          "929:   }",
          "930:   else if (resolvedValue instanceof Iterable) {",
          "931:    for (Object object : (Iterable<Object>) resolvedValue) {",
          "",
          "[Removed Lines]",
          "922:   if (resolvedValue instanceof String[]) {",
          "923:    for (Object object : (String[]) resolvedValue) {",
          "927:   else if (resolvedValue instanceof String) {",
          "928:    result.add((String) resolvedValue);",
          "",
          "[Added Lines]",
          "921:   if (resolvedValue instanceof String[] strArr) {",
          "922:    for (Object object : strArr) {",
          "926:   else if (resolvedValue instanceof String str) {",
          "927:    result.add(str);",
          "",
          "---------------",
          "--- Hunk 17 ---",
          "[Context before]",
          "942:  private void resolvePartitionAsInteger(String topic, Object resolvedValue,",
          "943:    List<TopicPartitionOffset> result, @Nullable Long offset, boolean isRelative, boolean checkDups) {",
          "947:     resolvePartitionAsInteger(topic, object, result, offset, isRelative, checkDups);",
          "948:    }",
          "949:   }",
          "952:      () -> \"partition in @TopicPartition for topic '\" + topic + \"' cannot be empty\");",
          "954:      .map(part -> new TopicPartitionOffset(topic, part, offset, isRelative))",
          "956:    if (checkDups) {",
          "957:     collected.forEach(tpo -> {",
          "958:      Assert.state(!result.contains(tpo), () ->",
          "",
          "[Removed Lines]",
          "945:   if (resolvedValue instanceof String[]) {",
          "946:    for (Object object : (String[]) resolvedValue) {",
          "950:   else if (resolvedValue instanceof String) {",
          "951:    Assert.state(StringUtils.hasText((String) resolvedValue),",
          "953:    List<TopicPartitionOffset> collected = parsePartitions((String) resolvedValue)",
          "955:      .collect(Collectors.toList());",
          "",
          "[Added Lines]",
          "944:   if (resolvedValue instanceof String[] strArr) {",
          "945:    for (Object object : strArr) {",
          "949:   else if (resolvedValue instanceof String str) {",
          "950:    Assert.state(StringUtils.hasText(str),",
          "952:    List<TopicPartitionOffset> collected = parsePartitions(str)",
          "954:      .toList();",
          "",
          "---------------",
          "--- Hunk 18 ---",
          "[Context before]",
          "962:    }",
          "963:    result.addAll(collected);",
          "964:   }",
          "967:     result.add(new TopicPartitionOffset(topic, partition));",
          "968:    }",
          "969:   }",
          "972:   }",
          "973:   else if (resolvedValue instanceof Iterable) {",
          "974:    for (Object object : (Iterable<Object>) resolvedValue) {",
          "",
          "[Removed Lines]",
          "965:   else if (resolvedValue instanceof Integer[]) {",
          "966:    for (Integer partition : (Integer[]) resolvedValue) {",
          "970:   else if (resolvedValue instanceof Integer) {",
          "971:    result.add(new TopicPartitionOffset(topic, (Integer) resolvedValue));",
          "",
          "[Added Lines]",
          "964:   else if (resolvedValue instanceof Integer[] intArr) {",
          "965:    for (Integer partition : intArr) {",
          "969:   else if (resolvedValue instanceof Integer intgr) {",
          "970:    result.add(new TopicPartitionOffset(topic, intgr));",
          "",
          "---------------",
          "--- Hunk 19 ---",
          "[Context before]",
          "984:  private String resolveExpressionAsString(String value, String attribute) {",
          "985:   Object resolved = resolveExpression(value);",
          "988:   }",
          "989:   else if (resolved != null) {",
          "990:    throw new IllegalStateException(THE_LEFT + attribute + \"] must resolve to a String. \"",
          "",
          "[Removed Lines]",
          "986:   if (resolved instanceof String) {",
          "987:    return (String) resolved;",
          "",
          "[Added Lines]",
          "985:   if (resolved instanceof String str) {",
          "986:    return str;",
          "",
          "---------------",
          "--- Hunk 20 ---",
          "[Context before]",
          "996:  @Nullable",
          "997:  private byte[] resolveExpressionAsBytes(String value, String attribute) {",
          "998:   Object resolved = resolveExpression(value);",
          "1002:    }",
          "1003:   }",
          "1006:   }",
          "1007:   else if (resolved != null) {",
          "1008:    throw new IllegalStateException(THE_LEFT + attribute + \"] must resolve to a String or byte[]. \"",
          "",
          "[Removed Lines]",
          "999:   if (resolved instanceof String) {",
          "1000:    if (StringUtils.hasText((CharSequence) resolved)) {",
          "1001:     return ((String) resolved).getBytes(this.charset);",
          "1004:   else if (resolved instanceof byte[]) {",
          "1005:    return (byte[]) resolved;",
          "",
          "[Added Lines]",
          "998:   if (resolved instanceof String str) {",
          "999:    if (StringUtils.hasText(str)) {",
          "1000:     return str.getBytes(this.charset);",
          "1003:   else if (resolved instanceof byte[] bytes) {",
          "1004:    return bytes;",
          "",
          "---------------",
          "--- Hunk 21 ---",
          "[Context before]",
          "1014:  private Integer resolveExpressionAsInteger(String value, String attribute) {",
          "1015:   Object resolved = resolveExpression(value);",
          "1016:   Integer result = null;",
          "1019:   }",
          "1022:   }",
          "1023:   else if (resolved != null) {",
          "1024:    throw new IllegalStateException(",
          "",
          "[Removed Lines]",
          "1017:   if (resolved instanceof String) {",
          "1018:    result = Integer.parseInt((String) resolved);",
          "1020:   else if (resolved instanceof Number) {",
          "1021:    result = ((Number) resolved).intValue();",
          "",
          "[Added Lines]",
          "1016:   if (resolved instanceof String str) {",
          "1017:    result = Integer.parseInt(str);",
          "1019:   else if (resolved instanceof Number nbr) {",
          "1020:    result = nbr.intValue();",
          "",
          "---------------",
          "--- Hunk 22 ---",
          "[Context before]",
          "1031:  private Boolean resolveExpressionAsBoolean(String value, String attribute) {",
          "1032:   Object resolved = resolveExpression(value);",
          "1033:   Boolean result = null;",
          "1036:   }",
          "1039:   }",
          "1040:   else if (resolved != null) {",
          "1041:    throw new IllegalStateException(",
          "",
          "[Removed Lines]",
          "1034:   if (resolved instanceof Boolean) {",
          "1035:    result = (Boolean) resolved;",
          "1037:   else if (resolved instanceof String) {",
          "1038:    result = Boolean.parseBoolean((String) resolved);",
          "",
          "[Added Lines]",
          "1033:   if (resolved instanceof Boolean bool) {",
          "1034:    result = bool;",
          "1036:   else if (resolved instanceof String str) {",
          "1037:    result = Boolean.parseBoolean(str);",
          "",
          "---------------",
          "--- Hunk 23 ---",
          "[Context before]",
          "1058:  private String resolve(String value) {",
          "1061:   }",
          "1062:   return value;",
          "1063:  }",
          "",
          "[Removed Lines]",
          "1059:   if (this.beanFactory != null && this.beanFactory instanceof ConfigurableBeanFactory) {",
          "1060:    return ((ConfigurableBeanFactory) this.beanFactory).resolveEmbeddedValue(value);",
          "",
          "[Added Lines]",
          "1058:   if (this.beanFactory instanceof ConfigurableBeanFactory cbf) {",
          "1059:    return cbf.resolveEmbeddedValue(value);",
          "",
          "---------------",
          "--- Hunk 24 ---",
          "[Context before]",
          "1078:  }",
          "1080:  private <T> Collection<T> getBeansOfType(Class<T> type) {",
          "1084:      .values();",
          "1085:   }",
          "1086:   else {",
          "",
          "[Removed Lines]",
          "1081:   if (KafkaListenerAnnotationBeanPostProcessor.this.beanFactory instanceof ListableBeanFactory) {",
          "1082:    return ((ListableBeanFactory) KafkaListenerAnnotationBeanPostProcessor.this.beanFactory)",
          "1083:      .getBeansOfType(type)",
          "",
          "[Added Lines]",
          "1080:   if (KafkaListenerAnnotationBeanPostProcessor.this.beanFactory instanceof ListableBeanFactory lbf) {",
          "1081:    return lbf.getBeansOfType(type)",
          "",
          "---------------",
          "--- Hunk 25 ---",
          "[Context before]",
          "1276:     return ByteBuffer.wrap(bytes).getInt();",
          "1277:    }",
          "1278:    else if (targetType.getType().equals(short.class) || targetType.getType().equals(Short.class)) {",
          "1280:     return ByteBuffer.wrap(bytes).getShort();",
          "1281:    }",
          "1282:    else if (targetType.getType().equals(byte.class) || targetType.getType().equals(Byte.class)) {",
          "1284:     return ByteBuffer.wrap(bytes).get();",
          "1285:    }",
          "1286:    return null;",
          "",
          "[Removed Lines]",
          "1279:     Assert.state(bytes.length >= 2, \"At least 2 bytes needed to convert a byte[] to a short\");",
          "1283:     Assert.state(bytes.length >= 1, \"At least 1 byte needed to convert a byte[] to a byte\");",
          "",
          "[Added Lines]",
          "1277:     Assert.state(bytes.length >= 2, \"At least 2 bytes needed to convert a byte[] to a short\"); // NOSONAR",
          "1281:     Assert.state(bytes.length >= 1, \"At least 1 byte needed to convert a byte[] to a byte\"); // NOSONAR",
          "",
          "---------------"
        ],
        "spring-kafka/src/main/java/org/springframework/kafka/listener/KafkaMessageListenerContainer.java||spring-kafka/src/main/java/org/springframework/kafka/listener/KafkaMessageListenerContainer.java": [
          "File: spring-kafka/src/main/java/org/springframework/kafka/listener/KafkaMessageListenerContainer.java -> spring-kafka/src/main/java/org/springframework/kafka/listener/KafkaMessageListenerContainer.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "411:   ListenerType listenerType = ListenerUtils.determineListenerType(listener);",
          "412:   if (listener instanceof DelegatingMessageListener) {",
          "413:    Object delegating = listener;",
          "416:    }",
          "417:    listenerType = ListenerUtils.determineListenerType(delegating);",
          "418:   }",
          "",
          "[Removed Lines]",
          "414:    while (delegating instanceof DelegatingMessageListener) {",
          "415:     delegating = ((DelegatingMessageListener<?>) delegating).getDelegate();",
          "",
          "[Added Lines]",
          "414:    while (delegating instanceof DelegatingMessageListener<?> dml) {",
          "415:     delegating = dml.getDelegate();",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1027:    if (!StringUtils.hasText(groupInstance)) {",
          "1028:     Object factoryConfig = consumerFactory.getConfigurationProperties()",
          "1029:       .get(ConsumerConfig.GROUP_INSTANCE_ID_CONFIG);",
          "1032:     }",
          "1033:    }",
          "1034:    if (StringUtils.hasText(KafkaMessageListenerContainer.this.clientIdSuffix)",
          "",
          "[Removed Lines]",
          "1030:     if (factoryConfig instanceof String) {",
          "1031:      groupInstance = (String) factoryConfig;",
          "",
          "[Added Lines]",
          "1030:     if (factoryConfig instanceof String str) {",
          "1031:      groupInstance = str;",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "1052:    DeliveryAttemptAware aware = null;",
          "1053:    if (this.containerProperties.isDeliveryAttemptHeader()) {",
          "1054:     if (this.transactionManager != null) {",
          "1057:      }",
          "1058:     }",
          "1059:     else {",
          "",
          "[Removed Lines]",
          "1055:      if (getAfterRollbackProcessor() instanceof DeliveryAttemptAware) {",
          "1056:       aware = (DeliveryAttemptAware) getAfterRollbackProcessor();",
          "",
          "[Added Lines]",
          "1055:      if (getAfterRollbackProcessor() instanceof DeliveryAttemptAware daa) {",
          "1056:       aware = daa;",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "1075:    String autoOffsetReset = consumerProperties.getProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG);",
          "1076:    if (autoOffsetReset == null) {",
          "1077:     Object config = factoryConfigs.get(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG);",
          "1080:     }",
          "1081:    }",
          "1082:    boolean resetLatest = autoOffsetReset == null || autoOffsetReset.equals(\"latest\");",
          "",
          "[Removed Lines]",
          "1078:     if (config instanceof String) {",
          "1079:      autoOffsetReset = (String) config;",
          "",
          "[Added Lines]",
          "1078:     if (config instanceof String str) {",
          "1079:      autoOffsetReset = str;",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "1092:       .get(ConsumerConfig.MAX_POLL_INTERVAL_MS_CONFIG);",
          "1093:    }",
          "1097:    }",
          "1100:    }",
          "1103:    }",
          "1104:    else {",
          "1105:     if (timeout != null) {",
          "",
          "[Removed Lines]",
          "1095:    if (timeout instanceof Duration) {",
          "1096:     return ((Duration) timeout).toMillis();",
          "1098:    else if (timeout instanceof Number) {",
          "1099:     return ((Number) timeout).longValue();",
          "1101:    else if (timeout instanceof String) {",
          "1102:     return Long.parseLong((String) timeout);",
          "",
          "[Added Lines]",
          "1095:    if (timeout instanceof Duration dur) {",
          "1096:     return dur.toMillis();",
          "1098:    else if (timeout instanceof Number nbr) {",
          "1099:     return nbr.longValue();",
          "1101:    else if (timeout instanceof String str) {",
          "1102:     return Long.parseLong(str);",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "1116:   @Nullable",
          "1117:   private ConsumerSeekAware checkConsumerSeekAware(GenericMessageListener<?> candidate) {",
          "1119:   }",
          "1121:   boolean isConsumerPaused() {",
          "",
          "[Removed Lines]",
          "1118:    return candidate instanceof ConsumerSeekAware ? (ConsumerSeekAware) candidate : null;",
          "",
          "[Added Lines]",
          "1118:    return candidate instanceof ConsumerSeekAware csa ? csa : null;",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "1178:      timeout = KafkaMessageListenerContainer.this.consumerFactory.getConfigurationProperties()",
          "1179:        .get(ConsumerConfig.DEFAULT_API_TIMEOUT_MS_CONFIG);",
          "1180:     }",
          "1183:     }",
          "1186:     }",
          "1189:     }",
          "1190:     else {",
          "1191:      if (timeout != null) {",
          "",
          "[Removed Lines]",
          "1181:     if (timeout instanceof Duration) {",
          "1182:      return (Duration) timeout;",
          "1184:     else if (timeout instanceof Number) {",
          "1185:      return Duration.ofMillis(((Number) timeout).longValue());",
          "1187:     else if (timeout instanceof String) {",
          "1188:      return Duration.ofMillis(Long.parseLong((String) timeout));",
          "",
          "[Added Lines]",
          "1181:     if (timeout instanceof Duration dur) {",
          "1182:      return dur;",
          "1184:     else if (timeout instanceof Number nbr) {",
          "1185:      return Duration.ofMillis(nbr.longValue());",
          "1187:     else if (timeout instanceof String str) {",
          "1188:      return Duration.ofMillis(Long.parseLong(str));",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "1250:   private boolean checkDeserializer(@Nullable Object deser) {",
          "1251:    Class<?> deserializer = null;",
          "1254:    }",
          "1256:     try {",
          "1257:      ApplicationContext applicationContext = getApplicationContext();",
          "1258:      ClassLoader classLoader = applicationContext == null",
          "1259:        ? getClass().getClassLoader()",
          "1260:        : applicationContext.getClassLoader();",
          "1262:     }",
          "1263:     catch (ClassNotFoundException | LinkageError e) {",
          "1264:      throw new IllegalStateException(e);",
          "",
          "[Removed Lines]",
          "1252:    if (deser instanceof Class) {",
          "1253:     deserializer = (Class<?>) deser;",
          "1255:    else if (deser instanceof String) {",
          "1261:      deserializer = ClassUtils.forName((String) deser, classLoader);",
          "",
          "[Added Lines]",
          "1252:    if (deser instanceof Class<?> deserClass) {",
          "1253:     deserializer = deserClass;",
          "1255:    else if (deser instanceof String str) {",
          "1261:      deserializer = ClassUtils.forName(str, classLoader);",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "1267:    else if (deser != null) {",
          "1268:     throw new IllegalStateException(\"Deserializer must be a class or class name, not a \" + deser.getClass());",
          "1269:    }",
          "1271:   }",
          "1273:   protected void checkConsumer() {",
          "",
          "[Removed Lines]",
          "1270:    return deserializer == null ? false : ErrorHandlingDeserializer.class.isAssignableFrom(deserializer);",
          "",
          "[Added Lines]",
          "1270:    return deserializer != null && ErrorHandlingDeserializer.class.isAssignableFrom(deserializer);",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "1973:   private void handleAcks() {",
          "1979:    }",
          "1980:   }",
          "1984:   }",
          "1988:    if (this.offsetsInThisBatch != null) { // NOSONAR (sync)",
          "1990:    }",
          "1991:    else {",
          "1993:    }",
          "1994:   }",
          "1997:    if (!Thread.currentThread().equals(this.consumerThread)) {",
          "1998:     try {",
          "2000:      if (this.isManualImmediateAck || this.pausedForAsyncAcks) {  // NOSONAR (sync)",
          "2001:       this.consumer.wakeup();",
          "2002:      }",
          "",
          "[Removed Lines]",
          "1974:    ConsumerRecord<K, V> record = this.acks.poll();",
          "1975:    while (record != null) {",
          "1976:     traceAck(record);",
          "1977:     processAck(record);",
          "1978:     record = this.acks.poll();",
          "1982:   private void traceAck(ConsumerRecord<K, V> record) {",
          "1983:    this.logger.trace(() -> \"Ack: \" + KafkaUtils.format(record));",
          "1986:   private void doAck(ConsumerRecord<K, V> record) {",
          "1987:    traceAck(record);",
          "1989:     ackInOrder(record);",
          "1992:     processAck(record);",
          "1996:   private void processAck(ConsumerRecord<K, V> record) {",
          "1999:      this.acks.put(record);",
          "",
          "[Added Lines]",
          "1974:    ConsumerRecord<K, V> cRecord = this.acks.poll();",
          "1975:    while (cRecord != null) {",
          "1976:     traceAck(cRecord);",
          "1977:     processAck(cRecord);",
          "1978:     cRecord = this.acks.poll();",
          "1982:   private void traceAck(ConsumerRecord<K, V> cRecord) {",
          "1983:    this.logger.trace(() -> \"Ack: \" + KafkaUtils.format(cRecord));",
          "1986:   private void doAck(ConsumerRecord<K, V> cRecord) {",
          "1987:    traceAck(cRecord);",
          "1989:     ackInOrder(cRecord);",
          "1992:     processAck(cRecord);",
          "1996:   private void processAck(ConsumerRecord<K, V> cRecord) {",
          "1999:      this.acks.put(cRecord);",
          "",
          "---------------",
          "--- Hunk 11 ---",
          "[Context before]",
          "2009:    else {",
          "2010:     if (this.isManualImmediateAck) {",
          "2011:      try {",
          "2013:      }",
          "2014:      catch (@SuppressWarnings(UNUSED) WakeupException e) {",
          "2016:      }",
          "2017:     }",
          "2018:     else {",
          "2020:     }",
          "2021:    }",
          "2022:   }",
          "",
          "[Removed Lines]",
          "2012:       ackImmediate(record);",
          "2019:      addOffset(record);",
          "",
          "[Added Lines]",
          "2012:       ackImmediate(cRecord);",
          "2019:      addOffset(cRecord);",
          "",
          "---------------",
          "--- Hunk 12 ---",
          "[Context before]",
          "2024:   private void processAcks(ConsumerRecords<K, V> records) {",
          "2025:    if (!Thread.currentThread().equals(this.consumerThread)) {",
          "2026:     try {",
          "2029:      }",
          "2030:      if (this.isManualImmediateAck) {",
          "2031:       this.consumer.wakeup();",
          "",
          "[Removed Lines]",
          "2027:      for (ConsumerRecord<K, V> record : records) {",
          "2028:       this.acks.put(record);",
          "",
          "[Added Lines]",
          "2027:      for (ConsumerRecord<K, V> cRecord : records) {",
          "2028:       this.acks.put(cRecord);",
          "",
          "---------------",
          "--- Hunk 13 ---",
          "[Context before]",
          "2046:      }",
          "2047:     }",
          "2048:     else {",
          "2051:      }",
          "2052:     }",
          "2053:    }",
          "2054:   }",
          "2058:    List<Long> offs = this.offsetsInThisBatch.get(part);",
          "2059:    List<ConsumerRecord<K, V>> deferred = this.deferredOffsets.get(part);",
          "2060:    if (!offs.isEmpty()) {",
          "2062:      offs.remove(0);",
          "2064:      if (!deferred.isEmpty()) {",
          "2065:       Collections.sort(deferred, (a, b) -> Long.compare(a.offset(), b.offset()));",
          "2066:       while (!deferred.isEmpty() && deferred.get(0).offset() == recordToAck.offset() + 1) {",
          "",
          "[Removed Lines]",
          "2049:      for (ConsumerRecord<K, V> record : records) {",
          "2050:       addOffset(record);",
          "2056:   private synchronized void ackInOrder(ConsumerRecord<K, V> record) {",
          "2057:    TopicPartition part = new TopicPartition(record.topic(), record.partition());",
          "2061:     if (offs.get(0) == record.offset()) {",
          "2063:      ConsumerRecord<K, V> recordToAck = record;",
          "",
          "[Added Lines]",
          "2049:      for (ConsumerRecord<K, V> cRecord : records) {",
          "2050:       addOffset(cRecord);",
          "2056:   private synchronized void ackInOrder(ConsumerRecord<K, V> cRecord) {",
          "2057:    TopicPartition part = new TopicPartition(cRecord.topic(), cRecord.partition());",
          "2061:     if (offs.get(0) == cRecord.offset()) {",
          "2063:      ConsumerRecord<K, V> recordToAck = cRecord;",
          "",
          "---------------",
          "--- Hunk 14 ---",
          "[Context before]",
          "2074:       this.offsetsInThisBatch.remove(part);",
          "2075:      }",
          "2076:     }",
          "2078:      throw new IllegalStateException(\"First remaining offset for this batch is \" + offs.get(0)",
          "2080:     }",
          "2081:     else {",
          "2083:     }",
          "2084:    }",
          "2085:    else {",
          "2087:       + \"; offsets list is empty\");",
          "2088:    }",
          "2089:   }",
          "2092:    Map<TopicPartition, OffsetAndMetadata> commits = Collections.singletonMap(",
          "2095:    this.commitLogger.log(() -> COMMITTING + commits);",
          "2096:    if (this.producer != null) {",
          "2097:     doSendOffsets(this.producer, commits);",
          "",
          "[Removed Lines]",
          "2077:     else if (record.offset() < offs.get(0)) {",
          "2079:        + \"; you are acknowledging a stale record: \" + KafkaUtils.format(record));",
          "2082:      deferred.add(record);",
          "2086:     throw new IllegalStateException(\"Unexpected ack for \" + KafkaUtils.format(record)",
          "2091:   private void ackImmediate(ConsumerRecord<K, V> record) {",
          "2093:      new TopicPartition(record.topic(), record.partition()),",
          "2094:      createOffsetAndMetadata(record.offset() + 1));",
          "",
          "[Added Lines]",
          "2077:     else if (cRecord.offset() < offs.get(0)) {",
          "2079:        + \"; you are acknowledging a stale record: \" + KafkaUtils.format(cRecord));",
          "2082:      deferred.add(cRecord);",
          "2086:     throw new IllegalStateException(\"Unexpected ack for \" + KafkaUtils.format(cRecord)",
          "2091:   private void ackImmediate(ConsumerRecord<K, V> cRecord) {",
          "2093:      new TopicPartition(cRecord.topic(), cRecord.partition()),",
          "2094:      createOffsetAndMetadata(cRecord.offset() + 1));",
          "",
          "---------------",
          "--- Hunk 15 ---",
          "[Context before]",
          "2374:    if (this.nackSleepDurationMillis >= 0) {",
          "2375:     int index = 0;",
          "2376:     toSeek = new ArrayList<>();",
          "2378:      if (index++ >= this.nackIndex) {",
          "2380:      }",
          "2381:     }",
          "2382:    }",
          "",
          "[Removed Lines]",
          "2377:     for (ConsumerRecord<K, V> record : records) {",
          "2379:       toSeek.add(record);",
          "",
          "[Added Lines]",
          "2377:     for (ConsumerRecord<K, V> cRecord : records) {",
          "2379:       toSeek.add(cRecord);",
          "",
          "---------------",
          "--- Hunk 16 ---",
          "[Context before]",
          "2398:   }",
          "2400:   private void ackBatch(final ConsumerRecords<K, V> records) throws InterruptedException {",
          "2403:    }",
          "2404:   }",
          "",
          "[Removed Lines]",
          "2401:    for (ConsumerRecord<K, V> record : getHighestOffsetRecords(records)) {",
          "2402:     this.acks.put(record);",
          "",
          "[Added Lines]",
          "2401:    for (ConsumerRecord<K, V> cRecord : getHighestOffsetRecords(records)) {",
          "2402:     this.acks.put(cRecord);",
          "",
          "---------------",
          "--- Hunk 17 ---",
          "[Context before]",
          "2505:     if (this.stopImmediate && !isRunning()) {",
          "2506:      break;",
          "2507:     }",
          "2510:      continue;",
          "2511:     }",
          "2513:     try {",
          "2515:     }",
          "2516:     catch (ProducerFencedException | FencedInstanceIdException e) {",
          "2517:      this.logger.error(e, \"Producer or 'group.instance.id' fenced during transaction\");",
          "",
          "[Removed Lines]",
          "2508:     final ConsumerRecord<K, V> record = checkEarlyIntercept(iterator.next());",
          "2509:     if (record == null) {",
          "2512:     this.logger.trace(() -> \"Processing \" + KafkaUtils.format(record));",
          "2514:      invokeInTransaction(iterator, record);",
          "",
          "[Added Lines]",
          "2508:     final ConsumerRecord<K, V> cRecord = checkEarlyIntercept(iterator.next());",
          "2509:     if (cRecord == null) {",
          "2512:     this.logger.trace(() -> \"Processing \" + KafkaUtils.format(cRecord));",
          "2514:      invokeInTransaction(iterator, cRecord);",
          "",
          "---------------",
          "--- Hunk 18 ---",
          "[Context before]",
          "2522:     }",
          "2523:     catch (RuntimeException ex) {",
          "2524:      this.logger.error(ex, \"Transaction rolled back\");",
          "2526:     }",
          "2527:     if (this.commonRecordInterceptor != null) {",
          "2529:     }",
          "2530:     if (this.nackSleepDurationMillis >= 0) {",
          "2532:      break;",
          "2533:     }",
          "2534:     if (checkImmediatePause(iterator)) {",
          "",
          "[Removed Lines]",
          "2525:      recordAfterRollback(iterator, record, ex);",
          "2528:      this.commonRecordInterceptor.afterRecord(record, this.consumer);",
          "2531:      handleNack(records, record);",
          "",
          "[Added Lines]",
          "2525:      recordAfterRollback(iterator, cRecord, ex);",
          "2528:      this.commonRecordInterceptor.afterRecord(cRecord, this.consumer);",
          "2531:      handleNack(records, cRecord);",
          "",
          "---------------",
          "--- Hunk 19 ---",
          "[Context before]",
          "2537:    }",
          "2538:   }",
          "2541:    this.transactionTemplate.execute(new TransactionCallbackWithoutResult() {",
          "2543:     @Override",
          "",
          "[Removed Lines]",
          "2540:   private void invokeInTransaction(Iterator<ConsumerRecord<K, V>> iterator, final ConsumerRecord<K, V> record) {",
          "",
          "[Added Lines]",
          "2540:   private void invokeInTransaction(Iterator<ConsumerRecord<K, V>> iterator, final ConsumerRecord<K, V> cRecord) {",
          "",
          "---------------",
          "--- Hunk 20 ---",
          "[Context before]",
          "2547:         .getResource(ListenerConsumer.this.kafkaTxManager.getProducerFactory()))",
          "2548:           .getProducer(); // NOSONAR",
          "2549:      }",
          "2551:      if (aborted != null) {",
          "2552:       throw aborted;",
          "2553:      }",
          "",
          "[Removed Lines]",
          "2550:      RuntimeException aborted = doInvokeRecordListener(record, iterator);",
          "",
          "[Added Lines]",
          "2550:      RuntimeException aborted = doInvokeRecordListener(cRecord, iterator);",
          "",
          "---------------",
          "--- Hunk 21 ---",
          "[Context before]",
          "2556:    });",
          "2557:   }",
          "2560:     RuntimeException e) {",
          "2562:    List<ConsumerRecord<K, V>> unprocessed = new ArrayList<>();",
          "2564:    while (iterator.hasNext()) {",
          "2565:     unprocessed.add(iterator.next());",
          "2566:    }",
          "",
          "[Removed Lines]",
          "2559:   private void recordAfterRollback(Iterator<ConsumerRecord<K, V>> iterator, final ConsumerRecord<K, V> record,",
          "2563:    unprocessed.add(record);",
          "",
          "[Added Lines]",
          "2559:   private void recordAfterRollback(Iterator<ConsumerRecord<K, V>> iterator, final ConsumerRecord<K, V> cRecord,",
          "2563:    unprocessed.add(cRecord);",
          "",
          "---------------",
          "--- Hunk 22 ---",
          "[Context before]",
          "2599:     if (this.stopImmediate && !isRunning()) {",
          "2600:      break;",
          "2601:     }",
          "2604:      continue;",
          "2605:     }",
          "2608:     if (this.commonRecordInterceptor !=  null) {",
          "2610:     }",
          "2611:     if (this.nackSleepDurationMillis >= 0) {",
          "2613:      break;",
          "2614:     }",
          "2615:     if (checkImmediatePause(iterator)) {",
          "",
          "[Removed Lines]",
          "2602:     final ConsumerRecord<K, V> record = checkEarlyIntercept(iterator.next());",
          "2603:     if (record == null) {",
          "2606:     this.logger.trace(() -> \"Processing \" + KafkaUtils.format(record));",
          "2607:     doInvokeRecordListener(record, iterator);",
          "2609:      this.commonRecordInterceptor.afterRecord(record, this.consumer);",
          "2612:      handleNack(records, record);",
          "",
          "[Added Lines]",
          "2602:     final ConsumerRecord<K, V> cRecord = checkEarlyIntercept(iterator.next());",
          "2603:     if (cRecord == null) {",
          "2606:     this.logger.trace(() -> \"Processing \" + KafkaUtils.format(cRecord));",
          "2607:     doInvokeRecordListener(cRecord, iterator);",
          "2609:      this.commonRecordInterceptor.afterRecord(cRecord, this.consumer);",
          "2612:      handleNack(records, cRecord);",
          "",
          "---------------",
          "--- Hunk 23 ---",
          "[Context before]",
          "2656:   @Nullable",
          "2657:   private ConsumerRecord<K, V> checkEarlyIntercept(ConsumerRecord<K, V> recordArg) {",
          "2658:    internalHeaders(recordArg);",
          "2660:    if (this.earlyRecordInterceptor != null) {",
          "2663:      this.logger.debug(() -> \"RecordInterceptor returned null, skipping: \"",
          "2664:       + KafkaUtils.format(recordArg));",
          "2665:      ackCurrent(recordArg);",
          "2666:     }",
          "2667:    }",
          "2669:   }",
          "2672:    if (this.deliveryAttemptAware != null) {",
          "2673:     byte[] buff = new byte[4]; // NOSONAR (magic #)",
          "2674:     ByteBuffer bb = ByteBuffer.wrap(buff);",
          "2675:     bb.putInt(this.deliveryAttemptAware",
          "2676:       .deliveryAttempt(",
          "2679:    }",
          "2680:    if (this.listenerinfo != null) {",
          "2682:    }",
          "2683:   }",
          "2687:   }",
          "2690:    if (!this.autoCommit && !this.isRecordAck) {",
          "2691:     processCommits();",
          "2692:    }",
          "",
          "[Removed Lines]",
          "2659:    ConsumerRecord<K, V> record = recordArg;",
          "2661:     record = this.earlyRecordInterceptor.intercept(record, this.consumer);",
          "2662:     if (record == null) {",
          "2668:    return record;",
          "2671:   private void internalHeaders(final ConsumerRecord<K, V> record) {",
          "2677:         new TopicPartitionOffset(record.topic(), record.partition(), record.offset())));",
          "2678:     record.headers().add(new RecordHeader(KafkaHeaders.DELIVERY_ATTEMPT, buff));",
          "2681:     listenerInfo(record);",
          "2685:   private void listenerInfo(final ConsumerRecord<K, V> record) {",
          "2686:    record.headers().add(this.infoHeader);",
          "2689:   private void handleNack(final ConsumerRecords<K, V> records, final ConsumerRecord<K, V> record) {",
          "",
          "[Added Lines]",
          "2659:    ConsumerRecord<K, V> cRecord = recordArg;",
          "2661:     cRecord = this.earlyRecordInterceptor.intercept(cRecord, this.consumer);",
          "2662:     if (cRecord == null) {",
          "2668:    return cRecord;",
          "2671:   private void internalHeaders(final ConsumerRecord<K, V> cRecord) {",
          "2677:         new TopicPartitionOffset(cRecord.topic(), cRecord.partition(), cRecord.offset())));",
          "2678:     cRecord.headers().add(new RecordHeader(KafkaHeaders.DELIVERY_ATTEMPT, buff));",
          "2681:     listenerInfo(cRecord);",
          "2685:   private void listenerInfo(final ConsumerRecord<K, V> cRecord) {",
          "2686:    cRecord.headers().add(this.infoHeader);",
          "2689:   private void handleNack(final ConsumerRecords<K, V> records, final ConsumerRecord<K, V> cRecord) {",
          "",
          "---------------",
          "--- Hunk 24 ---",
          "[Context before]",
          "2694:    Iterator<ConsumerRecord<K, V>> iterator2 = records.iterator();",
          "2695:    while (iterator2.hasNext()) {",
          "2696:     ConsumerRecord<K, V> next = iterator2.next();",
          "2698:      list.add(next);",
          "2699:     }",
          "2700:    }",
          "",
          "[Removed Lines]",
          "2697:     if (!list.isEmpty() || recordsEqual(record, next)) {",
          "",
          "[Added Lines]",
          "2697:     if (!list.isEmpty() || recordsEqual(cRecord, next)) {",
          "",
          "---------------",
          "--- Hunk 25 ---",
          "[Context before]",
          "2745:   @Nullable",
          "2747:     Iterator<ConsumerRecord<K, V>> iterator) {",
          "2749:    Object sample = startMicrometerSample();",
          "2750:    Observation observation = KafkaListenerObservation.LISTENER_OBSERVATION.observation(",
          "2751:      this.containerProperties.getObservationConvention(),",
          "2752:      DefaultKafkaListenerObservationConvention.INSTANCE,",
          "2754:      this.observationRegistry);",
          "2755:    return observation.observe(() -> {",
          "2756:     try {",
          "2758:      successTimer(sample);",
          "2760:     }",
          "2761:     catch (RuntimeException e) {",
          "2762:      failureTimer(sample);",
          "2764:      if (this.commonErrorHandler == null) {",
          "2765:       throw e;",
          "2766:      }",
          "2767:      try {",
          "2770:      }",
          "2771:      catch (KafkaException ke) {",
          "2772:       ke.selfLog(ERROR_HANDLER_THREW_AN_EXCEPTION, this.logger);",
          "",
          "[Removed Lines]",
          "2746:   private RuntimeException doInvokeRecordListener(final ConsumerRecord<K, V> record, // NOSONAR",
          "2753:      () -> new KafkaRecordReceiverContext(record, getListenerId(), this::clusterId),",
          "2757:      invokeOnMessage(record);",
          "2759:      recordInterceptAfter(record, null);",
          "2763:      recordInterceptAfter(record, e);",
          "2768:       invokeErrorHandler(record, iterator, e);",
          "2769:       commitOffsetsIfNeeded(record);",
          "",
          "[Added Lines]",
          "2746:   private RuntimeException doInvokeRecordListener(final ConsumerRecord<K, V> cRecord, // NOSONAR",
          "2753:      () -> new KafkaRecordReceiverContext(cRecord, getListenerId(), this::clusterId),",
          "2757:      invokeOnMessage(cRecord);",
          "2759:      recordInterceptAfter(cRecord, null);",
          "2763:      recordInterceptAfter(cRecord, e);",
          "2768:       invokeErrorHandler(cRecord, iterator, e);",
          "2769:       commitOffsetsIfNeeded(cRecord);",
          "",
          "---------------",
          "--- Hunk 26 ---",
          "[Context before]",
          "2785:    });",
          "2786:   }",
          "2789:    if ((!this.autoCommit && this.commonErrorHandler.isAckAfterHandle())",
          "2790:      || this.producer != null) {",
          "2791:     if (this.isManualAck) {",
          "2792:      this.commitRecovered = true;",
          "2793:     }",
          "2794:     if (this.remainingRecords == null",
          "2797:     }",
          "2798:     if (this.isManualAck) {",
          "2799:      this.commitRecovered = false;",
          "",
          "[Removed Lines]",
          "2788:   private void commitOffsetsIfNeeded(final ConsumerRecord<K, V> record) {",
          "2795:       || !record.equals(this.remainingRecords.iterator().next())) {",
          "2796:      ackCurrent(record);",
          "",
          "[Added Lines]",
          "2788:   private void commitOffsetsIfNeeded(final ConsumerRecord<K, V> cRecord) {",
          "2795:       || !cRecord.equals(this.remainingRecords.iterator().next())) {",
          "2796:      ackCurrent(cRecord);",
          "",
          "---------------",
          "--- Hunk 27 ---",
          "[Context before]",
          "2817:    }",
          "2818:   }",
          "2824:    }",
          "2827:    }",
          "2830:    }",
          "2833:    }",
          "2835:    if (this.nackSleepDurationMillis < 0 && !this.isManualImmediateAck) {",
          "2837:    }",
          "2838:   }",
          "2840:   private void doInvokeOnMessage(final ConsumerRecord<K, V> recordArg) {",
          "2842:    if (this.recordInterceptor != null) {",
          "2844:    }",
          "2846:     this.logger.debug(() -> \"RecordInterceptor returned null, skipping: \"",
          "2847:       + KafkaUtils.format(recordArg));",
          "2848:     ackCurrent(recordArg);",
          "",
          "[Removed Lines]",
          "2820:   private void invokeOnMessage(final ConsumerRecord<K, V> record) {",
          "2822:    if (record.value() instanceof DeserializationException) {",
          "2823:     throw (DeserializationException) record.value();",
          "2825:    if (record.key() instanceof DeserializationException) {",
          "2826:     throw (DeserializationException) record.key();",
          "2828:    if (record.value() == null && this.checkNullValueForExceptions) {",
          "2829:     checkDeser(record, SerializationUtils.VALUE_DESERIALIZER_EXCEPTION_HEADER);",
          "2831:    if (record.key() == null && this.checkNullKeyForExceptions) {",
          "2832:     checkDeser(record, SerializationUtils.KEY_DESERIALIZER_EXCEPTION_HEADER);",
          "2834:    doInvokeOnMessage(record);",
          "2836:     ackCurrent(record);",
          "2841:    ConsumerRecord<K, V> record = recordArg;",
          "2843:     record = this.recordInterceptor.intercept(record, this.consumer);",
          "2845:    if (record == null) {",
          "",
          "[Added Lines]",
          "2820:   private void invokeOnMessage(final ConsumerRecord<K, V> cRecord) {",
          "2822:    if (cRecord.value() instanceof DeserializationException ex) {",
          "2823:     throw ex;",
          "2825:    if (cRecord.key() instanceof DeserializationException ex) {",
          "2826:     throw ex;",
          "2828:    if (cRecord.value() == null && this.checkNullValueForExceptions) {",
          "2829:     checkDeser(cRecord, SerializationUtils.VALUE_DESERIALIZER_EXCEPTION_HEADER);",
          "2831:    if (cRecord.key() == null && this.checkNullKeyForExceptions) {",
          "2832:     checkDeser(cRecord, SerializationUtils.KEY_DESERIALIZER_EXCEPTION_HEADER);",
          "2834:    doInvokeOnMessage(cRecord);",
          "2836:     ackCurrent(cRecord);",
          "2841:    ConsumerRecord<K, V> cRecord = recordArg;",
          "2843:     cRecord = this.recordInterceptor.intercept(cRecord, this.consumer);",
          "2845:    if (cRecord == null) {",
          "",
          "---------------",
          "--- Hunk 28 ---",
          "[Context before]",
          "2851:     try {",
          "2852:      switch (this.listenerType) {",
          "2853:       case ACKNOWLEDGING_CONSUMER_AWARE:",
          "2855:          this.isAnyManualAck",
          "2857:            : null, this.consumer);",
          "2858:        break;",
          "2859:       case CONSUMER_AWARE:",
          "2861:        break;",
          "2862:       case ACKNOWLEDGING:",
          "2864:          this.isAnyManualAck",
          "2866:            : null);",
          "2867:        break;",
          "2868:       case SIMPLE:",
          "2870:        break;",
          "2871:      }",
          "2872:     }",
          "",
          "[Removed Lines]",
          "2854:        this.listener.onMessage(record,",
          "2856:            ? new ConsumerAcknowledgment(record)",
          "2860:        this.listener.onMessage(record, this.consumer);",
          "2863:        this.listener.onMessage(record,",
          "2865:            ? new ConsumerAcknowledgment(record)",
          "2869:        this.listener.onMessage(record);",
          "",
          "[Added Lines]",
          "2854:        this.listener.onMessage(cRecord,",
          "2856:            ? new ConsumerAcknowledgment(cRecord)",
          "2860:        this.listener.onMessage(cRecord, this.consumer);",
          "2863:        this.listener.onMessage(cRecord,",
          "2865:            ? new ConsumerAcknowledgment(cRecord)",
          "2869:        this.listener.onMessage(cRecord);",
          "",
          "---------------",
          "--- Hunk 29 ---",
          "[Context before]",
          "2876:    }",
          "2877:   }",
          "2880:     Iterator<ConsumerRecord<K, V>> iterator, RuntimeException rte) {",
          "2882:    if (this.commonErrorHandler.seeksAfterHandling() || rte instanceof CommitFailedException) {",
          "",
          "[Removed Lines]",
          "2879:   private void invokeErrorHandler(final ConsumerRecord<K, V> record,",
          "",
          "[Added Lines]",
          "2879:   private void invokeErrorHandler(final ConsumerRecord<K, V> cRecord,",
          "",
          "---------------",
          "--- Hunk 30 ---",
          "[Context before]",
          "2889:      this.logger.error(ex, \"Failed to commit before handling error\");",
          "2890:     }",
          "2891:     List<ConsumerRecord<?, ?>> records = new ArrayList<>();",
          "2893:     while (iterator.hasNext()) {",
          "2894:      records.add(iterator.next());",
          "2895:     }",
          "",
          "[Removed Lines]",
          "2892:     records.add(record);",
          "",
          "[Added Lines]",
          "2892:     records.add(cRecord);",
          "",
          "---------------",
          "--- Hunk 31 ---",
          "[Context before]",
          "2897:       KafkaMessageListenerContainer.this.thisOrParentContainer);",
          "2898:    }",
          "2899:    else {",
          "2901:       KafkaMessageListenerContainer.this.thisOrParentContainer);",
          "2902:     Map<TopicPartition, List<ConsumerRecord<K, V>>> records = new LinkedHashMap<>();",
          "2903:     if (!handled) {",
          "2906:     }",
          "2907:     while (iterator.hasNext()) {",
          "2908:      ConsumerRecord<K, V> next = iterator.next();",
          "",
          "[Removed Lines]",
          "2900:     boolean handled = this.commonErrorHandler.handleOne(rte, record, this.consumer,",
          "2904:      records.computeIfAbsent(new TopicPartition(record.topic(), record.partition()),",
          "2905:        tp -> new ArrayList<ConsumerRecord<K, V>>()).add(record);",
          "",
          "[Added Lines]",
          "2900:     boolean handled = this.commonErrorHandler.handleOne(rte, cRecord, this.consumer,",
          "2904:      records.computeIfAbsent(new TopicPartition(cRecord.topic(), cRecord.partition()),",
          "2905:        tp -> new ArrayList<ConsumerRecord<K, V>>()).add(cRecord);",
          "",
          "---------------",
          "--- Hunk 32 ---",
          "[Context before]",
          "2965:    }",
          "2966:   }",
          "2970:    if (exception != null) {",
          "",
          "[Removed Lines]",
          "2968:   public void checkDeser(final ConsumerRecord<K, V> record, String headerName) {",
          "2969:    DeserializationException exception = ListenerUtils.getExceptionFromHeader(record, headerName, this.logger);",
          "",
          "[Added Lines]",
          "2968:   public void checkDeser(final ConsumerRecord<K, V> cRecord, String headerName) {",
          "2969:    DeserializationException exception = ListenerUtils.getExceptionFromHeader(cRecord, headerName, this.logger);",
          "",
          "---------------",
          "--- Hunk 33 ---",
          "[Context before]",
          "2975:    }",
          "2976:   }",
          "2980:    if (this.isRecordAck) {",
          "2981:     Map<TopicPartition, OffsetAndMetadata> offsetsToCommit =",
          "2984:     if (this.producer == null) {",
          "2985:      this.commitLogger.log(() -> COMMITTING + offsetsToCommit);",
          "2986:      if (this.syncCommits) {",
          "",
          "[Removed Lines]",
          "2978:   public void ackCurrent(final ConsumerRecord<K, V> record) {",
          "2982:       Collections.singletonMap(new TopicPartition(record.topic(), record.partition()),",
          "2983:         createOffsetAndMetadata(record.offset() + 1));",
          "",
          "[Added Lines]",
          "2978:   public void ackCurrent(final ConsumerRecord<K, V> cRecord) {",
          "2982:       Collections.singletonMap(new TopicPartition(cRecord.topic(), cRecord.partition()),",
          "2983:         createOffsetAndMetadata(cRecord.offset() + 1));",
          "",
          "---------------",
          "--- Hunk 34 ---",
          "[Context before]",
          "2991:      }",
          "2992:     }",
          "2993:     else {",
          "2995:     }",
          "2996:    }",
          "2997:    else if (this.producer != null",
          "2998:      || ((!this.isAnyManualAck || this.commitRecovered) && !this.autoCommit)) {",
          "3000:    }",
          "3001:    if (this.producer != null) {",
          "3002:     sendOffsetsToTransaction();",
          "",
          "[Removed Lines]",
          "2994:      this.acks.add(record);",
          "2999:     this.acks.add(record);",
          "",
          "[Added Lines]",
          "2994:      this.acks.add(cRecord);",
          "2999:     this.acks.add(cRecord);",
          "",
          "---------------",
          "--- Hunk 35 ---",
          "[Context before]",
          "3222:   }",
          "3224:   private void updatePendingOffsets() {",
          "3229:    }",
          "3230:   }",
          "3235:   }",
          "3237:   private void commitIfNecessary() {",
          "",
          "[Removed Lines]",
          "3225:    ConsumerRecord<K, V> record = this.acks.poll();",
          "3226:    while (record != null) {",
          "3227:     addOffset(record);",
          "3228:     record = this.acks.poll();",
          "3232:   private void addOffset(ConsumerRecord<K, V> record) {",
          "3233:    this.offsets.computeIfAbsent(record.topic(), v -> new ConcurrentHashMap<>())",
          "3234:      .compute(record.partition(), (k, v) -> v == null ? record.offset() : Math.max(v, record.offset()));",
          "",
          "[Added Lines]",
          "3225:    ConsumerRecord<K, V> cRecord = this.acks.poll();",
          "3226:    while (cRecord != null) {",
          "3227:     addOffset(cRecord);",
          "3228:     cRecord = this.acks.poll();",
          "3232:   private void addOffset(ConsumerRecord<K, V> cRecord) {",
          "3233:    this.offsets.computeIfAbsent(cRecord.topic(), v -> new ConcurrentHashMap<>())",
          "3234:      .compute(cRecord.partition(), (k, v) -> v == null ? cRecord.offset() : Math.max(v, cRecord.offset()));",
          "",
          "---------------",
          "--- Hunk 36 ---",
          "[Context before]",
          "3372:   private final class ConsumerAcknowledgment implements Acknowledgment {",
          "3376:    private volatile boolean acked;",
          "3380:    }",
          "3382:    @Override",
          "3383:    public void acknowledge() {",
          "3384:     if (!this.acked) {",
          "3386:      this.acked = true;",
          "3387:     }",
          "3388:    }",
          "",
          "[Removed Lines]",
          "3374:    private final ConsumerRecord<K, V> record;",
          "3378:    ConsumerAcknowledgment(ConsumerRecord<K, V> record) {",
          "3379:     this.record = record;",
          "3385:      doAck(this.record);",
          "",
          "[Added Lines]",
          "3374:    private final ConsumerRecord<K, V> cRecord;",
          "3378:    ConsumerAcknowledgment(ConsumerRecord<K, V> cRecord) {",
          "3379:     this.cRecord = cRecord;",
          "3385:      doAck(this.cRecord);",
          "",
          "---------------",
          "--- Hunk 37 ---",
          "[Context before]",
          "3400:    @Override",
          "3401:    public String toString() {",
          "3403:    }",
          "3405:   }",
          "",
          "[Removed Lines]",
          "3402:     return \"Acknowledgment for \" + KafkaUtils.format(this.record);",
          "",
          "[Added Lines]",
          "3402:     return \"Acknowledgment for \" + KafkaUtils.format(this.cRecord);",
          "",
          "---------------",
          "--- Hunk 38 ---",
          "[Context before]",
          "3419:     Map<TopicPartition, List<Long>> offs = ListenerConsumer.this.offsetsInThisBatch;",
          "3420:     Map<TopicPartition, List<ConsumerRecord<K, V>>> deferred = ListenerConsumer.this.deferredOffsets;",
          "3421:     if (!this.acked) {",
          "3423:       if (offs != null) {",
          "3426:       }",
          "3427:      }",
          "3428:      processAcks(this.records);",
          "",
          "[Removed Lines]",
          "3422:      for (ConsumerRecord<K, V> record : getHighestOffsetRecords(this.records)) {",
          "3424:        offs.remove(new TopicPartition(record.topic(), record.partition()));",
          "3425:        deferred.remove(new TopicPartition(record.topic(), record.partition()));",
          "",
          "[Added Lines]",
          "3422:      for (ConsumerRecord<K, V> cRecord : getHighestOffsetRecords(this.records)) {",
          "3424:        offs.remove(new TopicPartition(cRecord.topic(), cRecord.partition()));",
          "3425:        deferred.remove(new TopicPartition(cRecord.topic(), cRecord.partition()));",
          "",
          "---------------",
          "--- Hunk 39 ---",
          "[Context before]",
          "3442:     ListenerConsumer.this.nackSleepDurationMillis = sleep.toMillis();",
          "3443:     int i = 0;",
          "3444:     List<ConsumerRecord<K, V>> toAck = new LinkedList<>();",
          "3446:      if (i++ < index) {",
          "3448:      }",
          "3449:      else {",
          "3450:       break;",
          "3451:      }",
          "3452:     }",
          "3453:     Map<TopicPartition, List<ConsumerRecord<K, V>>> newRecords = new HashMap<>();",
          "3457:     }",
          "3458:     processAcks(new ConsumerRecords<K, V>(newRecords));",
          "3459:    }",
          "",
          "[Removed Lines]",
          "3445:     for (ConsumerRecord<K, V> record : this.records) {",
          "3447:       toAck.add(record);",
          "3454:     for (ConsumerRecord<K, V> record : toAck) {",
          "3455:      newRecords.computeIfAbsent(new TopicPartition(record.topic(), record.partition()),",
          "3456:        tp -> new LinkedList<>()).add(record);",
          "",
          "[Added Lines]",
          "3445:     for (ConsumerRecord<K, V> cRecord : this.records) {",
          "3447:       toAck.add(cRecord);",
          "3454:     for (ConsumerRecord<K, V> cRecord : toAck) {",
          "3455:      newRecords.computeIfAbsent(new TopicPartition(cRecord.topic(), cRecord.partition()),",
          "3456:        tp -> new LinkedList<>()).add(cRecord);",
          "",
          "---------------",
          "--- Hunk 40 ---",
          "[Context before]",
          "3471:      .getConsumerRebalanceListener();",
          "3473:    private final ConsumerAwareRebalanceListener consumerAwareListener =",
          "3477:    private final Collection<TopicPartition> revoked = new LinkedList<>();",
          "",
          "[Removed Lines]",
          "3474:      this.userListener instanceof ConsumerAwareRebalanceListener",
          "3475:        ? (ConsumerAwareRebalanceListener) this.userListener : null;",
          "",
          "[Added Lines]",
          "3474:      this.userListener instanceof ConsumerAwareRebalanceListener carl ? carl : null;",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "87a577e4cab0b5e85aaa0048d11a96d2335eed29",
      "candidate_info": {
        "commit_hash": "87a577e4cab0b5e85aaa0048d11a96d2335eed29",
        "repo": "spring-projects/spring-kafka",
        "commit_url": "https://github.com/spring-projects/spring-kafka/commit/87a577e4cab0b5e85aaa0048d11a96d2335eed29",
        "files": [
          "spring-kafka/src/main/java/org/springframework/kafka/support/DefaultKafkaHeaderMapper.java",
          "spring-kafka/src/main/java/org/springframework/kafka/support/KafkaUtils.java",
          "spring-kafka/src/main/java/org/springframework/kafka/support/serializer/SerializationUtils.java",
          "spring-kafka/src/test/java/org/springframework/kafka/listener/DeadLetterPublishingRecovererTests.java",
          "spring-kafka/src/test/java/org/springframework/kafka/support/DefaultKafkaHeaderMapperTests.java",
          "spring-kafka/src/test/java/org/springframework/kafka/support/serializer/SerializationTestUtils.java"
        ],
        "message": "GH-3114: DeserializationException propagation\n\nFixes: #3114\n\n* Since DeserializationExceptionHeader is currently porpagated as a `byte[]`,\n  it encounters some issues when processing the header especially in batch\n  listeners. Fixing this by providing the deserialization header without `byte[]` conversion\n* Adding test to verify\n* Refactoring in SerializationTestUtils\n\n(cherry picked from commit 7cc7fc8deb68764cb066f0617321b5ea313d6c92)",
        "before_after_code_files": [
          "spring-kafka/src/main/java/org/springframework/kafka/support/DefaultKafkaHeaderMapper.java||spring-kafka/src/main/java/org/springframework/kafka/support/DefaultKafkaHeaderMapper.java",
          "spring-kafka/src/main/java/org/springframework/kafka/support/KafkaUtils.java||spring-kafka/src/main/java/org/springframework/kafka/support/KafkaUtils.java",
          "spring-kafka/src/main/java/org/springframework/kafka/support/serializer/SerializationUtils.java||spring-kafka/src/main/java/org/springframework/kafka/support/serializer/SerializationUtils.java",
          "spring-kafka/src/test/java/org/springframework/kafka/listener/DeadLetterPublishingRecovererTests.java||spring-kafka/src/test/java/org/springframework/kafka/listener/DeadLetterPublishingRecovererTests.java",
          "spring-kafka/src/test/java/org/springframework/kafka/support/DefaultKafkaHeaderMapperTests.java||spring-kafka/src/test/java/org/springframework/kafka/support/DefaultKafkaHeaderMapperTests.java",
          "spring-kafka/src/test/java/org/springframework/kafka/support/serializer/SerializationTestUtils.java||spring-kafka/src/test/java/org/springframework/kafka/support/serializer/SerializationTestUtils.java"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "spring-kafka/src/main/java/org/springframework/kafka/support/serializer/SerializationUtils.java||spring-kafka/src/main/java/org/springframework/kafka/support/serializer/SerializationUtils.java",
            "spring-kafka/src/test/java/org/springframework/kafka/listener/DeadLetterPublishingRecovererTests.java||spring-kafka/src/test/java/org/springframework/kafka/listener/DeadLetterPublishingRecovererTests.java",
            "spring-kafka/src/test/java/org/springframework/kafka/support/serializer/SerializationTestUtils.java||spring-kafka/src/test/java/org/springframework/kafka/support/serializer/SerializationTestUtils.java"
          ],
          "candidate": [
            "spring-kafka/src/main/java/org/springframework/kafka/support/serializer/SerializationUtils.java||spring-kafka/src/main/java/org/springframework/kafka/support/serializer/SerializationUtils.java",
            "spring-kafka/src/test/java/org/springframework/kafka/listener/DeadLetterPublishingRecovererTests.java||spring-kafka/src/test/java/org/springframework/kafka/listener/DeadLetterPublishingRecovererTests.java",
            "spring-kafka/src/test/java/org/springframework/kafka/support/serializer/SerializationTestUtils.java||spring-kafka/src/test/java/org/springframework/kafka/support/serializer/SerializationTestUtils.java"
          ]
        }
      },
      "candidate_diff": {
        "spring-kafka/src/main/java/org/springframework/kafka/support/DefaultKafkaHeaderMapper.java||spring-kafka/src/main/java/org/springframework/kafka/support/DefaultKafkaHeaderMapper.java": [
          "File: spring-kafka/src/main/java/org/springframework/kafka/support/DefaultKafkaHeaderMapper.java -> spring-kafka/src/main/java/org/springframework/kafka/support/DefaultKafkaHeaderMapper.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "323:    else if (headerName.equals(KafkaHeaders.LISTENER_INFO) && matchesForInbound(headerName)) {",
          "324:     headers.put(headerName, new String(header.value(), getCharset()));",
          "325:    }",
          "326:    else if (!(headerName.equals(JSON_TYPES)) && matchesForInbound(headerName)) {",
          "327:     if (jsonTypes.containsKey(headerName)) {",
          "328:      String requestedType = jsonTypes.get(headerName);",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "327:    else if (headerName.equals(KafkaUtils.KEY_DESERIALIZER_EXCEPTION_HEADER) ||",
          "328:      headerName.equals(KafkaUtils.VALUE_DESERIALIZER_EXCEPTION_HEADER)) {",
          "329:     headers.put(headerName, header);",
          "330:    }",
          "",
          "---------------"
        ],
        "spring-kafka/src/main/java/org/springframework/kafka/support/KafkaUtils.java||spring-kafka/src/main/java/org/springframework/kafka/support/KafkaUtils.java": [
          "File: spring-kafka/src/main/java/org/springframework/kafka/support/KafkaUtils.java -> spring-kafka/src/main/java/org/springframework/kafka/support/KafkaUtils.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "45: public final class KafkaUtils {",
          "47:  private static Function<ProducerRecord<?, ?>, String> prFormatter = ProducerRecord::toString;",
          "49:  private static Function<ConsumerRecord<?, ?>, String> crFormatter =",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "52:  public static final String DESERIALIZER_EXCEPTION_HEADER_PREFIX = \"springDeserializerException\";",
          "58:  public static final String KEY_DESERIALIZER_EXCEPTION_HEADER = DESERIALIZER_EXCEPTION_HEADER_PREFIX + \"Key\";",
          "64:  public static final String VALUE_DESERIALIZER_EXCEPTION_HEADER = DESERIALIZER_EXCEPTION_HEADER_PREFIX + \"Value\";",
          "",
          "---------------"
        ],
        "spring-kafka/src/main/java/org/springframework/kafka/support/serializer/SerializationUtils.java||spring-kafka/src/main/java/org/springframework/kafka/support/serializer/SerializationUtils.java": [
          "File: spring-kafka/src/main/java/org/springframework/kafka/support/serializer/SerializationUtils.java -> spring-kafka/src/main/java/org/springframework/kafka/support/serializer/SerializationUtils.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "67:  private SerializationUtils() {",
          "68:  }",
          "",
          "[Removed Lines]",
          "53:  public static final String DESERIALIZER_EXCEPTION_HEADER_PREFIX = \"springDeserializerException\";",
          "59:  public static final String KEY_DESERIALIZER_EXCEPTION_HEADER = DESERIALIZER_EXCEPTION_HEADER_PREFIX + \"Key\";",
          "65:  public static final String VALUE_DESERIALIZER_EXCEPTION_HEADER = DESERIALIZER_EXCEPTION_HEADER_PREFIX + \"Value\";",
          "",
          "[Added Lines]",
          "53:  public static final String DESERIALIZER_EXCEPTION_HEADER_PREFIX = KafkaUtils.DESERIALIZER_EXCEPTION_HEADER_PREFIX;",
          "59:  public static final String KEY_DESERIALIZER_EXCEPTION_HEADER = KafkaUtils.KEY_DESERIALIZER_EXCEPTION_HEADER;",
          "65:  public static final String VALUE_DESERIALIZER_EXCEPTION_HEADER = KafkaUtils.VALUE_DESERIALIZER_EXCEPTION_HEADER;",
          "",
          "---------------"
        ],
        "spring-kafka/src/test/java/org/springframework/kafka/listener/DeadLetterPublishingRecovererTests.java||spring-kafka/src/test/java/org/springframework/kafka/listener/DeadLetterPublishingRecovererTests.java": [
          "File: spring-kafka/src/test/java/org/springframework/kafka/listener/DeadLetterPublishingRecovererTests.java -> spring-kafka/src/test/java/org/springframework/kafka/listener/DeadLetterPublishingRecovererTests.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "33: import static org.mockito.Mockito.times;",
          "34: import static org.mockito.Mockito.verify;",
          "40: import java.time.Duration;",
          "41: import java.util.Collections;",
          "42: import java.util.HashMap;",
          "",
          "[Removed Lines]",
          "36: import java.io.ByteArrayOutputStream;",
          "37: import java.io.IOException;",
          "38: import java.io.ObjectOutputStream;",
          "39: import java.io.UncheckedIOException;",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "174:   DeadLetterPublishingRecoverer recoverer = new DeadLetterPublishingRecoverer(template);",
          "175:   Headers headers = new RecordHeaders();",
          "176:   headers.add(SerializationTestUtils.deserializationHeader(SerializationUtils.VALUE_DESERIALIZER_EXCEPTION_HEADER,",
          "178:   headers.add(SerializationTestUtils.deserializationHeader(SerializationUtils.KEY_DESERIALIZER_EXCEPTION_HEADER,",
          "180:   Headers custom = new RecordHeaders();",
          "181:   custom.add(new RecordHeader(\"foo\", \"bar\".getBytes()));",
          "182:   recoverer.setHeadersFunction((rec, ex) -> custom);",
          "",
          "[Removed Lines]",
          "177:     header(false)));",
          "179:     header(true)));",
          "",
          "[Added Lines]",
          "174:     SerializationTestUtils.header(false)));",
          "176:     SerializationTestUtils.header(true)));",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "206:   DeadLetterPublishingRecoverer recoverer = new DeadLetterPublishingRecoverer(template);",
          "207:   Headers headers = new RecordHeaders();",
          "208:   headers.add(SerializationTestUtils.deserializationHeader(SerializationUtils.KEY_DESERIALIZER_EXCEPTION_HEADER,",
          "210:   CompletableFuture future = new CompletableFuture();",
          "211:   future.complete(new Object());",
          "212:   willReturn(future).given(template).send(any(ProducerRecord.class));",
          "",
          "[Removed Lines]",
          "209:     header(true)));",
          "",
          "[Added Lines]",
          "206:     SerializationTestUtils.header(true)));",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "225:   KafkaOperations<?, ?> template = mock(KafkaOperations.class);",
          "226:   DeadLetterPublishingRecoverer recoverer = new DeadLetterPublishingRecoverer(template);",
          "227:   Headers headers = new RecordHeaders();",
          "229:   headers.add(SerializationTestUtils.deserializationHeader(SerializationUtils.KEY_DESERIALIZER_EXCEPTION_HEADER,",
          "231:   CompletableFuture future = new CompletableFuture();",
          "232:   future.complete(new Object());",
          "233:   willReturn(future).given(template).send(any(ProducerRecord.class));",
          "",
          "[Removed Lines]",
          "228:   DeserializationException deserEx = createDeserEx(true);",
          "230:     header(true, deserEx)));",
          "",
          "[Added Lines]",
          "225:   DeserializationException deserEx = SerializationTestUtils.createDeserEx(true);",
          "227:     SerializationTestUtils.header(deserEx)));",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "250:   recoverer.setRetainExceptionHeader(true);",
          "251:   Headers headers = new RecordHeaders();",
          "252:   headers.add(SerializationTestUtils.deserializationHeader(SerializationUtils.VALUE_DESERIALIZER_EXCEPTION_HEADER,",
          "254:   headers.add(SerializationTestUtils.deserializationHeader(SerializationUtils.KEY_DESERIALIZER_EXCEPTION_HEADER,",
          "256:   CompletableFuture future = new CompletableFuture();",
          "257:   future.complete(new Object());",
          "258:   willReturn(future).given(template).send(any(ProducerRecord.class));",
          "",
          "[Removed Lines]",
          "253:     header(false)));",
          "255:     header(true)));",
          "",
          "[Added Lines]",
          "250:     SerializationTestUtils.header(false)));",
          "252:     SerializationTestUtils.header(true)));",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "302:   verify(template2).send(any(ProducerRecord.class));",
          "303:  }",
          "326:  @SuppressWarnings({\"unchecked\", \"rawtypes\"})",
          "327:  @Test",
          "328:  void allOriginalHeaders() {",
          "",
          "[Removed Lines]",
          "305:  private byte[] header(boolean isKey) {",
          "306:   return header(isKey, createDeserEx(isKey));",
          "307:  }",
          "309:  private DeserializationException createDeserEx(boolean isKey) {",
          "310:   return new DeserializationException(",
          "311:     isKey ? \"testK\" : \"testV\",",
          "312:     isKey ? \"key\".getBytes() : \"value\".getBytes(), isKey, null);",
          "313:  }",
          "315:  private byte[] header(boolean isKey, DeserializationException deserEx) {",
          "316:   ByteArrayOutputStream baos = new ByteArrayOutputStream();",
          "317:   try {",
          "318:    new ObjectOutputStream(baos).writeObject(deserEx);",
          "319:   }",
          "320:   catch (IOException e) {",
          "321:    throw new UncheckedIOException(e);",
          "322:   }",
          "323:   return baos.toByteArray();",
          "324:  }",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "spring-kafka/src/test/java/org/springframework/kafka/support/DefaultKafkaHeaderMapperTests.java||spring-kafka/src/test/java/org/springframework/kafka/support/DefaultKafkaHeaderMapperTests.java": [
          "File: spring-kafka/src/test/java/org/springframework/kafka/support/DefaultKafkaHeaderMapperTests.java -> spring-kafka/src/test/java/org/springframework/kafka/support/DefaultKafkaHeaderMapperTests.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "34: import org.apache.kafka.common.header.internals.RecordHeaders;",
          "35: import org.junit.jupiter.api.Test;",
          "37: import org.springframework.kafka.support.DefaultKafkaHeaderMapper.NonTrustedHeaderType;",
          "38: import org.springframework.messaging.Message;",
          "39: import org.springframework.messaging.MessageHeaders;",
          "40: import org.springframework.messaging.support.ExecutorSubscribableChannel;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "37: import org.springframework.core.log.LogAccessor;",
          "39: import org.springframework.kafka.support.serializer.DeserializationException;",
          "40: import org.springframework.kafka.support.serializer.SerializationTestUtils;",
          "41: import org.springframework.kafka.support.serializer.SerializationUtils;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "321:     .containsKey(\"baz\");",
          "322:  }",
          "324:  public static final class Foo {",
          "326:   private String bar = \"bar\";",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "329:  @Test",
          "330:  void deserializationExceptionHeadersAreMappedAsNonByteArray() {",
          "331:   DefaultKafkaHeaderMapper mapper = new DefaultKafkaHeaderMapper();",
          "333:   byte[] keyDeserExceptionBytes = SerializationTestUtils.header(true);",
          "334:   Header keyHeader = SerializationTestUtils.deserializationHeader(SerializationUtils.KEY_DESERIALIZER_EXCEPTION_HEADER,",
          "335:     keyDeserExceptionBytes);",
          "336:   byte[] valueDeserExceptionBytes = SerializationTestUtils.header(false);",
          "337:   Header valueHeader = SerializationTestUtils.deserializationHeader(SerializationUtils.VALUE_DESERIALIZER_EXCEPTION_HEADER,",
          "338:     valueDeserExceptionBytes);",
          "339:   Headers headers = new RecordHeaders(",
          "340:     new Header[] { keyHeader, valueHeader });",
          "341:   Map<String, Object> springHeaders = new HashMap<>();",
          "342:   mapper.toHeaders(headers, springHeaders);",
          "343:   assertThat(springHeaders.get(SerializationUtils.KEY_DESERIALIZER_EXCEPTION_HEADER)).isEqualTo(keyHeader);",
          "344:   assertThat(springHeaders.get(SerializationUtils.VALUE_DESERIALIZER_EXCEPTION_HEADER)).isEqualTo(valueHeader);",
          "346:   LogAccessor logger = new LogAccessor(this.getClass());",
          "348:   DeserializationException keyDeserializationException = SerializationUtils.byteArrayToDeserializationException(logger, keyHeader);",
          "349:   assertThat(keyDeserExceptionBytes).containsExactly(SerializationTestUtils.header(keyDeserializationException));",
          "351:   DeserializationException valueDeserializationException =",
          "352:     SerializationUtils.byteArrayToDeserializationException(logger, valueHeader);",
          "353:   assertThat(valueDeserExceptionBytes).containsExactly(SerializationTestUtils.header(valueDeserializationException));",
          "355:   headers = new RecordHeaders();",
          "356:   mapper.fromHeaders(new MessageHeaders(springHeaders), headers);",
          "357:   assertThat(headers.lastHeader(SerializationUtils.KEY_DESERIALIZER_EXCEPTION_HEADER)).isNull();",
          "358:   assertThat(headers.lastHeader(SerializationUtils.VALUE_DESERIALIZER_EXCEPTION_HEADER)).isNull();",
          "359:  }",
          "",
          "---------------"
        ],
        "spring-kafka/src/test/java/org/springframework/kafka/support/serializer/SerializationTestUtils.java||spring-kafka/src/test/java/org/springframework/kafka/support/serializer/SerializationTestUtils.java": [
          "File: spring-kafka/src/test/java/org/springframework/kafka/support/serializer/SerializationTestUtils.java -> spring-kafka/src/test/java/org/springframework/kafka/support/serializer/SerializationTestUtils.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "17: package org.springframework.kafka.support.serializer;",
          "19: import org.apache.kafka.common.header.Header;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "19: import java.io.ByteArrayOutputStream;",
          "20: import java.io.IOException;",
          "21: import java.io.ObjectOutputStream;",
          "22: import java.io.UncheckedIOException;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "32:   return new DeserializationExceptionHeader(key, value);",
          "33:  }",
          "35: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "41:  public static byte[] header(boolean isKey) {",
          "42:   return header(createDeserEx(isKey));",
          "43:  }",
          "45:  public static DeserializationException createDeserEx(boolean isKey) {",
          "46:   return new DeserializationException(",
          "47:     isKey ? \"testK\" : \"testV\",",
          "48:     isKey ? \"key\".getBytes() : \"value\".getBytes(), isKey, null);",
          "49:  }",
          "51:  public static byte[] header(DeserializationException deserEx) {",
          "52:   ByteArrayOutputStream baos = new ByteArrayOutputStream();",
          "53:   try {",
          "54:    new ObjectOutputStream(baos).writeObject(deserEx);",
          "55:   }",
          "56:   catch (IOException e) {",
          "57:    throw new UncheckedIOException(e);",
          "58:   }",
          "59:   return baos.toByteArray();",
          "60:  }",
          "",
          "---------------"
        ]
      }
    }
  ]
}