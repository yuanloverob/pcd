{
  "cve_id": "CVE-2023-35005",
  "cve_desc": "In Apache Airflow, some potentially sensitive values were being shown to the user in certain situations.\n\nThis vulnerability is mitigated by the fact configuration is not shown in the UI by default (only if `[webserver] expose_config` is set to `non-sensitive-only`), and not all uncensored values are actually sentitive.\n\n\nThis issue affects Apache Airflow: from 2.5.0 before 2.6.2. Users are recommended to update to version 2.6.2 or later.\n\n\n",
  "repo": "apache/airflow",
  "patch_hash": "5679a01919ac9d5153e858f8b1390cbc7915f148",
  "patch_info": {
    "commit_hash": "5679a01919ac9d5153e858f8b1390cbc7915f148",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/5679a01919ac9d5153e858f8b1390cbc7915f148",
    "files": [
      "airflow/config_templates/config.yml",
      "airflow/config_templates/default_airflow.cfg",
      "airflow/configuration.py",
      "airflow/www/views.py",
      "tests/core/test_configuration.py",
      "tests/www/views/test_views_configuration.py"
    ],
    "message": "Use single source of truth for sensitive config items (#31820)\n\nPreviously we had them defined both in constant and in config.yml.\n\nNow just config.yml\n\n(cherry picked from commit cab342ee010bfd048006ca458c760b37470b6ea5)",
    "before_after_code_files": [
      "airflow/config_templates/default_airflow.cfg||airflow/config_templates/default_airflow.cfg",
      "airflow/configuration.py||airflow/configuration.py",
      "airflow/www/views.py||airflow/www/views.py",
      "tests/core/test_configuration.py||tests/core/test_configuration.py",
      "tests/www/views/test_views_configuration.py||tests/www/views/test_views_configuration.py"
    ]
  },
  "patch_diff": {
    "airflow/config_templates/default_airflow.cfg||airflow/config_templates/default_airflow.cfg": [
      "File: airflow/config_templates/default_airflow.cfg -> airflow/config_templates/default_airflow.cfg",
      "--- Hunk 1 ---",
      "[Context before]",
      "995: # Example: result_backend = db+postgresql://postgres:airflow@postgres/airflow",
      "996: # result_backend =",
      "998: # Celery Flower is a sweet UI for Celery. Airflow has a shortcut to start",
      "999: # it ``airflow celery flower``. This defines the IP that Celery Flower runs on",
      "1000: flower_host = 0.0.0.0",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "998: # Optional configuration dictionary to pass to the Celery result backend SQLAlchemy engine.",
      "999: # Example: result_backend_sqlalchemy_engine_options = {{\"pool_recycle\": 1800}}",
      "1000: result_backend_sqlalchemy_engine_options =",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "1018: # Import path for celery configuration options",
      "1019: celery_config_options = airflow.config_templates.default_celery.DEFAULT_CELERY_CONFIG",
      "1020: ssl_active = False",
      "1021: ssl_key =",
      "1022: ssl_cert =",
      "1023: ssl_cacert =",
      "1025: # Celery Pool implementation.",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "1026: # Path to the client key.",
      "1029: # Path to the client certificate.",
      "1032: # Path to the CA certificate.",
      "",
      "---------------"
    ],
    "airflow/configuration.py||airflow/configuration.py": [
      "File: airflow/configuration.py -> airflow/configuration.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "37: from contextlib import contextmanager, suppress",
      "38: from json.decoder import JSONDecodeError",
      "39: from re import Pattern",
      "41: from urllib.parse import urlsplit",
      "43: from typing_extensions import overload",
      "",
      "[Removed Lines]",
      "40: from typing import IO, Any, Dict, Iterable, Tuple, Union",
      "",
      "[Added Lines]",
      "40: from typing import IO, Any, Dict, Iterable, Set, Tuple, Union",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "147:         return yaml.safe_load(config_file)",
      "165: class AirflowConfigParser(ConfigParser):",
      "166:     \"\"\"Custom Airflow Configparser supporting defaults and deprecated options.\"\"\"",
      "",
      "[Removed Lines]",
      "150: SENSITIVE_CONFIG_VALUES = {",
      "151:     (\"database\", \"sql_alchemy_conn\"),",
      "152:     (\"core\", \"fernet_key\"),",
      "153:     (\"celery\", \"broker_url\"),",
      "154:     (\"celery\", \"flower_basic_auth\"),",
      "155:     (\"celery\", \"result_backend\"),",
      "156:     (\"atlas\", \"password\"),",
      "157:     (\"smtp\", \"smtp_password\"),",
      "158:     (\"webserver\", \"secret_key\"),",
      "159:     (\"secrets\", \"backend_kwargs\"),",
      "160:     # The following options are deprecated",
      "161:     (\"core\", \"sql_alchemy_conn\"),",
      "162: }",
      "",
      "[Added Lines]",
      "[None]",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "171:     # These configs can also be fetched from Secrets backend",
      "172:     # following the \"{section}__{name}__secret\" pattern",
      "176:     # A mapping of (new section, new option) -> (old section, old option, since_version).",
      "177:     # When reading new option, the old option will be checked to see if it exists. If it does a",
      "",
      "[Removed Lines]",
      "174:     sensitive_config_values: set[tuple[str, str]] = SENSITIVE_CONFIG_VALUES",
      "",
      "[Added Lines]",
      "159:     @cached_property",
      "160:     def sensitive_config_values(self) -> Set[tuple[str, str]]:  # noqa: UP006",
      "161:         default_config = default_config_yaml()",
      "162:         flattened = {",
      "163:             (s, k): item for s, s_c in default_config.items() for k, item in s_c.get(\"options\").items()",
      "164:         }",
      "165:         sensitive = {(section, key) for (section, key), v in flattened.items() if v.get(\"sensitive\") is True}",
      "166:         depr_option = {self.deprecated_options[x][:-1] for x in sensitive if x in self.deprecated_options}",
      "167:         depr_section = {",
      "168:             (self.deprecated_sections[s][0], k) for s, k in sensitive if s in self.deprecated_sections",
      "169:         }",
      "170:         sensitive.update(depr_section, depr_option)",
      "171:         return sensitive",
      "",
      "---------------"
    ],
    "airflow/www/views.py||airflow/www/views.py": [
      "File: airflow/www/views.py -> airflow/www/views.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "3951:         # TODO remove \"if raw\" usage in Airflow 3.0. Configuration can be fetched via the REST API.",
      "3952:         if raw:",
      "3953:             if expose_config == \"non-sensitive-only\":",
      "3956:                 updater = configupdater.ConfigUpdater()",
      "3957:                 updater.read(AIRFLOW_CONFIG)",
      "3959:                     if updater.has_option(sect, key):",
      "3960:                         updater[sect][key].value = \"< hidden >\"",
      "3961:                 config = str(updater)",
      "",
      "[Removed Lines]",
      "3954:                 from airflow.configuration import SENSITIVE_CONFIG_VALUES",
      "3958:                 for sect, key in SENSITIVE_CONFIG_VALUES:",
      "",
      "[Added Lines]",
      "3956:                 for sect, key in conf.sensitive_config_values:",
      "",
      "---------------"
    ],
    "tests/core/test_configuration.py||tests/core/test_configuration.py": [
      "File: tests/core/test_configuration.py -> tests/core/test_configuration.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "36:     AirflowConfigException,",
      "37:     AirflowConfigParser,",
      "38:     conf,",
      "39:     expand_env_var,",
      "40:     get_airflow_config,",
      "41:     get_airflow_home,",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "39:     default_config_yaml,",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "1447:             w = captured.pop()",
      "1448:             assert \"your `conf.get*` call to use the new name\" in str(w.message)",
      "1449:             assert w.category == FutureWarning",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "1453: def test_sensitive_values():",
      "1454:     from airflow.settings import conf",
      "1456:     # this list was hardcoded prior to 2.6.2",
      "1457:     # included here to avoid regression in refactor",
      "1458:     # inclusion of keys ending in \"password\" or \"kwargs\" is automated from 2.6.2",
      "1459:     # items not matching this pattern must be added here manually",
      "1460:     sensitive_values = {",
      "1461:         (\"database\", \"sql_alchemy_conn\"),",
      "1462:         (\"core\", \"fernet_key\"),",
      "1463:         (\"celery\", \"broker_url\"),",
      "1464:         (\"celery\", \"flower_basic_auth\"),",
      "1465:         (\"celery\", \"result_backend\"),",
      "1466:         (\"atlas\", \"password\"),",
      "1467:         (\"smtp\", \"smtp_password\"),",
      "1468:         (\"webserver\", \"secret_key\"),",
      "1469:         (\"secrets\", \"backend_kwargs\"),",
      "1470:         (\"sentry\", \"sentry_dsn\"),",
      "1471:         (\"database\", \"sql_alchemy_engine_args\"),",
      "1472:         (\"core\", \"sql_alchemy_conn\"),",
      "1473:     }",
      "1474:     default_config = default_config_yaml()",
      "1475:     all_keys = {(s, k) for s, v in default_config.items() for k in v.get(\"options\")}",
      "1476:     suspected_sensitive = {(s, k) for (s, k) in all_keys if k.endswith((\"password\", \"kwargs\"))}",
      "1477:     exclude_list = {",
      "1478:         (\"kubernetes_executor\", \"delete_option_kwargs\"),",
      "1479:     }",
      "1480:     suspected_sensitive -= exclude_list",
      "1481:     sensitive_values.update(suspected_sensitive)",
      "1482:     assert sensitive_values == conf.sensitive_config_values",
      "",
      "---------------"
    ],
    "tests/www/views/test_views_configuration.py||tests/www/views/test_views_configuration.py": [
      "File: tests/www/views/test_views_configuration.py -> tests/www/views/test_views_configuration.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "19: import html",
      "22: from tests.test_utils.config import conf_vars",
      "23: from tests.test_utils.www import check_content_in_response, check_content_not_in_response",
      "",
      "[Removed Lines]",
      "21: from airflow.configuration import SENSITIVE_CONFIG_VALUES, conf",
      "",
      "[Added Lines]",
      "21: from airflow.configuration import conf",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "36: @conf_vars({(\"webserver\", \"expose_config\"): \"True\"})",
      "37: def test_user_can_view_configuration(admin_client):",
      "38:     resp = admin_client.get(\"configuration\", follow_redirects=True)",
      "40:         value = conf.get(section, key, fallback=\"\")",
      "41:         if not value:",
      "42:             continue",
      "",
      "[Removed Lines]",
      "39:     for section, key in SENSITIVE_CONFIG_VALUES:",
      "",
      "[Added Lines]",
      "39:     for section, key in conf.sensitive_config_values:",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "46: @conf_vars({(\"webserver\", \"expose_config\"): \"non-sensitive-only\"})",
      "47: def test_configuration_redacted(admin_client):",
      "48:     resp = admin_client.get(\"configuration\", follow_redirects=True)",
      "50:         value = conf.get(section, key, fallback=\"\")",
      "51:         if not value or value == \"airflow\":",
      "52:             continue",
      "",
      "[Removed Lines]",
      "49:     for section, key in SENSITIVE_CONFIG_VALUES:",
      "",
      "[Added Lines]",
      "49:     for section, key in conf.sensitive_config_values:",
      "",
      "---------------",
      "--- Hunk 4 ---",
      "[Context before]",
      "58: @conf_vars({(\"webserver\", \"expose_config\"): \"non-sensitive-only\"})",
      "59: def test_configuration_redacted_in_running_configuration(admin_client):",
      "60:     resp = admin_client.get(\"configuration\", follow_redirects=True)",
      "62:         value = conf.get(section, key, fallback=\"\")",
      "63:         if not value or value == \"airflow\":",
      "64:             continue",
      "",
      "[Removed Lines]",
      "61:     for section, key in SENSITIVE_CONFIG_VALUES:",
      "",
      "[Added Lines]",
      "61:     for section, key in conf.sensitive_config_values:",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "af11a193694a100c75042fa0644b2818c2ccd6db",
      "candidate_info": {
        "commit_hash": "af11a193694a100c75042fa0644b2818c2ccd6db",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/af11a193694a100c75042fa0644b2818c2ccd6db",
        "files": [
          "airflow/providers/amazon/aws/log/s3_task_handler.py",
          "airflow/providers/microsoft/azure/log/wasb_task_handler.py",
          "airflow/utils/log/file_task_handler.py"
        ],
        "message": "Add docstring and signature for _read_remote_logs (#31623)\n\nCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>\n(cherry picked from commit ce7766e0a52e15b2b1ef7e7f9c613ea686fbfca6)",
        "before_after_code_files": [
          "airflow/providers/amazon/aws/log/s3_task_handler.py||airflow/providers/amazon/aws/log/s3_task_handler.py",
          "airflow/providers/microsoft/azure/log/wasb_task_handler.py||airflow/providers/microsoft/azure/log/wasb_task_handler.py",
          "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/31796"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/providers/amazon/aws/log/s3_task_handler.py||airflow/providers/amazon/aws/log/s3_task_handler.py": [
          "File: airflow/providers/amazon/aws/log/s3_task_handler.py -> airflow/providers/amazon/aws/log/s3_task_handler.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "110:         # Mark closed so we don't double write if close is called twice",
          "111:         self.closed = True",
          "114:         # Explicitly getting log relative path is necessary as the given",
          "115:         # task instance might be different than task instance passed in",
          "116:         # in set_context method.",
          "",
          "[Removed Lines]",
          "113:     def _read_remote_logs(self, ti, try_number, metadata=None):",
          "",
          "[Added Lines]",
          "113:     def _read_remote_logs(self, ti, try_number, metadata=None) -> tuple[list[str], list[str]]:",
          "",
          "---------------"
        ],
        "airflow/providers/microsoft/azure/log/wasb_task_handler.py||airflow/providers/microsoft/azure/log/wasb_task_handler.py": [
          "File: airflow/providers/microsoft/azure/log/wasb_task_handler.py -> airflow/providers/microsoft/azure/log/wasb_task_handler.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "128:         # Mark closed so we don't double write if close is called twice",
          "129:         self.closed = True",
          "132:         messages = []",
          "133:         logs = []",
          "134:         worker_log_relative_path = self._render_filename(ti, try_number)",
          "",
          "[Removed Lines]",
          "131:     def _read_remote_logs(self, ti, try_number, metadata=None):",
          "",
          "[Added Lines]",
          "131:     def _read_remote_logs(self, ti, try_number, metadata=None) -> tuple[list[str], list[str]]:",
          "",
          "---------------"
        ],
        "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py": [
          "File: airflow/utils/log/file_task_handler.py -> airflow/utils/log/file_task_handler.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "522:             logger.exception(\"Could not read served logs\")",
          "523:         return messages, logs",
          "527:         raise NotImplementedError",
          "",
          "[Removed Lines]",
          "525:     def _read_remote_logs(self, ti, try_number, metadata=None):",
          "526:         \"\"\"Implement in subclasses to read from the remote service\"\"\"",
          "",
          "[Added Lines]",
          "525:     def _read_remote_logs(self, ti, try_number, metadata=None) -> tuple[list[str], list[str]]:",
          "526:         \"\"\"",
          "527:         Implement in subclasses to read from the remote service.",
          "529:         This method should return two lists, messages and logs.",
          "532:           such as, \"reading from x file\".",
          "534:         \"\"\"",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "91cbb84bc924543b6b3754556410509ff902508b",
      "candidate_info": {
        "commit_hash": "91cbb84bc924543b6b3754556410509ff902508b",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/91cbb84bc924543b6b3754556410509ff902508b",
        "files": [
          "RELEASE_NOTES.rst",
          "airflow/utils/db.py"
        ],
        "message": "Update release note and revision head",
        "before_after_code_files": [
          "airflow/utils/db.py||airflow/utils/db.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/31796"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/utils/db.py||airflow/utils/db.py": [
          "File: airflow/utils/db.py -> airflow/utils/db.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "81:     \"2.5.3\": \"290244fb8b83\",",
          "82:     \"2.6.0\": \"98ae134e6fff\",",
          "83:     \"2.6.1\": \"98ae134e6fff\",",
          "85: }",
          "",
          "[Removed Lines]",
          "84:     \"2.6.2\": \"98ae134e6fff\",",
          "",
          "[Added Lines]",
          "84:     \"2.6.2\": \"c804e5c76e3e\",",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "aeb4ef367952510e4693da3c991400ca90425a27",
      "candidate_info": {
        "commit_hash": "aeb4ef367952510e4693da3c991400ca90425a27",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/aeb4ef367952510e4693da3c991400ca90425a27",
        "files": [
          "airflow/utils/task_group.py",
          "tests/decorators/test_task_group.py",
          "tests/utils/test_task_group.py"
        ],
        "message": "Fix overriding `default_args` in nested task groups (#31608)\n\n* add unit tests for default_args overriding in task group\n\nSigned-off-by: Hussein Awala <hussein@awala.fr>\n\n* fix overriding default args in nested task groups\n\nSigned-off-by: Hussein Awala <hussein@awala.fr>\n\n* Update airflow/utils/task_group.py\n\nCo-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>\n\n---------\n\nSigned-off-by: Hussein Awala <hussein@awala.fr>\nCo-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>\n(cherry picked from commit 9e8627faa71e9d2047816b291061c28585809508)",
        "before_after_code_files": [
          "airflow/utils/task_group.py||airflow/utils/task_group.py",
          "tests/decorators/test_task_group.py||tests/decorators/test_task_group.py",
          "tests/utils/test_task_group.py||tests/utils/test_task_group.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/31796"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/utils/task_group.py||airflow/utils/task_group.py": [
          "File: airflow/utils/task_group.py -> airflow/utils/task_group.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "141:         if parent_group:",
          "142:             parent_group.add(self)",
          "144:         self.used_group_ids.add(self.group_id)",
          "145:         if self.group_id:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "143:             self._update_default_args(parent_group)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "176:             else:",
          "177:                 self._group_id = f\"{base}__{suffixes[-1] + 1}\"",
          "179:     @classmethod",
          "180:     def create_root(cls, dag: DAG) -> TaskGroup:",
          "181:         \"\"\"Create a root TaskGroup with no group_id or parent.\"\"\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "180:     def _update_default_args(self, parent_group: TaskGroup):",
          "181:         if parent_group.default_args:",
          "182:             self.default_args = {**self.default_args, **parent_group.default_args}",
          "",
          "---------------"
        ],
        "tests/decorators/test_task_group.py||tests/decorators/test_task_group.py": [
          "File: tests/decorators/test_task_group.py -> tests/decorators/test_task_group.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "17: # under the License.",
          "18: from __future__ import annotations",
          "20: import pendulum",
          "21: import pytest",
          "23: from airflow.decorators import dag, task_group",
          "24: from airflow.models.expandinput import DictOfListsExpandInput, ListOfDictsExpandInput, MappedArgument",
          "25: from airflow.utils.task_group import MappedTaskGroup",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "20: from datetime import timedelta",
          "27: from airflow.operators.empty import EmptyOperator",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "186:     assert tg._expand_input == ListOfDictsExpandInput([{\"b\": \"x\"}, {\"b\": None}])",
          "188:     assert saved == {\"a\": 1, \"b\": MappedArgument(input=tg._expand_input, key=\"b\")}",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "194: def test_override_dag_default_args():",
          "195:     @dag(",
          "196:         dag_id=\"test_dag\",",
          "197:         start_date=pendulum.parse(\"20200101\"),",
          "198:         default_args={",
          "199:             \"retries\": 1,",
          "200:             \"owner\": \"x\",",
          "201:         },",
          "202:     )",
          "203:     def pipeline():",
          "204:         @task_group(",
          "205:             group_id=\"task_group\",",
          "206:             default_args={",
          "207:                 \"owner\": \"y\",",
          "208:                 \"execution_timeout\": timedelta(seconds=10),",
          "209:             },",
          "210:         )",
          "211:         def tg():",
          "212:             EmptyOperator(task_id=\"task\")",
          "214:         tg()",
          "216:     test_dag = pipeline()",
          "217:     test_task = test_dag.task_group_dict[\"task_group\"].children[\"task_group.task\"]",
          "218:     assert test_task.retries == 1",
          "219:     assert test_task.owner == \"y\"",
          "220:     assert test_task.execution_timeout == timedelta(seconds=10)",
          "223: def test_override_dag_default_args_nested_tg():",
          "224:     @dag(",
          "225:         dag_id=\"test_dag\",",
          "226:         start_date=pendulum.parse(\"20200101\"),",
          "227:         default_args={",
          "228:             \"retries\": 1,",
          "229:             \"owner\": \"x\",",
          "230:         },",
          "231:     )",
          "232:     def pipeline():",
          "233:         @task_group(",
          "234:             group_id=\"task_group\",",
          "235:             default_args={",
          "236:                 \"owner\": \"y\",",
          "237:                 \"execution_timeout\": timedelta(seconds=10),",
          "238:             },",
          "239:         )",
          "240:         def tg():",
          "241:             @task_group(group_id=\"nested_task_group\")",
          "242:             def nested_tg():",
          "243:                 @task_group(group_id=\"another_task_group\")",
          "244:                 def another_tg():",
          "245:                     EmptyOperator(task_id=\"task\")",
          "247:                 another_tg()",
          "249:             nested_tg()",
          "251:         tg()",
          "253:     test_dag = pipeline()",
          "254:     test_task = (",
          "255:         test_dag.task_group_dict[\"task_group\"]",
          "256:         .children[\"task_group.nested_task_group\"]",
          "257:         .children[\"task_group.nested_task_group.another_task_group\"]",
          "258:         .children[\"task_group.nested_task_group.another_task_group.task\"]",
          "259:     )",
          "260:     assert test_task.retries == 1",
          "261:     assert test_task.owner == \"y\"",
          "262:     assert test_task.execution_timeout == timedelta(seconds=10)",
          "",
          "---------------"
        ],
        "tests/utils/test_task_group.py||tests/utils/test_task_group.py": [
          "File: tests/utils/test_task_group.py -> tests/utils/test_task_group.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "17: # under the License.",
          "18: from __future__ import annotations",
          "20: import pendulum",
          "21: import pytest",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "20: from datetime import timedelta",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1301:         \"section_2.task3\",",
          "1302:         \"section_2.bash_task\",",
          "1303:     ]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1308: def test_override_dag_default_args():",
          "1309:     with DAG(",
          "1310:         dag_id=\"test_dag\",",
          "1311:         start_date=pendulum.parse(\"20200101\"),",
          "1312:         default_args={",
          "1313:             \"retries\": 1,",
          "1314:             \"owner\": \"x\",",
          "1315:         },",
          "1316:     ):",
          "1317:         with TaskGroup(",
          "1318:             group_id=\"task_group\",",
          "1319:             default_args={",
          "1320:                 \"owner\": \"y\",",
          "1321:                 \"execution_timeout\": timedelta(seconds=10),",
          "1322:             },",
          "1323:         ):",
          "1324:             task = EmptyOperator(task_id=\"task\")",
          "1326:     assert task.retries == 1",
          "1327:     assert task.owner == \"y\"",
          "1328:     assert task.execution_timeout == timedelta(seconds=10)",
          "1331: def test_override_dag_default_args_in_nested_tg():",
          "1332:     with DAG(",
          "1333:         dag_id=\"test_dag\",",
          "1334:         start_date=pendulum.parse(\"20200101\"),",
          "1335:         default_args={",
          "1336:             \"retries\": 1,",
          "1337:             \"owner\": \"x\",",
          "1338:         },",
          "1339:     ):",
          "1340:         with TaskGroup(",
          "1341:             group_id=\"task_group\",",
          "1342:             default_args={",
          "1343:                 \"owner\": \"y\",",
          "1344:                 \"execution_timeout\": timedelta(seconds=10),",
          "1345:             },",
          "1346:         ):",
          "1347:             with TaskGroup(group_id=\"nested_task_group\"):",
          "1348:                 task = EmptyOperator(task_id=\"task\")",
          "1350:     assert task.retries == 1",
          "1351:     assert task.owner == \"y\"",
          "1352:     assert task.execution_timeout == timedelta(seconds=10)",
          "1355: def test_override_dag_default_args_in_multi_level_nested_tg():",
          "1356:     with DAG(",
          "1357:         dag_id=\"test_dag\",",
          "1358:         start_date=pendulum.parse(\"20200101\"),",
          "1359:         default_args={",
          "1360:             \"retries\": 1,",
          "1361:             \"owner\": \"x\",",
          "1362:         },",
          "1363:     ):",
          "1364:         with TaskGroup(",
          "1365:             group_id=\"task_group\",",
          "1366:             default_args={",
          "1367:                 \"owner\": \"y\",",
          "1368:                 \"execution_timeout\": timedelta(seconds=10),",
          "1369:             },",
          "1370:         ):",
          "1371:             with TaskGroup(group_id=\"first_nested_task_group\"):",
          "1372:                 with TaskGroup(group_id=\"second_nested_task_group\"):",
          "1373:                     with TaskGroup(group_id=\"third_nested_task_group\"):",
          "1374:                         task = EmptyOperator(task_id=\"task\")",
          "1376:     assert task.retries == 1",
          "1377:     assert task.owner == \"y\"",
          "1378:     assert task.execution_timeout == timedelta(seconds=10)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "3bbf6fd73aef62c2030b8babaacb4791c49ba04a",
      "candidate_info": {
        "commit_hash": "3bbf6fd73aef62c2030b8babaacb4791c49ba04a",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/3bbf6fd73aef62c2030b8babaacb4791c49ba04a",
        "files": [
          "airflow/configuration.py",
          "airflow/providers/odbc/hooks/odbc.py",
          "docs/apache-airflow-providers-odbc/connections/odbc.rst",
          "docs/apache-airflow/howto/set-config.rst",
          "tests/core/test_configuration.py",
          "tests/providers/odbc/hooks/test_odbc.py"
        ],
        "message": "Control permissibility of driver config in extra from airflow.cfg (#31754)\n\nRefines https://github.com/apache/airflow/pull/31713, which disabled (by default) setting driver through extra.  Here we make it so that the flag to enable is located in airflow config instead of hook param.\n\n(cherry picked from commit 438ba41e142593f2b0916893eccbd08fbe4d277b)",
        "before_after_code_files": [
          "airflow/configuration.py||airflow/configuration.py",
          "airflow/providers/odbc/hooks/odbc.py||airflow/providers/odbc/hooks/odbc.py",
          "tests/core/test_configuration.py||tests/core/test_configuration.py",
          "tests/providers/odbc/hooks/test_odbc.py||tests/providers/odbc/hooks/test_odbc.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/31796"
        ],
        "olp_code_files": {
          "patch": [
            "airflow/configuration.py||airflow/configuration.py",
            "tests/core/test_configuration.py||tests/core/test_configuration.py"
          ],
          "candidate": [
            "airflow/configuration.py||airflow/configuration.py",
            "tests/core/test_configuration.py||tests/core/test_configuration.py"
          ]
        }
      },
      "candidate_diff": {
        "airflow/configuration.py||airflow/configuration.py": [
          "File: airflow/configuration.py -> airflow/configuration.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "488:         )",
          "490:     def _env_var_name(self, section: str, key: str) -> str:",
          "493:     def _get_env_var_option(self, section: str, key: str):",
          "494:         # must have format AIRFLOW__{SECTION}__{KEY} (note double underscore)",
          "",
          "[Removed Lines]",
          "491:         return f\"{ENV_VAR_PREFIX}{section.upper()}__{key.upper()}\"",
          "",
          "[Added Lines]",
          "491:         return f\"{ENV_VAR_PREFIX}{section.replace('.', '_').upper()}__{key.upper()}\"",
          "",
          "---------------"
        ],
        "airflow/providers/odbc/hooks/odbc.py||airflow/providers/odbc/hooks/odbc.py": [
          "File: airflow/providers/odbc/hooks/odbc.py -> airflow/providers/odbc/hooks/odbc.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "30:     \"\"\"",
          "31:     Interact with odbc data sources using pyodbc.",
          "33:     See :doc:`/connections/odbc` for full documentation.",
          "34:     \"\"\"",
          "36:     DEFAULT_SQLALCHEMY_SCHEME = \"mssql+pyodbc\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "33:     To configure driver, in addition to supplying as constructor arg, the following are also supported:",
          "36:           section ``providers.odbc`` section of airflow config.",
          "41:     :param args: passed to DbApiHook",
          "42:     :param database: database to use -- overrides connection ``schema``",
          "43:     :param driver: name of driver or path to driver. see above for more info",
          "44:     :param dsn: name of DSN to use.  overrides DSN supplied in connection ``extra``",
          "45:     :param connect_kwargs: keyword arguments passed to ``pyodbc.connect``",
          "46:     :param sqlalchemy_scheme: Scheme sqlalchemy connection.  Default is ``mssql+pyodbc`` Only used for",
          "47:         ``get_sqlalchemy_engine`` and ``get_sqlalchemy_connection`` methods.",
          "48:     :param kwargs: passed to DbApiHook",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "40:     hook_name = \"ODBC\"",
          "41:     supports_autocommit = True",
          "43:     def __init__(",
          "44:         self,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "58:     default_driver: str | None = None",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "102:     @property",
          "103:     def driver(self) -> str | None:",
          "104:         \"\"\"Driver from init param if given; else try to find one in connection extra.\"\"\"",
          "105:         if not self._driver:",
          "106:             driver = self.connection_extra_lower.get(\"driver\")",
          "107:             if driver:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "122:         extra_driver = self.connection_extra_lower.get(\"driver\")",
          "123:         from airflow.configuration import conf",
          "125:         if extra_driver and conf.getboolean(\"providers.odbc\", \"allow_driver_in_extra\", fallback=False):",
          "126:             self._driver = extra_driver",
          "127:         else:",
          "128:             self.log.warning(",
          "129:                 \"You have supplied 'driver' via connection extra but it will not be used. In order to \"",
          "130:                 \"use 'driver' from extra you must set airflow config setting `allow_driver_in_extra = True` \"",
          "131:                 \"in section `providers.odbc`. Alternatively you may specify driver via 'driver' parameter of \"",
          "132:                 \"the hook constructor or via 'hook_params' dictionary with key 'driver' if using SQL \"",
          "133:                 \"operators.\"",
          "134:             )",
          "",
          "---------------"
        ],
        "tests/core/test_configuration.py||tests/core/test_configuration.py": [
          "File: tests/core/test_configuration.py -> tests/core/test_configuration.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "226:         assert \"key4\" not in cfg_dict[\"test\"]",
          "227:         assert \"printf key4_result\" == cfg_dict[\"test\"][\"key4_cmd\"]",
          "229:     @mock.patch(\"airflow.providers.hashicorp._internal_client.vault_client.hvac\")",
          "230:     @conf_vars(",
          "231:         {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "229:     def test_can_read_dot_section(self):",
          "230:         test_config = \"\"\"[test.abc]",
          "231: key1 = true",
          "232: \"\"\"",
          "233:         test_conf = AirflowConfigParser()",
          "234:         test_conf.read_string(test_config)",
          "235:         section = \"test.abc\"",
          "236:         key = \"key1\"",
          "237:         assert test_conf.getboolean(section, key) is True",
          "239:         with mock.patch.dict(",
          "240:             \"os.environ\",",
          "241:             {",
          "242:                 \"AIRFLOW__TEST_ABC__KEY1\": \"false\",  # note that the '.' is converted to '_'",
          "243:             },",
          "244:         ):",
          "245:             assert test_conf.getboolean(section, key) is False",
          "",
          "---------------"
        ],
        "tests/providers/odbc/hooks/test_odbc.py||tests/providers/odbc/hooks/test_odbc.py": [
          "File: tests/providers/odbc/hooks/test_odbc.py -> tests/providers/odbc/hooks/test_odbc.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "177:         assert hook.driver == \"Blah driver\"",
          "178:         hook = self.get_hook(hook_params=dict(driver=\"{Blah driver}\"))",
          "179:         assert hook.driver == \"Blah driver\"",
          "181:         assert hook.driver == \"Blah driver\"",
          "182:         hook = self.get_hook(conn_params=dict(extra='{\"driver\": \"{Blah driver}\"}'))",
          "183:         assert hook.driver == \"Blah driver\"",
          "185:     def test_database(self):",
          "186:         hook = self.get_hook(hook_params=dict(database=\"abc\"))",
          "187:         assert hook.database == \"abc\"",
          "",
          "[Removed Lines]",
          "180:         hook = self.get_hook(conn_params=dict(extra='{\"driver\": \"Blah driver\"}'))",
          "",
          "[Added Lines]",
          "181:     def test_driver_extra_raises_warning_by_default(self, caplog):",
          "182:         with caplog.at_level(logging.WARNING, logger=\"airflow.providers.odbc.hooks.test_odbc\"):",
          "183:             driver = self.get_hook(conn_params=dict(extra='{\"driver\": \"Blah driver\"}')).driver",
          "184:             assert \"You have supplied 'driver' via connection extra but it will not be used\" in caplog.text",
          "185:             assert driver is None",
          "187:     @mock.patch.dict(\"os.environ\", {\"AIRFLOW__PROVIDERS_ODBC__ALLOW_DRIVER_IN_EXTRA\": \"TRUE\"})",
          "188:     def test_driver_extra_works_when_allow_driver_extra(self):",
          "189:         hook = self.get_hook(",
          "190:             conn_params=dict(extra='{\"driver\": \"Blah driver\"}'), hook_params=dict(allow_driver_extra=True)",
          "191:         )",
          "196:     def test_driver_none_by_default(self):",
          "197:         hook = self.get_hook()",
          "198:         assert hook.driver is None",
          "200:     def test_driver_extra_raises_warning_and_returns_default_driver_by_default(self, caplog):",
          "201:         with patch.object(OdbcHook, \"default_driver\", \"Blah driver\"):",
          "202:             with caplog.at_level(logging.WARNING, logger=\"airflow.providers.odbc.hooks.test_odbc\"):",
          "203:                 driver = self.get_hook(conn_params=dict(extra='{\"driver\": \"Blah driver2\"}')).driver",
          "204:                 assert \"have supplied 'driver' via connection extra but it will not be used\" in caplog.text",
          "205:                 assert driver == \"Blah driver\"",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "a1598ed80d392b4f986af03a302bfbd44f736ede",
      "candidate_info": {
        "commit_hash": "a1598ed80d392b4f986af03a302bfbd44f736ede",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/a1598ed80d392b4f986af03a302bfbd44f736ede",
        "files": [
          "airflow/models/skipmixin.py",
          "airflow/operators/python.py",
          "airflow/operators/subdag.py",
          "tests/operators/test_python.py",
          "tests/operators/test_subdag_operator.py"
        ],
        "message": "Add the missing `map_index` to the xcom key when skipping downstream tasks (#31541)\n\n* Add the missing map_index to the xcom key when skiping downstream tasks\n\n* fix subdag operator tests\n\n* Update tests/operators/test_subdag_operator.py\n\nCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>\n\n---------\n\nCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>\n(cherry picked from commit e2da3151d49dae636cb6901f3d3e124a49cbf514)",
        "before_after_code_files": [
          "airflow/models/skipmixin.py||airflow/models/skipmixin.py",
          "airflow/operators/python.py||airflow/operators/python.py",
          "airflow/operators/subdag.py||airflow/operators/subdag.py",
          "tests/operators/test_python.py||tests/operators/test_python.py",
          "tests/operators/test_subdag_operator.py||tests/operators/test_subdag_operator.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/31796"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/models/skipmixin.py||airflow/models/skipmixin.py": [
          "File: airflow/models/skipmixin.py -> airflow/models/skipmixin.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "86:         execution_date: DateTime,",
          "87:         tasks: Iterable[DAGNode],",
          "88:         session: Session = NEW_SESSION,",
          "89:     ):",
          "90:         \"\"\"",
          "91:         Sets tasks instances to skipped from the same dag run.",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "89:         map_index: int = -1,",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "98:         :param execution_date: execution_date",
          "99:         :param tasks: tasks to skip (not task_ids)",
          "100:         :param session: db session to use",
          "101:         \"\"\"",
          "102:         task_list = _ensure_tasks(tasks)",
          "103:         if not task_list:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "102:         :param map_index: map_index of the current task instance",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "142:                 task_id=task_id,",
          "143:                 dag_id=dag_run.dag_id,",
          "144:                 run_id=dag_run.run_id,",
          "145:                 session=session,",
          "146:             )",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "147:                 map_index=map_index,",
          "",
          "---------------"
        ],
        "airflow/operators/python.py||airflow/operators/python.py": [
          "File: airflow/operators/python.py -> airflow/operators/python.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "266:             if self.ignore_downstream_trigger_rules is True:",
          "267:                 self.log.info(\"Skipping all downstream tasks...\")",
          "269:             else:",
          "270:                 self.log.info(\"Skipping downstream tasks while respecting trigger rules...\")",
          "271:                 # Explicitly setting the state of the direct, downstream task(s) to \"skipped\" and letting the",
          "272:                 # Scheduler handle the remaining downstream task(s) appropriately.",
          "275:         self.log.info(\"Done.\")",
          "",
          "[Removed Lines]",
          "268:                 self.skip(dag_run, execution_date, downstream_tasks)",
          "273:                 self.skip(dag_run, execution_date, context[\"task\"].get_direct_relatives(upstream=False))",
          "",
          "[Added Lines]",
          "268:                 self.skip(dag_run, execution_date, downstream_tasks, map_index=context[\"ti\"].map_index)",
          "273:                 self.skip(",
          "274:                     dag_run,",
          "275:                     execution_date,",
          "276:                     context[\"task\"].get_direct_relatives(upstream=False),",
          "277:                     map_index=context[\"ti\"].map_index,",
          "278:                 )",
          "",
          "---------------"
        ],
        "airflow/operators/subdag.py||airflow/operators/subdag.py": [
          "File: airflow/operators/subdag.py -> airflow/operators/subdag.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "226:         self.log.debug(\"Downstream task_ids %s\", downstream_tasks)",
          "228:         if downstream_tasks:",
          "231:         self.log.info(\"Done.\")",
          "",
          "[Removed Lines]",
          "229:             self.skip(context[\"dag_run\"], context[\"execution_date\"], downstream_tasks)",
          "",
          "[Added Lines]",
          "229:             self.skip(",
          "230:                 context[\"dag_run\"],",
          "231:                 context[\"execution_date\"],",
          "232:                 downstream_tasks,",
          "233:                 map_index=context[\"ti\"].map_index,",
          "234:             )",
          "",
          "---------------"
        ],
        "tests/operators/test_python.py||tests/operators/test_python.py": [
          "File: tests/operators/test_python.py -> tests/operators/test_python.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "31: import pytest",
          "32: from slugify import slugify",
          "35: from airflow.models import DAG, DagRun, TaskInstance as TI",
          "36: from airflow.models.baseoperator import BaseOperator",
          "37: from airflow.models.taskinstance import clear_task_instances, set_current_context",
          "",
          "[Removed Lines]",
          "34: from airflow.exceptions import AirflowException, RemovedInAirflow3Warning",
          "",
          "[Added Lines]",
          "34: from airflow.decorators import task_group",
          "35: from airflow.exceptions import AirflowException, DeserializingResultError, RemovedInAirflow3Warning",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "604:         assert tis[0].xcom_pull(task_ids=short_op_push_xcom.task_id, key=\"return_value\") == \"signature\"",
          "605:         assert tis[0].xcom_pull(task_ids=short_op_no_push_xcom.task_id, key=\"return_value\") is None",
          "608: virtualenv_string_args: list[str] = []",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "608:     def test_xcom_push_skipped_tasks(self):",
          "609:         with self.dag:",
          "610:             short_op_push_xcom = ShortCircuitOperator(",
          "611:                 task_id=\"push_xcom_from_shortcircuit\", python_callable=lambda: False",
          "612:             )",
          "613:             empty_task = EmptyOperator(task_id=\"empty_task\")",
          "614:             short_op_push_xcom >> empty_task",
          "615:         dr = self.create_dag_run()",
          "616:         short_op_push_xcom.run(start_date=self.default_date, end_date=self.default_date)",
          "617:         tis = dr.get_task_instances()",
          "618:         assert tis[0].xcom_pull(task_ids=short_op_push_xcom.task_id, key=\"skipmixin_key\") == {",
          "619:             \"skipped\": [\"empty_task\"]",
          "620:         }",
          "622:     def test_mapped_xcom_push_skipped_tasks(self, session):",
          "623:         with self.dag:",
          "625:             @task_group",
          "626:             def group(x):",
          "627:                 short_op_push_xcom = ShortCircuitOperator(",
          "628:                     task_id=\"push_xcom_from_shortcircuit\",",
          "629:                     python_callable=lambda arg: arg % 2 == 0,",
          "630:                     op_kwargs={\"arg\": x},",
          "631:                 )",
          "632:                 empty_task = EmptyOperator(task_id=\"empty_task\")",
          "633:                 short_op_push_xcom >> empty_task",
          "635:             group.expand(x=[0, 1])",
          "636:         dr = self.create_dag_run()",
          "637:         decision = dr.task_instance_scheduling_decisions(session=session)",
          "638:         for ti in decision.schedulable_tis:",
          "639:             ti.run()",
          "640:         # dr.run(start_date=self.default_date, end_date=self.default_date)",
          "641:         tis = dr.get_task_instances()",
          "643:         assert (",
          "644:             tis[0].xcom_pull(task_ids=\"group.push_xcom_from_shortcircuit\", key=\"return_value\", map_indexes=0)",
          "645:             is True",
          "646:         )",
          "647:         assert (",
          "648:             tis[0].xcom_pull(task_ids=\"group.push_xcom_from_shortcircuit\", key=\"skipmixin_key\", map_indexes=0)",
          "649:             is None",
          "650:         )",
          "651:         assert tis[0].xcom_pull(",
          "652:             task_ids=\"group.push_xcom_from_shortcircuit\", key=\"skipmixin_key\", map_indexes=1",
          "653:         ) == {\"skipped\": [\"group.empty_task\"]}",
          "",
          "---------------"
        ],
        "tests/operators/test_subdag_operator.py||tests/operators/test_subdag_operator.py": [
          "File: tests/operators/test_subdag_operator.py -> tests/operators/test_subdag_operator.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "333:             for task, state in zip(dummy_subdag_tasks, states)",
          "334:         ]",
          "337:         subdag_task.post_execute(context)",
          "339:         if skip_parent:",
          "341:         else:",
          "342:             mock_skip.assert_not_called()",
          "",
          "[Removed Lines]",
          "336:         context = {\"execution_date\": DEFAULT_DATE, \"dag_run\": dag_run, \"task\": subdag_task}",
          "340:             mock_skip.assert_called_once_with(context[\"dag_run\"], context[\"execution_date\"], [dummy_dag_task])",
          "",
          "[Added Lines]",
          "336:         context = {",
          "337:             \"execution_date\": DEFAULT_DATE,",
          "338:             \"dag_run\": dag_run,",
          "339:             \"task\": subdag_task,",
          "340:             \"ti\": mock.MagicMock(map_index=-1),",
          "341:         }",
          "345:             mock_skip.assert_called_once_with(",
          "346:                 context[\"dag_run\"], context[\"execution_date\"], [dummy_dag_task], map_index=-1",
          "347:             )",
          "",
          "---------------"
        ]
      }
    }
  ]
}