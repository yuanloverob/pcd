{
  "cve_id": "CVE-2023-45348",
  "cve_desc": "Apache Airflow, versions 2.7.0 and 2.7.1, is affected by a vulnerability that allows an authenticated user to retrieve sensitive configuration information when the \"expose_config\" option is set to \"non-sensitive-only\". The `expose_config` option is False by default.\nIt is recommended to upgrade to a version that is not affected.",
  "repo": "apache/airflow",
  "patch_hash": "a4a0b5dd3d0ce05311c70bb9a32b66a650dbc0b4",
  "patch_info": {
    "commit_hash": "a4a0b5dd3d0ce05311c70bb9a32b66a650dbc0b4",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/a4a0b5dd3d0ce05311c70bb9a32b66a650dbc0b4",
    "files": [
      "airflow/api_connexion/endpoints/config_endpoint.py",
      "airflow/configuration.py",
      "tests/api_connexion/endpoints/test_config_endpoint.py"
    ],
    "message": "Check if the lower of provided values are sensitives in config endpoint (#34712)\n\n* Check if the lower of provided values are sensitives in config endpoint\n\n* update unit test\n\n* ensure that all values in sensitive dict are in lower characters\n\n(cherry picked from commit f044589b685855a8fce8f5376bea2564c5a001f7)",
    "before_after_code_files": [
      "airflow/api_connexion/endpoints/config_endpoint.py||airflow/api_connexion/endpoints/config_endpoint.py",
      "airflow/configuration.py||airflow/configuration.py",
      "tests/api_connexion/endpoints/test_config_endpoint.py||tests/api_connexion/endpoints/test_config_endpoint.py"
    ]
  },
  "patch_diff": {
    "airflow/api_connexion/endpoints/config_endpoint.py||airflow/api_connexion/endpoints/config_endpoint.py": [
      "File: airflow/api_connexion/endpoints/config_endpoint.py -> airflow/api_connexion/endpoints/config_endpoint.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "123:                 \"Config not found.\", detail=f\"The option [{section}/{option}] is not found in config.\"",
      "124:             )",
      "127:             value = \"< hidden >\"",
      "128:         else:",
      "129:             value = conf.get(section, option)",
      "",
      "[Removed Lines]",
      "126:         if (section, option) in conf.sensitive_config_values:",
      "",
      "[Added Lines]",
      "126:         if (section.lower(), option.lower()) in conf.sensitive_config_values:",
      "",
      "---------------"
    ],
    "airflow/configuration.py||airflow/configuration.py": [
      "File: airflow/configuration.py -> airflow/configuration.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "311:             for s, s_c in self.configuration_description.items()",
      "312:             for k, item in s_c.get(\"options\").items()  # type: ignore[union-attr]",
      "313:         }",
      "315:         depr_option = {self.deprecated_options[x][:-1] for x in sensitive if x in self.deprecated_options}",
      "316:         depr_section = {",
      "317:             (self.deprecated_sections[s][0], k) for s, k in sensitive if s in self.deprecated_sections",
      "",
      "[Removed Lines]",
      "314:         sensitive = {(section, key) for (section, key), v in flattened.items() if v.get(\"sensitive\") is True}",
      "",
      "[Added Lines]",
      "314:         sensitive = {",
      "315:             (section.lower(), key.lower())",
      "316:             for (section, key), v in flattened.items()",
      "317:             if v.get(\"sensitive\") is True",
      "318:         }",
      "",
      "---------------"
    ],
    "tests/api_connexion/endpoints/test_config_endpoint.py||tests/api_connexion/endpoints/test_config_endpoint.py": [
      "File: tests/api_connexion/endpoints/test_config_endpoint.py -> tests/api_connexion/endpoints/test_config_endpoint.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "247:         return_value=MOCK_CONF_WITH_SENSITIVE_VALUE,",
      "248:     )",
      "249:     @conf_vars({(\"webserver\", \"expose_config\"): \"non-sensitive-only\"})",
      "251:         response = self.client.get(",
      "253:             headers={\"Accept\": \"text/plain\"},",
      "254:             environ_overrides={\"REMOTE_USER\": \"test\"},",
      "255:         )",
      "256:         assert response.status_code == 200",
      "257:         expected = textwrap.dedent(",
      "261:         \"\"\"",
      "262:         )",
      "263:         assert expected == response.data.decode()",
      "",
      "[Removed Lines]",
      "250:     def test_should_respond_200_text_plain_with_non_sensitive_only(self, mock_as_dict):",
      "252:             \"/api/v1/config/section/core/option/sql_alchemy_conn\",",
      "258:             \"\"\"\\",
      "259:         [core]",
      "260:         sql_alchemy_conn = < hidden >",
      "",
      "[Added Lines]",
      "250:     @pytest.mark.parametrize(",
      "251:         \"section, option\",",
      "252:         [",
      "253:             (\"core\", \"sql_alchemy_conn\"),",
      "254:             (\"core\", \"SQL_ALCHEMY_CONN\"),",
      "255:             (\"corE\", \"sql_alchemy_conn\"),",
      "256:             (\"CORE\", \"sql_alchemy_conn\"),",
      "257:         ],",
      "258:     )",
      "259:     def test_should_respond_200_text_plain_with_non_sensitive_only(self, mock_as_dict, section, option):",
      "261:             f\"/api/v1/config/section/{section}/option/{option}\",",
      "267:             f\"\"\"\\",
      "268:         [{section}]",
      "269:         {option} = < hidden >",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "9748d03f9534fd16341c67dafaa7d5fcf2a36d0c",
      "candidate_info": {
        "commit_hash": "9748d03f9534fd16341c67dafaa7d5fcf2a36d0c",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/9748d03f9534fd16341c67dafaa7d5fcf2a36d0c",
        "files": [
          "airflow/models/dagrun.py",
          "airflow/models/taskinstance.py"
        ],
        "message": "Fix unfound ab_user table in the CLI session (#34120)\n\n(cherry picked from commit 2f32769bbb37236044f9c1ecb59809d82d0e3a62)",
        "before_after_code_files": [
          "airflow/models/dagrun.py||airflow/models/dagrun.py",
          "airflow/models/taskinstance.py||airflow/models/taskinstance.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/models/dagrun.py||airflow/models/dagrun.py": [
          "File: airflow/models/dagrun.py -> airflow/models/dagrun.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1407:     __tablename__ = \"dag_run_note\"",
          "1410:     dag_run_id = Column(Integer, primary_key=True, nullable=False)",
          "1411:     content = Column(String(1000).with_variant(Text(1000), \"mysql\"))",
          "1412:     created_at = Column(UtcDateTime, default=timezone.utcnow, nullable=False)",
          "",
          "[Removed Lines]",
          "1409:     user_id = Column(Integer, nullable=True)",
          "",
          "[Added Lines]",
          "1409:     user_id = Column(",
          "1410:         Integer,",
          "1411:         nullable=True,",
          "1412:         foreign_key=ForeignKey(\"ab_user.id\", name=\"dag_run_note_user_fkey\"),",
          "1413:     )",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1422:             name=\"dag_run_note_dr_fkey\",",
          "1423:             ondelete=\"CASCADE\",",
          "1424:         ),",
          "1430:     )",
          "1432:     def __init__(self, content, user_id=None):",
          "",
          "[Removed Lines]",
          "1425:         ForeignKeyConstraint(",
          "1426:             (user_id,),",
          "1427:             [\"ab_user.id\"],",
          "1428:             name=\"dag_run_note_user_fkey\",",
          "1429:         ),",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "airflow/models/taskinstance.py||airflow/models/taskinstance.py": [
          "File: airflow/models/taskinstance.py -> airflow/models/taskinstance.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "43:     Column,",
          "44:     DateTime,",
          "45:     Float,",
          "46:     ForeignKeyConstraint,",
          "47:     Index,",
          "48:     Integer,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "46:     ForeignKey,",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "3016:     __tablename__ = \"task_instance_note\"",
          "3019:     task_id = Column(StringID(), primary_key=True, nullable=False)",
          "3020:     dag_id = Column(StringID(), primary_key=True, nullable=False)",
          "3021:     run_id = Column(StringID(), primary_key=True, nullable=False)",
          "",
          "[Removed Lines]",
          "3018:     user_id = Column(Integer, nullable=True)",
          "",
          "[Added Lines]",
          "3019:     user_id = Column(",
          "3020:         Integer, nullable=True, foreign_key=ForeignKey(\"ab_user.id\", name=\"task_instance_note_user_fkey\")",
          "3021:     )",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "3041:             name=\"task_instance_note_ti_fkey\",",
          "3042:             ondelete=\"CASCADE\",",
          "3043:         ),",
          "3049:     )",
          "3051:     def __init__(self, content, user_id=None):",
          "",
          "[Removed Lines]",
          "3044:         ForeignKeyConstraint(",
          "3045:             (user_id,),",
          "3046:             [\"ab_user.id\"],",
          "3047:             name=\"task_instance_note_user_fkey\",",
          "3048:         ),",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "9fdbe7b96ab162548f137dd438334ed9916bb086",
      "candidate_info": {
        "commit_hash": "9fdbe7b96ab162548f137dd438334ed9916bb086",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/9fdbe7b96ab162548f137dd438334ed9916bb086",
        "files": [
          "airflow/serialization/serializers/timezone.py",
          "setup.py",
          "tests/serialization/serializers/test_serializers.py"
        ],
        "message": "Add support for ZoneInfo and generic UTC (#34683)\n\n* Add support for ZoneInfo and generic UTC\n\nCertain providers rely on other datetime implementations\nand fail to serialize.\n\n(cherry picked from commit 7707f4a9310e032476f392d25912b653771bbda2)",
        "before_after_code_files": [
          "airflow/serialization/serializers/timezone.py||airflow/serialization/serializers/timezone.py",
          "setup.py||setup.py",
          "tests/serialization/serializers/test_serializers.py||tests/serialization/serializers/test_serializers.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/serialization/serializers/timezone.py||airflow/serialization/serializers/timezone.py": [
          "File: airflow/serialization/serializers/timezone.py -> airflow/serialization/serializers/timezone.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "17: # under the License.",
          "18: from __future__ import annotations",
          "22: from airflow.utils.module_loading import qualname",
          "24: if TYPE_CHECKING:",
          "27:     from airflow.serialization.serde import U",
          "31: deserializers = serializers",
          "33: __version__ = 1",
          "",
          "[Removed Lines]",
          "20: from typing import TYPE_CHECKING",
          "25:     from pendulum.tz.timezone import Timezone",
          "30: serializers = [\"pendulum.tz.timezone.FixedTimezone\", \"pendulum.tz.timezone.Timezone\"]",
          "",
          "[Added Lines]",
          "20: import datetime",
          "21: from typing import TYPE_CHECKING, Any, cast",
          "29: serializers = [",
          "30:     \"pendulum.tz.timezone.FixedTimezone\",",
          "31:     \"pendulum.tz.timezone.Timezone\",",
          "32:     \"zoneinfo.ZoneInfo\",",
          "33:     \"backports.zoneinfo.ZoneInfo\",",
          "34: ]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "43:     0 without the special case), but passing 0 into ``pendulum.timezone`` does",
          "44:     not give us UTC (but ``+00:00``).",
          "45:     \"\"\"",
          "48:     name = qualname(o)",
          "49:     if isinstance(o, FixedTimezone):",
          "50:         if o.offset == 0:",
          "51:             return \"UTC\", name, __version__, True",
          "52:         return o.offset, name, __version__, True",
          "57:     return \"\", \"\", 0, False",
          "61:     from pendulum.tz import fixed_timezone, timezone",
          "63:     if not isinstance(data, (str, int)):",
          "",
          "[Removed Lines]",
          "46:     from pendulum.tz.timezone import FixedTimezone, Timezone",
          "54:     if isinstance(o, Timezone):",
          "55:         return o.name, name, __version__, True",
          "60: def deserialize(classname: str, version: int, data: object) -> Timezone:",
          "",
          "[Added Lines]",
          "51:     from pendulum.tz.timezone import FixedTimezone",
          "60:     tz_name = _get_tzinfo_name(cast(datetime.tzinfo, o))",
          "61:     if tz_name is not None:",
          "62:         return tz_name, name, __version__, True",
          "64:     if cast(datetime.tzinfo, o).utcoffset(None) == datetime.timedelta(0):",
          "65:         return \"UTC\", qualname(FixedTimezone), __version__, True",
          "70: def deserialize(classname: str, version: int, data: object) -> Any:",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "69:     if isinstance(data, int):",
          "70:         return fixed_timezone(data)",
          "72:     return timezone(data)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "82:     if classname == \"zoneinfo.ZoneInfo\":",
          "83:         from zoneinfo import ZoneInfo",
          "85:         return ZoneInfo(data)",
          "87:     if classname == \"backports.zoneinfo.ZoneInfo\":",
          "88:         # python version might have been upgraded, so we need to check",
          "89:         try:",
          "90:             from backports.zoneinfo import ZoneInfo",
          "91:         except ImportError:",
          "92:             from zoneinfo import ZoneInfo",
          "94:         return ZoneInfo(data)",
          "99: # ported from pendulum.tz.timezone._get_tzinfo_name",
          "100: def _get_tzinfo_name(tzinfo: datetime.tzinfo | None) -> str | None:",
          "101:     if tzinfo is None:",
          "102:         return None",
          "104:     if hasattr(tzinfo, \"key\"):",
          "105:         # zoneinfo timezone",
          "106:         return tzinfo.key",
          "107:     elif hasattr(tzinfo, \"name\"):",
          "108:         # Pendulum timezone",
          "109:         return tzinfo.name",
          "110:     elif hasattr(tzinfo, \"zone\"):",
          "111:         # pytz timezone",
          "112:         return tzinfo.zone  # type: ignore[no-any-return]",
          "114:     return None",
          "",
          "---------------"
        ],
        "setup.py||setup.py": [
          "File: setup.py -> setup.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "463: _devel_only_tests = [",
          "464:     \"aioresponses\",",
          "465:     \"beautifulsoup4>=4.7.1\",",
          "466:     \"coverage>=7.2\",",
          "467:     \"pytest\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "465:     \"backports.zoneinfo>=0.2.1;python_version<'3.9'\",",
          "",
          "---------------"
        ],
        "tests/serialization/serializers/test_serializers.py||tests/serialization/serializers/test_serializers.py": [
          "File: tests/serialization/serializers/test_serializers.py -> tests/serialization/serializers/test_serializers.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "23: import pandas as pd",
          "24: import pendulum.tz",
          "25: import pytest",
          "26: from pendulum import DateTime",
          "28: from airflow.models.param import Param, ParamsDict",
          "29: from airflow.serialization.serde import DATA, deserialize, serialize",
          "32: class TestSerializers:",
          "33:     def test_datetime(self):",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "26: from dateutil.tz import tzutc",
          "29: from airflow import PY39",
          "33: if PY39:",
          "34:     from zoneinfo import ZoneInfo",
          "35: else:",
          "36:     from backports.zoneinfo import ZoneInfo",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "63:         d = deserialize(s)",
          "64:         assert i.timestamp() == d.timestamp()",
          "68:         s = {",
          "69:             \"__classname__\": \"pendulum.datetime.DateTime\",",
          "70:             \"__version__\": 1,",
          "",
          "[Removed Lines]",
          "66:     def test_deserialize_datetime_v1(self):",
          "",
          "[Added Lines]",
          "73:         i = DateTime(2022, 7, 10, tzinfo=tzutc())",
          "74:         s = serialize(i)",
          "75:         d = deserialize(s)",
          "76:         assert i.timestamp() == d.timestamp()",
          "78:         i = DateTime(2022, 7, 10, tzinfo=ZoneInfo(\"Europe/Paris\"))",
          "79:         s = serialize(i)",
          "80:         d = deserialize(s)",
          "81:         assert i.timestamp() == d.timestamp()",
          "83:     def test_deserialize_datetime_v1(self):",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "b328c8601a8b16bd442b55b2ce9f242dc7a6b515",
      "candidate_info": {
        "commit_hash": "b328c8601a8b16bd442b55b2ce9f242dc7a6b515",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/b328c8601a8b16bd442b55b2ce9f242dc7a6b515",
        "files": [
          "airflow/api_connexion/endpoints/task_instance_endpoint.py",
          "tests/api_connexion/endpoints/test_task_instance_endpoint.py"
        ],
        "message": "REST API: Fix task instance access issue in the batch endpoint (#34315)\n\nCurrently, there's no restriction on the task instances a user can access in\nthe REST API batch fetch task instances endpoint.\nThis PR fixes it\n\n(cherry picked from commit 3df1af4c45705d67598753a96debf5619bbfee04)",
        "before_after_code_files": [
          "airflow/api_connexion/endpoints/task_instance_endpoint.py||airflow/api_connexion/endpoints/task_instance_endpoint.py",
          "tests/api_connexion/endpoints/test_task_instance_endpoint.py||tests/api_connexion/endpoints/test_task_instance_endpoint.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/api_connexion/endpoints/task_instance_endpoint.py||airflow/api_connexion/endpoints/task_instance_endpoint.py": [
          "File: airflow/api_connexion/endpoints/task_instance_endpoint.py -> airflow/api_connexion/endpoints/task_instance_endpoint.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "19: from typing import Any, Iterable, TypeVar",
          "21: from marshmallow import ValidationError",
          "22: from sqlalchemy import and_, or_, select",
          "23: from sqlalchemy.exc import MultipleResultsFound",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "21: from flask import g",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "27: from airflow.api_connexion import security",
          "28: from airflow.api_connexion.endpoints.request_dict import get_json_request_dict",
          "30: from airflow.api_connexion.parameters import format_datetime, format_parameters",
          "31: from airflow.api_connexion.schemas.task_instance_schema import (",
          "32:     TaskInstanceCollection,",
          "",
          "[Removed Lines]",
          "29: from airflow.api_connexion.exceptions import BadRequest, NotFound",
          "",
          "[Added Lines]",
          "30: from airflow.api_connexion.exceptions import BadRequest, NotFound, PermissionDenied",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "396:         data = task_instance_batch_form.load(body)",
          "397:     except ValidationError as err:",
          "398:         raise BadRequest(detail=str(err.messages))",
          "399:     states = _convert_ti_states(data[\"state\"])",
          "400:     base_query = select(TI).join(TI.dag_run)",
          "403:     base_query = _apply_array_filter(base_query, key=TI.run_id, values=data[\"dag_run_ids\"])",
          "404:     base_query = _apply_array_filter(base_query, key=TI.task_id, values=data[\"task_ids\"])",
          "405:     base_query = _apply_range_filter(",
          "",
          "[Removed Lines]",
          "402:     base_query = _apply_array_filter(base_query, key=TI.dag_id, values=data[\"dag_ids\"])",
          "",
          "[Added Lines]",
          "400:     dag_ids = data[\"dag_ids\"]",
          "401:     if dag_ids:",
          "402:         cannot_access_dag_ids = set()",
          "403:         for id in dag_ids:",
          "404:             if not get_airflow_app().appbuilder.sm.can_read_dag(id, g.user):",
          "405:                 cannot_access_dag_ids.add(id)",
          "406:         if cannot_access_dag_ids:",
          "407:             raise PermissionDenied(",
          "408:                 detail=f\"User not allowed to access these DAGs: {list(cannot_access_dag_ids)}\"",
          "409:             )",
          "410:     else:",
          "411:         dag_ids = get_airflow_app().appbuilder.sm.get_accessible_dag_ids(g.user)",
          "416:     base_query = _apply_array_filter(base_query, key=TI.dag_id, values=dag_ids)",
          "",
          "---------------"
        ],
        "tests/api_connexion/endpoints/test_task_instance_endpoint.py||tests/api_connexion/endpoints/test_task_instance_endpoint.py": [
          "File: tests/api_connexion/endpoints/test_task_instance_endpoint.py -> tests/api_connexion/endpoints/test_task_instance_endpoint.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "24: import pytest",
          "25: from sqlalchemy.orm import contains_eager",
          "27: from airflow.jobs.job import Job",
          "28: from airflow.jobs.triggerer_job_runner import TriggererJobRunner",
          "29: from airflow.models import DagRun, SlaMiss, TaskInstance, Trigger",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "27: from airflow.api_connexion.exceptions import EXCEPTIONS_LINK_MAP",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "82:             (permissions.ACTION_CAN_READ, permissions.RESOURCE_TASK_INSTANCE),",
          "83:         ],",
          "84:     )",
          "85:     create_user(app, username=\"test_no_permissions\", role_name=\"TestNoPermissions\")  # type: ignore",
          "87:     yield app",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "86:     create_user(",
          "87:         app,  # type: ignore",
          "88:         username=\"test_read_only_one_dag\",",
          "89:         role_name=\"TestReadOnlyOneDag\",",
          "90:         permissions=[",
          "91:             (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG_RUN),",
          "92:             (permissions.ACTION_CAN_READ, permissions.RESOURCE_TASK_INSTANCE),",
          "93:         ],",
          "94:     )",
          "95:     # For some reason, \"DAG:example_python_operator\" is not synced when in the above list of perms,",
          "96:     # so do it manually here:",
          "97:     app.appbuilder.sm.bulk_sync_roles(",
          "98:         [",
          "99:             {",
          "100:                 \"role\": \"TestReadOnlyOneDag\",",
          "101:                 \"perms\": [(permissions.ACTION_CAN_READ, \"DAG:example_python_operator\")],",
          "102:             }",
          "103:         ]",
          "104:     )",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "90:     delete_user(app, username=\"test_dag_read_only\")  # type: ignore",
          "91:     delete_user(app, username=\"test_task_read_only\")  # type: ignore",
          "92:     delete_user(app, username=\"test_no_permissions\")  # type: ignore",
          "93:     delete_roles(app)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "113:     delete_user(app, username=\"test_read_only_one_dag\")  # type: ignore",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "905:         )",
          "906:         assert response.status_code == 403",
          "908:     def test_should_raise_400_for_no_json(self):",
          "909:         response = self.client.post(",
          "910:             \"/api/v1/dags/~/dagRuns/~/taskInstances/list\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "929:     def test_returns_403_forbidden_when_user_has_access_to_only_some_dags(self, session):",
          "930:         self.create_task_instances(session=session)",
          "931:         self.create_task_instances(session=session, dag_id=\"example_skip_dag\")",
          "932:         payload = {\"dag_ids\": [\"example_python_operator\", \"example_skip_dag\"]}",
          "934:         response = self.client.post(",
          "935:             \"/api/v1/dags/~/dagRuns/~/taskInstances/list\",",
          "936:             environ_overrides={\"REMOTE_USER\": \"test_read_only_one_dag\"},",
          "937:             json=payload,",
          "938:         )",
          "939:         assert response.status_code == 403",
          "940:         assert response.json == {",
          "941:             \"detail\": \"User not allowed to access these DAGs: ['example_skip_dag']\",",
          "942:             \"status\": 403,",
          "943:             \"title\": \"Forbidden\",",
          "944:             \"type\": EXCEPTIONS_LINK_MAP[403],",
          "945:         }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "580954e7457d948714bc3a2664192e2e8c428de7",
      "candidate_info": {
        "commit_hash": "580954e7457d948714bc3a2664192e2e8c428de7",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/580954e7457d948714bc3a2664192e2e8c428de7",
        "files": [
          "airflow/www/static/js/cluster-activity/index.test.tsx",
          "airflow/www/static/js/types/index.ts",
          "docs/apache-airflow/core-concepts/tasks.rst",
          "docs/apache-airflow/howto/customize-ui.rst"
        ],
        "message": "Cleanup: remove remnants of 'shutdown' task instance state (#33893)\n\nCo-authored-by: daniel.dylag <danieldylag1990@gmail.com>\n(cherry picked from commit b37eaae15716ddd777c79478d175bab35a0b004c)",
        "before_after_code_files": [
          "airflow/www/static/js/cluster-activity/index.test.tsx||airflow/www/static/js/cluster-activity/index.test.tsx",
          "airflow/www/static/js/types/index.ts||airflow/www/static/js/types/index.ts"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/www/static/js/cluster-activity/index.test.tsx||airflow/www/static/js/cluster-activity/index.test.tsx": [
          "File: airflow/www/static/js/cluster-activity/index.test.tsx -> airflow/www/static/js/cluster-activity/index.test.tsx",
          "--- Hunk 1 ---",
          "[Context before]",
          "50:     restarting: 0,",
          "51:     running: 0,",
          "52:     scheduled: 0,",
          "54:     skipped: 0,",
          "55:     success: 1634,",
          "56:     up_for_reschedule: 0,",
          "",
          "[Removed Lines]",
          "53:     shutdown: 0,",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "airflow/www/static/js/types/index.ts||airflow/www/static/js/types/index.ts": [
          "File: airflow/www/static/js/types/index.ts -> airflow/www/static/js/types/index.ts",
          "--- Hunk 1 ---",
          "[Context before]",
          "34:   | RunState",
          "35:   | \"removed\"",
          "36:   | \"scheduled\"",
          "38:   | \"restarting\"",
          "39:   | \"up_for_retry\"",
          "40:   | \"up_for_reschedule\"",
          "",
          "[Removed Lines]",
          "37:   | \"shutdown\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "fe97031a1f582b80da4dd0d498fab5f5e2c0989b",
      "candidate_info": {
        "commit_hash": "fe97031a1f582b80da4dd0d498fab5f5e2c0989b",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/fe97031a1f582b80da4dd0d498fab5f5e2c0989b",
        "files": [
          "airflow/cli/commands/standalone_command.py",
          "airflow/cli/commands/user_command.py",
          "airflow/providers/google/cloud/hooks/cloud_sql.py",
          "airflow/providers/google/cloud/hooks/mlengine.py",
          "tests/providers/elasticsearch/log/elasticmock/utilities/__init__.py",
          "tests/providers/ssh/hooks/test_ssh.py",
          "tests/system/providers/apache/kafka/example_dag_event_listener.py",
          "tests/test_utils/azure_system_helpers.py"
        ],
        "message": "Refactor: Use random.choices (#33631)\n\n(cherry picked from commit ba0bab0114a430ef0ac776980f7e29b34d48b726)",
        "before_after_code_files": [
          "airflow/cli/commands/standalone_command.py||airflow/cli/commands/standalone_command.py",
          "airflow/cli/commands/user_command.py||airflow/cli/commands/user_command.py",
          "airflow/providers/google/cloud/hooks/cloud_sql.py||airflow/providers/google/cloud/hooks/cloud_sql.py",
          "airflow/providers/google/cloud/hooks/mlengine.py||airflow/providers/google/cloud/hooks/mlengine.py",
          "tests/providers/elasticsearch/log/elasticmock/utilities/__init__.py||tests/providers/elasticsearch/log/elasticmock/utilities/__init__.py",
          "tests/providers/ssh/hooks/test_ssh.py||tests/providers/ssh/hooks/test_ssh.py",
          "tests/system/providers/apache/kafka/example_dag_event_listener.py||tests/system/providers/apache/kafka/example_dag_event_listener.py",
          "tests/test_utils/azure_system_helpers.py||tests/test_utils/azure_system_helpers.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/cli/commands/standalone_command.py||airflow/cli/commands/standalone_command.py": [
          "File: airflow/cli/commands/standalone_command.py -> airflow/cli/commands/standalone_command.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "193:             self.print_output(\"standalone\", \"Creating admin user\")",
          "194:             role = appbuilder.sm.find_role(\"Admin\")",
          "195:             assert role is not None",
          "199:             with open(password_path, \"w\") as file:",
          "200:                 file.write(password)",
          "201:             make_group_other_inaccessible(password_path)",
          "",
          "[Removed Lines]",
          "196:             password = \"\".join(",
          "197:                 random.choice(\"abcdefghkmnpqrstuvwxyzABCDEFGHKMNPQRSTUVWXYZ23456789\") for i in range(16)",
          "198:             )",
          "",
          "[Added Lines]",
          "196:             # password does not contain visually similar characters: ijlIJL1oO0",
          "197:             password = \"\".join(random.choices(\"abcdefghkmnpqrstuvwxyzABCDEFGHKMNPQRSTUVWXYZ23456789\", k=16))",
          "",
          "---------------"
        ],
        "airflow/cli/commands/user_command.py||airflow/cli/commands/user_command.py": [
          "File: airflow/cli/commands/user_command.py -> airflow/cli/commands/user_command.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "73:             valid_roles = appbuilder.sm.get_all_roles()",
          "74:             raise SystemExit(f\"{args.role} is not a valid role. Valid roles are: {valid_roles}\")",
          "75:         if args.use_random_password:",
          "77:         elif args.password:",
          "78:             password = args.password",
          "79:         else:",
          "",
          "[Removed Lines]",
          "76:             password = \"\".join(random.choice(string.printable) for _ in range(16))",
          "",
          "[Added Lines]",
          "76:             password = \"\".join(random.choices(string.printable, k=16))",
          "",
          "---------------"
        ],
        "airflow/providers/google/cloud/hooks/cloud_sql.py||airflow/providers/google/cloud/hooks/cloud_sql.py": [
          "File: airflow/providers/google/cloud/hooks/cloud_sql.py -> airflow/providers/google/cloud/hooks/cloud_sql.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "887:         random.seed()",
          "888:         while True:",
          "889:             candidate = os.path.join(",
          "891:             )",
          "892:             if not os.path.exists(candidate):",
          "893:                 return candidate",
          "",
          "[Removed Lines]",
          "890:                 gettempdir(), \"\".join(random.choice(string.ascii_lowercase + string.digits) for _ in range(8))",
          "",
          "[Added Lines]",
          "890:                 gettempdir(), \"\".join(random.choices(string.ascii_lowercase + string.digits, k=8))",
          "",
          "---------------"
        ],
        "airflow/providers/google/cloud/hooks/mlengine.py||airflow/providers/google/cloud/hooks/mlengine.py": [
          "File: airflow/providers/google/cloud/hooks/mlengine.py -> airflow/providers/google/cloud/hooks/mlengine.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "65:                 log.info(\"Operation is done: %s\", response)",
          "66:                 return response",
          "69:         except HttpError as e:",
          "70:             if e.resp.status != 429:",
          "71:                 log.info(\"Something went wrong. Not retrying: %s\", format(e))",
          "72:                 raise",
          "73:             else:",
          "76:     raise ValueError(f\"Connection could not be established after {max_n} retries.\")",
          "",
          "[Removed Lines]",
          "68:             time.sleep((2**i) + (random.randint(0, 1000) / 1000))",
          "74:                 time.sleep((2**i) + (random.randint(0, 1000) / 1000))",
          "",
          "[Added Lines]",
          "68:             time.sleep((2**i) + random.random())",
          "74:                 time.sleep((2**i) + random.random())",
          "",
          "---------------"
        ],
        "tests/providers/elasticsearch/log/elasticmock/utilities/__init__.py||tests/providers/elasticsearch/log/elasticmock/utilities/__init__.py": [
          "File: tests/providers/elasticsearch/log/elasticmock/utilities/__init__.py -> tests/providers/elasticsearch/log/elasticmock/utilities/__init__.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "55: def get_random_id(size=DEFAULT_ELASTICSEARCH_ID_SIZE):",
          "56:     \"\"\"Returns random if for elasticsearch\"\"\"",
          "60: def query_params(*es_query_params, **kwargs):",
          "",
          "[Removed Lines]",
          "57:     return \"\".join(random.choice(CHARSET_FOR_ELASTICSEARCH_ID) for _ in range(size))",
          "",
          "[Added Lines]",
          "57:     return \"\".join(random.choices(CHARSET_FOR_ELASTICSEARCH_ID, k=size))",
          "",
          "---------------"
        ],
        "tests/providers/ssh/hooks/test_ssh.py||tests/providers/ssh/hooks/test_ssh.py": [
          "File: tests/providers/ssh/hooks/test_ssh.py -> tests/providers/ssh/hooks/test_ssh.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "77: TEST_CMD_TIMEOUT_NOT_SET = \"NOT SET\"",
          "78: TEST_CMD_TIMEOUT_EXTRA = 15",
          "81: TEST_ENCRYPTED_PRIVATE_KEY = generate_key_string(pkey=TEST_PKEY, passphrase=PASSPHRASE)",
          "83: TEST_DISABLED_ALGORITHMS = {\"pubkeys\": [\"rsa-sha2-256\", \"rsa-sha2-512\"]}",
          "",
          "[Removed Lines]",
          "80: PASSPHRASE = \"\".join(random.choice(string.ascii_letters) for i in range(10))",
          "",
          "[Added Lines]",
          "80: PASSPHRASE = \"\".join(random.choices(string.ascii_letters, k=10))",
          "",
          "---------------"
        ],
        "tests/system/providers/apache/kafka/example_dag_event_listener.py||tests/system/providers/apache/kafka/example_dag_event_listener.py": [
          "File: tests/system/providers/apache/kafka/example_dag_event_listener.py -> tests/system/providers/apache/kafka/example_dag_event_listener.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "71: def _generate_uuid():",
          "76: with DAG(",
          "",
          "[Removed Lines]",
          "72:     letters = string.ascii_lowercase",
          "73:     return \"\".join(random.choice(letters) for i in range(6))",
          "",
          "[Added Lines]",
          "72:     return \"\".join(random.choices(string.ascii_lowercase, k=6))",
          "",
          "---------------"
        ],
        "tests/test_utils/azure_system_helpers.py||tests/test_utils/azure_system_helpers.py": [
          "File: tests/test_utils/azure_system_helpers.py -> tests/test_utils/azure_system_helpers.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "146:         cls.create_directory(",
          "147:             share_name=share_name, azure_fileshare_conn_id=azure_fileshare_conn_id, directory=directory",
          "148:         )",
          "150:         cls.upload_file_from_string(",
          "151:             string_data=string_data,",
          "152:             share_name=share_name,",
          "",
          "[Removed Lines]",
          "149:         string_data = \"\".join(random.choice(string.ascii_letters) for _ in range(1024))",
          "",
          "[Added Lines]",
          "149:         string_data = \"\".join(random.choices(string.ascii_letters, k=1024))",
          "",
          "---------------"
        ]
      }
    }
  ]
}