{
  "cve_id": "CVE-2023-50944",
  "cve_desc": "Apache Airflow, versions before 2.8.1, have a vulnerability that allows an authenticated user to access the source code of a DAG to which they don't have access.\u00a0This vulnerability is considered low since it requires an authenticated user to exploit it. Users are recommended to upgrade to version 2.8.1, which fixes this issue.",
  "repo": "apache/airflow",
  "patch_hash": "8d76538d6e105947272b000581c6fabec20146b1",
  "patch_info": {
    "commit_hash": "8d76538d6e105947272b000581c6fabec20146b1",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/8d76538d6e105947272b000581c6fabec20146b1",
    "files": [
      "airflow/api_connexion/endpoints/dag_source_endpoint.py",
      "airflow/models/dagcode.py",
      "tests/api_connexion/endpoints/test_dag_source_endpoint.py"
    ],
    "message": "Check DAG read permission before accessing DAG code (#36257)\n\n(cherry picked from commit 30ea37e0d247ce54c2d25b115e807fdb0074d795)",
    "before_after_code_files": [
      "airflow/api_connexion/endpoints/dag_source_endpoint.py||airflow/api_connexion/endpoints/dag_source_endpoint.py",
      "airflow/models/dagcode.py||airflow/models/dagcode.py",
      "tests/api_connexion/endpoints/test_dag_source_endpoint.py||tests/api_connexion/endpoints/test_dag_source_endpoint.py"
    ]
  },
  "patch_diff": {
    "airflow/api_connexion/endpoints/dag_source_endpoint.py||airflow/api_connexion/endpoints/dag_source_endpoint.py": [
      "File: airflow/api_connexion/endpoints/dag_source_endpoint.py -> airflow/api_connexion/endpoints/dag_source_endpoint.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "17: from __future__ import annotations",
      "19: from http import HTTPStatus",
      "21: from flask import Response, current_app, request",
      "22: from itsdangerous import BadSignature, URLSafeSerializer",
      "24: from airflow.api_connexion import security",
      "26: from airflow.api_connexion.schemas.dag_source_schema import dag_source_schema",
      "27: from airflow.auth.managers.models.resource_details import DagAccessEntity",
      "28: from airflow.models.dagcode import DagCode",
      "31: @security.requires_access_dag(\"GET\", DagAccessEntity.CODE)",
      "33:     \"\"\"Get source code using file token.\"\"\"",
      "34:     secret_key = current_app.config[\"SECRET_KEY\"]",
      "35:     auth_s = URLSafeSerializer(secret_key)",
      "36:     try:",
      "37:         path = auth_s.loads(file_token)",
      "39:     except (BadSignature, FileNotFoundError):",
      "40:         raise NotFound(\"Dag source not found\")",
      "",
      "[Removed Lines]",
      "25: from airflow.api_connexion.exceptions import NotFound",
      "32: def get_dag_source(*, file_token: str) -> Response:",
      "38:         dag_source = DagCode.code(path)",
      "",
      "[Added Lines]",
      "20: from typing import TYPE_CHECKING",
      "26: from airflow.api_connexion.exceptions import NotFound, PermissionDenied",
      "28: from airflow.api_connexion.security import get_readable_dags",
      "30: from airflow.models.dag import DagModel",
      "32: from airflow.utils.session import NEW_SESSION, provide_session",
      "34: if TYPE_CHECKING:",
      "35:     from sqlalchemy.orm import Session",
      "39: @provide_session",
      "40: def get_dag_source(*, file_token: str, session: Session = NEW_SESSION) -> Response:",
      "46:         dag_ids = session.query(DagModel.dag_id).filter(DagModel.fileloc == path).all()",
      "47:         readable_dags = get_readable_dags()",
      "48:         # Check if user has read access to all the DAGs defined in the file",
      "49:         if any(dag_id[0] not in readable_dags for dag_id in dag_ids):",
      "50:             raise PermissionDenied()",
      "51:         dag_source = DagCode.code(path, session=session)",
      "",
      "---------------"
    ],
    "airflow/models/dagcode.py||airflow/models/dagcode.py": [
      "File: airflow/models/dagcode.py -> airflow/models/dagcode.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "177:         return cls.code(fileloc)",
      "179:     @classmethod",
      "181:         \"\"\"Return source code for this DagCode object.",
      "183:         :return: source code as string",
      "184:         \"\"\"",
      "187:     @staticmethod",
      "188:     def _get_code_from_file(fileloc):",
      "",
      "[Removed Lines]",
      "180:     def code(cls, fileloc) -> str:",
      "185:         return cls._get_code_from_db(fileloc)",
      "",
      "[Added Lines]",
      "180:     @provide_session",
      "181:     def code(cls, fileloc, session: Session = NEW_SESSION) -> str:",
      "186:         return cls._get_code_from_db(fileloc, session)",
      "",
      "---------------"
    ],
    "tests/api_connexion/endpoints/test_dag_source_endpoint.py||tests/api_connexion/endpoints/test_dag_source_endpoint.py": [
      "File: tests/api_connexion/endpoints/test_dag_source_endpoint.py -> tests/api_connexion/endpoints/test_dag_source_endpoint.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "35: ROOT_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), os.pardir, os.pardir))",
      "36: EXAMPLE_DAG_FILE = os.path.join(\"airflow\", \"example_dags\", \"example_bash_operator.py\")",
      "39: @pytest.fixture(scope=\"module\")",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "37: EXAMPLE_DAG_ID = \"example_bash_operator\"",
      "38: TEST_DAG_ID = \"latest_only\"",
      "39: NOT_READABLE_DAG_ID = \"latest_only_with_trigger\"",
      "40: TEST_MULTIPLE_DAGS_ID = \"dataset_produces_1\"",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "45:         role_name=\"Test\",",
      "46:         permissions=[(permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG_CODE)],  # type: ignore",
      "47:     )",
      "48:     create_user(app, username=\"test_no_permissions\", role_name=\"TestNoPermissions\")  # type: ignore",
      "50:     yield app",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "52:     app.appbuilder.sm.sync_perm_for_dag(  # type: ignore",
      "53:         TEST_DAG_ID,",
      "54:         access_control={\"Test\": [permissions.ACTION_CAN_READ]},",
      "55:     )",
      "56:     app.appbuilder.sm.sync_perm_for_dag(  # type: ignore",
      "57:         EXAMPLE_DAG_ID,",
      "58:         access_control={\"Test\": [permissions.ACTION_CAN_READ]},",
      "59:     )",
      "60:     app.appbuilder.sm.sync_perm_for_dag(  # type: ignore",
      "61:         TEST_MULTIPLE_DAGS_ID,",
      "62:         access_control={\"Test\": [permissions.ACTION_CAN_READ]},",
      "63:     )",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "80:     def test_should_respond_200_text(self, url_safe_serializer):",
      "81:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
      "82:         dagbag.sync_to_db()",
      "87:         response = self.client.get(",
      "88:             url, headers={\"Accept\": \"text/plain\"}, environ_overrides={\"REMOTE_USER\": \"test\"}",
      "89:         )",
      "",
      "[Removed Lines]",
      "83:         first_dag: DAG = next(iter(dagbag.dags.values()))",
      "84:         dag_docstring = self._get_dag_file_docstring(first_dag.fileloc)",
      "86:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(first_dag.fileloc)}\"",
      "",
      "[Added Lines]",
      "99:         test_dag: DAG = dagbag.dags[TEST_DAG_ID]",
      "100:         dag_docstring = self._get_dag_file_docstring(test_dag.fileloc)",
      "102:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(test_dag.fileloc)}\"",
      "",
      "---------------",
      "--- Hunk 4 ---",
      "[Context before]",
      "95:     def test_should_respond_200_json(self, url_safe_serializer):",
      "96:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
      "97:         dagbag.sync_to_db()",
      "102:         response = self.client.get(",
      "103:             url, headers={\"Accept\": \"application/json\"}, environ_overrides={\"REMOTE_USER\": \"test\"}",
      "104:         )",
      "",
      "[Removed Lines]",
      "98:         first_dag: DAG = next(iter(dagbag.dags.values()))",
      "99:         dag_docstring = self._get_dag_file_docstring(first_dag.fileloc)",
      "101:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(first_dag.fileloc)}\"",
      "",
      "[Added Lines]",
      "114:         test_dag: DAG = dagbag.dags[TEST_DAG_ID]",
      "115:         dag_docstring = self._get_dag_file_docstring(test_dag.fileloc)",
      "117:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(test_dag.fileloc)}\"",
      "",
      "---------------",
      "--- Hunk 5 ---",
      "[Context before]",
      "110:     def test_should_respond_406(self, url_safe_serializer):",
      "111:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
      "112:         dagbag.sync_to_db()",
      "116:         response = self.client.get(",
      "117:             url, headers={\"Accept\": \"image/webp\"}, environ_overrides={\"REMOTE_USER\": \"test\"}",
      "118:         )",
      "",
      "[Removed Lines]",
      "113:         first_dag: DAG = next(iter(dagbag.dags.values()))",
      "115:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(first_dag.fileloc)}\"",
      "",
      "[Added Lines]",
      "129:         test_dag: DAG = dagbag.dags[TEST_DAG_ID]",
      "131:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(test_dag.fileloc)}\"",
      "",
      "---------------",
      "--- Hunk 6 ---",
      "[Context before]",
      "151:             environ_overrides={\"REMOTE_USER\": \"test_no_permissions\"},",
      "152:         )",
      "153:         assert response.status_code == 403",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "171:     def test_should_respond_403_not_readable(self, url_safe_serializer):",
      "172:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
      "173:         dagbag.sync_to_db()",
      "174:         dag: DAG = dagbag.dags[NOT_READABLE_DAG_ID]",
      "176:         response = self.client.get(",
      "177:             f\"/api/v1/dagSources/{url_safe_serializer.dumps(dag.fileloc)}\",",
      "178:             headers={\"Accept\": \"text/plain\"},",
      "179:             environ_overrides={\"REMOTE_USER\": \"test\"},",
      "180:         )",
      "181:         read_dag = self.client.get(",
      "182:             f\"/api/v1/dags/{NOT_READABLE_DAG_ID}\",",
      "183:             environ_overrides={\"REMOTE_USER\": \"test\"},",
      "184:         )",
      "185:         assert response.status_code == 403",
      "186:         assert read_dag.status_code == 403",
      "188:     def test_should_respond_403_some_dags_not_readable_in_the_file(self, url_safe_serializer):",
      "189:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
      "190:         dagbag.sync_to_db()",
      "191:         dag: DAG = dagbag.dags[TEST_MULTIPLE_DAGS_ID]",
      "193:         response = self.client.get(",
      "194:             f\"/api/v1/dagSources/{url_safe_serializer.dumps(dag.fileloc)}\",",
      "195:             headers={\"Accept\": \"text/plain\"},",
      "196:             environ_overrides={\"REMOTE_USER\": \"test\"},",
      "197:         )",
      "199:         read_dag = self.client.get(",
      "200:             f\"/api/v1/dags/{TEST_MULTIPLE_DAGS_ID}\",",
      "201:             environ_overrides={\"REMOTE_USER\": \"test\"},",
      "202:         )",
      "203:         assert response.status_code == 403",
      "204:         assert read_dag.status_code == 200",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "2c4c5bc604e9ab0cc1e98f7bee7d31d566579462",
      "candidate_info": {
        "commit_hash": "2c4c5bc604e9ab0cc1e98f7bee7d31d566579462",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/2c4c5bc604e9ab0cc1e98f7bee7d31d566579462",
        "files": [
          "airflow/models/xcom.py",
          "tests/api_connexion/schemas/test_xcom_schema.py",
          "tests/models/test_xcom.py"
        ],
        "message": "Stop deserializing pickle when enable_xcom_pickling is False (#36255)\n\n* Stop deserializing pickle when enable_xcom_pickling is False\n\n* Fix unit tests\n\n(cherry picked from commit 63e97abec5d56bc62a293c93f5227f364561e51c)",
        "before_after_code_files": [
          "airflow/models/xcom.py||airflow/models/xcom.py",
          "tests/api_connexion/schemas/test_xcom_schema.py||tests/api_connexion/schemas/test_xcom_schema.py",
          "tests/models/test_xcom.py||tests/models/test_xcom.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/models/xcom.py||airflow/models/xcom.py": [
          "File: airflow/models/xcom.py -> airflow/models/xcom.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "685:             except pickle.UnpicklingError:",
          "686:                 return json.loads(result.value.decode(\"UTF-8\"), cls=XComDecoder, object_hook=object_hook)",
          "687:         else:",
          "693:     @staticmethod",
          "694:     def deserialize_value(result: XCom) -> Any:",
          "",
          "[Removed Lines]",
          "688:             try:",
          "689:                 return json.loads(result.value.decode(\"UTF-8\"), cls=XComDecoder, object_hook=object_hook)",
          "690:             except (json.JSONDecodeError, UnicodeDecodeError):",
          "691:                 return pickle.loads(result.value)",
          "",
          "[Added Lines]",
          "688:             # Since xcom_pickling is disabled, we should only try to deserialize with JSON",
          "689:             return json.loads(result.value.decode(\"UTF-8\"), cls=XComDecoder, object_hook=object_hook)",
          "",
          "---------------"
        ],
        "tests/api_connexion/schemas/test_xcom_schema.py||tests/api_connexion/schemas/test_xcom_schema.py": [
          "File: tests/api_connexion/schemas/test_xcom_schema.py -> tests/api_connexion/schemas/test_xcom_schema.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "30: from airflow.models import DagRun, XCom",
          "31: from airflow.utils.dates import parse_execution_date",
          "32: from airflow.utils.session import create_session",
          "34: pytestmark = pytest.mark.db_test",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "33: from tests.test_utils.config import conf_vars",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "188:     default_time = \"2016-04-02T21:00:00+00:00\"",
          "189:     default_time_parsed = parse_execution_date(default_time)",
          "191:     def test_serialize(self, create_xcom, session):",
          "192:         create_xcom(",
          "193:             dag_id=\"test_dag\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "192:     @conf_vars({(\"core\", \"enable_xcom_pickling\"): \"True\"})",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "208:             \"map_index\": -1,",
          "209:         }",
          "211:     def test_deserialize(self):",
          "212:         xcom_dump = {",
          "213:             \"key\": \"test_key\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "213:     @conf_vars({(\"core\", \"enable_xcom_pickling\"): \"True\"})",
          "",
          "---------------"
        ],
        "tests/models/test_xcom.py||tests/models/test_xcom.py": [
          "File: tests/models/test_xcom.py -> tests/models/test_xcom.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "140:             ret_value = XCom.get_value(key=\"xcom_test3\", ti_key=ti_key, session=session)",
          "141:         assert ret_value == {\"key\": \"value\"}",
          "144:         with conf_vars({(\"core\", \"enable_xcom_pickling\"): \"True\"}):",
          "145:             XCom.set(",
          "146:                 key=\"xcom_test3\",",
          "",
          "[Removed Lines]",
          "143:     def test_xcom_deserialize_with_pickle_to_json_switch(self, task_instance, session):",
          "",
          "[Added Lines]",
          "143:     def test_xcom_deserialize_pickle_when_xcom_pickling_is_disabled(self, task_instance, session):",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "151:                 session=session,",
          "152:             )",
          "153:         with conf_vars({(\"core\", \"enable_xcom_pickling\"): \"False\"}):",
          "163:     @conf_vars({(\"core\", \"xcom_enable_pickling\"): \"False\"})",
          "164:     def test_xcom_disable_pickle_type_fail_on_non_json(self, task_instance, session):",
          "",
          "[Removed Lines]",
          "154:             ret_value = XCom.get_one(",
          "155:                 key=\"xcom_test3\",",
          "156:                 dag_id=task_instance.dag_id,",
          "157:                 task_id=task_instance.task_id,",
          "158:                 run_id=task_instance.run_id,",
          "159:                 session=session,",
          "160:             )",
          "161:         assert ret_value == {\"key\": \"value\"}",
          "",
          "[Added Lines]",
          "154:             with pytest.raises(UnicodeDecodeError):",
          "155:                 XCom.get_one(",
          "156:                     key=\"xcom_test3\",",
          "157:                     dag_id=task_instance.dag_id,",
          "158:                     task_id=task_instance.task_id,",
          "159:                     run_id=task_instance.run_id,",
          "160:                     session=session,",
          "161:                 )",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "7b5395bb64db9d131bf8e809adc332eabd9763c3",
      "candidate_info": {
        "commit_hash": "7b5395bb64db9d131bf8e809adc332eabd9763c3",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/7b5395bb64db9d131bf8e809adc332eabd9763c3",
        "files": [
          "airflow/www/auth.py",
          "tests/www/test_auth.py"
        ],
        "message": "Redirect to index when user does not have permission to access a page (#36623)\n\n(cherry picked from commit 535c8be599f5e1a9455b6e6ab1840aa446ce3b1e)",
        "before_after_code_files": [
          "airflow/www/auth.py||airflow/www/auth.py",
          "tests/www/test_auth.py||tests/www/test_auth.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/www/auth.py||airflow/www/auth.py": [
          "File: airflow/www/auth.py -> airflow/www/auth.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "22: from functools import wraps",
          "23: from typing import TYPE_CHECKING, Callable, Sequence, TypeVar, cast",
          "26: from flask_appbuilder._compat import as_unicode",
          "27: from flask_appbuilder.const import (",
          "28:     FLAMSG_ERR_SEC_ACCESS_DENIED,",
          "",
          "[Removed Lines]",
          "25: from flask import flash, redirect, render_template, request",
          "",
          "[Added Lines]",
          "25: from flask import flash, redirect, render_template, request, url_for",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "176:             ),",
          "177:             403,",
          "178:         )",
          "179:     else:",
          "180:         access_denied = get_access_denied_message()",
          "181:         flash(access_denied, \"danger\")",
          "185: def has_access_cluster_activity(method: ResourceMethod) -> Callable[[T], T]:",
          "",
          "[Removed Lines]",
          "182:     return redirect(get_auth_manager().get_url_login(next=request.url))",
          "",
          "[Added Lines]",
          "179:     elif not get_auth_manager().is_logged_in():",
          "180:         return redirect(get_auth_manager().get_url_login(next=request.url))",
          "184:     return redirect(url_for(\"Airflow.index\"))",
          "",
          "---------------"
        ],
        "tests/www/test_auth.py||tests/www/test_auth.py": [
          "File: tests/www/test_auth.py -> tests/www/test_auth.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "206:             result = auth.has_access_dag_entities(\"GET\", dag_access_entity)(self.method_test)(None, items)",
          "208:         mock_call.assert_not_called()",
          "212: @pytest.mark.db_test",
          "",
          "[Removed Lines]",
          "209:         assert result.status_code == 302",
          "",
          "[Added Lines]",
          "209:         assert result.headers[\"Location\"] == \"/home\"",
          "211:     @pytest.mark.db_test",
          "212:     @patch(\"airflow.www.auth.get_auth_manager\")",
          "213:     def test_has_access_dag_entities_when_logged_out(self, mock_get_auth_manager, app, dag_access_entity):",
          "214:         auth_manager = Mock()",
          "215:         auth_manager.batch_is_authorized_dag.return_value = False",
          "216:         auth_manager.is_logged_in.return_value = False",
          "217:         auth_manager.get_url_login.return_value = \"login_url\"",
          "218:         mock_get_auth_manager.return_value = auth_manager",
          "219:         items = [Mock(dag_id=\"dag_1\"), Mock(dag_id=\"dag_2\")]",
          "221:         with app.test_request_context():",
          "222:             result = auth.has_access_dag_entities(\"GET\", dag_access_entity)(self.method_test)(None, items)",
          "224:         mock_call.assert_not_called()",
          "225:         assert result.headers[\"Location\"] == \"login_url\"",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "ce43d4abeaf8b08dfe7c9022cea46a71efaa6a78",
      "candidate_info": {
        "commit_hash": "ce43d4abeaf8b08dfe7c9022cea46a71efaa6a78",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/ce43d4abeaf8b08dfe7c9022cea46a71efaa6a78",
        "files": [
          "dev/breeze/src/airflow_breeze/utils/path_utils.py",
          "dev/breeze/tests/test_find_airflow_directory.py"
        ],
        "message": "Change detection mechanism for Breeze self-upgrade (#36635)\n\nBreeze auto-detects if it should upgrade itself - based on\nfinding Airflow directory it is in and calculating the hash of\nthe pyproject.toml it uses. Finding the airflow sources to\nact on was using setup.cfg from Airflow and checking the package\nname inside, but since we are about to remove setup.cfg, and\nmove all project configuration to pyproject.toml (see #36537), this\nmechanism will stop working.\n\nThis PR changes it by just checking if `airflow` subdir is present,\nand contains `__init__.py` with \"airflow\" inside. That should be\n\"good enough\" and fast, and also it should be backwards compatible\nin case new Breeze is used in older airflow sources.\n\n(cherry picked from commit c5de0db05e6ec5aff135d03de63f4683b3141a95)",
        "before_after_code_files": [
          "dev/breeze/src/airflow_breeze/utils/path_utils.py||dev/breeze/src/airflow_breeze/utils/path_utils.py",
          "dev/breeze/tests/test_find_airflow_directory.py||dev/breeze/tests/test_find_airflow_directory.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/breeze/src/airflow_breeze/utils/path_utils.py||dev/breeze/src/airflow_breeze/utils/path_utils.py": [
          "File: dev/breeze/src/airflow_breeze/utils/path_utils.py -> dev/breeze/src/airflow_breeze/utils/path_utils.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "34: from airflow_breeze.utils.reinstall import reinstall_breeze, warn_dependencies_changed, warn_non_editable",
          "35: from airflow_breeze.utils.shared_options import get_verbose, set_forced_answer",
          "40: def search_upwards_for_airflow_sources_root(start_from: Path) -> Path | None:",
          "41:     root = Path(start_from.root)",
          "42:     d = start_from",
          "43:     while d != root:",
          "47:         d = d.parent",
          "48:     return None",
          "",
          "[Removed Lines]",
          "37: AIRFLOW_CFG_FILE = \"setup.cfg\"",
          "44:         attempt = d / AIRFLOW_CFG_FILE",
          "45:         if attempt.exists() and \"name = apache-airflow\\n\" in attempt.read_text():",
          "46:             return attempt.parent",
          "",
          "[Added Lines]",
          "37: PYPROJECT_TOML_FILE = \"pyproject.toml\"",
          "44:         airflow_candidate = d / \"airflow\"",
          "45:         airflow_candidate_init_py = airflow_candidate / \"__init__.py\"",
          "46:         if (",
          "47:             airflow_candidate.is_dir()",
          "48:             and airflow_candidate_init_py.is_file()",
          "49:             and \"airflow\" in airflow_candidate_init_py.read_text().lower()",
          "50:         ):",
          "51:             return airflow_candidate.parent",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "97:     return \"NOT FOUND\"",
          "101:     try:",
          "102:         the_hash = hashlib.new(\"blake2b\")",
          "104:         return the_hash.hexdigest()",
          "105:     except FileNotFoundError as e:",
          "106:         return f\"Missing file {e.filename}\"",
          "",
          "[Removed Lines]",
          "100: def get_sources_setup_metadata_hash(sources: Path) -> str:",
          "103:         the_hash.update((sources / \"dev\" / \"breeze\" / \"pyproject.toml\").read_bytes())",
          "",
          "[Added Lines]",
          "105: def get_pyproject_toml_hash(sources: Path) -> str:",
          "108:         the_hash.update((sources / \"dev\" / \"breeze\" / PYPROJECT_TOML_FILE).read_bytes())",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "109: def get_installation_sources_config_metadata_hash() -> str:",
          "110:     \"\"\"",
          "113:     This is used in order to determine if we need to upgrade Breeze, because some",
          "114:     setup files changed. Blake2b algorithm will not be flagged by security checkers",
          "",
          "[Removed Lines]",
          "111:     Retrieves hash of setup.py and setup.cfg files from the source of installation of Breeze.",
          "",
          "[Added Lines]",
          "116:     Retrieves hash of pyproject.toml from the source of installation of Breeze.",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "118:     installation_sources = get_installation_airflow_sources()",
          "119:     if installation_sources is None:",
          "120:         return \"NOT FOUND\"",
          "124: def get_used_sources_setup_metadata_hash() -> str:",
          "125:     \"\"\"",
          "126:     Retrieves hash of setup files from the currently used sources.",
          "127:     \"\"\"",
          "131: def set_forced_answer_for_upgrade_check():",
          "",
          "[Removed Lines]",
          "121:     return get_sources_setup_metadata_hash(installation_sources)",
          "128:     return get_sources_setup_metadata_hash(get_used_airflow_sources())",
          "",
          "[Added Lines]",
          "126:     return get_pyproject_toml_hash(installation_sources)",
          "133:     return get_pyproject_toml_hash(get_used_airflow_sources())",
          "",
          "---------------"
        ],
        "dev/breeze/tests/test_find_airflow_directory.py||dev/breeze/tests/test_find_airflow_directory.py": [
          "File: dev/breeze/tests/test_find_airflow_directory.py -> dev/breeze/tests/test_find_airflow_directory.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "42:     assert output == \"\"",
          "46: @mock.patch(\"airflow_breeze.utils.path_utils.Path.cwd\")",
          "47: def test_find_airflow_root_from_installation_dir(mock_cwd, capsys):",
          "48:     mock_cwd.return_value = ROOT_PATH",
          "",
          "[Removed Lines]",
          "45: @mock.patch(\"airflow_breeze.utils.path_utils.AIRFLOW_CFG_FILE\", \"bad_name.cfg\")",
          "",
          "[Added Lines]",
          "45: @mock.patch(\"airflow_breeze.utils.path_utils.PYPROJECT_TOML_FILE\", \"bad_name.toml\")",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "0d12706dd9e8559942c303df8bb86d107ae9b039",
      "candidate_info": {
        "commit_hash": "0d12706dd9e8559942c303df8bb86d107ae9b039",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/0d12706dd9e8559942c303df8bb86d107ae9b039",
        "files": [
          "dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "dev/breeze/src/airflow_breeze/utils/docs_publisher.py",
          "dev/breeze/src/airflow_breeze/utils/publish_docs_builder.py",
          "docs/exts/docs_build/docs_builder.py"
        ],
        "message": "Remove rendundant and unused code in docs building/publishing (#36346)\n\nCurrently docs building happens insid of the container image and code\ndoing that sits in `docs` folder, while publishing has already been\nmoved to `breeze` code (and is executed in the Breeze venv, not in the\ncontainer). Both building and publishing code were present in both\n(copy&pasted) and the parts of it not relevant to the `other` function\nhas not been used.\n\nWhile eventually we will move docs building also to `breeze` the first\nstep of that is to remove the redundancy and clean-up unused code, so\nthat we can make the transition cleaner.\n\n(cherry picked from commit bf90992dd48bce7de9f2a687860479e95575cd24)",
        "before_after_code_files": [
          "dev/breeze/src/airflow_breeze/commands/release_management_commands.py||dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "dev/breeze/src/airflow_breeze/utils/docs_publisher.py||dev/breeze/src/airflow_breeze/utils/docs_publisher.py",
          "dev/breeze/src/airflow_breeze/utils/publish_docs_builder.py||dev/breeze/src/airflow_breeze/utils/publish_docs_builder.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/breeze/src/airflow_breeze/commands/release_management_commands.py||dev/breeze/src/airflow_breeze/commands/release_management_commands.py": [
          "File: dev/breeze/src/airflow_breeze/commands/release_management_commands.py -> dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "109:     fix_ownership_using_docker,",
          "110:     perform_environment_checks,",
          "111: )",
          "112: from airflow_breeze.utils.github import download_constraints_file, get_active_airflow_versions",
          "113: from airflow_breeze.utils.packages import (",
          "114:     PackageSuspendedException,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "112: from airflow_breeze.utils.docs_publisher import DocsPublisher",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "139:     generate_providers_metadata_for_package,",
          "140:     get_related_providers,",
          "141: )",
          "143: from airflow_breeze.utils.python_versions import get_python_version_list",
          "144: from airflow_breeze.utils.run_utils import (",
          "145:     clean_www_assets,",
          "",
          "[Removed Lines]",
          "142: from airflow_breeze.utils.publish_docs_builder import PublishDocsBuilder",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "1083:     verbose: bool,",
          "1084:     output: Output | None,",
          "1085: ) -> tuple[int, str]:",
          "1087:     builder.publish(override_versioned=override_versioned, airflow_site_dir=airflow_site_directory)",
          "1088:     return (",
          "1089:         0,",
          "",
          "[Removed Lines]",
          "1086:     builder = PublishDocsBuilder(package_name=package_name, output=output, verbose=verbose)",
          "",
          "[Added Lines]",
          "1086:     builder = DocsPublisher(package_name=package_name, output=output, verbose=verbose)",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/utils/docs_publisher.py||dev/breeze/src/airflow_breeze/utils/docs_publisher.py": [
          "File: dev/breeze/src/airflow_breeze/utils/docs_publisher.py -> dev/breeze/src/airflow_breeze/utils/docs_publisher.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: # Licensed to the Apache Software Foundation (ASF) under one",
          "2: # or more contributor license agreements.  See the NOTICE file",
          "3: # distributed with this work for additional information",
          "4: # regarding copyright ownership.  The ASF licenses this file",
          "5: # to you under the Apache License, Version 2.0 (the",
          "6: # \"License\"); you may not use this file except in compliance",
          "7: # with the License.  You may obtain a copy of the License at",
          "8: #",
          "9: #   http://www.apache.org/licenses/LICENSE-2.0",
          "10: #",
          "11: # Unless required by applicable law or agreed to in writing,",
          "12: # software distributed under the License is distributed on an",
          "13: # \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
          "14: # KIND, either express or implied.  See the License for the",
          "15: # specific language governing permissions and limitations",
          "16: # under the License.",
          "17: from __future__ import annotations",
          "19: import os",
          "20: import shutil",
          "21: from pathlib import Path",
          "23: from airflow_breeze.global_constants import get_airflow_version",
          "24: from airflow_breeze.utils.console import Output, get_console",
          "25: from airflow_breeze.utils.helm_chart_utils import chart_version",
          "26: from airflow_breeze.utils.packages import get_provider_packages_metadata, get_short_package_name",
          "27: from airflow_breeze.utils.publish_docs_helpers import pretty_format_path",
          "29: PROCESS_TIMEOUT = 15 * 60",
          "31: ROOT_PROJECT_DIR = Path(__file__).parents[5].resolve()",
          "32: DOCS_DIR = os.path.join(ROOT_PROJECT_DIR, \"docs\")",
          "35: class DocsPublisher:",
          "36:     \"\"\"Documentation builder for Airflow Docs Publishing.\"\"\"",
          "38:     def __init__(self, package_name: str, output: Output | None, verbose: bool):",
          "39:         self.package_name = package_name",
          "40:         self.output = output",
          "41:         self.verbose = verbose",
          "43:     @property",
          "44:     def is_versioned(self):",
          "45:         \"\"\"Is current documentation package versioned?\"\"\"",
          "46:         # Disable versioning. This documentation does not apply to any released product and we can update",
          "47:         # it as needed, i.e. with each new package of providers.",
          "48:         return self.package_name not in (\"apache-airflow-providers\", \"docker-stack\")",
          "50:     @property",
          "51:     def _build_dir(self) -> str:",
          "52:         if self.is_versioned:",
          "53:             version = \"stable\"",
          "54:             return f\"{DOCS_DIR}/_build/docs/{self.package_name}/{version}\"",
          "55:         else:",
          "56:             return f\"{DOCS_DIR}/_build/docs/{self.package_name}\"",
          "58:     @property",
          "59:     def _current_version(self):",
          "60:         if not self.is_versioned:",
          "61:             raise Exception(\"This documentation package is not versioned\")",
          "62:         if self.package_name == \"apache-airflow\":",
          "63:             return get_airflow_version()",
          "64:         if self.package_name.startswith(\"apache-airflow-providers-\"):",
          "65:             provider = get_provider_packages_metadata().get(get_short_package_name(self.package_name))",
          "66:             return provider[\"versions\"][0]",
          "67:         if self.package_name == \"helm-chart\":",
          "68:             return chart_version()",
          "69:         return Exception(f\"Unsupported package: {self.package_name}\")",
          "71:     @property",
          "72:     def _publish_dir(self) -> str:",
          "73:         if self.is_versioned:",
          "74:             return f\"docs-archive/{self.package_name}/{self._current_version}\"",
          "75:         else:",
          "76:             return f\"docs-archive/{self.package_name}\"",
          "78:     def publish(self, override_versioned: bool, airflow_site_dir: str):",
          "79:         \"\"\"Copy documentation packages files to airflow-site repository.\"\"\"",
          "80:         get_console(output=self.output).print(f\"Publishing docs for {self.package_name}\")",
          "81:         output_dir = os.path.join(airflow_site_dir, self._publish_dir)",
          "82:         pretty_source = pretty_format_path(self._build_dir, os.getcwd())",
          "83:         pretty_target = pretty_format_path(output_dir, airflow_site_dir)",
          "84:         get_console(output=self.output).print(f\"Copy directory: {pretty_source} => {pretty_target}\")",
          "85:         if os.path.exists(output_dir):",
          "86:             if self.is_versioned:",
          "87:                 if override_versioned:",
          "88:                     get_console(output=self.output).print(f\"Overriding previously existing {output_dir}! \")",
          "89:                 else:",
          "90:                     get_console(output=self.output).print(",
          "91:                         f\"Skipping previously existing {output_dir}! \"",
          "92:                         f\"Delete it manually if you want to regenerate it!\"",
          "93:                     )",
          "94:                     get_console(output=self.output).print()",
          "95:                     return",
          "96:             shutil.rmtree(output_dir)",
          "97:         shutil.copytree(self._build_dir, output_dir)",
          "98:         if self.is_versioned:",
          "99:             with open(os.path.join(output_dir, \"..\", \"stable.txt\"), \"w\") as stable_file:",
          "100:                 stable_file.write(self._current_version)",
          "101:         get_console(output=self.output).print()",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/utils/publish_docs_builder.py||dev/breeze/src/airflow_breeze/utils/publish_docs_builder.py": [
          "File: dev/breeze/src/airflow_breeze/utils/publish_docs_builder.py -> dev/breeze/src/airflow_breeze/utils/publish_docs_builder.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "23d06bed8eaf99e4c44c4ba46c05238de1ed8c91",
      "candidate_info": {
        "commit_hash": "23d06bed8eaf99e4c44c4ba46c05238de1ed8c91",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/23d06bed8eaf99e4c44c4ba46c05238de1ed8c91",
        "files": [
          ".github/ISSUE_TEMPLATE/airflow_providers_bug_report.yml",
          "PROVIDERS.rst",
          "airflow/provider.yaml.schema.json",
          "airflow/providers/MANAGING_PROVIDERS_LIFECYCLE.rst",
          "airflow/providers/airbyte/provider.yaml",
          "airflow/providers/alibaba/provider.yaml",
          "airflow/providers/amazon/provider.yaml",
          "airflow/providers/apache/beam/provider.yaml",
          "airflow/providers/apache/cassandra/provider.yaml",
          "airflow/providers/apache/drill/provider.yaml",
          "airflow/providers/apache/druid/provider.yaml",
          "airflow/providers/apache/flink/provider.yaml",
          "airflow/providers/apache/hdfs/provider.yaml",
          "airflow/providers/apache/hive/provider.yaml",
          "airflow/providers/apache/impala/provider.yaml",
          "airflow/providers/apache/kafka/provider.yaml",
          "airflow/providers/apache/kylin/provider.yaml",
          "airflow/providers/apache/livy/provider.yaml",
          "airflow/providers/apache/pig/provider.yaml",
          "airflow/providers/apache/pinot/provider.yaml",
          "airflow/providers/apache/spark/provider.yaml",
          "airflow/providers/apache/sqoop/provider.yaml",
          "airflow/providers/apprise/provider.yaml",
          "airflow/providers/arangodb/provider.yaml",
          "airflow/providers/asana/provider.yaml",
          "airflow/providers/atlassian/jira/provider.yaml",
          "airflow/providers/celery/provider.yaml",
          "airflow/providers/cloudant/provider.yaml",
          "airflow/providers/cncf/kubernetes/provider.yaml",
          "airflow/providers/cohere/provider.yaml",
          "airflow/providers/common/io/provider.yaml",
          "airflow/providers/common/sql/provider.yaml",
          "airflow/providers/daskexecutor/provider.yaml",
          "airflow/providers/databricks/provider.yaml",
          "airflow/providers/datadog/provider.yaml",
          "airflow/providers/dbt/cloud/provider.yaml",
          "airflow/providers/dingding/provider.yaml",
          "airflow/providers/discord/provider.yaml",
          "airflow/providers/docker/provider.yaml",
          "airflow/providers/elasticsearch/provider.yaml",
          "airflow/providers/exasol/provider.yaml",
          "airflow/providers/facebook/provider.yaml",
          "airflow/providers/ftp/provider.yaml",
          "airflow/providers/github/provider.yaml",
          "airflow/providers/google/provider.yaml",
          "airflow/providers/grpc/provider.yaml",
          "airflow/providers/hashicorp/provider.yaml",
          "airflow/providers/http/provider.yaml",
          "airflow/providers/imap/provider.yaml",
          "airflow/providers/influxdb/provider.yaml",
          "airflow/providers/jdbc/provider.yaml",
          "airflow/providers/jenkins/provider.yaml",
          "airflow/providers/microsoft/azure/provider.yaml",
          "airflow/providers/microsoft/mssql/provider.yaml",
          "airflow/providers/microsoft/psrp/provider.yaml",
          "airflow/providers/microsoft/winrm/provider.yaml",
          "airflow/providers/mongo/provider.yaml",
          "airflow/providers/mysql/provider.yaml",
          "airflow/providers/neo4j/provider.yaml",
          "airflow/providers/odbc/provider.yaml",
          "airflow/providers/openai/provider.yaml",
          "airflow/providers/openfaas/provider.yaml",
          "airflow/providers/openlineage/provider.yaml",
          "airflow/providers/opensearch/provider.yaml",
          "airflow/providers/opsgenie/provider.yaml",
          "airflow/providers/oracle/provider.yaml",
          "airflow/providers/pagerduty/provider.yaml",
          "airflow/providers/papermill/provider.yaml",
          "airflow/providers/pgvector/provider.yaml",
          "airflow/providers/pinecone/provider.yaml",
          "airflow/providers/plexus/provider.yaml",
          "airflow/providers/postgres/provider.yaml",
          "airflow/providers/presto/provider.yaml",
          "airflow/providers/redis/provider.yaml",
          "airflow/providers/salesforce/provider.yaml",
          "airflow/providers/samba/provider.yaml",
          "airflow/providers/segment/provider.yaml",
          "airflow/providers/sendgrid/provider.yaml",
          "airflow/providers/sftp/provider.yaml",
          "airflow/providers/singularity/provider.yaml",
          "airflow/providers/slack/provider.yaml",
          "airflow/providers/smtp/provider.yaml",
          "airflow/providers/snowflake/provider.yaml",
          "airflow/providers/sqlite/provider.yaml",
          "airflow/providers/ssh/provider.yaml",
          "airflow/providers/tableau/provider.yaml",
          "airflow/providers/tabular/provider.yaml",
          "airflow/providers/telegram/provider.yaml",
          "airflow/providers/trino/provider.yaml",
          "airflow/providers/vertica/provider.yaml",
          "airflow/providers/weaviate/provider.yaml",
          "airflow/providers/yandex/provider.yaml",
          "airflow/providers/zendesk/provider.yaml",
          "dev/breeze/src/airflow_breeze/breeze.py",
          "dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "dev/breeze/src/airflow_breeze/utils/packages.py",
          "dev/breeze/tests/test_packages.py",
          "docs/exts/docs_build/package_filter.py",
          "docs/exts/provider_yaml_utils.py",
          "generated/provider_dependencies.json",
          "images/breeze/output_build-docs.svg",
          "images/breeze/output_build-docs.txt",
          "images/breeze/output_release-management_add-back-references.svg",
          "images/breeze/output_release-management_add-back-references.txt",
          "images/breeze/output_release-management_generate-constraints.svg",
          "images/breeze/output_release-management_publish-docs.svg",
          "images/breeze/output_release-management_publish-docs.txt",
          "images/breeze/output_sbom_generate-providers-requirements.svg",
          "images/breeze/output_sbom_generate-providers-requirements.txt",
          "scripts/ci/pre_commit/pre_commit_check_provider_docs.py",
          "scripts/ci/pre_commit/pre_commit_update_providers_dependencies.py",
          "scripts/in_container/run_provider_yaml_files_check.py",
          "setup.py",
          "tests/always/test_example_dags.py"
        ],
        "message": "Speed up autocompletion of Breeze by simplifying provider state (#36499)\n\nSome recent changes, adding removed and suspended state for breeze\ncaused significant slow-down of autocompletion retrieval - as it\nturned out, because we loaded and parsed all provider yaml files\nduring auto-completion - in order to determine list of providers\navailable for some commands.\n\nWe already planned to replace the several states (suspended,\nnot-ready, removed) with a single state field - by doing it and\naddding the field to pre-commit generated \"provider_dependencies.json\"\nwe could switch to parsing the single provider_dependencies.json\nfile and retrieve provider list from there following the state stored\nin that json file.\n\nThis also simplifies state management following the recently\nadded state diagram by following the same state lifecycle:\n\n\"not-ready\" -> \"ready\" -> \"suspended\" -> \"removed\"",
        "before_after_code_files": [
          "dev/breeze/src/airflow_breeze/breeze.py||dev/breeze/src/airflow_breeze/breeze.py",
          "dev/breeze/src/airflow_breeze/commands/release_management_commands.py||dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "dev/breeze/src/airflow_breeze/utils/packages.py||dev/breeze/src/airflow_breeze/utils/packages.py",
          "dev/breeze/tests/test_packages.py||dev/breeze/tests/test_packages.py",
          "scripts/ci/pre_commit/pre_commit_check_provider_docs.py||scripts/ci/pre_commit/pre_commit_check_provider_docs.py",
          "scripts/ci/pre_commit/pre_commit_update_providers_dependencies.py||scripts/ci/pre_commit/pre_commit_update_providers_dependencies.py",
          "scripts/in_container/run_provider_yaml_files_check.py||scripts/in_container/run_provider_yaml_files_check.py",
          "setup.py||setup.py",
          "tests/always/test_example_dags.py||tests/always/test_example_dags.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/breeze/src/airflow_breeze/breeze.py||dev/breeze/src/airflow_breeze/breeze.py": [
          "File: dev/breeze/src/airflow_breeze/breeze.py -> dev/breeze/src/airflow_breeze/breeze.py"
        ],
        "dev/breeze/src/airflow_breeze/commands/release_management_commands.py||dev/breeze/src/airflow_breeze/commands/release_management_commands.py": [
          "File: dev/breeze/src/airflow_breeze/commands/release_management_commands.py -> dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "450:         except PrepareReleaseDocsUserQuitException:",
          "451:             break",
          "452:         else:",
          "454:                 removed_packages.append(provider_id)",
          "455:             else:",
          "456:                 success_packages.append(provider_id)",
          "",
          "[Removed Lines]",
          "453:             if provider_metadata.get(\"removed\"):",
          "",
          "[Added Lines]",
          "453:             if provider_metadata[\"state\"] == \"removed\":",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "481:     if not provider_metadata:",
          "482:         get_console().print(f\"[error]The package {provider_package_id} is not a provider package. Exiting[/]\")",
          "483:         sys.exit(1)",
          "485:         get_console().print(",
          "486:             f\"[warning]The package: {provider_package_id} is scheduled for removal, but \"",
          "487:             f\"since you asked for it, it will be built [/]\\n\"",
          "488:         )",
          "490:         get_console().print(f\"[warning]The package: {provider_package_id} is suspended \" f\"skipping it [/]\\n\")",
          "491:         raise PackageSuspendedException()",
          "492:     return provider_metadata",
          "",
          "[Removed Lines]",
          "484:     if provider_metadata.get(\"removed\", False):",
          "489:     elif provider_metadata.get(\"suspended\"):",
          "",
          "[Added Lines]",
          "484:     if provider_metadata[\"state\"] == \"removed\":",
          "489:     elif provider_metadata.get(\"state\") == \"suspended\":",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/utils/packages.py||dev/breeze/src/airflow_breeze/utils/packages.py": [
          "File: dev/breeze/src/airflow_breeze/utils/packages.py -> dev/breeze/src/airflow_breeze/utils/packages.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "156:     refresh_provider_metadata_from_yaml_file(provider_yaml_path)",
          "159: def get_provider_packages_metadata() -> dict[str, dict[str, Any]]:",
          "160:     \"\"\"",
          "161:     Load all data from providers files",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "159: @lru_cache(maxsize=1)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "208: @lru_cache",
          "209: def get_suspended_provider_ids() -> list[str]:",
          "217: @lru_cache",
          "",
          "[Removed Lines]",
          "210:     return [",
          "211:         provider_id",
          "212:         for provider_id, provider_metadata in get_provider_packages_metadata().items()",
          "213:         if provider_metadata.get(\"suspended\", False)",
          "214:     ]",
          "",
          "[Added Lines]",
          "211:     return get_available_packages(include_suspended=True, include_regular=False)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "222: @lru_cache",
          "223: def get_removed_provider_ids() -> list[str]:",
          "231: @lru_cache",
          "232: def get_not_ready_provider_ids() -> list[str]:",
          "240: def get_provider_requirements(provider_id: str) -> list[str]:",
          "",
          "[Removed Lines]",
          "224:     return [",
          "225:         provider_id",
          "226:         for provider_id, provider_metadata in get_provider_packages_metadata().items()",
          "227:         if provider_metadata.get(\"removed\", False)",
          "228:     ]",
          "233:     return [",
          "234:         provider_id",
          "235:         for provider_id, provider_metadata in get_provider_packages_metadata().items()",
          "236:         if provider_metadata.get(\"not-ready\", False)",
          "237:     ]",
          "",
          "[Added Lines]",
          "221:     return get_available_packages(include_removed=True, include_regular=False)",
          "226:     return get_available_packages(include_not_ready=True, include_regular=False)",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "249:     include_suspended: bool = False,",
          "250:     include_removed: bool = False,",
          "251:     include_not_ready: bool = False,",
          "252: ) -> list[str]:",
          "253:     \"\"\"",
          "254:     Return provider ids for all packages that are available currently (not suspended).",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "241:     include_regular: bool = True,",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "256:     :rtype: object",
          "257:     :param include_suspended: whether the suspended packages should be included",
          "258:     :param include_removed: whether the removed packages should be included",
          "260:     :param include_non_provider_doc_packages: whether the non-provider doc packages should be included",
          "261:            (packages like apache-airflow, helm-chart, docker-stack)",
          "262:     :param include_all_providers: whether \"all-providers\" should be included ni the list.",
          "264:     \"\"\"",
          "272:     if include_non_provider_doc_packages:",
          "273:         available_packages.extend(REGULAR_DOC_PACKAGES)",
          "274:     if include_all_providers:",
          "275:         available_packages.append(\"all-providers\")",
          "281:     return sorted(set(available_packages))",
          "",
          "[Removed Lines]",
          "259:     :param include_not_ready: whether the not-ready ppackages should be included",
          "265:     provider_ids: list[str] = list(json.loads(PROVIDER_DEPENDENCIES_JSON_FILE_PATH.read_text()).keys())",
          "266:     available_packages = []",
          "267:     not_ready_provider_ids = get_not_ready_provider_ids()",
          "268:     if not include_not_ready:",
          "269:         provider_ids = [",
          "270:             provider_id for provider_id in provider_ids if provider_id not in not_ready_provider_ids",
          "271:         ]",
          "276:     available_packages.extend(provider_ids)",
          "277:     if include_suspended:",
          "278:         available_packages.extend(get_suspended_provider_ids())",
          "279:     if include_removed:",
          "280:         available_packages.extend(get_removed_provider_ids())",
          "",
          "[Added Lines]",
          "249:     :param include_not_ready: whether the not-ready packages should be included",
          "250:     :param include_regular: whether the regular packages should be included",
          "256:     provider_dependencies = json.loads(PROVIDER_DEPENDENCIES_JSON_FILE_PATH.read_text())",
          "258:     valid_states = set()",
          "259:     if include_not_ready:",
          "260:         valid_states.add(\"not-ready\")",
          "261:     if include_regular:",
          "262:         valid_states.add(\"ready\")",
          "263:     if include_suspended:",
          "264:         valid_states.add(\"suspended\")",
          "265:     if include_removed:",
          "266:         valid_states.add(\"removed\")",
          "267:     available_packages: list[str] = [",
          "268:         provider_id",
          "269:         for provider_id, provider_dependencies in provider_dependencies.items()",
          "270:         if provider_dependencies[\"state\"] in valid_states",
          "271:     ]",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "499:         versions=provider_info[\"versions\"],",
          "500:         excluded_python_versions=provider_info.get(\"excluded-python-versions\") or [],",
          "501:         plugins=plugins,",
          "503:     )",
          "",
          "[Removed Lines]",
          "502:         removed=provider_info.get(\"removed\", False),",
          "",
          "[Added Lines]",
          "497:         removed=provider_info[\"state\"] == \"removed\",",
          "",
          "---------------"
        ],
        "dev/breeze/tests/test_packages.py||dev/breeze/tests/test_packages.py": [
          "File: dev/breeze/tests/test_packages.py -> dev/breeze/tests/test_packages.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "110: def test_get_removed_providers():",
          "111:     # Modify it every time we schedule provider for removal or remove it",
          "115: def test_get_suspended_provider_ids():",
          "",
          "[Removed Lines]",
          "112:     assert [\"apache.sqoop\", \"daskexecutor\", \"plexus\"] == get_removed_provider_ids()",
          "",
          "[Added Lines]",
          "112:     assert [] == get_removed_provider_ids()",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "340:     assert provider_info_dict[\"name\"] == \"Amazon\"",
          "341:     assert provider_info_dict[\"package-name\"] == \"apache-airflow-providers-amazon\"",
          "342:     assert \"Amazon\" in provider_info_dict[\"description\"]",
          "344:     assert provider_info_dict[\"filesystems\"] == [\"airflow.providers.amazon.aws.fs.s3\"]",
          "345:     assert len(provider_info_dict[\"versions\"]) > 45",
          "346:     assert len(provider_info_dict[\"dependencies\"]) > 10",
          "",
          "[Removed Lines]",
          "343:     assert provider_info_dict[\"suspended\"] is False",
          "",
          "[Added Lines]",
          "343:     assert provider_info_dict[\"state\"] == \"ready\"",
          "",
          "---------------"
        ],
        "scripts/ci/pre_commit/pre_commit_check_provider_docs.py||scripts/ci/pre_commit/pre_commit_check_provider_docs.py": [
          "File: scripts/ci/pre_commit/pre_commit_check_provider_docs.py -> scripts/ci/pre_commit/pre_commit_check_provider_docs.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "94:     for provider_file in AIRFLOW_PROVIDERS_DIR.rglob(\"provider.yaml\"):",
          "95:         provider_name = str(provider_file.parent.relative_to(AIRFLOW_PROVIDERS_DIR)).replace(os.sep, \".\")",
          "96:         provider_info = yaml.safe_load(provider_file.read_text())",
          "98:             ALL_PROVIDERS[provider_name] = provider_info",
          "",
          "[Removed Lines]",
          "97:         if not provider_info[\"suspended\"]:",
          "",
          "[Added Lines]",
          "97:         if provider_info[\"state\"] != \"suspended\":",
          "",
          "---------------"
        ],
        "scripts/ci/pre_commit/pre_commit_update_providers_dependencies.py||scripts/ci/pre_commit/pre_commit_update_providers_dependencies.py": [
          "File: scripts/ci/pre_commit/pre_commit_update_providers_dependencies.py -> scripts/ci/pre_commit/pre_commit_update_providers_dependencies.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "97:                     os.sep, \".\"",
          "98:                 )",
          "99:                 provider_info = yaml.safe_load(provider_file.read_text())",
          "103:                     suspended_paths.append(provider_file.parent.relative_to(AIRFLOW_PROVIDERS_DIR).as_posix())",
          "104:             path = Path(root, filename)",
          "105:             if path.is_file() and path.name.endswith(\".py\"):",
          "106:                 ALL_PROVIDER_FILES.append(Path(root, filename))",
          "",
          "[Removed Lines]",
          "100:                 if not provider_info[\"suspended\"]:",
          "101:                     ALL_PROVIDERS[provider_name] = provider_info",
          "102:                 else:",
          "",
          "[Added Lines]",
          "100:                 if provider_info[\"state\"] == \"suspended\":",
          "102:                 ALL_PROVIDERS[provider_name] = provider_info",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "174:             ALL_DEPENDENCIES[file_provider][\"cross-providers-deps\"].append(imported_provider)",
          "177: if __name__ == \"__main__\":",
          "178:     find_all_providers_and_provider_files()",
          "179:     num_files = len(ALL_PROVIDER_FILES)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "176: STATES: dict[str, str] = {}",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "182:     for file in ALL_PROVIDER_FILES:",
          "183:         check_if_different_provider_used(file)",
          "184:     for provider, provider_yaml_content in ALL_PROVIDERS.items():",
          "187:     if warnings:",
          "188:         console.print(\"[yellow]Warnings!\\n\")",
          "189:         for warning in warnings:",
          "",
          "[Removed Lines]",
          "185:         if not provider_yaml_content.get(\"suspended\"):",
          "186:             ALL_DEPENDENCIES[provider][\"deps\"].extend(provider_yaml_content[\"dependencies\"])",
          "",
          "[Added Lines]",
          "187:         ALL_DEPENDENCIES[provider][\"deps\"].extend(provider_yaml_content[\"dependencies\"])",
          "188:         STATES[provider] = provider_yaml_content[\"state\"]",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "194:         for error in errors:",
          "195:             console.print(f\"[red] {error}\")",
          "196:         console.print(f\"[bright_blue]Total: {len(errors)} errors.\")",
          "198:     for key in sorted(ALL_DEPENDENCIES.keys()):",
          "199:         unique_sorted_dependencies[key][\"deps\"] = sorted(ALL_DEPENDENCIES[key][\"deps\"])",
          "200:         unique_sorted_dependencies[key][\"cross-providers-deps\"] = sorted(",
          "",
          "[Removed Lines]",
          "197:     unique_sorted_dependencies: dict[str, dict[str, list[str]]] = defaultdict(dict)",
          "",
          "[Added Lines]",
          "199:     unique_sorted_dependencies: dict[str, dict[str, list[str] | str]] = defaultdict(dict)",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "202:         )",
          "203:         excluded_versions = ALL_PROVIDERS[key].get(\"excluded-python-versions\")",
          "204:         unique_sorted_dependencies[key][\"excluded-python-versions\"] = excluded_versions or []",
          "205:     if errors:",
          "206:         console.print()",
          "207:         console.print(\"[red]Errors found during verification. Exiting!\")",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "207:         unique_sorted_dependencies[key][\"state\"] = STATES[key]",
          "",
          "---------------"
        ],
        "scripts/in_container/run_provider_yaml_files_check.py||scripts/in_container/run_provider_yaml_files_check.py": [
          "File: scripts/in_container/run_provider_yaml_files_check.py -> scripts/in_container/run_provider_yaml_files_check.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "114:             jsonschema.validate(provider, schema=schema)",
          "115:         except jsonschema.ValidationError:",
          "116:             raise Exception(f\"Unable to parse: {rel_path}.\")",
          "118:             result[rel_path] = provider",
          "119:         else:",
          "120:             suspended_providers.add(provider[\"package-name\"])",
          "",
          "[Removed Lines]",
          "117:         if not provider.get(\"suspended\"):",
          "",
          "[Added Lines]",
          "117:         if provider[\"state\"] not in [\"suspended\", \"removed\"]:",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "557:                 op[\"how-to-guide\"] for op in provider[\"transfers\"] if \"how-to-guide\" in op",
          "558:             )",
          "559:     if suspended_providers:",
          "561:         console.print(suspended_providers)",
          "563:     expected_doc_files = itertools.chain(",
          "",
          "[Removed Lines]",
          "560:         console.print(\"[yellow]Suspended providers:[/]\")",
          "",
          "[Added Lines]",
          "560:         console.print(\"[yellow]Suspended/Removed providers:[/]\")",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "680:     return num_providers, num_errors",
          "703: if __name__ == \"__main__\":",
          "704:     ProvidersManager().initialize_providers_configuration()",
          "705:     architecture = Architecture.get_current()",
          "",
          "[Removed Lines]",
          "683: @run_check(\"Checking remove flag only set for suspended providers\")",
          "684: def check_removed_flag_only_set_for_suspended_providers(yaml_files: dict[str, dict]):",
          "685:     num_errors = 0",
          "686:     num_providers = 0",
          "687:     for package_info in yaml_files.values():",
          "688:         num_providers += 1",
          "689:         package_name = package_info[\"package-name\"]",
          "690:         suspended = package_info[\"suspended\"]",
          "691:         removed = package_info.get(\"removed\", False)",
          "692:         if removed and not suspended:",
          "693:             errors.append(",
          "694:                 f\"The provider {package_name} has removed set to True in their provider.yaml file \"",
          "695:                 f\"but suspended flag is set to false. You should only set removed flag in order to \"",
          "696:                 f\"prepare last release for a provider that has been previously suspended. \"",
          "697:                 f\"[yellow]How to fix it[/]: Please suspend the provider first before removing it.\"",
          "698:             )",
          "699:             num_errors += 1",
          "700:     return num_providers, num_errors",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "726:     check_notification_classes(all_parsed_yaml_files)",
          "727:     check_unique_provider_name(all_parsed_yaml_files)",
          "728:     check_providers_have_all_documentation_files(all_parsed_yaml_files)",
          "731:     if all_files_loaded:",
          "732:         # Only check those if all provider files are loaded",
          "",
          "[Removed Lines]",
          "729:     check_removed_flag_only_set_for_suspended_providers(all_parsed_yaml_files)",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "setup.py||setup.py": [
          "File: setup.py -> setup.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "105:             dependencies = json.load(f)",
          "106:         provider_dict = {}",
          "107:         for key, value in dependencies.items():",
          "108:             if value.get(DEPS):",
          "109:                 apply_pypi_suffix_to_airflow_packages(value[DEPS])",
          "110:             if CURRENT_PYTHON_VERSION not in value[\"excluded-python-versions\"] or skip_python_version_check:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "108:             if value[\"state\"] in [\"suspended\", \"removed\"]:",
          "109:                 continue",
          "",
          "---------------"
        ],
        "tests/always/test_example_dags.py||tests/always/test_example_dags.py": [
          "File: tests/always/test_example_dags.py -> tests/always/test_example_dags.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "40:     suspended_providers = []",
          "41:     for provider_path in AIRFLOW_PROVIDERS_ROOT.rglob(\"provider.yaml\"):",
          "42:         provider_yaml = yaml.safe_load(provider_path.read_text())",
          "44:             suspended_providers.append(",
          "45:                 provider_path.parent.relative_to(AIRFLOW_SOURCES_ROOT)",
          "46:                 .as_posix()",
          "",
          "[Removed Lines]",
          "43:         if provider_yaml.get(\"suspended\"):",
          "",
          "[Added Lines]",
          "43:         if provider_yaml[\"state\"] == \"suspended\":",
          "",
          "---------------"
        ]
      }
    }
  ]
}