{
  "cve_id": "CVE-2023-35005",
  "cve_desc": "In Apache Airflow, some potentially sensitive values were being shown to the user in certain situations.\n\nThis vulnerability is mitigated by the fact configuration is not shown in the UI by default (only if `[webserver] expose_config` is set to `non-sensitive-only`), and not all uncensored values are actually sentitive.\n\n\nThis issue affects Apache Airflow: from 2.5.0 before 2.6.2. Users are recommended to update to version 2.6.2 or later.\n\n\n",
  "repo": "apache/airflow",
  "patch_hash": "5679a01919ac9d5153e858f8b1390cbc7915f148",
  "patch_info": {
    "commit_hash": "5679a01919ac9d5153e858f8b1390cbc7915f148",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/5679a01919ac9d5153e858f8b1390cbc7915f148",
    "files": [
      "airflow/config_templates/config.yml",
      "airflow/config_templates/default_airflow.cfg",
      "airflow/configuration.py",
      "airflow/www/views.py",
      "tests/core/test_configuration.py",
      "tests/www/views/test_views_configuration.py"
    ],
    "message": "Use single source of truth for sensitive config items (#31820)\n\nPreviously we had them defined both in constant and in config.yml.\n\nNow just config.yml\n\n(cherry picked from commit cab342ee010bfd048006ca458c760b37470b6ea5)",
    "before_after_code_files": [
      "airflow/config_templates/default_airflow.cfg||airflow/config_templates/default_airflow.cfg",
      "airflow/configuration.py||airflow/configuration.py",
      "airflow/www/views.py||airflow/www/views.py",
      "tests/core/test_configuration.py||tests/core/test_configuration.py",
      "tests/www/views/test_views_configuration.py||tests/www/views/test_views_configuration.py"
    ]
  },
  "patch_diff": {
    "airflow/config_templates/default_airflow.cfg||airflow/config_templates/default_airflow.cfg": [
      "File: airflow/config_templates/default_airflow.cfg -> airflow/config_templates/default_airflow.cfg",
      "--- Hunk 1 ---",
      "[Context before]",
      "995: # Example: result_backend = db+postgresql://postgres:airflow@postgres/airflow",
      "996: # result_backend =",
      "998: # Celery Flower is a sweet UI for Celery. Airflow has a shortcut to start",
      "999: # it ``airflow celery flower``. This defines the IP that Celery Flower runs on",
      "1000: flower_host = 0.0.0.0",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "998: # Optional configuration dictionary to pass to the Celery result backend SQLAlchemy engine.",
      "999: # Example: result_backend_sqlalchemy_engine_options = {{\"pool_recycle\": 1800}}",
      "1000: result_backend_sqlalchemy_engine_options =",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "1018: # Import path for celery configuration options",
      "1019: celery_config_options = airflow.config_templates.default_celery.DEFAULT_CELERY_CONFIG",
      "1020: ssl_active = False",
      "1021: ssl_key =",
      "1022: ssl_cert =",
      "1023: ssl_cacert =",
      "1025: # Celery Pool implementation.",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "1026: # Path to the client key.",
      "1029: # Path to the client certificate.",
      "1032: # Path to the CA certificate.",
      "",
      "---------------"
    ],
    "airflow/configuration.py||airflow/configuration.py": [
      "File: airflow/configuration.py -> airflow/configuration.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "37: from contextlib import contextmanager, suppress",
      "38: from json.decoder import JSONDecodeError",
      "39: from re import Pattern",
      "41: from urllib.parse import urlsplit",
      "43: from typing_extensions import overload",
      "",
      "[Removed Lines]",
      "40: from typing import IO, Any, Dict, Iterable, Tuple, Union",
      "",
      "[Added Lines]",
      "40: from typing import IO, Any, Dict, Iterable, Set, Tuple, Union",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "147:         return yaml.safe_load(config_file)",
      "165: class AirflowConfigParser(ConfigParser):",
      "166:     \"\"\"Custom Airflow Configparser supporting defaults and deprecated options.\"\"\"",
      "",
      "[Removed Lines]",
      "150: SENSITIVE_CONFIG_VALUES = {",
      "151:     (\"database\", \"sql_alchemy_conn\"),",
      "152:     (\"core\", \"fernet_key\"),",
      "153:     (\"celery\", \"broker_url\"),",
      "154:     (\"celery\", \"flower_basic_auth\"),",
      "155:     (\"celery\", \"result_backend\"),",
      "156:     (\"atlas\", \"password\"),",
      "157:     (\"smtp\", \"smtp_password\"),",
      "158:     (\"webserver\", \"secret_key\"),",
      "159:     (\"secrets\", \"backend_kwargs\"),",
      "160:     # The following options are deprecated",
      "161:     (\"core\", \"sql_alchemy_conn\"),",
      "162: }",
      "",
      "[Added Lines]",
      "[None]",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "171:     # These configs can also be fetched from Secrets backend",
      "172:     # following the \"{section}__{name}__secret\" pattern",
      "176:     # A mapping of (new section, new option) -> (old section, old option, since_version).",
      "177:     # When reading new option, the old option will be checked to see if it exists. If it does a",
      "",
      "[Removed Lines]",
      "174:     sensitive_config_values: set[tuple[str, str]] = SENSITIVE_CONFIG_VALUES",
      "",
      "[Added Lines]",
      "159:     @cached_property",
      "160:     def sensitive_config_values(self) -> Set[tuple[str, str]]:  # noqa: UP006",
      "161:         default_config = default_config_yaml()",
      "162:         flattened = {",
      "163:             (s, k): item for s, s_c in default_config.items() for k, item in s_c.get(\"options\").items()",
      "164:         }",
      "165:         sensitive = {(section, key) for (section, key), v in flattened.items() if v.get(\"sensitive\") is True}",
      "166:         depr_option = {self.deprecated_options[x][:-1] for x in sensitive if x in self.deprecated_options}",
      "167:         depr_section = {",
      "168:             (self.deprecated_sections[s][0], k) for s, k in sensitive if s in self.deprecated_sections",
      "169:         }",
      "170:         sensitive.update(depr_section, depr_option)",
      "171:         return sensitive",
      "",
      "---------------"
    ],
    "airflow/www/views.py||airflow/www/views.py": [
      "File: airflow/www/views.py -> airflow/www/views.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "3951:         # TODO remove \"if raw\" usage in Airflow 3.0. Configuration can be fetched via the REST API.",
      "3952:         if raw:",
      "3953:             if expose_config == \"non-sensitive-only\":",
      "3956:                 updater = configupdater.ConfigUpdater()",
      "3957:                 updater.read(AIRFLOW_CONFIG)",
      "3959:                     if updater.has_option(sect, key):",
      "3960:                         updater[sect][key].value = \"< hidden >\"",
      "3961:                 config = str(updater)",
      "",
      "[Removed Lines]",
      "3954:                 from airflow.configuration import SENSITIVE_CONFIG_VALUES",
      "3958:                 for sect, key in SENSITIVE_CONFIG_VALUES:",
      "",
      "[Added Lines]",
      "3956:                 for sect, key in conf.sensitive_config_values:",
      "",
      "---------------"
    ],
    "tests/core/test_configuration.py||tests/core/test_configuration.py": [
      "File: tests/core/test_configuration.py -> tests/core/test_configuration.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "36:     AirflowConfigException,",
      "37:     AirflowConfigParser,",
      "38:     conf,",
      "39:     expand_env_var,",
      "40:     get_airflow_config,",
      "41:     get_airflow_home,",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "39:     default_config_yaml,",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "1447:             w = captured.pop()",
      "1448:             assert \"your `conf.get*` call to use the new name\" in str(w.message)",
      "1449:             assert w.category == FutureWarning",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "1453: def test_sensitive_values():",
      "1454:     from airflow.settings import conf",
      "1456:     # this list was hardcoded prior to 2.6.2",
      "1457:     # included here to avoid regression in refactor",
      "1458:     # inclusion of keys ending in \"password\" or \"kwargs\" is automated from 2.6.2",
      "1459:     # items not matching this pattern must be added here manually",
      "1460:     sensitive_values = {",
      "1461:         (\"database\", \"sql_alchemy_conn\"),",
      "1462:         (\"core\", \"fernet_key\"),",
      "1463:         (\"celery\", \"broker_url\"),",
      "1464:         (\"celery\", \"flower_basic_auth\"),",
      "1465:         (\"celery\", \"result_backend\"),",
      "1466:         (\"atlas\", \"password\"),",
      "1467:         (\"smtp\", \"smtp_password\"),",
      "1468:         (\"webserver\", \"secret_key\"),",
      "1469:         (\"secrets\", \"backend_kwargs\"),",
      "1470:         (\"sentry\", \"sentry_dsn\"),",
      "1471:         (\"database\", \"sql_alchemy_engine_args\"),",
      "1472:         (\"core\", \"sql_alchemy_conn\"),",
      "1473:     }",
      "1474:     default_config = default_config_yaml()",
      "1475:     all_keys = {(s, k) for s, v in default_config.items() for k in v.get(\"options\")}",
      "1476:     suspected_sensitive = {(s, k) for (s, k) in all_keys if k.endswith((\"password\", \"kwargs\"))}",
      "1477:     exclude_list = {",
      "1478:         (\"kubernetes_executor\", \"delete_option_kwargs\"),",
      "1479:     }",
      "1480:     suspected_sensitive -= exclude_list",
      "1481:     sensitive_values.update(suspected_sensitive)",
      "1482:     assert sensitive_values == conf.sensitive_config_values",
      "",
      "---------------"
    ],
    "tests/www/views/test_views_configuration.py||tests/www/views/test_views_configuration.py": [
      "File: tests/www/views/test_views_configuration.py -> tests/www/views/test_views_configuration.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "19: import html",
      "22: from tests.test_utils.config import conf_vars",
      "23: from tests.test_utils.www import check_content_in_response, check_content_not_in_response",
      "",
      "[Removed Lines]",
      "21: from airflow.configuration import SENSITIVE_CONFIG_VALUES, conf",
      "",
      "[Added Lines]",
      "21: from airflow.configuration import conf",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "36: @conf_vars({(\"webserver\", \"expose_config\"): \"True\"})",
      "37: def test_user_can_view_configuration(admin_client):",
      "38:     resp = admin_client.get(\"configuration\", follow_redirects=True)",
      "40:         value = conf.get(section, key, fallback=\"\")",
      "41:         if not value:",
      "42:             continue",
      "",
      "[Removed Lines]",
      "39:     for section, key in SENSITIVE_CONFIG_VALUES:",
      "",
      "[Added Lines]",
      "39:     for section, key in conf.sensitive_config_values:",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "46: @conf_vars({(\"webserver\", \"expose_config\"): \"non-sensitive-only\"})",
      "47: def test_configuration_redacted(admin_client):",
      "48:     resp = admin_client.get(\"configuration\", follow_redirects=True)",
      "50:         value = conf.get(section, key, fallback=\"\")",
      "51:         if not value or value == \"airflow\":",
      "52:             continue",
      "",
      "[Removed Lines]",
      "49:     for section, key in SENSITIVE_CONFIG_VALUES:",
      "",
      "[Added Lines]",
      "49:     for section, key in conf.sensitive_config_values:",
      "",
      "---------------",
      "--- Hunk 4 ---",
      "[Context before]",
      "58: @conf_vars({(\"webserver\", \"expose_config\"): \"non-sensitive-only\"})",
      "59: def test_configuration_redacted_in_running_configuration(admin_client):",
      "60:     resp = admin_client.get(\"configuration\", follow_redirects=True)",
      "62:         value = conf.get(section, key, fallback=\"\")",
      "63:         if not value or value == \"airflow\":",
      "64:             continue",
      "",
      "[Removed Lines]",
      "61:     for section, key in SENSITIVE_CONFIG_VALUES:",
      "",
      "[Added Lines]",
      "61:     for section, key in conf.sensitive_config_values:",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "e19ff6f620a2d0d682a2b243593119ca5eb501fb",
      "candidate_info": {
        "commit_hash": "e19ff6f620a2d0d682a2b243593119ca5eb501fb",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/e19ff6f620a2d0d682a2b243593119ca5eb501fb",
        "files": [
          "airflow/migrations/versions/0125_2_6_2_add_onupdate_cascade_to_taskmap.py",
          "airflow/models/taskmap.py",
          "docs/apache-airflow/img/airflow_erd.sha256",
          "docs/apache-airflow/img/airflow_erd.svg",
          "docs/apache-airflow/migrations-ref.rst",
          "tests/models/test_taskinstance.py"
        ],
        "message": "Cascade update of taskinstance to TaskMap table (#31445)\n\n(cherry picked from commit f6bb4746efbc6a94fa17b6c77b31d9fb17305ffc)",
        "before_after_code_files": [
          "airflow/migrations/versions/0125_2_6_2_add_onupdate_cascade_to_taskmap.py||airflow/migrations/versions/0125_2_6_2_add_onupdate_cascade_to_taskmap.py",
          "airflow/models/taskmap.py||airflow/models/taskmap.py",
          "tests/models/test_taskinstance.py||tests/models/test_taskinstance.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/31796"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/migrations/versions/0125_2_6_2_add_onupdate_cascade_to_taskmap.py||airflow/migrations/versions/0125_2_6_2_add_onupdate_cascade_to_taskmap.py": [
          "File: airflow/migrations/versions/0125_2_6_2_add_onupdate_cascade_to_taskmap.py -> airflow/migrations/versions/0125_2_6_2_add_onupdate_cascade_to_taskmap.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: #",
          "2: # Licensed to the Apache Software Foundation (ASF) under one",
          "3: # or more contributor license agreements.  See the NOTICE file",
          "4: # distributed with this work for additional information",
          "5: # regarding copyright ownership.  The ASF licenses this file",
          "6: # to you under the Apache License, Version 2.0 (the",
          "7: # \"License\"); you may not use this file except in compliance",
          "8: # with the License.  You may obtain a copy of the License at",
          "9: #",
          "10: #   http://www.apache.org/licenses/LICENSE-2.0",
          "11: #",
          "12: # Unless required by applicable law or agreed to in writing,",
          "13: # software distributed under the License is distributed on an",
          "14: # \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
          "15: # KIND, either express or implied.  See the License for the",
          "16: # specific language governing permissions and limitations",
          "17: # under the License.",
          "19: \"\"\"Add ``onupdate`` cascade to ``task_map`` table",
          "21: Revision ID: c804e5c76e3e",
          "22: Revises: 98ae134e6fff",
          "23: Create Date: 2023-05-19 23:30:57.368617",
          "25: \"\"\"",
          "26: from __future__ import annotations",
          "28: from alembic import op",
          "30: # revision identifiers, used by Alembic.",
          "31: revision = \"c804e5c76e3e\"",
          "32: down_revision = \"98ae134e6fff\"",
          "33: branch_labels = None",
          "34: depends_on = None",
          "35: airflow_version = \"2.6.2\"",
          "38: def upgrade():",
          "39:     \"\"\"Apply Add onupdate cascade to taskmap\"\"\"",
          "40:     with op.batch_alter_table(\"task_map\") as batch_op:",
          "41:         batch_op.drop_constraint(\"task_map_task_instance_fkey\", type_=\"foreignkey\")",
          "42:         batch_op.create_foreign_key(",
          "43:             \"task_map_task_instance_fkey\",",
          "44:             \"task_instance\",",
          "45:             [\"dag_id\", \"task_id\", \"run_id\", \"map_index\"],",
          "46:             [\"dag_id\", \"task_id\", \"run_id\", \"map_index\"],",
          "47:             ondelete=\"CASCADE\",",
          "48:             onupdate=\"CASCADE\",",
          "49:         )",
          "52: def downgrade():",
          "53:     \"\"\"Unapply Add onupdate cascade to taskmap\"\"\"",
          "54:     with op.batch_alter_table(\"task_map\") as batch_op:",
          "55:         batch_op.drop_constraint(\"task_map_task_instance_fkey\", type_=\"foreignkey\")",
          "56:         batch_op.create_foreign_key(",
          "57:             \"task_map_task_instance_fkey\",",
          "58:             \"task_instance\",",
          "59:             [\"dag_id\", \"task_id\", \"run_id\", \"map_index\"],",
          "60:             [\"dag_id\", \"task_id\", \"run_id\", \"map_index\"],",
          "61:             ondelete=\"CASCADE\",",
          "62:         )",
          "",
          "---------------"
        ],
        "airflow/models/taskmap.py||airflow/models/taskmap.py": [
          "File: airflow/models/taskmap.py -> airflow/models/taskmap.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "72:             ],",
          "73:             name=\"task_map_task_instance_fkey\",",
          "74:             ondelete=\"CASCADE\",",
          "75:         ),",
          "76:     )",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "75:             onupdate=\"CASCADE\",",
          "",
          "---------------"
        ],
        "tests/models/test_taskinstance.py||tests/models/test_taskinstance.py": [
          "File: tests/models/test_taskinstance.py -> tests/models/test_taskinstance.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "69: from airflow.operators.python import PythonOperator",
          "70: from airflow.sensors.base import BaseSensorOperator",
          "71: from airflow.sensors.python import PythonSensor",
          "73: from airflow.settings import TIMEZONE",
          "74: from airflow.stats import Stats",
          "75: from airflow.ti_deps.dep_context import DepContext",
          "",
          "[Removed Lines]",
          "72: from airflow.serialization.serialized_objects import SerializedBaseOperator",
          "",
          "[Added Lines]",
          "72: from airflow.serialization.serialized_objects import SerializedBaseOperator, SerializedDAG",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "3484:         assert task_map.length == expected_length",
          "3485:         assert task_map.keys == expected_keys",
          "3488: class TestMappedTaskInstanceReceiveValue:",
          "3489:     @pytest.mark.parametrize(",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "3487:     def test_no_error_on_changing_from_non_mapped_to_mapped(self, dag_maker, session):",
          "3488:         \"\"\"If a task changes from non-mapped to mapped, don't fail on integrity error.\"\"\"",
          "3489:         with dag_maker(dag_id=\"test_no_error_on_changing_from_non_mapped_to_mapped\") as dag:",
          "3491:             @dag.task()",
          "3492:             def add_one(x):",
          "3493:                 return [x + 1]",
          "3495:             @dag.task()",
          "3496:             def add_two(x):",
          "3497:                 return x + 2",
          "3499:             task1 = add_one(2)",
          "3500:             add_two.expand(x=task1)",
          "3502:         dr = dag_maker.create_dagrun()",
          "3503:         ti = dr.get_task_instance(task_id=\"add_one\")",
          "3504:         ti.run()",
          "3505:         assert ti.state == TaskInstanceState.SUCCESS",
          "3506:         dag._remove_task(\"add_one\")",
          "3507:         with dag:",
          "3508:             task1 = add_one.expand(x=[1, 2, 3]).operator",
          "3509:         serialized_dag = SerializedDAG.from_dict(SerializedDAG.to_dict(dag))",
          "3511:         dr.dag = serialized_dag",
          "3512:         dr.verify_integrity(session=session)",
          "3513:         ti = dr.get_task_instance(task_id=\"add_one\")",
          "3514:         assert ti.state == TaskInstanceState.REMOVED",
          "3515:         dag.clear()",
          "3516:         ti.refresh_from_task(task1)",
          "3517:         # This should not raise an integrity error",
          "3518:         dr.task_instance_scheduling_decisions()",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "f529d150aaa484b7ebebeb1c1e15a2b2cc89c0c5",
      "candidate_info": {
        "commit_hash": "f529d150aaa484b7ebebeb1c1e15a2b2cc89c0c5",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/f529d150aaa484b7ebebeb1c1e15a2b2cc89c0c5",
        "files": [
          "airflow/example_dags/example_params_ui_tutorial.py",
          "airflow/www/templates/airflow/trigger.html"
        ],
        "message": "Fix dropdown default and adjust tutorial to use 42 as default for proof (#31400)\n\n(cherry picked from commit 58aab1118a95ef63ba00784760fd13730dd46501)",
        "before_after_code_files": [
          "airflow/example_dags/example_params_ui_tutorial.py||airflow/example_dags/example_params_ui_tutorial.py",
          "airflow/www/templates/airflow/trigger.html||airflow/www/templates/airflow/trigger.html"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/31796"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/example_dags/example_params_ui_tutorial.py||airflow/example_dags/example_params_ui_tutorial.py": [
          "File: airflow/example_dags/example_params_ui_tutorial.py -> airflow/example_dags/example_params_ui_tutorial.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "65:         ),",
          "66:         # If you want to have a selection list box then you can use the enum feature of JSON schema",
          "67:         \"pick_one\": Param(",
          "69:             type=\"string\",",
          "70:             title=\"Select one Value\",",
          "71:             description=\"You can use JSON schema enum's to generate drop down selection boxes.\",",
          "73:         ),",
          "74:         # Boolean as proper parameter with description",
          "75:         \"bool\": Param(",
          "",
          "[Removed Lines]",
          "68:             \"value 1\",",
          "72:             enum=[f\"value {i}\" for i in range(1, 42)],",
          "",
          "[Added Lines]",
          "68:             \"value 42\",",
          "72:             enum=[f\"value {i}\" for i in range(16, 64)],",
          "",
          "---------------"
        ],
        "airflow/www/templates/airflow/trigger.html||airflow/www/templates/airflow/trigger.html": [
          "File: airflow/www/templates/airflow/trigger.html -> airflow/www/templates/airflow/trigger.html",
          "--- Hunk 1 ---",
          "[Context before]",
          "71:       </div>",
          "72:     {% elif \"enum\" in form_details.schema and form_details.schema.enum %}",
          "73:       <select class=\"my_select2 form-control\" name=\"element_{{ form_key }}\" id=\"element_{{ form_key }}\" data-placeholder=\"Select Value\"",
          "75:         {%- if not \"null\" in form_details.schema.type %} required=\"\"{% endif %}>",
          "76:         {% for option in form_details.schema.enum -%}",
          "78:         {% endfor -%}",
          "79:       </select>",
          "80:     {% elif form_details.schema and \"array\" in form_details.schema.type %}",
          "",
          "[Removed Lines]",
          "74:         value=\"{% if form_details.value %}{{ form_details.value }}{% endif %}\" onchange=\"updateJSONconf();\"",
          "77:         <option>{{ option }}</option>",
          "",
          "[Added Lines]",
          "74:         onchange=\"updateJSONconf();\"",
          "77:         <option{% if form_details.value == option %} selected=\"true\"{% endif %}>{{ option }}</option>",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "e9721441977d0eed53199abe836f412a290c2f91",
      "candidate_info": {
        "commit_hash": "e9721441977d0eed53199abe836f412a290c2f91",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/e9721441977d0eed53199abe836f412a290c2f91",
        "files": [
          "airflow/jobs/scheduler_job_runner.py",
          "tests/jobs/test_scheduler_job.py"
        ],
        "message": "Save scheduler execution time during search for queued dag_runs (#30699)\n\n* Function returns list of dagruns and not query\n\n* Changed pytests\n\n* Changed all to _start_queued_dagruns\n\n* Added comment and fixed tests\n\n* Fixed typo\n\n(cherry picked from commit 0fd42ff015be02d1a6a6c2e1a080f8267194b3a5)",
        "before_after_code_files": [
          "airflow/jobs/scheduler_job_runner.py||airflow/jobs/scheduler_job_runner.py",
          "tests/jobs/test_scheduler_job.py||tests/jobs/test_scheduler_job.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/31796"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/jobs/scheduler_job_runner.py||airflow/jobs/scheduler_job_runner.py": [
          "File: airflow/jobs/scheduler_job_runner.py -> airflow/jobs/scheduler_job_runner.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1299:     def _start_queued_dagruns(self, session: Session) -> None:",
          "1300:         \"\"\"Find DagRuns in queued state and decide moving them to running state.\"\"\"",
          "1303:         active_runs_of_dags = Counter(",
          "1304:             DagRun.active_runs_of_dags((dr.dag_id for dr in dag_runs), only_running=True, session=session),",
          "",
          "[Removed Lines]",
          "1301:         dag_runs = self._get_next_dagruns_to_examine(DagRunState.QUEUED, session)",
          "",
          "[Added Lines]",
          "1301:         # added all() to save runtime, otherwise query is executed more than once",
          "1302:         dag_runs: Collection[DagRun] = self._get_next_dagruns_to_examine(DagRunState.QUEUED, session).all()",
          "",
          "---------------"
        ],
        "tests/jobs/test_scheduler_job.py||tests/jobs/test_scheduler_job.py": [
          "File: tests/jobs/test_scheduler_job.py -> tests/jobs/test_scheduler_job.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "5144:             with assert_queries_count(expected_query_count, margin=15):",
          "5145:                 with mock.patch.object(DagRun, \"next_dagruns_to_examine\") as mock_dagruns:",
          "5148:                     self.job_runner._run_scheduler_loop()",
          "",
          "[Removed Lines]",
          "5146:                     mock_dagruns.return_value = dagruns",
          "",
          "[Added Lines]",
          "5146:                     query = MagicMock()",
          "5147:                     query.all.return_value = dagruns",
          "5148:                     mock_dagruns.return_value = query",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "8ba85baa3a7008ece44d7c108344a5f0ecd61e97",
      "candidate_info": {
        "commit_hash": "8ba85baa3a7008ece44d7c108344a5f0ecd61e97",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/8ba85baa3a7008ece44d7c108344a5f0ecd61e97",
        "files": [
          "tests/www/views/test_views_acl.py"
        ],
        "message": "Fix failing AIP-52 test",
        "before_after_code_files": [
          "tests/www/views/test_views_acl.py||tests/www/views/test_views_acl.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/31796"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "tests/www/views/test_views_acl.py||tests/www/views/test_views_acl.py": [
          "File: tests/www/views/test_views_acl.py -> tests/www/views/test_views_acl.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "26: from airflow.models import DagModel",
          "27: from airflow.security import permissions",
          "29: from airflow.utils import timezone",
          "30: from airflow.utils.session import create_session",
          "31: from airflow.utils.state import State",
          "",
          "[Removed Lines]",
          "28: from airflow.settings import _ENABLE_AIP_52",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "258:         {\"name\": \"tutorial_taskflow_api\", \"type\": \"dag\"},",
          "259:         {\"name\": \"tutorial_taskflow_api_virtualenv\", \"type\": \"dag\"},",
          "260:     ]",
          "264:     assert resp.json == expected",
          "",
          "[Removed Lines]",
          "261:     if _ENABLE_AIP_52:",
          "262:         expected.insert(1, {\"name\": \"example_setup_teardown_taskflow\", \"type\": \"dag\"})",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "7295524620aeb15ce15b2f33091e3100e0d13d39",
      "candidate_info": {
        "commit_hash": "7295524620aeb15ce15b2f33091e3100e0d13d39",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/7295524620aeb15ce15b2f33091e3100e0d13d39",
        "files": [
          "airflow/www/views.py"
        ],
        "message": "Fix sorting of tags (#31553)\n\n* Fix sorting of tags\n\n* Change sorting in views.py\n\n* Add ordering into query\n\n(cherry picked from commit 24e52f92bd9305bf534c411f9455460060515ea7)",
        "before_after_code_files": [
          "airflow/www/views.py||airflow/www/views.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/31796"
        ],
        "olp_code_files": {
          "patch": [
            "airflow/www/views.py||airflow/www/views.py"
          ],
          "candidate": [
            "airflow/www/views.py||airflow/www/views.py"
          ]
        }
      },
      "candidate_diff": {
        "airflow/www/views.py||airflow/www/views.py": [
          "File: airflow/www/views.py -> airflow/www/views.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "806:                 dag.can_trigger = dag.can_edit and can_create_dag_run",
          "807:                 dag.can_delete = get_airflow_app().appbuilder.sm.can_delete_dag(dag.dag_id, g.user)",
          "810:             tags = [",
          "811:                 {\"name\": name, \"selected\": bool(arg_tags_filter and name in arg_tags_filter)}",
          "812:                 for name, in dagtags",
          "",
          "[Removed Lines]",
          "809:             dagtags = session.query(func.distinct(DagTag.name)).all()",
          "",
          "[Added Lines]",
          "809:             dagtags = session.query(func.distinct(DagTag.name)).order_by(DagTag.name).all()",
          "",
          "---------------"
        ]
      }
    }
  ]
}