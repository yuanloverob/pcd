{
  "cve_id": "CVE-2022-31777",
  "cve_desc": "A stored cross-site scripting (XSS) vulnerability in Apache Spark 3.2.1 and earlier, and 3.3.0, allows remote attackers to execute arbitrary JavaScript in the web browser of a user, by including a malicious payload into the logs which would be returned in logs rendered in the UI.",
  "repo": "apache/spark",
  "patch_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
  "patch_info": {
    "commit_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
    "repo": "apache/spark",
    "commit_url": "https://github.com/apache/spark/commit/ad90195de56688ce0898691eb9d04297ab0871ad",
    "files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ],
    "message": "[SPARK-39505][UI] Escape log content rendered in UI\n\n### What changes were proposed in this pull request?\n\nEscape log content rendered to the UI.\n\n### Why are the changes needed?\n\nLog content may contain reserved characters or other code in the log and be misinterpreted in the UI as HTML.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nExisting tests\n\nCloses #36902 from srowen/LogViewEscape.\n\nAuthored-by: Sean Owen <srowen@gmail.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
    "before_after_code_files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ]
  },
  "patch_diff": {
    "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js": [
      "File: core/src/main/resources/org/apache/spark/ui/static/log-view.js -> core/src/main/resources/org/apache/spark/ui/static/log-view.js",
      "--- Hunk 1 ---",
      "[Context before]",
      "85:       if (retStartByte == 0) {",
      "86:         disableMoreButton();",
      "87:       }",
      "90:       curLogLength = curLogLength + (startByte - retStartByte);",
      "91:       startByte = retStartByte;",
      "",
      "[Removed Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(cleanData);",
      "",
      "[Added Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(document.createTextNode(cleanData));",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "115:             var retLogLength = dataInfo[2];",
      "117:             var cleanData = data.substring(newlineIndex + 1);",
      "120:             curLogLength = curLogLength + (retEndByte - retStartByte);",
      "121:             endByte = retEndByte;",
      "",
      "[Removed Lines]",
      "118:             $(\"pre\", \".log-content\").append(cleanData);",
      "",
      "[Added Lines]",
      "118:             $(\"pre\", \".log-content\").append(document.createTextNode(cleanData));",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "1f6b142e6966cbbda08f1a568974734d2d4f6208",
      "candidate_info": {
        "commit_hash": "1f6b142e6966cbbda08f1a568974734d2d4f6208",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/1f6b142e6966cbbda08f1a568974734d2d4f6208",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveInlineTables.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/ResolveInlineTablesSuite.scala",
          "sql/core/src/test/resources/sql-tests/inputs/inline-table.sql",
          "sql/core/src/test/resources/sql-tests/results/inline-table.sql.out"
        ],
        "message": "[SPARK-39570][SQL] Inline table should allow expressions with alias\n\n### What changes were proposed in this pull request?\n\n`ResolveInlineTables` requires the column expressions to be foldable, however, `Alias` is not foldable. Inline-table does not use the names in the column expressions, and we should trim aliases before checking foldable. We did something similar in `ResolvePivot`.\n\n### Why are the changes needed?\n\nTo make inline-table handle more cases, and also fixed a regression caused by https://github.com/apache/spark/pull/31844 . After https://github.com/apache/spark/pull/31844 , we always add an alias for function literals like `current_timestamp`, which breaks inline table.\n\n### Does this PR introduce _any_ user-facing change?\n\nyea, some failed queries can be run after this PR.\n\n### How was this patch tested?\n\nnew tests\n\nCloses #36967 from cloud-fan/bug.\n\nAuthored-by: Wenchen Fan <wenchen@databricks.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\n(cherry picked from commit 1df992f03fd935ac215424576530ab57d1ab939b)\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveInlineTables.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveInlineTables.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/ResolveInlineTablesSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/ResolveInlineTablesSuite.scala",
          "sql/core/src/test/resources/sql-tests/inputs/inline-table.sql||sql/core/src/test/resources/sql-tests/inputs/inline-table.sql"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "739:     }",
          "740:   }",
          "743:     def apply(plan: LogicalPlan): LogicalPlan = plan.resolveOperatorsWithPruning(",
          "744:       _.containsPattern(PIVOT), ruleId) {",
          "745:       case p: Pivot if !p.childrenResolved || !p.aggregates.forall(_.resolved)",
          "",
          "[Removed Lines]",
          "742:   object ResolvePivot extends Rule[LogicalPlan] {",
          "",
          "[Added Lines]",
          "742:   object ResolvePivot extends Rule[LogicalPlan] with AliasHelper {",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "753:         aggregates.foreach(checkValidAggregateExpression)",
          "755:         val evalPivotValues = pivotValues.map { value =>",
          "760:           if (!foldable) {",
          "761:             throw QueryCompilationErrors.nonLiteralPivotValError(value)",
          "762:           }",
          "",
          "[Removed Lines]",
          "756:           val foldable = value match {",
          "757:             case Alias(v, _) => v.foldable",
          "758:             case _ => value.foldable",
          "759:           }",
          "",
          "[Added Lines]",
          "756:           val foldable = trimAliases(value).foldable",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveInlineTables.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveInlineTables.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveInlineTables.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveInlineTables.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: import scala.util.control.NonFatal",
          "22: import org.apache.spark.sql.catalyst.InternalRow",
          "23: import org.apache.spark.sql.catalyst.plans.logical.{LocalRelation, LogicalPlan}",
          "24: import org.apache.spark.sql.catalyst.rules.Rule",
          "25: import org.apache.spark.sql.catalyst.trees.AlwaysProcess",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "23: import org.apache.spark.sql.catalyst.expressions.AliasHelper",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "32:   override def apply(plan: LogicalPlan): LogicalPlan = plan.resolveOperatorsWithPruning(",
          "33:     AlwaysProcess.fn, ruleId) {",
          "34:     case table: UnresolvedInlineTable if table.expressionsResolved =>",
          "",
          "[Removed Lines]",
          "31: object ResolveInlineTables extends Rule[LogicalPlan] with CastSupport {",
          "",
          "[Added Lines]",
          "32: object ResolveInlineTables extends Rule[LogicalPlan] with CastSupport with AliasHelper {",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "65:     table.rows.foreach { row =>",
          "66:       row.foreach { e =>",
          "69:           e.failAnalysis(s\"cannot evaluate expression ${e.sql} in inline table definition\")",
          "70:         }",
          "71:       }",
          "",
          "[Removed Lines]",
          "68:         if (!e.resolved || !e.foldable) {",
          "",
          "[Added Lines]",
          "69:         if (!e.resolved || !trimAliases(e).foldable) {",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/ResolveInlineTablesSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/ResolveInlineTablesSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/ResolveInlineTablesSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/ResolveInlineTablesSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: import org.scalatest.BeforeAndAfter",
          "22: import org.apache.spark.sql.AnalysisException",
          "24: import org.apache.spark.sql.catalyst.expressions.aggregate.Count",
          "25: import org.apache.spark.sql.catalyst.plans.logical.LocalRelation",
          "26: import org.apache.spark.sql.types.{LongType, NullType, TimestampType}",
          "",
          "[Removed Lines]",
          "23: import org.apache.spark.sql.catalyst.expressions.{Cast, Literal, Rand}",
          "",
          "[Added Lines]",
          "23: import org.apache.spark.sql.catalyst.expressions.{Alias, Cast, Literal, Rand}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "38:     ResolveInlineTables.validateInputEvaluable(",
          "39:       UnresolvedInlineTable(Seq(\"c1\", \"c2\"), Seq(Seq(lit(1)))))",
          "42:     intercept[AnalysisException] {",
          "43:       ResolveInlineTables.validateInputEvaluable(",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "42:     ResolveInlineTables.validateInputEvaluable(",
          "43:       UnresolvedInlineTable(Seq(\"c1\", \"c2\"), Seq(Seq(Alias(lit(1), \"a\")()))))",
          "",
          "---------------"
        ],
        "sql/core/src/test/resources/sql-tests/inputs/inline-table.sql||sql/core/src/test/resources/sql-tests/inputs/inline-table.sql": [
          "File: sql/core/src/test/resources/sql-tests/inputs/inline-table.sql -> sql/core/src/test/resources/sql-tests/inputs/inline-table.sql",
          "--- Hunk 1 ---",
          "[Context before]",
          "23: -- foldable expressions",
          "24: select * from values (\"one\", 1 + 0), (\"two\", 1 + 3L) as data(a, b);",
          "26: -- complex types",
          "27: select * from values (\"one\", array(0, 1)), (\"two\", array(2, 3)) as data(a, b);",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "26: -- expressions with alias",
          "27: select * from values (\"one\", 1 as one) as data(a, b);",
          "29: -- literal functions",
          "30: select a from values (\"one\", current_timestamp) as data(a, b);",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "ea6d57715fd7a0ac5294b895b3ad607b3f4e983b",
      "candidate_info": {
        "commit_hash": "ea6d57715fd7a0ac5294b895b3ad607b3f4e983b",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/ea6d57715fd7a0ac5294b895b3ad607b3f4e983b",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/EliminateSortsSuite.scala"
        ],
        "message": "[SPARK-39911][SQL][3.3] Optimize global Sort to RepartitionByExpression\n\nthis is for backport https://github.com/apache/spark/pull/37330 into branch-3.3\n### What changes were proposed in this pull request?\n\nOptimize Global sort to RepartitionByExpression, for example:\n```\nSort local             Sort local\n  Sort global    =>      RepartitionByExpression\n```\n\n### Why are the changes needed?\n\nIf a global sort below a local sort, the only meaningful thing is it's distribution. So this pr optimizes that global sort to RepartitionByExpression to save a local sort.\n\n### Does this PR introduce _any_ user-facing change?\n\nwe fix a bug in https://github.com/apache/spark/pull/37250 and that pr backport into branch-3.3. However, that fix may introduce performance regression. This pr itself is only to improve performance but in order to avoid the regression, we also backport this pr. see the details https://github.com/apache/spark/pull/37330#issuecomment-1201979396\n\n### How was this patch tested?\n\nadd test\n\nCloses #37330 from ulysses-you/optimize-sort.\n\nAuthored-by: ulysses-you <ulyssesyou18gmail.com>\nSigned-off-by: Wenchen Fan <wenchendatabricks.com>\n\nCloses #37373 from ulysses-you/SPARK-39911-3.3.\n\nAuthored-by: ulysses-you <ulyssesyou18@gmail.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/EliminateSortsSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/EliminateSortsSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "1466:     plan match {",
          "1467:       case Sort(_, global, child) if canRemoveGlobalSort || !global =>",
          "1468:         recursiveRemoveSort(child, canRemoveGlobalSort)",
          "1469:       case other if canEliminateSort(other) =>",
          "1470:         other.withNewChildren(other.children.map(c => recursiveRemoveSort(c, canRemoveGlobalSort)))",
          "1471:       case other if canEliminateGlobalSort(other) =>",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1469:       case Sort(sortOrder, true, child) =>",
          "1474:         RepartitionByExpression(sortOrder, recursiveRemoveSort(child, true), None)",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/EliminateSortsSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/EliminateSortsSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/EliminateSortsSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/EliminateSortsSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "426:   test(\"SPARK-39835: Fix EliminateSorts remove global sort below the local sort\") {",
          "428:     val plan = testRelation.orderBy($\"a\".asc).sortBy($\"c\".asc).analyze",
          "432:     val plan2 = testRelation.orderBy($\"a\".asc).orderBy($\"b\".asc).sortBy($\"c\".asc).analyze",
          "434:     comparePlans(Optimize.execute(plan2), expected2)",
          "437:     val plan3 = testRelation.sortBy($\"a\".asc).orderBy($\"b\".asc).sortBy($\"c\".asc).analyze",
          "439:     comparePlans(Optimize.execute(plan3), expected3)",
          "440:   }",
          "441: }",
          "",
          "[Removed Lines]",
          "429:     comparePlans(Optimize.execute(plan), plan)",
          "433:     val expected2 = testRelation.orderBy($\"b\".asc).sortBy($\"c\".asc).analyze",
          "438:     val expected3 = testRelation.orderBy($\"b\".asc).sortBy($\"c\".asc).analyze",
          "",
          "[Added Lines]",
          "429:     val expect = RepartitionByExpression($\"a\".asc :: Nil, testRelation, None)",
          "430:       .sortBy($\"c\".asc).analyze",
          "431:     comparePlans(Optimize.execute(plan), expect)",
          "435:     val expected2 = RepartitionByExpression($\"b\".asc :: Nil, testRelation, None)",
          "436:       .sortBy($\"c\".asc).analyze",
          "441:     val expected3 = RepartitionByExpression($\"b\".asc :: Nil, testRelation, None)",
          "442:       .sortBy($\"c\".asc).analyze",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "aeafb175875c00519e03e0ea5b5f22f765dc3607",
      "candidate_info": {
        "commit_hash": "aeafb175875c00519e03e0ea5b5f22f765dc3607",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/aeafb175875c00519e03e0ea5b5f22f765dc3607",
        "files": [
          "sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileScanRDD.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/FileMetadataStructSuite.scala"
        ],
        "message": "[SPARK-39806][SQL] Accessing `_metadata` on partitioned table can crash a query\n\nThis changes alters the projection used in `FileScanRDD` to attach file metadata to a row produced by the reader. This\nprojection used to remove the partitioning columns from the produced row. The produced row had different schema than expected by the consumers, and was missing part of the data, which resulted in query failure.\n\nThis is a bug. `FileScanRDD` should produce rows matching expected schema, and containing all the requested data. Queries should not crash due to internal errors.\n\nNo.\n\nAdds a new test in `FileMetadataStructSuite.scala` that reproduces the issue.\n\nCloses #37214 from ala/metadata-partition-by.\n\nAuthored-by: Ala Luszczak <ala@databricks.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\n(cherry picked from commit 385f1c8e4037928afafbf6664e30dc268510c05e)\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>",
        "before_after_code_files": [
          "sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileScanRDD.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileScanRDD.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/FileMetadataStructSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/FileMetadataStructSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "621:     }",
          "623:     new FileScanRDD(fsRelation.sparkSession, readFile, filePartitions,",
          "625:   }",
          "",
          "[Removed Lines]",
          "624:       requiredSchema, metadataColumns)",
          "",
          "[Added Lines]",
          "624:       new StructType(requiredSchema.fields ++ fsRelation.partitionSchema.fields), metadataColumns)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "678:       FilePartition.getFilePartitions(relation.sparkSession, splitFiles, maxSplitBytes)",
          "680:     new FileScanRDD(fsRelation.sparkSession, readFile, partitions,",
          "682:   }",
          "",
          "[Removed Lines]",
          "681:       requiredSchema, metadataColumns)",
          "",
          "[Added Lines]",
          "681:       new StructType(requiredSchema.fields ++ fsRelation.partitionSchema.fields), metadataColumns)",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileScanRDD.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileScanRDD.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileScanRDD.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileScanRDD.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "68:     @transient private val sparkSession: SparkSession,",
          "69:     readFunction: (PartitionedFile) => Iterator[InternalRow],",
          "70:     @transient val filePartitions: Seq[FilePartition],",
          "72:     val metadataColumns: Seq[AttributeReference] = Seq.empty)",
          "73:   extends RDD[InternalRow](sparkSession.sparkContext, Nil) {",
          "",
          "[Removed Lines]",
          "71:     val readDataSchema: StructType,",
          "",
          "[Added Lines]",
          "71:     val readSchema: StructType,",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "127:       private lazy val projection = {",
          "128:         val joinedExpressions =",
          "130:         UnsafeProjection.create(joinedExpressions)",
          "131:       }",
          "",
          "[Removed Lines]",
          "129:           readDataSchema.fields.map(_.dataType) ++ metadataColumns.map(_.dataType)",
          "",
          "[Added Lines]",
          "129:           readSchema.fields.map(_.dataType) ++ metadataColumns.map(_.dataType)",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/FileMetadataStructSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/FileMetadataStructSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/FileMetadataStructSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/FileMetadataStructSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "21: import java.sql.Timestamp",
          "22: import java.text.SimpleDateFormat",
          "24: import org.apache.spark.sql.{AnalysisException, Column, DataFrame, QueryTest, Row}",
          "25: import org.apache.spark.sql.execution.FileSourceScanExec",
          "26: import org.apache.spark.sql.functions._",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "24: import org.apache.spark.TestUtils",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "31: class FileMetadataStructSuite extends QueryTest with SharedSparkSession {",
          "33:   val data0: Seq[Row] = Seq(Row(\"jack\", 24, Row(12345L, \"uom\")))",
          "35:   val data1: Seq[Row] = Seq(Row(\"lily\", 31, Row(54321L, \"ucb\")))",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "34:   import testImplicits._",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "564:       )",
          "565:     }",
          "566:   }",
          "567: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "571:   Seq(true, false).foreach { useVectorizedReader =>",
          "572:     val label = if (useVectorizedReader) \"reading batches\" else \"reading rows\"",
          "573:     test(s\"SPARK-39806: metadata for a partitioned table ($label)\") {",
          "574:       withSQLConf(SQLConf.PARQUET_VECTORIZED_READER_ENABLED.key -> useVectorizedReader.toString) {",
          "575:         withTempPath { dir =>",
          "577:           Seq(1 -> 1).toDF(\"a\", \"b\").write.format(\"parquet\").partitionBy(\"b\")",
          "578:             .save(dir.getAbsolutePath)",
          "581:           val file = TestUtils.recursiveList(dir)",
          "582:             .filter(_.getName.endsWith(\".parquet\")).head",
          "583:           val expectedDf = Seq(1 -> 1).toDF(\"a\", \"b\")",
          "584:             .withColumn(FileFormat.FILE_NAME, lit(file.getName))",
          "585:             .withColumn(FileFormat.FILE_SIZE, lit(file.length()))",
          "587:           checkAnswer(spark.read.parquet(dir.getAbsolutePath)",
          "588:             .select(\"*\", METADATA_FILE_NAME, METADATA_FILE_SIZE), expectedDf)",
          "589:         }",
          "590:       }",
          "591:     }",
          "592:   }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "2ee196dbb0bf9ecfd96a1928cbaf15b7c3856d3d",
      "candidate_info": {
        "commit_hash": "2ee196dbb0bf9ecfd96a1928cbaf15b7c3856d3d",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/2ee196dbb0bf9ecfd96a1928cbaf15b7c3856d3d",
        "files": [
          "mllib/src/main/scala/org/apache/spark/ml/feature/Imputer.scala",
          "mllib/src/test/scala/org/apache/spark/ml/feature/ImputerSuite.scala"
        ],
        "message": "[SPARK-40079] Add Imputer inputCols validation for empty input case\n\nSigned-off-by: Weichen Xu <weichen.xudatabricks.com>\n\n### What changes were proposed in this pull request?\nAdd Imputer inputCols validation for empty input case\n\n### Why are the changes needed?\nIf Imputer inputCols is empty, the `fit` works fine but when saving model, error will be raised:\n\n>\nAnalysisException:\nDatasource does not support writing empty or nested empty schemas.\nPlease make sure the data schema has at least one or more column(s).\n\n### Does this PR introduce _any_ user-facing change?\nNo.\n\n### How was this patch tested?\nUnit test.\n\nCloses #37518 from WeichenXu123/imputer-param-validation.\n\nAuthored-by: Weichen Xu <weichen.xu@databricks.com>\nSigned-off-by: Weichen Xu <weichen.xu@databricks.com>\n(cherry picked from commit 87094f89655b7df09cdecb47c653461ae855b0ac)\nSigned-off-by: Weichen Xu <weichen.xu@databricks.com>",
        "before_after_code_files": [
          "mllib/src/main/scala/org/apache/spark/ml/feature/Imputer.scala||mllib/src/main/scala/org/apache/spark/ml/feature/Imputer.scala",
          "mllib/src/test/scala/org/apache/spark/ml/feature/ImputerSuite.scala||mllib/src/test/scala/org/apache/spark/ml/feature/ImputerSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "mllib/src/main/scala/org/apache/spark/ml/feature/Imputer.scala||mllib/src/main/scala/org/apache/spark/ml/feature/Imputer.scala": [
          "File: mllib/src/main/scala/org/apache/spark/ml/feature/Imputer.scala -> mllib/src/main/scala/org/apache/spark/ml/feature/Imputer.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "81:   protected def validateAndTransformSchema(schema: StructType): StructType = {",
          "82:     ParamValidators.checkSingleVsMultiColumnParams(this, Seq(outputCol), Seq(outputCols))",
          "83:     val (inputColNames, outputColNames) = getInOutCols()",
          "84:     require(inputColNames.length == inputColNames.distinct.length, s\"inputCols contains\" +",
          "85:       s\" duplicates: (${inputColNames.mkString(\", \")})\")",
          "86:     require(outputColNames.length == outputColNames.distinct.length, s\"outputCols contains\" +",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "84:     require(inputColNames.length > 0, \"inputCols cannot be empty\")",
          "",
          "---------------"
        ],
        "mllib/src/test/scala/org/apache/spark/ml/feature/ImputerSuite.scala||mllib/src/test/scala/org/apache/spark/ml/feature/ImputerSuite.scala": [
          "File: mllib/src/test/scala/org/apache/spark/ml/feature/ImputerSuite.scala -> mllib/src/test/scala/org/apache/spark/ml/feature/ImputerSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "268:         }",
          "269:         assert(e.getMessage.contains(\"outputCols contains duplicates\"))",
          "270:       }",
          "271:     }",
          "272:   }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "272:       withClue(\"Imputer should fail if inputCols param is empty.\") {",
          "273:         val e: IllegalArgumentException = intercept[IllegalArgumentException] {",
          "274:           val imputer = new Imputer().setStrategy(strategy)",
          "275:             .setInputCols(Array[String]())",
          "276:             .setOutputCols(Array[String]())",
          "277:           val model = imputer.fit(df)",
          "278:         }",
          "279:         assert(e.getMessage.contains(\"inputCols cannot be empty\"))",
          "280:       }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "6e9a58fb5f51612702608ec690dc33035fe1ca21",
      "candidate_info": {
        "commit_hash": "6e9a58fb5f51612702608ec690dc33035fe1ca21",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/6e9a58fb5f51612702608ec690dc33035fe1ca21",
        "files": [
          "dev/deps/spark-deps-hadoop-2-hive-2.3",
          "dev/deps/spark-deps-hadoop-3-hive-2.3",
          "pom.xml"
        ],
        "message": "[SPARK-39947][BUILD] Upgrade Jersey to 2.36\n\n### What changes were proposed in this pull request?\nThis pr upgrade Jersey from 2.35 to 2.36.\n\n### Why are the changes needed?\nThis version adapts to Jack 2.13.3, which is also used by Spark currently\n\n- [Adopt Jackson 2.13](https://github.com/eclipse-ee4j/jersey/pull/4928)\n- [Update Jackson to 2.13.3](https://github.com/eclipse-ee4j/jersey/pull/5076)\n\nThe release notes as follows:\n\n- https://github.com/eclipse-ee4j/jersey/releases/tag/2.36\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\nPass GitHub Actions\n\nCloses #37375 from LuciferYang/jersey-236.\n\nAuthored-by: yangjie01 <yangjie01@baidu.com>\nSigned-off-by: Sean Owen <srowen@gmail.com>\n(cherry picked from commit d1c145b0b0b892fcbf1e1adda7b8ecff75c56f6d)\nSigned-off-by: Sean Owen <srowen@gmail.com>\n\n# Conflicts:\n#\tdev/deps/spark-deps-hadoop-2-hive-2.3\n#\tdev/deps/spark-deps-hadoop-3-hive-2.3\n#\tpom.xml",
        "before_after_code_files": [
          "dev/deps/spark-deps-hadoop-2-hive-2.3||dev/deps/spark-deps-hadoop-2-hive-2.3",
          "dev/deps/spark-deps-hadoop-3-hive-2.3||dev/deps/spark-deps-hadoop-3-hive-2.3"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/deps/spark-deps-hadoop-2-hive-2.3||dev/deps/spark-deps-hadoop-2-hive-2.3": [
          "File: dev/deps/spark-deps-hadoop-2-hive-2.3 -> dev/deps/spark-deps-hadoop-2-hive-2.3",
          "--- Hunk 1 ---",
          "[Context before]",
          "138: jaxb-runtime/2.3.2//jaxb-runtime-2.3.2.jar",
          "139: jcl-over-slf4j/1.7.32//jcl-over-slf4j-1.7.32.jar",
          "140: jdo-api/3.0.1//jdo-api-3.0.1.jar",
          "147: jetty-sslengine/6.1.26//jetty-sslengine-6.1.26.jar",
          "148: jetty-util/6.1.26//jetty-util-6.1.26.jar",
          "149: jetty-util/9.4.46.v20220331//jetty-util-9.4.46.v20220331.jar",
          "",
          "[Removed Lines]",
          "141: jersey-client/2.34//jersey-client-2.34.jar",
          "142: jersey-common/2.34//jersey-common-2.34.jar",
          "143: jersey-container-servlet-core/2.34//jersey-container-servlet-core-2.34.jar",
          "144: jersey-container-servlet/2.34//jersey-container-servlet-2.34.jar",
          "145: jersey-hk2/2.34//jersey-hk2-2.34.jar",
          "146: jersey-server/2.34//jersey-server-2.34.jar",
          "",
          "[Added Lines]",
          "141: jersey-client/2.36//jersey-client-2.36.jar",
          "142: jersey-common/2.36//jersey-common-2.36.jar",
          "143: jersey-container-servlet-core/2.36//jersey-container-servlet-core-2.36.jar",
          "144: jersey-container-servlet/2.36//jersey-container-servlet-2.36.jar",
          "145: jersey-hk2/2.36//jersey-hk2-2.36.jar",
          "146: jersey-server/2.36//jersey-server-2.36.jar",
          "",
          "---------------"
        ],
        "dev/deps/spark-deps-hadoop-3-hive-2.3||dev/deps/spark-deps-hadoop-3-hive-2.3": [
          "File: dev/deps/spark-deps-hadoop-3-hive-2.3 -> dev/deps/spark-deps-hadoop-3-hive-2.3",
          "--- Hunk 1 ---",
          "[Context before]",
          "126: jcl-over-slf4j/1.7.32//jcl-over-slf4j-1.7.32.jar",
          "127: jdo-api/3.0.1//jdo-api-3.0.1.jar",
          "128: jdom2/2.0.6//jdom2-2.0.6.jar",
          "135: jettison/1.1//jettison-1.1.jar",
          "136: jetty-util-ajax/9.4.46.v20220331//jetty-util-ajax-9.4.46.v20220331.jar",
          "137: jetty-util/9.4.46.v20220331//jetty-util-9.4.46.v20220331.jar",
          "",
          "[Removed Lines]",
          "129: jersey-client/2.34//jersey-client-2.34.jar",
          "130: jersey-common/2.34//jersey-common-2.34.jar",
          "131: jersey-container-servlet-core/2.34//jersey-container-servlet-core-2.34.jar",
          "132: jersey-container-servlet/2.34//jersey-container-servlet-2.34.jar",
          "133: jersey-hk2/2.34//jersey-hk2-2.34.jar",
          "134: jersey-server/2.34//jersey-server-2.34.jar",
          "",
          "[Added Lines]",
          "129: jersey-client/2.36//jersey-client-2.36.jar",
          "130: jersey-common/2.36//jersey-common-2.36.jar",
          "131: jersey-container-servlet-core/2.36//jersey-container-servlet-core-2.36.jar",
          "132: jersey-container-servlet/2.36//jersey-container-servlet-2.36.jar",
          "133: jersey-hk2/2.36//jersey-hk2-2.36.jar",
          "134: jersey-server/2.36//jersey-server-2.36.jar",
          "",
          "---------------"
        ]
      }
    }
  ]
}