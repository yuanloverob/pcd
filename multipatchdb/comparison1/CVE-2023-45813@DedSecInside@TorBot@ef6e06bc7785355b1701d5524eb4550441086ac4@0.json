{
  "cve_id": "CVE-2023-45813",
  "cve_desc": "Torbot is an open source tor network intelligence tool. In affected versions the `torbot.modules.validators.validate_link function` uses the python-validators URL validation regex. This particular regular expression has an exponential complexity which allows an attacker to cause an application crash using a well-crafted argument. An attacker can use a well-crafted URL argument to exploit the vulnerability in the regular expression and cause a Denial of Service on the system. The validators file has been removed in version 4.0.0. Users are advised to upgrade. There are no known workarounds for this vulnerability.",
  "repo": "DedSecInside/TorBot",
  "patch_hash": "ef6e06bc7785355b1701d5524eb4550441086ac4",
  "patch_info": {
    "commit_hash": "ef6e06bc7785355b1701d5524eb4550441086ac4",
    "repo": "DedSecInside/TorBot",
    "commit_url": "https://github.com/DedSecInside/TorBot/commit/ef6e06bc7785355b1701d5524eb4550441086ac4",
    "files": [
      "torbot/modules/validators.py"
    ],
    "message": "remove unused validators file",
    "before_after_code_files": [
      "torbot/modules/validators.py||torbot/modules/validators.py"
    ]
  },
  "patch_diff": {
    "torbot/modules/validators.py||torbot/modules/validators.py": [
      "File: torbot/modules/validators.py -> torbot/modules/validators.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "[No context available]",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "[None]",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "ddabe8a0dc676ffe1663e479464c764517331a8a",
      "candidate_info": {
        "commit_hash": "ddabe8a0dc676ffe1663e479464c764517331a8a",
        "repo": "DedSecInside/TorBot",
        "commit_url": "https://github.com/DedSecInside/TorBot/commit/ddabe8a0dc676ffe1663e479464c764517331a8a",
        "files": [
          "poetry.lock",
          "pyproject.toml",
          "requirements.txt",
          "torbot/modules/linktree.py"
        ],
        "message": "syntax fix and removing threadsafe",
        "before_after_code_files": [
          "poetry.lock||poetry.lock",
          "torbot/modules/linktree.py||torbot/modules/linktree.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/DedSecInside/TorBot/pull/307"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "poetry.lock||poetry.lock": [
          "File: poetry.lock -> poetry.lock",
          "--- Hunk 1 ---",
          "[Context before]",
          "612:     {file = \"threadpoolctl-3.1.0.tar.gz\", hash = \"sha256:a335baacfaa4400ae1f0d8e3a58d6674d2f8828e3716bb2802c44955ad391380\"},",
          "613: ]",
          "626: [[package]]",
          "627: name = \"treelib\"",
          "628: version = \"1.7.0\"",
          "",
          "[Removed Lines]",
          "615: [[package]]",
          "616: name = \"threadsafe\"",
          "617: version = \"1.0.0\"",
          "618: description = \"Thread-safe data structures\"",
          "619: optional = false",
          "620: python-versions = \"*\"",
          "621: files = [",
          "622:     {file = \"threadsafe-1.0.0-py3-none-any.whl\", hash = \"sha256:acbd59278ca8221dc3a8051443fe24c647ee9ac81808058e280ef6f75dd4387b\"},",
          "623:     {file = \"threadsafe-1.0.0.tar.gz\", hash = \"sha256:7c61f9fdd0b3cd6c07b427de355dafcd337578d30871634cb1e8985ee4955edc\"},",
          "624: ]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "693: [metadata]",
          "694: lock-version = \"2.0\"",
          "695: python-versions = \">=3.9,<=3.11.4\"",
          "",
          "[Removed Lines]",
          "696: content-hash = \"fa048130f884a71b33d42a8dd2940a2c17365309afe56ae1c6abc2dfc6ee5a40\"",
          "",
          "[Added Lines]",
          "685: content-hash = \"1e6d83812ac5be9a550b998795ee28f76bb788e972c933e497e68b20f548a0ea\"",
          "",
          "---------------"
        ],
        "torbot/modules/linktree.py||torbot/modules/linktree.py": [
          "File: torbot/modules/linktree.py -> torbot/modules/linktree.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "60:         except exceptions.DuplicatedNodeIdError:",
          "61:             logging.debug(f\"found a duplicate URL {id}\")",
          "64:         \"\"\"",
          "65:         Builds a tree from the root to the given depth.",
          "66:         \"\"\"",
          "",
          "[Removed Lines]",
          "63:     def _build_tree(self,  url: str, depth: int) -> None:",
          "",
          "[Added Lines]",
          "63:     def _build_tree(self, url: str, depth: int) -> None:",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "182f51abb5596410e8eb875885d61dc0b9b19096",
      "candidate_info": {
        "commit_hash": "182f51abb5596410e8eb875885d61dc0b9b19096",
        "repo": "DedSecInside/TorBot",
        "commit_url": "https://github.com/DedSecInside/TorBot/commit/182f51abb5596410e8eb875885d61dc0b9b19096",
        "files": [
          ".env",
          "README.md",
          "torbot/main.py",
          "torbot/modules/config.py",
          "torbot/modules/io.py",
          "torbot/modules/linktree.py",
          "torbot/modules/savefile.py",
          "torbot/modules/tests/test_savetofile.py"
        ],
        "message": "more major changes",
        "before_after_code_files": [
          "torbot/main.py||torbot/main.py",
          "torbot/modules/config.py||torbot/modules/config.py",
          "torbot/modules/io.py||torbot/modules/io.py",
          "torbot/modules/linktree.py||torbot/modules/linktree.py",
          "torbot/modules/savefile.py||torbot/modules/savefile.py",
          "torbot/modules/tests/test_savetofile.py||torbot/modules/tests/test_savetofile.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/DedSecInside/TorBot/pull/307"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "torbot/main.py||torbot/main.py": [
          "File: torbot/main.py -> torbot/main.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1: \"\"\"",
          "2: Core",
          "3: \"\"\"",
          "4: import argparse",
          "5: import sys",
          "6: import logging",
          "9: from modules.color import color",
          "10: from modules.updater import check_version",
          "11: from modules.info import execute_all",
          "12: from modules.linktree import LinkTree",
          "18:     \"\"\"",
          "19:     Prints the TorBot banner including version and license.",
          "20:     \"\"\"",
          "",
          "[Removed Lines]",
          "8: from modules.io import pprint_tree, print_tor_ip_address",
          "14: VERSION = '3.1.2'",
          "17: def print_header() -> None:",
          "",
          "[Added Lines]",
          "4: import os",
          "8: import tomllib",
          "10: from modules.api import get_ip",
          "15: from modules.config import project_root_directory",
          "18: def print_tor_ip_address() -> None:",
          "19:     \"\"\"",
          "20:     https://check.torproject.org/ tells you if you are using tor and it",
          "21:     displays your IP address which we scape and display",
          "22:     \"\"\"",
          "23:     resp = get_ip()",
          "24:     print(resp[\"header\"])",
          "25:     print(color(resp[\"body\"], \"yellow\"))",
          "28: def print_header(version: str) -> None:",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "24:                             / /_/ __ \\/ __ \\/ /_  ____/_  __/",
          "25:                         / __/ / / / /_/ / __ \\/ __ \\/ /",
          "26:                         / /_/ /_/ / _, _/ /_/ / /_/ / /",
          "29:     banner = color(banner, \"red\")",
          "31:     title = r\"\"\"",
          "",
          "[Removed Lines]",
          "27:                         \\__/\\____/_/ |_/_____/\\____/_/  V{VERSION}",
          "28:             \"\"\".format(VERSION=VERSION)",
          "",
          "[Added Lines]",
          "38:                         \\__/\\____/_/ |_/_____/\\____/_/  v{VERSION}",
          "39:             \"\"\".format(VERSION=version)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "42:     print(title)",
          "46:     args = arg_parser.parse_args()",
          "48:     # setup logging",
          "",
          "[Removed Lines]",
          "45: def run(arg_parser: argparse.ArgumentParser) -> None:",
          "",
          "[Added Lines]",
          "56: def run(arg_parser: argparse.ArgumentParser, version: str) -> None:",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "59:     # Print verison then exit",
          "60:     if args.version:",
          "62:         sys.exit()",
          "64:     # check version and update if necessary",
          "",
          "[Removed Lines]",
          "61:         print(f\"TorBot Version: {VERSION}\")",
          "",
          "[Added Lines]",
          "72:         print(f\"TorBot Version: {version}\")",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "69:     # print header and IP address if not set to quiet",
          "70:     if not args.quiet:",
          "72:         print_tor_ip_address()",
          "74:     if args.info:",
          "",
          "[Removed Lines]",
          "71:         print_header()",
          "",
          "[Added Lines]",
          "82:         print_header(version)",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "77:     tree = LinkTree(url=args.url, depth=args.depth)",
          "78:     tree.load()",
          "81:         tree.save()",
          "84:         tree.show()",
          "91:     print(\"\\n\\n\")",
          "",
          "[Removed Lines]",
          "79:     # save tree and continue",
          "80:     if args.save:",
          "83:     if args.visualize:",
          "86:     pprint_tree(tree)",
          "87:     '''",
          "88:     elif args.save or args.mail or args.phone:",
          "89:         self.handle_json_args(args)",
          "90:     '''",
          "",
          "[Added Lines]",
          "91:     # save data if desired",
          "92:     if args.save == 'tree':",
          "94:     elif args.save == 'json':",
          "95:         tree.saveJSON()",
          "97:     # always print something, table is the default",
          "98:     if args.visualize == 'table' or not args.visualize:",
          "99:         tree.showTable()",
          "100:     elif args.visualize == 'tree':",
          "102:     elif args.visualize == 'json':",
          "103:         tree.showJSON()",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "98:     parser = argparse.ArgumentParser(prog=\"TorBot\", usage=\"Gather and analayze data from Tor sites.\")",
          "99:     parser.add_argument(\"-u\", \"--url\", type=str, required=True, help=\"Specifiy a website link to crawl\")",
          "100:     parser.add_argument(\"--depth\", type=int, help=\"Specifiy max depth of crawler (default 1)\", default=1)",
          "101:     parser.add_argument(\"-q\", \"--quiet\", action=\"store_true\")",
          "104:     parser.add_argument(\"--version\", action=\"store_true\", help=\"Show current version of TorBot.\")",
          "105:     parser.add_argument(\"--update\", action=\"store_true\", help=\"Update TorBot to the latest stable version\")",
          "107:     parser.add_argument(\"--info\", action=\"store_true\", help=\"Info displays basic info of the scanned site. Only supports a single URL at a time.\")",
          "109:     parser.add_argument(\"-v\", action=\"store_true\", help=\"verbose logging\")",
          "111:     return parser",
          "",
          "[Removed Lines]",
          "102:     parser.add_argument(\"-m\", \"--mail\", action=\"store_true\", help=\"Get e-mail addresses from the crawled sites\")",
          "103:     parser.add_argument(\"-p\", \"--phone\", action=\"store_true\", help=\"Get phone numbers from the crawled sites\")",
          "106:     parser.add_argument(\"--save\", action=\"store_true\", help=\"Save results in a file\")",
          "108:     parser.add_argument(\"--visualize\", action=\"store_true\", help=\"Visualizes tree of data gathered.\")",
          "",
          "[Added Lines]",
          "115:     parser.add_argument(\"--save\", type=str, choices=['tree', 'json'], help=\"Save results in a file\")",
          "116:     parser.add_argument(\"--visualize\", type=str, choices=['table', 'tree', 'json'], help=\"Visualizes data collection.\")",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "114: if __name__ == '__main__':",
          "115:     try:",
          "116:         arg_parser = set_arguments()",
          "118:     except KeyboardInterrupt:",
          "119:         print(\"Interrupt received! Exiting cleanly...\")",
          "",
          "[Removed Lines]",
          "117:         run(arg_parser)",
          "",
          "[Added Lines]",
          "129:         config_file_path = os.path.join(project_root_directory, \"pyproject.toml\")",
          "130:         try:",
          "131:             version = None",
          "132:             with open(config_file_path, \"rb\") as f:",
          "133:                 data = tomllib.load(f)",
          "134:                 version = data['tool']['poetry']['version']",
          "135:         except Exception as e:",
          "136:             raise Exception(\"unable to find version from pyprojec.toml.\\n\", e)",
          "138:         run(arg_parser, version)",
          "",
          "---------------"
        ],
        "torbot/modules/config.py||torbot/modules/config.py": [
          "File: torbot/modules/config.py -> torbot/modules/config.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1: import os",
          "4: from dotenv import load_dotenv",
          "5: from inspect import getsourcefile",
          "",
          "[Removed Lines]",
          "2: import logging",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "torbot/modules/io.py||torbot/modules/io.py": [
          "File: torbot/modules/io.py -> torbot/modules/io.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "torbot/modules/linktree.py||torbot/modules/linktree.py": [
          "File: torbot/modules/linktree.py -> torbot/modules/linktree.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1: \"\"\"",
          "2: Module is used for analyzing link relationships",
          "3: \"\"\"",
          "4: import os",
          "5: import httpx",
          "6: import validators",
          "7: import logging",
          "9: from treelib import Tree, exceptions, Node",
          "10: from bs4 import BeautifulSoup",
          "12: from .config import project_root_directory",
          "13: from .nlp.main import classify",
          "16: class LinkNode(Node):",
          "19:         super().__init__()",
          "20:         self.identifier = url",
          "21:         self.tag = title",
          "22:         self.status = status",
          "23:         self.classification = classification",
          "24:         self.accuracy = accuracy",
          "26: class LinkTree(Tree):",
          "27:     def __init__(self, url: str, depth: int) -> None:",
          "",
          "[Removed Lines]",
          "17:     def __init__(self, title: str, url: str, status: int,",
          "18:                  classification: str, accuracy: float):",
          "",
          "[Added Lines]",
          "4: import http.client",
          "9: import phonenumbers",
          "11: from urllib import parse",
          "12: from tabulate import tabulate",
          "16: from .color import color",
          "22:     def __init__(self, title: str, url: str, status: int, classification: str, accuracy: float,",
          "23:                  numbers: list[str], emails: list[str]):",
          "30:         self.numbers = numbers",
          "31:         self.emails = emails",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "38:         Creates a node for a tree using the given ID which corresponds to a URL.",
          "39:         If the parent_id is None, this will be considered a root node.",
          "40:         \"\"\"",
          "42:         soup = BeautifulSoup(resp.text, 'html.parser')",
          "44:         try:",
          "45:             [classification, accuracy] = classify(resp.text)",
          "47:             self.create_node(title, identifier=id, parent=parent_id, data=data)",
          "48:         except exceptions.DuplicatedNodeIdError:",
          "49:             logging.debug(f\"found a duplicate URL {id}\")",
          "",
          "[Removed Lines]",
          "41:         resp = httpx.get(id, proxies='socks5://127.0.0.1:9050')",
          "43:         title = soup.title.text.strip() if soup.title is not None else id",
          "46:             data = LinkNode(title, id, resp.status_code, classification, accuracy)",
          "",
          "[Added Lines]",
          "49:         resp = httpx.get(id, timeout=60, proxies='socks5://127.0.0.1:9050')",
          "51:         title = soup.title.text.strip() if soup.title is not None else parse_hostname(id)",
          "54:             numbers = parse_phone_numbers(soup)",
          "55:             emails = parse_emails(soup)",
          "56:             data = LinkNode(title, id, resp.status_code, classification, accuracy, numbers, emails)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "54:         \"\"\"",
          "55:         if depth > 0:",
          "56:             depth -= 1",
          "58:             children = parse_links(resp.text)",
          "59:             for child in children:",
          "60:                 self._append_node(id=child, parent_id=url)",
          "61:                 self._build_tree(url=child, depth=depth)",
          "63:     def save(self) -> None:",
          "64:         \"\"\"",
          "65:         Saves the tree to the current working directory under the given file name.",
          "66:         \"\"\"",
          "72: def parse_links(html: str) -> list[str]:",
          "",
          "[Removed Lines]",
          "57:             resp = httpx.get(url, proxies='socks5://127.0.0.1:9050')",
          "67:         root_id = self.root",
          "68:         root_node = self.get_node(root_id)",
          "69:         self.save2file(os.path.join(project_root_directory, root_node.tag))",
          "",
          "[Added Lines]",
          "67:             resp = httpx.get(url, timeout=60, proxies='socks5://127.0.0.1:9050')",
          "73:     def _get_tree_file_name(self) -> str:",
          "74:         root_id = self.root",
          "75:         root_node = self.get_node(root_id)",
          "76:         if root_node is None:",
          "77:             raise Exception('no root node can be found.')",
          "79:         return os.path.join(project_root_directory, f'{root_node.tag} - Depth {self._depth}')",
          "85:         file_name = self._get_tree_file_name()",
          "86:         self.save2file(file_name)",
          "88:     def saveJSON(self) -> None:",
          "89:         \"\"\"",
          "90:         Saves the tree to the current working directory under the given file name in JSON.",
          "91:         \"\"\"",
          "92:         json_data = self.to_json()",
          "93:         with open(self._get_tree_file_name(), 'w+') as f:",
          "94:             f.write(json_data)",
          "96:     def showJSON(self) -> None:",
          "97:         \"\"\"",
          "98:         Prints tree to console as JSON",
          "99:         \"\"\"",
          "100:         json_data = self.to_json()",
          "101:         print(json_data)",
          "103:     def showTable(self) -> None:",
          "104:         \"\"\"",
          "105:         Prints the status of a link based on it's connection status",
          "106:         \"\"\"",
          "107:         nodes = self.all_nodes_itr()",
          "108:         table_data = []",
          "110:         def insert(node, color_code):",
          "111:             status = str(node.data.status)",
          "112:             code = http.client.responses[node.data.status]",
          "113:             status_message = f'{status} {code}'",
          "114:             table_data.append([",
          "115:                 node.tag,",
          "116:                 node.identifier,",
          "117:                 color(status_message, color_code),",
          "118:                 node.data.numbers,",
          "119:                 node.data.emails,",
          "120:                 node.data.classification,",
          "121:             ])",
          "123:         for node in nodes:",
          "124:             status_code = node.data.status",
          "125:             if status_code >= 200 and status_code < 300:",
          "126:                 insert(node, 'green')",
          "127:             elif status_code >= 300 and status_code < 400:",
          "128:                 insert(node, 'yellow')",
          "129:             else:",
          "130:                 insert(node, 'red')",
          "132:         headers = [\"Title\", \"URL\", \"Status\", \"Phone Numbers\", \"Emails\", \"Category\"]",
          "133:         table = tabulate(table_data, headers=headers)",
          "134:         print(table)",
          "137: def parse_hostname(url: str) -> str:",
          "138:     hostname = parse.urlsplit(url).hostname",
          "139:     if hostname is not None:",
          "140:         return hostname",
          "142:     raise Exception('unable to parse hostname from URL')",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "77:     tags = soup.find_all('a')",
          "78:     return [tag['href'] for tag in tags if tag.has_attr('href') and validators.url(tag['href'])]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "154: def parse_emails(soup: BeautifulSoup) -> list[str]:",
          "155:     \"\"\"",
          "156:     Finds all anchor tags and parses the email href attributes.",
          "157:     example attribute: `mailto:example@example.com`",
          "158:     \"\"\"",
          "159:     tags = soup.find_all('a')",
          "161:     emails = set()",
          "162:     for tag in tags:",
          "163:         if tag.has_attr('href') and 'mailto:' in tag['href']:",
          "164:             email = tag['href'].split('mailto:', 1)[1]",
          "165:             if validators.email(email):",
          "166:                 emails.add(set)",
          "168:     return list(emails)",
          "171: def parse_phone_numbers(soup: BeautifulSoup) -> list[str]:",
          "172:     \"\"\"",
          "173:     Finds all anchor tags and parses the href attribute.",
          "174:     \"\"\"",
          "175:     tags = soup.find_all('a')",
          "176:     numbers = set()",
          "177:     for tag in tags:",
          "178:         if tag.has_attr('href') and 'tel:' in tag['href']:",
          "179:             number = tag['href'].split('tel:', 1)[1]",
          "180:             try:",
          "181:                 if phonenumbers.is_valid_number(number):",
          "182:                     numbers.add(number)",
          "183:             except:",
          "184:                 pass",
          "186:             try:",
          "187:                 if phonenumbers.is_valid_number(tag['href']):",
          "188:                     numbers.add(tag['href'])",
          "189:             except:",
          "190:                 pass",
          "192:     return list(numbers)",
          "",
          "---------------"
        ],
        "torbot/modules/savefile.py||torbot/modules/savefile.py": [
          "File: torbot/modules/savefile.py -> torbot/modules/savefile.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "torbot/modules/tests/test_savetofile.py||torbot/modules/tests/test_savetofile.py": [
          "File: torbot/modules/tests/test_savetofile.py -> torbot/modules/tests/test_savetofile.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "71f8cc887853d50d45fd0eec938ecfe9f3177a3f",
      "candidate_info": {
        "commit_hash": "71f8cc887853d50d45fd0eec938ecfe9f3177a3f",
        "repo": "DedSecInside/TorBot",
        "commit_url": "https://github.com/DedSecInside/TorBot/commit/71f8cc887853d50d45fd0eec938ecfe9f3177a3f",
        "files": [
          "torbot/modules/link_io.py"
        ],
        "message": "Utilize new api in IO module",
        "before_after_code_files": [
          "torbot/modules/link_io.py||torbot/modules/link_io.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/DedSecInside/TorBot/pull/307"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "torbot/modules/link_io.py||torbot/modules/link_io.py": [
          "File: torbot/modules/link_io.py -> torbot/modules/link_io.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "2: This module is used for reading HTML pages using either bs4.BeautifulSoup",
          "3: objects or url strings",
          "4: \"\"\"",
          "6: from pprint import pprint",
          "11: from .color import color",
          "16:     \"\"\"",
          "17:     https://check.torproject.org/ tells you if you are using tor and it",
          "18:     displays your IP address which we scape and display",
          "19:     \"\"\"",
          "26:     \"\"\"",
          "27:     Prints the status of a link based on it's connection status",
          "28:     \"\"\"",
          "39:         else:",
          "64: def print_json(url: str, depth: int = 1):",
          "",
          "[Removed Lines]",
          "7: from typing import Any",
          "9: from .linktree import LinkTree",
          "10: from .api import get_web_content, get_node, get_emails, get_phone, get_ip",
          "12: from .nlp.main import classify",
          "15: def print_tor_ip_address():",
          "20:     print('Attempting to connect to https://check.torproject.org/')",
          "21:     ip_string = color(get_ip(), 'yellow')",
          "22:     print(f'Tor IP Address: {ip_string}')",
          "25: def print_node(node: LinkTree, classify_page: bool):",
          "29:     try:",
          "30:         title = node['url']",
          "31:         status_text = f\"{node['status_code']} {node['status']}\"",
          "32:         if classify_page:",
          "33:             classification = classify(get_web_content(node['url']))",
          "34:             status_text += f\" {classification}\"",
          "35:         if node['status_code'] >= 200 and node['status_code'] < 300:",
          "36:             status = color(status_text, 'green')",
          "37:         elif node['status_code'] >= 300 and node['status_code'] < 400:",
          "38:             status = color(status_text, 'yellow')",
          "40:             status = color(status_text, 'red')",
          "41:     except Exception:",
          "42:         title = \"NOT FOUND\"",
          "43:         status = color('Unable to reach destination.', 'red')",
          "45:     status_msg = \"%-60s %-20s\" % (title, status)",
          "46:     print(status_msg)",
          "49: def cascade(node: LinkTree, work: Any, classify_page: bool):",
          "50:     work(node, classify_page)",
          "51:     if node['children']:",
          "52:         for child in node['children']:",
          "53:             cascade(child, work, classify_page)",
          "56: def print_tree(url: str, depth: int = 1, classify_page: bool = False):",
          "57:     \"\"\"",
          "58:     Prints the entire tree in a user friendly fashion",
          "59:     \"\"\"",
          "60:     root = get_node(url, depth)",
          "61:     cascade(root, print_node, classify_page)",
          "",
          "[Added Lines]",
          "5: import http.client",
          "6: import tabulate",
          "9: from treelib import Tree",
          "11: from .api import get_node, get_emails, get_phone, get_ip",
          "15: def print_tor_ip_address() -> None:",
          "20:     resp = get_ip()",
          "21:     print(resp[\"header\"])",
          "22:     print(color(resp[\"body\"], \"yellow\"))",
          "25: def pprint_tree(tree: Tree) -> None:",
          "29:     nodes = tree.all_nodes_itr()",
          "30:     table_data = []",
          "32:     def insert(node, color_code):",
          "33:         status = str(node.data.status)",
          "34:         code = http.client.responses[node.data.status]",
          "35:         status_message = f'{status} {code}'",
          "36:         table_data.append([",
          "37:             node.tag,",
          "38:             node.identifier,",
          "39:             color(status_message, color_code),",
          "40:             node.data.classification,",
          "41:         ])",
          "43:     for node in nodes:",
          "44:         status_code = node.data.status",
          "45:         if status_code >= 200 and status_code < 300:",
          "46:             insert(node, 'green')",
          "47:         elif status_code >= 300 and status_code < 400:",
          "48:             insert(node, 'yellow')",
          "50:             insert(node, 'red')",
          "52:     headers = [\"Title\", \"URL\", \"Status\", \"Category\"]",
          "53:     table = tabulate.tabulate(table_data, headers=headers)",
          "54:     print(table)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "69:         root (dict): Dictionary containing the root node and it's children",
          "70:     \"\"\"",
          "71:     root = get_node(url, depth)",
          "76: def print_emails(url: str):",
          "",
          "[Removed Lines]",
          "72:     pprint(root)",
          "73:     return root",
          "",
          "[Added Lines]",
          "65:     print(root.to_json())",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "5cfcde398d0d0a143096e6301053955d01b9ac70",
      "candidate_info": {
        "commit_hash": "5cfcde398d0d0a143096e6301053955d01b9ac70",
        "repo": "DedSecInside/TorBot",
        "commit_url": "https://github.com/DedSecInside/TorBot/commit/5cfcde398d0d0a143096e6301053955d01b9ac70",
        "files": [
          ".env",
          "poetry.lock",
          "pyproject.toml",
          "requirements.txt",
          "torbot/main.py",
          "torbot/modules/api.py",
          "torbot/modules/config.py",
          "torbot/modules/info.py",
          "torbot/modules/linktree.py",
          "torbot/modules/tests/test_pagereader.py"
        ],
        "message": "more major changes",
        "before_after_code_files": [
          "poetry.lock||poetry.lock",
          "torbot/main.py||torbot/main.py",
          "torbot/modules/api.py||torbot/modules/api.py",
          "torbot/modules/config.py||torbot/modules/config.py",
          "torbot/modules/info.py||torbot/modules/info.py",
          "torbot/modules/linktree.py||torbot/modules/linktree.py",
          "torbot/modules/tests/test_pagereader.py||torbot/modules/tests/test_pagereader.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/DedSecInside/TorBot/pull/307"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "poetry.lock||poetry.lock": [
          "File: poetry.lock -> poetry.lock",
          "--- Hunk 1 ---",
          "[Context before]",
          "413:     {file = \"pywin32_ctypes-0.2.2-py3-none-any.whl\", hash = \"sha256:bf490a1a709baf35d688fe0ecf980ed4de11d2b3e37b51e5442587a75d9957e7\"},",
          "414: ]",
          "437: [[package]]",
          "438: name = \"scikit-learn\"",
          "439: version = \"1.3.0\"",
          "",
          "[Removed Lines]",
          "416: [[package]]",
          "417: name = \"requests\"",
          "418: version = \"2.31.0\"",
          "419: description = \"Python HTTP for Humans.\"",
          "420: optional = false",
          "421: python-versions = \">=3.7\"",
          "422: files = [",
          "423:     {file = \"requests-2.31.0-py3-none-any.whl\", hash = \"sha256:58cd2187c01e70e6e26505bca751777aa9f2ee0b7f4300988b709f44e013003f\"},",
          "424:     {file = \"requests-2.31.0.tar.gz\", hash = \"sha256:942c5a758f98d790eaed1a29cb6eefc7ffb0d1cf7af05c3d2791656dbd6ad1e1\"},",
          "425: ]",
          "427: [package.dependencies]",
          "428: certifi = \">=2017.4.17\"",
          "429: charset-normalizer = \">=2,<4\"",
          "430: idna = \">=2.5,<4\"",
          "431: urllib3 = \">=1.21.1,<3\"",
          "433: [package.extras]",
          "434: socks = [\"PySocks (>=1.5.6,!=1.5.7)\"]",
          "435: use-chardet-on-py3 = [\"chardet (>=3.0.2,<6)\"]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "714: [metadata]",
          "715: lock-version = \"2.0\"",
          "716: python-versions = \">=3.9,<=3.11.4\"",
          "",
          "[Removed Lines]",
          "717: content-hash = \"7b3ae36389472ec97dd5aacc437381b5c7f13f3d08e4ab738ef699b46c85a17a\"",
          "",
          "[Added Lines]",
          "696: content-hash = \"fa048130f884a71b33d42a8dd2940a2c17365309afe56ae1c6abc2dfc6ee5a40\"",
          "",
          "---------------"
        ],
        "torbot/main.py||torbot/main.py": [
          "File: torbot/main.py -> torbot/main.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "6: import sys",
          "7: import logging",
          "8: import tomllib",
          "10: from modules.api import get_ip",
          "11: from modules.color import color",
          "12: from modules.updater import check_version",
          "13: from modules.info import execute_all",
          "14: from modules.linktree import LinkTree",
          "19:     \"\"\"",
          "20:     https://check.torproject.org/ tells you if you are using tor and it",
          "21:     displays your IP address which we scape and display",
          "22:     \"\"\"",
          "24:     print(resp[\"header\"])",
          "25:     print(color(resp[\"body\"], \"yellow\"))",
          "",
          "[Removed Lines]",
          "15: from modules.config import project_root_directory",
          "18: def print_tor_ip_address() -> None:",
          "23:     resp = get_ip()",
          "",
          "[Added Lines]",
          "9: import httpx",
          "16: from modules.config import project_root_directory, socks5_host, socks5_port",
          "19: def print_tor_ip_address(client: httpx.Client) -> None:",
          "24:     resp = get_ip(client)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "77:         check_version()",
          "78:         sys.exit()",
          "105:     print(\"\\n\\n\")",
          "",
          "[Removed Lines]",
          "80:     # print header and IP address if not set to quiet",
          "81:     if not args.quiet:",
          "82:         print_header(version)",
          "83:         print_tor_ip_address()",
          "85:     if args.info:",
          "86:         execute_all(args.url)",
          "88:     tree = LinkTree(url=args.url, depth=args.depth)",
          "89:     tree.load()",
          "91:     # save data if desired",
          "92:     if args.save == 'tree':",
          "93:         tree.save()",
          "94:     elif args.save == 'json':",
          "95:         tree.saveJSON()",
          "97:     # always print something, table is the default",
          "98:     if args.visualize == 'table' or not args.visualize:",
          "99:         tree.showTable()",
          "100:     elif args.visualize == 'tree':",
          "101:         print(tree)",
          "102:     elif args.visualize == 'json':",
          "103:         tree.showJSON()",
          "",
          "[Added Lines]",
          "82:     socks5_proxy = f'socks5://{socks5_host}:{socks5_port}'",
          "83:     with httpx.Client(timeout=60, proxies=socks5_proxy) as client:",
          "84:         # print header and IP address if not set to quiet",
          "85:         if not args.quiet:",
          "86:             print_header(version)",
          "87:             print_tor_ip_address(client)",
          "89:         if args.info:",
          "90:             execute_all(client, args.url)",
          "92:         tree = LinkTree(url=args.url, depth=args.depth, client=client)",
          "93:         tree.load()",
          "95:         # save data if desired",
          "96:         if args.save == 'tree':",
          "97:             tree.save()",
          "98:         elif args.save == 'json':",
          "99:             tree.saveJSON()",
          "101:         # always print something, table is the default",
          "102:         if args.visualize == 'table' or not args.visualize:",
          "103:             tree.showTable()",
          "104:         elif args.visualize == 'tree':",
          "105:             print(tree)",
          "106:         elif args.visualize == 'json':",
          "107:             tree.showJSON()",
          "",
          "---------------"
        ],
        "torbot/modules/api.py||torbot/modules/api.py": [
          "File: torbot/modules/api.py -> torbot/modules/api.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "12: logging.getLogger(\"httpx\").setLevel(logging.WARNING)",
          "16:     \"\"\"",
          "17:     Returns the IP address of the current Tor client the service is using.",
          "18:     \"\"\"",
          "20:     soup = BeautifulSoup(resp.text, 'html.parser')",
          "22:     # Get the content of check tor project, this contains the header and body",
          "",
          "[Removed Lines]",
          "15: def get_ip() -> dict:",
          "19:     resp = httpx.get(\"https://check.torproject.org/\", proxies='socks5://127.0.0.1:9050')",
          "",
          "[Added Lines]",
          "15: def get_ip(client: httpx.Client) -> dict:",
          "19:     resp = client.get(\"https://check.torproject.org/\")",
          "",
          "---------------"
        ],
        "torbot/modules/config.py||torbot/modules/config.py": [
          "File: torbot/modules/config.py -> torbot/modules/config.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: dotenv_path = os.path.join(project_root_directory, '.env')",
          "19: load_dotenv(dotenv_path=dotenv_path, verbose=True)",
          "21: def get_data_directory():",
          "22:     data_directory = os.getenv('TORBOT_DATA_DIR')",
          "23:     # if a path is not set, write data to the config directory",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "21: socks5_host = os.getenv('SOCKS5_HOST')",
          "22: socks5_port = os.getenv('SOCKS5_PORT')",
          "",
          "---------------"
        ],
        "torbot/modules/info.py||torbot/modules/info.py": [
          "File: torbot/modules/info.py -> torbot/modules/info.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "8: from urllib.parse import urlsplit",
          "9: from bs4 import BeautifulSoup",
          "10: from termcolor import cprint",
          "14: keys = set()  # high entropy strings, prolly secret keys",
          "",
          "[Removed Lines]",
          "11: from requests.exceptions import HTTPError",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "32: ]",
          "36:     \"\"\"Initialise datasets and functions to retrieve data, and execute",
          "37:     each for a given link.",
          "",
          "[Removed Lines]",
          "35: def execute_all(link, *, display_status=False):",
          "",
          "[Added Lines]",
          "34: def execute_all(client: httpx.Client, link: str, *, display_status: bool = False) -> None:",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "42:             attempts to terminal.",
          "43:     \"\"\"",
          "46:     soup = BeautifulSoup(resp.text, 'html.parser')",
          "47:     validation_functions = [",
          "48:         get_robots_txt, get_dot_git, get_dot_svn, get_dot_git, get_intel, get_dot_htaccess, get_bitcoin_address",
          "49:     ]",
          "50:     for validate_func in validation_functions:",
          "51:         try:",
          "54:             cprint('Error', 'red')",
          "56:     display_webpage_description(soup)",
          "",
          "[Removed Lines]",
          "45:     resp = httpx.get(link, proxies='socks5://127.0.0.1:9050')",
          "52:             validate_func(link, resp)",
          "53:         except (ConnectionError, HTTPError):",
          "",
          "[Added Lines]",
          "44:     resp = client.get(link)",
          "51:             validate_func(client, link, resp)",
          "52:         except:",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "71:         print('*', key, ':', val)",
          "75:     \"\"\" Check link for Robot.txt, and if found, add link to robots dataset.",
          "77:     Args:",
          "",
          "[Removed Lines]",
          "74: def get_robots_txt(target, response):",
          "",
          "[Added Lines]",
          "73: def get_robots_txt(client: httpx.Client, target: str, response: str) -> None:",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "81:     cprint(\"[*]Checking for Robots.txt\", 'yellow')",
          "82:     url = target",
          "83:     target = \"{0.scheme}://{0.netloc}/\".format(urlsplit(url))",
          "85:     print(target + \"robots.txt\")",
          "86:     matches = re.findall(r'Allow: (.*)|Disallow: (.*)', response)",
          "87:     for match in matches:",
          "",
          "[Removed Lines]",
          "84:     httpx.get(target + \"robots.txt\", proxies='socks5://127.0.0.1:9050')",
          "",
          "[Added Lines]",
          "83:     client.get(target + \"robots.txt\")",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "93:         print(robots)",
          "97:     \"\"\" Check link for intel, and if found, add link to intel dataset,",
          "98:     including but not limited to website accounts and AWS buckets.",
          "",
          "[Removed Lines]",
          "96: def get_intel(link, response):",
          "",
          "[Added Lines]",
          "95: def get_intel(client: httpx.Client, url: str, response: str) -> None:",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "109:         intel.add(match)",
          "113:     \"\"\" Check link for .git folders exposed on public domain.",
          "115:     Args:",
          "",
          "[Removed Lines]",
          "112: def get_dot_git(target, response):",
          "",
          "[Added Lines]",
          "111: def get_dot_git(client: httpx.Client, target: str, response: str) -> None:",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "119:     cprint(\"[*]Checking for .git folder\", 'yellow')",
          "120:     url = target",
          "121:     target = \"{0.scheme}://{0.netloc}/\".format(urlsplit(url))",
          "123:     if not resp.text.__contains__(\"404\"):",
          "124:         cprint(\"Alert!\", 'red')",
          "125:         cprint(\".git folder exposed publicly\", 'red')",
          "",
          "[Removed Lines]",
          "122:     resp = httpx.get(target + \"/.git/config\", proxies='socks5://127.0.0.1:9050')",
          "",
          "[Added Lines]",
          "121:     resp = client.get(target + \"/.git/config\")",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "127:         cprint(\"NO .git folder found\", 'blue')",
          "131:     \"\"\" Check link for Bitcoin addresses, and if found, print.",
          "133:     Args:",
          "",
          "[Removed Lines]",
          "130: def get_bitcoin_address(target, response):",
          "",
          "[Added Lines]",
          "129: def get_bitcoin_address(client: httpx.Client, target: str, response: str) -> None:",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "140:         print(\"BTC: \", bitcoin)",
          "144:     \"\"\" Check link for .svn folders exposed on public domain=.",
          "146:     Args:",
          "",
          "[Removed Lines]",
          "143: def get_dot_svn(target, response):",
          "",
          "[Added Lines]",
          "142: def get_dot_svn(client: httpx.Client, target: str, response: str) -> None:",
          "",
          "---------------",
          "--- Hunk 11 ---",
          "[Context before]",
          "158:         cprint(\"NO .SVN folder found\", 'blue')",
          "162:     \"\"\" Check link for .htaccess files on public domain.",
          "164:     Args:",
          "",
          "[Removed Lines]",
          "161: def get_dot_htaccess(target, response):",
          "",
          "[Added Lines]",
          "160: def get_dot_htaccess(client: httpx.Client, target: str, response: str) -> None:",
          "",
          "---------------",
          "--- Hunk 12 ---",
          "[Context before]",
          "179:         cprint(resp, 'blue')",
          "183:     \"\"\"Print all meta tags found in page.",
          "185:     Args:",
          "",
          "[Removed Lines]",
          "182: def display_webpage_description(soup):",
          "",
          "[Added Lines]",
          "181: def display_webpage_description(soup: BeautifulSoup) -> None:",
          "",
          "---------------"
        ],
        "torbot/modules/linktree.py||torbot/modules/linktree.py": [
          "File: torbot/modules/linktree.py -> torbot/modules/linktree.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "34: class LinkTree(Tree):",
          "36:         super().__init__()",
          "37:         self._url = url",
          "38:         self._depth = depth",
          "40:     def load(self) -> None:",
          "41:         self._append_node(id=self._url, parent_id=None)",
          "",
          "[Removed Lines]",
          "35:     def __init__(self, url: str, depth: int) -> None:",
          "",
          "[Added Lines]",
          "35:     def __init__(self, url: str, depth: int, client: httpx.Client) -> None:",
          "39:         self._client = client",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "46:         Creates a node for a tree using the given ID which corresponds to a URL.",
          "47:         If the parent_id is None, this will be considered a root node.",
          "48:         \"\"\"",
          "50:         soup = BeautifulSoup(resp.text, 'html.parser')",
          "51:         title = soup.title.text.strip() if soup.title is not None else parse_hostname(id)",
          "52:         try:",
          "",
          "[Removed Lines]",
          "49:         resp = httpx.get(id, timeout=60, proxies='socks5://127.0.0.1:9050')",
          "",
          "[Added Lines]",
          "50:         resp = self._client.get(id)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "64:         \"\"\"",
          "65:         if depth > 0:",
          "66:             depth -= 1",
          "68:             children = parse_links(resp.text)",
          "69:             for child in children:",
          "70:                 self._append_node(id=child, parent_id=url)",
          "",
          "[Removed Lines]",
          "67:             resp = httpx.get(url, timeout=60, proxies='socks5://127.0.0.1:9050')",
          "",
          "[Added Lines]",
          "68:             resp = self._client.get(url)",
          "",
          "---------------"
        ],
        "torbot/modules/tests/test_pagereader.py||torbot/modules/tests/test_pagereader.py": [
          "File: torbot/modules/tests/test_pagereader.py -> torbot/modules/tests/test_pagereader.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "d727061f73859219b9b5da7c1f8feeb7a4cc98b4",
      "candidate_info": {
        "commit_hash": "d727061f73859219b9b5da7c1f8feeb7a4cc98b4",
        "repo": "DedSecInside/TorBot",
        "commit_url": "https://github.com/DedSecInside/TorBot/commit/d727061f73859219b9b5da7c1f8feeb7a4cc98b4",
        "files": [
          "torbot/modules/linktree.py"
        ],
        "message": "Remove LinkTree class and use treelib strcuture for hosting nodes",
        "before_after_code_files": [
          "torbot/modules/linktree.py||torbot/modules/linktree.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/DedSecInside/TorBot/pull/307"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "torbot/modules/linktree.py||torbot/modules/linktree.py": [
          "File: torbot/modules/linktree.py -> torbot/modules/linktree.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "2: Module is used for analyzing link relationships",
          "3: \"\"\"",
          "4: import os",
          "43:     \"\"\"",
          "48:     \"\"\"",
          "",
          "[Removed Lines]",
          "6: from treelib import Tree, exceptions",
          "8: from .api import get_node",
          "9: from .config import get_data_directory",
          "10: from .log import debug",
          "13: def formatNode(n):",
          "14:     return f\"{n['url']} {n['status_code']} {n['status']}\"",
          "17: def build_tree_recursive(t, n):",
          "19:     # this will only be ran on the root node since others will exist before being passed",
          "20:     parent_id = n[\"url\"]",
          "21:     if not t.contains(parent_id):",
          "22:         debug(f\"adding id {parent_id}\")",
          "23:         t.create_node(formatNode(n), parent_id)",
          "25:     # if there are no children, there's nothing to process",
          "26:     children = n[\"children\"]",
          "27:     if not children:",
          "28:         return",
          "30:     for child in children:",
          "31:         try:",
          "32:             child_id = child[\"url\"]",
          "33:             debug(f\"adding child_id {child_id} to parent_id {parent_id}\")",
          "34:             t.create_node(formatNode(child), child_id, parent=parent_id)",
          "35:         except exceptions.DuplicatedNodeIdError:",
          "36:             debug(f\"found a duplicate url {child_id}\")",
          "37:             continue  # this node has already been processed somewhere else",
          "39:         build_tree_recursive(t, child)",
          "42: class LinkTree:",
          "44:     This is a class that represents a tree of links within TorBot. This can",
          "45:     be used to build a tree, examine the number of nodes, check if a node",
          "46:     exists within a tree, displaying the tree, and downloading the tree. It",
          "47:     will be expanded in the future to meet further needs.",
          "50:     def __init__(self, root: str, depth: int):",
          "51:         self.__build_tree(root, depth)",
          "53:     def __build_tree(self, url: str, depth: int = 1):",
          "54:         \"\"\"",
          "55:         Builds link tree by traversing through children nodes.",
          "57:         Returns:",
          "58:             tree (ete3.Tree): Built tree.",
          "59:         \"\"\"",
          "60:         debug(f\"building tree for {url} at {depth} depth\")",
          "61:         n = get_node(url, depth)",
          "62:         t = Tree()",
          "63:         build_tree_recursive(t, n)",
          "64:         self._tree = t",
          "65:         debug(\"tree built successfully\")",
          "67:     def save(self, file_name: str):",
          "68:         \"\"\"",
          "69:         Saves LinkTree to file with given file_name",
          "70:         Current file types supported are .txt",
          "71:         \"\"\"",
          "72:         print(f\"saving link tree as {file_name}\")",
          "73:         data_directory = get_data_directory()",
          "74:         file_path = os.path.join(data_directory, file_name)",
          "75:         try:",
          "76:             self._tree.save2file(file_path)",
          "77:         except Exception as e:",
          "78:             print(f\"failed to save link tree to {file_path}\")",
          "79:             debug(e)",
          "80:             raise e",
          "82:         print(f\"file saved successfully to {file_path}\")",
          "84:     def show(self):",
          "85:         \"\"\"",
          "86:         Displays image of LinkTree",
          "87:         \"\"\"",
          "88:         self._tree.show()",
          "",
          "[Added Lines]",
          "5: import re",
          "6: import httpx",
          "7: import validators",
          "8: import logging",
          "10: from treelib import Tree, exceptions, Node",
          "11: from bs4 import BeautifulSoup",
          "13: from .nlp.main import classify",
          "15: class Link(Node):",
          "16:     def __init__(self, title: str, url: str, status: int, classification: str, accuracy: float):",
          "17:         self.identifier = url",
          "18:         self.tag = title",
          "19:         self.status = status",
          "20:         self.classification = classification",
          "21:         self.accuracy = accuracy",
          "24: def parse_links(html: str) -> list[str]:",
          "26:     Finds all anchor tags and parses the href attribute.",
          "28:     soup = BeautifulSoup(html, 'html.parser')",
          "29:     tags = soup.find_all('a')",
          "30:     return [tag['href'] for tag in tags if tag.has_attr('href') and validators.url(tag['href'])]",
          "33: def append_node(tree: Tree, id: str, parent_id: str | None) -> None:",
          "34:     \"\"\"",
          "35:     Creates a node for a tree using the given ID which corresponds to a URL.",
          "36:     If the parent_id is None, this will be considered a root node.",
          "37:     \"\"\"",
          "38:     resp = httpx.get(id)",
          "39:     soup = BeautifulSoup(resp.text, 'html.parser')",
          "40:     title = soup.title.text.strip() if soup.title is not None else id",
          "41:     try:",
          "42:         [classification, accuracy] = classify(resp.text)",
          "43:         data = Link(title, id, resp.status_code, classification, accuracy)",
          "44:         tree.create_node(title, identifier=id, parent=parent_id, data=data)",
          "45:     except exceptions.DuplicatedNodeIdError:",
          "46:         logging.debug(f\"found a duplicate URL {id}\")",
          "49: def build_tree(tree: Tree, url: str, depth: int) -> None:",
          "50:     \"\"\"",
          "51:     Builds a tree from the root to the given depth.",
          "52:     \"\"\"",
          "53:     if depth > 0:",
          "54:         depth -= 1",
          "55:         resp = httpx.get(url)",
          "56:         children = parse_links(resp.text)",
          "57:         for child in children:",
          "58:             append_node(tree, id=child, parent_id=url)",
          "59:             build_tree(tree, child, depth)",
          "62: def save(tree: Tree, file_name: str) -> None:",
          "63:     \"\"\"",
          "64:     Saves the tree to the current working directory under the given file name.",
          "65:     \"\"\"",
          "66:     tree.save2file(os.path.join(os.getcwd(), file_name))",
          "69: def show(tree: Tree) -> None:",
          "70:     \"\"\"",
          "71:     Prints the tree",
          "72:     \"\"\"",
          "73:     tree.show()",
          "",
          "---------------"
        ]
      }
    }
  ]
}