{
  "cve_id": "CVE-2022-40604",
  "cve_desc": "In Apache Airflow 2.3.0 through 2.3.4, part of a url was unnecessarily formatted, allowing for possible information extraction.",
  "repo": "apache/airflow",
  "patch_hash": "6f24836e5ee56c452947aa87f84a21dd4f8eb87c",
  "patch_info": {
    "commit_hash": "6f24836e5ee56c452947aa87f84a21dd4f8eb87c",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/6f24836e5ee56c452947aa87f84a21dd4f8eb87c",
    "files": [
      "airflow/utils/log/file_task_handler.py"
    ],
    "message": "Properly build URL to retrieve logs independently from system (#26337)\n\nThe previous way of building the path depended on the OS path\nbut it was really used to build the URL so we should use\nurllib instead of os.path.join.\n\n(cherry picked from commit 18386026c28939fa6d91d198c5489c295a05dcd2)",
    "before_after_code_files": [
      "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py"
    ]
  },
  "patch_diff": {
    "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py": [
      "File: airflow/utils/log/file_task_handler.py -> airflow/utils/log/file_task_handler.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "21: import warnings",
      "22: from pathlib import Path",
      "23: from typing import TYPE_CHECKING, Optional",
      "25: from airflow.configuration import AirflowConfigException, conf",
      "26: from airflow.exceptions import RemovedInAirflow3Warning",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "24: from urllib.parse import urljoin",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "194:         else:",
      "195:             import httpx",
      "199:             )",
      "200:             log += f\"*** Log file does not exist: {location}\\n\"",
      "201:             log += f\"*** Fetching from: {url}\\n\"",
      "",
      "[Removed Lines]",
      "197:             url = os.path.join(\"http://{ti.hostname}:{worker_log_server_port}/log\", log_relative_path).format(",
      "198:                 ti=ti, worker_log_server_port=conf.get('logging', 'WORKER_LOG_SERVER_PORT')",
      "",
      "[Added Lines]",
      "198:             url = urljoin(",
      "199:                 f\"http://{ti.hostname}:{conf.get('logging', 'WORKER_LOG_SERVER_PORT')}/log\", log_relative_path",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "047f9d9349dd70ea5df398a89c56dd8cf31437da",
      "candidate_info": {
        "commit_hash": "047f9d9349dd70ea5df398a89c56dd8cf31437da",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/047f9d9349dd70ea5df398a89c56dd8cf31437da",
        "files": [
          "scripts/in_container/run_prepare_airflow_packages.sh"
        ],
        "message": "Skip overriding version from parameter with the one from tag (#26244)\n\nSince we are not always updating the version suffix in code\n(rc1/rc2/b1/b2), the tag specified via --tag-build prefix should\noverride the one in code rathe than the other way round.\n\nSo what's left now - we will just print warning if the suffix does\nnot match.\n\n(cherry picked from commit 9cf6f6a63d923e01fc7a3a634a0fa75f43c4c198)",
        "before_after_code_files": [
          "scripts/in_container/run_prepare_airflow_packages.sh||scripts/in_container/run_prepare_airflow_packages.sh"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/26371"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "scripts/in_container/run_prepare_airflow_packages.sh||scripts/in_container/run_prepare_airflow_packages.sh": [
          "File: scripts/in_container/run_prepare_airflow_packages.sh -> scripts/in_container/run_prepare_airflow_packages.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "54:             tag_build=('egg_info' '--tag-build' \"${VERSION_SUFFIX_FOR_PYPI}\")",
          "55:         else",
          "56:             if [[ ${AIRFLOW_VERSION} != *${VERSION_SUFFIX_FOR_PYPI} ]]; then",
          "69:             fi",
          "70:         fi",
          "71:     fi",
          "",
          "[Removed Lines]",
          "57:                 if [[ $AIRFLOW_VERSION =~ ^[0-9\\.]+([a-z0-9]*)$ ]]; then",
          "58:                     echo",
          "59:                     echo \"${COLOR_YELLOW}The requested PyPI suffix ${VERSION_SUFFIX_FOR_PYPI} does not match the one in ${AIRFLOW_VERSION}. Overriding it with one from ${AIRFLOW_VERSION}.${COLOR_RESET}\"",
          "60:                     echo",
          "61:                     VERSION_SUFFIX_FOR_PYPI=\"${BASH_REMATCH[1]}\"",
          "62:                     export VERSION_SUFFIX_FOR_PYPI",
          "63:                 else",
          "64:                     echo",
          "65:                     echo \"${COLOR_RED}The ${AIRFLOW_VERSION} does not follow the right version pattern 'X.Y.Zsuffix'. Quitting.${COLOR_RESET}\"",
          "66:                     echo",
          "67:                     exit 1",
          "68:                 fi",
          "",
          "[Added Lines]",
          "57:                 echo",
          "58:                 echo \"${COLOR_YELLOW}The requested PyPI suffix ${VERSION_SUFFIX_FOR_PYPI} does not match the one in ${AIRFLOW_VERSION}. Overriding it with one from ${VERSION_SUFFIX_FOR_PYPI}.${COLOR_RESET}\"",
          "59:                 echo",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "9fd23bbf5425fa69fd395b785600f93f33b95d1c",
      "candidate_info": {
        "commit_hash": "9fd23bbf5425fa69fd395b785600f93f33b95d1c",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/9fd23bbf5425fa69fd395b785600f93f33b95d1c",
        "files": [
          "airflow/__init__.py"
        ],
        "message": "Fix 'from airflow import version' lazy import (#26239)\n\nThis incorrectly accesses the version *attribute* in airflow.version\ninstead of the airflow.version module itself, breaking compatibility.\n\nI actually rewrote the entire __getattr__ hook; much of the logic seems\nto be quite redundant.\n\n(cherry picked from commit b7a603cf89728e02a187409c83983d58cc554457)",
        "before_after_code_files": [
          "airflow/__init__.py||airflow/__init__.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/26371"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/__init__.py||airflow/__init__.py": [
          "File: airflow/__init__.py -> airflow/__init__.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "26: isort:skip_file",
          "27: \"\"\"",
          "30: # flake8: noqa: F401",
          "32: import os",
          "33: import sys",
          "36: from airflow import settings",
          "",
          "[Removed Lines]",
          "34: from typing import Callable, Optional",
          "",
          "[Added Lines]",
          "31: from __future__ import annotations",
          "35: from typing import Callable",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "47: if not os.environ.get(\"_AIRFLOW__AS_LIBRARY\", None):",
          "48:     settings.initialize()",
          "52: PY36 = sys.version_info >= (3, 6)",
          "53: PY37 = sys.version_info >= (3, 7)",
          "",
          "[Removed Lines]",
          "50: login: Optional[Callable] = None",
          "",
          "[Added Lines]",
          "51: login: Callable | None = None",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "55: PY39 = sys.version_info >= (3, 9)",
          "56: PY310 = sys.version_info >= (3, 10)",
          "65: }",
          "68: def __getattr__(name: str):",
          "69:     # PEP-562: Lazy loaded attributes on python modules",
          "78:         raise AttributeError(f\"module {__name__!r} has no attribute {name!r}\")",
          "88:     # Store for next time",
          "89:     globals()[name] = val",
          "90:     return val",
          "",
          "[Removed Lines]",
          "58: # Things to lazy import in form 'name': 'path.to.module'",
          "59: __lazy_imports = {",
          "60:     'DAG': 'airflow.models.dag',",
          "61:     'Dataset': 'airflow.datasets',",
          "62:     'XComArg': 'airflow.models.xcom_arg',",
          "63:     'AirflowException': 'airflow.exceptions',",
          "64:     'version': 'airflow.version',",
          "70:     module_attr = name.rsplit('.', 1)[-1]",
          "71:     path: Optional[str]",
          "72:     if name == '__version__':",
          "73:         module_attr = 'version'",
          "74:         path = 'airflow.version'",
          "75:     else:",
          "76:         path = __lazy_imports.get(name)",
          "77:     if not path:",
          "80:     import operator",
          "82:     # Strip off the \"airflow.\" prefix because of how `__import__` works (it always returns the top level",
          "83:     # module)",
          "84:     without_prefix = path.split('.', 1)[-1]",
          "86:     getter = operator.attrgetter(f'{without_prefix}.{module_attr}')",
          "87:     val = getter(__import__(path))",
          "",
          "[Added Lines]",
          "59: # Things to lazy import in form {local_name: ('target_module', 'target_name')}",
          "60: __lazy_imports: dict[str, tuple[str, str]] = {",
          "61:     'DAG': ('.models.dag', 'DAG'),",
          "62:     'Dataset': ('.datasets', 'Dataset'),",
          "63:     'XComArg': ('.models.xcom_arg', 'XComArg'),",
          "64:     'AirflowException': ('.exceptions', 'AirflowException'),",
          "65:     'version': ('.version', ''),",
          "66:     '__version__': ('.version', 'version'),",
          "72:     module_path, attr_name = __lazy_imports.get(name, ('', ''))",
          "73:     if not module_path:",
          "76:     import importlib",
          "78:     mod = importlib.import_module(module_path, __name__)",
          "79:     if attr_name:",
          "80:         val = getattr(mod, attr_name)",
          "81:     else:",
          "82:         val = mod",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "38d3d4f0f11a88260e4a849a8bed5755b5490878",
      "candidate_info": {
        "commit_hash": "38d3d4f0f11a88260e4a849a8bed5755b5490878",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/38d3d4f0f11a88260e4a849a8bed5755b5490878",
        "files": [
          "airflow/cli/cli_parser.py",
          "tests/cli/commands/test_dag_command.py"
        ],
        "message": "Require dag_id arg for dags list-runs (#26357)\n\nWhile it would ideal to transition to a positional arg like was\nattempted in #25978, this unfortunately does result in a breaking change\nso we cannot do it now.\n\nBy instead marking the existing arg as required, we maintain backcompat\nwhile also providing a helpful error message to the user if they forget\nit.\n\nThis reverts commit ed6ea72f181a1d381cc1ff6c801f10cc0bc0d830.\n\n(cherry picked from commit f6c579c1c0efb8cdd2eaf905909cda7bc7314f88)",
        "before_after_code_files": [
          "airflow/cli/cli_parser.py||airflow/cli/cli_parser.py",
          "tests/cli/commands/test_dag_command.py||tests/cli/commands/test_dag_command.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/26371"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/cli/cli_parser.py||airflow/cli/cli_parser.py": [
          "File: airflow/cli/cli_parser.py -> airflow/cli/cli_parser.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "263: )",
          "265: # list_dag_runs",
          "266: ARG_NO_BACKFILL = Arg(",
          "267:     (\"--no-backfill\",), help=\"filter all the backfill dagruns given the dag id\", action=\"store_true\"",
          "268: )",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "266: ARG_DAG_ID_REQ_FLAG = Arg(",
          "267:     (\"-d\", \"--dag-id\"), required=True, help=\"The id of the dag\"",
          "268: )  # TODO: convert this to a positional arg in Airflow 3",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1008:         ),",
          "1009:         func=lazy_load_command('airflow.cli.commands.dag_command.dag_list_dag_runs'),",
          "1010:         args=(",
          "1012:             ARG_NO_BACKFILL,",
          "1013:             ARG_STATE,",
          "1014:             ARG_OUTPUT,",
          "",
          "[Removed Lines]",
          "1011:             ARG_DAG_ID,",
          "",
          "[Added Lines]",
          "1014:             ARG_DAG_ID_REQ_FLAG,",
          "",
          "---------------"
        ],
        "tests/cli/commands/test_dag_command.py||tests/cli/commands/test_dag_command.py": [
          "File: tests/cli/commands/test_dag_command.py -> tests/cli/commands/test_dag_command.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "499:             [",
          "500:                 'dags',",
          "501:                 'list-runs',",
          "502:                 '--no-backfill',",
          "503:                 '--start-date',",
          "504:                 DEFAULT_DATE.isoformat(),",
          "505:                 '--end-date',",
          "506:                 timezone.make_aware(datetime.max).isoformat(),",
          "508:             ]",
          "509:         )",
          "510:         dag_command.dag_list_dag_runs(args)",
          "",
          "[Removed Lines]",
          "507:                 'example_bash_operator',",
          "",
          "[Added Lines]",
          "502:                 '--dag-id',",
          "503:                 'example_bash_operator',",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "1ea86de21886e977cab2fcda7f87b6ebc91b0e3f",
      "candidate_info": {
        "commit_hash": "1ea86de21886e977cab2fcda7f87b6ebc91b0e3f",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/1ea86de21886e977cab2fcda7f87b6ebc91b0e3f",
        "files": [
          ".pre-commit-config.yaml",
          "airflow/models/baseoperator.py",
          "airflow/providers/google/cloud/hooks/gcs.py"
        ],
        "message": "Work around pyupgrade edge cases (#26384)\n\n(cherry picked from commit 9444d9789bc88e1063d81d28e219446b2251c0e1)",
        "before_after_code_files": [
          "airflow/models/baseoperator.py||airflow/models/baseoperator.py",
          "airflow/providers/google/cloud/hooks/gcs.py||airflow/providers/google/cloud/hooks/gcs.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/26371"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/models/baseoperator.py||airflow/models/baseoperator.py": [
          "File: airflow/models/baseoperator.py -> airflow/models/baseoperator.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1738:         task.set_downstream(to_tasks)",
          "1741: @attr.s(auto_attribs=True)",
          "1742: class BaseOperatorLink(metaclass=ABCMeta):",
          "1743:     \"\"\"Abstract base class that defines how we get an operator link.\"\"\"",
          "1746:     \"\"\"",
          "1747:     This property will be used by Airflow Plugins to find the Operators to which you want",
          "1748:     to assign this Operator Link",
          "",
          "[Removed Lines]",
          "1745:     operators: ClassVar[List[Type[BaseOperator]]] = []",
          "",
          "[Added Lines]",
          "1741: # pyupgrade assumes all type annotations can be lazily evaluated, but this is",
          "1742: # not the case for attrs-decorated classes, since cattrs needs to evaluate the",
          "1743: # annotation expressions at runtime, and Python before 3.9.0 does not lazily",
          "1744: # evaluate those. Putting the expression in a top-level assignment statement",
          "1745: # communicates this runtime requirement to pyupgrade.",
          "1746: BaseOperatorClassList = List[Type[BaseOperator]]",
          "1753:     operators: ClassVar[BaseOperatorClassList] = []",
          "",
          "---------------"
        ],
        "airflow/providers/google/cloud/hooks/gcs.py||airflow/providers/google/cloud/hooks/gcs.py": [
          "File: airflow/providers/google/cloud/hooks/gcs.py -> airflow/providers/google/cloud/hooks/gcs.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "15: # KIND, either express or implied.  See the License for the",
          "16: # specific language governing permissions and limitations",
          "17: # under the License.",
          "19: \"\"\"This module contains a Google Cloud Storage hook.\"\"\"",
          "20: import functools",
          "21: import gzip as gz",
          "22: import os",
          "",
          "[Removed Lines]",
          "18: #",
          "",
          "[Added Lines]",
          "19: from __future__ import annotations",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "28: from io import BytesIO",
          "29: from os import path",
          "30: from tempfile import NamedTemporaryFile",
          "45: from urllib.parse import urlparse",
          "47: from google.api_core.exceptions import NotFound",
          "",
          "[Removed Lines]",
          "31: from typing import (",
          "32:     IO,",
          "33:     Callable,",
          "34:     Generator,",
          "35:     List,",
          "36:     Optional,",
          "37:     Sequence,",
          "38:     Set,",
          "39:     Tuple,",
          "40:     TypeVar,",
          "41:     Union,",
          "42:     cast,",
          "43:     overload,",
          "44: )",
          "",
          "[Added Lines]",
          "32: from typing import IO, Callable, Generator, Sequence, TypeVar, cast, overload",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "60: RT = TypeVar('RT')",
          "61: T = TypeVar(\"T\", bound=Callable)",
          "63: # Use default timeout from google-cloud-storage",
          "64: DEFAULT_TIMEOUT = 60",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "51: # GCSHook has a method named 'list' (to junior devs: please don't do this), so",
          "52: # we need to create an alias to prevent Mypy being confused.",
          "53: List = list",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "81:     def _wrapper(func: T):",
          "82:         @functools.wraps(func)",
          "84:             if args:",
          "85:                 raise AirflowException(",
          "86:                     \"You must use keyword arguments in this methods rather than positional\"",
          "",
          "[Removed Lines]",
          "83:         def _inner_wrapper(self: \"GCSHook\", *args, **kwargs) -> RT:",
          "",
          "[Added Lines]",
          "75:         def _inner_wrapper(self: GCSHook, *args, **kwargs) -> RT:",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "139:     connection.",
          "140:     \"\"\"",
          "144:     def __init__(",
          "145:         self,",
          "146:         gcp_conn_id: str = \"google_cloud_default\",",
          "149:     ) -> None:",
          "150:         super().__init__(",
          "151:             gcp_conn_id=gcp_conn_id,",
          "",
          "[Removed Lines]",
          "142:     _conn = None  # type: Optional[storage.Client]",
          "147:         delegate_to: Optional[str] = None,",
          "148:         impersonation_chain: Optional[Union[str, Sequence[str]]] = None,",
          "",
          "[Added Lines]",
          "134:     _conn: storage.Client | None = None",
          "139:         delegate_to: str | None = None,",
          "140:         impersonation_chain: str | Sequence[str] | None = None,",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "166:         self,",
          "167:         source_bucket: str,",
          "168:         source_object: str,",
          "171:     ) -> None:",
          "172:         \"\"\"",
          "173:         Copies an object from a bucket to another, with renaming if requested.",
          "",
          "[Removed Lines]",
          "169:         destination_bucket: Optional[str] = None,",
          "170:         destination_object: Optional[str] = None,",
          "",
          "[Added Lines]",
          "161:         destination_bucket: str | None = None,",
          "162:         destination_object: str | None = None,",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "215:         source_bucket: str,",
          "216:         source_object: str,",
          "217:         destination_bucket: str,",
          "219:     ) -> None:",
          "220:         \"\"\"",
          "221:         Has the same functionality as copy, except that will work on files",
          "",
          "[Removed Lines]",
          "218:         destination_object: Optional[str] = None,",
          "",
          "[Added Lines]",
          "210:         destination_object: str | None = None,",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "270:         bucket_name: str,",
          "271:         object_name: str,",
          "272:         filename: None = None,",
          "276:     ) -> bytes:",
          "277:         ...",
          "",
          "[Removed Lines]",
          "273:         chunk_size: Optional[int] = None,",
          "274:         timeout: Optional[int] = DEFAULT_TIMEOUT,",
          "275:         num_max_attempts: Optional[int] = 1,",
          "",
          "[Added Lines]",
          "265:         chunk_size: int | None = None,",
          "266:         timeout: int | None = DEFAULT_TIMEOUT,",
          "267:         num_max_attempts: int | None = 1,",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "282:         bucket_name: str,",
          "283:         object_name: str,",
          "284:         filename: str,",
          "288:     ) -> str:",
          "289:         ...",
          "",
          "[Removed Lines]",
          "285:         chunk_size: Optional[int] = None,",
          "286:         timeout: Optional[int] = DEFAULT_TIMEOUT,",
          "287:         num_max_attempts: Optional[int] = 1,",
          "",
          "[Added Lines]",
          "277:         chunk_size: int | None = None,",
          "278:         timeout: int | None = DEFAULT_TIMEOUT,",
          "279:         num_max_attempts: int | None = 1,",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "292:         self,",
          "293:         bucket_name: str,",
          "294:         object_name: str,",
          "300:         \"\"\"",
          "301:         Downloads a file from Google Cloud Storage.",
          "",
          "[Removed Lines]",
          "295:         filename: Optional[str] = None,",
          "296:         chunk_size: Optional[int] = None,",
          "297:         timeout: Optional[int] = DEFAULT_TIMEOUT,",
          "298:         num_max_attempts: Optional[int] = 1,",
          "299:     ) -> Union[str, bytes]:",
          "",
          "[Added Lines]",
          "287:         filename: str | None = None,",
          "288:         chunk_size: int | None = None,",
          "289:         timeout: int | None = DEFAULT_TIMEOUT,",
          "290:         num_max_attempts: int | None = 1,",
          "291:     ) -> str | bytes:",
          "",
          "---------------",
          "--- Hunk 11 ---",
          "[Context before]",
          "351:         self,",
          "352:         bucket_name: str,",
          "353:         object_name: str,",
          "357:     ) -> bytes:",
          "358:         \"\"\"",
          "359:         Downloads a file from Google Cloud Storage.",
          "",
          "[Removed Lines]",
          "354:         chunk_size: Optional[int] = None,",
          "355:         timeout: Optional[int] = DEFAULT_TIMEOUT,",
          "356:         num_max_attempts: Optional[int] = 1,",
          "",
          "[Added Lines]",
          "346:         chunk_size: int | None = None,",
          "347:         timeout: int | None = DEFAULT_TIMEOUT,",
          "348:         num_max_attempts: int | None = 1,",
          "",
          "---------------",
          "--- Hunk 12 ---",
          "[Context before]",
          "383:     def provide_file(",
          "384:         self,",
          "385:         bucket_name: str = PROVIDE_BUCKET,",
          "389:     ) -> Generator[IO[bytes], None, None]:",
          "390:         \"\"\"",
          "391:         Downloads the file to a temporary directory and returns a file handle",
          "",
          "[Removed Lines]",
          "386:         object_name: Optional[str] = None,",
          "387:         object_url: Optional[str] = None,",
          "388:         dir: Optional[str] = None,",
          "",
          "[Added Lines]",
          "378:         object_name: str | None = None,",
          "379:         object_url: str | None = None,",
          "380:         dir: str | None = None,",
          "",
          "---------------",
          "--- Hunk 13 ---",
          "[Context before]",
          "412:     def provide_file_and_upload(",
          "413:         self,",
          "414:         bucket_name: str = PROVIDE_BUCKET,",
          "417:     ) -> Generator[IO[bytes], None, None]:",
          "418:         \"\"\"",
          "419:         Creates temporary file, returns a file handle and uploads the files content",
          "",
          "[Removed Lines]",
          "415:         object_name: Optional[str] = None,",
          "416:         object_url: Optional[str] = None,",
          "",
          "[Added Lines]",
          "407:         object_name: str | None = None,",
          "408:         object_url: str | None = None,",
          "",
          "---------------",
          "--- Hunk 14 ---",
          "[Context before]",
          "440:         self,",
          "441:         bucket_name: str,",
          "442:         object_name: str,",
          "446:         gzip: bool = False,",
          "447:         encoding: str = 'utf-8',",
          "450:         num_max_attempts: int = 1,",
          "452:     ) -> None:",
          "453:         \"\"\"",
          "454:         Uploads a local file or file data as string or bytes to Google Cloud Storage.",
          "",
          "[Removed Lines]",
          "443:         filename: Optional[str] = None,",
          "444:         data: Optional[Union[str, bytes]] = None,",
          "445:         mime_type: Optional[str] = None,",
          "448:         chunk_size: Optional[int] = None,",
          "449:         timeout: Optional[int] = DEFAULT_TIMEOUT,",
          "451:         metadata: Optional[dict] = None,",
          "",
          "[Added Lines]",
          "435:         filename: str | None = None,",
          "436:         data: str | bytes | None = None,",
          "437:         mime_type: str | None = None,",
          "440:         chunk_size: int | None = None,",
          "441:         timeout: int | None = DEFAULT_TIMEOUT,",
          "443:         metadata: dict | None = None,",
          "",
          "---------------",
          "--- Hunk 15 ---",
          "[Context before]",
          "683:         except NotFound:",
          "684:             self.log.info(\"Bucket %s not exists\", bucket_name)",
          "687:         \"\"\"",
          "688:         List all objects from the bucket with the give string prefix in name",
          "",
          "[Removed Lines]",
          "686:     def list(self, bucket_name, versions=None, max_results=None, prefix=None, delimiter=None) -> list:",
          "",
          "[Added Lines]",
          "678:     def list(self, bucket_name, versions=None, max_results=None, prefix=None, delimiter=None) -> List:",
          "",
          "---------------",
          "--- Hunk 16 ---",
          "[Context before]",
          "730:         bucket_name: str,",
          "731:         timespan_start: datetime,",
          "732:         timespan_end: datetime,",
          "737:     ) -> List[str]:",
          "738:         \"\"\"",
          "739:         List all objects from the bucket with the give string prefix in name that were",
          "",
          "[Removed Lines]",
          "733:         versions: Optional[bool] = None,",
          "734:         max_results: Optional[int] = None,",
          "735:         prefix: Optional[str] = None,",
          "736:         delimiter: Optional[str] = None,",
          "",
          "[Added Lines]",
          "725:         versions: bool | None = None,",
          "726:         max_results: int | None = None,",
          "727:         prefix: str | None = None,",
          "728:         delimiter: str | None = None,",
          "",
          "---------------",
          "--- Hunk 17 ---",
          "[Context before]",
          "838:     def create_bucket(",
          "839:         self,",
          "840:         bucket_name: str,",
          "842:         storage_class: str = 'MULTI_REGIONAL',",
          "843:         location: str = 'US',",
          "846:     ) -> str:",
          "847:         \"\"\"",
          "848:         Creates a new bucket. Google Cloud Storage uses a flat namespace, so",
          "",
          "[Removed Lines]",
          "841:         resource: Optional[dict] = None,",
          "844:         project_id: Optional[str] = None,",
          "845:         labels: Optional[dict] = None,",
          "",
          "[Added Lines]",
          "833:         resource: dict | None = None,",
          "836:         project_id: str | None = None,",
          "837:         labels: dict | None = None,",
          "",
          "---------------",
          "--- Hunk 18 ---",
          "[Context before]",
          "900:         return bucket.id",
          "902:     def insert_bucket_acl(",
          "904:     ) -> None:",
          "905:         \"\"\"",
          "906:         Creates a new ACL entry on the specified bucket_name.",
          "",
          "[Removed Lines]",
          "903:         self, bucket_name: str, entity: str, role: str, user_project: Optional[str] = None",
          "",
          "[Added Lines]",
          "895:         self, bucket_name: str, entity: str, role: str, user_project: str | None = None",
          "",
          "---------------",
          "--- Hunk 19 ---",
          "[Context before]",
          "933:         object_name: str,",
          "934:         entity: str,",
          "935:         role: str,",
          "938:     ) -> None:",
          "939:         \"\"\"",
          "940:         Creates a new ACL entry on the specified object.",
          "",
          "[Removed Lines]",
          "936:         generation: Optional[int] = None,",
          "937:         user_project: Optional[str] = None,",
          "",
          "[Added Lines]",
          "928:         generation: int | None = None,",
          "929:         user_project: str | None = None,",
          "",
          "---------------",
          "--- Hunk 20 ---",
          "[Context before]",
          "968:         self.log.info('A new ACL entry created for object: %s in bucket: %s', object_name, bucket_name)",
          "971:         \"\"\"",
          "972:         Composes a list of existing object into a new object in the same storage bucket_name",
          "",
          "[Removed Lines]",
          "970:     def compose(self, bucket_name: str, source_objects: List, destination_object: str) -> None:",
          "",
          "[Added Lines]",
          "962:     def compose(self, bucket_name: str, source_objects: List[str], destination_object: str) -> None:",
          "",
          "---------------",
          "--- Hunk 21 ---",
          "[Context before]",
          "1002:         self,",
          "1003:         source_bucket: str,",
          "1004:         destination_bucket: str,",
          "1007:         recursive: bool = True,",
          "1008:         allow_overwrite: bool = False,",
          "1009:         delete_extra_files: bool = False,",
          "",
          "[Removed Lines]",
          "1005:         source_object: Optional[str] = None,",
          "1006:         destination_object: Optional[str] = None,",
          "",
          "[Added Lines]",
          "997:         source_object: str | None = None,",
          "998:         destination_object: str | None = None,",
          "",
          "---------------",
          "--- Hunk 22 ---",
          "[Context before]",
          "1104:         self.log.info(\"Synchronization finished.\")",
          "1106:     def _calculate_sync_destination_path(",
          "1108:     ) -> str:",
          "1109:         return (",
          "1110:             path.join(destination_object, blob.name[source_object_prefix_len:])",
          "",
          "[Removed Lines]",
          "1107:         self, blob: storage.Blob, destination_object: Optional[str], source_object_prefix_len: int",
          "",
          "[Added Lines]",
          "1099:         self, blob: storage.Blob, destination_object: str | None, source_object_prefix_len: int",
          "",
          "---------------",
          "--- Hunk 23 ---",
          "[Context before]",
          "1116:     def _prepare_sync_plan(",
          "1117:         source_bucket: storage.Bucket,",
          "1118:         destination_bucket: storage.Bucket,",
          "1121:         recursive: bool,",
          "1123:         # Calculate the number of characters that remove from the name, because they contain information",
          "1124:         # about the parent's path",
          "1125:         source_object_prefix_len = len(source_object) if source_object else 0",
          "",
          "[Removed Lines]",
          "1119:         source_object: Optional[str],",
          "1120:         destination_object: Optional[str],",
          "1122:     ) -> Tuple[Set[storage.Blob], Set[storage.Blob], Set[storage.Blob]]:",
          "",
          "[Added Lines]",
          "1111:         source_object: str | None,",
          "1112:         destination_object: str | None,",
          "1114:     ) -> tuple[set[storage.Blob], set[storage.Blob], set[storage.Blob]]:",
          "",
          "---------------",
          "--- Hunk 24 ---",
          "[Context before]",
          "1139:         # Determine objects to copy and delete",
          "1140:         to_copy = source_names - destination_names",
          "1141:         to_delete = destination_names - source_names",
          "1144:         # Find names that are in both buckets",
          "1145:         names_to_check = source_names.intersection(destination_names)",
          "1147:         # Compare objects based on crc32",
          "1148:         for current_name in names_to_check:",
          "1149:             source_blob = source_names_index[current_name]",
          "",
          "[Removed Lines]",
          "1142:         to_copy_blobs = {source_names_index[a] for a in to_copy}  # type: Set[storage.Blob]",
          "1143:         to_delete_blobs = {destination_names_index[a] for a in to_delete}  # type: Set[storage.Blob]",
          "1146:         to_rewrite_blobs = set()  # type: Set[storage.Blob]",
          "",
          "[Added Lines]",
          "1134:         to_copy_blobs: set[storage.Blob] = {source_names_index[a] for a in to_copy}",
          "1135:         to_delete_blobs: set[storage.Blob] = {destination_names_index[a] for a in to_delete}",
          "1138:         to_rewrite_blobs: set[storage.Blob] = set()",
          "",
          "---------------",
          "--- Hunk 25 ---",
          "[Context before]",
          "1164:     return len(blob) == 0 or blob.endswith('/')",
          "1168:     \"\"\"",
          "1169:     Given a Google Cloud Storage URL (gs://<bucket>/<blob>), returns a",
          "1170:     tuple containing the corresponding bucket and blob.",
          "",
          "[Removed Lines]",
          "1167: def _parse_gcs_url(gsurl: str) -> Tuple[str, str]:",
          "",
          "[Added Lines]",
          "1159: def _parse_gcs_url(gsurl: str) -> tuple[str, str]:",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "e6177a0270fdb73a42bb966b35e52012faecc800",
      "candidate_info": {
        "commit_hash": "e6177a0270fdb73a42bb966b35e52012faecc800",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/e6177a0270fdb73a42bb966b35e52012faecc800",
        "files": [
          "airflow/serialization/serialized_objects.py",
          "tests/serialization/test_dag_serialization.py"
        ],
        "message": "Handle list when serializing expand_kwargs (#26369)\n\n(cherry picked from commit b816a6b243d16da87ca00e443619c75e9f6f5816)",
        "before_after_code_files": [
          "airflow/serialization/serialized_objects.py||airflow/serialization/serialized_objects.py",
          "tests/serialization/test_dag_serialization.py||tests/serialization/test_dag_serialization.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/26371"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/serialization/serialized_objects.py||airflow/serialization/serialized_objects.py": [
          "File: airflow/serialization/serialized_objects.py -> airflow/serialization/serialized_objects.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "17: \"\"\"Serialized DAG and BaseOperator\"\"\"",
          "18: from __future__ import annotations",
          "20: import datetime",
          "21: import enum",
          "22: import logging",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "20: import collections.abc",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "24: import weakref",
          "25: from dataclasses import dataclass",
          "26: from inspect import Parameter, signature",
          "29: import cattr",
          "30: import lazy_object_proxy",
          "",
          "[Removed Lines]",
          "27: from typing import TYPE_CHECKING, Any, Iterable, NamedTuple, Type",
          "",
          "[Added Lines]",
          "28: from typing import TYPE_CHECKING, Any, Collection, Iterable, Mapping, NamedTuple, Type, Union",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "207:         return deserialize_xcom_arg(self.data, dag)",
          "210: class _ExpandInputRef(NamedTuple):",
          "211:     \"\"\"Used to store info needed to create a mapped operator's expand input.",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "211: # These two should be kept in sync. Note that these are intentionally not using",
          "212: # the type declarations in expandinput.py so we always remember to update",
          "213: # serialization logic when adding new ExpandInput variants. If you add things to",
          "214: # the unions, be sure to update _ExpandInputRef to match.",
          "215: _ExpandInputOriginalValue = Union[",
          "216:     # For .expand(**kwargs).",
          "217:     Mapping[str, Any],",
          "218:     # For expand_kwargs(arg).",
          "219:     XComArg,",
          "220:     Collection[Union[XComArg, Mapping[str, Any]]],",
          "221: ]",
          "222: _ExpandInputSerializedValue = Union[",
          "223:     # For .expand(**kwargs).",
          "224:     Mapping[str, Any],",
          "225:     # For expand_kwargs(arg).",
          "226:     _XComRef,",
          "227:     Collection[Union[_XComRef, Mapping[str, Any]]],",
          "228: ]",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "215:     \"\"\"",
          "217:     key: str",
          "220:     def deref(self, dag: DAG) -> ExpandInput:",
          "221:         if isinstance(self.value, _XComRef):",
          "222:             value: Any = self.value.deref(dag)",
          "224:             value = {k: v.deref(dag) if isinstance(v, _XComRef) else v for k, v in self.value.items()}",
          "225:         return create_expand_input(self.key, value)",
          "",
          "[Removed Lines]",
          "218:     value: _XComRef | dict[str, Any]",
          "223:         else:",
          "",
          "[Added Lines]",
          "239:     value: _ExpandInputSerializedValue",
          "241:     @classmethod",
          "242:     def validate_expand_input_value(cls, value: _ExpandInputOriginalValue) -> None:",
          "243:         \"\"\"Validate we've covered all ``ExpandInput.value`` types.",
          "245:         This function does not actually do anything, but is called during",
          "246:         serialization so Mypy will *statically* check we have handled all",
          "247:         possible ExpandInput cases.",
          "248:         \"\"\"",
          "251:         \"\"\"De-reference into a concrete ExpandInput object.",
          "253:         If you add more cases here, be sure to update _ExpandInputOriginalValue",
          "254:         and _ExpandInputSerializedValue to match the logic.",
          "255:         \"\"\"",
          "258:         elif isinstance(self.value, collections.abc.Mapping):",
          "260:         else:",
          "261:             value = [v.deref(dag) if isinstance(v, _XComRef) else v for v in self.value]",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "663:         serialized_op = cls._serialize_node(op, include_deps=op.deps != MappedOperator.deps_for(BaseOperator))",
          "664:         # Handle expand_input and op_kwargs_expand_input.",
          "665:         expansion_kwargs = op._get_specified_expand_input()",
          "666:         serialized_op[op._expand_input_attr] = {",
          "667:             \"type\": get_map_type_key(expansion_kwargs),",
          "668:             \"value\": cls.serialize(expansion_kwargs.value),",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "703:         if TYPE_CHECKING:  # Let Mypy check the input type for us!",
          "704:             _ExpandInputRef.validate_expand_input_value(expansion_kwargs.value)",
          "",
          "---------------"
        ],
        "tests/serialization/test_dag_serialization.py||tests/serialization/test_dag_serialization.py": [
          "File: tests/serialization/test_dag_serialization.py -> tests/serialization/test_dag_serialization.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1963: @pytest.mark.parametrize(\"strict\", [True, False])",
          "1965:     from airflow.models.xcom_arg import PlainXComArg, XComArg",
          "1966:     from airflow.serialization.serialized_objects import _XComRef",
          "",
          "[Removed Lines]",
          "1964: def test_operator_expand_kwargs_serde(strict):",
          "",
          "[Added Lines]",
          "1964: def test_operator_expand_kwargs_literal_serde(strict):",
          "1965:     from airflow.models.xcom_arg import PlainXComArg, XComArg",
          "1966:     from airflow.serialization.serialized_objects import _XComRef",
          "1968:     with DAG(\"test-dag\", start_date=datetime(2020, 1, 1)) as dag:",
          "1969:         task1 = BaseOperator(task_id=\"op1\")",
          "1970:         mapped = MockOperator.partial(task_id='task_2').expand_kwargs(",
          "1971:             [{\"a\": \"x\"}, {\"a\": XComArg(task1)}],",
          "1972:             strict=strict,",
          "1973:         )",
          "1975:     serialized = SerializedBaseOperator.serialize(mapped)",
          "1976:     assert serialized == {",
          "1977:         '_is_empty': False,",
          "1978:         '_is_mapped': True,",
          "1979:         '_task_module': 'tests.test_utils.mock_operators',",
          "1980:         '_task_type': 'MockOperator',",
          "1981:         'downstream_task_ids': [],",
          "1982:         'expand_input': {",
          "1983:             \"type\": \"list-of-dicts\",",
          "1984:             \"value\": [",
          "1985:                 {\"__type\": \"dict\", \"__var\": {\"a\": \"x\"}},",
          "1986:                 {",
          "1987:                     \"__type\": \"dict\",",
          "1988:                     \"__var\": {\"a\": {'__type': 'xcomref', '__var': {'task_id': 'op1', 'key': 'return_value'}}},",
          "1989:                 },",
          "1990:             ],",
          "1991:         },",
          "1992:         'partial_kwargs': {},",
          "1993:         'task_id': 'task_2',",
          "1994:         'template_fields': ['arg1', 'arg2'],",
          "1995:         'template_ext': [],",
          "1996:         'template_fields_renderers': {},",
          "1997:         'operator_extra_links': [],",
          "1998:         'ui_color': '#fff',",
          "1999:         'ui_fgcolor': '#000',",
          "2000:         \"_disallow_kwargs_override\": strict,",
          "2001:         '_expand_input_attr': 'expand_input',",
          "2002:     }",
          "2004:     op = SerializedBaseOperator.deserialize_operator(serialized)",
          "2005:     assert op.deps is MappedOperator.deps_for(BaseOperator)",
          "2006:     assert op._disallow_kwargs_override == strict",
          "2008:     # The XComArg can't be deserialized before the DAG is.",
          "2009:     expand_value = op.expand_input.value",
          "2010:     assert expand_value == [{\"a\": \"x\"}, {\"a\": _XComRef({\"task_id\": \"op1\", \"key\": XCOM_RETURN_KEY})}]",
          "2012:     serialized_dag: DAG = SerializedDAG.from_dict(SerializedDAG.to_dict(dag))",
          "2014:     resolved_expand_value = serialized_dag.task_dict['task_2'].expand_input.value",
          "2015:     resolved_expand_value == [{\"a\": \"x\"}, {\"a\": PlainXComArg(serialized_dag.task_dict['op1'])}]",
          "2018: @pytest.mark.parametrize(\"strict\", [True, False])",
          "2019: def test_operator_expand_kwargs_xcomarg_serde(strict):",
          "",
          "---------------"
        ]
      }
    }
  ]
}