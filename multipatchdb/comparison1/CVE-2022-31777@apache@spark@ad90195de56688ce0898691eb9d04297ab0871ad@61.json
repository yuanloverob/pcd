{
  "cve_id": "CVE-2022-31777",
  "cve_desc": "A stored cross-site scripting (XSS) vulnerability in Apache Spark 3.2.1 and earlier, and 3.3.0, allows remote attackers to execute arbitrary JavaScript in the web browser of a user, by including a malicious payload into the logs which would be returned in logs rendered in the UI.",
  "repo": "apache/spark",
  "patch_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
  "patch_info": {
    "commit_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
    "repo": "apache/spark",
    "commit_url": "https://github.com/apache/spark/commit/ad90195de56688ce0898691eb9d04297ab0871ad",
    "files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ],
    "message": "[SPARK-39505][UI] Escape log content rendered in UI\n\n### What changes were proposed in this pull request?\n\nEscape log content rendered to the UI.\n\n### Why are the changes needed?\n\nLog content may contain reserved characters or other code in the log and be misinterpreted in the UI as HTML.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nExisting tests\n\nCloses #36902 from srowen/LogViewEscape.\n\nAuthored-by: Sean Owen <srowen@gmail.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
    "before_after_code_files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ]
  },
  "patch_diff": {
    "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js": [
      "File: core/src/main/resources/org/apache/spark/ui/static/log-view.js -> core/src/main/resources/org/apache/spark/ui/static/log-view.js",
      "--- Hunk 1 ---",
      "[Context before]",
      "85:       if (retStartByte == 0) {",
      "86:         disableMoreButton();",
      "87:       }",
      "90:       curLogLength = curLogLength + (startByte - retStartByte);",
      "91:       startByte = retStartByte;",
      "",
      "[Removed Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(cleanData);",
      "",
      "[Added Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(document.createTextNode(cleanData));",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "115:             var retLogLength = dataInfo[2];",
      "117:             var cleanData = data.substring(newlineIndex + 1);",
      "120:             curLogLength = curLogLength + (retEndByte - retStartByte);",
      "121:             endByte = retEndByte;",
      "",
      "[Removed Lines]",
      "118:             $(\"pre\", \".log-content\").append(cleanData);",
      "",
      "[Added Lines]",
      "118:             $(\"pre\", \".log-content\").append(document.createTextNode(cleanData));",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "2fe16015cdd701f395693b4e6bfa72cd101a8b8c",
      "candidate_info": {
        "commit_hash": "2fe16015cdd701f395693b4e6bfa72cd101a8b8c",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/2fe16015cdd701f395693b4e6bfa72cd101a8b8c",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/SubquerySuite.scala"
        ],
        "message": "[SPARK-39672][SQL][3.1] Fix removing project before filter with correlated subquery\n\nAdd more checks to`removeProjectBeforeFilter` in `ColumnPruning` and only remove the project if\n1. the filter condition contains correlated subquery\n2. same attribute exists in both output of child of Project and subquery\n\nThis is a legitimate self-join query and should not throw exception when de-duplicating attributes in subquery and outer values.\n\n```sql\nselect * from\n(\nselect v1.a, v1.b, v2.c\nfrom v1\ninner join v2\non v1.a=v2.a) t3\nwhere not exists (\n  select 1\n  from v2\n  where t3.a=v2.a and t3.b=v2.b and t3.c=v2.c\n)\n```\n\nHere's what happens with the current code. The above query is analyzed into following `LogicalPlan` before `ColumnPruning`.\n```\nProject [a#250, b#251, c#268]\n+- Filter NOT exists#272 [(a#250 = a#266) && (b#251 = b#267) && (c#268 = c#268#277)]\n   :  +- Project [1 AS 1#273, _1#259 AS a#266, _2#260 AS b#267, _3#261 AS c#268#277]\n   :     +- LocalRelation [_1#259, _2#260, _3#261]\n   +- Project [a#250, b#251, c#268]\n      +- Join Inner, (a#250 = a#266)\n         :- Project [a#250, b#251]\n         :  +- Project [_1#243 AS a#250, _2#244 AS b#251]\n         :     +- LocalRelation [_1#243, _2#244, _3#245]\n         +- Project [a#266, c#268]\n            +- Project [_1#259 AS a#266, _3#261 AS c#268]\n               +- LocalRelation [_1#259, _2#260, _3#261]\n```\n\nThen in `ColumnPruning`, the Project before Filter (between Filter and Join) is removed. This changes the `outputSet` of the child of Filter among which the same attribute also exists in the subquery. Later, when `RewritePredicateSubquery` de-duplicates conflicting attributes, it would complain `Found conflicting attributes a#266 in the condition joining outer plan`.\n\nNo.\n\nAdd UT.\n\nCloses #37074 from manuzhang/spark-39672.\n\nLead-authored-by: tianlzhang <tianlzhang@ebay.com>\nCo-authored-by: Manu Zhang <OwenZhang1990@gmail.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\n(cherry picked from commit 36fc73e7c42b84e05b15b2caecc0f804610dce20)\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/SubquerySuite.scala||sql/core/src/test/scala/org/apache/spark/sql/SubquerySuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "923:   private def removeProjectBeforeFilter(plan: LogicalPlan): LogicalPlan = plan transformUp {",
          "925:       if p2.outputSet.subsetOf(child.outputSet) &&",
          "928:       p1.copy(child = f.copy(child = child))",
          "929:   }",
          "930: }",
          "",
          "[Removed Lines]",
          "924:     case p1 @ Project(_, f @ Filter(_, p2 @ Project(_, child)))",
          "927:         p2.projectList.forall(_.isInstanceOf[AttributeReference]) =>",
          "",
          "[Added Lines]",
          "924:     case p1 @ Project(_, f @ Filter(e, p2 @ Project(_, child)))",
          "927:         p2.projectList.forall(_.isInstanceOf[AttributeReference]) &&",
          "930:         !hasConflictingAttrsWithSubquery(e, child) =>",
          "934:   private def hasConflictingAttrsWithSubquery(",
          "935:       predicate: Expression,",
          "936:       child: LogicalPlan): Boolean = {",
          "937:     predicate.find {",
          "938:       case s: SubqueryExpression if s.plan.outputSet.intersect(child.outputSet).nonEmpty => true",
          "939:       case _ => false",
          "940:     }.isDefined",
          "941:   }",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/SubquerySuite.scala||sql/core/src/test/scala/org/apache/spark/sql/SubquerySuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/SubquerySuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/SubquerySuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: import scala.collection.mutable.ArrayBuffer",
          "22: import org.apache.spark.sql.catalyst.expressions.SubqueryExpression",
          "24: import org.apache.spark.sql.execution.{ColumnarToRowExec, ExecSubqueryExpression, FileSourceScanExec, InputAdapter, ReusedSubqueryExec, ScalarSubquery, SubqueryExec, WholeStageCodegenExec}",
          "25: import org.apache.spark.sql.execution.adaptive.{AdaptiveSparkPlanHelper, DisableAdaptiveExecution}",
          "26: import org.apache.spark.sql.execution.datasources.FileScanRDD",
          "",
          "[Removed Lines]",
          "23: import org.apache.spark.sql.catalyst.plans.logical.{Join, LogicalPlan, Sort}",
          "",
          "[Added Lines]",
          "23: import org.apache.spark.sql.catalyst.plans.logical.{Join, LogicalPlan, Project, Sort}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "2213:             |\"\"\".stripMargin),",
          "2214:       Row(\"2022-06-01\"))",
          "2215:   }",
          "2216: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "2217:   test(\"SPARK-39672: Fix removing project before filter with correlated subquery\") {",
          "2218:     withTempView(\"v1\", \"v2\") {",
          "2219:       Seq((1, 2, 3), (4, 5, 6)).toDF(\"a\", \"b\", \"c\").createTempView(\"v1\")",
          "2220:       Seq((1, 3, 5), (4, 5, 6)).toDF(\"a\", \"b\", \"c\").createTempView(\"v2\")",
          "2222:       def findProject(df: DataFrame): Seq[Project] = {",
          "2223:         df.queryExecution.optimizedPlan.collect {",
          "2224:           case p: Project => p",
          "2225:         }",
          "2226:       }",
          "2230:       val df1 = sql(",
          "2231:         \"\"\"",
          "2232:          |select * from",
          "2233:          |(",
          "2234:          |select",
          "2235:          |v1.a,",
          "2236:          |v1.b,",
          "2237:          |v2.c",
          "2238:          |from v1",
          "2239:          |inner join v2",
          "2240:          |on v1.a=v2.a) t3",
          "2241:          |where not exists (",
          "2242:          |  select 1",
          "2243:          |  from v2",
          "2244:          |  where t3.a=v2.a and t3.b=v2.b and t3.c=v2.c",
          "2245:          |)",
          "2246:          |\"\"\".stripMargin)",
          "2247:       checkAnswer(df1, Row(1, 2, 5))",
          "2248:       assert(findProject(df1).size == 4)",
          "2251:       val df2 = sql(",
          "2252:         \"\"\"",
          "2253:          |select * from",
          "2254:          |(",
          "2255:          |select",
          "2256:          |v1.b,",
          "2257:          |v2.c",
          "2258:          |from v1",
          "2259:          |inner join v2",
          "2260:          |on v1.b=v2.c) t3",
          "2261:          |where not exists (",
          "2262:          |  select 1",
          "2263:          |  from v2",
          "2264:          |  where t3.b=v2.b and t3.c=v2.c",
          "2265:          |)",
          "2266:          |\"\"\".stripMargin)",
          "2268:       checkAnswer(df2, Row(5, 5))",
          "2269:       assert(findProject(df2).size == 3)",
          "2270:     }",
          "2271:   }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "9601be96a86eced683e6aa2b772c726eeb231de8",
      "candidate_info": {
        "commit_hash": "9601be96a86eced683e6aa2b772c726eeb231de8",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/9601be96a86eced683e6aa2b772c726eeb231de8",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala"
        ],
        "message": "[SPARK-39887][SQL][FOLLOW-UP] Do not exclude Union's first child attributes when traversing other children in RemoveRedundantAliases\n\n### What changes were proposed in this pull request?\nDo not exclude `Union`'s first child attributes when traversing other children in `RemoveRedundantAliases`.\n\n### Why are the changes needed?\nWe don't need to exclude those attributes that `Union` inherits from its first child. See discussion here: https://github.com/apache/spark/pull/37496#discussion_r944509115\n\n### Does this PR introduce _any_ user-facing change?\nNo.\n\n### How was this patch tested?\nExisting UTs.\n\nCloses #37534 from peter-toth/SPARK-39887-keep-attributes-of-unions-first-child-follow-up.\n\nAuthored-by: Peter Toth <ptoth@cloudera.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\n(cherry picked from commit e732232dac420826af269d8cf5efacb52933f59a)\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "544:         })",
          "545:         Join(newLeft, newRight, joinType, newCondition, hint)",
          "548:         var first = true",
          "549:         plan.mapChildren { child =>",
          "550:           if (first) {",
          "",
          "[Removed Lines]",
          "547:       case _: Union =>",
          "",
          "[Added Lines]",
          "547:       case u: Union =>",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "556:             removeRedundantAliases(child, excluded ++ child.outputSet)",
          "557:           } else {",
          "559:           }",
          "560:         }",
          "",
          "[Removed Lines]",
          "558:             removeRedundantAliases(child, excluded)",
          "",
          "[Added Lines]",
          "559:             removeRedundantAliases(child, excluded -- u.children.head.outputSet)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "bd3f36f6626f0fb71ab0ceb9bbe7fa4d05c628f5",
      "candidate_info": {
        "commit_hash": "bd3f36f6626f0fb71ab0ceb9bbe7fa4d05c628f5",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/bd3f36f6626f0fb71ab0ceb9bbe7fa4d05c628f5",
        "files": [
          "sql/core/src/main/scala/org/apache/spark/sql/execution/python/AggregateInPandasExec.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/python/PythonUDFSuite.scala"
        ],
        "message": "[SPARK-39962][PYTHON][SQL] Apply projection when group attributes are empty\n\n### What changes were proposed in this pull request?\n\nThis PR proposes to apply the projection to respect the reordered columns in its child when group attributes are empty.\n\n### Why are the changes needed?\n\nTo respect the column order in the child.\n\n### Does this PR introduce _any_ user-facing change?\n\nYes, it fixes a bug as below:\n\n```python\nimport pandas as pd\nfrom pyspark.sql import functions as f\n\nf.pandas_udf(\"double\")\ndef AVG(x: pd.Series) -> float:\n    return x.mean()\n\nabc = spark.createDataFrame([(1.0, 5.0, 17.0)], schema=[\"a\", \"b\", \"c\"])\nabc.agg(AVG(\"a\"), AVG(\"c\")).show()\nabc.select(\"c\", \"a\").agg(AVG(\"a\"), AVG(\"c\")).show()\n```\n\n**Before**\n\n```\n+------+------+\n|AVG(a)|AVG(c)|\n+------+------+\n|  17.0|   1.0|\n+------+------+\n```\n\n**After**\n\n```\n+------+------+\n|AVG(a)|AVG(c)|\n+------+------+\n|   1.0|  17.0|\n+------+------+\n```\n\n### How was this patch tested?\n\nManually tested, and added an unittest.\n\nCloses #37390 from HyukjinKwon/SPARK-39962.\n\nAuthored-by: Hyukjin Kwon <gurwls223@apache.org>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>\n(cherry picked from commit 5335c784ae76c9cc0aaa7a4b57b3cd6b3891ad9a)\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>",
        "before_after_code_files": [
          "sql/core/src/main/scala/org/apache/spark/sql/execution/python/AggregateInPandasExec.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/python/AggregateInPandasExec.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/python/PythonUDFSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/python/PythonUDFSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/core/src/main/scala/org/apache/spark/sql/execution/python/AggregateInPandasExec.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/python/AggregateInPandasExec.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/python/AggregateInPandasExec.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/python/AggregateInPandasExec.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "131:       val newIter: Iterator[InternalRow] = mayAppendUpdatingSessionIterator(iter)",
          "132:       val prunedProj = UnsafeProjection.create(allInputs.toSeq, child.output)",
          "136:         Iterator((new UnsafeRow(), newIter))",
          "137:       } else {",
          "138:         GroupedIterator(newIter, groupingExpressions, child.output)",
          "140:         (key, rows.map(prunedProj))",
          "141:       }",
          "",
          "[Removed Lines]",
          "134:       val grouped = if (groupingExpressions.isEmpty) {",
          "139:       }.map { case (key, rows) =>",
          "",
          "[Added Lines]",
          "134:       val groupedItr = if (groupingExpressions.isEmpty) {",
          "139:       }",
          "140:       val grouped = groupedItr.map { case (key, rows) =>",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/execution/python/PythonUDFSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/python/PythonUDFSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/execution/python/PythonUDFSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/execution/python/PythonUDFSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "71:         pythonTestUDF(count(pythonTestUDF(base(\"a\") + 1))))",
          "72:     checkAnswer(df1, df2)",
          "73:   }",
          "74: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "75:   test(\"SPARK-39962: Global aggregation of Pandas UDF should respect the column order\") {",
          "76:     assume(shouldTestGroupedAggPandasUDFs)",
          "77:     val df = Seq[(java.lang.Integer, java.lang.Integer)]((1, null)).toDF(\"a\", \"b\")",
          "79:     val pandasTestUDF = TestGroupedAggPandasUDF(name = \"pandas_udf\")",
          "80:     val reorderedDf = df.select(\"b\", \"a\")",
          "81:     val actual = reorderedDf.agg(",
          "82:       pandasTestUDF(reorderedDf(\"a\")), pandasTestUDF(reorderedDf(\"b\")))",
          "83:     val expected = df.agg(pandasTestUDF(df(\"a\")), pandasTestUDF(df(\"b\")))",
          "85:     checkAnswer(actual, expected)",
          "86:   }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "fb58c3e507113e2e9e398cb77703e54603bfa29a",
      "candidate_info": {
        "commit_hash": "fb58c3e507113e2e9e398cb77703e54603bfa29a",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/fb58c3e507113e2e9e398cb77703e54603bfa29a",
        "files": [
          "python/docs/source/reference/pyspark.sql.rst",
          "python/pyspark/sql/types.py"
        ],
        "message": "[SPARK-38828][PYTHON] Remove TimestampNTZ type Python support in Spark 3.3\n\nThis PR proposes to remove `TimestampNTZ` type Python support in Spark 3.3 from documentation and `pyspark.sql.types` module.\n\nThe purpose of this PR is just hide `TimestampNTZ` type from end-users.\n\nBecause the `TimestampNTZ` project is not finished yet:\n\n- Lack Hive metastore support\n- Lack JDBC support\n- Need to spend time scanning the codebase to find out any missing support. The current code usages of TimestampType are larger than TimestampNTZType\n\nNo.\n\nThe existing tests should cover.\n\nCloses #36255 from itholic/SPARK-38828.\n\nAuthored-by: itholic <haejoon.lee@databricks.com>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>\n(cherry picked from commit 581000de24377ca373df7fa94b214baa7e9b0462)\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>",
        "before_after_code_files": [
          "python/pyspark/sql/types.py||python/pyspark/sql/types.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "python/pyspark/sql/types.py||python/pyspark/sql/types.py": [
          "File: python/pyspark/sql/types.py -> python/pyspark/sql/types.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "59:     \"BooleanType\",",
          "60:     \"DateType\",",
          "61:     \"TimestampType\",",
          "63:     \"DecimalType\",",
          "64:     \"DoubleType\",",
          "65:     \"FloatType\",",
          "",
          "[Removed Lines]",
          "62:     \"TimestampNTZType\",",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "b27e8842c349bd8bd9937b30b153546060ec56bb",
      "candidate_info": {
        "commit_hash": "b27e8842c349bd8bd9937b30b153546060ec56bb",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/b27e8842c349bd8bd9937b30b153546060ec56bb",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/Sum.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala"
        ],
        "message": "[SPARK-38548][SQL][FOLLOWUP] try_sum: return null if overflow happens before merging\n\n### What changes were proposed in this pull request?\n\nThis PR is to fix a bug in the new function `try_sum`. It should return null if overflow happens before merging the sums from map tasks.\nFor example:\nMAP TASK 1: partial aggregation TRY_SUM(large_numbers_column) -> overflows, turns into NULL\nMAP TASK 2: partial aggregation TRY_SUM(large_numbers_column) -> succeeds, returns 12345\nREDUCE TASK: merge TRY_SUM(NULL, 12345) -> returns 12345\n\nWe should use a new slot buffer `isEmpty` to track if there is a non-empty value in partial aggregation. If the partial result is null and there is non-empty value, the merge result should be `NULL`.\n### Why are the changes needed?\n\nBug fix\n\n### Does this PR introduce _any_ user-facing change?\n\nNo, the new function is not release yet.\n\n### How was this patch tested?\n\nUT\n\nCloses #36097 from gengliangwang/fixTrySum.\n\nLead-authored-by: Gengliang Wang <gengliang@apache.org>\nCo-authored-by: Gengliang Wang <ltnwgl@gmail.com>\nSigned-off-by: Gengliang Wang <gengliang@apache.org>\n(cherry picked from commit 061127b67d2fbae0042505f1dabfad10eed4a782)\nSigned-off-by: Gengliang Wang <gengliang@apache.org>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/Sum.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/Sum.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala||sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/Sum.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/Sum.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/Sum.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/Sum.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "33:   def failOnError: Boolean",
          "35:   override def nullable: Boolean = true",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "35:   protected def shouldTrackIsEmpty: Boolean",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "46:   final override val nodePatterns: Seq[TreePattern] = Seq(SUM)",
          "49:     case DecimalType.Fixed(precision, scale) =>",
          "50:       DecimalType.bounded(precision + 10, scale)",
          "51:     case _: IntegralType => LongType",
          "",
          "[Removed Lines]",
          "48:   private lazy val resultType = child.dataType match {",
          "",
          "[Added Lines]",
          "50:   protected lazy val resultType = child.dataType match {",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "61:   private lazy val zero = Literal.default(resultType)",
          "66:   }",
          "103:     }",
          "104:   }",
          "",
          "[Removed Lines]",
          "63:   override lazy val aggBufferAttributes = resultType match {",
          "64:     case _: DecimalType => sum :: isEmpty :: Nil",
          "65:     case _ => sum :: Nil",
          "68:   override lazy val initialValues: Seq[Expression] = resultType match {",
          "69:     case _: DecimalType => Seq(zero, Literal(true, BooleanType))",
          "70:     case _ => Seq(Literal(null, resultType))",
          "71:   }",
          "73:   protected def getUpdateExpressions: Seq[Expression] = {",
          "74:     resultType match {",
          "75:       case _: DecimalType =>",
          "79:         val sumExpr = if (child.nullable) {",
          "80:           If(child.isNull, sum,",
          "81:             Add(sum, KnownNotNull(child).cast(resultType), failOnError = failOnError))",
          "82:         } else {",
          "83:           Add(sum, child.cast(resultType), failOnError = failOnError)",
          "84:         }",
          "86:         val isEmptyExpr = if (child.nullable) {",
          "87:           isEmpty && child.isNull",
          "88:         } else {",
          "89:           Literal(false, BooleanType)",
          "90:         }",
          "91:         Seq(sumExpr, isEmptyExpr)",
          "92:       case _ =>",
          "97:         if (child.nullable) {",
          "98:           Seq(coalesce(Add(coalesce(sum, zero), child.cast(resultType), failOnError = failOnError),",
          "99:             sum))",
          "100:         } else {",
          "101:           Seq(Add(coalesce(sum, zero), child.cast(resultType), failOnError = failOnError))",
          "102:         }",
          "",
          "[Added Lines]",
          "65:   override lazy val aggBufferAttributes = if (shouldTrackIsEmpty) {",
          "66:     sum :: isEmpty :: Nil",
          "67:   } else {",
          "68:     sum :: Nil",
          "71:   override lazy val initialValues: Seq[Expression] =",
          "72:     if (shouldTrackIsEmpty) {",
          "73:       Seq(zero, Literal(true, BooleanType))",
          "74:     } else {",
          "75:       Seq(Literal(null, resultType))",
          "76:     }",
          "78:   protected def getUpdateExpressions: Seq[Expression] = if (shouldTrackIsEmpty) {",
          "82:     val sumExpr = if (child.nullable) {",
          "83:       If(child.isNull, sum,",
          "84:         Add(sum, KnownNotNull(child).cast(resultType), failOnError = failOnError))",
          "85:     } else {",
          "86:       Add(sum, child.cast(resultType), failOnError = failOnError)",
          "87:     }",
          "89:     val isEmptyExpr = if (child.nullable) {",
          "90:       isEmpty && child.isNull",
          "91:     } else {",
          "92:       Literal(false, BooleanType)",
          "93:     }",
          "94:     Seq(sumExpr, isEmptyExpr)",
          "95:   } else {",
          "100:     if (child.nullable) {",
          "101:       Seq(coalesce(Add(coalesce(sum, zero), child.cast(resultType), failOnError = failOnError),",
          "102:         sum))",
          "103:     } else {",
          "104:       Seq(Add(coalesce(sum, zero), child.cast(resultType), failOnError = failOnError))",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "136:   }",
          "",
          "[Removed Lines]",
          "118:   protected def getMergeExpressions: Seq[Expression] = {",
          "119:     resultType match {",
          "120:       case _: DecimalType =>",
          "121:         val bufferOverflow = !isEmpty.left && sum.left.isNull",
          "122:         val inputOverflow = !isEmpty.right && sum.right.isNull",
          "123:         Seq(",
          "124:           If(",
          "125:             bufferOverflow || inputOverflow,",
          "126:             Literal.create(null, resultType),",
          "130:             KnownNotNull(sum.left) + KnownNotNull(sum.right)),",
          "131:           isEmpty.left && isEmpty.right)",
          "132:       case _ => Seq(coalesce(",
          "133:         Add(coalesce(sum.left, zero), sum.right, failOnError = failOnError),",
          "134:         sum.left))",
          "135:     }",
          "",
          "[Added Lines]",
          "120:   protected def getMergeExpressions: Seq[Expression] = if (shouldTrackIsEmpty) {",
          "121:     val bufferOverflow = !isEmpty.left && sum.left.isNull",
          "122:     val inputOverflow = !isEmpty.right && sum.right.isNull",
          "123:     Seq(",
          "124:       If(",
          "125:         bufferOverflow || inputOverflow,",
          "126:         Literal.create(null, resultType),",
          "130:         Add(KnownNotNull(sum.left), KnownNotNull(sum.right), failOnError)),",
          "131:       isEmpty.left && isEmpty.right)",
          "132:   } else {",
          "133:     Seq(coalesce(",
          "134:       Add(coalesce(sum.left, zero), sum.right, failOnError = failOnError),",
          "135:       sum.left))",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "146:     case d: DecimalType =>",
          "147:       If(isEmpty, Literal.create(null, resultType),",
          "148:         CheckOverflowInSum(sum, d, !failOnError))",
          "149:     case _ => sum",
          "150:   }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "149:     case _ if shouldTrackIsEmpty =>",
          "150:       If(isEmpty, Literal.create(null, resultType), sum)",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "172:   extends SumBase(child) {",
          "173:   def this(child: Expression) = this(child, failOnError = SQLConf.get.ansiEnabled)",
          "175:   override protected def withNewChildInternal(newChild: Expression): Sum = copy(child = newChild)",
          "177:   override lazy val updateExpressions: Seq[Expression] = getUpdateExpressions",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "177:   override def shouldTrackIsEmpty: Boolean = resultType match {",
          "178:     case _: DecimalType => true",
          "179:     case _ => false",
          "180:   }",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "208:     case _ => true",
          "209:   }",
          "211:   override lazy val updateExpressions: Seq[Expression] =",
          "212:     if (failOnError) {",
          "213:       val expressions = getUpdateExpressions",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "218:   override def shouldTrackIsEmpty: Boolean = resultType match {",
          "220:     case _: DecimalType | _: IntegralType | _: YearMonthIntervalType | _: DayTimeIntervalType =>",
          "221:       true",
          "222:     case _ =>",
          "223:       false",
          "224:   }",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala||sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "4328:         Row(3, 2, 6) :: Nil)",
          "4329:     }",
          "4330:   }",
          "4331: }",
          "4333: case class Foo(bar: Option[String])",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "4332:   test(\"SPARK-38548: try_sum should return null if overflow happens before merging\") {",
          "4333:     val longDf = Seq(Long.MaxValue, Long.MaxValue, 2).toDF(\"v\")",
          "4334:     val yearMonthDf = Seq(Int.MaxValue, Int.MaxValue, 2)",
          "4335:       .map(Period.ofMonths)",
          "4336:       .toDF(\"v\")",
          "4337:     val dayTimeDf = Seq(106751991L, 106751991L, 2L)",
          "4338:       .map(Duration.ofDays)",
          "4339:       .toDF(\"v\")",
          "4340:     Seq(longDf, yearMonthDf, dayTimeDf).foreach { df =>",
          "4341:       checkAnswer(df.repartitionByRange(2, col(\"v\")).selectExpr(\"try_sum(v)\"), Row(null))",
          "4342:     }",
          "4343:   }",
          "",
          "---------------"
        ]
      }
    }
  ]
}