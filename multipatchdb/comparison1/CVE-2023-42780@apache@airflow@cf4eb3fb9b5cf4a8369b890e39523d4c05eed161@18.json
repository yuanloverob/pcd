{
  "cve_id": "CVE-2023-42780",
  "cve_desc": "Apache Airflow, versions prior to 2.7.2, contains a security vulnerability that allows authenticated users of Airflow to list warnings for all DAGs, even if the user had no permission to see those DAGs. It would reveal the dag_ids and the stack-traces of import errors for those DAGs with import errors.\nUsers of Apache Airflow are advised to upgrade to version 2.7.2 or newer to mitigate the risk associated with this vulnerability.\n\n",
  "repo": "apache/airflow",
  "patch_hash": "cf4eb3fb9b5cf4a8369b890e39523d4c05eed161",
  "patch_info": {
    "commit_hash": "cf4eb3fb9b5cf4a8369b890e39523d4c05eed161",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/cf4eb3fb9b5cf4a8369b890e39523d4c05eed161",
    "files": [
      "airflow/api_connexion/endpoints/dag_warning_endpoint.py",
      "tests/api_connexion/endpoints/test_dag_warning_endpoint.py"
    ],
    "message": "Fix dag warning endpoint permissions (#34355)\n\n* Fix dag warning endpoint permissions\n\n* update the query to have an accurate result for total entries and pagination\n\n* add unit tests\n\n* Update test_dag_warning_endpoint.py\n\nCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>\n\n---------\n\nCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>\n(cherry picked from commit 3570bbfbea69e2965f91b9964ce28bc268c68129)",
    "before_after_code_files": [
      "airflow/api_connexion/endpoints/dag_warning_endpoint.py||airflow/api_connexion/endpoints/dag_warning_endpoint.py",
      "tests/api_connexion/endpoints/test_dag_warning_endpoint.py||tests/api_connexion/endpoints/test_dag_warning_endpoint.py"
    ]
  },
  "patch_diff": {
    "airflow/api_connexion/endpoints/dag_warning_endpoint.py||airflow/api_connexion/endpoints/dag_warning_endpoint.py": [
      "File: airflow/api_connexion/endpoints/dag_warning_endpoint.py -> airflow/api_connexion/endpoints/dag_warning_endpoint.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "16: # under the License.",
      "17: from __future__ import annotations",
      "19: from sqlalchemy import select",
      "20: from sqlalchemy.orm import Session",
      "22: from airflow.api_connexion import security",
      "23: from airflow.api_connexion.parameters import apply_sorting, check_limit, format_parameters",
      "24: from airflow.api_connexion.schemas.dag_warning_schema import (",
      "25:     DagWarningCollection,",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "19: from flask import g",
      "24: from airflow.api_connexion.exceptions import PermissionDenied",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "28: from airflow.api_connexion.types import APIResponse",
      "29: from airflow.models.dagwarning import DagWarning as DagWarningModel",
      "30: from airflow.security import permissions",
      "31: from airflow.utils.db import get_query_count",
      "32: from airflow.utils.session import NEW_SESSION, provide_session",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "33: from airflow.utils.airflow_flask_app import get_airflow_app",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "52:     allowed_filter_attrs = [\"dag_id\", \"warning_type\", \"message\", \"timestamp\"]",
      "53:     query = select(DagWarningModel)",
      "54:     if dag_id:",
      "55:         query = query.where(DagWarningModel.dag_id == dag_id)",
      "56:     if warning_type:",
      "57:         query = query.where(DagWarningModel.warning_type == warning_type)",
      "58:     total_entries = get_query_count(query, session=session)",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "58:         if not get_airflow_app().appbuilder.sm.can_read_dag(dag_id, g.user):",
      "59:             raise PermissionDenied(detail=f\"User not allowed to access this DAG: {dag_id}\")",
      "61:     else:",
      "62:         readable_dags = get_airflow_app().appbuilder.sm.get_accessible_dag_ids(g.user)",
      "63:         query = query.where(DagWarningModel.dag_id.in_(readable_dags))",
      "",
      "---------------"
    ],
    "tests/api_connexion/endpoints/test_dag_warning_endpoint.py||tests/api_connexion/endpoints/test_dag_warning_endpoint.py": [
      "File: tests/api_connexion/endpoints/test_dag_warning_endpoint.py -> tests/api_connexion/endpoints/test_dag_warning_endpoint.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "35:         app,  # type:ignore",
      "36:         username=\"test\",",
      "37:         role_name=\"Test\",",
      "39:     )",
      "40:     create_user(app, username=\"test_no_permissions\", role_name=\"TestNoPermissions\")  # type: ignore",
      "42:     yield minimal_app_for_api",
      "44:     delete_user(app, username=\"test\")  # type: ignore",
      "45:     delete_user(app, username=\"test_no_permissions\")  # type: ignore",
      "48: class TestBaseDagWarning:",
      "",
      "[Removed Lines]",
      "38:         permissions=[(permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG_WARNING)],  # type: ignore",
      "",
      "[Added Lines]",
      "38:         permissions=[",
      "39:             (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG_WARNING),",
      "40:             (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG),",
      "41:         ],  # type: ignore",
      "44:     create_user(",
      "45:         app,  # type:ignore",
      "46:         username=\"test_with_dag2_read\",",
      "47:         role_name=\"TestWithDag2Read\",",
      "48:         permissions=[",
      "49:             (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG_WARNING),",
      "50:             (permissions.ACTION_CAN_READ, f\"{permissions.RESOURCE_DAG_PREFIX}dag2\"),",
      "51:         ],  # type: ignore",
      "52:     )",
      "58:     delete_user(app, username=\"test_with_dag2_read\")  # type: ignore",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "147:             \"/api/v1/dagWarnings\", environ_overrides={\"REMOTE_USER\": \"test_no_permissions\"}",
      "148:         )",
      "149:         assert response.status_code == 403",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "164:     def test_should_raise_403_forbidden_when_user_has_no_dag_read_permission(self):",
      "165:         response = self.client.get(",
      "166:             \"/api/v1/dagWarnings\",",
      "167:             environ_overrides={\"REMOTE_USER\": \"test_with_dag2_read\"},",
      "168:             query_string={\"dag_id\": \"dag1\"},",
      "169:         )",
      "170:         assert response.status_code == 403",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "f04bca6f32e57bf6d97a4b57e7bd97b590d3fef1",
      "candidate_info": {
        "commit_hash": "f04bca6f32e57bf6d97a4b57e7bd97b590d3fef1",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/f04bca6f32e57bf6d97a4b57e7bd97b590d3fef1",
        "files": [
          "airflow/cli/cli_parser.py",
          "airflow/cli/commands/standalone_command.py",
          "airflow/providers/google/cloud/sensors/tasks.py",
          "airflow/providers/oracle/hooks/oracle.py",
          "tests/api_connexion/endpoints/test_user_endpoint.py",
          "tests/api_experimental/common/experimental/test_pool.py",
          "tests/operators/test_bash.py",
          "tests/providers/oracle/operators/test_oracle.py",
          "tests/sensors/test_external_task_sensor.py",
          "tests/utils/test_edgemodifier.py"
        ],
        "message": "Refactor: Use f-strings (#33734)\n\n(cherry picked from commit 94c82916294791df10d30a1cb444daf5d8a34304)",
        "before_after_code_files": [
          "airflow/cli/cli_parser.py||airflow/cli/cli_parser.py",
          "airflow/cli/commands/standalone_command.py||airflow/cli/commands/standalone_command.py",
          "airflow/providers/google/cloud/sensors/tasks.py||airflow/providers/google/cloud/sensors/tasks.py",
          "airflow/providers/oracle/hooks/oracle.py||airflow/providers/oracle/hooks/oracle.py",
          "tests/api_connexion/endpoints/test_user_endpoint.py||tests/api_connexion/endpoints/test_user_endpoint.py",
          "tests/api_experimental/common/experimental/test_pool.py||tests/api_experimental/common/experimental/test_pool.py",
          "tests/operators/test_bash.py||tests/operators/test_bash.py",
          "tests/providers/oracle/operators/test_oracle.py||tests/providers/oracle/operators/test_oracle.py",
          "tests/sensors/test_external_task_sensor.py||tests/sensors/test_external_task_sensor.py",
          "tests/utils/test_edgemodifier.py||tests/utils/test_edgemodifier.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/cli/cli_parser.py||airflow/cli/cli_parser.py": [
          "File: airflow/cli/cli_parser.py -> airflow/cli/cli_parser.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "81:             action_subcommands, group_subcommands = partition(",
          "82:                 lambda d: isinstance(ALL_COMMANDS_DICT[d.dest], GroupCommand), subactions",
          "83:             )",
          "85:             self._indent()",
          "86:             yield from group_subcommands",
          "87:             self._dedent()",
          "90:             self._indent()",
          "91:             yield from action_subcommands",
          "92:             self._dedent()",
          "",
          "[Removed Lines]",
          "84:             yield Action([], \"\\n%*s%s:\" % (self._current_indent, \"\", \"Groups\"), nargs=0)",
          "89:             yield Action([], \"\\n%*s%s:\" % (self._current_indent, \"\", \"Commands\"), nargs=0)",
          "",
          "[Added Lines]",
          "84:             yield Action([], f\"\\n{' ':{self._current_indent}}Groups\", nargs=0)",
          "89:             yield Action([], f\"\\n{' ':{self._current_indent}}Commands:\", nargs=0)",
          "",
          "---------------"
        ],
        "airflow/cli/commands/standalone_command.py||airflow/cli/commands/standalone_command.py": [
          "File: airflow/cli/commands/standalone_command.py -> airflow/cli/commands/standalone_command.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "139:             \"triggerer\": \"cyan\",",
          "140:             \"standalone\": \"white\",",
          "141:         }.get(name, \"white\")",
          "143:         for line in output.splitlines():",
          "144:             print(f\"{colorised_name} | {line.strip()}\")",
          "",
          "[Removed Lines]",
          "142:         colorised_name = colored(\"%10s\" % name, color)",
          "",
          "[Added Lines]",
          "142:         colorised_name = colored(f\"{name:10}\", color)",
          "",
          "---------------"
        ],
        "airflow/providers/google/cloud/sensors/tasks.py||airflow/providers/google/cloud/sensors/tasks.py": [
          "File: airflow/providers/google/cloud/sensors/tasks.py -> airflow/providers/google/cloud/sensors/tasks.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "83:             # page_size=1",
          "84:         )",
          "88:         return len(tasks) == 0",
          "",
          "[Removed Lines]",
          "86:         self.log.info(\"tasks exhausted in cloud task queue?: %s\" % (len(tasks) == 0))",
          "",
          "[Added Lines]",
          "86:         self.log.info(\"tasks exhausted in cloud task queue?: %s\", (len(tasks) == 0))",
          "",
          "---------------"
        ],
        "airflow/providers/oracle/hooks/oracle.py||airflow/providers/oracle/hooks/oracle.py": [
          "File: airflow/providers/oracle/hooks/oracle.py -> airflow/providers/oracle/hooks/oracle.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "347:         prepared_stm = \"insert into {tablename} {columns} values ({values})\".format(",
          "348:             tablename=table,",
          "349:             columns=\"({})\".format(\", \".join(target_fields)) if target_fields else \"\",",
          "351:         )",
          "352:         row_count = 0",
          "353:         # Chunk the rows",
          "",
          "[Removed Lines]",
          "350:             values=\", \".join(\":%s\" % i for i in range(1, len(values_base) + 1)),",
          "",
          "[Added Lines]",
          "350:             values=\", \".join(f\":{i}\" for i in range(1, len(values_base) + 1)),",
          "",
          "---------------"
        ],
        "tests/api_connexion/endpoints/test_user_endpoint.py||tests/api_connexion/endpoints/test_user_endpoint.py": [
          "File: tests/api_connexion/endpoints/test_user_endpoint.py -> tests/api_connexion/endpoints/test_user_endpoint.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "617:             environ_overrides={\"REMOTE_USER\": \"test\"},",
          "618:         )",
          "619:         assert response.status_code == 400, response.json",
          "622:     @pytest.mark.usefixtures(\"autoclean_admin_user\")",
          "623:     def test_username_can_be_updated(self, autoclean_user_payload, autoclean_username):",
          "",
          "[Removed Lines]",
          "620:         assert response.json[\"detail\"] == \"{'%s': ['Missing data for required field.']}\" % field",
          "",
          "[Added Lines]",
          "620:         assert response.json[\"detail\"] == f\"{{'{field}': ['Missing data for required field.']}}\"",
          "",
          "---------------"
        ],
        "tests/api_experimental/common/experimental/test_pool.py||tests/api_experimental/common/experimental/test_pool.py": [
          "File: tests/api_experimental/common/experimental/test_pool.py -> tests/api_experimental/common/experimental/test_pool.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "100:         long_name = \"\".join(random.choices(string.ascii_lowercase, k=300))",
          "101:         column_length = models.Pool.pool.property.columns[0].type.length",
          "102:         with pytest.raises(",
          "104:         ):",
          "105:             pool_api.create_pool(",
          "106:                 name=long_name,",
          "",
          "[Removed Lines]",
          "103:             AirflowBadRequest, match=\"^Pool name can't be more than %d characters$\" % column_length",
          "",
          "[Added Lines]",
          "103:             AirflowBadRequest, match=f\"^Pool name can't be more than {column_length} characters$\"",
          "",
          "---------------"
        ],
        "tests/operators/test_bash.py||tests/operators/test_bash.py": [
          "File: tests/operators/test_bash.py -> tests/operators/test_bash.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "204:     def test_bash_operator_kill(self, dag_maker):",
          "205:         import psutil",
          "208:         with dag_maker():",
          "209:             op = BashOperator(",
          "210:                 task_id=\"test_bash_operator_kill\",",
          "",
          "[Removed Lines]",
          "207:         sleep_time = \"100%d\" % os.getpid()",
          "",
          "[Added Lines]",
          "207:         sleep_time = f\"100{os.getpid()}\"",
          "",
          "---------------"
        ],
        "tests/providers/oracle/operators/test_oracle.py||tests/providers/oracle/operators/test_oracle.py": [
          "File: tests/providers/oracle/operators/test_oracle.py -> tests/providers/oracle/operators/test_oracle.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "90:         oracle_conn_id = \"oracle_default\"",
          "91:         parameters = {\"parameter\": \"value\"}",
          "92:         task_id = \"test_push\"",
          "94:         error = f\"ORA-{ora_exit_code}: This is a five-digit ORA error code\"",
          "95:         mock_callproc.side_effect = oracledb.DatabaseError(error)",
          "",
          "[Removed Lines]",
          "93:         ora_exit_code = \"%05d\" % randrange(10**5)",
          "",
          "[Added Lines]",
          "93:         ora_exit_code = f\"{randrange(10**5):05}\"",
          "",
          "---------------"
        ],
        "tests/sensors/test_external_task_sensor.py||tests/sensors/test_external_task_sensor.py": [
          "File: tests/sensors/test_external_task_sensor.py -> tests/sensors/test_external_task_sensor.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1344:             task_id=f\"{daily_task.task_id}_{i}\",",
          "1345:             external_dag_id=daily_dag.dag_id,",
          "1346:             external_task_id=daily_task.task_id,",
          "1348:             dag=agg_dag,",
          "1349:         )",
          "1350:         begin >> task",
          "",
          "[Removed Lines]",
          "1347:             execution_date=\"{{ macros.ds_add(ds, -1 * %s) }}\" % i,",
          "",
          "[Added Lines]",
          "1347:             execution_date=f\"{{{{ macros.ds_add(ds, -1 * {i}) }}}}\",",
          "",
          "---------------"
        ],
        "tests/utils/test_edgemodifier.py||tests/utils/test_edgemodifier.py": [
          "File: tests/utils/test_edgemodifier.py -> tests/utils/test_edgemodifier.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "45:         return f\"OP:{task_id}\"",
          "47:     with DAG(dag_id=\"test_xcom_dag\", default_args=DEFAULT_ARGS) as dag:",
          "49:         return dag, operators",
          "",
          "[Removed Lines]",
          "48:         operators = [PythonOperator(python_callable=f, task_id=\"test_op_%i\" % i) for i in range(4)]",
          "",
          "[Added Lines]",
          "48:         operators = [PythonOperator(python_callable=f, task_id=f\"test_op_{i}\") for i in range(4)]",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "30ca37e6037fc7fd4c0dd7e4a73ccca8608fbbcf",
      "candidate_info": {
        "commit_hash": "30ca37e6037fc7fd4c0dd7e4a73ccca8608fbbcf",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/30ca37e6037fc7fd4c0dd7e4a73ccca8608fbbcf",
        "files": [
          "airflow/api/common/trigger_dag.py",
          "airflow/api_connexion/schemas/common_schema.py",
          "airflow/api_connexion/schemas/enum_schemas.py",
          "airflow/cli/cli_config.py",
          "airflow/cli/commands/standalone_command.py",
          "airflow/configuration.py",
          "airflow/jobs/backfill_job_runner.py",
          "airflow/kubernetes/pre_7_4_0_compatibility/pod_generator.py",
          "airflow/models/dag.py",
          "airflow/utils/python_virtualenv.py",
          "airflow/utils/state.py",
          "airflow/www/utils.py"
        ],
        "message": "Replace sequence concatination by unpacking in Airflow core (#33934)\n\n(cherry picked from commit 33e5d0351d7d09033c57a4a0b851b9e1b50bfb01)",
        "before_after_code_files": [
          "airflow/api/common/trigger_dag.py||airflow/api/common/trigger_dag.py",
          "airflow/api_connexion/schemas/common_schema.py||airflow/api_connexion/schemas/common_schema.py",
          "airflow/api_connexion/schemas/enum_schemas.py||airflow/api_connexion/schemas/enum_schemas.py",
          "airflow/cli/cli_config.py||airflow/cli/cli_config.py",
          "airflow/cli/commands/standalone_command.py||airflow/cli/commands/standalone_command.py",
          "airflow/configuration.py||airflow/configuration.py",
          "airflow/jobs/backfill_job_runner.py||airflow/jobs/backfill_job_runner.py",
          "airflow/kubernetes/pre_7_4_0_compatibility/pod_generator.py||airflow/kubernetes/pre_7_4_0_compatibility/pod_generator.py",
          "airflow/models/dag.py||airflow/models/dag.py",
          "airflow/utils/python_virtualenv.py||airflow/utils/python_virtualenv.py",
          "airflow/utils/state.py||airflow/utils/state.py",
          "airflow/www/utils.py||airflow/www/utils.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/api/common/trigger_dag.py||airflow/api/common/trigger_dag.py": [
          "File: airflow/api/common/trigger_dag.py -> airflow/api/common/trigger_dag.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "82:         run_conf = conf if isinstance(conf, dict) else json.loads(conf)",
          "84:     dag_runs = []",
          "86:     for _dag in dags_to_run:",
          "87:         dag_run = _dag.create_dagrun(",
          "88:             run_id=run_id,",
          "",
          "[Removed Lines]",
          "85:     dags_to_run = [dag] + dag.subdags",
          "",
          "[Added Lines]",
          "85:     dags_to_run = [dag, *dag.subdags]",
          "",
          "---------------"
        ],
        "airflow/api_connexion/schemas/common_schema.py||airflow/api_connexion/schemas/common_schema.py": [
          "File: airflow/api_connexion/schemas/common_schema.py -> airflow/api_connexion/schemas/common_schema.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "136:     def __init__(self, **metadata):",
          "137:         super().__init__(**metadata)",
          "141: class WeightRuleField(fields.String):",
          "",
          "[Removed Lines]",
          "138:         self.validators = [validate.Regexp(\"^#[a-fA-F0-9]{3,6}$\")] + list(self.validators)",
          "",
          "[Added Lines]",
          "138:         self.validators = [validate.Regexp(\"^#[a-fA-F0-9]{3,6}$\"), *self.validators]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "144:     def __init__(self, **metadata):",
          "145:         super().__init__(**metadata)",
          "149: class TimezoneField(fields.String):",
          "",
          "[Removed Lines]",
          "146:         self.validators = [validate.OneOf(WeightRule.all_weight_rules())] + list(self.validators)",
          "",
          "[Added Lines]",
          "146:         self.validators = [validate.OneOf(WeightRule.all_weight_rules()), *self.validators]",
          "",
          "---------------"
        ],
        "airflow/api_connexion/schemas/enum_schemas.py||airflow/api_connexion/schemas/enum_schemas.py": [
          "File: airflow/api_connexion/schemas/enum_schemas.py -> airflow/api_connexion/schemas/enum_schemas.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "27:     def __init__(self, **metadata):",
          "28:         super().__init__(**metadata)",
          "32: class TaskInstanceStateField(fields.String):",
          "",
          "[Removed Lines]",
          "29:         self.validators = [validate.OneOf(State.dag_states)] + list(self.validators)",
          "",
          "[Added Lines]",
          "29:         self.validators = [validate.OneOf(State.dag_states), *self.validators]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "35:     def __init__(self, **metadata):",
          "36:         super().__init__(**metadata)",
          "",
          "[Removed Lines]",
          "37:         self.validators = [validate.OneOf(State.task_states)] + list(self.validators)",
          "",
          "[Added Lines]",
          "37:         self.validators = [validate.OneOf(State.task_states), *self.validators]",
          "",
          "---------------"
        ],
        "airflow/cli/cli_config.py||airflow/cli/cli_config.py": [
          "File: airflow/cli/cli_config.py -> airflow/cli/cli_config.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1676:         name=\"add\",",
          "1677:         help=\"Add a connection\",",
          "1678:         func=lazy_load_command(\"airflow.cli.commands.connection_command.connections_add\"),",
          "1680:     ),",
          "1681:     ActionCommand(",
          "1682:         name=\"delete\",",
          "",
          "[Removed Lines]",
          "1679:         args=(ARG_CONN_ID, ARG_CONN_URI, ARG_CONN_JSON, ARG_CONN_EXTRA) + tuple(ALTERNATIVE_CONN_SPECS_ARGS),",
          "",
          "[Added Lines]",
          "1679:         args=(ARG_CONN_ID, ARG_CONN_URI, ARG_CONN_JSON, ARG_CONN_EXTRA, *ALTERNATIVE_CONN_SPECS_ARGS),",
          "",
          "---------------"
        ],
        "airflow/cli/commands/standalone_command.py||airflow/cli/commands/standalone_command.py": [
          "File: airflow/cli/commands/standalone_command.py -> airflow/cli/commands/standalone_command.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "288:     def run(self):",
          "289:         \"\"\"Run the actual process and captures it output to a queue.\"\"\"",
          "290:         self.process = subprocess.Popen(",
          "292:             stdout=subprocess.PIPE,",
          "293:             stderr=subprocess.STDOUT,",
          "294:             env=self.env,",
          "",
          "[Removed Lines]",
          "291:             [\"airflow\"] + self.command,",
          "",
          "[Added Lines]",
          "291:             [\"airflow\", *self.command],",
          "",
          "---------------"
        ],
        "airflow/configuration.py||airflow/configuration.py": [
          "File: airflow/configuration.py -> airflow/configuration.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "465:         (\"logging\", \"logging_level\"): _available_logging_levels,",
          "466:         (\"logging\", \"fab_logging_level\"): _available_logging_levels,",
          "467:         # celery_logging_level can be empty, which uses logging_level as fallback",
          "469:         (\"webserver\", \"analytical_tool\"): [\"google_analytics\", \"metarouter\", \"segment\", \"\"],",
          "470:     }",
          "",
          "[Removed Lines]",
          "468:         (\"logging\", \"celery_logging_level\"): _available_logging_levels + [\"\"],",
          "",
          "[Added Lines]",
          "468:         (\"logging\", \"celery_logging_level\"): [*_available_logging_levels, \"\"],",
          "",
          "---------------"
        ],
        "airflow/jobs/backfill_job_runner.py||airflow/jobs/backfill_job_runner.py": [
          "File: airflow/jobs/backfill_job_runner.py -> airflow/jobs/backfill_job_runner.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "785:             yield tabulate_ti_keys_set([ti.key for ti in ti_status.deadlocked])",
          "787:     def _get_dag_with_subdags(self) -> list[DAG]:",
          "790:     @provide_session",
          "791:     def _execute_dagruns(",
          "",
          "[Removed Lines]",
          "788:         return [self.dag] + self.dag.subdags",
          "",
          "[Added Lines]",
          "788:         return [self.dag, *self.dag.subdags]",
          "",
          "---------------"
        ],
        "airflow/kubernetes/pre_7_4_0_compatibility/pod_generator.py||airflow/kubernetes/pre_7_4_0_compatibility/pod_generator.py": [
          "File: airflow/kubernetes/pre_7_4_0_compatibility/pod_generator.py -> airflow/kubernetes/pre_7_4_0_compatibility/pod_generator.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "363:         client_container = extend_object_field(base_container, client_container, \"volume_devices\")",
          "364:         client_container = merge_objects(base_container, client_container)",
          "370:     @classmethod",
          "371:     def construct_pod(",
          "",
          "[Removed Lines]",
          "366:         return [client_container] + PodGenerator.reconcile_containers(",
          "367:             base_containers[1:], client_containers[1:]",
          "368:         )",
          "",
          "[Added Lines]",
          "366:         return [",
          "367:             client_container,",
          "369:         ]",
          "",
          "---------------"
        ],
        "airflow/models/dag.py||airflow/models/dag.py": [
          "File: airflow/models/dag.py -> airflow/models/dag.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1706:         if include_subdags:",
          "1707:             # Crafting the right filter for dag_id and task_ids combo",
          "1708:             conditions = []",
          "1710:                 conditions.append(",
          "1711:                     (TaskInstance.dag_id == dag.dag_id) & TaskInstance.task_id.in_(dag.task_ids)",
          "1712:                 )",
          "",
          "[Removed Lines]",
          "1709:             for dag in self.subdags + [self]:",
          "",
          "[Added Lines]",
          "1709:             for dag in [*self.subdags, self]:",
          "",
          "---------------"
        ],
        "airflow/utils/python_virtualenv.py||airflow/utils/python_virtualenv.py": [
          "File: airflow/utils/python_virtualenv.py -> airflow/utils/python_virtualenv.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "40: def _generate_pip_install_cmd_from_file(",
          "41:     tmp_dir: str, requirements_file_path: str, pip_install_options: list[str]",
          "42: ) -> list[str]:",
          "47: def _generate_pip_install_cmd_from_list(",
          "48:     tmp_dir: str, requirements: list[str], pip_install_options: list[str]",
          "49: ) -> list[str]:",
          "54: def remove_task_decorator(python_source: str, task_decorator_name: str) -> str:",
          "",
          "[Removed Lines]",
          "43:     cmd = [f\"{tmp_dir}/bin/pip\", \"install\"] + pip_install_options + [\"-r\"]",
          "44:     return cmd + [requirements_file_path]",
          "50:     cmd = [f\"{tmp_dir}/bin/pip\", \"install\"] + pip_install_options",
          "51:     return cmd + requirements",
          "",
          "[Added Lines]",
          "43:     return [f\"{tmp_dir}/bin/pip\", \"install\", *pip_install_options, \"-r\", requirements_file_path]",
          "49:     return [f\"{tmp_dir}/bin/pip\", \"install\", *pip_install_options, *requirements]",
          "",
          "---------------"
        ],
        "airflow/utils/state.py||airflow/utils/state.py": [
          "File: airflow/utils/state.py -> airflow/utils/state.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "119:     finished_dr_states: frozenset[DagRunState] = frozenset([DagRunState.SUCCESS, DagRunState.FAILED])",
          "120:     unfinished_dr_states: frozenset[DagRunState] = frozenset([DagRunState.QUEUED, DagRunState.RUNNING])",
          "124:     dag_states: tuple[DagRunState, ...] = (",
          "125:         DagRunState.QUEUED,",
          "",
          "[Removed Lines]",
          "122:     task_states: tuple[TaskInstanceState | None, ...] = (None,) + tuple(TaskInstanceState)",
          "",
          "[Added Lines]",
          "122:     task_states: tuple[TaskInstanceState | None, ...] = (None, *TaskInstanceState)",
          "",
          "---------------"
        ],
        "airflow/www/utils.py||airflow/www/utils.py": [
          "File: airflow/www/utils.py -> airflow/www/utils.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "775:             \"is_extendedjson\",",
          "776:             [],",
          "777:         ),",
          "780:     def __init__(self, datamodel):",
          "781:         super().__init__(datamodel)",
          "",
          "[Removed Lines]",
          "778:     ) + fab_sqlafilters.SQLAFilterConverter.conversion_table",
          "",
          "[Added Lines]",
          "779:     )",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "877: # place",
          "878: FieldConverter.conversion_table = (",
          "879:     (\"is_utcdatetime\", DateTimeWithTimezoneField, AirflowDateTimePickerWidget),",
          "883: class UIAlert:",
          "",
          "[Removed Lines]",
          "880: ) + FieldConverter.conversion_table",
          "",
          "[Added Lines]",
          "882: )",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "7a496510777989ba3baf2b2faf4d80b7cd9fe1c0",
      "candidate_info": {
        "commit_hash": "7a496510777989ba3baf2b2faf4d80b7cd9fe1c0",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/7a496510777989ba3baf2b2faf4d80b7cd9fe1c0",
        "files": [
          "airflow/utils/db.py",
          "newsfragments/34348.bugfix.rst",
          "tests/utils/test_db.py"
        ],
        "message": "Fixed rows count in the migration script (#34348)\n\n* Fixed row count for SQLAlchemy 1.4+\n\n* Updated newsfragments\n\n* Fixed typo\n\n* Added newline\n\n* Added test for `check_bad_references`\n\n(cherry picked from commit f349fda125c2251ac4129c2c28fbf6f7dbb69294)",
        "before_after_code_files": [
          "airflow/utils/db.py||airflow/utils/db.py",
          "tests/utils/test_db.py||tests/utils/test_db.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/utils/db.py||airflow/utils/db.py": [
          "File: airflow/utils/db.py -> airflow/utils/db.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1447:         dangling_table_name = _format_airflow_moved_table_name(source_table.name, change_version, \"dangling\")",
          "1448:         if dangling_table_name in existing_table_names:",
          "1450:             if invalid_row_count:",
          "1451:                 yield _format_dangling_error(",
          "1452:                     source_table=source_table.name,",
          "",
          "[Removed Lines]",
          "1449:             invalid_row_count = bad_rows_query.count()",
          "",
          "[Added Lines]",
          "1449:             invalid_row_count = get_query_count(bad_rows_query, session=session)",
          "",
          "---------------"
        ],
        "tests/utils/test_db.py||tests/utils/test_db.py": [
          "File: tests/utils/test_db.py -> tests/utils/test_db.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "31: from alembic.migration import MigrationContext",
          "32: from alembic.runtime.environment import EnvironmentContext",
          "33: from alembic.script import ScriptDirectory",
          "36: from airflow.exceptions import AirflowException",
          "37: from airflow.models import Base as airflow_base",
          "38: from airflow.settings import engine",
          "39: from airflow.utils.db import (",
          "40:     _get_alembic_config,",
          "41:     check_migrations,",
          "42:     compare_server_default,",
          "43:     compare_type,",
          "",
          "[Removed Lines]",
          "34: from sqlalchemy import MetaData",
          "",
          "[Added Lines]",
          "34: from sqlalchemy import MetaData, Table",
          "35: from sqlalchemy.sql import Select",
          "42:     check_bad_references,",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "49:     resetdb,",
          "50:     upgradedb,",
          "51: )",
          "54: class TestDb:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "54: from airflow.utils.session import NEW_SESSION",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "58:         airflow.models.import_all_models()",
          "59:         all_meta_data = MetaData()",
          "61:             all_meta_data._add_table(table_name, table.schema, table)",
          "63:         # create diff between database schema and SQLAlchemy model",
          "",
          "[Removed Lines]",
          "60:         for (table_name, table) in airflow_base.metadata.tables.items():",
          "",
          "[Added Lines]",
          "63:         for table_name, table in airflow_base.metadata.tables.items():",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "251:         import airflow",
          "253:         assert config.config_file_name == os.path.join(os.path.dirname(airflow.__file__), \"alembic.ini\")",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "258:     @mock.patch(\"airflow.utils.db._move_dangling_data_to_new_table\")",
          "259:     @mock.patch(\"airflow.utils.db.get_query_count\")",
          "260:     @mock.patch(\"airflow.utils.db._dangling_against_task_instance\")",
          "261:     @mock.patch(\"airflow.utils.db._dangling_against_dag_run\")",
          "262:     @mock.patch(\"airflow.utils.db.reflect_tables\")",
          "263:     @mock.patch(\"airflow.utils.db.inspect\")",
          "264:     def test_check_bad_references(",
          "265:         self,",
          "266:         mock_inspect: MagicMock,",
          "267:         mock_reflect_tables: MagicMock,",
          "268:         mock_dangling_against_dag_run: MagicMock,",
          "269:         mock_dangling_against_task_instance: MagicMock,",
          "270:         mock_get_query_count: MagicMock,",
          "271:         mock_move_dangling_data_to_new_table: MagicMock,",
          "272:     ):",
          "273:         from airflow.models.dagrun import DagRun",
          "274:         from airflow.models.renderedtifields import RenderedTaskInstanceFields",
          "275:         from airflow.models.taskfail import TaskFail",
          "276:         from airflow.models.taskinstance import TaskInstance",
          "277:         from airflow.models.taskreschedule import TaskReschedule",
          "278:         from airflow.models.xcom import XCom",
          "280:         mock_session = MagicMock(spec=NEW_SESSION)",
          "281:         mock_bind = MagicMock()",
          "282:         mock_session.get_bind.return_value = mock_bind",
          "283:         task_instance_table = MagicMock(spec=Table)",
          "284:         task_instance_table.name = TaskInstance.__tablename__",
          "285:         dag_run_table = MagicMock(spec=Table)",
          "286:         task_fail_table = MagicMock(spec=Table)",
          "287:         task_fail_table.name = TaskFail.__tablename__",
          "289:         mock_reflect_tables.return_value = MagicMock(",
          "290:             tables={",
          "291:                 DagRun.__tablename__: dag_run_table,",
          "292:                 TaskInstance.__tablename__: task_instance_table,",
          "293:                 TaskFail.__tablename__: task_fail_table,",
          "294:             }",
          "295:         )",
          "297:         # Simulate that there is a moved `task_instance` table from the",
          "298:         # previous run, but no moved `task_fail` table",
          "299:         dangling_task_instance_table_name = f\"_airflow_moved__2_2__dangling__{task_instance_table.name}\"",
          "300:         dangling_task_fail_table_name = f\"_airflow_moved__2_3__dangling__{task_fail_table.name}\"",
          "301:         mock_get_table_names = MagicMock(",
          "302:             return_value=[",
          "303:                 TaskInstance.__tablename__,",
          "304:                 DagRun.__tablename__,",
          "305:                 TaskFail.__tablename__,",
          "306:                 dangling_task_instance_table_name,",
          "307:             ]",
          "308:         )",
          "309:         mock_inspect.return_value = MagicMock(",
          "310:             get_table_names=mock_get_table_names,",
          "311:         )",
          "312:         mock_select = MagicMock(spec=Select)",
          "313:         mock_dangling_against_dag_run.return_value = mock_select",
          "314:         mock_dangling_against_task_instance.return_value = mock_select",
          "315:         mock_get_query_count.return_value = 1",
          "317:         # Should return a single error related to the dangling `task_instance` table",
          "318:         errs = list(check_bad_references(session=mock_session))",
          "319:         assert len(errs) == 1",
          "320:         assert dangling_task_instance_table_name in errs[0]",
          "322:         mock_reflect_tables.assert_called_once_with(",
          "323:             [TaskInstance, TaskReschedule, RenderedTaskInstanceFields, TaskFail, XCom, DagRun, TaskInstance],",
          "324:             mock_session,",
          "325:         )",
          "326:         mock_inspect.assert_called_once_with(mock_bind)",
          "327:         mock_get_table_names.assert_called_once()",
          "328:         mock_dangling_against_dag_run.assert_called_once_with(",
          "329:             mock_session, task_instance_table, dag_run=dag_run_table",
          "330:         )",
          "331:         mock_get_query_count.assert_called_once_with(mock_select, session=mock_session)",
          "332:         mock_move_dangling_data_to_new_table.assert_called_once_with(",
          "333:             mock_session, task_fail_table, mock_select, dangling_task_fail_table_name",
          "334:         )",
          "335:         mock_session.rollback.assert_called_once()",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "0643e911ad6ec7dc2201c827e98e3e285a7df5d2",
      "candidate_info": {
        "commit_hash": "0643e911ad6ec7dc2201c827e98e3e285a7df5d2",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/0643e911ad6ec7dc2201c827e98e3e285a7df5d2",
        "files": [
          "dev/breeze/src/airflow_breeze/commands/developer_commands.py",
          "dev/breeze/src/airflow_breeze/commands/testing_commands.py",
          "dev/breeze/src/airflow_breeze/global_constants.py",
          "dev/breeze/src/airflow_breeze/utils/docker_command_utils.py",
          "dev/breeze/tests/test_docker_command_utils.py"
        ],
        "message": "Bump min-version for docker and docker-compose (#33572)\n\nIt's been quite some time since we bumped min versions for docker\nand docker-compose for Breeze and there are already a few cases where\nit held us back from more efficiently using some newer docker or\ndocker-compose features (for example in #33547). Also docker-compose\nv1 is sufficiently old an unmaintained, that it's about time to\nget rid of it and ask the users to migrate to docker-compose v2.\n\nThe new min versions propose:\n\n* Docker: 23.0.0 released 2023-02-01 (6 months ago)\n* Docker Compose 2.14.0 release 2022-12-15 (8 months ago)\n\nDocker v1 is not supported any more and the users are directed\nto migrate to v2 to continue using Breeze.\n\n(cherry picked from commit 73a37333918abe0612120d95169b9e377274810b)",
        "before_after_code_files": [
          "dev/breeze/src/airflow_breeze/commands/developer_commands.py||dev/breeze/src/airflow_breeze/commands/developer_commands.py",
          "dev/breeze/src/airflow_breeze/commands/testing_commands.py||dev/breeze/src/airflow_breeze/commands/testing_commands.py",
          "dev/breeze/src/airflow_breeze/global_constants.py||dev/breeze/src/airflow_breeze/global_constants.py",
          "dev/breeze/src/airflow_breeze/utils/docker_command_utils.py||dev/breeze/src/airflow_breeze/utils/docker_command_utils.py",
          "dev/breeze/tests/test_docker_command_utils.py||dev/breeze/tests/test_docker_command_utils.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/breeze/src/airflow_breeze/commands/developer_commands.py||dev/breeze/src/airflow_breeze/commands/developer_commands.py": [
          "File: dev/breeze/src/airflow_breeze/commands/developer_commands.py -> dev/breeze/src/airflow_breeze/commands/developer_commands.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "79: from airflow_breeze.utils.console import get_console",
          "80: from airflow_breeze.utils.custom_param_types import BetterChoice, NotVerifiedBetterChoice",
          "81: from airflow_breeze.utils.docker_command_utils import (",
          "83:     check_docker_resources,",
          "84:     get_env_variables_for_docker_commands,",
          "85:     get_extra_docker_flags,",
          "",
          "[Removed Lines]",
          "82:     DOCKER_COMPOSE_COMMAND,",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "663: @option_dry_run",
          "664: def down(preserve_volumes: bool, cleanup_mypy_cache: bool):",
          "665:     perform_environment_checks()",
          "667:     if not preserve_volumes:",
          "668:         command_to_execute.append(\"--volumes\")",
          "669:     shell_params = ShellParams(backend=\"all\", include_mypy_volume=True)",
          "",
          "[Removed Lines]",
          "666:     command_to_execute = [*DOCKER_COMPOSE_COMMAND, \"down\", \"--remove-orphans\"]",
          "",
          "[Added Lines]",
          "665:     command_to_execute = [\"docker\", \"compose\", \"down\", \"--remove-orphans\"]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "734:     if shell_params.include_mypy_volume:",
          "735:         create_mypy_volume_if_needed()",
          "736:     shell_params.print_badge_info()",
          "738:     cmd_added = shell_params.command_passed",
          "739:     env_variables = get_env_variables_for_docker_commands(shell_params)",
          "740:     if cmd_added is not None:",
          "",
          "[Removed Lines]",
          "737:     cmd = [*DOCKER_COMPOSE_COMMAND, \"run\", \"--service-ports\", \"-e\", \"BREEZE\", \"--rm\", \"airflow\"]",
          "",
          "[Added Lines]",
          "736:     cmd = [\"docker\", \"compose\", \"run\", \"--service-ports\", \"-e\", \"BREEZE\", \"--rm\", \"airflow\"]",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "774:     check_docker_resources(exec_shell_params.airflow_image_name)",
          "775:     exec_shell_params.print_badge_info()",
          "776:     env_variables = get_env_variables_for_docker_commands(exec_shell_params)",
          "778:     docker_compose_ps_command = run_command(",
          "779:         cmd, text=True, capture_output=True, env=env_variables, check=False",
          "780:     )",
          "",
          "[Removed Lines]",
          "777:     cmd = [*DOCKER_COMPOSE_COMMAND, \"ps\", \"--all\", \"--filter\", \"status=running\", \"airflow\"]",
          "",
          "[Added Lines]",
          "776:     cmd = [\"docker\", \"compose\", \"ps\", \"--all\", \"--filter\", \"status=running\", \"airflow\"]",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/commands/testing_commands.py||dev/breeze/src/airflow_breeze/commands/testing_commands.py": [
          "File: dev/breeze/src/airflow_breeze/commands/testing_commands.py -> dev/breeze/src/airflow_breeze/commands/testing_commands.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "60: from airflow_breeze.utils.console import Output, get_console",
          "61: from airflow_breeze.utils.custom_param_types import BetterChoice, NotVerifiedBetterChoice",
          "62: from airflow_breeze.utils.docker_command_utils import (",
          "64:     get_env_variables_for_docker_commands,",
          "65:     perform_environment_checks,",
          "66:     remove_docker_networks,",
          "",
          "[Removed Lines]",
          "63:     DOCKER_COMPOSE_COMMAND,",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "172:     # This is needed for Docker-compose 1 compatibility",
          "173:     env_variables[\"COMPOSE_PROJECT_NAME\"] = compose_project_name",
          "174:     down_cmd = [",
          "176:         \"--project-name\",",
          "177:         compose_project_name,",
          "178:         \"down\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "174:         \"docker\",",
          "175:         \"compose\",",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "180:     ]",
          "181:     run_command(down_cmd, env=env_variables, output=output, check=False)",
          "182:     run_cmd = [",
          "184:         \"--project-name\",",
          "185:         compose_project_name,",
          "186:         \"run\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "183:         \"docker\",",
          "184:         \"compose\",",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "228:         if not skip_docker_compose_down:",
          "229:             run_command(",
          "230:                 [",
          "232:                     \"--project-name\",",
          "233:                     compose_project_name,",
          "234:                     \"rm\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "232:                     \"docker\",",
          "233:                     \"compose\",",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "587:         env_variables[\"HELM_TEST_PACKAGE\"] = helm_test_package",
          "588:     perform_environment_checks()",
          "589:     cleanup_python_generated_files()",
          "591:     result = run_command(cmd, env=env_variables, check=False, output_outside_the_group=True)",
          "592:     sys.exit(result.returncode)",
          "",
          "[Removed Lines]",
          "590:     cmd = [*DOCKER_COMPOSE_COMMAND, \"run\", \"--service-ports\", \"--rm\", \"airflow\", *extra_pytest_args]",
          "",
          "[Added Lines]",
          "592:     cmd = [\"docker\", \"compose\", \"run\", \"--service-ports\", \"--rm\", \"airflow\", *extra_pytest_args]",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/global_constants.py||dev/breeze/src/airflow_breeze/global_constants.py": [
          "File: dev/breeze/src/airflow_breeze/global_constants.py -> dev/breeze/src/airflow_breeze/global_constants.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "335: ISSUE_ID = \"\"",
          "336: NUM_RUNS = \"\"",
          "341: AIRFLOW_SOURCES_FROM = \".\"",
          "342: AIRFLOW_SOURCES_TO = \"/opt/airflow\"",
          "",
          "[Removed Lines]",
          "338: MIN_DOCKER_VERSION = \"20.10.0\"",
          "339: MIN_DOCKER_COMPOSE_VERSION = \"1.29.0\"",
          "",
          "[Added Lines]",
          "338: MIN_DOCKER_VERSION = \"23.0.0\"",
          "339: MIN_DOCKER_COMPOSE_VERSION = \"2.14.0\"",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/utils/docker_command_utils.py||dev/breeze/src/airflow_breeze/utils/docker_command_utils.py": [
          "File: dev/breeze/src/airflow_breeze/utils/docker_command_utils.py -> dev/breeze/src/airflow_breeze/utils/docker_command_utils.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "250: [warning]install docker at least: {MIN_DOCKER_VERSION} version.[/]",
          "251: \"\"\"",
          "252:             )",
          "253:         else:",
          "254:             good_version = compare_version(docker_version, MIN_DOCKER_VERSION)",
          "255:             if good_version:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "253:             sys.exit(1)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "257:             else:",
          "258:                 get_console().print(",
          "259:                     f\"\"\"",
          "262: \"\"\"",
          "263:                 )",
          "266: def check_remote_ghcr_io_commands():",
          "",
          "[Removed Lines]",
          "260: [warning]Your version of docker is too old:{docker_version}.",
          "261: Please upgrade to at least {MIN_DOCKER_VERSION}[/]",
          "",
          "[Added Lines]",
          "261: [error]Your version of docker is too old: {docker_version}.\\n[/]",
          "262: [warning]Please upgrade to at least {MIN_DOCKER_VERSION}.\\n[/]",
          "263: You can find installation instructions here: https://docs.docker.com/engine/install/",
          "266:                 sys.exit(1)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "306:             sys.exit(1)",
          "312: def check_docker_compose_version():",
          "313:     \"\"\"Checks if the docker compose version is as expected.",
          "",
          "[Removed Lines]",
          "309: DOCKER_COMPOSE_COMMAND = [\"docker\", \"compose\"]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "328:             dry_run_override=False,",
          "329:         )",
          "330:     except Exception:",
          "338:         )",
          "341:     if docker_compose_version_result.returncode == 0:",
          "342:         docker_compose_version = docker_compose_version_result.stdout",
          "343:         version_extracted = version_pattern.search(docker_compose_version)",
          "344:         if version_extracted is not None:",
          "347:             if good_version:",
          "349:             else:",
          "350:                 get_console().print(",
          "351:                     f\"\"\"",
          "359: \"\"\"",
          "360:                 )",
          "361:     else:",
          "362:         get_console().print(",
          "366: \"\"\"",
          "367:         )",
          "370: def get_env_variable_value(arg_name: str, params: CommonBuildParams | ShellParams):",
          "",
          "[Removed Lines]",
          "331:         docker_compose_version_command = [\"docker-compose\", \"--version\"]",
          "332:         docker_compose_version_result = run_command(",
          "333:             docker_compose_version_command,",
          "334:             no_output_dump_on_exception=True,",
          "335:             capture_output=True,",
          "336:             text=True,",
          "337:             dry_run_override=False,",
          "339:         DOCKER_COMPOSE_COMMAND.clear()",
          "340:         DOCKER_COMPOSE_COMMAND.append(\"docker-compose\")",
          "345:             docker_version = \".\".join(version_extracted.groups())",
          "346:             good_version = compare_version(docker_version, MIN_DOCKER_COMPOSE_VERSION)",
          "348:                 get_console().print(f\"[success]Good version of docker-compose: {docker_version}[/]\")",
          "352: [warning]You have too old version of docker-compose: {docker_version}! At least 1.29 needed! Please upgrade!",
          "353: \"\"\"",
          "354:                 )",
          "355:                 get_console().print(",
          "356:                     \"\"\"",
          "357: See https://docs.docker.com/compose/install/ for instructions.",
          "358: Make sure docker-compose you install is first on the PATH variable of yours.",
          "363:             \"\"\"",
          "364: [warning]Unknown docker-compose version. At least 1.29 is needed![/]",
          "365: [warning]If Breeze fails upgrade to latest available docker-compose version.[/]",
          "",
          "[Added Lines]",
          "331:         get_console().print(",
          "332:             \"[error]You either do not have docker-composer or have docker-compose v1 installed.[/]\\n\"",
          "333:             \"[warning]Breeze does not support docker-compose v1 any more as it has been replaced by v2.[/]\\n\"",
          "334:             \"Follow https://docs.docker.com/compose/migrate/ to migrate to v2\"",
          "336:         sys.exit(1)",
          "341:             docker_compose_version = \".\".join(version_extracted.groups())",
          "342:             good_version = compare_version(docker_compose_version, MIN_DOCKER_COMPOSE_VERSION)",
          "344:                 get_console().print(f\"[success]Good version of docker-compose: {docker_compose_version}[/]\")",
          "348: [error]You have too old version of docker-compose: {docker_compose_version}!\\n[/]",
          "349: [warning]At least {MIN_DOCKER_COMPOSE_VERSION} needed! Please upgrade!\\n[/]",
          "350: See https://docs.docker.com/compose/install/ for installation instructions.\\n",
          "351: Make sure docker-compose you install is first on the PATH variable of yours.\\n",
          "354:                 sys.exit(1)",
          "357:             f\"\"\"",
          "358: [error]Unknown docker-compose version.[/]",
          "359: [warning]At least {MIN_DOCKER_COMPOSE_VERSION} needed! Please upgrade!\\n[/]",
          "360: See https://docs.docker.com/compose/install/ for installation instructions.\\n",
          "361: Make sure docker-compose you install is first on the PATH variable of yours.\\n",
          "364:         sys.exit(1)",
          "",
          "---------------"
        ],
        "dev/breeze/tests/test_docker_command_utils.py||dev/breeze/tests/test_docker_command_utils.py": [
          "File: dev/breeze/tests/test_docker_command_utils.py -> dev/breeze/tests/test_docker_command_utils.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "36:     mock_get_console, mock_run_command, mock_check_docker_permission_denied",
          "37: ):",
          "38:     mock_check_docker_permission_denied.return_value = False",
          "40:     expected_run_command_calls = [",
          "41:         call(",
          "42:             [\"docker\", \"version\", \"--format\", \"{{.Client.Version}}\"],",
          "",
          "[Removed Lines]",
          "39:     check_docker_version()",
          "",
          "[Added Lines]",
          "39:     with pytest.raises(SystemExit) as e:",
          "40:         check_docker_version()",
          "41:     assert e.value.code == 1",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "51:     mock_get_console.return_value.print.assert_called_with(",
          "52:         \"\"\"",
          "53: [warning]Your version of docker is unknown. If the scripts fail, please make sure to[/]",
          "55: \"\"\"",
          "56:     )",
          "",
          "[Removed Lines]",
          "54: [warning]install docker at least: 20.10.0 version.[/]",
          "",
          "[Added Lines]",
          "56: [warning]install docker at least: 23.0.0 version.[/]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "65:     mock_check_docker_permission_denied.return_value = False",
          "66:     mock_run_command.return_value.returncode = 0",
          "67:     mock_run_command.return_value.stdout = \"0.9\"",
          "69:     mock_check_docker_permission_denied.assert_called()",
          "70:     mock_run_command.assert_called_with(",
          "71:         [\"docker\", \"version\", \"--format\", \"{{.Client.Version}}\"],",
          "",
          "[Removed Lines]",
          "68:     check_docker_version()",
          "",
          "[Added Lines]",
          "70:     with pytest.raises(SystemExit) as e:",
          "71:         check_docker_version()",
          "72:     assert e.value.code == 1",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "77:     )",
          "78:     mock_get_console.return_value.print.assert_called_with(",
          "79:         \"\"\"",
          "81: \"\"\"",
          "82:     )",
          "",
          "[Removed Lines]",
          "80: [warning]Your version of docker is too old:0.9.\\nPlease upgrade to at least 20.10.0[/]",
          "",
          "[Added Lines]",
          "84: [error]Your version of docker is too old: 0.9.\\n[/]\\n[warning]Please upgrade to at least 23.0.0.\\n[/]\\n\\",
          "85: You can find installation instructions here: https://docs.docker.com/engine/install/",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "88: def test_check_docker_version_ok(mock_get_console, mock_run_command, mock_check_docker_permission_denied):",
          "89:     mock_check_docker_permission_denied.return_value = False",
          "90:     mock_run_command.return_value.returncode = 0",
          "92:     check_docker_version()",
          "93:     mock_check_docker_permission_denied.assert_called()",
          "94:     mock_run_command.assert_called_with(",
          "",
          "[Removed Lines]",
          "91:     mock_run_command.return_value.stdout = \"20.10.0\"",
          "",
          "[Added Lines]",
          "96:     mock_run_command.return_value.stdout = \"23.0.0\"",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "99:         check=False,",
          "100:         dry_run_override=False,",
          "101:     )",
          "105: @mock.patch(\"airflow_breeze.utils.docker_command_utils.check_docker_permission_denied\")",
          "",
          "[Removed Lines]",
          "102:     mock_get_console.return_value.print.assert_called_with(\"[success]Good version of Docker: 20.10.0.[/]\")",
          "",
          "[Added Lines]",
          "107:     mock_get_console.return_value.print.assert_called_with(\"[success]Good version of Docker: 23.0.0.[/]\")",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "108: def test_check_docker_version_higher(mock_get_console, mock_run_command, mock_check_docker_permission_denied):",
          "109:     mock_check_docker_permission_denied.return_value = False",
          "110:     mock_run_command.return_value.returncode = 0",
          "112:     check_docker_version()",
          "113:     mock_check_docker_permission_denied.assert_called()",
          "114:     mock_run_command.assert_called_with(",
          "",
          "[Removed Lines]",
          "111:     mock_run_command.return_value.stdout = \"21.10.0\"",
          "",
          "[Added Lines]",
          "116:     mock_run_command.return_value.stdout = \"24.0.0\"",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "119:         check=False,",
          "120:         dry_run_override=False,",
          "121:     )",
          "125: @mock.patch(\"airflow_breeze.utils.docker_command_utils.run_command\")",
          "126: @mock.patch(\"airflow_breeze.utils.docker_command_utils.get_console\")",
          "127: def test_check_docker_compose_version_unknown(mock_get_console, mock_run_command):",
          "129:     expected_run_command_calls = [",
          "130:         call(",
          "131:             [\"docker\", \"compose\", \"version\"],",
          "",
          "[Removed Lines]",
          "122:     mock_get_console.return_value.print.assert_called_with(\"[success]Good version of Docker: 21.10.0.[/]\")",
          "128:     check_docker_compose_version()",
          "",
          "[Added Lines]",
          "127:     mock_get_console.return_value.print.assert_called_with(\"[success]Good version of Docker: 24.0.0.[/]\")",
          "133:     with pytest.raises(SystemExit) as e:",
          "134:         check_docker_compose_version()",
          "135:     assert e.value.code == 1",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "138:     mock_run_command.assert_has_calls(expected_run_command_calls)",
          "139:     mock_get_console.return_value.print.assert_called_with(",
          "140:         \"\"\"",
          "143: \"\"\"",
          "144:     )",
          "",
          "[Removed Lines]",
          "141: [warning]Unknown docker-compose version. At least 1.29 is needed![/]",
          "142: [warning]If Breeze fails upgrade to latest available docker-compose version.[/]",
          "",
          "[Added Lines]",
          "148: [error]Unknown docker-compose version.[/]\\n[warning]At least 2.14.0 needed! Please upgrade!\\n[/]",
          "149: See https://docs.docker.com/compose/install/ for installation instructions.\\n",
          "150: Make sure docker-compose you install is first on the PATH variable of yours.\\n",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "149: def test_check_docker_compose_version_low(mock_get_console, mock_run_command):",
          "150:     mock_run_command.return_value.returncode = 0",
          "151:     mock_run_command.return_value.stdout = \"1.28.5\"",
          "153:     mock_run_command.assert_called_with(",
          "154:         [\"docker\", \"compose\", \"version\"],",
          "155:         no_output_dump_on_exception=True,",
          "",
          "[Removed Lines]",
          "152:     check_docker_compose_version()",
          "",
          "[Added Lines]",
          "160:     with pytest.raises(SystemExit) as e:",
          "161:         check_docker_compose_version()",
          "162:     assert e.value.code == 1",
          "",
          "---------------",
          "--- Hunk 11 ---",
          "[Context before]",
          "157:         text=True,",
          "158:         dry_run_override=False,",
          "159:     )",
          "170: \"\"\"",
          "176: @mock.patch(\"airflow_breeze.utils.docker_command_utils.run_command\")",
          "177: @mock.patch(\"airflow_breeze.utils.docker_command_utils.get_console\")",
          "178: def test_check_docker_compose_version_ok(mock_get_console, mock_run_command):",
          "179:     mock_run_command.return_value.returncode = 0",
          "181:     check_docker_compose_version()",
          "182:     mock_run_command.assert_called_with(",
          "183:         [\"docker\", \"compose\", \"version\"],",
          "",
          "[Removed Lines]",
          "160:     expected_print_calls = [",
          "161:         call(",
          "162:             \"\"\"",
          "163: [warning]You have too old version of docker-compose: 1.28.5! At least 1.29 needed! Please upgrade!",
          "164: \"\"\"",
          "165:         ),",
          "166:         call(",
          "167:             \"\"\"",
          "168: See https://docs.docker.com/compose/install/ for instructions.",
          "169: Make sure docker-compose you install is first on the PATH variable of yours.",
          "171:         ),",
          "172:     ]",
          "173:     mock_get_console.return_value.print.assert_has_calls(expected_print_calls)",
          "180:     mock_run_command.return_value.stdout = \"1.29.0\"",
          "",
          "[Added Lines]",
          "170:     mock_get_console.return_value.print.assert_called_with(",
          "171:         \"\"\"",
          "172: [error]You have too old version of docker-compose: 1.28.5!\\n[/]",
          "173: [warning]At least 2.14.0 needed! Please upgrade!\\n[/]",
          "174: See https://docs.docker.com/compose/install/ for installation instructions.\\n",
          "175: Make sure docker-compose you install is first on the PATH variable of yours.\\n",
          "177:     )",
          "184:     mock_run_command.return_value.stdout = \"2.14.0\"",
          "",
          "---------------",
          "--- Hunk 12 ---",
          "[Context before]",
          "187:         dry_run_override=False,",
          "188:     )",
          "189:     mock_get_console.return_value.print.assert_called_with(",
          "191:     )",
          "",
          "[Removed Lines]",
          "190:         \"[success]Good version of docker-compose: 1.29.0[/]\"",
          "",
          "[Added Lines]",
          "194:         \"[success]Good version of docker-compose: 2.14.0[/]\"",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "739bfb44bf20281fac8e34b506f6e4a258f3d0bb",
      "candidate_info": {
        "commit_hash": "739bfb44bf20281fac8e34b506f6e4a258f3d0bb",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/739bfb44bf20281fac8e34b506f6e4a258f3d0bb",
        "files": [
          "dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "dev/breeze/src/airflow_breeze/commands/setup_commands.py",
          "dev/breeze/src/airflow_breeze/commands/testing_commands.py",
          "dev/breeze/src/airflow_breeze/utils/run_utils.py",
          "dev/breeze/src/airflow_breeze/utils/selective_checks.py"
        ],
        "message": "Refactor: Simplify code in breeze (#33273)\n\n* Refactor: Simplify code in breeze\n\n---------\n\nCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>\n(cherry picked from commit 7c950a85b769e2c136c968bdbe103f87a2c45d3c)",
        "before_after_code_files": [
          "dev/breeze/src/airflow_breeze/commands/release_management_commands.py||dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "dev/breeze/src/airflow_breeze/commands/setup_commands.py||dev/breeze/src/airflow_breeze/commands/setup_commands.py",
          "dev/breeze/src/airflow_breeze/commands/testing_commands.py||dev/breeze/src/airflow_breeze/commands/testing_commands.py",
          "dev/breeze/src/airflow_breeze/utils/run_utils.py||dev/breeze/src/airflow_breeze/utils/run_utils.py",
          "dev/breeze/src/airflow_breeze/utils/selective_checks.py||dev/breeze/src/airflow_breeze/utils/selective_checks.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/breeze/src/airflow_breeze/commands/release_management_commands.py||dev/breeze/src/airflow_breeze/commands/release_management_commands.py": [
          "File: dev/breeze/src/airflow_breeze/commands/release_management_commands.py -> dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1033: def is_package_in_dist(dist_files: list[str], package: str) -> bool:",
          "1034:     \"\"\"Check if package has been prepared in dist folder.\"\"\"",
          "1043: def get_prs_for_package(package_id: str) -> list[int]:",
          "",
          "[Removed Lines]",
          "1035:     for file in dist_files:",
          "1036:         if file.startswith(f'apache_airflow_providers_{package.replace(\".\", \"_\")}') or file.startswith(",
          "1037:             f'apache-airflow-providers-{package.replace(\".\", \"-\")}'",
          "1038:         ):",
          "1039:             return True",
          "1040:     return False",
          "",
          "[Added Lines]",
          "1035:     return any(",
          "1036:         file.startswith(",
          "1037:             (",
          "1038:                 f'apache_airflow_providers_{package.replace(\".\", \"_\")}',",
          "1039:                 f'apache-airflow-providers-{package.replace(\".\", \"-\")}',",
          "1040:             )",
          "1041:         )",
          "1042:         for file in dist_files",
          "1043:     )",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/commands/setup_commands.py||dev/breeze/src/airflow_breeze/commands/setup_commands.py": [
          "File: dev/breeze/src/airflow_breeze/commands/setup_commands.py -> dev/breeze/src/airflow_breeze/commands/setup_commands.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "410:     results = {}",
          "411:     for line in hash_file_content.splitlines():",
          "412:         strip_line = line.strip()",
          "414:             continue",
          "415:         command = \":\".join(strip_line.split(\":\")[:-1])",
          "416:         the_hash = strip_line.split(\":\")[-1]",
          "",
          "[Removed Lines]",
          "413:         if strip_line.strip() == \"\" or strip_line.startswith(\"#\"):",
          "",
          "[Added Lines]",
          "413:         if not strip_line or strip_line.startswith(\"#\"):",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/commands/testing_commands.py||dev/breeze/src/airflow_breeze/commands/testing_commands.py": [
          "File: dev/breeze/src/airflow_breeze/commands/testing_commands.py -> dev/breeze/src/airflow_breeze/commands/testing_commands.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "587:         env_variables[\"HELM_TEST_PACKAGE\"] = helm_test_package",
          "588:     perform_environment_checks()",
          "589:     cleanup_python_generated_files()",
          "592:     result = run_command(cmd, env=env_variables, check=False, output_outside_the_group=True)",
          "593:     sys.exit(result.returncode)",
          "",
          "[Removed Lines]",
          "590:     cmd = [*DOCKER_COMPOSE_COMMAND, \"run\", \"--service-ports\", \"--rm\", \"airflow\"]",
          "591:     cmd.extend(list(extra_pytest_args))",
          "",
          "[Added Lines]",
          "590:     cmd = [*DOCKER_COMPOSE_COMMAND, \"run\", \"--service-ports\", \"--rm\", \"airflow\", *extra_pytest_args]",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/utils/run_utils.py||dev/breeze/src/airflow_breeze/utils/run_utils.py": [
          "File: dev/breeze/src/airflow_breeze/utils/run_utils.py -> dev/breeze/src/airflow_breeze/utils/run_utils.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "362: def filter_out_none(**kwargs) -> dict:",
          "363:     \"\"\"Filters out all None values from parameters passed.\"\"\"",
          "370: def check_if_image_exists(image: str) -> bool:",
          "",
          "[Removed Lines]",
          "364:     for key in list(kwargs):",
          "365:         if kwargs[key] is None:",
          "366:             kwargs.pop(key)",
          "367:     return kwargs",
          "",
          "[Added Lines]",
          "364:     return {key: val for key, val in kwargs.items() if val is not None}",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/utils/selective_checks.py||dev/breeze/src/airflow_breeze/utils/selective_checks.py": [
          "File: dev/breeze/src/airflow_breeze/utils/selective_checks.py -> dev/breeze/src/airflow_breeze/utils/selective_checks.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "621:             get_console().print(",
          "622:                 \"[warning]There are no core/other files. Only tests relevant to the changed files are run.[/]\"",
          "623:             )",
          "625:         get_console().print(\"[warning]Selected test type candidates to run:[/]\")",
          "626:         get_console().print(sorted_candidate_test_types)",
          "627:         return sorted_candidate_test_types",
          "",
          "[Removed Lines]",
          "624:         sorted_candidate_test_types = list(sorted(candidate_test_types))",
          "",
          "[Added Lines]",
          "624:         sorted_candidate_test_types = sorted(candidate_test_types)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "714:         ):",
          "715:             return _ALL_DOCS_LIST",
          "716:         packages = []",
          "720:             packages.append(\"apache-airflow\")",
          "722:             packages.append(\"apache-airflow-providers\")",
          "724:             packages.append(\"helm-chart\")",
          "726:             packages.append(\"docker-stack\")",
          "727:         if providers_affected:",
          "728:             for provider in providers_affected:",
          "",
          "[Removed Lines]",
          "717:         if any(",
          "718:             [file.startswith(\"airflow/\") or file.startswith(\"docs/apache-airflow/\") for file in self._files]",
          "719:         ):",
          "721:         if any([file.startswith(\"docs/apache-airflow-providers/\") for file in self._files]):",
          "723:         if any([file.startswith(\"chart/\") or file.startswith(\"docs/helm-chart\") for file in self._files]):",
          "725:         if any([file.startswith(\"docs/docker-stack/\") for file in self._files]):",
          "",
          "[Added Lines]",
          "717:         if any(file.startswith((\"airflow/\", \"docs/apache-airflow/\")) for file in self._files):",
          "719:         if any(file.startswith(\"docs/apache-airflow-providers/\") for file in self._files):",
          "721:         if any(file.startswith((\"chart/\", \"docs/helm-chart\")) for file in self._files):",
          "723:         if any(file.startswith(\"docs/docker-stack/\") for file in self._files):",
          "",
          "---------------"
        ]
      }
    }
  ]
}