{
  "cve_id": "CVE-2022-31777",
  "cve_desc": "A stored cross-site scripting (XSS) vulnerability in Apache Spark 3.2.1 and earlier, and 3.3.0, allows remote attackers to execute arbitrary JavaScript in the web browser of a user, by including a malicious payload into the logs which would be returned in logs rendered in the UI.",
  "repo": "apache/spark",
  "patch_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
  "patch_info": {
    "commit_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
    "repo": "apache/spark",
    "commit_url": "https://github.com/apache/spark/commit/ad90195de56688ce0898691eb9d04297ab0871ad",
    "files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ],
    "message": "[SPARK-39505][UI] Escape log content rendered in UI\n\n### What changes were proposed in this pull request?\n\nEscape log content rendered to the UI.\n\n### Why are the changes needed?\n\nLog content may contain reserved characters or other code in the log and be misinterpreted in the UI as HTML.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nExisting tests\n\nCloses #36902 from srowen/LogViewEscape.\n\nAuthored-by: Sean Owen <srowen@gmail.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
    "before_after_code_files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ]
  },
  "patch_diff": {
    "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js": [
      "File: core/src/main/resources/org/apache/spark/ui/static/log-view.js -> core/src/main/resources/org/apache/spark/ui/static/log-view.js",
      "--- Hunk 1 ---",
      "[Context before]",
      "85:       if (retStartByte == 0) {",
      "86:         disableMoreButton();",
      "87:       }",
      "90:       curLogLength = curLogLength + (startByte - retStartByte);",
      "91:       startByte = retStartByte;",
      "",
      "[Removed Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(cleanData);",
      "",
      "[Added Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(document.createTextNode(cleanData));",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "115:             var retLogLength = dataInfo[2];",
      "117:             var cleanData = data.substring(newlineIndex + 1);",
      "120:             curLogLength = curLogLength + (retEndByte - retStartByte);",
      "121:             endByte = retEndByte;",
      "",
      "[Removed Lines]",
      "118:             $(\"pre\", \".log-content\").append(cleanData);",
      "",
      "[Added Lines]",
      "118:             $(\"pre\", \".log-content\").append(document.createTextNode(cleanData));",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "c6778bacd481e794ef013efd241303421f8400e4",
      "candidate_info": {
        "commit_hash": "c6778bacd481e794ef013efd241303421f8400e4",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/c6778bacd481e794ef013efd241303421f8400e4",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/ToNumberParser.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/StringExpressionsSuite.scala"
        ],
        "message": "[SPARK-38796][SQL] Update to_number and try_to_number functions to allow PR with positive numbers\n\n### What changes were proposed in this pull request?\n\nUpdate `to_number` and `try_to_number` functions to allow the `PR` format token with input strings comprising positive numbers.\n\nBefore this bug fix, function calls like `to_number(' 123 ', '999PR')` would fail. Now they succeed, which is helpful since `PR` should allow both positive and negative numbers.\n\nThis satisfies the following specification:\n\n```\nto_number(expr, fmt)\nfmt\n  { ' [ MI | S ] [ L | $ ]\n      [ 0 | 9 | G | , ] [...]\n      [ . | D ]\n      [ 0 | 9 ] [...]\n      [ L | $ ] [ PR | MI | S ] ' }\n```\n\n### Why are the changes needed?\n\nAfter reviewing the specification, this behavior makes the most sense.\n\n### Does this PR introduce _any_ user-facing change?\n\nYes, a slight change in the behavior of the format string.\n\n### How was this patch tested?\n\nExisting and updated unit test coverage.\n\nCloses #36861 from dtenedor/to-number-fix-pr.\n\nAuthored-by: Daniel Tenedorio <daniel.tenedorio@databricks.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\n(cherry picked from commit 4a803ca22a9a98f9bbbbd1a5a33b9ae394fb7c49)\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/ToNumberParser.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/ToNumberParser.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/StringExpressionsSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/StringExpressionsSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/ToNumberParser.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/ToNumberParser.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/ToNumberParser.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/ToNumberParser.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "397:     beforeDecimalPoint.clear()",
          "398:     afterDecimalPoint.clear()",
          "399:     var reachedDecimalPoint = false",
          "401:     var negateResult = false",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "401:     var reachedOpeningAngleBracket = false",
          "402:     var reachedClosingAngleBracket = false",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "408:     while (formatIndex < formatTokens.size) {",
          "409:       val token: InputToken = formatTokens(formatIndex)",
          "410:       token match {",
          "411:         case d: DigitGroups =>",
          "412:           inputIndex = parseDigitGroups(d, inputString, inputIndex, reachedDecimalPoint).getOrElse(",
          "413:             return formatMatchFailure(input, numberFormat))",
          "414:         case DecimalPoint() =>",
          "422:           }",
          "423:         case DollarSign() =>",
          "428:           }",
          "430:         case OptionalPlusOrMinusSign() =>",
          "441:           }",
          "442:         case OptionalMinusSign() =>",
          "450:           }",
          "451:         case OpeningAngleBracket() =>",
          "456:           }",
          "458:         case ClosingAngleBracket() =>",
          "463:           }",
          "466:       }",
          "467:       formatIndex += 1",
          "468:     }",
          "472:       formatMatchFailure(input, numberFormat)",
          "",
          "[Removed Lines]",
          "415:           if (inputIndex < inputLength &&",
          "416:             inputString(inputIndex) == POINT_SIGN) {",
          "417:             reachedDecimalPoint = true",
          "418:             inputIndex += 1",
          "419:           } else {",
          "424:           if (inputIndex >= inputLength ||",
          "425:             inputString(inputIndex) != DOLLAR_SIGN) {",
          "427:             return formatMatchFailure(input, numberFormat)",
          "429:           inputIndex += 1",
          "431:           if (inputIndex < inputLength &&",
          "432:             inputString(inputIndex) == PLUS_SIGN) {",
          "433:             inputIndex += 1",
          "434:           } else if (inputIndex < inputLength &&",
          "435:             inputString(inputIndex) == MINUS_SIGN) {",
          "436:             negateResult = !negateResult",
          "437:             inputIndex += 1",
          "438:           } else {",
          "443:           if (inputIndex < inputLength &&",
          "444:             inputString(inputIndex) == MINUS_SIGN) {",
          "445:             negateResult = !negateResult",
          "446:             inputIndex += 1",
          "447:           } else {",
          "452:           if (inputIndex >= inputLength ||",
          "453:             inputString(inputIndex) != ANGLE_BRACKET_OPEN) {",
          "455:             return formatMatchFailure(input, numberFormat)",
          "457:           inputIndex += 1",
          "459:           if (inputIndex >= inputLength ||",
          "460:             inputString(inputIndex) != ANGLE_BRACKET_CLOSE) {",
          "462:             return formatMatchFailure(input, numberFormat)",
          "464:           negateResult = !negateResult",
          "465:           inputIndex += 1",
          "469:     if (inputIndex < inputLength) {",
          "",
          "[Added Lines]",
          "413:       val inputChar: Option[Char] =",
          "414:         if (inputIndex < inputLength) {",
          "415:           Some(inputString(inputIndex))",
          "416:         } else {",
          "417:           Option.empty[Char]",
          "418:         }",
          "424:           inputChar.foreach {",
          "425:             case POINT_SIGN =>",
          "426:               reachedDecimalPoint = true",
          "427:               inputIndex += 1",
          "428:             case _ =>",
          "433:           inputChar.foreach {",
          "434:             case DOLLAR_SIGN =>",
          "435:               inputIndex += 1",
          "436:             case _ =>",
          "438:               return formatMatchFailure(input, numberFormat)",
          "441:           inputChar.foreach {",
          "442:             case PLUS_SIGN =>",
          "443:               inputIndex += 1",
          "444:             case MINUS_SIGN =>",
          "445:               negateResult = !negateResult",
          "446:               inputIndex += 1",
          "447:             case _ =>",
          "452:           inputChar.foreach {",
          "453:             case MINUS_SIGN =>",
          "454:               negateResult = !negateResult",
          "455:               inputIndex += 1",
          "456:             case _ =>",
          "461:           inputChar.foreach {",
          "462:             case ANGLE_BRACKET_OPEN =>",
          "463:               if (reachedOpeningAngleBracket) {",
          "464:                 return formatMatchFailure(input, numberFormat)",
          "465:               }",
          "466:               reachedOpeningAngleBracket = true",
          "467:               inputIndex += 1",
          "468:             case _ =>",
          "471:           inputChar.foreach {",
          "472:             case ANGLE_BRACKET_CLOSE =>",
          "473:               if (!reachedOpeningAngleBracket) {",
          "474:                 return formatMatchFailure(input, numberFormat)",
          "475:               }",
          "476:               reachedClosingAngleBracket = true",
          "477:               negateResult = !negateResult",
          "478:               inputIndex += 1",
          "479:             case _ =>",
          "484:     if (inputIndex < inputLength ||",
          "485:       reachedOpeningAngleBracket != reachedClosingAngleBracket) {",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/StringExpressionsSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/StringExpressionsSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/StringExpressionsSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/StringExpressionsSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "969:       (\"+$89,1,2,3,45.123\", \"S$999,0,0,0,999.00000\") -> Decimal(8912345.123),",
          "970:       (\"-454\", \"S999\") -> Decimal(-454),",
          "971:       (\"+454\", \"S999\") -> Decimal(454),",
          "973:       (\"454-\", \"999MI\") -> Decimal(-454),",
          "974:       (\"-$54\", \"MI$99\") -> Decimal(-54),",
          "",
          "[Removed Lines]",
          "972:       (\"<454>\", \"999PR\") -> Decimal(-454),",
          "",
          "[Added Lines]",
          "972:       (\"454\", \"999PR\") -> Decimal(454),",
          "973:       (\" 454 \", \"999PR\") -> Decimal(454),",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1090:       (\"<454\", \"999PR\"),",
          "1091:       (\"454>\", \"999PR\"),",
          "1092:       (\"<<454>>\", \"999PR\"),",
          "1094:       (\"45\", \"S$999,099.99\"),",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1093:       (\"<454 \", \"999PR\"),",
          "1094:       (\" 454>\", \"999PR\"),",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "c2fa3b80e6807d4f66d23795d54c2ee59478358b",
      "candidate_info": {
        "commit_hash": "c2fa3b80e6807d4f66d23795d54c2ee59478358b",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/c2fa3b80e6807d4f66d23795d54c2ee59478358b",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/sources/filters.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/v2/V2PredicateSuite.scala"
        ],
        "message": "[SPARK-38432][SQL][FOLLOWUP] Fix problems in And/Or/Not to V2 Filter\n\n### What changes were proposed in this pull request?\nInstead of having\n```\noverride def toV2: Predicate = new Predicate(\"AND\", Seq(left, right).map(_.toV2).toArray)\n```\nI think we should construct a V2 `And` directly.\n```\noverride def toV2: Predicate = new org.apache.spark.sql.connector.expressions.filter.And(left.toV2, right.toV2)\n```\nsame for `Or` and `Not`.\n\n### Why are the changes needed?\nbug fixing\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\nNew tests\n\nCloses #36290 from huaxingao/toV1.\n\nAuthored-by: huaxingao <huaxin_gao@apple.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\n(cherry picked from commit 36fc8bd185da99b64954ca0dd393b452fb788226)\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/sources/filters.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/sources/filters.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/v2/V2PredicateSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/v2/V2PredicateSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/sources/filters.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/sources/filters.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/sources/filters.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/sources/filters.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "21: import org.apache.spark.sql.catalyst.expressions.Literal",
          "22: import org.apache.spark.sql.connector.catalog.CatalogV2Implicits.parseColumnPath",
          "23: import org.apache.spark.sql.connector.expressions.{FieldReference, LiteralValue}",
          "25: import org.apache.spark.sql.types.StringType",
          "26: import org.apache.spark.unsafe.types.UTF8String",
          "",
          "[Removed Lines]",
          "24: import org.apache.spark.sql.connector.expressions.filter.{AlwaysFalse => V2AlwaysFalse, AlwaysTrue => V2AlwaysTrue, Predicate}",
          "",
          "[Added Lines]",
          "24: import org.apache.spark.sql.connector.expressions.filter.{AlwaysFalse => V2AlwaysFalse, AlwaysTrue => V2AlwaysTrue, And => V2And, Not => V2Not, Or => V2Or, Predicate}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "270: @Stable",
          "271: case class And(left: Filter, right: Filter) extends Filter {",
          "272:   override def references: Array[String] = left.references ++ right.references",
          "274: }",
          "",
          "[Removed Lines]",
          "273:   override def toV2: Predicate = new Predicate(\"AND\", Seq(left, right).map(_.toV2).toArray)",
          "",
          "[Added Lines]",
          "273:   override def toV2: Predicate = new V2And(left.toV2, right.toV2)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "281: @Stable",
          "282: case class Or(left: Filter, right: Filter) extends Filter {",
          "283:   override def references: Array[String] = left.references ++ right.references",
          "285: }",
          "",
          "[Removed Lines]",
          "284:   override def toV2: Predicate = new Predicate(\"OR\", Seq(left, right).map(_.toV2).toArray)",
          "",
          "[Added Lines]",
          "284:   override def toV2: Predicate = new V2Or(left.toV2, right.toV2)",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "292: @Stable",
          "293: case class Not(child: Filter) extends Filter {",
          "294:   override def references: Array[String] = child.references",
          "296: }",
          "",
          "[Removed Lines]",
          "295:   override def toV2: Predicate = new Predicate(\"NOT\", Array(child.toV2))",
          "",
          "[Added Lines]",
          "295:   override def toV2: Predicate = new V2Not(child.toV2)",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/v2/V2PredicateSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/v2/V2PredicateSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/v2/V2PredicateSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/v2/V2PredicateSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "21: import org.apache.spark.sql.connector.expressions.{Expression, FieldReference, Literal, LiteralValue}",
          "22: import org.apache.spark.sql.connector.expressions.filter._",
          "23: import org.apache.spark.sql.execution.datasources.v2.V2PredicateSuite.ref",
          "24: import org.apache.spark.sql.types.{IntegerType, StringType}",
          "25: import org.apache.spark.unsafe.types.UTF8String",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "24: import org.apache.spark.sql.sources.{AlwaysFalse => V1AlwaysFalse, AlwaysTrue => V1AlwaysTrue, And => V1And, EqualNullSafe, EqualTo, GreaterThan, GreaterThanOrEqual, In, IsNotNull, IsNull, LessThan, LessThanOrEqual, Not => V1Not, Or => V1Or, StringContains, StringEndsWith, StringStartsWith}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "31:       new Predicate(\"=\", Array[Expression](ref(\"a\", \"B\"), LiteralValue(1, IntegerType)))",
          "32:     assert(predicate1.references.map(_.describe()).toSeq == Seq(\"a.B\"))",
          "33:     assert(predicate1.describe.equals(\"a.B = 1\"))",
          "35:     val predicate2 =",
          "36:       new Predicate(\"=\", Array[Expression](ref(\"a\", \"b.c\"), LiteralValue(1, IntegerType)))",
          "37:     assert(predicate2.references.map(_.describe()).toSeq == Seq(\"a.`b.c`\"))",
          "38:     assert(predicate2.describe.equals(\"a.`b.c` = 1\"))",
          "40:     val predicate3 =",
          "41:       new Predicate(\"=\", Array[Expression](ref(\"`a`.b\", \"c\"), LiteralValue(1, IntegerType)))",
          "42:     assert(predicate3.references.map(_.describe()).toSeq == Seq(\"```a``.b`.c\"))",
          "43:     assert(predicate3.describe.equals(\"```a``.b`.c = 1\"))",
          "44:   }",
          "46:   test(\"AlwaysTrue\") {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "35:     val v1Filter1 = EqualTo(ref(\"a\", \"B\").describe(), 1)",
          "36:     assert(v1Filter1.toV2 == predicate1)",
          "42:     val v1Filter2 = EqualTo(ref(\"a\", \"b.c\").describe(), 1)",
          "43:     assert(v1Filter2.toV2 == predicate2)",
          "49:     val v1Filter3 = EqualTo(ref(\"`a`.b\", \"c\").describe(), 1)",
          "50:     assert(v1Filter3.toV2 == predicate3)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "49:     assert(predicate1.equals(predicate2))",
          "50:     assert(predicate1.references.map(_.describe()).length == 0)",
          "51:     assert(predicate1.describe.equals(\"TRUE\"))",
          "52:   }",
          "54:   test(\"AlwaysFalse\") {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "60:     val v1Filter = V1AlwaysTrue",
          "61:     assert(v1Filter.toV2 == predicate1)",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "57:     assert(predicate1.equals(predicate2))",
          "58:     assert(predicate1.references.map(_.describe()).length == 0)",
          "59:     assert(predicate1.describe.equals(\"FALSE\"))",
          "60:   }",
          "62:   test(\"EqualTo\") {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "71:     val v1Filter = V1AlwaysFalse",
          "72:     assert(v1Filter.toV2 == predicate1)",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "65:     assert(predicate1.equals(predicate2))",
          "66:     assert(predicate1.references.map(_.describe()).toSeq == Seq(\"a\"))",
          "67:     assert(predicate1.describe.equals(\"a = 1\"))",
          "68:   }",
          "70:   test(\"EqualNullSafe\") {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "82:     val v1Filter = EqualTo(\"a\", 1)",
          "83:     assert(v1Filter.toV2 == predicate1)",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "73:     assert(predicate1.equals(predicate2))",
          "74:     assert(predicate1.references.map(_.describe()).toSeq == Seq(\"a\"))",
          "75:     assert(predicate1.describe.equals(\"(a = 1) OR (a IS NULL AND 1 IS NULL)\"))",
          "76:   }",
          "78:   test(\"In\") {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "93:     val v1Filter = EqualNullSafe(\"a\", 1)",
          "94:     assert(v1Filter.toV2 == predicate1)",
          "95:   }",
          "97:   test(\"LessThan\") {",
          "98:     val predicate1 = new Predicate(\"<\", Array[Expression](ref(\"a\"), LiteralValue(1, IntegerType)))",
          "99:     val predicate2 = new Predicate(\"<\", Array[Expression](ref(\"a\"), LiteralValue(1, IntegerType)))",
          "100:     assert(predicate1.equals(predicate2))",
          "101:     assert(predicate1.references.map(_.describe()).toSeq == Seq(\"a\"))",
          "102:     assert(predicate1.describe.equals(\"a < 1\"))",
          "104:     val v1Filter = LessThan(\"a\", 1)",
          "105:     assert(v1Filter.toV2 == predicate1)",
          "106:   }",
          "108:   test(\"LessThanOrEqual\") {",
          "109:     val predicate1 = new Predicate(\"<=\", Array[Expression](ref(\"a\"), LiteralValue(1, IntegerType)))",
          "110:     val predicate2 = new Predicate(\"<=\", Array[Expression](ref(\"a\"), LiteralValue(1, IntegerType)))",
          "111:     assert(predicate1.equals(predicate2))",
          "112:     assert(predicate1.references.map(_.describe()).toSeq == Seq(\"a\"))",
          "113:     assert(predicate1.describe.equals(\"a <= 1\"))",
          "115:     val v1Filter = LessThanOrEqual(\"a\", 1)",
          "116:     assert(v1Filter.toV2 == predicate1)",
          "117:   }",
          "119:   test(\"GreatThan\") {",
          "120:     val predicate1 = new Predicate(\">\", Array[Expression](ref(\"a\"), LiteralValue(1, IntegerType)))",
          "121:     val predicate2 = new Predicate(\">\", Array[Expression](ref(\"a\"), LiteralValue(1, IntegerType)))",
          "122:     assert(predicate1.equals(predicate2))",
          "123:     assert(predicate1.references.map(_.describe()).toSeq == Seq(\"a\"))",
          "124:     assert(predicate1.describe.equals(\"a > 1\"))",
          "126:     val v1Filter = GreaterThan(\"a\", 1)",
          "127:     assert(v1Filter.toV2 == predicate1)",
          "128:   }",
          "130:   test(\"GreatThanOrEqual\") {",
          "131:     val predicate1 = new Predicate(\">=\", Array[Expression](ref(\"a\"), LiteralValue(1, IntegerType)))",
          "132:     val predicate2 = new Predicate(\">=\", Array[Expression](ref(\"a\"), LiteralValue(1, IntegerType)))",
          "133:     assert(predicate1.equals(predicate2))",
          "134:     assert(predicate1.references.map(_.describe()).toSeq == Seq(\"a\"))",
          "135:     assert(predicate1.describe.equals(\"a >= 1\"))",
          "137:     val v1Filter = GreaterThanOrEqual(\"a\", 1)",
          "138:     assert(v1Filter.toV2 == predicate1)",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "95:     expected = expected.dropRight(2)  // remove the last \", \"",
          "96:     expected += \")\"",
          "97:     assert(predicate3.describe.equals(expected))",
          "98:   }",
          "100:   test(\"IsNull\") {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "162:     val v1Filter1 = In(\"a\", Array(1, 2, 3, 4))",
          "163:     assert(v1Filter1.toV2 == predicate1)",
          "165:     val v1Filter2 = In(\"a\", values.map(_.value()))",
          "166:     assert(v1Filter2.toV2 == predicate3)",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "103:     assert(predicate1.equals(predicate2))",
          "104:     assert(predicate1.references.map(_.describe()).toSeq == Seq(\"a\"))",
          "105:     assert(predicate1.describe.equals(\"a IS NULL\"))",
          "106:   }",
          "108:   test(\"IsNotNull\") {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "176:     val v1Filter = IsNull(\"a\")",
          "177:     assert(v1Filter.toV2 == predicate1)",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "111:     assert(predicate1.equals(predicate2))",
          "112:     assert(predicate1.references.map(_.describe()).toSeq == Seq(\"a\"))",
          "113:     assert(predicate1.describe.equals(\"a IS NOT NULL\"))",
          "114:   }",
          "116:   test(\"Not\") {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "187:     val v1Filter = IsNotNull(\"a\")",
          "188:     assert(v1Filter.toV2 == predicate1)",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "121:     assert(predicate1.equals(predicate2))",
          "122:     assert(predicate1.references.map(_.describe()).toSeq == Seq(\"a\"))",
          "123:     assert(predicate1.describe.equals(\"NOT (a < 1)\"))",
          "124:   }",
          "126:   test(\"And\") {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "200:     val v1Filter = V1Not(LessThan(\"a\", 1))",
          "201:     assert(v1Filter.toV2 == predicate1)",
          "",
          "---------------",
          "--- Hunk 11 ---",
          "[Context before]",
          "133:     assert(predicate1.equals(predicate2))",
          "134:     assert(predicate1.references.map(_.describe()).toSeq == Seq(\"a\", \"b\"))",
          "135:     assert(predicate1.describe.equals(\"(a = 1) AND (b = 1)\"))",
          "136:   }",
          "138:   test(\"Or\") {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "215:     val v1Filter = V1And(EqualTo(\"a\", 1), EqualTo(\"b\", 1))",
          "216:     assert(v1Filter.toV2 == predicate1)",
          "",
          "---------------",
          "--- Hunk 12 ---",
          "[Context before]",
          "145:     assert(predicate1.equals(predicate2))",
          "146:     assert(predicate1.references.map(_.describe()).toSeq == Seq(\"a\", \"b\"))",
          "147:     assert(predicate1.describe.equals(\"(a = 1) OR (b = 1)\"))",
          "148:   }",
          "150:   test(\"StringStartsWith\") {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "230:     val v1Filter = V1Or(EqualTo(\"a\", 1), EqualTo(\"b\", 1))",
          "231:     assert(v1Filter.toV2.equals(predicate1))",
          "",
          "---------------",
          "--- Hunk 13 ---",
          "[Context before]",
          "156:     assert(predicate1.equals(predicate2))",
          "157:     assert(predicate1.references.map(_.describe()).toSeq == Seq(\"a\"))",
          "158:     assert(predicate1.describe.equals(\"a LIKE 'str%'\"))",
          "159:   }",
          "161:   test(\"StringEndsWith\") {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "244:     val v1Filter = StringStartsWith(\"a\", \"str\")",
          "245:     assert(v1Filter.toV2.equals(predicate1))",
          "",
          "---------------",
          "--- Hunk 14 ---",
          "[Context before]",
          "167:     assert(predicate1.equals(predicate2))",
          "168:     assert(predicate1.references.map(_.describe()).toSeq == Seq(\"a\"))",
          "169:     assert(predicate1.describe.equals(\"a LIKE '%str'\"))",
          "170:   }",
          "172:   test(\"StringContains\") {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "258:     val v1Filter = StringEndsWith(\"a\", \"str\")",
          "259:     assert(v1Filter.toV2.equals(predicate1))",
          "",
          "---------------",
          "--- Hunk 15 ---",
          "[Context before]",
          "178:     assert(predicate1.equals(predicate2))",
          "179:     assert(predicate1.references.map(_.describe()).toSeq == Seq(\"a\"))",
          "180:     assert(predicate1.describe.equals(\"a LIKE '%str%'\"))",
          "181:   }",
          "182: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "272:     val v1Filter = StringContains(\"a\", \"str\")",
          "273:     assert(v1Filter.toV2.equals(predicate1))",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "1e7cdda24175ad076ccb059499b82384ce47f868",
      "candidate_info": {
        "commit_hash": "1e7cdda24175ad076ccb059499b82384ce47f868",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/1e7cdda24175ad076ccb059499b82384ce47f868",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/BloomFilterMightContain.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/MergeScalarSubqueries.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/PushdownPredicatesAndPruneColumnsForCTEDef.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/ReplaceCTERefWithRepartition.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/trees/TreePatterns.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/MergeScalarSubqueriesSuite.scala",
          "sql/core/src/main/java/org/apache/spark/sql/execution/UnsafeFixedWidthAggregationMap.java",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkOptimizer.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggUtils.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala",
          "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q9.sf100/explain.txt",
          "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q9.sf100/simplified.txt",
          "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q9/explain.txt",
          "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q9/simplified.txt",
          "sql/core/src/test/scala/org/apache/spark/sql/InjectRuntimeFilterSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/SubquerySuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/LogicalPlanTagInSparkPlanSuite.scala"
        ],
        "message": "[SPARK-34079][SQL] Merge non-correlated scalar subqueries\n\n### What changes were proposed in this pull request?\nThis PR adds a new optimizer rule `MergeScalarSubqueries` to merge multiple non-correlated `ScalarSubquery`s to compute multiple scalar values once.\n\nE.g. the following query:\n```\nSELECT\n  (SELECT avg(a) FROM t),\n  (SELECT sum(b) FROM t)\n```\nis optimized from:\n```\n== Optimized Logical Plan ==\nProject [scalar-subquery#242 [] AS scalarsubquery()#253, scalar-subquery#243 [] AS scalarsubquery()#254L]\n:  :- Aggregate [avg(a#244) AS avg(a)#247]\n:  :  +- Project [a#244]\n:  :     +- Relation default.t[a#244,b#245] parquet\n:  +- Aggregate [sum(a#251) AS sum(a)#250L]\n:     +- Project [a#251]\n:        +- Relation default.t[a#251,b#252] parquet\n+- OneRowRelation\n```\nto:\n```\n== Optimized Logical Plan ==\nProject [scalar-subquery#242 [].avg(a) AS scalarsubquery()#253, scalar-subquery#243 [].sum(a) AS scalarsubquery()#254L]\n:  :- Project [named_struct(avg(a), avg(a)#247, sum(a), sum(a)#250L) AS mergedValue#260]\n:  :  +- Aggregate [avg(a#244) AS avg(a)#247, sum(a#244) AS sum(a)#250L]\n:  :     +- Project [a#244]\n:  :        +- Relation default.t[a#244,b#245] parquet\n:  +- Project [named_struct(avg(a), avg(a)#247, sum(a), sum(a)#250L) AS mergedValue#260]\n:     +- Aggregate [avg(a#244) AS avg(a)#247, sum(a#244) AS sum(a)#250L]\n:        +- Project [a#244]\n:           +- Relation default.t[a#244,b#245] parquet\n+- OneRowRelation\n```\nand in the physical plan subqueries are reused:\n```\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=true\n+- == Final Plan ==\n   *(1) Project [Subquery subquery#242, [id=#113].avg(a) AS scalarsubquery()#253, ReusedSubquery Subquery subquery#242, [id=#113].sum(a) AS scalarsubquery()#254L]\n   :  :- Subquery subquery#242, [id=#113]\n   :  :  +- AdaptiveSparkPlan isFinalPlan=true\n         +- == Final Plan ==\n            *(2) Project [named_struct(avg(a), avg(a)#247, sum(a), sum(a)#250L) AS mergedValue#260]\n            +- *(2) HashAggregate(keys=[], functions=[avg(a#244), sum(a#244)], output=[avg(a)#247, sum(a)#250L])\n               +- ShuffleQueryStage 0\n                  +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [id=#158]\n                     +- *(1) HashAggregate(keys=[], functions=[partial_avg(a#244), partial_sum(a#244)], output=[sum#262, count#263L, sum#264L])\n                        +- *(1) ColumnarToRow\n                           +- FileScan parquet default.t[a#244] Batched: true, DataFilters: [], Format: Parquet, Location: ..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<a:int>\n         +- == Initial Plan ==\n            Project [named_struct(avg(a), avg(a)#247, sum(a), sum(a)#250L) AS mergedValue#260]\n            +- HashAggregate(keys=[], functions=[avg(a#244), sum(a#244)], output=[avg(a)#247, sum(a)#250L])\n               +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [id=#110]\n                  +- HashAggregate(keys=[], functions=[partial_avg(a#244), partial_sum(a#244)], output=[sum#262, count#263L, sum#264L])\n                     +- FileScan parquet default.t[a#244] Batched: true, DataFilters: [], Format: Parquet, Location: ..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<a:int>\n   :  +- ReusedSubquery Subquery subquery#242, [id=#113]\n   +- *(1) Scan OneRowRelation[]\n+- == Initial Plan ==\n...\n```\n\nPlease note that the above simple example could be easily optimized into a common select expression without reuse node, but this PR can handle more complex queries as well.\n\n### Why are the changes needed?\nPerformance improvement.\n```\n[info] TPCDS Snappy:                             Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative\n[info] ------------------------------------------------------------------------------------------------------------------------\n[info] q9 - MergeScalarSubqueries off                    50798          52521        1423          0.0      Infinity       1.0X\n[info] q9 - MergeScalarSubqueries on                     19484          19675         226          0.0      Infinity       2.6X\n\n[info] TPCDS Snappy:                             Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative\n[info] ------------------------------------------------------------------------------------------------------------------------\n[info] q9b - MergeScalarSubqueries off                   15430          17803         NaN          0.0      Infinity       1.0X\n[info] q9b - MergeScalarSubqueries on                     3862           4002         196          0.0      Infinity       4.0X\n```\nPlease find `q9b` in the description of SPARK-34079. It is a variant of [q9.sql](https://github.com/apache/spark/blob/master/sql/core/src/test/resources/tpcds/q9.sql) using CTE.\nThe performance improvement in case of `q9` comes from merging 15 subqueries into 5 and in case of `q9b` it comes from merging 5 subqueries into 1.\n\n### Does this PR introduce _any_ user-facing change?\nNo. But this optimization can be disabled with `spark.sql.optimizer.excludedRules` config.\n\n### How was this patch tested?\nExisting and new UTs.\n\nCloses #32298 from peter-toth/SPARK-34079-multi-column-scalar-subquery.\n\nLead-authored-by: Peter Toth <peter.toth@gmail.com>\nCo-authored-by: attilapiros <piros.attila.zsolt@gmail.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\n(cherry picked from commit e00b81ee9b37067ce8e8242907b26d3ae200f401)\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/BloomFilterMightContain.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/BloomFilterMightContain.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/MergeScalarSubqueries.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/MergeScalarSubqueries.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/PushdownPredicatesAndPruneColumnsForCTEDef.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/PushdownPredicatesAndPruneColumnsForCTEDef.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/ReplaceCTERefWithRepartition.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/ReplaceCTERefWithRepartition.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/trees/TreePatterns.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/trees/TreePatterns.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/MergeScalarSubqueriesSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/MergeScalarSubqueriesSuite.scala",
          "sql/core/src/main/java/org/apache/spark/sql/execution/UnsafeFixedWidthAggregationMap.java||sql/core/src/main/java/org/apache/spark/sql/execution/UnsafeFixedWidthAggregationMap.java",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkOptimizer.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/SparkOptimizer.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggUtils.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggUtils.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/InjectRuntimeFilterSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/InjectRuntimeFilterSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/SubquerySuite.scala||sql/core/src/test/scala/org/apache/spark/sql/SubquerySuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/LogicalPlanTagInSparkPlanSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/LogicalPlanTagInSparkPlanSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/BloomFilterMightContain.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/BloomFilterMightContain.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/BloomFilterMightContain.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/BloomFilterMightContain.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "56:           case e : Expression if e.foldable => TypeCheckResult.TypeCheckSuccess",
          "57:           case subquery : PlanExpression[_] if !subquery.containsPattern(OUTER_REFERENCE) =>",
          "58:             TypeCheckResult.TypeCheckSuccess",
          "59:           case _ =>",
          "60:             TypeCheckResult.TypeCheckFailure(s\"The Bloom filter binary input to $prettyName \" +",
          "61:               \"should be either a constant value or a scalar subquery expression\")",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "59:           case GetStructField(subquery: PlanExpression[_], _, _)",
          "60:             if !subquery.containsPattern(OUTER_REFERENCE) =>",
          "61:             TypeCheckResult.TypeCheckSuccess",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/MergeScalarSubqueries.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/MergeScalarSubqueries.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/MergeScalarSubqueries.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/MergeScalarSubqueries.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "18: package org.apache.spark.sql.catalyst.optimizer",
          "20: import scala.collection.mutable.ArrayBuffer",
          "22: import org.apache.spark.sql.catalyst.expressions._",
          "23: import org.apache.spark.sql.catalyst.expressions.aggregate.AggregateExpression",
          "24: import org.apache.spark.sql.catalyst.plans.logical.{Aggregate, CTERelationDef, CTERelationRef, Filter, Join, LogicalPlan, Project, Subquery, WithCTE}",
          "25: import org.apache.spark.sql.catalyst.rules.Rule",
          "26: import org.apache.spark.sql.catalyst.trees.TreePattern.{SCALAR_SUBQUERY, SCALAR_SUBQUERY_REFERENCE, TreePattern}",
          "27: import org.apache.spark.sql.internal.SQLConf",
          "28: import org.apache.spark.sql.types.DataType",
          "103: object MergeScalarSubqueries extends Rule[LogicalPlan] with PredicateHelper {",
          "104:   def apply(plan: LogicalPlan): LogicalPlan = {",
          "105:     plan match {",
          "107:       case _ if !conf.getConf(SQLConf.SUBQUERY_REUSE_ENABLED) => plan",
          "110:       case _: Subquery => plan",
          "113:       case _: WithCTE => plan",
          "115:       case _ => extractCommonScalarSubqueries(plan)",
          "116:     }",
          "117:   }",
          "130:   case class Header(attributes: Seq[Attribute], plan: LogicalPlan, merged: Boolean)",
          "132:   private def extractCommonScalarSubqueries(plan: LogicalPlan) = {",
          "133:     val cache = ArrayBuffer.empty[Header]",
          "134:     val planWithReferences = insertReferences(plan, cache)",
          "135:     cache.zipWithIndex.foreach { case (header, i) =>",
          "136:       cache(i) = cache(i).copy(plan =",
          "137:         if (header.merged) {",
          "138:           CTERelationDef(",
          "139:             createProject(header.attributes, removeReferences(header.plan, cache)),",
          "140:             underSubquery = true)",
          "141:         } else {",
          "142:           removeReferences(header.plan, cache)",
          "143:         })",
          "144:     }",
          "145:     val newPlan = removeReferences(planWithReferences, cache)",
          "146:     val subqueryCTEs = cache.filter(_.merged).map(_.plan.asInstanceOf[CTERelationDef])",
          "147:     if (subqueryCTEs.nonEmpty) {",
          "148:       WithCTE(newPlan, subqueryCTEs.toSeq)",
          "149:     } else {",
          "150:       newPlan",
          "151:     }",
          "152:   }",
          "155:   private def insertReferences(plan: LogicalPlan, cache: ArrayBuffer[Header]): LogicalPlan = {",
          "156:     plan.transformUpWithSubqueries {",
          "157:       case n => n.transformExpressionsUpWithPruning(_.containsAnyPattern(SCALAR_SUBQUERY)) {",
          "158:         case s: ScalarSubquery if !s.isCorrelated && s.deterministic =>",
          "159:           val (subqueryIndex, headerIndex) = cacheSubquery(s.plan, cache)",
          "160:           ScalarSubqueryReference(subqueryIndex, headerIndex, s.dataType, s.exprId)",
          "161:       }",
          "162:     }",
          "163:   }",
          "167:   private def cacheSubquery(plan: LogicalPlan, cache: ArrayBuffer[Header]): (Int, Int) = {",
          "168:     val output = plan.output.head",
          "169:     cache.zipWithIndex.collectFirst(Function.unlift { case (header, subqueryIndex) =>",
          "170:       checkIdenticalPlans(plan, header.plan).map { outputMap =>",
          "171:         val mappedOutput = mapAttributes(output, outputMap)",
          "172:         val headerIndex = header.attributes.indexWhere(_.exprId == mappedOutput.exprId)",
          "173:         subqueryIndex -> headerIndex",
          "174:       }.orElse(tryMergePlans(plan, header.plan).map {",
          "175:         case (mergedPlan, outputMap) =>",
          "176:           val mappedOutput = mapAttributes(output, outputMap)",
          "177:           var headerIndex = header.attributes.indexWhere(_.exprId == mappedOutput.exprId)",
          "178:           val newHeaderAttributes = if (headerIndex == -1) {",
          "179:             headerIndex = header.attributes.size",
          "180:             header.attributes :+ mappedOutput",
          "181:           } else {",
          "182:             header.attributes",
          "183:           }",
          "184:           cache(subqueryIndex) = Header(newHeaderAttributes, mergedPlan, true)",
          "185:           subqueryIndex -> headerIndex",
          "186:       })",
          "187:     }).getOrElse {",
          "188:       cache += Header(Seq(output), plan, false)",
          "189:       cache.length - 1 -> 0",
          "190:     }",
          "191:   }",
          "194:   private def checkIdenticalPlans(",
          "195:       newPlan: LogicalPlan,",
          "196:       cachedPlan: LogicalPlan): Option[AttributeMap[Attribute]] = {",
          "197:     if (newPlan.canonicalized == cachedPlan.canonicalized) {",
          "198:       Some(AttributeMap(newPlan.output.zip(cachedPlan.output)))",
          "199:     } else {",
          "200:       None",
          "201:     }",
          "202:   }",
          "208:   private def tryMergePlans(",
          "209:       newPlan: LogicalPlan,",
          "210:       cachedPlan: LogicalPlan): Option[(LogicalPlan, AttributeMap[Attribute])] = {",
          "211:     checkIdenticalPlans(newPlan, cachedPlan).map(cachedPlan -> _).orElse(",
          "212:       (newPlan, cachedPlan) match {",
          "213:         case (np: Project, cp: Project) =>",
          "214:           tryMergePlans(np.child, cp.child).map { case (mergedChild, outputMap) =>",
          "215:             val (mergedProjectList, newOutputMap) =",
          "216:               mergeNamedExpressions(np.projectList, outputMap, cp.projectList)",
          "217:             val mergedPlan = Project(mergedProjectList, mergedChild)",
          "218:             mergedPlan -> newOutputMap",
          "219:           }",
          "220:         case (np, cp: Project) =>",
          "221:           tryMergePlans(np, cp.child).map { case (mergedChild, outputMap) =>",
          "222:             val (mergedProjectList, newOutputMap) =",
          "223:               mergeNamedExpressions(np.output, outputMap, cp.projectList)",
          "224:             val mergedPlan = Project(mergedProjectList, mergedChild)",
          "225:             mergedPlan -> newOutputMap",
          "226:           }",
          "227:         case (np: Project, cp) =>",
          "228:           tryMergePlans(np.child, cp).map { case (mergedChild, outputMap) =>",
          "229:             val (mergedProjectList, newOutputMap) =",
          "230:               mergeNamedExpressions(np.projectList, outputMap, cp.output)",
          "231:             val mergedPlan = Project(mergedProjectList, mergedChild)",
          "232:             mergedPlan -> newOutputMap",
          "233:           }",
          "234:         case (np: Aggregate, cp: Aggregate) if supportedAggregateMerge(np, cp) =>",
          "235:           tryMergePlans(np.child, cp.child).flatMap { case (mergedChild, outputMap) =>",
          "236:             val mappedNewGroupingExpression =",
          "237:               np.groupingExpressions.map(mapAttributes(_, outputMap))",
          "241:             if (mappedNewGroupingExpression.map(_.canonicalized) ==",
          "242:               cp.groupingExpressions.map(_.canonicalized)) {",
          "243:               val (mergedAggregateExpressions, newOutputMap) =",
          "244:                 mergeNamedExpressions(np.aggregateExpressions, outputMap, cp.aggregateExpressions)",
          "245:               val mergedPlan =",
          "246:                 Aggregate(cp.groupingExpressions, mergedAggregateExpressions, mergedChild)",
          "247:               Some(mergedPlan -> newOutputMap)",
          "248:             } else {",
          "249:               None",
          "250:             }",
          "251:           }",
          "253:         case (np: Filter, cp: Filter) =>",
          "254:           tryMergePlans(np.child, cp.child).flatMap { case (mergedChild, outputMap) =>",
          "255:             val mappedNewCondition = mapAttributes(np.condition, outputMap)",
          "258:             if (mappedNewCondition.canonicalized == cp.condition.canonicalized) {",
          "259:               val mergedPlan = cp.withNewChildren(Seq(mergedChild))",
          "260:               Some(mergedPlan -> outputMap)",
          "261:             } else {",
          "262:               None",
          "263:             }",
          "264:           }",
          "266:         case (np: Join, cp: Join) if np.joinType == cp.joinType && np.hint == cp.hint =>",
          "267:           tryMergePlans(np.left, cp.left).flatMap { case (mergedLeft, leftOutputMap) =>",
          "268:             tryMergePlans(np.right, cp.right).flatMap { case (mergedRight, rightOutputMap) =>",
          "269:               val outputMap = leftOutputMap ++ rightOutputMap",
          "270:               val mappedNewCondition = np.condition.map(mapAttributes(_, outputMap))",
          "273:               if (mappedNewCondition.map(_.canonicalized) == cp.condition.map(_.canonicalized)) {",
          "274:                 val mergedPlan = cp.withNewChildren(Seq(mergedLeft, mergedRight))",
          "275:                 Some(mergedPlan -> outputMap)",
          "276:               } else {",
          "277:                 None",
          "278:               }",
          "279:             }",
          "280:           }",
          "283:         case _ => None",
          "284:       })",
          "285:   }",
          "287:   private def createProject(attributes: Seq[Attribute], plan: LogicalPlan): Project = {",
          "288:     Project(",
          "289:       Seq(Alias(",
          "290:         CreateNamedStruct(attributes.flatMap(a => Seq(Literal(a.name), a))),",
          "291:         \"mergedValue\")()),",
          "292:       plan)",
          "293:   }",
          "295:   private def mapAttributes[T <: Expression](expr: T, outputMap: AttributeMap[Attribute]) = {",
          "296:     expr.transform {",
          "297:       case a: Attribute => outputMap.getOrElse(a, a)",
          "298:     }.asInstanceOf[T]",
          "299:   }",
          "304:   private def mergeNamedExpressions(",
          "305:       newExpressions: Seq[NamedExpression],",
          "306:       outputMap: AttributeMap[Attribute],",
          "307:       cachedExpressions: Seq[NamedExpression]) = {",
          "308:     val mergedExpressions = ArrayBuffer[NamedExpression](cachedExpressions: _*)",
          "309:     val newOutputMap = AttributeMap(newExpressions.map { ne =>",
          "310:       val mapped = mapAttributes(ne, outputMap)",
          "311:       val withoutAlias = mapped match {",
          "312:         case Alias(child, _) => child",
          "313:         case e => e",
          "314:       }",
          "315:       ne.toAttribute -> mergedExpressions.find {",
          "316:         case Alias(child, _) => child semanticEquals withoutAlias",
          "317:         case e => e semanticEquals withoutAlias",
          "318:       }.getOrElse {",
          "319:         mergedExpressions += mapped",
          "320:         mapped",
          "321:       }.toAttribute",
          "322:     })",
          "323:     (mergedExpressions.toSeq, newOutputMap)",
          "324:   }",
          "328:   private def supportedAggregateMerge(newPlan: Aggregate, cachedPlan: Aggregate) = {",
          "329:     val newPlanAggregateExpressions = newPlan.aggregateExpressions.flatMap(_.collect {",
          "330:       case a: AggregateExpression => a",
          "331:     })",
          "332:     val cachedPlanAggregateExpressions = cachedPlan.aggregateExpressions.flatMap(_.collect {",
          "333:       case a: AggregateExpression => a",
          "334:     })",
          "335:     val newPlanSupportsHashAggregate = Aggregate.supportsHashAggregate(",
          "336:       newPlanAggregateExpressions.flatMap(_.aggregateFunction.aggBufferAttributes))",
          "337:     val cachedPlanSupportsHashAggregate = Aggregate.supportsHashAggregate(",
          "338:       cachedPlanAggregateExpressions.flatMap(_.aggregateFunction.aggBufferAttributes))",
          "339:     newPlanSupportsHashAggregate && cachedPlanSupportsHashAggregate ||",
          "340:       newPlanSupportsHashAggregate == cachedPlanSupportsHashAggregate && {",
          "341:         val newPlanSupportsObjectHashAggregate =",
          "342:           Aggregate.supportsObjectHashAggregate(newPlanAggregateExpressions)",
          "343:         val cachedPlanSupportsObjectHashAggregate =",
          "344:           Aggregate.supportsObjectHashAggregate(cachedPlanAggregateExpressions)",
          "345:         newPlanSupportsObjectHashAggregate && cachedPlanSupportsObjectHashAggregate ||",
          "346:           newPlanSupportsObjectHashAggregate == cachedPlanSupportsObjectHashAggregate",
          "347:       }",
          "348:   }",
          "353:   private def removeReferences(",
          "354:       plan: LogicalPlan,",
          "355:       cache: ArrayBuffer[Header]) = {",
          "356:     plan.transformUpWithSubqueries {",
          "357:       case n =>",
          "358:         n.transformExpressionsWithPruning(_.containsAnyPattern(SCALAR_SUBQUERY_REFERENCE)) {",
          "359:           case ssr: ScalarSubqueryReference =>",
          "360:             val header = cache(ssr.subqueryIndex)",
          "361:             if (header.merged) {",
          "362:               val subqueryCTE = header.plan.asInstanceOf[CTERelationDef]",
          "363:               GetStructField(",
          "364:                 ScalarSubquery(",
          "365:                   CTERelationRef(subqueryCTE.id, _resolved = true, subqueryCTE.output),",
          "366:                   exprId = ssr.exprId),",
          "367:                 ssr.headerIndex)",
          "368:             } else {",
          "369:               ScalarSubquery(header.plan, exprId = ssr.exprId)",
          "370:             }",
          "371:         }",
          "372:     }",
          "373:   }",
          "374: }",
          "379: case class ScalarSubqueryReference(",
          "380:     subqueryIndex: Int,",
          "381:     headerIndex: Int,",
          "382:     dataType: DataType,",
          "383:     exprId: ExprId) extends LeafExpression with Unevaluable {",
          "384:   override def nullable: Boolean = true",
          "386:   final override val nodePatterns: Seq[TreePattern] = Seq(SCALAR_SUBQUERY_REFERENCE)",
          "388:   override def stringArgs: Iterator[Any] = Iterator(subqueryIndex, headerIndex, dataType, exprId.id)",
          "389: }",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/PushdownPredicatesAndPruneColumnsForCTEDef.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/PushdownPredicatesAndPruneColumnsForCTEDef.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/PushdownPredicatesAndPruneColumnsForCTEDef.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/PushdownPredicatesAndPruneColumnsForCTEDef.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "121:   private def pushdownPredicatesAndAttributes(",
          "122:       plan: LogicalPlan,",
          "123:       cteMap: CTEMap): LogicalPlan = plan.transformWithSubqueries {",
          "125:       val (_, _, newPreds, newAttrSet) = cteMap(id)",
          "126:       val originalPlan = originalPlanWithPredicates.map(_._1).getOrElse(child)",
          "127:       val preds = originalPlanWithPredicates.map(_._2).getOrElse(Seq.empty)",
          "",
          "[Removed Lines]",
          "124:     case cteDef @ CTERelationDef(child, id, originalPlanWithPredicates) =>",
          "",
          "[Added Lines]",
          "124:     case cteDef @ CTERelationDef(child, id, originalPlanWithPredicates, _) =>",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "169: object CleanUpTempCTEInfo extends Rule[LogicalPlan] {",
          "170:   override def apply(plan: LogicalPlan): LogicalPlan =",
          "171:     plan.transformWithPruning(_.containsPattern(CTE)) {",
          "173:         cteDef.copy(originalPlanWithPredicates = None)",
          "174:     }",
          "175: }",
          "",
          "[Removed Lines]",
          "172:       case cteDef @ CTERelationDef(_, _, Some(_)) =>",
          "",
          "[Added Lines]",
          "172:       case cteDef @ CTERelationDef(_, _, Some(_), _) =>",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/ReplaceCTERefWithRepartition.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/ReplaceCTERefWithRepartition.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/ReplaceCTERefWithRepartition.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/ReplaceCTERefWithRepartition.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "47:     case WithCTE(child, cteDefs) =>",
          "48:       cteDefs.foreach { cteDef =>",
          "49:         val inlined = replaceWithRepartition(cteDef.child, cteMap)",
          "57:         cteMap.put(cteDef.id, withRepartition)",
          "58:       }",
          "59:       replaceWithRepartition(child, cteMap)",
          "",
          "[Removed Lines]",
          "50:         val withRepartition = if (inlined.isInstanceOf[RepartitionOperation]) {",
          "53:           inlined",
          "54:         } else {",
          "55:           Repartition(conf.numShufflePartitions, shuffle = true, inlined)",
          "56:         }",
          "",
          "[Added Lines]",
          "50:         val withRepartition =",
          "51:           if (inlined.isInstanceOf[RepartitionOperation] || cteDef.underSubquery) {",
          "54:             inlined",
          "55:           } else {",
          "56:             Repartition(conf.numShufflePartitions, shuffle = true, inlined)",
          "57:           }",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "22: import org.apache.spark.sql.catalyst.catalog.{CatalogStorageFormat, CatalogTable}",
          "23: import org.apache.spark.sql.catalyst.catalog.CatalogTable.VIEW_STORING_ANALYZED_PLAN",
          "24: import org.apache.spark.sql.catalyst.expressions._",
          "26: import org.apache.spark.sql.catalyst.plans._",
          "27: import org.apache.spark.sql.catalyst.plans.physical.{HashPartitioning, Partitioning, RangePartitioning, RoundRobinPartitioning, SinglePartition}",
          "28: import org.apache.spark.sql.catalyst.trees.TreeNodeTag",
          "",
          "[Removed Lines]",
          "25: import org.apache.spark.sql.catalyst.expressions.aggregate.AggregateExpression",
          "",
          "[Added Lines]",
          "25: import org.apache.spark.sql.catalyst.expressions.aggregate.{AggregateExpression, TypedImperativeAggregate}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "667: case class CTERelationDef(",
          "668:     child: LogicalPlan,",
          "669:     id: Long = CTERelationDef.newId,",
          "672:   final override val nodePatterns: Seq[TreePattern] = Seq(CTE)",
          "",
          "[Removed Lines]",
          "670:     originalPlanWithPredicates: Option[(LogicalPlan, Seq[Expression])] = None) extends UnaryNode {",
          "",
          "[Added Lines]",
          "672:     originalPlanWithPredicates: Option[(LogicalPlan, Seq[Expression])] = None,",
          "673:     underSubquery: Boolean = false) extends UnaryNode {",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "678: }",
          "680: object CTERelationDef {",
          "682:   def newId: Long = curId.getAndIncrement()",
          "683: }",
          "693: case class CTERelationRef(",
          "694:     cteId: Long,",
          "",
          "[Removed Lines]",
          "681:   private val curId = new java.util.concurrent.atomic.AtomicLong()",
          "",
          "[Added Lines]",
          "684:   private[sql] val curId = new java.util.concurrent.atomic.AtomicLong()",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "1014:   }",
          "1015: }",
          "1017: case class Window(",
          "1018:     windowExpressions: Seq[NamedExpression],",
          "1019:     partitionSpec: Seq[Expression],",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1022: object Aggregate {",
          "1023:   def isAggregateBufferMutable(schema: StructType): Boolean = {",
          "1024:     schema.forall(f => UnsafeRow.isMutable(f.dataType))",
          "1025:   }",
          "1027:   def supportsHashAggregate(aggregateBufferAttributes: Seq[Attribute]): Boolean = {",
          "1028:     val aggregationBufferSchema = StructType.fromAttributes(aggregateBufferAttributes)",
          "1029:     isAggregateBufferMutable(aggregationBufferSchema)",
          "1030:   }",
          "1032:   def supportsObjectHashAggregate(aggregateExpressions: Seq[AggregateExpression]): Boolean = {",
          "1033:     aggregateExpressions.map(_.aggregateFunction).exists {",
          "1034:       case _: TypedImperativeAggregate[_] => true",
          "1035:       case _ => false",
          "1036:     }",
          "1037:   }",
          "1038: }",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/trees/TreePatterns.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/trees/TreePatterns.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/trees/TreePatterns.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/trees/TreePatterns.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "77:   val REGEXP_REPLACE: Value = Value",
          "78:   val RUNTIME_REPLACEABLE: Value = Value",
          "79:   val SCALAR_SUBQUERY: Value = Value",
          "80:   val SCALA_UDF: Value = Value",
          "81:   val SORT: Value = Value",
          "82:   val SUBQUERY_ALIAS: Value = Value",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "80:   val SCALAR_SUBQUERY_REFERENCE: Value = Value",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/MergeScalarSubqueriesSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/MergeScalarSubqueriesSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/MergeScalarSubqueriesSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/MergeScalarSubqueriesSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "18: package org.apache.spark.sql.catalyst.optimizer",
          "20: import org.apache.spark.sql.catalyst.dsl.expressions._",
          "21: import org.apache.spark.sql.catalyst.dsl.plans._",
          "22: import org.apache.spark.sql.catalyst.expressions.{Attribute, CreateNamedStruct, GetStructField, Literal, ScalarSubquery}",
          "23: import org.apache.spark.sql.catalyst.expressions.aggregate.{CollectList, CollectSet}",
          "24: import org.apache.spark.sql.catalyst.plans._",
          "25: import org.apache.spark.sql.catalyst.plans.logical._",
          "26: import org.apache.spark.sql.catalyst.rules._",
          "28: class MergeScalarSubqueriesSuite extends PlanTest {",
          "30:   override def beforeEach(): Unit = {",
          "31:     CTERelationDef.curId.set(0)",
          "32:   }",
          "34:   private object Optimize extends RuleExecutor[LogicalPlan] {",
          "35:     val batches = Batch(\"MergeScalarSubqueries\", Once, MergeScalarSubqueries) :: Nil",
          "36:   }",
          "38:   val testRelation = LocalRelation('a.int, 'b.int, 'c.string)",
          "40:   private def definitionNode(plan: LogicalPlan, cteIndex: Int) = {",
          "41:     CTERelationDef(plan, cteIndex, underSubquery = true)",
          "42:   }",
          "44:   private def extractorExpression(cteIndex: Int, output: Seq[Attribute], fieldIndex: Int) = {",
          "45:     GetStructField(ScalarSubquery(CTERelationRef(cteIndex, _resolved = true, output)), fieldIndex)",
          "46:       .as(\"scalarsubquery()\")",
          "47:   }",
          "49:   test(\"Merging subqueries with projects\") {",
          "50:     val subquery1 = ScalarSubquery(testRelation.select(('a + 1).as(\"a_plus1\")))",
          "51:     val subquery2 = ScalarSubquery(testRelation.select(('a + 2).as(\"a_plus2\")))",
          "52:     val subquery3 = ScalarSubquery(testRelation.select('b))",
          "53:     val subquery4 = ScalarSubquery(testRelation.select(('a + 1).as(\"a_plus1_2\")))",
          "54:     val subquery5 = ScalarSubquery(testRelation.select(('a + 2).as(\"a_plus2_2\")))",
          "55:     val subquery6 = ScalarSubquery(testRelation.select('b.as(\"b_2\")))",
          "56:     val originalQuery = testRelation",
          "57:       .select(",
          "58:         subquery1,",
          "59:         subquery2,",
          "60:         subquery3,",
          "61:         subquery4,",
          "62:         subquery5,",
          "63:         subquery6)",
          "65:     val mergedSubquery = testRelation",
          "66:       .select(",
          "67:         ('a + 1).as(\"a_plus1\"),",
          "68:         ('a + 2).as(\"a_plus2\"),",
          "69:         'b)",
          "70:       .select(",
          "71:         CreateNamedStruct(Seq(",
          "72:           Literal(\"a_plus1\"), 'a_plus1,",
          "73:           Literal(\"a_plus2\"), 'a_plus2,",
          "74:           Literal(\"b\"), 'b",
          "75:         )).as(\"mergedValue\"))",
          "76:     val analyzedMergedSubquery = mergedSubquery.analyze",
          "77:     val correctAnswer = WithCTE(",
          "78:       testRelation",
          "79:         .select(",
          "80:           extractorExpression(0, analyzedMergedSubquery.output, 0),",
          "81:           extractorExpression(0, analyzedMergedSubquery.output, 1),",
          "82:           extractorExpression(0, analyzedMergedSubquery.output, 2),",
          "83:           extractorExpression(0, analyzedMergedSubquery.output, 0),",
          "84:           extractorExpression(0, analyzedMergedSubquery.output, 1),",
          "85:           extractorExpression(0, analyzedMergedSubquery.output, 2)),",
          "86:       Seq(definitionNode(analyzedMergedSubquery, 0)))",
          "88:     comparePlans(Optimize.execute(originalQuery.analyze), correctAnswer.analyze)",
          "89:   }",
          "91:   test(\"Merging subqueries with aggregates\") {",
          "92:     val subquery1 = ScalarSubquery(testRelation.groupBy('b)(max('a).as(\"max_a\")))",
          "93:     val subquery2 = ScalarSubquery(testRelation.groupBy('b)(sum('a).as(\"sum_a\")))",
          "94:     val subquery3 = ScalarSubquery(testRelation.groupBy('b)('b))",
          "95:     val subquery4 = ScalarSubquery(testRelation.groupBy('b)(max('a).as(\"max_a_2\")))",
          "96:     val subquery5 = ScalarSubquery(testRelation.groupBy('b)(sum('a).as(\"sum_a_2\")))",
          "97:     val subquery6 = ScalarSubquery(testRelation.groupBy('b)('b.as(\"b_2\")))",
          "98:     val originalQuery = testRelation",
          "99:       .select(",
          "100:         subquery1,",
          "101:         subquery2,",
          "102:         subquery3,",
          "103:         subquery4,",
          "104:         subquery5,",
          "105:         subquery6)",
          "107:     val mergedSubquery = testRelation",
          "108:       .groupBy('b)(",
          "109:         max('a).as(\"max_a\"),",
          "110:         sum('a).as(\"sum_a\"),",
          "111:         'b)",
          "112:       .select(CreateNamedStruct(Seq(",
          "113:         Literal(\"max_a\"), 'max_a,",
          "114:         Literal(\"sum_a\"), 'sum_a,",
          "115:         Literal(\"b\"), 'b",
          "116:       )).as(\"mergedValue\"))",
          "117:     val analyzedMergedSubquery = mergedSubquery.analyze",
          "118:     val correctAnswer = WithCTE(",
          "119:       testRelation",
          "120:         .select(",
          "121:           extractorExpression(0, analyzedMergedSubquery.output, 0),",
          "122:           extractorExpression(0, analyzedMergedSubquery.output, 1),",
          "123:           extractorExpression(0, analyzedMergedSubquery.output, 2),",
          "124:           extractorExpression(0, analyzedMergedSubquery.output, 0),",
          "125:           extractorExpression(0, analyzedMergedSubquery.output, 1),",
          "126:           extractorExpression(0, analyzedMergedSubquery.output, 2)),",
          "127:       Seq(definitionNode(analyzedMergedSubquery, 0)))",
          "129:     comparePlans(Optimize.execute(originalQuery.analyze), correctAnswer.analyze)",
          "130:   }",
          "132:   test(\"Merging subqueries with aggregates with complex grouping expressions\") {",
          "133:     val subquery1 = ScalarSubquery(testRelation.groupBy('b > 1 && 'a === 2)(max('a).as(\"max_a\")))",
          "134:     val subquery2 = ScalarSubquery(",
          "135:       testRelation",
          "136:         .select('a, 'b.as(\"b_2\"))",
          "137:         .groupBy(Literal(2) === 'a && Literal(1) < 'b_2)(sum('a).as(\"sum_a\")))",
          "139:     val originalQuery = testRelation",
          "140:       .select(",
          "141:         subquery1,",
          "142:         subquery2)",
          "144:     val mergedSubquery = testRelation",
          "145:       .select('a, 'b, 'c)",
          "146:       .groupBy('b > 1 && 'a === 2)(",
          "147:         max('a).as(\"max_a\"),",
          "148:         sum('a).as(\"sum_a\"))",
          "149:       .select(CreateNamedStruct(Seq(",
          "150:         Literal(\"max_a\"), 'max_a,",
          "151:         Literal(\"sum_a\"), 'sum_a",
          "152:       )).as(\"mergedValue\"))",
          "153:     val analyzedMergedSubquery = mergedSubquery.analyze",
          "154:     val correctAnswer = WithCTE(",
          "155:       testRelation",
          "156:         .select(",
          "157:           extractorExpression(0, analyzedMergedSubquery.output, 0),",
          "158:           extractorExpression(0, analyzedMergedSubquery.output, 1)),",
          "159:       Seq(definitionNode(analyzedMergedSubquery, 0)))",
          "161:     comparePlans(Optimize.execute(originalQuery.analyze), correctAnswer.analyze)",
          "162:   }",
          "164:   test(\"Merging subqueries with aggregates with multiple grouping expressions\") {",
          "166:     val subquery1 = ScalarSubquery(testRelation.groupBy('b, 'c)(max('a).as(\"max_a\")))",
          "167:     val subquery2 = ScalarSubquery(testRelation.groupBy('b, 'c)(min('a).as(\"min_a\")))",
          "169:     val originalQuery = testRelation",
          "170:       .select(",
          "171:         subquery1,",
          "172:         subquery2)",
          "174:     val hashAggregates = testRelation",
          "175:       .groupBy('b, 'c)(",
          "176:         max('a).as(\"max_a\"),",
          "177:         min('a).as(\"min_a\"))",
          "178:       .select(CreateNamedStruct(Seq(",
          "179:         Literal(\"max_a\"), 'max_a,",
          "180:         Literal(\"min_a\"), 'min_a",
          "181:       )).as(\"mergedValue\"))",
          "182:     val analyzedHashAggregates = hashAggregates.analyze",
          "183:     val correctAnswer = WithCTE(",
          "184:       testRelation",
          "185:         .select(",
          "186:           extractorExpression(0, analyzedHashAggregates.output, 0),",
          "187:           extractorExpression(0, analyzedHashAggregates.output, 1)),",
          "188:       Seq(definitionNode(analyzedHashAggregates, 0)))",
          "190:     comparePlans(Optimize.execute(originalQuery.analyze), correctAnswer.analyze)",
          "191:   }",
          "193:   test(\"Merging subqueries with filters\") {",
          "194:     val subquery1 = ScalarSubquery(testRelation.where('a > 1).select('a))",
          "196:     val subquery2 = ScalarSubquery(testRelation.where('a > 1).select('b.as(\"b_1\")).select('b_1))",
          "199:     val subquery3 = ScalarSubquery(testRelation.select('a.as(\"a_2\")).where('a_2 > 1).select('a_2))",
          "200:     val subquery4 = ScalarSubquery(",
          "201:       testRelation.select('a.as(\"a_2\"), 'b).where('a_2 > 1).select('b.as(\"b_2\")))",
          "202:     val originalQuery = testRelation",
          "203:       .select(",
          "204:         subquery1,",
          "205:         subquery2,",
          "206:         subquery3,",
          "207:         subquery4)",
          "209:     val mergedSubquery = testRelation",
          "210:       .select('a, 'b, 'c)",
          "211:       .where('a > 1)",
          "212:       .select('a, 'b, 'c)",
          "213:       .select('a, 'b)",
          "214:       .select(CreateNamedStruct(Seq(",
          "215:         Literal(\"a\"), 'a,",
          "216:         Literal(\"b\"), 'b",
          "217:       )).as(\"mergedValue\"))",
          "218:     val analyzedMergedSubquery = mergedSubquery.analyze",
          "219:     val correctAnswer = WithCTE(",
          "220:       testRelation",
          "221:         .select(",
          "222:           extractorExpression(0, analyzedMergedSubquery.output, 0),",
          "223:           extractorExpression(0, analyzedMergedSubquery.output, 1),",
          "224:           extractorExpression(0, analyzedMergedSubquery.output, 0),",
          "225:           extractorExpression(0, analyzedMergedSubquery.output, 1)),",
          "226:       Seq(definitionNode(analyzedMergedSubquery, 0)))",
          "228:     comparePlans(Optimize.execute(originalQuery.analyze), correctAnswer.analyze)",
          "229:   }",
          "231:   test(\"Merging subqueries with complex filter conditions\") {",
          "232:     val subquery1 = ScalarSubquery(testRelation.where('a > 1 && 'b === 2).select('a))",
          "233:     val subquery2 = ScalarSubquery(",
          "234:       testRelation",
          "235:         .select('a.as(\"a_2\"), 'b)",
          "236:         .where(Literal(2) === 'b && Literal(1) < 'a_2)",
          "237:         .select('b.as(\"b_2\")))",
          "238:     val originalQuery = testRelation",
          "239:       .select(",
          "240:         subquery1,",
          "241:         subquery2)",
          "243:     val mergedSubquery = testRelation",
          "244:       .select('a, 'b, 'c)",
          "245:       .where('a > 1 && 'b === 2)",
          "246:       .select('a, 'b.as(\"b_2\"))",
          "247:       .select(CreateNamedStruct(Seq(",
          "248:         Literal(\"a\"), 'a,",
          "249:         Literal(\"b_2\"), 'b_2",
          "250:       )).as(\"mergedValue\"))",
          "251:     val analyzedMergedSubquery = mergedSubquery.analyze",
          "252:     val correctAnswer = WithCTE(",
          "253:       testRelation",
          "254:         .select(",
          "255:           extractorExpression(0, analyzedMergedSubquery.output, 0),",
          "256:           extractorExpression(0, analyzedMergedSubquery.output, 1)),",
          "257:       Seq(definitionNode(analyzedMergedSubquery, 0)))",
          "259:     comparePlans(Optimize.execute(originalQuery.analyze), correctAnswer.analyze)",
          "260:   }",
          "262:   test(\"Do not merge subqueries with different filter conditions\") {",
          "263:     val subquery1 = ScalarSubquery(testRelation.where('a > 1).select('a))",
          "264:     val subquery2 = ScalarSubquery(testRelation.where('a < 1).select('a))",
          "266:     val originalQuery = testRelation",
          "267:       .select(",
          "268:         subquery1,",
          "269:         subquery2)",
          "271:     comparePlans(Optimize.execute(originalQuery.analyze), originalQuery.analyze)",
          "272:   }",
          "274:   test(\"Merging subqueries with aggregate filters\") {",
          "275:     val subquery1 = ScalarSubquery(",
          "276:       testRelation.having('b)(max('a).as(\"max_a\"))(max('a) > 1))",
          "277:     val subquery2 = ScalarSubquery(",
          "278:       testRelation.having('b)(sum('a).as(\"sum_a\"))(max('a) > 1))",
          "279:     val originalQuery = testRelation.select(",
          "280:       subquery1,",
          "281:       subquery2)",
          "283:     val mergedSubquery = testRelation",
          "284:       .having('b)(",
          "285:         max('a).as(\"max_a\"),",
          "286:         sum('a).as(\"sum_a\"))('max_a > 1)",
          "287:       .select(",
          "288:         'max_a,",
          "289:         'sum_a)",
          "290:       .select(CreateNamedStruct(Seq(",
          "291:         Literal(\"max_a\"), 'max_a,",
          "292:         Literal(\"sum_a\"), 'sum_a",
          "293:       )).as(\"mergedValue\"))",
          "294:     val analyzedMergedSubquery = mergedSubquery.analyze",
          "295:     val correctAnswer = WithCTE(",
          "296:       testRelation",
          "297:         .select(",
          "298:           extractorExpression(0, analyzedMergedSubquery.output, 0),",
          "299:           extractorExpression(0, analyzedMergedSubquery.output, 1)),",
          "300:       Seq(definitionNode(analyzedMergedSubquery, 0)))",
          "302:     comparePlans(Optimize.execute(originalQuery.analyze), correctAnswer.analyze)",
          "303:   }",
          "305:   test(\"Merging subqueries with joins\") {",
          "306:     val subquery1 = ScalarSubquery(testRelation.as(\"t1\")",
          "307:       .join(",
          "308:         testRelation.as(\"t2\"),",
          "309:         Inner,",
          "310:         Some($\"t1.b\" === $\"t2.b\"))",
          "311:       .select($\"t1.a\").analyze)",
          "312:     val subquery2 = ScalarSubquery(testRelation.as(\"t1\")",
          "313:       .select('a.as(\"a_1\"), 'b.as(\"b_1\"), 'c.as(\"c_1\"))",
          "314:       .join(",
          "315:         testRelation.as(\"t2\").select('a.as(\"a_2\"), 'b.as(\"b_2\"), 'c.as(\"c_2\")),",
          "316:         Inner,",
          "317:         Some('b_1 === 'b_2))",
          "318:       .select('c_2).analyze)",
          "319:     val originalQuery = testRelation.select(",
          "320:       subquery1,",
          "321:       subquery2)",
          "323:     val mergedSubquery = testRelation.as(\"t1\")",
          "324:       .select('a, 'b, 'c)",
          "325:       .join(",
          "326:         testRelation.as(\"t2\").select('a, 'b, 'c),",
          "327:         Inner,",
          "328:         Some($\"t1.b\" === $\"t2.b\"))",
          "329:       .select($\"t1.a\", $\"t2.c\")",
          "330:       .select(CreateNamedStruct(Seq(",
          "331:         Literal(\"a\"), 'a,",
          "332:         Literal(\"c\"), 'c",
          "333:       )).as(\"mergedValue\"))",
          "334:     val analyzedMergedSubquery = mergedSubquery.analyze",
          "335:     val correctAnswer = WithCTE(",
          "336:       testRelation",
          "337:         .select(",
          "338:           extractorExpression(0, analyzedMergedSubquery.output, 0),",
          "339:           extractorExpression(0, analyzedMergedSubquery.output, 1)),",
          "340:       Seq(definitionNode(analyzedMergedSubquery, 0)))",
          "342:     comparePlans(Optimize.execute(originalQuery.analyze), correctAnswer.analyze)",
          "343:   }",
          "345:   test(\"Merge subqueries with complex join conditions\") {",
          "346:     val subquery1 = ScalarSubquery(testRelation.as(\"t1\")",
          "347:       .join(",
          "348:         testRelation.as(\"t2\"),",
          "349:         Inner,",
          "350:         Some($\"t1.b\" < $\"t2.b\" && $\"t1.a\" === $\"t2.c\"))",
          "351:       .select($\"t1.a\").analyze)",
          "352:     val subquery2 = ScalarSubquery(testRelation.as(\"t1\")",
          "353:       .select('a.as(\"a_1\"), 'b.as(\"b_1\"), 'c.as(\"c_1\"))",
          "354:       .join(",
          "355:         testRelation.as(\"t2\").select('a.as(\"a_2\"), 'b.as(\"b_2\"), 'c.as(\"c_2\")),",
          "356:         Inner,",
          "357:         Some('c_2 === 'a_1 && 'b_1 < 'b_2))",
          "358:       .select('c_2).analyze)",
          "359:     val originalQuery = testRelation.select(",
          "360:       subquery1,",
          "361:       subquery2)",
          "363:     val mergedSubquery = testRelation.as(\"t1\")",
          "364:       .select('a, 'b, 'c)",
          "365:       .join(",
          "366:         testRelation.as(\"t2\").select('a, 'b, 'c),",
          "367:         Inner,",
          "368:         Some($\"t1.b\" < $\"t2.b\" && $\"t1.a\" === $\"t2.c\"))",
          "369:       .select($\"t1.a\", $\"t2.c\")",
          "370:       .select(CreateNamedStruct(Seq(",
          "371:         Literal(\"a\"), 'a,",
          "372:         Literal(\"c\"), 'c",
          "373:       )).as(\"mergedValue\"))",
          "374:     val analyzedMergedSubquery = mergedSubquery.analyze",
          "375:     val correctAnswer = WithCTE(",
          "376:       testRelation",
          "377:         .select(",
          "378:           extractorExpression(0, analyzedMergedSubquery.output, 0),",
          "379:           extractorExpression(0, analyzedMergedSubquery.output, 1)),",
          "380:       Seq(definitionNode(analyzedMergedSubquery, 0)))",
          "382:     comparePlans(Optimize.execute(originalQuery.analyze), correctAnswer.analyze)",
          "383:   }",
          "385:   test(\"Do not merge subqueries with different join types\") {",
          "386:     val subquery1 = ScalarSubquery(testRelation.as(\"t1\")",
          "387:       .join(",
          "388:         testRelation.as(\"t2\"),",
          "389:         Inner,",
          "390:         Some($\"t1.b\" === $\"t2.b\"))",
          "391:       .select($\"t1.a\"))",
          "392:     val subquery2 = ScalarSubquery(testRelation.as(\"t1\")",
          "393:       .join(",
          "394:         testRelation.as(\"t2\"),",
          "395:         LeftOuter,",
          "396:         Some($\"t1.b\" === $\"t2.b\"))",
          "397:       .select($\"t1.a\"))",
          "398:     val originalQuery = testRelation.select(",
          "399:       subquery1,",
          "400:       subquery2)",
          "402:     comparePlans(Optimize.execute(originalQuery.analyze), originalQuery.analyze)",
          "403:   }",
          "405:   test(\"Do not merge subqueries with different join conditions\") {",
          "406:     val subquery1 = ScalarSubquery(testRelation.as(\"t1\")",
          "407:       .join(",
          "408:         testRelation.as(\"t2\"),",
          "409:         Inner,",
          "410:         Some($\"t1.b\" < $\"t2.b\"))",
          "411:       .select($\"t1.a\"))",
          "412:     val subquery2 = ScalarSubquery(testRelation.as(\"t1\")",
          "413:       .join(",
          "414:         testRelation.as(\"t2\"),",
          "415:         Inner,",
          "416:         Some($\"t1.b\" > $\"t2.b\"))",
          "417:       .select($\"t1.a\"))",
          "418:     val originalQuery = testRelation.select(",
          "419:       subquery1,",
          "420:       subquery2)",
          "422:     comparePlans(Optimize.execute(originalQuery.analyze), originalQuery.analyze)",
          "423:   }",
          "425:   test(\"Do not merge subqueries with nondeterministic elements\") {",
          "426:     val subquery1 = ScalarSubquery(testRelation.select(('a + rand(0)).as(\"rand_a\")))",
          "427:     val subquery2 = ScalarSubquery(testRelation.select(('b + rand(0)).as(\"rand_b\")))",
          "428:     val originalQuery = testRelation",
          "429:       .select(",
          "430:         subquery1,",
          "431:         subquery2)",
          "433:     comparePlans(Optimize.execute(originalQuery.analyze), originalQuery.analyze)",
          "435:     val subquery3 = ScalarSubquery(testRelation.where('a > rand(0)).select('a))",
          "436:     val subquery4 = ScalarSubquery(testRelation.where('a > rand(0)).select('b))",
          "437:     val originalQuery2 = testRelation",
          "438:       .select(",
          "439:         subquery3,",
          "440:         subquery4)",
          "442:     comparePlans(Optimize.execute(originalQuery2.analyze), originalQuery2.analyze)",
          "444:     val subquery5 = ScalarSubquery(testRelation.groupBy()((max('a) + rand(0)).as(\"max_a\")))",
          "445:     val subquery6 = ScalarSubquery(testRelation.groupBy()((max('b) + rand(0)).as(\"max_b\")))",
          "446:     val originalQuery3 = testRelation",
          "447:       .select(",
          "448:         subquery5,",
          "449:         subquery6)",
          "451:     comparePlans(Optimize.execute(originalQuery3.analyze), originalQuery3.analyze)",
          "452:   }",
          "454:   test(\"Do not merge different aggregate implementations\") {",
          "456:     val subquery1 = ScalarSubquery(testRelation.groupBy('b)(max('a).as(\"max_a\")))",
          "457:     val subquery2 = ScalarSubquery(testRelation.groupBy('b)(min('a).as(\"min_a\")))",
          "460:     val subquery3 = ScalarSubquery(testRelation",
          "461:       .groupBy('b)(CollectList('a).toAggregateExpression(isDistinct = false).as(\"collectlist_a\")))",
          "462:     val subquery4 = ScalarSubquery(testRelation",
          "463:       .groupBy('b)(CollectSet('a).toAggregateExpression(isDistinct = false).as(\"collectset_a\")))",
          "466:     val subquery5 = ScalarSubquery(testRelation.groupBy('b)(max('c).as(\"max_c\")))",
          "467:     val subquery6 = ScalarSubquery(testRelation.groupBy('b)(min('c).as(\"min_c\")))",
          "469:     val originalQuery = testRelation",
          "470:       .select(",
          "471:         subquery1,",
          "472:         subquery2,",
          "473:         subquery3,",
          "474:         subquery4,",
          "475:         subquery5,",
          "476:         subquery6)",
          "478:     val hashAggregates = testRelation",
          "479:       .groupBy('b)(",
          "480:         max('a).as(\"max_a\"),",
          "481:         min('a).as(\"min_a\"))",
          "482:       .select(CreateNamedStruct(Seq(",
          "483:         Literal(\"max_a\"), 'max_a,",
          "484:         Literal(\"min_a\"), 'min_a",
          "485:       )).as(\"mergedValue\"))",
          "486:     val analyzedHashAggregates = hashAggregates.analyze",
          "487:     val objectHashAggregates = testRelation",
          "488:       .groupBy('b)(",
          "489:         CollectList('a).toAggregateExpression(isDistinct = false).as(\"collectlist_a\"),",
          "490:         CollectSet('a).toAggregateExpression(isDistinct = false).as(\"collectset_a\"))",
          "491:       .select(CreateNamedStruct(Seq(",
          "492:         Literal(\"collectlist_a\"), 'collectlist_a,",
          "493:         Literal(\"collectset_a\"), 'collectset_a",
          "494:       )).as(\"mergedValue\"))",
          "495:     val analyzedObjectHashAggregates = objectHashAggregates.analyze",
          "496:     val sortAggregates = testRelation",
          "497:       .groupBy('b)(",
          "498:         max('c).as(\"max_c\"),",
          "499:         min('c).as(\"min_c\"))",
          "500:       .select(CreateNamedStruct(Seq(",
          "501:         Literal(\"max_c\"), 'max_c,",
          "502:         Literal(\"min_c\"), 'min_c",
          "503:       )).as(\"mergedValue\"))",
          "504:     val analyzedSortAggregates = sortAggregates.analyze",
          "505:     val correctAnswer = WithCTE(",
          "506:       testRelation",
          "507:         .select(",
          "508:           extractorExpression(0, analyzedHashAggregates.output, 0),",
          "509:           extractorExpression(0, analyzedHashAggregates.output, 1),",
          "510:           extractorExpression(1, analyzedObjectHashAggregates.output, 0),",
          "511:           extractorExpression(1, analyzedObjectHashAggregates.output, 1),",
          "512:           extractorExpression(2, analyzedSortAggregates.output, 0),",
          "513:           extractorExpression(2, analyzedSortAggregates.output, 1)),",
          "514:         Seq(",
          "515:           definitionNode(analyzedHashAggregates, 0),",
          "516:           definitionNode(analyzedObjectHashAggregates, 1),",
          "517:           definitionNode(analyzedSortAggregates, 2)))",
          "519:     comparePlans(Optimize.execute(originalQuery.analyze), correctAnswer.analyze)",
          "520:   }",
          "522:   test(\"Do not merge subqueries with different aggregate grouping orders\") {",
          "524:     val subquery1 = ScalarSubquery(testRelation.groupBy('b, 'c)(max('a).as(\"max_a\")))",
          "525:     val subquery2 = ScalarSubquery(testRelation.groupBy('c, 'b)(min('a).as(\"min_a\")))",
          "527:     val originalQuery = testRelation",
          "528:       .select(",
          "529:         subquery1,",
          "530:         subquery2)",
          "532:     comparePlans(Optimize.execute(originalQuery.analyze), originalQuery.analyze)",
          "533:   }",
          "535:   test(\"Merging subqueries from different places\") {",
          "536:     val subquery1 = ScalarSubquery(testRelation.select(('a + 1).as(\"a_plus1\")))",
          "537:     val subquery2 = ScalarSubquery(testRelation.select(('a + 2).as(\"a_plus2\")))",
          "538:     val subquery3 = ScalarSubquery(testRelation.select('b))",
          "539:     val subquery4 = ScalarSubquery(testRelation.select(('a + 1).as(\"a_plus1_2\")))",
          "540:     val subquery5 = ScalarSubquery(testRelation.select(('a + 2).as(\"a_plus2_2\")))",
          "541:     val subquery6 = ScalarSubquery(testRelation.select('b.as(\"b_2\")))",
          "542:     val originalQuery = testRelation",
          "543:       .select(",
          "544:         subquery1,",
          "545:         subquery2,",
          "546:         subquery3)",
          "547:       .where(",
          "548:         subquery4 +",
          "549:         subquery5 +",
          "550:         subquery6 === 0)",
          "552:     val mergedSubquery = testRelation",
          "553:       .select(",
          "554:         ('a + 1).as(\"a_plus1\"),",
          "555:         ('a + 2).as(\"a_plus2\"),",
          "556:         'b)",
          "557:       .select(",
          "558:         CreateNamedStruct(Seq(",
          "559:           Literal(\"a_plus1\"), 'a_plus1,",
          "560:           Literal(\"a_plus2\"), 'a_plus2,",
          "561:           Literal(\"b\"), 'b",
          "562:         )).as(\"mergedValue\"))",
          "563:     val analyzedMergedSubquery = mergedSubquery.analyze",
          "564:     val correctAnswer = WithCTE(",
          "565:       testRelation",
          "566:         .select(",
          "567:           extractorExpression(0, analyzedMergedSubquery.output, 0),",
          "568:           extractorExpression(0, analyzedMergedSubquery.output, 1),",
          "569:           extractorExpression(0, analyzedMergedSubquery.output, 2))",
          "570:         .where(",
          "571:           extractorExpression(0, analyzedMergedSubquery.output, 0) +",
          "572:           extractorExpression(0, analyzedMergedSubquery.output, 1) +",
          "573:           extractorExpression(0, analyzedMergedSubquery.output, 2) === 0),",
          "574:       Seq(definitionNode(analyzedMergedSubquery, 0)))",
          "576:     comparePlans(Optimize.execute(originalQuery.analyze), correctAnswer.analyze)",
          "577:   }",
          "578: }",
          "",
          "---------------"
        ],
        "sql/core/src/main/java/org/apache/spark/sql/execution/UnsafeFixedWidthAggregationMap.java||sql/core/src/main/java/org/apache/spark/sql/execution/UnsafeFixedWidthAggregationMap.java": [
          "File: sql/core/src/main/java/org/apache/spark/sql/execution/UnsafeFixedWidthAggregationMap.java -> sql/core/src/main/java/org/apache/spark/sql/execution/UnsafeFixedWidthAggregationMap.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "25: import org.apache.spark.sql.catalyst.InternalRow;",
          "26: import org.apache.spark.sql.catalyst.expressions.UnsafeProjection;",
          "27: import org.apache.spark.sql.catalyst.expressions.UnsafeRow;",
          "29: import org.apache.spark.sql.types.StructType;",
          "30: import org.apache.spark.unsafe.KVIterator;",
          "31: import org.apache.spark.unsafe.Platform;",
          "",
          "[Removed Lines]",
          "28: import org.apache.spark.sql.types.StructField;",
          "",
          "[Added Lines]",
          "28: import org.apache.spark.sql.catalyst.plans.logical.Aggregate$;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "70:   public static boolean supportsAggregationBufferSchema(StructType schema) {",
          "77:   }",
          "",
          "[Removed Lines]",
          "71:     for (StructField field: schema.fields()) {",
          "72:       if (!UnsafeRow.isMutable(field.dataType())) {",
          "73:         return false;",
          "74:       }",
          "75:     }",
          "76:     return true;",
          "",
          "[Added Lines]",
          "71:     return Aggregate$.MODULE$.isAggregateBufferMutable(schema);",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkOptimizer.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/SparkOptimizer.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/SparkOptimizer.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/SparkOptimizer.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "54:     Batch(\"InjectRuntimeFilter\", FixedPoint(1),",
          "55:       InjectRuntimeFilter,",
          "56:       RewritePredicateSubquery) :+",
          "57:     Batch(\"Pushdown Filters from PartitionPruning\", fixedPoint,",
          "58:       PushDownPredicates) :+",
          "59:     Batch(\"Cleanup filters that cannot be pushed down\", Once,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "57:     Batch(\"MergeScalarSubqueries\", Once,",
          "58:       MergeScalarSubqueries) :+",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggUtils.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggUtils.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggUtils.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggUtils.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: import org.apache.spark.sql.AnalysisException",
          "21: import org.apache.spark.sql.catalyst.expressions._",
          "22: import org.apache.spark.sql.catalyst.expressions.aggregate._",
          "23: import org.apache.spark.sql.execution.SparkPlan",
          "24: import org.apache.spark.sql.execution.streaming._",
          "25: import org.apache.spark.sql.internal.SQLConf",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "23: import org.apache.spark.sql.catalyst.plans.logical.Aggregate",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "73:       initialInputBufferOffset: Int = 0,",
          "74:       resultExpressions: Seq[NamedExpression] = Nil,",
          "75:       child: SparkPlan): SparkPlan = {",
          "77:       aggregateExpressions.flatMap(_.aggregateFunction.aggBufferAttributes))",
          "78:     val forceSortAggregate = forceApplySortAggregate(child.conf)",
          "",
          "[Removed Lines]",
          "76:     val useHash = HashAggregateExec.supportsAggregate(",
          "",
          "[Added Lines]",
          "77:     val useHash = Aggregate.supportsHashAggregate(",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "90:         child = child)",
          "91:     } else {",
          "92:       val objectHashEnabled = child.conf.useObjectHashAggregation",
          "95:       if (objectHashEnabled && useObjectHash && !forceSortAggregate) {",
          "96:         ObjectHashAggregateExec(",
          "",
          "[Removed Lines]",
          "93:       val useObjectHash = ObjectHashAggregateExec.supportsAggregate(aggregateExpressions)",
          "",
          "[Added Lines]",
          "94:       val useObjectHash = Aggregate.supportsObjectHashAggregate(aggregateExpressions)",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "30: import org.apache.spark.sql.catalyst.expressions.aggregate._",
          "31: import org.apache.spark.sql.catalyst.expressions.codegen._",
          "32: import org.apache.spark.sql.catalyst.expressions.codegen.Block._",
          "33: import org.apache.spark.sql.catalyst.util.DateTimeConstants.NANOS_PER_MILLIS",
          "34: import org.apache.spark.sql.catalyst.util.truncatedString",
          "35: import org.apache.spark.sql.execution._",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "33: import org.apache.spark.sql.catalyst.plans.logical.Aggregate",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "55:     child: SparkPlan)",
          "56:   extends AggregateCodegenSupport {",
          "60:   override lazy val allAttributes: AttributeSeq =",
          "61:     child.output ++ aggregateBufferAttributes ++ aggregateAttributes ++",
          "",
          "[Removed Lines]",
          "58:   require(HashAggregateExec.supportsAggregate(aggregateBufferAttributes))",
          "",
          "[Added Lines]",
          "59:   require(Aggregate.supportsHashAggregate(aggregateBufferAttributes))",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "885:   override protected def withNewChildInternal(newChild: SparkPlan): HashAggregateExec =",
          "886:     copy(child = newChild)",
          "887: }",
          "",
          "[Removed Lines]",
          "889: object HashAggregateExec {",
          "890:   def supportsAggregate(aggregateBufferAttributes: Seq[Attribute]): Boolean = {",
          "891:     val aggregationBufferSchema = StructType.fromAttributes(aggregateBufferAttributes)",
          "892:     UnsafeFixedWidthAggregationMap.supportsAggregationBufferSchema(aggregationBufferSchema)",
          "893:   }",
          "894: }",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "143:   override protected def withNewChildInternal(newChild: SparkPlan): ObjectHashAggregateExec =",
          "144:     copy(child = newChild)",
          "145: }",
          "",
          "[Removed Lines]",
          "147: object ObjectHashAggregateExec {",
          "148:   def supportsAggregate(aggregateExpressions: Seq[AggregateExpression]): Boolean = {",
          "149:     aggregateExpressions.map(_.aggregateFunction).exists {",
          "150:       case _: TypedImperativeAggregate[_] => true",
          "151:       case _ => false",
          "152:     }",
          "153:   }",
          "154: }",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/InjectRuntimeFilterSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/InjectRuntimeFilterSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/InjectRuntimeFilterSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/InjectRuntimeFilterSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "264:     }",
          "265:   }",
          "280:         }.sum",
          "282:     }.sum",
          "283:     val numMightContains = plan.collect {",
          "284:       case Filter(condition, _) => condition.collect {",
          "285:         case BloomFilterMightContain(_, _) => 1",
          "286:       }.sum",
          "287:     }.sum",
          "289:     numMightContains",
          "290:   }",
          "",
          "[Removed Lines]",
          "267:   def getNumBloomFilters(plan: LogicalPlan): Integer = {",
          "268:     val numBloomFilterAggs = plan.collect {",
          "269:       case Filter(condition, _) => condition.collect {",
          "270:         case subquery: org.apache.spark.sql.catalyst.expressions.ScalarSubquery",
          "271:         => subquery.plan.collect {",
          "272:           case Aggregate(_, aggregateExpressions, _) =>",
          "273:             aggregateExpressions.map {",
          "274:               case Alias(AggregateExpression(bfAgg : BloomFilterAggregate, _, _, _, _),",
          "275:               _) =>",
          "276:                 assert(bfAgg.estimatedNumItemsExpression.isInstanceOf[Literal])",
          "277:                 assert(bfAgg.numBitsExpression.isInstanceOf[Literal])",
          "278:                 1",
          "279:             }.sum",
          "281:       }.sum",
          "288:     assert(numBloomFilterAggs == numMightContains)",
          "",
          "[Added Lines]",
          "269:   def getNumBloomFilters(plan: LogicalPlan, scalarSubqueryCTEMultiplicator: Int = 1): Integer = {",
          "270:     print(plan)",
          "271:     val numBloomFilterAggs = plan.collectWithSubqueries {",
          "272:       case Aggregate(_, aggregateExpressions, _) =>",
          "273:         aggregateExpressions.collect {",
          "274:           case Alias(AggregateExpression(bfAgg: BloomFilterAggregate, _, _, _, _), _) =>",
          "275:             assert(bfAgg.estimatedNumItemsExpression.isInstanceOf[Literal])",
          "276:             assert(bfAgg.numBitsExpression.isInstanceOf[Literal])",
          "277:             1",
          "285:     assert(numBloomFilterAggs == numMightContains * scalarSubqueryCTEMultiplicator)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "389:         planEnabled = sql(query).queryExecution.optimizedPlan",
          "390:         checkAnswer(sql(query), expectedAnswer)",
          "391:       }",
          "393:     }",
          "394:   }",
          "",
          "[Removed Lines]",
          "392:       assert(getNumBloomFilters(planEnabled) == getNumBloomFilters(planDisabled) + 2)",
          "",
          "[Added Lines]",
          "389:       assert(getNumBloomFilters(planEnabled, 2) == getNumBloomFilters(planDisabled) + 2)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "417:           checkAnswer(sql(query), expectedAnswer)",
          "418:         }",
          "419:         if (numFilterThreshold < 3) {",
          "422:         } else {",
          "424:         }",
          "425:       }",
          "426:     }",
          "",
          "[Removed Lines]",
          "420:           assert(getNumBloomFilters(planEnabled) == getNumBloomFilters(planDisabled)",
          "421:             + numFilterThreshold)",
          "423:           assert(getNumBloomFilters(planEnabled) == getNumBloomFilters(planDisabled) + 2)",
          "",
          "[Added Lines]",
          "417:           assert(getNumBloomFilters(planEnabled, numFilterThreshold) ==",
          "418:             getNumBloomFilters(planDisabled) + numFilterThreshold)",
          "420:           assert(getNumBloomFilters(planEnabled, 2) == getNumBloomFilters(planDisabled) + 2)",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/SubquerySuite.scala||sql/core/src/test/scala/org/apache/spark/sql/SubquerySuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/SubquerySuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/SubquerySuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "2018:       }.getMessage.contains(\"Correlated column is not allowed in predicate\"))",
          "2019:     }",
          "2020:   }",
          "2021: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "2022:   test(\"Merge non-correlated scalar subqueries\") {",
          "2023:     Seq(false, true).foreach { enableAQE =>",
          "2024:       withSQLConf(",
          "2025:         SQLConf.ADAPTIVE_EXECUTION_ENABLED.key -> enableAQE.toString) {",
          "2026:         val df = sql(",
          "2027:           \"\"\"",
          "2028:             |SELECT",
          "2029:             |  (SELECT avg(key) FROM testData),",
          "2030:             |  (SELECT sum(key) FROM testData),",
          "2031:             |  (SELECT count(distinct key) FROM testData)",
          "2032:           \"\"\".stripMargin)",
          "2034:         checkAnswer(df, Row(50.5, 5050, 100) :: Nil)",
          "2036:         val plan = df.queryExecution.executedPlan",
          "2037:         val subqueryIds = collectWithSubqueries(plan) { case s: SubqueryExec => s.id }",
          "2038:         val reusedSubqueryIds = collectWithSubqueries(plan) {",
          "2039:           case rs: ReusedSubqueryExec => rs.child.id",
          "2040:         }",
          "2042:         assert(subqueryIds.size == 1, \"Missing or unexpected SubqueryExec in the plan\")",
          "2043:         assert(reusedSubqueryIds.size == 2,",
          "2044:           \"Missing or unexpected reused ReusedSubqueryExec in the plan\")",
          "2045:       }",
          "2046:     }",
          "2047:   }",
          "2049:   test(\"Merge non-correlated scalar subqueries in a subquery\") {",
          "2050:     Seq(false, true).foreach { enableAQE =>",
          "2051:       withSQLConf(",
          "2052:         SQLConf.ADAPTIVE_EXECUTION_ENABLED.key -> enableAQE.toString) {",
          "2053:         val df = sql(",
          "2054:           \"\"\"",
          "2055:             |SELECT (",
          "2056:             |  SELECT",
          "2057:             |    SUM(",
          "2058:             |      (SELECT avg(key) FROM testData) +",
          "2059:             |      (SELECT sum(key) FROM testData) +",
          "2060:             |      (SELECT count(distinct key) FROM testData))",
          "2061:             |   FROM testData",
          "2062:             |)",
          "2063:           \"\"\".stripMargin)",
          "2065:         checkAnswer(df, Row(520050.0) :: Nil)",
          "2067:         val plan = df.queryExecution.executedPlan",
          "2068:         val subqueryIds = collectWithSubqueries(plan) { case s: SubqueryExec => s.id }",
          "2069:         val reusedSubqueryIds = collectWithSubqueries(plan) {",
          "2070:           case rs: ReusedSubqueryExec => rs.child.id",
          "2071:         }",
          "2073:         if (enableAQE) {",
          "2074:           assert(subqueryIds.size == 3, \"Missing or unexpected SubqueryExec in the plan\")",
          "2075:           assert(reusedSubqueryIds.size == 4,",
          "2076:             \"Missing or unexpected reused ReusedSubqueryExec in the plan\")",
          "2077:         } else {",
          "2078:           assert(subqueryIds.size == 2, \"Missing or unexpected SubqueryExec in the plan\")",
          "2079:           assert(reusedSubqueryIds.size == 5,",
          "2080:             \"Missing or unexpected reused ReusedSubqueryExec in the plan\")",
          "2081:         }",
          "2082:       }",
          "2083:     }",
          "2084:   }",
          "2086:   test(\"Merge non-correlated scalar subqueries from different levels\") {",
          "2087:     Seq(false, true).foreach { enableAQE =>",
          "2088:       withSQLConf(",
          "2089:         SQLConf.ADAPTIVE_EXECUTION_ENABLED.key -> enableAQE.toString) {",
          "2090:         val df = sql(",
          "2091:           \"\"\"",
          "2092:             |SELECT",
          "2093:             |  (SELECT avg(key) FROM testData),",
          "2094:             |  (",
          "2095:             |    SELECT",
          "2096:             |      SUM(",
          "2097:             |        (SELECT sum(key) FROM testData)",
          "2098:             |      )",
          "2099:             |    FROM testData",
          "2100:             |  )",
          "2101:           \"\"\".stripMargin)",
          "2103:         checkAnswer(df, Row(50.5, 505000) :: Nil)",
          "2105:         val plan = df.queryExecution.executedPlan",
          "2106:         val subqueryIds = collectWithSubqueries(plan) { case s: SubqueryExec => s.id }",
          "2107:         val reusedSubqueryIds = collectWithSubqueries(plan) {",
          "2108:           case rs: ReusedSubqueryExec => rs.child.id",
          "2109:         }",
          "2111:         assert(subqueryIds.size == 2, \"Missing or unexpected SubqueryExec in the plan\")",
          "2112:         assert(reusedSubqueryIds.size == 2,",
          "2113:           \"Missing or unexpected reused ReusedSubqueryExec in the plan\")",
          "2114:       }",
          "2115:     }",
          "2116:   }",
          "2118:   test(\"Merge non-correlated scalar subqueries from different parent plans\") {",
          "2119:     Seq(false, true).foreach { enableAQE =>",
          "2120:       withSQLConf(",
          "2121:         SQLConf.ADAPTIVE_EXECUTION_ENABLED.key -> enableAQE.toString) {",
          "2122:         val df = sql(",
          "2123:           \"\"\"",
          "2124:             |SELECT",
          "2125:             |  (",
          "2126:             |    SELECT",
          "2127:             |      SUM(",
          "2128:             |        (SELECT avg(key) FROM testData)",
          "2129:             |      )",
          "2130:             |    FROM testData",
          "2131:             |  ),",
          "2132:             |  (",
          "2133:             |    SELECT",
          "2134:             |      SUM(",
          "2135:             |        (SELECT sum(key) FROM testData)",
          "2136:             |      )",
          "2137:             |    FROM testData",
          "2138:             |  )",
          "2139:           \"\"\".stripMargin)",
          "2141:         checkAnswer(df, Row(5050.0, 505000) :: Nil)",
          "2143:         val plan = df.queryExecution.executedPlan",
          "2144:         val subqueryIds = collectWithSubqueries(plan) { case s: SubqueryExec => s.id }",
          "2145:         val reusedSubqueryIds = collectWithSubqueries(plan) {",
          "2146:           case rs: ReusedSubqueryExec => rs.child.id",
          "2147:         }",
          "2149:         if (enableAQE) {",
          "2150:           assert(subqueryIds.size == 3, \"Missing or unexpected SubqueryExec in the plan\")",
          "2151:           assert(reusedSubqueryIds.size == 3,",
          "2152:             \"Missing or unexpected reused ReusedSubqueryExec in the plan\")",
          "2153:         } else {",
          "2154:           assert(subqueryIds.size == 2, \"Missing or unexpected SubqueryExec in the plan\")",
          "2155:           assert(reusedSubqueryIds.size == 4,",
          "2156:             \"Missing or unexpected reused ReusedSubqueryExec in the plan\")",
          "2157:         }",
          "2158:       }",
          "2159:     }",
          "2160:   }",
          "2162:   test(\"Merge non-correlated scalar subqueries with conflicting names\") {",
          "2163:     Seq(false, true).foreach { enableAQE =>",
          "2164:       withSQLConf(",
          "2165:         SQLConf.ADAPTIVE_EXECUTION_ENABLED.key -> enableAQE.toString) {",
          "2166:         val df = sql(",
          "2167:           \"\"\"",
          "2168:             |SELECT",
          "2169:             |  (SELECT avg(key) AS key FROM testData),",
          "2170:             |  (SELECT sum(key) AS key FROM testData),",
          "2171:             |  (SELECT count(distinct key) AS key FROM testData)",
          "2172:           \"\"\".stripMargin)",
          "2174:         checkAnswer(df, Row(50.5, 5050, 100) :: Nil)",
          "2176:         val plan = df.queryExecution.executedPlan",
          "2177:         val subqueryIds = collectWithSubqueries(plan) { case s: SubqueryExec => s.id }",
          "2178:         val reusedSubqueryIds = collectWithSubqueries(plan) {",
          "2179:           case rs: ReusedSubqueryExec => rs.child.id",
          "2180:         }",
          "2182:         assert(subqueryIds.size == 1, \"Missing or unexpected SubqueryExec in the plan\")",
          "2183:         assert(reusedSubqueryIds.size == 2,",
          "2184:           \"Missing or unexpected reused ReusedSubqueryExec in the plan\")",
          "2185:       }",
          "2186:     }",
          "2187:   }",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/execution/LogicalPlanTagInSparkPlanSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/LogicalPlanTagInSparkPlanSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/execution/LogicalPlanTagInSparkPlanSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/execution/LogicalPlanTagInSparkPlanSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "22: import org.apache.spark.sql.TPCDSQuerySuite",
          "23: import org.apache.spark.sql.catalyst.expressions.aggregate.{AggregateExpression, Complete, Final}",
          "25: import org.apache.spark.sql.execution.adaptive.DisableAdaptiveExecutionSuite",
          "26: import org.apache.spark.sql.execution.aggregate.{HashAggregateExec, ObjectHashAggregateExec, SortAggregateExec}",
          "27: import org.apache.spark.sql.execution.columnar.{InMemoryRelation, InMemoryTableScanExec}",
          "",
          "[Removed Lines]",
          "24: import org.apache.spark.sql.catalyst.plans.logical.{Aggregate, Generate, Join, LocalRelation, LogicalPlan, Range, Sample, Union, Window}",
          "",
          "[Added Lines]",
          "24: import org.apache.spark.sql.catalyst.plans.logical.{Aggregate, Generate, Join, LocalRelation, LogicalPlan, Range, Sample, Union, Window, WithCTE}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "112:         val physicalLeaves = plan.collectLeaves()",
          "113:         assert(logicalLeaves.length == 1)",
          "114:         assert(physicalLeaves.length == 1)",
          "",
          "[Removed Lines]",
          "111:         val logicalLeaves = getLogicalPlan(actualPlan).collectLeaves()",
          "",
          "[Added Lines]",
          "111:         val logicalPlan = getLogicalPlan(actualPlan) match {",
          "112:           case w: WithCTE => w.plan",
          "113:           case o => o",
          "114:         }",
          "115:         val logicalLeaves = logicalPlan.collectLeaves()",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "d3aadb40370c0613c2d2ce41d8b905f0fafcd69c",
      "candidate_info": {
        "commit_hash": "d3aadb40370c0613c2d2ce41d8b905f0fafcd69c",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/d3aadb40370c0613c2d2ce41d8b905f0fafcd69c",
        "files": [
          "core/src/main/resources/error/error-classes.json",
          "core/src/test/scala/org/apache/spark/SparkThrowableSuite.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/ArithmeticExpressionSuite.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CollectionExpressionsSuite.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/ComplexTypeSuite.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/IntervalExpressionsSuite.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/StringExpressionsSuite.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/IntervalUtilsSuite.scala",
          "sql/core/src/test/resources/sql-tests/results/ansi/array.sql.out",
          "sql/core/src/test/resources/sql-tests/results/ansi/interval.sql.out",
          "sql/core/src/test/resources/sql-tests/results/interval.sql.out",
          "sql/core/src/test/resources/sql-tests/results/pivot.sql.out",
          "sql/core/src/test/resources/sql-tests/results/postgreSQL/case.sql.out",
          "sql/core/src/test/resources/sql-tests/results/postgreSQL/int8.sql.out",
          "sql/core/src/test/resources/sql-tests/results/postgreSQL/select_having.sql.out",
          "sql/core/src/test/resources/sql-tests/results/udf/postgreSQL/udf-case.sql.out",
          "sql/core/src/test/resources/sql-tests/results/udf/postgreSQL/udf-select_having.sql.out",
          "sql/core/src/test/resources/sql-tests/results/udf/udf-pivot.sql.out",
          "sql/core/src/test/scala/org/apache/spark/sql/ColumnExpressionSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/SQLViewSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/streaming/FileStreamSourceSuite.scala"
        ],
        "message": "[SPARK-39087][SQL][3.3] Improve messages of error classes\n\n### What changes were proposed in this pull request?\nIn the PR, I propose to modify error messages of the following error classes:\n- INVALID_JSON_SCHEMA_MAP_TYPE\n- INCOMPARABLE_PIVOT_COLUMN\n- INVALID_ARRAY_INDEX_IN_ELEMENT_AT\n- INVALID_ARRAY_INDEX\n- DIVIDE_BY_ZERO\n\nThis is a backport of https://github.com/apache/spark/pull/36428.\n\n### Why are the changes needed?\nTo improve readability of error messages.\n\n### Does this PR introduce _any_ user-facing change?\nYes. It changes user-facing error messages.\n\n### How was this patch tested?\nBy running the modified test suites:\n```\n$ build/sbt \"sql/testOnly *QueryCompilationErrorsSuite*\"\n$ build/sbt \"sql/testOnly *QueryExecutionErrorsSuite*\"\n$ build/sbt \"sql/testOnly *QueryExecutionAnsiErrorsSuite\"\n$ build/sbt \"test:testOnly *SparkThrowableSuite\"\n```\n\nAuthored-by: Max Gekk <max.gekkgmail.com>\nSigned-off-by: Max Gekk <max.gekkgmail.com>\n(cherry picked from commit 040526391a45ad610422a48c05aa69ba5133f922)\nSigned-off-by: Max Gekk <max.gekkgmail.com>\n\nCloses #36439 from MaxGekk/error-class-improve-msg-3.3.\n\nAuthored-by: Max Gekk <max.gekk@gmail.com>\nSigned-off-by: Max Gekk <max.gekk@gmail.com>",
        "before_after_code_files": [
          "core/src/test/scala/org/apache/spark/SparkThrowableSuite.scala||core/src/test/scala/org/apache/spark/SparkThrowableSuite.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/ArithmeticExpressionSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/ArithmeticExpressionSuite.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CollectionExpressionsSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CollectionExpressionsSuite.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/ComplexTypeSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/ComplexTypeSuite.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/IntervalExpressionsSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/IntervalExpressionsSuite.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/StringExpressionsSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/StringExpressionsSuite.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/IntervalUtilsSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/IntervalUtilsSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/ColumnExpressionSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/ColumnExpressionSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/SQLViewSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/SQLViewSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/streaming/FileStreamSourceSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/streaming/FileStreamSourceSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "core/src/test/scala/org/apache/spark/SparkThrowableSuite.scala||core/src/test/scala/org/apache/spark/SparkThrowableSuite.scala": [
          "File: core/src/test/scala/org/apache/spark/SparkThrowableSuite.scala -> core/src/test/scala/org/apache/spark/SparkThrowableSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "127:     assert(getMessage(\"DIVIDE_BY_ZERO\", Array(\"foo\", \"bar\", \"baz\")) ==",
          "129:         \"(except for ANSI interval type) to bypass this error.bar\")",
          "130:   }",
          "",
          "[Removed Lines]",
          "128:       \"divide by zero. To return NULL instead, use 'try_divide'. If necessary set foo to false \" +",
          "",
          "[Added Lines]",
          "128:       \"Division by zero. To return NULL instead, use `try_divide`. If necessary set foo to false \" +",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "76:   def unorderablePivotColError(pivotCol: Expression): Throwable = {",
          "77:     new AnalysisException(",
          "78:       errorClass = \"INCOMPARABLE_PIVOT_COLUMN\",",
          "80:   }",
          "82:   def nonLiteralPivotValError(pivotVal: Expression): Throwable = {",
          "",
          "[Removed Lines]",
          "79:       messageParameters = Array(pivotCol.toString))",
          "",
          "[Added Lines]",
          "79:       messageParameters = Array(toSQLId(pivotCol.sql)))",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "2378:   def invalidJsonSchema(schema: DataType): Throwable = {",
          "2379:     new AnalysisException(",
          "2382:   }",
          "2384:   def tableIndexNotSupportedError(errorMessage: String): Throwable = {",
          "",
          "[Removed Lines]",
          "2380:       errorClass = \"INVALID_JSON_SCHEMA_MAPTYPE\",",
          "2381:       messageParameters = Array(schema.toString))",
          "",
          "[Added Lines]",
          "2380:       errorClass = \"INVALID_JSON_SCHEMA_MAP_TYPE\",",
          "2381:       messageParameters = Array(toSQLType(schema)))",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/ArithmeticExpressionSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/ArithmeticExpressionSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/ArithmeticExpressionSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/ArithmeticExpressionSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "243:       }",
          "244:       withSQLConf(SQLConf.ANSI_ENABLED.key -> \"true\") {",
          "245:         checkExceptionInExpression[ArithmeticException](",
          "247:       }",
          "248:     }",
          "",
          "[Removed Lines]",
          "246:           Divide(left, Literal(convert(0))), \"divide by zero\")",
          "",
          "[Added Lines]",
          "246:           Divide(left, Literal(convert(0))), \"Division by zero\")",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "287:       }",
          "288:       withSQLConf(SQLConf.ANSI_ENABLED.key -> \"true\") {",
          "289:         checkExceptionInExpression[ArithmeticException](",
          "291:       }",
          "292:     }",
          "293:     checkEvaluation(IntegralDivide(positiveLongLit, negativeLongLit), 0L)",
          "",
          "[Removed Lines]",
          "290:           IntegralDivide(left, Literal(convert(0))), \"divide by zero\")",
          "",
          "[Added Lines]",
          "290:           IntegralDivide(left, Literal(convert(0))), \"Division by zero\")",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "339:       }",
          "340:       withSQLConf(SQLConf.ANSI_ENABLED.key -> \"true\") {",
          "341:         checkExceptionInExpression[ArithmeticException](",
          "343:       }",
          "344:     }",
          "345:     checkEvaluation(Remainder(positiveShortLit, positiveShortLit), 0.toShort)",
          "",
          "[Removed Lines]",
          "342:           Remainder(left, Literal(convert(0))), \"divide by zero\")",
          "",
          "[Added Lines]",
          "342:           Remainder(left, Literal(convert(0))), \"Division by zero\")",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "444:       }",
          "445:       withSQLConf(SQLConf.ANSI_ENABLED.key -> \"true\") {",
          "446:         checkExceptionInExpression[ArithmeticException](",
          "448:       }",
          "449:     }",
          "450:     checkEvaluation(Pmod(Literal(-7), Literal(3)), 2)",
          "",
          "[Removed Lines]",
          "447:           Pmod(left, Literal(convert(0))), \"divide by zero\")",
          "",
          "[Added Lines]",
          "447:           Pmod(left, Literal(convert(0))), \"Division by zero\")",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "608:     }",
          "609:     withSQLConf(SQLConf.ANSI_ENABLED.key -> \"true\") {",
          "610:       checkExceptionInExpression[ArithmeticException](",
          "612:     }",
          "614:     checkEvaluation(DecimalPrecision.decimalAndDecimal.apply(IntegralDivide(",
          "",
          "[Removed Lines]",
          "611:         IntegralDivide(Literal(Decimal(0.2)), Literal(Decimal(0.0))), \"divide by zero\")",
          "",
          "[Added Lines]",
          "611:         IntegralDivide(Literal(Decimal(0.2)), Literal(Decimal(0.0))), \"Division by zero\")",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "755:           val zero = Literal(convert(0))",
          "756:           checkEvaluation(operator(Literal.create(null, one.dataType), zero), null)",
          "757:           checkEvaluation(operator(one, Literal.create(null, zero.dataType)), null)",
          "759:         }",
          "760:       }",
          "761:     }",
          "",
          "[Removed Lines]",
          "758:           checkExceptionInExpression[ArithmeticException](operator(one, zero), \"divide by zero\")",
          "",
          "[Added Lines]",
          "758:           checkExceptionInExpression[ArithmeticException](operator(one, zero), \"Division by zero\")",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "814:           checkEvaluation(operator(Literal.create(null, one.dataType), zero), null)",
          "815:           checkEvaluation(operator(one, Literal.create(null, zero.dataType)), null)",
          "816:           checkExceptionInExpression[SparkArithmeticException](operator(one, zero),",
          "818:         }",
          "819:       }",
          "820:     }",
          "",
          "[Removed Lines]",
          "817:             \"divide by zero\")",
          "",
          "[Added Lines]",
          "817:             \"Division by zero\")",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "862:     }",
          "863:     withSQLConf(SQLConf.ANSI_ENABLED.key -> \"true\") {",
          "864:       checkExceptionInExpression[ArithmeticException](",
          "866:       checkExceptionInExpression[ArithmeticException](",
          "868:       checkExceptionInExpression[ArithmeticException](",
          "870:       checkExceptionInExpression[ArithmeticException](",
          "872:     }",
          "874:     checkEvaluation(IntegralDivide(Literal.create(null, YearMonthIntervalType()),",
          "",
          "[Removed Lines]",
          "865:         IntegralDivide(Literal(Period.ZERO), Literal(Period.ZERO)), \"divide by zero\")",
          "867:         IntegralDivide(Literal(Period.ofYears(1)), Literal(Period.ZERO)), \"divide by zero\")",
          "869:         IntegralDivide(Period.ofMonths(Int.MinValue), Literal(Period.ZERO)), \"divide by zero\")",
          "871:         IntegralDivide(Period.ofMonths(Int.MaxValue), Literal(Period.ZERO)), \"divide by zero\")",
          "",
          "[Added Lines]",
          "865:         IntegralDivide(Literal(Period.ZERO), Literal(Period.ZERO)), \"Division by zero\")",
          "867:         IntegralDivide(Literal(Period.ofYears(1)), Literal(Period.ZERO)), \"Division by zero\")",
          "869:         IntegralDivide(Period.ofMonths(Int.MinValue), Literal(Period.ZERO)), \"Division by zero\")",
          "871:         IntegralDivide(Period.ofMonths(Int.MaxValue), Literal(Period.ZERO)), \"Division by zero\")",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "914:     }",
          "915:     withSQLConf(SQLConf.ANSI_ENABLED.key -> \"true\") {",
          "916:       checkExceptionInExpression[ArithmeticException](",
          "918:       checkExceptionInExpression[ArithmeticException](",
          "919:         IntegralDivide(Literal(Duration.ofDays(1)),",
          "921:       checkExceptionInExpression[ArithmeticException](",
          "922:         IntegralDivide(Literal(Duration.of(Long.MaxValue, ChronoUnit.MICROS)),",
          "924:       checkExceptionInExpression[ArithmeticException](",
          "925:         IntegralDivide(Literal(Duration.of(Long.MinValue, ChronoUnit.MICROS)),",
          "927:     }",
          "929:     checkEvaluation(IntegralDivide(Literal.create(null, DayTimeIntervalType()),",
          "",
          "[Removed Lines]",
          "917:         IntegralDivide(Literal(Duration.ZERO), Literal(Duration.ZERO)), \"divide by zero\")",
          "920:           Literal(Duration.ZERO)), \"divide by zero\")",
          "923:           Literal(Duration.ZERO)), \"divide by zero\")",
          "926:           Literal(Duration.ZERO)), \"divide by zero\")",
          "",
          "[Added Lines]",
          "917:         IntegralDivide(Literal(Duration.ZERO), Literal(Duration.ZERO)), \"Division by zero\")",
          "920:           Literal(Duration.ZERO)), \"Division by zero\")",
          "923:           Literal(Duration.ZERO)), \"Division by zero\")",
          "926:           Literal(Duration.ZERO)), \"Division by zero\")",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CollectionExpressionsSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CollectionExpressionsSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CollectionExpressionsSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CollectionExpressionsSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "2288:         val array = Literal.create(Seq(1, 2, 3), ArrayType(IntegerType))",
          "2289:         var expr: Expression = ElementAt(array, Literal(5))",
          "2290:         if (ansiEnabled) {",
          "2292:           checkExceptionInExpression[Exception](expr, errMsg)",
          "2293:         } else {",
          "2294:           checkEvaluation(expr, null)",
          "",
          "[Removed Lines]",
          "2291:           val errMsg = \"Invalid index: 5, numElements: 3\"",
          "",
          "[Added Lines]",
          "2291:           val errMsg = \"The index 5 is out of bounds. The array has 3 elements.\"",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "2297:         expr = ElementAt(array, Literal(-5))",
          "2298:         if (ansiEnabled) {",
          "2300:           checkExceptionInExpression[Exception](expr, errMsg)",
          "2301:         } else {",
          "2302:           checkEvaluation(expr, null)",
          "",
          "[Removed Lines]",
          "2299:           val errMsg = \"Invalid index: -5, numElements: 3\"",
          "",
          "[Added Lines]",
          "2299:           val errMsg = \"The index -5 is out of bounds. The array has 3 elements.\"",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/ComplexTypeSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/ComplexTypeSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/ComplexTypeSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/ComplexTypeSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "70:         if (ansiEnabled) {",
          "71:           checkExceptionInExpression[Exception](",
          "72:             GetArrayItem(array, Literal(5)),",
          "74:           )",
          "76:           checkExceptionInExpression[Exception](",
          "77:             GetArrayItem(array, Literal(-1)),",
          "79:           )",
          "80:         } else {",
          "81:           checkEvaluation(GetArrayItem(array, Literal(5)), null)",
          "",
          "[Removed Lines]",
          "73:             \"Invalid index: 5, numElements: 2\"",
          "78:             \"Invalid index: -1, numElements: 2\"",
          "",
          "[Added Lines]",
          "73:             \"The index 5 is out of bounds. The array has 2 elements.\"",
          "78:             \"The index -1 is out of bounds. The array has 2 elements.\"",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/IntervalExpressionsSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/IntervalExpressionsSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/IntervalExpressionsSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/IntervalExpressionsSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "176:     check(\"2 years -8 seconds\", 0.5, \"4 years -16 seconds\")",
          "177:     check(\"-1 month 2 microseconds\", -0.25, \"4 months -8 microseconds\")",
          "178:     check(\"1 month 3 microsecond\", 1.5, \"2 microseconds\")",
          "180:     check(\"1 second\", 0, null, Some(false))",
          "181:     check(s\"${Int.MaxValue} months\", 0.9, \"integer overflow\", Some(true))",
          "182:     check(s\"${Int.MaxValue} months\", 0.9, Int.MaxValue + \" months\", Some(false))",
          "",
          "[Removed Lines]",
          "179:     check(\"1 second\", 0, \"divide by zero\", Some(true))",
          "",
          "[Added Lines]",
          "179:     check(\"1 second\", 0, \"Division by zero\", Some(true))",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "412:     }",
          "414:     Seq(",
          "417:       (Period.ofMonths(-100), Float.NaN) -> \"input is infinite or NaN\"",
          "418:     ).foreach { case ((period, num), expectedErrMsg) =>",
          "419:       checkExceptionInExpression[ArithmeticException](",
          "",
          "[Removed Lines]",
          "415:       (Period.ofMonths(1), 0) -> \"divide by zero\",",
          "416:       (Period.ofMonths(Int.MinValue), 0d) -> \"divide by zero\",",
          "",
          "[Added Lines]",
          "415:       (Period.ofMonths(1), 0) -> \"Division by zero\",",
          "416:       (Period.ofMonths(Int.MinValue), 0d) -> \"Division by zero\",",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "447:     }",
          "449:     Seq(",
          "452:       (Duration.ofSeconds(-100), Float.NaN) -> \"input is infinite or NaN\"",
          "453:     ).foreach { case ((period, num), expectedErrMsg) =>",
          "454:       checkExceptionInExpression[ArithmeticException](",
          "",
          "[Removed Lines]",
          "450:       (Duration.ofDays(1), 0) -> \"divide by zero\",",
          "451:       (Duration.ofMillis(Int.MinValue), 0d) -> \"divide by zero\",",
          "",
          "[Added Lines]",
          "450:       (Duration.ofDays(1), 0) -> \"Division by zero\",",
          "451:       (Duration.ofMillis(Int.MinValue), 0d) -> \"Division by zero\",",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/StringExpressionsSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/StringExpressionsSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/StringExpressionsSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/StringExpressionsSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "1215:       withSQLConf(SQLConf.ANSI_ENABLED.key -> ansiEnabled.toString) {",
          "1216:         var expr: Expression = Elt(Seq(Literal(4), Literal(\"123\"), Literal(\"456\")))",
          "1217:         if (ansiEnabled) {",
          "1219:           checkExceptionInExpression[Exception](expr, errMsg)",
          "1220:         } else {",
          "1221:           checkEvaluation(expr, null)",
          "",
          "[Removed Lines]",
          "1218:           val errMsg = \"Invalid index: 4, numElements: 2\"",
          "",
          "[Added Lines]",
          "1218:           val errMsg = \"The index 4 is out of bounds. The array has 2 elements.\"",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1224:         expr = Elt(Seq(Literal(0), Literal(\"123\"), Literal(\"456\")))",
          "1225:         if (ansiEnabled) {",
          "1227:           checkExceptionInExpression[Exception](expr, errMsg)",
          "1228:         } else {",
          "1229:           checkEvaluation(expr, null)",
          "",
          "[Removed Lines]",
          "1226:           val errMsg = \"Invalid index: 0, numElements: 2\"",
          "",
          "[Added Lines]",
          "1226:           val errMsg = \"The index 0 is out of bounds. The array has 2 elements.\"",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "1232:         expr = Elt(Seq(Literal(-1), Literal(\"123\"), Literal(\"456\")))",
          "1233:         if (ansiEnabled) {",
          "1235:           checkExceptionInExpression[Exception](expr, errMsg)",
          "1236:         } else {",
          "1237:           checkEvaluation(expr, null)",
          "",
          "[Removed Lines]",
          "1234:           val errMsg = \"Invalid index: -1, numElements: 2\"",
          "",
          "[Added Lines]",
          "1234:           val errMsg = \"The index -1 is out of bounds. The array has 2 elements.\"",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/IntervalUtilsSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/IntervalUtilsSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/IntervalUtilsSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/IntervalUtilsSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "307:     interval = new CalendarInterval(123, 456, 789)",
          "308:     assert(divide(interval, 0) === null)",
          "309:     val e2 = intercept[ArithmeticException](divideExact(interval, 0))",
          "311:   }",
          "313:   test(\"from day-time string\") {",
          "",
          "[Removed Lines]",
          "310:     assert(e2.getMessage.contains(\"divide by zero\"))",
          "",
          "[Added Lines]",
          "310:     assert(e2.getMessage.contains(\"Division by zero\"))",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/ColumnExpressionSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/ColumnExpressionSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/ColumnExpressionSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/ColumnExpressionSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "2766:       Seq((Period.ofYears(9999), 0)).toDF(\"i\", \"n\").select($\"i\" / $\"n\").collect()",
          "2767:     }.getCause",
          "2768:     assert(e.isInstanceOf[ArithmeticException])",
          "2771:     val e2 = intercept[SparkException] {",
          "2772:       Seq((Period.ofYears(9999), 0d)).toDF(\"i\", \"n\").select($\"i\" / $\"n\").collect()",
          "2773:     }.getCause",
          "2774:     assert(e2.isInstanceOf[ArithmeticException])",
          "2777:     val e3 = intercept[SparkException] {",
          "2778:       Seq((Period.ofYears(9999), BigDecimal(0))).toDF(\"i\", \"n\").select($\"i\" / $\"n\").collect()",
          "2779:     }.getCause",
          "2780:     assert(e3.isInstanceOf[ArithmeticException])",
          "2782:   }",
          "2784:   test(\"SPARK-34875: divide day-time interval by numeric\") {",
          "",
          "[Removed Lines]",
          "2769:     assert(e.getMessage.contains(\"divide by zero\"))",
          "2775:     assert(e2.getMessage.contains(\"divide by zero\"))",
          "2781:     assert(e3.getMessage.contains(\"divide by zero\"))",
          "",
          "[Added Lines]",
          "2769:     assert(e.getMessage.contains(\"Division by zero\"))",
          "2775:     assert(e2.getMessage.contains(\"Division by zero\"))",
          "2781:     assert(e3.getMessage.contains(\"Division by zero\"))",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "2813:       Seq((Duration.ofDays(9999), 0)).toDF(\"i\", \"n\").select($\"i\" / $\"n\").collect()",
          "2814:     }.getCause",
          "2815:     assert(e.isInstanceOf[ArithmeticException])",
          "2818:     val e2 = intercept[SparkException] {",
          "2819:       Seq((Duration.ofDays(9999), 0d)).toDF(\"i\", \"n\").select($\"i\" / $\"n\").collect()",
          "2820:     }.getCause",
          "2821:     assert(e2.isInstanceOf[ArithmeticException])",
          "2824:     val e3 = intercept[SparkException] {",
          "2825:       Seq((Duration.ofDays(9999), BigDecimal(0))).toDF(\"i\", \"n\").select($\"i\" / $\"n\").collect()",
          "2826:     }.getCause",
          "2827:     assert(e3.isInstanceOf[ArithmeticException])",
          "2829:   }",
          "2831:   test(\"SPARK-34896: return day-time interval from dates subtraction\") {",
          "",
          "[Removed Lines]",
          "2816:     assert(e.getMessage.contains(\"divide by zero\"))",
          "2822:     assert(e2.getMessage.contains(\"divide by zero\"))",
          "2828:     assert(e3.getMessage.contains(\"divide by zero\"))",
          "",
          "[Added Lines]",
          "2816:     assert(e.getMessage.contains(\"Division by zero\"))",
          "2822:     assert(e2.getMessage.contains(\"Division by zero\"))",
          "2828:     assert(e3.getMessage.contains(\"Division by zero\"))",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "3043:     ).foreach { case (schema, jsonData) =>",
          "3044:       withTempDir { dir =>",
          "3045:         val colName = \"col\"",
          "3048:         val thrown1 = intercept[AnalysisException](",
          "3049:           spark.read.schema(StructType(Seq(StructField(colName, schema))))",
          "",
          "[Removed Lines]",
          "3046:         val msg = \"can only contain StringType as a key type for a MapType\"",
          "",
          "[Added Lines]",
          "3046:         val msg = \"can only contain STRING as a key type for a MAP\"",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/execution/SQLViewSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/SQLViewSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/execution/SQLViewSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/execution/SQLViewSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "897:             val e = intercept[ArithmeticException] {",
          "898:               sql(\"SELECT * FROM v5\").collect()",
          "899:             }.getMessage",
          "901:           }",
          "902:         }",
          "",
          "[Removed Lines]",
          "900:             assert(e.contains(\"divide by zero\"))",
          "",
          "[Added Lines]",
          "900:             assert(e.contains(\"Division by zero\"))",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "907:         val e = intercept[ArithmeticException] {",
          "908:           sql(\"SELECT * FROM v1\").collect()",
          "909:         }.getMessage",
          "911:       }",
          "912:     }",
          "913:   }",
          "",
          "[Removed Lines]",
          "910:         assert(e.contains(\"divide by zero\"))",
          "",
          "[Added Lines]",
          "910:         assert(e.contains(\"Division by zero\"))",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/streaming/FileStreamSourceSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/streaming/FileStreamSourceSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/streaming/FileStreamSourceSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/streaming/FileStreamSourceSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "2332:     ).foreach { schema =>",
          "2333:       withTempDir { dir =>",
          "2334:         val colName = \"col\"",
          "2337:         val thrown1 = intercept[AnalysisException](",
          "2338:           spark.readStream.schema(StructType(Seq(StructField(colName, schema))))",
          "",
          "[Removed Lines]",
          "2335:         val msg = \"can only contain StringType as a key type for a MapType\"",
          "",
          "[Added Lines]",
          "2335:         val msg = \"can only contain STRING as a key type for a MAP\"",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "1ad1c18fc283acea6d18bc4c8753d3b6e50408ed",
      "candidate_info": {
        "commit_hash": "1ad1c18fc283acea6d18bc4c8753d3b6e50408ed",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/1ad1c18fc283acea6d18bc4c8753d3b6e50408ed",
        "files": [
          "core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorter.java"
        ],
        "message": "[SPARK-39283][CORE] Fix deadlock between TaskMemoryManager and UnsafeExternalSorter.SpillableIterator\n\n### What changes were proposed in this pull request?\n\nThis PR fixes a deadlock between TaskMemoryManager and UnsafeExternalSorter.SpillableIterator.\n\n### Why are the changes needed?\n\nWe are facing the deadlock issue b/w TaskMemoryManager and UnsafeExternalSorter.SpillableIterator during the join. It turns out that in UnsafeExternalSorter.SpillableIterator#spill() function, it tries to get lock on UnsafeExternalSorter`SpillableIterator` and UnsafeExternalSorter and call `freePage` to free all allocated pages except the last one which takes the lock on TaskMemoryManager.\nAt the same time, there can be another `MemoryConsumer` using `UnsafeExternalSorter` as part of sorting can try to allocatePage needs to get lock on `TaskMemoryManager` which can cause spill to happen which requires lock on `UnsafeExternalSorter` again causing deadlock.\n\nThere is a similar fix here as well:\nhttps://issues.apache.org/jira/browse/SPARK-27338\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nExisting unit tests.\n\nCloses #36680 from sandeepvinayak/SPARK-39283.\n\nAuthored-by: sandeepvinayak <sandeep.pal@outlook.com>\nSigned-off-by: Josh Rosen <joshrosen@databricks.com>\n(cherry picked from commit 8d0c035f102b005c2e85f03253f1c0c24f0a539f)\nSigned-off-by: Josh Rosen <joshrosen@databricks.com>",
        "before_after_code_files": [
          "core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorter.java||core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorter.java"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorter.java||core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorter.java": [
          "File: core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorter.java -> core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorter.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "21: import java.io.File;",
          "22: import java.io.IOException;",
          "23: import java.util.LinkedList;",
          "24: import java.util.Queue;",
          "25: import java.util.function.Supplier;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "24: import java.util.List;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "300:   private long freeMemory() {",
          "302:     long memoryFreed = 0;",
          "304:       memoryFreed += block.size();",
          "305:       freePage(block);",
          "306:     }",
          "307:     allocatedPages.clear();",
          "308:     currentPage = null;",
          "309:     pageCursor = 0;",
          "311:   }",
          "",
          "[Removed Lines]",
          "301:     updatePeakMemoryUsed();",
          "303:     for (MemoryBlock block : allocatedPages) {",
          "310:     return memoryFreed;",
          "",
          "[Added Lines]",
          "302:     List<MemoryBlock> pagesToFree = clearAndGetAllocatedPagesToFree();",
          "304:     for (MemoryBlock block : pagesToFree) {",
          "308:     return memoryFreed;",
          "309:   }",
          "319:   private List<MemoryBlock> clearAndGetAllocatedPagesToFree() {",
          "320:     updatePeakMemoryUsed();",
          "321:     List<MemoryBlock> pagesToFree = new LinkedList<>(allocatedPages);",
          "325:     return pagesToFree;",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "330:   public void cleanupResources() {",
          "337:       }",
          "338:     }",
          "339:   }",
          "",
          "[Removed Lines]",
          "331:     synchronized (this) {",
          "332:       deleteSpillFiles();",
          "333:       freeMemory();",
          "334:       if (inMemSorter != null) {",
          "335:         inMemSorter.freeMemory();",
          "336:         inMemSorter = null;",
          "",
          "[Added Lines]",
          "350:     UnsafeInMemorySorter inMemSorterToFree = null;",
          "351:     List<MemoryBlock> pagesToFree = null;",
          "352:     try {",
          "353:       synchronized (this) {",
          "354:         deleteSpillFiles();",
          "355:         pagesToFree = clearAndGetAllocatedPagesToFree();",
          "356:         if (inMemSorter != null) {",
          "357:           inMemSorterToFree = inMemSorter;",
          "358:           inMemSorter = null;",
          "359:         }",
          "360:       }",
          "361:     } finally {",
          "362:       for (MemoryBlock pageToFree : pagesToFree) {",
          "363:         freePage(pageToFree);",
          "364:       }",
          "365:       if (inMemSorterToFree != null) {",
          "366:         inMemSorterToFree.freeMemory();",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "576:     }",
          "578:     public long spill() throws IOException {",
          "611:             }",
          "612:           }",
          "631:       }",
          "632:     }",
          "",
          "[Removed Lines]",
          "579:       synchronized (this) {",
          "580:         if (inMemSorter == null) {",
          "581:           return 0L;",
          "582:         }",
          "584:         long currentPageNumber = upstream.getCurrentPageNumber();",
          "586:         ShuffleWriteMetrics writeMetrics = new ShuffleWriteMetrics();",
          "587:         if (numRecords > 0) {",
          "589:           final UnsafeSorterSpillWriter spillWriter = new UnsafeSorterSpillWriter(",
          "590:                   blockManager, fileBufferSizeBytes, writeMetrics, numRecords);",
          "591:           spillIterator(upstream, spillWriter);",
          "592:           spillWriters.add(spillWriter);",
          "593:           upstream = spillWriter.getReader(serializerManager);",
          "594:         } else {",
          "597:           upstream = null;",
          "598:         }",
          "600:         long released = 0L;",
          "601:         synchronized (UnsafeExternalSorter.this) {",
          "605:           for (MemoryBlock page : allocatedPages) {",
          "606:             if (!loaded || page.pageNumber != currentPageNumber) {",
          "607:               released += page.size();",
          "608:               freePage(page);",
          "609:             } else {",
          "610:               lastPage = page;",
          "613:           allocatedPages.clear();",
          "614:           if (lastPage != null) {",
          "617:             allocatedPages.add(lastPage);",
          "618:           }",
          "619:         }",
          "622:         assert(inMemSorter != null);",
          "623:         released += inMemSorter.getMemoryUsage();",
          "624:         totalSortTimeNanos += inMemSorter.getSortTimeNanos();",
          "625:         inMemSorter.freeMemory();",
          "626:         inMemSorter = null;",
          "627:         taskContext.taskMetrics().incMemoryBytesSpilled(released);",
          "628:         taskContext.taskMetrics().incDiskBytesSpilled(writeMetrics.bytesWritten());",
          "629:         totalSpillBytes += released;",
          "630:         return released;",
          "",
          "[Added Lines]",
          "609:       UnsafeInMemorySorter inMemSorterToFree = null;",
          "610:       List<MemoryBlock> pagesToFree = new LinkedList<>();",
          "611:       try {",
          "612:         synchronized (this) {",
          "613:           if (inMemSorter == null) {",
          "614:             return 0L;",
          "615:           }",
          "617:           long currentPageNumber = upstream.getCurrentPageNumber();",
          "619:           ShuffleWriteMetrics writeMetrics = new ShuffleWriteMetrics();",
          "620:           if (numRecords > 0) {",
          "622:             final UnsafeSorterSpillWriter spillWriter = new UnsafeSorterSpillWriter(",
          "623:                     blockManager, fileBufferSizeBytes, writeMetrics, numRecords);",
          "624:             spillIterator(upstream, spillWriter);",
          "625:             spillWriters.add(spillWriter);",
          "626:             upstream = spillWriter.getReader(serializerManager);",
          "627:           } else {",
          "630:             upstream = null;",
          "631:           }",
          "633:           long released = 0L;",
          "634:           synchronized (UnsafeExternalSorter.this) {",
          "638:             for (MemoryBlock page : allocatedPages) {",
          "639:               if (!loaded || page.pageNumber != currentPageNumber) {",
          "640:                 released += page.size();",
          "646:                 pagesToFree.add(page);",
          "647:               } else {",
          "648:                 lastPage = page;",
          "649:               }",
          "650:             }",
          "651:             allocatedPages.clear();",
          "652:             if (lastPage != null) {",
          "655:               allocatedPages.add(lastPage);",
          "660:           assert (inMemSorter != null);",
          "661:           released += inMemSorter.getMemoryUsage();",
          "662:           totalSortTimeNanos += inMemSorter.getSortTimeNanos();",
          "665:           inMemSorterToFree = inMemSorter;",
          "666:           inMemSorter = null;",
          "667:           taskContext.taskMetrics().incMemoryBytesSpilled(released);",
          "668:           taskContext.taskMetrics().incDiskBytesSpilled(writeMetrics.bytesWritten());",
          "669:           totalSpillBytes += released;",
          "670:           return released;",
          "671:         }",
          "672:       } finally {",
          "673:         for (MemoryBlock pageToFree : pagesToFree) {",
          "674:           freePage(pageToFree);",
          "675:         }",
          "676:         if (inMemSorterToFree != null) {",
          "677:           inMemSorterToFree.freeMemory();",
          "678:         }",
          "",
          "---------------"
        ]
      }
    }
  ]
}