{
  "cve_id": "CVE-2022-31777",
  "cve_desc": "A stored cross-site scripting (XSS) vulnerability in Apache Spark 3.2.1 and earlier, and 3.3.0, allows remote attackers to execute arbitrary JavaScript in the web browser of a user, by including a malicious payload into the logs which would be returned in logs rendered in the UI.",
  "repo": "apache/spark",
  "patch_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
  "patch_info": {
    "commit_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
    "repo": "apache/spark",
    "commit_url": "https://github.com/apache/spark/commit/ad90195de56688ce0898691eb9d04297ab0871ad",
    "files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ],
    "message": "[SPARK-39505][UI] Escape log content rendered in UI\n\n### What changes were proposed in this pull request?\n\nEscape log content rendered to the UI.\n\n### Why are the changes needed?\n\nLog content may contain reserved characters or other code in the log and be misinterpreted in the UI as HTML.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nExisting tests\n\nCloses #36902 from srowen/LogViewEscape.\n\nAuthored-by: Sean Owen <srowen@gmail.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
    "before_after_code_files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ]
  },
  "patch_diff": {
    "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js": [
      "File: core/src/main/resources/org/apache/spark/ui/static/log-view.js -> core/src/main/resources/org/apache/spark/ui/static/log-view.js",
      "--- Hunk 1 ---",
      "[Context before]",
      "85:       if (retStartByte == 0) {",
      "86:         disableMoreButton();",
      "87:       }",
      "90:       curLogLength = curLogLength + (startByte - retStartByte);",
      "91:       startByte = retStartByte;",
      "",
      "[Removed Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(cleanData);",
      "",
      "[Added Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(document.createTextNode(cleanData));",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "115:             var retLogLength = dataInfo[2];",
      "117:             var cleanData = data.substring(newlineIndex + 1);",
      "120:             curLogLength = curLogLength + (retEndByte - retStartByte);",
      "121:             endByte = retEndByte;",
      "",
      "[Removed Lines]",
      "118:             $(\"pre\", \".log-content\").append(cleanData);",
      "",
      "[Added Lines]",
      "118:             $(\"pre\", \".log-content\").append(document.createTextNode(cleanData));",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "27f78e6672e4ca5449539f88793fd6f76c8e1df2",
      "candidate_info": {
        "commit_hash": "27f78e6672e4ca5449539f88793fd6f76c8e1df2",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/27f78e6672e4ca5449539f88793fd6f76c8e1df2",
        "files": [
          "resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/Client.scala"
        ],
        "message": "[SPARK-39657][YARN] YARN AM client should call the non-static setTokensConf method\n\n### What changes were proposed in this pull request?\n\nThis fixes a bug in the original SPARK-37205 PR, where we treat the method `setTokensConf` as a static method, but it should be non-static instead.\n\n### Why are the changes needed?\n\nThe method `setTokensConf` is non-static so the current code will fail:\n```\n06/29/2022 - 17:28:16  - Exception in thread \"main\" java.lang.IllegalArgumentException: object is not an instance of declaring class\n06/29/2022 - 17:28:16  - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n06/29/2022 - 17:28:16  - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n06/29/2022 - 17:28:16  - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n06/29/2022 - 17:28:16  - at java.base/java.lang.reflect.Method.invoke(Method.java:566)\n```\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nManually tested this change internally and it now works.\n\nCloses #37050 from sunchao/SPARK-39657.\n\nAuthored-by: Chao Sun <sunchao@apple.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>\n(cherry picked from commit 6624d91c9644526f1cb6fdfb4677604b40aa786f)\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
        "before_after_code_files": [
          "resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/Client.scala||resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/Client.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/Client.scala||resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/Client.scala": [
          "File: resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/Client.scala -> resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/Client.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "391:           throw new SparkException(s\"Cannot find setTokensConf method in ${amContainer.getClass}.\" +",
          "392:               s\" Please check YARN version and make sure it is 2.9+ or 3.x\")",
          "393:       }",
          "395:     }",
          "396:   }",
          "",
          "[Removed Lines]",
          "394:       setTokensConfMethod.invoke(ByteBuffer.wrap(dob.getData))",
          "",
          "[Added Lines]",
          "394:       setTokensConfMethod.invoke(amContainer, ByteBuffer.wrap(dob.getData))",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "8f599bae52e7578341f03ec6ec2ae8f08c5ef477",
      "candidate_info": {
        "commit_hash": "8f599bae52e7578341f03ec6ec2ae8f08c5ef477",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/8f599bae52e7578341f03ec6ec2ae8f08c5ef477",
        "files": [
          "external/avro/src/main/scala/org/apache/spark/sql/avro/AvroUtils.scala",
          "sql/catalyst/src/main/java/org/apache/spark/sql/connector/util/V2ExpressionSQLBuilder.java",
          "sql/catalyst/src/main/java/org/apache/spark/sql/connector/write/RowLevelOperation.java",
          "sql/catalyst/src/main/java/org/apache/spark/sql/util/NumericHistogram.java",
          "sql/catalyst/src/main/java/org/apache/spark/sql/vectorized/ColumnarBatchRow.java",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryErrorsBase.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryParsingErrors.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/jdbc/JdbcDialects.scala"
        ],
        "message": "[SPARK-39367][DOCS][SQL] Review and fix issues in Scala/Java API docs of SQL module\n\n### What changes were proposed in this pull request?\n\nCompare the 3.3.0 API doc with the latest release version 3.2.1. Fix the following issues:\n\n* Add missing Since annotation for new APIs\n* Remove the leaking class/object in API doc\n\n### Why are the changes needed?\n\nImprove API docs\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nExisting UT\n\nCloses #36754 from gengliangwang/apiDoc.\n\nAuthored-by: Gengliang Wang <gengliang@apache.org>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>\n(cherry picked from commit 4c7888dd9159dc203628b0d84f0ee2f90ab4bf13)\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>",
        "before_after_code_files": [
          "external/avro/src/main/scala/org/apache/spark/sql/avro/AvroUtils.scala||external/avro/src/main/scala/org/apache/spark/sql/avro/AvroUtils.scala",
          "sql/catalyst/src/main/java/org/apache/spark/sql/connector/util/V2ExpressionSQLBuilder.java||sql/catalyst/src/main/java/org/apache/spark/sql/connector/util/V2ExpressionSQLBuilder.java",
          "sql/catalyst/src/main/java/org/apache/spark/sql/connector/write/RowLevelOperation.java||sql/catalyst/src/main/java/org/apache/spark/sql/connector/write/RowLevelOperation.java",
          "sql/catalyst/src/main/java/org/apache/spark/sql/util/NumericHistogram.java||sql/catalyst/src/main/java/org/apache/spark/sql/util/NumericHistogram.java",
          "sql/catalyst/src/main/java/org/apache/spark/sql/vectorized/ColumnarBatchRow.java||sql/catalyst/src/main/java/org/apache/spark/sql/vectorized/ColumnarBatchRow.java",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryErrorsBase.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryErrorsBase.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryParsingErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryParsingErrors.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/jdbc/JdbcDialects.scala||sql/core/src/main/scala/org/apache/spark/sql/jdbc/JdbcDialects.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "external/avro/src/main/scala/org/apache/spark/sql/avro/AvroUtils.scala||external/avro/src/main/scala/org/apache/spark/sql/avro/AvroUtils.scala": [
          "File: external/avro/src/main/scala/org/apache/spark/sql/avro/AvroUtils.scala -> external/avro/src/main/scala/org/apache/spark/sql/avro/AvroUtils.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "206:   }",
          "210:       catalystField: StructField,",
          "211:       catalystPosition: Int,",
          "212:       avroField: Schema.Field)",
          "",
          "[Removed Lines]",
          "209:   case class AvroMatchedField(",
          "",
          "[Added Lines]",
          "209:   private[sql] case class AvroMatchedField(",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/java/org/apache/spark/sql/connector/util/V2ExpressionSQLBuilder.java||sql/catalyst/src/main/java/org/apache/spark/sql/connector/util/V2ExpressionSQLBuilder.java": [
          "File: sql/catalyst/src/main/java/org/apache/spark/sql/connector/util/V2ExpressionSQLBuilder.java -> sql/catalyst/src/main/java/org/apache/spark/sql/connector/util/V2ExpressionSQLBuilder.java"
        ],
        "sql/catalyst/src/main/java/org/apache/spark/sql/connector/write/RowLevelOperation.java||sql/catalyst/src/main/java/org/apache/spark/sql/connector/write/RowLevelOperation.java": [
          "File: sql/catalyst/src/main/java/org/apache/spark/sql/connector/write/RowLevelOperation.java -> sql/catalyst/src/main/java/org/apache/spark/sql/connector/write/RowLevelOperation.java"
        ],
        "sql/catalyst/src/main/java/org/apache/spark/sql/util/NumericHistogram.java||sql/catalyst/src/main/java/org/apache/spark/sql/util/NumericHistogram.java": [
          "File: sql/catalyst/src/main/java/org/apache/spark/sql/util/NumericHistogram.java -> sql/catalyst/src/main/java/org/apache/spark/sql/util/NumericHistogram.java"
        ],
        "sql/catalyst/src/main/java/org/apache/spark/sql/vectorized/ColumnarBatchRow.java||sql/catalyst/src/main/java/org/apache/spark/sql/vectorized/ColumnarBatchRow.java": [
          "File: sql/catalyst/src/main/java/org/apache/spark/sql/vectorized/ColumnarBatchRow.java -> sql/catalyst/src/main/java/org/apache/spark/sql/vectorized/ColumnarBatchRow.java"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "51:   def groupingIDMismatchError(groupingID: GroupingID, groupByExprs: Seq[Expression]): Throwable = {",
          "52:     new AnalysisException(",
          "",
          "[Removed Lines]",
          "49: object QueryCompilationErrors extends QueryErrorsBase {",
          "",
          "[Added Lines]",
          "49: private[sql] object QueryCompilationErrors extends QueryErrorsBase {",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryErrorsBase.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryErrorsBase.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryErrorsBase.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryErrorsBase.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "47:   def toSQLValue(v: Any, t: DataType): String = Literal.create(v, t) match {",
          "48:     case Literal(null, _) => \"NULL\"",
          "",
          "[Removed Lines]",
          "45: trait QueryErrorsBase {",
          "",
          "[Added Lines]",
          "45: private[sql] trait QueryErrorsBase {",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "71:   def cannotEvaluateExpressionError(expression: Expression): Throwable = {",
          "72:     new SparkUnsupportedOperationException(errorClass = \"INTERNAL_ERROR\",",
          "",
          "[Removed Lines]",
          "69: object QueryExecutionErrors extends QueryErrorsBase {",
          "",
          "[Added Lines]",
          "69: private[sql] object QueryExecutionErrors extends QueryErrorsBase {",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryParsingErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryParsingErrors.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryParsingErrors.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryParsingErrors.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "34:   def invalidInsertIntoError(ctx: InsertIntoContext): Throwable = {",
          "35:     new ParseException(\"Invalid InsertIntoContext\", ctx)",
          "",
          "[Removed Lines]",
          "32: object QueryParsingErrors extends QueryErrorsBase {",
          "",
          "[Added Lines]",
          "32: private[sql] object QueryParsingErrors extends QueryErrorsBase {",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/jdbc/JdbcDialects.scala||sql/core/src/main/scala/org/apache/spark/sql/jdbc/JdbcDialects.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/jdbc/JdbcDialects.scala -> sql/core/src/main/scala/org/apache/spark/sql/jdbc/JdbcDialects.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "221:     case _ => value",
          "222:   }",
          "225:     override def visitLiteral(literal: Literal[_]): String = {",
          "226:       compileValue(",
          "227:         CatalystTypeConverters.convertToScala(literal.value(), literal.dataType())).toString",
          "",
          "[Removed Lines]",
          "224:   class JDBCSQLBuilder extends V2ExpressionSQLBuilder {",
          "",
          "[Added Lines]",
          "224:   private[jdbc] class JDBCSQLBuilder extends V2ExpressionSQLBuilder {",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "c2bd7bac76a5cf7ffc5ef61a1df2b8bb5a72f131",
      "candidate_info": {
        "commit_hash": "c2bd7bac76a5cf7ffc5ef61a1df2b8bb5a72f131",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/c2bd7bac76a5cf7ffc5ef61a1df2b8bb5a72f131",
        "files": [
          "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ShuffleExchangeExec.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/python/ExtractPythonUDFs.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/memory.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/TextSocketMicroBatchStream.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/subquery.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/window/AggregateProcessor.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/window/WindowExecBase.scala",
          "sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveInspectors.scala",
          "sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveClientImpl.scala"
        ],
        "message": "[SPARK-39165][SQL][3.3] Replace `sys.error` by `IllegalStateException`\n\n### What changes were proposed in this pull request?\nReplace all invokes of `sys.error()` by throwing of `IllegalStateException` in the `sql` namespace.\n\nThis is a backport of https://github.com/apache/spark/pull/36524.\n\n### Why are the changes needed?\nIn the context of wrapping all internal errors like asserts/illegal state exceptions (see https://github.com/apache/spark/pull/36500), it is impossible to distinguish `RuntimeException` of `sys.error()` from Spark's exceptions like `SparkRuntimeException`. The last one can be propagated to the user space but `sys.error` exceptions shouldn't be visible to users in regular cases.\n\n### Does this PR introduce _any_ user-facing change?\nNo, shouldn't. sys.error shouldn't propagate exception to user space in regular cases.\n\n### How was this patch tested?\nBy running the existing test suites.\n\nAuthored-by: Max Gekk <max.gekkgmail.com>\nSigned-off-by: Max Gekk <max.gekkgmail.com>\n(cherry picked from commit 95c7efd7571464d8adfb76fb22e47a5816cf73fb)\nSigned-off-by: Max Gekk <max.gekkgmail.com>\n\nCloses #36532 from MaxGekk/sys_error-internal-3.3.\n\nAuthored-by: Max Gekk <max.gekk@gmail.com>\nSigned-off-by: Max Gekk <max.gekk@gmail.com>",
        "before_after_code_files": [
          "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ShuffleExchangeExec.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ShuffleExchangeExec.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/python/ExtractPythonUDFs.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/python/ExtractPythonUDFs.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/memory.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/memory.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/TextSocketMicroBatchStream.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/TextSocketMicroBatchStream.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/subquery.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/subquery.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/window/AggregateProcessor.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/window/AggregateProcessor.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/window/WindowExecBase.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/window/WindowExecBase.scala",
          "sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveInspectors.scala||sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveInspectors.scala",
          "sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveClientImpl.scala||sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveClientImpl.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "503:           _.aggregateFunction.children.filterNot(_.foldable).toSet).distinct.length > 1) {",
          "508:         }",
          "",
          "[Removed Lines]",
          "506:           sys.error(\"You hit a query analyzer bug. Please report your query to \" +",
          "507:               \"Spark user mailing list.\")",
          "",
          "[Added Lines]",
          "506:           throw new IllegalStateException(",
          "507:             \"You hit a query analyzer bug. Please report your query to Spark user mailing list.\")",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "539:         DataWritingCommand.propogateMetrics(sparkSession.sparkContext, resolved, metrics)",
          "541:         copy(userSpecifiedSchema = Some(outputColumns.toStructType.asNullable)).resolveRelation()",
          "544:     }",
          "545:   }",
          "",
          "[Removed Lines]",
          "542:       case _ =>",
          "543:         sys.error(s\"${providingClass.getCanonicalName} does not allow create table as select.\")",
          "",
          "[Added Lines]",
          "542:       case _ => throw new IllegalStateException(",
          "543:         s\"${providingClass.getCanonicalName} does not allow create table as select.\")",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "556:         disallowWritingIntervals(data.schema.map(_.dataType), forbidAnsiIntervals = false)",
          "557:         DataSource.validateSchema(data.schema)",
          "558:         planForWritingFileFormat(format, mode, data)",
          "561:     }",
          "562:   }",
          "",
          "[Removed Lines]",
          "559:       case _ =>",
          "560:         sys.error(s\"${providingClass.getCanonicalName} does not allow create table as select.\")",
          "",
          "[Added Lines]",
          "559:       case _ => throw new IllegalStateException(",
          "560:         s\"${providingClass.getCanonicalName} does not allow create table as select.\")",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "255:       case t: UserDefinedType[_] => makeWriter(t.sqlType)",
          "259:     }",
          "260:   }",
          "",
          "[Removed Lines]",
          "258:       case _ => sys.error(s\"Unsupported data type $dataType.\")",
          "",
          "[Added Lines]",
          "257:       case _ => throw new IllegalStateException(s\"Unsupported data type $dataType.\")",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ShuffleExchangeExec.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ShuffleExchangeExec.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ShuffleExchangeExec.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ShuffleExchangeExec.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "300:           override def numPartitions: Int = 1",
          "301:           override def getPartition(key: Any): Int = 0",
          "302:         }",
          "305:     }",
          "306:     def getPartitionKeyExtractor(): InternalRow => Any = newPartitioning match {",
          "",
          "[Removed Lines]",
          "303:       case _ => sys.error(s\"Exchange not implemented for $newPartitioning\")",
          "",
          "[Added Lines]",
          "303:       case _ => throw new IllegalStateException(s\"Exchange not implemented for $newPartitioning\")",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "319:         val projection = UnsafeProjection.create(sortingExpressions.map(_.child), outputAttributes)",
          "320:         row => projection(row)",
          "321:       case SinglePartition => identity",
          "323:     }",
          "325:     val isRoundRobin = newPartitioning.isInstanceOf[RoundRobinPartitioning] &&",
          "",
          "[Removed Lines]",
          "322:       case _ => sys.error(s\"Exchange not implemented for $newPartitioning\")",
          "",
          "[Added Lines]",
          "322:       case _ => throw new IllegalStateException(s\"Exchange not implemented for $newPartitioning\")",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/execution/python/ExtractPythonUDFs.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/python/ExtractPythonUDFs.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/python/ExtractPythonUDFs.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/python/ExtractPythonUDFs.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "284:       }",
          "289:       }",
          "291:       val rewritten = plan.withNewChildren(newChildren).transformExpressions {",
          "",
          "[Removed Lines]",
          "287:       udfs.map(canonicalizeDeterministic).filterNot(attributeMap.contains).foreach {",
          "288:         udf => sys.error(s\"Invalid PythonUDF $udf, requires attributes from more than one child.\")",
          "",
          "[Added Lines]",
          "287:       udfs.map(canonicalizeDeterministic).filterNot(attributeMap.contains).foreach { udf =>",
          "288:         throw new IllegalStateException(",
          "289:           s\"Invalid PythonUDF $udf, requires attributes from more than one child.\")",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/memory.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/memory.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/memory.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/memory.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "257:     val offsetDiff = (newOffset.offset - lastOffsetCommitted.offset).toInt",
          "259:     if (offsetDiff < 0) {",
          "261:     }",
          "263:     batches.trimStart(offsetDiff)",
          "",
          "[Removed Lines]",
          "260:       sys.error(s\"Offsets committed out of order: $lastOffsetCommitted followed by $end\")",
          "",
          "[Added Lines]",
          "260:       throw new IllegalStateException(",
          "261:         s\"Offsets committed out of order: $lastOffsetCommitted followed by $end\")",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/TextSocketMicroBatchStream.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/TextSocketMicroBatchStream.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/TextSocketMicroBatchStream.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/TextSocketMicroBatchStream.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "155:     val offsetDiff = (newOffset.offset - lastOffsetCommitted.offset).toInt",
          "157:     if (offsetDiff < 0) {",
          "159:     }",
          "161:     batches.trimStart(offsetDiff)",
          "",
          "[Removed Lines]",
          "158:       sys.error(s\"Offsets committed out of order: $lastOffsetCommitted followed by $end\")",
          "",
          "[Added Lines]",
          "158:       throw new IllegalStateException(",
          "159:         s\"Offsets committed out of order: $lastOffsetCommitted followed by $end\")",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/execution/subquery.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/subquery.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/subquery.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/subquery.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "79:   def updateResult(): Unit = {",
          "80:     val rows = plan.executeCollect()",
          "81:     if (rows.length > 1) {",
          "83:     }",
          "84:     if (rows.length == 1) {",
          "85:       assert(rows(0).numFields == 1,",
          "",
          "[Removed Lines]",
          "82:       sys.error(s\"more than one row returned by a subquery used as an expression:\\n$plan\")",
          "",
          "[Added Lines]",
          "82:       throw new IllegalStateException(",
          "83:         s\"more than one row returned by a subquery used as an expression:\\n$plan\")",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/execution/window/AggregateProcessor.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/window/AggregateProcessor.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/window/AggregateProcessor.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/window/AggregateProcessor.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "90:         updateExpressions ++= noOps",
          "91:         evaluateExpressions += imperative",
          "92:       case other =>",
          "94:     }",
          "",
          "[Removed Lines]",
          "93:         sys.error(s\"Unsupported aggregate function: $other\")",
          "",
          "[Added Lines]",
          "93:         throw new IllegalStateException(s\"Unsupported aggregate function: $other\")",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/execution/window/WindowExecBase.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/window/WindowExecBase.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/window/WindowExecBase.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/window/WindowExecBase.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "97:         RowBoundOrdering(offset)",
          "99:       case (RowFrame, _) =>",
          "102:       case (RangeFrame, CurrentRow) =>",
          "103:         val ordering = RowOrdering.create(orderSpec, child.output)",
          "",
          "[Removed Lines]",
          "100:         sys.error(s\"Unhandled bound in windows expressions: $bound\")",
          "",
          "[Added Lines]",
          "100:         throw new IllegalStateException(s\"Unhandled bound in windows expressions: $bound\")",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "139:         RangeBoundOrdering(ordering, current, bound)",
          "141:       case (RangeFrame, _) =>",
          "143:           \"with multiple order expressions.\")",
          "144:     }",
          "145:   }",
          "",
          "[Removed Lines]",
          "142:         sys.error(\"Non-Zero range offsets are not supported for windows \" +",
          "",
          "[Added Lines]",
          "142:         throw new IllegalStateException(\"Non-Zero range offsets are not supported for windows \" +",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "189:               }",
          "190:             case f: AggregateWindowFunction => collect(\"AGGREGATE\", frame, e, f)",
          "191:             case f: PythonUDF => collect(\"AGGREGATE\", frame, e, f)",
          "193:           }",
          "194:         case _ =>",
          "195:       }",
          "",
          "[Removed Lines]",
          "192:             case f => sys.error(s\"Unsupported window function: $f\")",
          "",
          "[Added Lines]",
          "192:             case f => throw new IllegalStateException(s\"Unsupported window function: $f\")",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "296:             }",
          "298:           case _ =>",
          "300:         }",
          "",
          "[Removed Lines]",
          "299:             sys.error(s\"Unsupported factory: $key\")",
          "",
          "[Added Lines]",
          "299:             throw new IllegalStateException(s\"Unsupported factory: $key\")",
          "",
          "---------------"
        ],
        "sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveInspectors.scala||sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveInspectors.scala": [
          "File: sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveInspectors.scala -> sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveInspectors.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "922:     case Literal(_, dt: UserDefinedType[_]) =>",
          "923:       toInspector(dt.sqlType)",
          "928:     case _ if expr.foldable => toInspector(Literal.create(expr.eval(), expr.dataType))",
          "",
          "[Removed Lines]",
          "925:     case Literal(_, dt) => sys.error(s\"Hive doesn't support the constant type [$dt].\")",
          "",
          "[Added Lines]",
          "925:     case Literal(_, dt) =>",
          "926:       throw new IllegalStateException(s\"Hive doesn't support the constant type [$dt].\")",
          "",
          "---------------"
        ],
        "sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveClientImpl.scala||sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveClientImpl.scala": [
          "File: sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveClientImpl.scala -> sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveClientImpl.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "801:     val maxResults = 100000",
          "802:     val results = runHive(sql, maxResults)",
          "805:     results",
          "806:   }",
          "",
          "[Removed Lines]",
          "804:     if (results.size == maxResults) sys.error(\"RESULTS POSSIBLY TRUNCATED\")",
          "",
          "[Added Lines]",
          "804:     if (results.size == maxResults) throw new IllegalStateException(\"RESULTS POSSIBLY TRUNCATED\")",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "82351bee9bba75b631dc3a93aa5bc4cd9f46724c",
      "candidate_info": {
        "commit_hash": "82351bee9bba75b631dc3a93aa5bc4cd9f46724c",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/82351bee9bba75b631dc3a93aa5bc4cd9f46724c",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/PropagateEmptyRelation.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AQEPropagateEmptyRelation.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AQEUtils.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdjustShuffleExchangePosition.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/LogicalQueryStage.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala"
        ],
        "message": "[SPARK-39915][SQL] Ensure the output partitioning is user-specified in AQE\n\n### What changes were proposed in this pull request?\n\n- Support get user-specified root repartition through  `DeserializeToObjectExec`\n- Skip optimize empty for the root repartition which is user-specified\n- Add a new rule `AdjustShuffleExchangePosition` to adjust the shuffle we add back, so that we can restore shuffle safely.\n\n### Why are the changes needed?\n\nAQE can not completely respect the user-specified repartition. The main reasons are:\n\n1. the AQE optimzier will convert empty to local relation which does not reserve the partitioning info\n2. the machine of AQE `requiredDistribution` only restore the repartition which does not support through `DeserializeToObjectExec`\n\nAfter the fix:\nThe partition number of `spark.range(0).repartition(5).rdd.getNumPartitions` should be 5.\n\n### Does this PR introduce _any_ user-facing change?\n\nyes, ensure the user-specified distribution.\n\n### How was this patch tested?\n\nadd tests\n\nCloses #37612 from ulysses-you/output-partition.\n\nLead-authored-by: ulysses-you <ulyssesyou18@gmail.com>\nCo-authored-by: Wenchen Fan <cloud0fan@gmail.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\n(cherry picked from commit 801ca252f43b20cdd629c01d734ca9049e6eccf4)\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/PropagateEmptyRelation.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/PropagateEmptyRelation.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AQEPropagateEmptyRelation.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AQEPropagateEmptyRelation.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AQEUtils.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AQEUtils.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdjustShuffleExchangePosition.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdjustShuffleExchangePosition.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/LogicalQueryStage.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/LogicalQueryStage.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/PropagateEmptyRelation.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/PropagateEmptyRelation.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/PropagateEmptyRelation.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/PropagateEmptyRelation.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "24: import org.apache.spark.sql.catalyst.plans.logical._",
          "25: import org.apache.spark.sql.catalyst.rules._",
          "26: import org.apache.spark.sql.catalyst.trees.TreeNodeTag",
          "",
          "[Removed Lines]",
          "27: import org.apache.spark.sql.catalyst.trees.TreePattern.{LOCAL_RELATION, TRUE_OR_FALSE_LITERAL}",
          "",
          "[Added Lines]",
          "27: import org.apache.spark.sql.catalyst.trees.TreePattern.{LOCAL_RELATION, REPARTITION_OPERATION, TRUE_OR_FALSE_LITERAL}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "193:   }",
          "195:   override def apply(plan: LogicalPlan): LogicalPlan = {",
          "",
          "[Removed Lines]",
          "186:   private def addTagForRootRepartition(plan: LogicalPlan): LogicalPlan = plan match {",
          "187:     case p: Project => p.mapChildren(addTagForRootRepartition)",
          "188:     case f: Filter => f.mapChildren(addTagForRootRepartition)",
          "189:     case r if userSpecifiedRepartition(r) =>",
          "190:       r.setTagValue(ROOT_REPARTITION, ())",
          "191:       r",
          "192:     case _ => plan",
          "",
          "[Added Lines]",
          "186:   private def addTagForRootRepartition(plan: LogicalPlan): LogicalPlan = {",
          "187:     if (!plan.containsPattern(REPARTITION_OPERATION)) {",
          "188:       return plan",
          "189:     }",
          "191:     plan match {",
          "192:       case p: Project => p.mapChildren(addTagForRootRepartition)",
          "193:       case f: Filter => f.mapChildren(addTagForRootRepartition)",
          "194:       case d: DeserializeToObject => d.mapChildren(addTagForRootRepartition)",
          "195:       case r if userSpecifiedRepartition(r) =>",
          "196:         r.setTagValue(ROOT_REPARTITION, ())",
          "197:         r",
          "198:       case _ => plan",
          "199:     }",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AQEPropagateEmptyRelation.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AQEPropagateEmptyRelation.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AQEPropagateEmptyRelation.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AQEPropagateEmptyRelation.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "22: import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan",
          "23: import org.apache.spark.sql.catalyst.trees.TreePattern.{LOCAL_RELATION, LOGICAL_QUERY_STAGE, TRUE_OR_FALSE_LITERAL}",
          "24: import org.apache.spark.sql.execution.aggregate.BaseAggregateExec",
          "25: import org.apache.spark.sql.execution.joins.HashedRelationWithAllNullKeys",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "25: import org.apache.spark.sql.execution.exchange.{REPARTITION_BY_COL, REPARTITION_BY_NUM, ShuffleExchangeLike}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "34: object AQEPropagateEmptyRelation extends PropagateEmptyRelationBase {",
          "35:   override protected def isEmpty(plan: LogicalPlan): Boolean =",
          "38:   override protected def nonEmpty(plan: LogicalPlan): Boolean =",
          "39:     super.nonEmpty(plan) || getEstimatedRowCount(plan).exists(_ > 0)",
          "",
          "[Removed Lines]",
          "36:     super.isEmpty(plan) || getEstimatedRowCount(plan).contains(0)",
          "",
          "[Added Lines]",
          "37:     super.isEmpty(plan) || (!isRootRepartition(plan) && getEstimatedRowCount(plan).contains(0))",
          "42:   private def isRootRepartition(plan: LogicalPlan): Boolean = plan match {",
          "43:     case l: LogicalQueryStage if l.getTagValue(ROOT_REPARTITION).isDefined => true",
          "44:     case _ => false",
          "45:   }",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "69:       empty(j)",
          "70:   }",
          "72:   override protected def applyInternal(p: LogicalPlan): LogicalPlan = p.transformUpWithPruning(",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "78:   override protected def userSpecifiedRepartition(p: LogicalPlan): Boolean = p match {",
          "79:     case LogicalQueryStage(_, ShuffleQueryStageExec(_, shuffle: ShuffleExchangeLike, _))",
          "80:       if shuffle.shuffleOrigin == REPARTITION_BY_COL ||",
          "81:         shuffle.shuffleOrigin == REPARTITION_BY_NUM => true",
          "82:     case _ => false",
          "83:   }",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AQEUtils.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AQEUtils.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AQEUtils.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AQEUtils.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: package org.apache.spark.sql.execution.adaptive",
          "20: import org.apache.spark.sql.catalyst.plans.physical.{ClusteredDistribution, Distribution, HashPartitioning, UnspecifiedDistribution}",
          "22: import org.apache.spark.sql.execution.exchange.{REPARTITION_BY_COL, REPARTITION_BY_NUM, ShuffleExchangeExec}",
          "24: object AQEUtils {",
          "",
          "[Removed Lines]",
          "21: import org.apache.spark.sql.execution.{CollectMetricsExec, FilterExec, ProjectExec, SortExec, SparkPlan}",
          "",
          "[Added Lines]",
          "21: import org.apache.spark.sql.execution.{CollectMetricsExec, DeserializeToObjectExec, FilterExec, ProjectExec, SortExec, SparkPlan}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "41:     case f: FilterExec => getRequiredDistribution(f.child)",
          "42:     case s: SortExec if !s.global => getRequiredDistribution(s.child)",
          "43:     case c: CollectMetricsExec => getRequiredDistribution(c.child)",
          "44:     case p: ProjectExec =>",
          "45:       getRequiredDistribution(p.child).flatMap {",
          "46:         case h: ClusteredDistribution =>",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "44:     case d: DeserializeToObjectExec => getRequiredDistribution(d.child)",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "116:     Seq(",
          "117:       RemoveRedundantProjects,",
          "118:       ensureRequirements,",
          "119:       ValidateSparkPlan,",
          "120:       ReplaceHashWithSortAgg,",
          "121:       RemoveRedundantSorts,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "119:       AdjustShuffleExchangePosition,",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdjustShuffleExchangePosition.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdjustShuffleExchangePosition.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdjustShuffleExchangePosition.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdjustShuffleExchangePosition.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "18: package org.apache.spark.sql.execution.adaptive",
          "20: import org.apache.spark.sql.catalyst.rules.Rule",
          "21: import org.apache.spark.sql.execution.{DeserializeToObjectExec, SparkPlan}",
          "22: import org.apache.spark.sql.execution.exchange.ShuffleExchangeLike",
          "28: object AdjustShuffleExchangePosition extends Rule[SparkPlan] {",
          "29:   private def shouldAdjust(plan: SparkPlan): Boolean = plan match {",
          "34:     case _: DeserializeToObjectExec => true",
          "35:     case _ => false",
          "36:   }",
          "38:   override def apply(plan: SparkPlan): SparkPlan = plan match {",
          "39:     case shuffle: ShuffleExchangeLike if shouldAdjust(shuffle.child) =>",
          "40:       shuffle.child.withNewChildren(shuffle.withNewChildren(shuffle.child.children) :: Nil)",
          "41:     case _ => plan",
          "42:   }",
          "43: }",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/LogicalQueryStage.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/LogicalQueryStage.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/LogicalQueryStage.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/LogicalQueryStage.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: package org.apache.spark.sql.execution.adaptive",
          "20: import org.apache.spark.sql.catalyst.expressions.{Attribute, SortOrder}",
          "23: import org.apache.spark.sql.execution.SparkPlan",
          "",
          "[Removed Lines]",
          "21: import org.apache.spark.sql.catalyst.plans.logical.{LeafNode, LogicalPlan, Statistics}",
          "22: import org.apache.spark.sql.catalyst.trees.TreePattern.{LOGICAL_QUERY_STAGE, TreePattern}",
          "",
          "[Added Lines]",
          "21: import org.apache.spark.sql.catalyst.plans.logical.{LeafNode, LogicalPlan, RepartitionOperation, Statistics}",
          "22: import org.apache.spark.sql.catalyst.trees.TreePattern.{LOGICAL_QUERY_STAGE, REPARTITION_OPERATION, TreePattern}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "40:   override def output: Seq[Attribute] = logicalPlan.output",
          "41:   override val isStreaming: Boolean = logicalPlan.isStreaming",
          "42:   override val outputOrdering: Seq[SortOrder] = physicalPlan.outputOrdering",
          "45:   override def computeStats(): Statistics = {",
          "",
          "[Removed Lines]",
          "43:   override protected val nodePatterns: Seq[TreePattern] = Seq(LOGICAL_QUERY_STAGE)",
          "",
          "[Added Lines]",
          "43:   override protected val nodePatterns: Seq[TreePattern] = {",
          "46:     val repartitionPattern = logicalPlan match {",
          "47:       case _: RepartitionOperation => Some(REPARTITION_OPERATION)",
          "48:       case _ => None",
          "49:     }",
          "50:     Seq(LOGICAL_QUERY_STAGE) ++ repartitionPattern",
          "51:   }",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "482:       testData.select(\"key\").coalesce(1).select(\"key\"),",
          "483:       testData.select(\"key\").collect().toSeq)",
          "486:   }",
          "488:   test(\"convert $\\\"attribute name\\\" into unresolved attribute\") {",
          "",
          "[Removed Lines]",
          "485:     assert(spark.emptyDataFrame.coalesce(1).rdd.partitions.size === 0)",
          "",
          "[Added Lines]",
          "485:     assert(spark.emptyDataFrame.coalesce(1).rdd.partitions.size === 1)",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "2606:       assert(findTopLevelBroadcastNestedLoopJoin(adaptivePlan).size == 1)",
          "2607:     }",
          "2608:   }",
          "2609: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "2610:   test(\"SPARK-39915: Dataset.repartition(N) may not create N partitions\") {",
          "2611:     withSQLConf(SQLConf.SHUFFLE_PARTITIONS.key -> \"6\") {",
          "2614:       assert(spark.range(0).repartition(5, $\"id\").rdd.getNumPartitions == 5)",
          "2617:       assert(spark.range(0).repartition($\"id\").rdd.getNumPartitions == 1)",
          "2619:       assert(spark.range(0).selectExpr(\"id % 3 as c1\", \"id % 7 as c2\")",
          "2620:         .repartition(5, $\"c1\").select($\"c2\").rdd.getNumPartitions == 5)",
          "2625:       assert(spark.range(0).repartitionByRange(5, $\"id\").rdd.getNumPartitions == 1)",
          "2627:       assert(spark.range(0).repartitionByRange($\"id\").rdd.getNumPartitions == 1)",
          "2631:       assert(spark.range(0).repartition(5).rdd.getNumPartitions == 5)",
          "2633:       assert(spark.range(0).repartition().rdd.getNumPartitions == 0)",
          "2635:       assert(spark.range(0).selectExpr(\"id % 3 as c1\", \"id % 7 as c2\")",
          "2636:         .repartition(5).select($\"c2\").rdd.getNumPartitions == 5)",
          "2639:       assert(spark.range(0).repartition(1).rdd.getNumPartitions == 1)",
          "2640:     }",
          "2641:   }",
          "2643:   test(\"SPARK-39915: Ensure the output partitioning is user-specified\") {",
          "2644:     withSQLConf(SQLConf.SHUFFLE_PARTITIONS.key -> \"3\",",
          "2645:         SQLConf.AUTO_BROADCASTJOIN_THRESHOLD.key -> \"-1\") {",
          "2646:       val df1 = spark.range(1).selectExpr(\"id as c1\")",
          "2647:       val df2 = spark.range(1).selectExpr(\"id as c2\")",
          "2648:       val df = df1.join(df2, col(\"c1\") === col(\"c2\")).repartition(3, col(\"c1\"))",
          "2649:       assert(df.rdd.getNumPartitions == 3)",
          "2650:     }",
          "2651:   }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "3e28f338fad66393b6d2f7a2da6ce5eee60a626e",
      "candidate_info": {
        "commit_hash": "3e28f338fad66393b6d2f7a2da6ce5eee60a626e",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/3e28f338fad66393b6d2f7a2da6ce5eee60a626e",
        "files": [
          "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/DynamicPartitionPruningSuite.scala"
        ],
        "message": "[SPARK-39447][SQL] Avoid AssertionError in AdaptiveSparkPlanExec.doExecuteBroadcast\n\n### What changes were proposed in this pull request?\n\nChange `currentPhysicalPlan` to `inputPlan ` when we restore the broadcast exchange for DPP.\n\n### Why are the changes needed?\n\nThe currentPhysicalPlan can be wrapped with broadcast query stage so it is not safe to match it. For example:\n The broadcast exchange which is added by DPP is running before than the normal broadcast exchange(e.g. introduced by join).\n\n### Does this PR introduce _any_ user-facing change?\n\nyes bug fix\n\n### How was this patch tested?\n\nadd test\n\nCloses #36974 from ulysses-you/inputplan.\n\nAuthored-by: ulysses-you <ulyssesyou18@gmail.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\n(cherry picked from commit c320a5d51b2c8427fc5d6648984bfd266891b451)\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>",
        "before_after_code_files": [
          "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/DynamicPartitionPruningSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/DynamicPartitionPruningSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "663:         case b: BroadcastExchangeLike",
          "664:           if (!newPlan.isInstanceOf[BroadcastExchangeLike]) => b.withNewChildren(Seq(newPlan))",
          "665:         case _ => newPlan",
          "",
          "[Removed Lines]",
          "662:       val finalPlan = currentPhysicalPlan match {",
          "",
          "[Added Lines]",
          "662:       val finalPlan = inputPlan match {",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/DynamicPartitionPruningSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/DynamicPartitionPruningSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/DynamicPartitionPruningSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/DynamicPartitionPruningSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "1694: class DynamicPartitionPruningV1SuiteAEOn extends DynamicPartitionPruningV1Suite",
          "1695:   with EnableAdaptiveExecutionSuite {",
          "1697:   test(\"SPARK-37995: PlanAdaptiveDynamicPruningFilters should use prepareExecutedPlan \" +",
          "1698:     \"rather than createSparkPlan to re-plan subquery\") {",
          "1699:     withSQLConf(SQLConf.DYNAMIC_PARTITION_PRUNING_ENABLED.key -> \"true\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1697:   test(\"SPARK-39447: Avoid AssertionError in AdaptiveSparkPlanExec.doExecuteBroadcast\") {",
          "1698:     val df = sql(",
          "1699:       \"\"\"",
          "1700:         |WITH empty_result AS (",
          "1701:         |  SELECT * FROM fact_stats WHERE product_id < 0",
          "1702:         |)",
          "1703:         |SELECT *",
          "1704:         |FROM   (SELECT /*+ SHUFFLE_MERGE(fact_sk) */ empty_result.store_id",
          "1705:         |        FROM   fact_sk",
          "1706:         |               JOIN empty_result",
          "1707:         |                 ON fact_sk.product_id = empty_result.product_id) t2",
          "1708:         |       JOIN empty_result",
          "1709:         |         ON t2.store_id = empty_result.store_id",
          "1710:       \"\"\".stripMargin)",
          "1712:     checkPartitionPruningPredicate(df, false, false)",
          "1713:     checkAnswer(df, Nil)",
          "1714:   }",
          "",
          "---------------"
        ]
      }
    }
  ]
}