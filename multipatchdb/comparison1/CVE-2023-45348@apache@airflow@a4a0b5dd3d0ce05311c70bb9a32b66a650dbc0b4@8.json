{
  "cve_id": "CVE-2023-45348",
  "cve_desc": "Apache Airflow, versions 2.7.0 and 2.7.1, is affected by a vulnerability that allows an authenticated user to retrieve sensitive configuration information when the \"expose_config\" option is set to \"non-sensitive-only\". The `expose_config` option is False by default.\nIt is recommended to upgrade to a version that is not affected.",
  "repo": "apache/airflow",
  "patch_hash": "a4a0b5dd3d0ce05311c70bb9a32b66a650dbc0b4",
  "patch_info": {
    "commit_hash": "a4a0b5dd3d0ce05311c70bb9a32b66a650dbc0b4",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/a4a0b5dd3d0ce05311c70bb9a32b66a650dbc0b4",
    "files": [
      "airflow/api_connexion/endpoints/config_endpoint.py",
      "airflow/configuration.py",
      "tests/api_connexion/endpoints/test_config_endpoint.py"
    ],
    "message": "Check if the lower of provided values are sensitives in config endpoint (#34712)\n\n* Check if the lower of provided values are sensitives in config endpoint\n\n* update unit test\n\n* ensure that all values in sensitive dict are in lower characters\n\n(cherry picked from commit f044589b685855a8fce8f5376bea2564c5a001f7)",
    "before_after_code_files": [
      "airflow/api_connexion/endpoints/config_endpoint.py||airflow/api_connexion/endpoints/config_endpoint.py",
      "airflow/configuration.py||airflow/configuration.py",
      "tests/api_connexion/endpoints/test_config_endpoint.py||tests/api_connexion/endpoints/test_config_endpoint.py"
    ]
  },
  "patch_diff": {
    "airflow/api_connexion/endpoints/config_endpoint.py||airflow/api_connexion/endpoints/config_endpoint.py": [
      "File: airflow/api_connexion/endpoints/config_endpoint.py -> airflow/api_connexion/endpoints/config_endpoint.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "123:                 \"Config not found.\", detail=f\"The option [{section}/{option}] is not found in config.\"",
      "124:             )",
      "127:             value = \"< hidden >\"",
      "128:         else:",
      "129:             value = conf.get(section, option)",
      "",
      "[Removed Lines]",
      "126:         if (section, option) in conf.sensitive_config_values:",
      "",
      "[Added Lines]",
      "126:         if (section.lower(), option.lower()) in conf.sensitive_config_values:",
      "",
      "---------------"
    ],
    "airflow/configuration.py||airflow/configuration.py": [
      "File: airflow/configuration.py -> airflow/configuration.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "311:             for s, s_c in self.configuration_description.items()",
      "312:             for k, item in s_c.get(\"options\").items()  # type: ignore[union-attr]",
      "313:         }",
      "315:         depr_option = {self.deprecated_options[x][:-1] for x in sensitive if x in self.deprecated_options}",
      "316:         depr_section = {",
      "317:             (self.deprecated_sections[s][0], k) for s, k in sensitive if s in self.deprecated_sections",
      "",
      "[Removed Lines]",
      "314:         sensitive = {(section, key) for (section, key), v in flattened.items() if v.get(\"sensitive\") is True}",
      "",
      "[Added Lines]",
      "314:         sensitive = {",
      "315:             (section.lower(), key.lower())",
      "316:             for (section, key), v in flattened.items()",
      "317:             if v.get(\"sensitive\") is True",
      "318:         }",
      "",
      "---------------"
    ],
    "tests/api_connexion/endpoints/test_config_endpoint.py||tests/api_connexion/endpoints/test_config_endpoint.py": [
      "File: tests/api_connexion/endpoints/test_config_endpoint.py -> tests/api_connexion/endpoints/test_config_endpoint.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "247:         return_value=MOCK_CONF_WITH_SENSITIVE_VALUE,",
      "248:     )",
      "249:     @conf_vars({(\"webserver\", \"expose_config\"): \"non-sensitive-only\"})",
      "251:         response = self.client.get(",
      "253:             headers={\"Accept\": \"text/plain\"},",
      "254:             environ_overrides={\"REMOTE_USER\": \"test\"},",
      "255:         )",
      "256:         assert response.status_code == 200",
      "257:         expected = textwrap.dedent(",
      "261:         \"\"\"",
      "262:         )",
      "263:         assert expected == response.data.decode()",
      "",
      "[Removed Lines]",
      "250:     def test_should_respond_200_text_plain_with_non_sensitive_only(self, mock_as_dict):",
      "252:             \"/api/v1/config/section/core/option/sql_alchemy_conn\",",
      "258:             \"\"\"\\",
      "259:         [core]",
      "260:         sql_alchemy_conn = < hidden >",
      "",
      "[Added Lines]",
      "250:     @pytest.mark.parametrize(",
      "251:         \"section, option\",",
      "252:         [",
      "253:             (\"core\", \"sql_alchemy_conn\"),",
      "254:             (\"core\", \"SQL_ALCHEMY_CONN\"),",
      "255:             (\"corE\", \"sql_alchemy_conn\"),",
      "256:             (\"CORE\", \"sql_alchemy_conn\"),",
      "257:         ],",
      "258:     )",
      "259:     def test_should_respond_200_text_plain_with_non_sensitive_only(self, mock_as_dict, section, option):",
      "261:             f\"/api/v1/config/section/{section}/option/{option}\",",
      "267:             f\"\"\"\\",
      "268:         [{section}]",
      "269:         {option} = < hidden >",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "711fdf79306394fdd3f975f02e8f004697c0f47c",
      "candidate_info": {
        "commit_hash": "711fdf79306394fdd3f975f02e8f004697c0f47c",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/711fdf79306394fdd3f975f02e8f004697c0f47c",
        "files": [
          "airflow/www/static/css/main.css"
        ],
        "message": "Remove infinite animation for pinwheel, spin for 1.5s (#34020)\n\n* Remove infinite animation for pinwheel, spin for 1.5s\n\n* Use media query as per @ryanahamilton's suggestion\n\n* Fix static checks\n\n(cherry picked from commit f8a5c8bf2b23b8a5a69b00e21ff37b58559c9dd6)",
        "before_after_code_files": [
          "airflow/www/static/css/main.css||airflow/www/static/css/main.css"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/www/static/css/main.css||airflow/www/static/css/main.css": [
          "File: airflow/www/static/css/main.css -> airflow/www/static/css/main.css",
          "--- Hunk 1 ---",
          "[Context before]",
          "102:   }",
          "103: }",
          "108: }",
          "110: .navbar-user-icon {",
          "",
          "[Removed Lines]",
          "105: .navbar-brand:hover .brand-logo-pinwheel {",
          "106:   transform-origin: 17.66px 17.66px;",
          "107:   animation: pinSpin 1.5s linear infinite;",
          "",
          "[Added Lines]",
          "105: @media (prefers-reduced-motion: no-preference) {",
          "106:   .navbar-brand:hover .brand-logo-pinwheel {",
          "107:     transform-origin: 17.66px 17.66px;",
          "108:     animation: pinSpin 1.5s linear;",
          "109:   }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "9dbac3670ccb28caff51ece9a93ad22a0d49e8d4",
      "candidate_info": {
        "commit_hash": "9dbac3670ccb28caff51ece9a93ad22a0d49e8d4",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/9dbac3670ccb28caff51ece9a93ad22a0d49e8d4",
        "files": [
          "tests/dag_processing/test_job_runner.py"
        ],
        "message": "Refactor: tmp_path in tests/dag_processing (#33740)\n\n(cherry picked from commit eb93f67b2b2478dfe413778cbd4673c95b4ca773)",
        "before_after_code_files": [
          "tests/dag_processing/test_job_runner.py||tests/dag_processing/test_job_runner.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "tests/dag_processing/test_job_runner.py||tests/dag_processing/test_job_runner.py": [
          "File: tests/dag_processing/test_job_runner.py -> tests/dag_processing/test_job_runner.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "29: import time",
          "30: from datetime import datetime, timedelta",
          "31: from logging.config import dictConfig",
          "33: from textwrap import dedent",
          "34: from unittest import mock",
          "35: from unittest.mock import MagicMock, PropertyMock",
          "",
          "[Removed Lines]",
          "32: from tempfile import TemporaryDirectory",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "145:             raise RuntimeError(\"Shouldn't get here - nothing to read, but manager not finished!\")",
          "147:     @conf_vars({(\"core\", \"load_examples\"): \"False\"})",
          "151:         # Generate original import error",
          "155:         child_pipe, parent_pipe = multiprocessing.Pipe()",
          "",
          "[Removed Lines]",
          "148:     def test_remove_file_clears_import_error(self, tmpdir):",
          "149:         filename_to_parse = tmpdir / \"temp_dag.py\"",
          "152:         with open(filename_to_parse, \"w\") as file_to_parse:",
          "153:             file_to_parse.writelines(\"an invalid airflow DAG\")",
          "",
          "[Added Lines]",
          "147:     def test_remove_file_clears_import_error(self, tmp_path):",
          "148:         path_to_parse = tmp_path / \"temp_dag.py\"",
          "151:         path_to_parse.write_text(\"an invalid airflow DAG\")",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "158:         manager = DagProcessorJobRunner(",
          "159:             job=Job(),",
          "160:             processor=DagFileProcessorManager(",
          "162:                 max_runs=1,",
          "163:                 processor_timeout=timedelta(days=365),",
          "164:                 signal_conn=child_pipe,",
          "",
          "[Removed Lines]",
          "161:                 dag_directory=tmpdir,",
          "",
          "[Added Lines]",
          "159:                 dag_directory=path_to_parse.parent,",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "174:             import_errors = session.query(errors.ImportError).all()",
          "175:             assert len(import_errors) == 1",
          "179:             # Rerun the scheduler once the dag file has been removed",
          "180:             self.run_processor_manager_one_loop(manager, parent_pipe)",
          "",
          "[Removed Lines]",
          "177:             filename_to_parse.remove()",
          "",
          "[Added Lines]",
          "175:             path_to_parse.unlink()",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "187:         parent_pipe.close()",
          "189:     @conf_vars({(\"core\", \"load_examples\"): \"False\"})",
          "191:         child_pipe, parent_pipe = multiprocessing.Pipe()",
          "209:         child_pipe.close()",
          "210:         parent_pipe.close()",
          "",
          "[Removed Lines]",
          "190:     def test_max_runs_when_no_files(self):",
          "193:         with TemporaryDirectory(prefix=\"empty-airflow-dags-\") as dags_folder:",
          "194:             async_mode = \"sqlite\" not in conf.get(\"database\", \"sql_alchemy_conn\")",
          "195:             manager = DagProcessorJobRunner(",
          "196:                 job=Job(),",
          "197:                 processor=DagFileProcessorManager(",
          "198:                     dag_directory=dags_folder,",
          "199:                     max_runs=1,",
          "200:                     processor_timeout=timedelta(days=365),",
          "201:                     signal_conn=child_pipe,",
          "202:                     dag_ids=[],",
          "203:                     pickle_dags=False,",
          "204:                     async_mode=async_mode,",
          "205:                 ),",
          "206:             )",
          "208:             self.run_processor_manager_one_loop(manager, parent_pipe)",
          "",
          "[Added Lines]",
          "188:     def test_max_runs_when_no_files(self, tmp_path):",
          "191:         async_mode = \"sqlite\" not in conf.get(\"database\", \"sql_alchemy_conn\")",
          "192:         manager = DagProcessorJobRunner(",
          "193:             job=Job(),",
          "194:             processor=DagFileProcessorManager(",
          "195:                 dag_directory=os.fspath(tmp_path),",
          "196:                 max_runs=1,",
          "197:                 processor_timeout=timedelta(days=365),",
          "198:                 signal_conn=child_pipe,",
          "199:                 dag_ids=[],",
          "200:                 pickle_dags=False,",
          "201:                 async_mode=async_mode,",
          "202:             ),",
          "203:         )",
          "205:         self.run_processor_manager_one_loop(manager, parent_pipe)",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "894:     @conf_vars({(\"core\", \"load_examples\"): \"False\"})",
          "895:     @mock.patch(\"airflow.dag_processing.manager.Stats.timing\")",
          "898:         dag_code = dedent(",
          "899:             \"\"\"",
          "900:         from airflow import DAG",
          "901:         dag = DAG(dag_id='temp_dag', schedule='0 0 * * *')",
          "902:         \"\"\"",
          "903:         )",
          "907:         child_pipe, parent_pipe = multiprocessing.Pipe()",
          "",
          "[Removed Lines]",
          "896:     def test_send_file_processing_statsd_timing(self, statsd_timing_mock, tmpdir):",
          "897:         filename_to_parse = tmpdir / \"temp_dag.py\"",
          "904:         with open(filename_to_parse, \"w\") as file_to_parse:",
          "905:             file_to_parse.writelines(dag_code)",
          "",
          "[Added Lines]",
          "893:     def test_send_file_processing_statsd_timing(self, statsd_timing_mock, tmp_path):",
          "894:         path_to_parse = tmp_path / \"temp_dag.py\"",
          "901:         path_to_parse.write_text(dag_code)",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "910:         manager = DagProcessorJobRunner(",
          "911:             job=Job(),",
          "912:             processor=DagFileProcessorManager(",
          "914:                 max_runs=1,",
          "915:                 processor_timeout=timedelta(days=365),",
          "916:                 signal_conn=child_pipe,",
          "",
          "[Removed Lines]",
          "913:                 dag_directory=tmpdir,",
          "",
          "[Added Lines]",
          "909:                 dag_directory=path_to_parse.parent,",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "938:             any_order=True,",
          "939:         )",
          "942:         \"\"\"Test DagProcessorJobRunner._refresh_dag_dir method\"\"\"",
          "943:         manager = DagProcessorJobRunner(",
          "944:             job=Job(),",
          "",
          "[Removed Lines]",
          "941:     def test_refresh_dags_dir_doesnt_delete_zipped_dags(self, tmpdir):",
          "",
          "[Added Lines]",
          "937:     def test_refresh_dags_dir_doesnt_delete_zipped_dags(self, tmp_path):",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "952:                 async_mode=True,",
          "953:             ),",
          "954:         )",
          "956:         zipped_dag_path = os.path.join(TEST_DAGS_FOLDER, \"test_zip.zip\")",
          "957:         dagbag.process_file(zipped_dag_path)",
          "958:         dag = dagbag.get_dag(\"test_zip_dag\")",
          "",
          "[Removed Lines]",
          "955:         dagbag = DagBag(dag_folder=tmpdir, include_examples=False)",
          "",
          "[Added Lines]",
          "951:         dagbag = DagBag(dag_folder=tmp_path, include_examples=False)",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "967:         # assert dag still active",
          "968:         assert dag.get_is_active()",
          "971:         \"\"\"Test DagProcessorJobRunner._refresh_dag_dir method\"\"\"",
          "972:         manager = DagProcessorJobRunner(",
          "973:             job=Job(),",
          "",
          "[Removed Lines]",
          "970:     def test_refresh_dags_dir_deactivates_deleted_zipped_dags(self, tmpdir):",
          "",
          "[Added Lines]",
          "966:     def test_refresh_dags_dir_deactivates_deleted_zipped_dags(self, tmp_path):",
          "",
          "---------------",
          "--- Hunk 11 ---",
          "[Context before]",
          "981:                 async_mode=True,",
          "982:             ),",
          "983:         )",
          "985:         zipped_dag_path = os.path.join(TEST_DAGS_FOLDER, \"test_zip.zip\")",
          "986:         dagbag.process_file(zipped_dag_path)",
          "987:         dag = dagbag.get_dag(\"test_zip_dag\")",
          "",
          "[Removed Lines]",
          "984:         dagbag = DagBag(dag_folder=tmpdir, include_examples=False)",
          "",
          "[Added Lines]",
          "980:         dagbag = DagBag(dag_folder=tmp_path, include_examples=False)",
          "",
          "---------------",
          "--- Hunk 12 ---",
          "[Context before]",
          "1000:         # assert dag deactivated",
          "1001:         assert not dag.get_is_active()",
          "1004:         \"\"\"Test DagProcessorJobRunner._refresh_dag_dir should not update dags outside its processor_subdir\"\"\"",
          "1007:         dag_path = os.path.join(TEST_DAGS_FOLDER, \"test_miscellaneous.py\")",
          "1008:         dagbag.process_file(dag_path)",
          "1009:         dag = dagbag.get_dag(\"miscellaneous_test_dag\")",
          "",
          "[Removed Lines]",
          "1003:     def test_refresh_dags_dir_does_not_interfer_with_dags_outside_its_subdir(self, tmpdir):",
          "1006:         dagbag = DagBag(dag_folder=tmpdir, include_examples=False)",
          "",
          "[Added Lines]",
          "999:     def test_refresh_dags_dir_does_not_interfer_with_dags_outside_its_subdir(self, tmp_path):",
          "1002:         dagbag = DagBag(dag_folder=tmp_path, include_examples=False)",
          "",
          "---------------",
          "--- Hunk 13 ---",
          "[Context before]",
          "1040:             (\"scheduler\", \"standalone_dag_processor\"): \"True\",",
          "1041:         }",
          "1042:     )",
          "1044:         \"\"\"Test DagProcessorJobRunner._fetch_callbacks method\"\"\"",
          "1045:         dag_filepath = TEST_DAG_FOLDER / \"test_on_failure_callback_dag.py\"",
          "",
          "[Removed Lines]",
          "1043:     def test_fetch_callbacks_from_database(self, tmpdir):",
          "",
          "[Added Lines]",
          "1039:     def test_fetch_callbacks_from_database(self, tmp_path):",
          "",
          "---------------",
          "--- Hunk 14 ---",
          "[Context before]",
          "1048:             dag_id=\"test_start_date_scheduling\",",
          "1049:             full_filepath=str(dag_filepath),",
          "1050:             is_failure_callback=True,",
          "1052:             run_id=\"123\",",
          "1053:         )",
          "1054:         callback2 = DagCallbackRequest(",
          "1055:             dag_id=\"test_start_date_scheduling\",",
          "1056:             full_filepath=str(dag_filepath),",
          "1057:             is_failure_callback=True,",
          "1059:             run_id=\"456\",",
          "1060:         )",
          "1061:         callback3 = SlaCallbackRequest(",
          "1062:             dag_id=\"test_start_date_scheduling\",",
          "1063:             full_filepath=str(dag_filepath),",
          "1065:         )",
          "1067:         with create_session() as session:",
          "",
          "[Removed Lines]",
          "1051:             processor_subdir=str(tmpdir),",
          "1058:             processor_subdir=str(tmpdir),",
          "1064:             processor_subdir=str(tmpdir),",
          "",
          "[Added Lines]",
          "1047:             processor_subdir=os.fspath(tmp_path),",
          "1054:             processor_subdir=os.fspath(tmp_path),",
          "1060:             processor_subdir=os.fspath(tmp_path),",
          "",
          "---------------",
          "--- Hunk 15 ---",
          "[Context before]",
          "1073:         manager = DagProcessorJobRunner(",
          "1074:             job=Job(),",
          "1075:             processor=DagFileProcessorManager(",
          "1077:                 max_runs=1,",
          "1078:                 processor_timeout=timedelta(days=365),",
          "1079:                 signal_conn=child_pipe,",
          "",
          "[Removed Lines]",
          "1076:                 dag_directory=str(tmpdir),",
          "",
          "[Added Lines]",
          "1072:                 dag_directory=os.fspath(tmp_path),",
          "",
          "---------------",
          "--- Hunk 16 ---",
          "[Context before]",
          "1093:             (\"scheduler\", \"standalone_dag_processor\"): \"True\",",
          "1094:         }",
          "1095:     )",
          "1097:         \"\"\"Test DagProcessorJobRunner._fetch_callbacks method\"\"\"",
          "1098:         dag_filepath = TEST_DAG_FOLDER / \"test_on_failure_callback_dag.py\"",
          "",
          "[Removed Lines]",
          "1096:     def test_fetch_callbacks_for_current_dag_directory_only(self, tmpdir):",
          "",
          "[Added Lines]",
          "1092:     def test_fetch_callbacks_for_current_dag_directory_only(self, tmp_path):",
          "",
          "---------------",
          "--- Hunk 17 ---",
          "[Context before]",
          "1101:             dag_id=\"test_start_date_scheduling\",",
          "1102:             full_filepath=str(dag_filepath),",
          "1103:             is_failure_callback=True,",
          "1105:             run_id=\"123\",",
          "1106:         )",
          "1107:         callback2 = DagCallbackRequest(",
          "",
          "[Removed Lines]",
          "1104:             processor_subdir=str(tmpdir),",
          "",
          "[Added Lines]",
          "1100:             processor_subdir=os.fspath(tmp_path),",
          "",
          "---------------",
          "--- Hunk 18 ---",
          "[Context before]",
          "1120:         manager = DagProcessorJobRunner(",
          "1121:             job=Job(),",
          "1122:             processor=DagFileProcessorManager(",
          "1124:                 max_runs=1,",
          "1125:                 processor_timeout=timedelta(days=365),",
          "1126:                 signal_conn=child_pipe,",
          "",
          "[Removed Lines]",
          "1123:                 dag_directory=tmpdir,",
          "",
          "[Added Lines]",
          "1119:                 dag_directory=tmp_path,",
          "",
          "---------------",
          "--- Hunk 19 ---",
          "[Context before]",
          "1141:             (\"core\", \"load_examples\"): \"False\",",
          "1142:         }",
          "1143:     )",
          "1145:         \"\"\"Test DagProcessorJobRunner._fetch_callbacks method\"\"\"",
          "1146:         dag_filepath = TEST_DAG_FOLDER / \"test_on_failure_callback_dag.py\"",
          "",
          "[Removed Lines]",
          "1144:     def test_fetch_callbacks_from_database_max_per_loop(self, tmpdir):",
          "",
          "[Added Lines]",
          "1140:     def test_fetch_callbacks_from_database_max_per_loop(self, tmp_path):",
          "",
          "---------------",
          "--- Hunk 20 ---",
          "[Context before]",
          "1152:                     full_filepath=str(dag_filepath),",
          "1153:                     is_failure_callback=True,",
          "1154:                     run_id=str(i),",
          "1156:                 )",
          "1157:                 session.add(DbCallbackRequest(callback=callback, priority_weight=i))",
          "",
          "[Removed Lines]",
          "1155:                     processor_subdir=str(tmpdir),",
          "",
          "[Added Lines]",
          "1151:                     processor_subdir=os.fspath(tmp_path),",
          "",
          "---------------",
          "--- Hunk 21 ---",
          "[Context before]",
          "1160:         manager = DagProcessorJobRunner(",
          "1161:             job=Job(),",
          "1162:             processor=DagFileProcessorManager(",
          "1164:                 max_runs=1,",
          "1165:                 processor_timeout=timedelta(days=365),",
          "1166:                 signal_conn=child_pipe,",
          "",
          "[Removed Lines]",
          "1163:                 dag_directory=str(tmpdir),",
          "",
          "[Added Lines]",
          "1159:                 dag_directory=str(tmp_path),",
          "",
          "---------------",
          "--- Hunk 22 ---",
          "[Context before]",
          "1184:             (\"core\", \"load_examples\"): \"False\",",
          "1185:         }",
          "1186:     )",
          "1188:         dag_filepath = TEST_DAG_FOLDER / \"test_on_failure_callback_dag.py\"",
          "1190:         with create_session() as session:",
          "",
          "[Removed Lines]",
          "1187:     def test_fetch_callbacks_from_database_not_standalone(self, tmpdir):",
          "",
          "[Added Lines]",
          "1183:     def test_fetch_callbacks_from_database_not_standalone(self, tmp_path):",
          "",
          "---------------",
          "--- Hunk 23 ---",
          "[Context before]",
          "1192:                 dag_id=\"test_start_date_scheduling\",",
          "1193:                 full_filepath=str(dag_filepath),",
          "1194:                 is_failure_callback=True,",
          "1196:                 run_id=\"123\",",
          "1197:             )",
          "1198:             session.add(DbCallbackRequest(callback=callback, priority_weight=10))",
          "",
          "[Removed Lines]",
          "1195:                 processor_subdir=str(tmpdir),",
          "",
          "[Added Lines]",
          "1191:                 processor_subdir=str(tmp_path),",
          "",
          "---------------",
          "--- Hunk 24 ---",
          "[Context before]",
          "1201:         manager = DagProcessorJobRunner(",
          "1202:             job=Job(),",
          "1203:             processor=DagFileProcessorManager(",
          "1205:                 max_runs=1,",
          "1206:                 processor_timeout=timedelta(days=365),",
          "1207:                 signal_conn=child_pipe,",
          "",
          "[Removed Lines]",
          "1204:                 dag_directory=tmpdir,",
          "",
          "[Added Lines]",
          "1200:                 dag_directory=tmp_path,",
          "",
          "---------------",
          "--- Hunk 25 ---",
          "[Context before]",
          "1219:         with create_session() as session:",
          "1220:             assert session.query(DbCallbackRequest).count() == 1",
          "1223:         # given",
          "1224:         manager = DagProcessorJobRunner(",
          "1225:             job=Job(),",
          "",
          "[Removed Lines]",
          "1222:     def test_callback_queue(self, tmpdir):",
          "",
          "[Added Lines]",
          "1218:     def test_callback_queue(self, tmp_path):",
          "",
          "---------------",
          "--- Hunk 26 ---",
          "[Context before]",
          "1239:             dag_id=\"dag1\",",
          "1240:             run_id=\"run1\",",
          "1241:             is_failure_callback=False,",
          "1243:             msg=None,",
          "1244:         )",
          "1245:         dag1_req2 = DagCallbackRequest(",
          "",
          "[Removed Lines]",
          "1242:             processor_subdir=tmpdir,",
          "",
          "[Added Lines]",
          "1238:             processor_subdir=tmp_path,",
          "",
          "---------------",
          "--- Hunk 27 ---",
          "[Context before]",
          "1247:             dag_id=\"dag1\",",
          "1248:             run_id=\"run1\",",
          "1249:             is_failure_callback=False,",
          "1251:             msg=None,",
          "1252:         )",
          "1253:         dag1_sla1 = SlaCallbackRequest(",
          "1254:             full_filepath=\"/green_eggs/ham/file1.py\",",
          "1255:             dag_id=\"dag1\",",
          "1257:         )",
          "1258:         dag1_sla2 = SlaCallbackRequest(",
          "1259:             full_filepath=\"/green_eggs/ham/file1.py\",",
          "1260:             dag_id=\"dag1\",",
          "1262:         )",
          "1264:         dag2_req1 = DagCallbackRequest(",
          "",
          "[Removed Lines]",
          "1250:             processor_subdir=tmpdir,",
          "1256:             processor_subdir=tmpdir,",
          "1261:             processor_subdir=tmpdir,",
          "",
          "[Added Lines]",
          "1246:             processor_subdir=tmp_path,",
          "1252:             processor_subdir=tmp_path,",
          "1257:             processor_subdir=tmp_path,",
          "",
          "---------------",
          "--- Hunk 28 ---",
          "[Context before]",
          "1266:             dag_id=\"dag2\",",
          "1267:             run_id=\"run1\",",
          "1268:             is_failure_callback=False,",
          "1270:             msg=None,",
          "1271:         )",
          "1273:         dag3_sla1 = SlaCallbackRequest(",
          "1274:             full_filepath=\"/green_eggs/ham/file3.py\",",
          "1275:             dag_id=\"dag3\",",
          "1277:         )",
          "1279:         # when",
          "",
          "[Removed Lines]",
          "1269:             processor_subdir=tmpdir,",
          "1276:             processor_subdir=tmpdir,",
          "",
          "[Added Lines]",
          "1265:             processor_subdir=tmp_path,",
          "1272:             processor_subdir=tmp_path,",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "1fdfa9af65e41251af2e69ff9511e0b6165388e0",
      "candidate_info": {
        "commit_hash": "1fdfa9af65e41251af2e69ff9511e0b6165388e0",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/1fdfa9af65e41251af2e69ff9511e0b6165388e0",
        "files": [
          "airflow/serialization/serializers/timezone.py"
        ],
        "message": "zoneinfo.ZoneInfo is not available on < Python 3.9 (#34804)\n\n(cherry picked from commit 25cd12d307be6359c844f4764b6f90e954bccb56)",
        "before_after_code_files": [
          "airflow/serialization/serializers/timezone.py||airflow/serialization/serializers/timezone.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/serialization/serializers/timezone.py||airflow/serialization/serializers/timezone.py": [
          "File: airflow/serialization/serializers/timezone.py -> airflow/serialization/serializers/timezone.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: from __future__ import annotations",
          "20: import datetime",
          "21: from typing import TYPE_CHECKING, Any, cast",
          "23: from airflow.utils.module_loading import qualname",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "21: import sys",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "29: serializers = [",
          "30:     \"pendulum.tz.timezone.FixedTimezone\",",
          "31:     \"pendulum.tz.timezone.Timezone\",",
          "34: ]",
          "36: deserializers = serializers",
          "38: __version__ = 1",
          "",
          "[Removed Lines]",
          "32:     \"zoneinfo.ZoneInfo\",",
          "33:     \"backports.zoneinfo.ZoneInfo\",",
          "",
          "[Added Lines]",
          "35: PY39 = sys.version_info >= (3, 9)",
          "37: if PY39:",
          "38:     serializers.append(\"zoneinfo.ZoneInfo\")",
          "39: else:",
          "40:     serializers.append(\"backports.zoneinfo.ZoneInfo\")",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "79:     if isinstance(data, int):",
          "80:         return fixed_timezone(data)",
          "89:         try:",
          "92:             from zoneinfo import ZoneInfo",
          "94:         return ZoneInfo(data)",
          "",
          "[Removed Lines]",
          "82:     if classname == \"zoneinfo.ZoneInfo\":",
          "83:         from zoneinfo import ZoneInfo",
          "85:         return ZoneInfo(data)",
          "87:     if classname == \"backports.zoneinfo.ZoneInfo\":",
          "88:         # python version might have been upgraded, so we need to check",
          "90:             from backports.zoneinfo import ZoneInfo",
          "91:         except ImportError:",
          "",
          "[Added Lines]",
          "88:     if \"zoneinfo.ZoneInfo\" in classname:",
          "91:         except ImportError:",
          "92:             from backports.zoneinfo import ZoneInfo",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "f0d4981f9cdb5d2219bd2fdc72f652f824c2ad3c",
      "candidate_info": {
        "commit_hash": "f0d4981f9cdb5d2219bd2fdc72f652f824c2ad3c",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/f0d4981f9cdb5d2219bd2fdc72f652f824c2ad3c",
        "files": [
          "airflow/www/static/js/dag/Main.tsx",
          "airflow/www/static/js/dag/details/index.tsx"
        ],
        "message": "Ensure details panel is shown when any tab is selected (#34136)\n\nThe query param `tab` is set on the DAGs view when any of the tabs _within_ the details panel is selected, thus we should show the details panel, even if the user previously hid it.\n\nThere are 2 main scenarios where a user would navigate to such a link:\n1. User clicks on one of the pills on the DAG view (Graph, Gantt, Code). Imo, the user will expect to see the relevant tab in the details panel, even if they have it hidden.\n2. User clicks on a link (e.g. from DAG runs, or someone sharing a link). Less clear cut than scenario 1, but still I think the most user friendly result is to show the requested tab (in the details panel).\n\n(cherry picked from commit 0b319e79ec97f6f4a8f8ce55119b6539138481cd)",
        "before_after_code_files": [
          "airflow/www/static/js/dag/Main.tsx||airflow/www/static/js/dag/Main.tsx",
          "airflow/www/static/js/dag/details/index.tsx||airflow/www/static/js/dag/details/index.tsx"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/www/static/js/dag/Main.tsx||airflow/www/static/js/dag/Main.tsx": [
          "File: airflow/www/static/js/dag/Main.tsx -> airflow/www/static/js/dag/Main.tsx",
          "--- Hunk 1 ---",
          "[Context before]",
          "29: import ShortcutCheatSheet from \"src/components/ShortcutCheatSheet\";",
          "30: import { useKeysPress } from \"src/utils/useKeysPress\";",
          "32: import Grid from \"./grid\";",
          "33: import FilterBar from \"./nav/FilterBar\";",
          "34: import LegendRow from \"./nav/LegendRow\";",
          "",
          "[Removed Lines]",
          "31: import Details from \"./details\";",
          "",
          "[Added Lines]",
          "31: import { useSearchParams } from \"react-router-dom\";",
          "32: import Details, { TAB_PARAM } from \"./details\";",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "66:     isLoading,",
          "67:   } = useGridData();",
          "68:   const [isGridCollapsed, setIsGridCollapsed] = useState(false);",
          "69:   const resizeRef = useRef<HTMLDivElement>(null);",
          "70:   const gridRef = useRef<HTMLDivElement>(null);",
          "71:   const gridScrollRef = useRef<HTMLDivElement>(null);",
          "72:   const ganttScrollRef = useRef<HTMLDivElement>(null);",
          "74:   const { isOpen, onToggle } = useDisclosure({ defaultIsOpen: isPanelOpen });",
          "75:   const [hoveredTaskState, setHoveredTaskState] = useState<",
          "76:     string | null | undefined",
          "",
          "[Removed Lines]",
          "73:   const isPanelOpen = localStorage.getItem(detailsPanelKey) !== \"true\";",
          "",
          "[Added Lines]",
          "70:   const [searchParams] = useSearchParams();",
          "75:   const isPanelOpen =",
          "76:     localStorage.getItem(detailsPanelKey) !== \"true\" ||",
          "77:     !!searchParams.get(TAB_PARAM);",
          "",
          "---------------"
        ],
        "airflow/www/static/js/dag/details/index.tsx||airflow/www/static/js/dag/details/index.tsx": [
          "File: airflow/www/static/js/dag/details/index.tsx -> airflow/www/static/js/dag/details/index.tsx",
          "--- Hunk 1 ---",
          "[Context before]",
          "109:   }",
          "110: };",
          "114: const Details = ({",
          "115:   openGroupIds,",
          "",
          "[Removed Lines]",
          "112: const TAB_PARAM = \"tab\";",
          "",
          "[Added Lines]",
          "112: export const TAB_PARAM = \"tab\";",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "7a590f021906136356ba669058e91bc83a62818d",
      "candidate_info": {
        "commit_hash": "7a590f021906136356ba669058e91bc83a62818d",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/7a590f021906136356ba669058e91bc83a62818d",
        "files": [
          "airflow/cli/commands/connection_command.py"
        ],
        "message": "fix connections exported output (#34640)\n\n(cherry picked from commit a5f5e2fc7f7b7f461458645c8826f015c1fa8d78)",
        "before_after_code_files": [
          "airflow/cli/commands/connection_command.py||airflow/cli/commands/connection_command.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/cli/commands/connection_command.py||airflow/cli/commands/connection_command.py": [
          "File: airflow/cli/commands/connection_command.py -> airflow/cli/commands/connection_command.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "203:         f.write(msg)",
          "205:     if file_is_stdout:",
          "207:     else:",
          "211: alternative_conn_specs = [\"conn_type\", \"conn_host\", \"conn_login\", \"conn_password\", \"conn_schema\", \"conn_port\"]",
          "",
          "[Removed Lines]",
          "206:         print(\"\\nConnections successfully exported.\", file=sys.stderr)",
          "208:         print(f\"Connections successfully exported to {args.file.name}.\")",
          "",
          "[Added Lines]",
          "206:         print(f\"\\n{len(connections)} connections successfully exported.\", file=sys.stderr)",
          "208:         print(f\"{len(connections)} connections successfully exported to {args.file.name}.\")",
          "",
          "---------------"
        ]
      }
    }
  ]
}