{
  "cve_id": "CVE-2021-45229",
  "cve_desc": "It was discovered that the \"Trigger DAG with config\" screen was susceptible to XSS attacks via the `origin` query argument. This issue affects Apache Airflow versions 2.2.3 and below.",
  "repo": "apache/airflow",
  "patch_hash": "628aa1f99c865d97d0b1c7c76e630e43a7b8d319",
  "patch_info": {
    "commit_hash": "628aa1f99c865d97d0b1c7c76e630e43a7b8d319",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/628aa1f99c865d97d0b1c7c76e630e43a7b8d319",
    "files": [
      "airflow/www/templates/airflow/trigger.html",
      "tests/www/views/test_views_trigger_dag.py"
    ],
    "message": "Simplify trigger cancel button (#21591)\n\nCo-authored-by: Jed Cunningham <jedcunningham@apache.org>\n(cherry picked from commit 65297673a318660fba76797e50d0c06804dfcafc)",
    "before_after_code_files": [
      "airflow/www/templates/airflow/trigger.html||airflow/www/templates/airflow/trigger.html",
      "tests/www/views/test_views_trigger_dag.py||tests/www/views/test_views_trigger_dag.py"
    ]
  },
  "patch_diff": {
    "airflow/www/templates/airflow/trigger.html||airflow/www/templates/airflow/trigger.html": [
      "File: airflow/www/templates/airflow/trigger.html -> airflow/www/templates/airflow/trigger.html",
      "--- Hunk 1 ---",
      "[Context before]",
      "63:       </label>",
      "64:     </div>",
      "65:     <button type=\"submit\" class=\"btn btn-primary\">Trigger</button>",
      "67:   </form>",
      "68: {% endblock %}",
      "",
      "[Removed Lines]",
      "66:     <button type=\"button\" class=\"btn\" onclick=\"location.href = '{{ origin }}'; return false\">Cancel</button>",
      "",
      "[Added Lines]",
      "66:     <a class=\"btn\" href=\"{{ origin }}\">Cancel</a>",
      "",
      "---------------"
    ],
    "tests/www/views/test_views_trigger_dag.py||tests/www/views/test_views_trigger_dag.py": [
      "File: tests/www/views/test_views_trigger_dag.py -> tests/www/views/test_views_trigger_dag.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "133:         (\"javascript:alert(1)\", \"/home\"),",
      "134:         (\"http://google.com\", \"/home\"),",
      "135:         (\"36539'%3balert(1)%2f%2f166\", \"/home\"),",
      "136:         (",
      "137:             \"%2Ftree%3Fdag_id%3Dexample_bash_operator';alert(33)//\",",
      "138:             \"/home\",",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "136:         (",
      "137:             '\"><script>alert(99)</script><a href=\"',",
      "138:             \"&#34;&gt;&lt;script&gt;alert(99)&lt;/script&gt;&lt;a href=&#34;\",",
      "139:         ),",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "145:     test_dag_id = \"example_bash_operator\"",
      "147:     resp = admin_client.get(f'trigger?dag_id={test_dag_id}&origin={test_origin}')",
      "156: @pytest.mark.parametrize(",
      "",
      "[Removed Lines]",
      "148:     check_content_in_response(",
      "149:         '<button type=\"button\" class=\"btn\" onclick=\"location.href = \\'{}\\'; return false\">'.format(",
      "150:             expected_origin",
      "151:         ),",
      "152:         resp,",
      "153:     )",
      "",
      "[Added Lines]",
      "152:     check_content_in_response(f'<a class=\"btn\" href=\"{expected_origin}\">Cancel</a>', resp)",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "95b9d484bbeddc5115b8ce79e0e6919887b008ad",
      "candidate_info": {
        "commit_hash": "95b9d484bbeddc5115b8ce79e0e6919887b008ad",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/95b9d484bbeddc5115b8ce79e0e6919887b008ad",
        "files": [
          "airflow/providers/amazon/aws/log/cloudwatch_task_handler.py",
          "setup.py"
        ],
        "message": "Move to watchtower 2.0.1 (#19907)\n\n- This version of watchtower contains patches that fixes #15279\n  where empty log lines would crash Watchtower.\n\n(cherry picked from commit 2539cb44b47d78e81a88fde51087f4cc77c924c5)",
        "before_after_code_files": [
          "airflow/providers/amazon/aws/log/cloudwatch_task_handler.py||airflow/providers/amazon/aws/log/cloudwatch_task_handler.py",
          "setup.py||setup.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/providers/amazon/aws/log/cloudwatch_task_handler.py||airflow/providers/amazon/aws/log/cloudwatch_task_handler.py": [
          "File: airflow/providers/amazon/aws/log/cloudwatch_task_handler.py -> airflow/providers/amazon/aws/log/cloudwatch_task_handler.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "81:         self.handler = watchtower.CloudWatchLogHandler(",
          "82:             log_group=self.log_group,",
          "83:             stream_name=self._render_filename(ti, ti.try_number),",
          "85:         )",
          "87:     def close(self):",
          "",
          "[Removed Lines]",
          "84:             boto3_session=self.hook.get_session(self.region_name),",
          "",
          "[Added Lines]",
          "84:             boto3_client=self.hook.get_conn(),",
          "",
          "---------------"
        ],
        "setup.py||setup.py": [
          "File: setup.py -> setup.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "186: ]",
          "187: amazon = [",
          "188:     'boto3>=1.15.0,<1.19.0',",
          "190:     'jsonpath_ng>=1.5.3',",
          "191:     'redshift_connector~=2.0.888',",
          "192:     'sqlalchemy_redshift~=0.8.6',",
          "",
          "[Removed Lines]",
          "189:     'watchtower~=1.0.6',",
          "",
          "[Added Lines]",
          "189:     'watchtower~=2.0.1',",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "f9ff33f1a68ad7b956079e18b4d90eaa5f4541e0",
      "candidate_info": {
        "commit_hash": "f9ff33f1a68ad7b956079e18b4d90eaa5f4541e0",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/f9ff33f1a68ad7b956079e18b4d90eaa5f4541e0",
        "files": [
          "airflow/providers/docker/operators/docker_swarm.py",
          "setup.py",
          "tests/providers/docker/operators/test_docker_swarm.py"
        ],
        "message": "Remove the docker timeout workaround (#18872)\n\n(cherry picked from commit 3154935138748a8ac89aa4c8fde848e31610941b)",
        "before_after_code_files": [
          "airflow/providers/docker/operators/docker_swarm.py||airflow/providers/docker/operators/docker_swarm.py",
          "setup.py||setup.py",
          "tests/providers/docker/operators/test_docker_swarm.py||tests/providers/docker/operators/test_docker_swarm.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/providers/docker/operators/docker_swarm.py||airflow/providers/docker/operators/docker_swarm.py": [
          "File: airflow/providers/docker/operators/docker_swarm.py -> airflow/providers/docker/operators/docker_swarm.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "17: \"\"\"Run ephemeral Docker Swarm services\"\"\"",
          "18: from typing import List, Optional, Union",
          "21: from docker import types",
          "23: from airflow.exceptions import AirflowException",
          "",
          "[Removed Lines]",
          "20: import requests",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "204:         while True:",
          "205:             try:",
          "206:                 log = next(logs)",
          "213:             except StopIteration:",
          "214:                 # If the service log stream terminated, stop fetching logs further.",
          "215:                 break",
          "",
          "[Removed Lines]",
          "207:             # TODO: Remove this clause once https://github.com/docker/docker-py/issues/931 is fixed",
          "208:             except requests.exceptions.ConnectionError:",
          "209:                 # If the service log stream stopped sending messages, check if it the service has",
          "210:                 # terminated.",
          "211:                 if self._has_service_terminated():",
          "212:                     break",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "setup.py||setup.py": [
          "File: setup.py -> setup.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "258:     'sphinxcontrib-spelling==7.2.1',",
          "259: ]",
          "260: docker = [",
          "262: ]",
          "263: drill = ['sqlalchemy-drill>=1.1.0', 'sqlparse>=0.4.1']",
          "264: druid = [",
          "",
          "[Removed Lines]",
          "261:     'docker',",
          "",
          "[Added Lines]",
          "261:     'docker>=5.0.3',",
          "",
          "---------------"
        ],
        "tests/providers/docker/operators/test_docker_swarm.py||tests/providers/docker/operators/test_docker_swarm.py": [
          "File: tests/providers/docker/operators/test_docker_swarm.py -> tests/providers/docker/operators/test_docker_swarm.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: from unittest import mock",
          "22: import pytest",
          "24: from docker import APIClient, types",
          "25: from parameterized import parameterized",
          "",
          "[Removed Lines]",
          "23: import requests",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "184:             operator.execute(None)",
          "185:         assert str(ctx.value) == msg",
          "234:     def test_on_kill(self):",
          "235:         client_mock = mock.Mock(spec=APIClient)",
          "",
          "[Removed Lines]",
          "187:     @mock.patch('airflow.providers.docker.operators.docker.APIClient')",
          "188:     @mock.patch('airflow.providers.docker.operators.docker_swarm.types')",
          "189:     def test_logging_with_requests_timeout(self, types_mock, client_class_mock):",
          "191:         mock_obj = mock.Mock()",
          "193:         def _client_tasks_side_effect():",
          "194:             for _ in range(2):",
          "195:                 yield [{'Status': {'State': 'pending'}}]",
          "196:             while True:",
          "197:                 yield [{'Status': {'State': 'complete'}}]",
          "199:         def _client_service_logs_effect():",
          "200:             yield b'Testing is awesome.'",
          "201:             raise requests.exceptions.ConnectionError('')",
          "203:         client_mock = mock.Mock(spec=APIClient)",
          "204:         client_mock.create_service.return_value = {'ID': 'some_id'}",
          "205:         client_mock.service_logs.return_value = _client_service_logs_effect()",
          "206:         client_mock.images.return_value = []",
          "207:         client_mock.pull.return_value = [b'{\"status\":\"pull log\"}']",
          "208:         client_mock.tasks.side_effect = _client_tasks_side_effect()",
          "209:         types_mock.TaskTemplate.return_value = mock_obj",
          "210:         types_mock.ContainerSpec.return_value = mock_obj",
          "211:         types_mock.RestartPolicy.return_value = mock_obj",
          "212:         types_mock.Resources.return_value = mock_obj",
          "214:         client_class_mock.return_value = client_mock",
          "216:         operator = DockerSwarmOperator(",
          "217:             api_version='1.19',",
          "218:             command='env',",
          "219:             environment={'UNIT': 'TEST'},",
          "220:             image='ubuntu:latest',",
          "221:             mem_limit='128m',",
          "222:             user='unittest',",
          "223:             task_id='unittest',",
          "224:             auto_remove=True,",
          "225:             tty=True,",
          "226:             enable_logging=True,",
          "227:         )",
          "228:         operator.execute(None)",
          "230:         client_mock.service_logs.assert_called_once_with(",
          "231:             'some_id', follow=True, stdout=True, stderr=True, is_tty=True",
          "232:         )",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "dd0a3a3d768b8cb2118c0b8d89ed0af0b393d865",
      "candidate_info": {
        "commit_hash": "dd0a3a3d768b8cb2118c0b8d89ed0af0b393d865",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/dd0a3a3d768b8cb2118c0b8d89ed0af0b393d865",
        "files": [
          "airflow/example_dags/tutorial_etl_dag.py"
        ],
        "message": "update tutorial_etl_dag notes (#21503)\n\n* update tutorial_etl_dag notes\n\n(cherry picked from commit a42607a4b75586a396d6a56145ed048d127dd344)",
        "before_after_code_files": [
          "airflow/example_dags/tutorial_etl_dag.py||airflow/example_dags/tutorial_etl_dag.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/example_dags/tutorial_etl_dag.py||airflow/example_dags/tutorial_etl_dag.py": [
          "File: airflow/example_dags/tutorial_etl_dag.py -> airflow/example_dags/tutorial_etl_dag.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: \"\"\"",
          "21: ### ETL DAG Tutorial Documentation",
          "25: \"\"\"",
          "26: # [START tutorial]",
          "27: # [START import_module]",
          "",
          "[Removed Lines]",
          "22: This ETL DAG is compatible with Airflow 1.10.x (specifically tested with 1.10.12) and is referenced",
          "23: as part of the documentation that goes along with the Airflow Functional DAG tutorial located",
          "24: [here](https://airflow.apache.org/tutorial_decorated_flows.html)",
          "",
          "[Added Lines]",
          "22: This ETL DAG is demonstrating an Extract -> Transform -> Load pipeline",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "83a4090c415dfeabfb7859490df29bf1554eb893",
      "candidate_info": {
        "commit_hash": "83a4090c415dfeabfb7859490df29bf1554eb893",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/83a4090c415dfeabfb7859490df29bf1554eb893",
        "files": [
          "tests/sensors/test_external_task_sensor.py"
        ],
        "message": "Fix occasional external task sensor tests (#18853)\n\nOccassionally the sensor tests fail with assertion where\nstate seems to be None. This might be caused by\n\n```\n      def assert_ti_state_equal(task_instance, state):\n          \"\"\"\n          Assert state of task_instances equals the given state.\n          \"\"\"\n          task_instance.refresh_from_db()\n  >       assert task_instance.state == state\n  E       AssertionError: assert None == <TaskInstanceState.SUCCESS: 'success'>\n  E        +  where None = <TaskI$anstance: dag_1.task_b_1 manual__2015-01-01T00:00:00+00:00 [None]>.state\n```\n\nTurned out it was because the task instance fields from\ndagrun.taskinstance relationship could be returned in different\norder so some of the dependencies were not met for some of the\ntasks when later task was returned before earlier one.\n\nDeterministic sorting according to task_id solved the problem.\n\n(cherry picked from commit 7a28ee370945de81fe8a16eac63197cbe93b3c3a)",
        "before_after_code_files": [
          "tests/sensors/test_external_task_sensor.py||tests/sensors/test_external_task_sensor.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "tests/sensors/test_external_task_sensor.py||tests/sensors/test_external_task_sensor.py": [
          "File: tests/sensors/test_external_task_sensor.py -> tests/sensors/test_external_task_sensor.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "569:             run_type=DagRunType.MANUAL,",
          "570:             session=session,",
          "571:         )",
          "573:             ti.refresh_from_task(dag.get_task(ti.task_id))",
          "574:             tis[ti.task_id] = ti",
          "575:             ti.run(session=session)",
          "",
          "[Removed Lines]",
          "572:         for ti in dagrun.task_instances:",
          "",
          "[Added Lines]",
          "572:         # we use sorting by task_id here because for the test DAG structure of ours",
          "573:         # this is equivalent to topological sort. It would not work in general case",
          "574:         # but it works for our case because we specifically constructed test DAGS",
          "575:         # in the way that those two sort methods are equivalent",
          "576:         tasks = sorted((ti for ti in dagrun.task_instances), key=lambda ti: ti.task_id)",
          "577:         for ti in tasks:",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "58a65e1acfd462deedd0fb1f6071a484864dcc07",
      "candidate_info": {
        "commit_hash": "58a65e1acfd462deedd0fb1f6071a484864dcc07",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/58a65e1acfd462deedd0fb1f6071a484864dcc07",
        "files": [
          "setup.cfg"
        ],
        "message": "Bump PyJWT from `<2` to `<3` (#20490)\n\n* Bump pyjwt from `<2` to `<3`\n\n* Update setup.cfg\n\n(cherry picked from commit 9e3fad9209031e52d3bcca6089c7813b4d1b8408)",
        "before_after_code_files": [
          "setup.cfg||setup.cfg"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "setup.cfg||setup.cfg": [
          "File: setup.cfg -> setup.cfg",
          "--- Hunk 1 ---",
          "[Context before]",
          "132:     pep562~=1.0;python_version<\"3.7\"",
          "133:     psutil>=4.2.0, <6.0.0",
          "134:     pygments>=2.0.1, <3.0",
          "137:     # python daemon crashes with 'socket operation on non-socket' for python 3.8+ in version < 2.2.4",
          "138:     # https://pagure.io/python-daemon/issue/34",
          "139:     python-daemon>=2.2.4",
          "",
          "[Removed Lines]",
          "135:     # Required for flask-jwt-extended and msal",
          "136:     pyjwt<2",
          "",
          "[Added Lines]",
          "135:     pyjwt<3",
          "",
          "---------------"
        ]
      }
    }
  ]
}