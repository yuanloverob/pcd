{
  "cve_id": "CVE-2022-31777",
  "cve_desc": "A stored cross-site scripting (XSS) vulnerability in Apache Spark 3.2.1 and earlier, and 3.3.0, allows remote attackers to execute arbitrary JavaScript in the web browser of a user, by including a malicious payload into the logs which would be returned in logs rendered in the UI.",
  "repo": "apache/spark",
  "patch_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
  "patch_info": {
    "commit_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
    "repo": "apache/spark",
    "commit_url": "https://github.com/apache/spark/commit/ad90195de56688ce0898691eb9d04297ab0871ad",
    "files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ],
    "message": "[SPARK-39505][UI] Escape log content rendered in UI\n\n### What changes were proposed in this pull request?\n\nEscape log content rendered to the UI.\n\n### Why are the changes needed?\n\nLog content may contain reserved characters or other code in the log and be misinterpreted in the UI as HTML.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nExisting tests\n\nCloses #36902 from srowen/LogViewEscape.\n\nAuthored-by: Sean Owen <srowen@gmail.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
    "before_after_code_files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ]
  },
  "patch_diff": {
    "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js": [
      "File: core/src/main/resources/org/apache/spark/ui/static/log-view.js -> core/src/main/resources/org/apache/spark/ui/static/log-view.js",
      "--- Hunk 1 ---",
      "[Context before]",
      "85:       if (retStartByte == 0) {",
      "86:         disableMoreButton();",
      "87:       }",
      "90:       curLogLength = curLogLength + (startByte - retStartByte);",
      "91:       startByte = retStartByte;",
      "",
      "[Removed Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(cleanData);",
      "",
      "[Added Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(document.createTextNode(cleanData));",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "115:             var retLogLength = dataInfo[2];",
      "117:             var cleanData = data.substring(newlineIndex + 1);",
      "120:             curLogLength = curLogLength + (retEndByte - retStartByte);",
      "121:             endByte = retEndByte;",
      "",
      "[Removed Lines]",
      "118:             $(\"pre\", \".log-content\").append(cleanData);",
      "",
      "[Added Lines]",
      "118:             $(\"pre\", \".log-content\").append(document.createTextNode(cleanData));",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "6d23b40273a84c9e65f2d78af312956954e47897",
      "candidate_info": {
        "commit_hash": "6d23b40273a84c9e65f2d78af312956954e47897",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/6d23b40273a84c9e65f2d78af312956954e47897",
        "files": [
          "mllib/src/main/scala/org/apache/spark/ml/recommendation/ALS.scala"
        ],
        "message": "[SPARK-38816][ML][DOCS] Fix comment about choice of initial factors in ALS\n\n### What changes were proposed in this pull request?\n\nChange a comment in ALS code to match impl. The comment refers to taking the absolute value of a Normal(0,1) value, but it doesn't.\n\n### Why are the changes needed?\n\nThe docs and impl are inconsistent. The current behavior actually seems fine, desirable, so, change the comments.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nExisting tests\n\nCloses #36228 from srowen/SPARK-38816.\n\nAuthored-by: Sean Owen <srowen@gmail.com>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>\n(cherry picked from commit b2b350b1566b8b45c6dba2f79ccbc2dc4e95816d)\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>",
        "before_after_code_files": [
          "mllib/src/main/scala/org/apache/spark/ml/recommendation/ALS.scala||mllib/src/main/scala/org/apache/spark/ml/recommendation/ALS.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "mllib/src/main/scala/org/apache/spark/ml/recommendation/ALS.scala||mllib/src/main/scala/org/apache/spark/ml/recommendation/ALS.scala": [
          "File: mllib/src/main/scala/org/apache/spark/ml/recommendation/ALS.scala -> mllib/src/main/scala/org/apache/spark/ml/recommendation/ALS.scala"
        ]
      }
    },
    {
      "candidate_hash": "30c6802574e5993e6f0f10d4c189c6e8325bcc5c",
      "candidate_info": {
        "commit_hash": "30c6802574e5993e6f0f10d4c189c6e8325bcc5c",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/30c6802574e5993e6f0f10d4c189c6e8325bcc5c",
        "files": [
          "external/docker-integration-tests/src/test/scala/org/apache/spark/sql/jdbc/MsSqlServerIntegrationSuite.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/jdbc/MsSqlServerDialect.scala"
        ],
        "message": "[SPARK-38889][SQL] Compile boolean column filters to use the bit type for MSSQL data source\n\n### What changes were proposed in this pull request?\nThis PR compiles the boolean data type to the bit data type for pushed column filters while querying the MSSQL data soruce. Microsoft SQL Server does not support the boolean type, so the JDBC dialect should use the bit data type instead.\n\n### Why are the changes needed?\n\nTo fix a bug that was exposed by the boolean column filter pushdown to SQL server data source.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nAdded a new integration test.\n\nCloses #36182 from allisonwang-db/spark-38889-mssql-predicate-pushdown.\n\nAuthored-by: allisonwang-db <allison.wang@databricks.com>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>\n(cherry picked from commit 320f88d54440e05228a90ef5663991e28ae07c95)\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>",
        "before_after_code_files": [
          "external/docker-integration-tests/src/test/scala/org/apache/spark/sql/jdbc/MsSqlServerIntegrationSuite.scala||external/docker-integration-tests/src/test/scala/org/apache/spark/sql/jdbc/MsSqlServerIntegrationSuite.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/jdbc/MsSqlServerDialect.scala||sql/core/src/main/scala/org/apache/spark/sql/jdbc/MsSqlServerDialect.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "external/docker-integration-tests/src/test/scala/org/apache/spark/sql/jdbc/MsSqlServerIntegrationSuite.scala||external/docker-integration-tests/src/test/scala/org/apache/spark/sql/jdbc/MsSqlServerIntegrationSuite.scala": [
          "File: external/docker-integration-tests/src/test/scala/org/apache/spark/sql/jdbc/MsSqlServerIntegrationSuite.scala -> external/docker-integration-tests/src/test/scala/org/apache/spark/sql/jdbc/MsSqlServerIntegrationSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "22: import java.util.Properties",
          "24: import org.apache.spark.sql.catalyst.util.DateTimeTestUtils._",
          "25: import org.apache.spark.sql.internal.SQLConf",
          "26: import org.apache.spark.tags.DockerTest",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "25: import org.apache.spark.sql.functions.col",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "140:         |'MULTIPOLYGON(((2 2, 2 -2, -2 -2, -2 2, 2 2)),((1 1, 3 1, 3 3, 1 3, 1 1)))',",
          "141:         |'GEOMETRYCOLLECTION(LINESTRING(1 1, 3 5),POLYGON((-1 -1, -1 -5, -5 -5, -5 -1, -1 -1)))')",
          "142:       \"\"\".stripMargin).executeUpdate()",
          "143:   }",
          "145:   test(\"Basic test\") {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "144:     conn.prepareStatement(",
          "145:       \"\"\"",
          "146:         |CREATE TABLE bits(a INT, b INT, c BIT)",
          "147:         |\"\"\".stripMargin).executeUpdate()",
          "148:     conn.prepareStatement(",
          "149:       \"\"\"",
          "150:         |INSERT INTO bits VALUES (1, 2, 1)",
          "151:       \"\"\".stripMargin).executeUpdate()",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "357:         0, 3, 0, 0, 0, -1, -1, -1, -1, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0,",
          "358:         0, 0, 0, 1, 0, 0, 0, 3))",
          "359:   }",
          "360: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "370:   test(\"SPARK-38889: MsSqlServerDialect should handle boolean filter push down\") {",
          "371:     val df = spark.read.jdbc(jdbcUrl, \"bits\", new Properties)",
          "372:     val rows = df.collect()",
          "373:     assert(rows.length == 1)",
          "374:     val filtered = df.where(col(\"c\") === 0).collect()",
          "375:     assert(filtered.length == 0)",
          "376:   }",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/jdbc/MsSqlServerDialect.scala||sql/core/src/main/scala/org/apache/spark/sql/jdbc/MsSqlServerDialect.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/jdbc/MsSqlServerDialect.scala -> sql/core/src/main/scala/org/apache/spark/sql/jdbc/MsSqlServerDialect.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "40:   override def canHandle(url: String): Boolean =",
          "41:     url.toLowerCase(Locale.ROOT).startsWith(\"jdbc:sqlserver\")",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "48:   override def compileValue(value: Any): Any = value match {",
          "49:     case booleanValue: Boolean => if (booleanValue) 1 else 0",
          "50:     case other => super.compileValue(other)",
          "51:   }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "2de364a4134670b46f606acabd59f204bcbd7dc2",
      "candidate_info": {
        "commit_hash": "2de364a4134670b46f606acabd59f204bcbd7dc2",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/2de364a4134670b46f606acabd59f204bcbd7dc2",
        "files": [
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CollectionExpressionsSuite.scala"
        ],
        "message": "[SPARK-40152][SQL][TESTS][FOLLOW-UP] Disable ANSI for out of bound test at ElementAt\n\nThis PR proposes to fix the test to pass with ANSI mode on. Currently `elementAt` test fails when ANSI mode is on:\n\n```\n[info] - elementAt *** FAILED *** (309 milliseconds)\n[info]   Exception evaluating element_at(stringsplitsql(11.12.13, .), 10, Some(), true) (ExpressionEvalHelper.scala:205)\n[info]   org.scalatest.exceptions.TestFailedException:\n[info]   at org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:472)\n[info]   at org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:471)\n[info]   at org.scalatest.funsuite.AnyFunSuite.newAssertionFailedException(AnyFunSuite.scala:1563)\n[info]   at org.scalatest.Assertions.fail(Assertions.scala:949)\n[info]   at org.scalatest.Assertions.fail$(Assertions.scala:945)\n[info]   at org.scalatest.funsuite.AnyFunSuite.fail(AnyFunSuite.scala:1563)\n[info]   at org.apache.spark.sql.catalyst.expressions.ExpressionEvalHelper.checkEvaluationWithoutCodegen(ExpressionEvalHelper.scala:205)\n[info]   at org.apache.spark.sql.catalyst.expressions.ExpressionEvalHelper.checkEvaluationWithoutCodegen$(ExpressionEvalHelper.scala:199)\n[info]   at org.apache.spark.sql.catalyst.expressions.CollectionExpressionsSuite.checkEvaluationWithoutCodegen(CollectionExpressionsSuite.scala:39)\n[info]   at org.apache.spark.sql.catalyst.expressions.ExpressionEvalHelper.checkEvaluation(ExpressionEvalHelper.scala:87)\n[info]   at org.apache.spark.sql.catalyst.expressions.ExpressionEvalHelper.checkEvaluation$(ExpressionEvalHelper.scala:82)\n[info]   at org.apache.spark.sql.catalyst.expressions.CollectionExpressionsSuite.checkEvaluation(CollectionExpressionsSuite.scala:39)\n[info]   at org.apache.spark.sql.catalyst.expressions.CollectionExpressionsSuite.$anonfun$new$333(CollectionExpressionsSuite.scala:1546)\n```\n\nhttps://github.com/apache/spark/runs/8046961366?check_suite_focus=true\n\nTo recover the build with ANSI mode.\n\nNo, test-only.\n\nUnittest fixed.\n\nCloses #37684 from HyukjinKwon/SPARK-40152.\n\nAuthored-by: Hyukjin Kwon <gurwls223@apache.org>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>\n(cherry picked from commit 4b0c3bab1ab082565a051990bf45774f15962deb)\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>",
        "before_after_code_files": [
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CollectionExpressionsSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CollectionExpressionsSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CollectionExpressionsSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CollectionExpressionsSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CollectionExpressionsSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CollectionExpressionsSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "1483:       checkEvaluation(ElementAt(a0, Literal(0)), null)",
          "1484:     }.getMessage.contains(\"SQL array indices start at 1\")",
          "1485:     intercept[Exception] { checkEvaluation(ElementAt(a0, Literal(1.1)), null) }",
          "1487:       checkEvaluation(ElementAt(a0, Literal(4)), null)",
          "1488:       checkEvaluation(ElementAt(a0, Literal(-4)), null)",
          "1489:     }",
          "",
          "[Removed Lines]",
          "1486:     withSQLConf(SQLConf.ANSI_ENABLED.key -> \"false\") {",
          "",
          "[Added Lines]",
          "1486:     withSQLConf(SQLConf.ANSI_ENABLED.key -> false.toString) {",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1513:     assert(ElementAt(m0, Literal(1.0)).checkInputDataTypes().isFailure)",
          "1516:       checkEvaluation(ElementAt(m0, Literal(\"d\")), null)",
          "1517:       checkEvaluation(ElementAt(m1, Literal(\"a\")), null)",
          "1518:     }",
          "",
          "[Removed Lines]",
          "1515:     withSQLConf(SQLConf.ANSI_ENABLED.key -> \"false\") {",
          "",
          "[Added Lines]",
          "1515:     withSQLConf(SQLConf.ANSI_ENABLED.key -> false.toString) {",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "1529:       MapType(BinaryType, StringType))",
          "1530:     val mb1 = Literal.create(Map[Array[Byte], String](), MapType(BinaryType, StringType))",
          "1533:       checkEvaluation(ElementAt(mb0, Literal(Array[Byte](1, 2, 3))), null)",
          "1534:       checkEvaluation(ElementAt(mb1, Literal(Array[Byte](1, 2))), null)",
          "1535:     }",
          "",
          "[Removed Lines]",
          "1532:     withSQLConf(SQLConf.ANSI_ENABLED.key -> \"false\") {",
          "",
          "[Added Lines]",
          "1532:     withSQLConf(SQLConf.ANSI_ENABLED.key -> false.toString) {",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "1537:     checkEvaluation(ElementAt(mb0, Literal(Array[Byte](3, 4))), null)",
          "1556:   }",
          "1558:   test(\"correctly handles ElementAt nullability for arrays\") {",
          "",
          "[Removed Lines]",
          "1540:     val delimiter = Literal.create(\".\", StringType)",
          "1541:     val str = StringSplitSQL(Literal.create(\"11.12.13\", StringType), delimiter)",
          "1542:     val outOfBoundValue = Some(Literal.create(\"\", StringType))",
          "1544:     checkEvaluation(ElementAt(str, Literal(3), outOfBoundValue), UTF8String.fromString(\"13\"))",
          "1545:     checkEvaluation(ElementAt(str, Literal(1), outOfBoundValue), UTF8String.fromString(\"11\"))",
          "1546:     checkEvaluation(ElementAt(str, Literal(10), outOfBoundValue), UTF8String.fromString(\"\"))",
          "1547:     checkEvaluation(ElementAt(str, Literal(-10), outOfBoundValue), UTF8String.fromString(\"\"))",
          "1549:     checkEvaluation(ElementAt(StringSplitSQL(Literal.create(null, StringType), delimiter),",
          "1550:       Literal(1), outOfBoundValue), null)",
          "1551:     checkEvaluation(ElementAt(StringSplitSQL(Literal.create(\"11.12.13\", StringType),",
          "1552:       Literal.create(null, StringType)), Literal(1), outOfBoundValue), null)",
          "1554:     checkExceptionInExpression[Exception](",
          "1555:       ElementAt(str, Literal(0), outOfBoundValue), \"The index 0 is invalid\")",
          "",
          "[Added Lines]",
          "1540:     withSQLConf(SQLConf.ANSI_ENABLED.key -> false.toString) {",
          "1541:       val delimiter = Literal.create(\".\", StringType)",
          "1542:       val str = StringSplitSQL(Literal.create(\"11.12.13\", StringType), delimiter)",
          "1543:       val outOfBoundValue = Some(Literal.create(\"\", StringType))",
          "1545:       checkEvaluation(ElementAt(str, Literal(3), outOfBoundValue), UTF8String.fromString(\"13\"))",
          "1546:       checkEvaluation(ElementAt(str, Literal(1), outOfBoundValue), UTF8String.fromString(\"11\"))",
          "1547:       checkEvaluation(ElementAt(str, Literal(10), outOfBoundValue), UTF8String.fromString(\"\"))",
          "1548:       checkEvaluation(ElementAt(str, Literal(-10), outOfBoundValue), UTF8String.fromString(\"\"))",
          "1550:       checkEvaluation(ElementAt(StringSplitSQL(Literal.create(null, StringType), delimiter),",
          "1551:         Literal(1), outOfBoundValue), null)",
          "1552:       checkEvaluation(ElementAt(StringSplitSQL(Literal.create(\"11.12.13\", StringType),",
          "1553:         Literal.create(null, StringType)), Literal(1), outOfBoundValue), null)",
          "1555:       checkExceptionInExpression[Exception](",
          "1556:         ElementAt(str, Literal(0), outOfBoundValue), \"The index 0 is invalid\")",
          "1557:     }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "b39ed56193ea0ade80960cd920536426d85680f1",
      "candidate_info": {
        "commit_hash": "b39ed56193ea0ade80960cd920536426d85680f1",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/b39ed56193ea0ade80960cd920536426d85680f1",
        "files": [
          "core/src/main/resources/error/error-classes.json",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/AnalysisErrorSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/GeneratorFunctionSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryCompilationErrorsSuite.scala"
        ],
        "message": "[SPARK-38687][SQL][3.3] Use error classes in the compilation errors of generators\n\n## What changes were proposed in this pull request?\nMigrate the following errors in QueryCompilationErrors onto use error classes:\n\n- nestedGeneratorError => UNSUPPORTED_GENERATOR.NESTED_IN_EXPRESSIONS\n- moreThanOneGeneratorError => UNSUPPORTED_GENERATOR.MULTI_GENERATOR\n- generatorOutsideSelectError => UNSUPPORTED_GENERATOR.OUTSIDE_SELECT\n- generatorNotExpectedError => UNSUPPORTED_GENERATOR.NOT_GENERATOR\n\nThis is a backport of https://github.com/apache/spark/pull/36617.\n\n### Why are the changes needed?\nPorting compilation errors of generator to new error framework, improve test coverage, and document expected error messages in tests.\n\n### Does this PR introduce any user-facing change?\nNo\n\n### How was this patch tested?\nBy running new test:\n```\n$ build/sbt \"sql/testOnly *QueryCompilationErrorsSuite*\"\n```\n\nCloses #36956 from panbingkun/branch-3.3-SPARK-38687.\n\nAuthored-by: panbingkun <pbk1982@gmail.com>\nSigned-off-by: Max Gekk <max.gekk@gmail.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/AnalysisErrorSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/AnalysisErrorSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/GeneratorFunctionSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/GeneratorFunctionSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryCompilationErrorsSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/errors/QueryCompilationErrorsSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "30: import org.apache.spark.sql.catalyst.plans.JoinType",
          "31: import org.apache.spark.sql.catalyst.plans.logical.{InsertIntoStatement, Join, LogicalPlan, SerdeInfo, Window}",
          "32: import org.apache.spark.sql.catalyst.trees.{Origin, TreeNode}",
          "34: import org.apache.spark.sql.connector.catalog._",
          "35: import org.apache.spark.sql.connector.catalog.CatalogV2Implicits._",
          "36: import org.apache.spark.sql.connector.catalog.functions.{BoundFunction, UnboundFunction}",
          "",
          "[Removed Lines]",
          "33: import org.apache.spark.sql.catalyst.util.{toPrettySQL, FailFastMode, ParseMode, PermissiveMode}",
          "",
          "[Added Lines]",
          "33: import org.apache.spark.sql.catalyst.util.{FailFastMode, ParseMode, PermissiveMode}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "113:   }",
          "115:   def nestedGeneratorError(trimmedNestedGenerator: Expression): Throwable = {",
          "119:   }",
          "121:   def moreThanOneGeneratorError(generators: Seq[Expression], clause: String): Throwable = {",
          "125:   }",
          "127:   def generatorOutsideSelectError(plan: LogicalPlan): Throwable = {",
          "131:   }",
          "133:   def legacyStoreAssignmentPolicyError(): Throwable = {",
          "",
          "[Removed Lines]",
          "116:     new AnalysisException(",
          "117:       \"Generators are not supported when it's nested in \" +",
          "118:         \"expressions, but got: \" + toPrettySQL(trimmedNestedGenerator))",
          "122:     new AnalysisException(",
          "123:       s\"Only one generator allowed per $clause clause but found \" +",
          "124:         generators.size + \": \" + generators.map(toPrettySQL).mkString(\", \"))",
          "128:     new AnalysisException(",
          "129:       \"Generators are not supported outside the SELECT clause, but \" +",
          "130:         \"got: \" + plan.simpleString(SQLConf.get.maxToStringFields))",
          "",
          "[Added Lines]",
          "116:     new AnalysisException(errorClass = \"UNSUPPORTED_GENERATOR\",",
          "117:       messageParameters = Array(\"NESTED_IN_EXPRESSIONS\", toSQLExpr(trimmedNestedGenerator)))",
          "121:     new AnalysisException(errorClass = \"UNSUPPORTED_GENERATOR\",",
          "122:       messageParameters = Array(\"MULTI_GENERATOR\",",
          "123:         clause, generators.size.toString, generators.map(toSQLExpr).mkString(\", \")))",
          "127:     new AnalysisException(errorClass = \"UNSUPPORTED_GENERATOR\",",
          "128:       messageParameters = Array(\"OUTSIDE_SELECT\", plan.simpleString(SQLConf.get.maxToStringFields)))",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "324:   }",
          "326:   def generatorNotExpectedError(name: FunctionIdentifier, classCanonicalName: String): Throwable = {",
          "329:   }",
          "331:   def functionWithUnsupportedSyntaxError(prettyName: String, syntax: String): Throwable = {",
          "",
          "[Removed Lines]",
          "327:     new AnalysisException(s\"$name is expected to be a generator. However, \" +",
          "328:       s\"its class is $classCanonicalName, which is not a generator.\")",
          "",
          "[Added Lines]",
          "325:     new AnalysisException(errorClass = \"UNSUPPORTED_GENERATOR\",",
          "326:       messageParameters = Array(\"NOT_GENERATOR\", toSQLId(name.toString), classCanonicalName))",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/AnalysisErrorSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/AnalysisErrorSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/AnalysisErrorSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/AnalysisErrorSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "483:   errorTest(",
          "484:     \"generator nested in expressions\",",
          "485:     listRelation.select(Explode($\"list\") + 1),",
          "487:       :: Nil",
          "488:   )",
          "",
          "[Removed Lines]",
          "486:     \"Generators are not supported when it's nested in expressions, but got: (explode(list) + 1)\"",
          "",
          "[Added Lines]",
          "486:     \"The generator is not supported: nested in expressions \\\"(explode(list) + 1)\\\"\"",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "494:         AttributeReference(\"nestedList\", ArrayType(ArrayType(IntegerType)))())",
          "495:       nestedListRelation.select(Explode(Explode($\"nestedList\")))",
          "496:     },",
          "499:   )",
          "501:   errorTest(",
          "502:     \"SPARK-30998: unsupported nested inner generators for aggregates\",",
          "503:     testRelation.select(Explode(Explode(",
          "504:       CreateArray(CreateArray(min($\"a\") :: max($\"a\") :: Nil) :: Nil)))),",
          "507:   )",
          "509:   errorTest(",
          "510:     \"generator nested in expressions for aggregates\",",
          "511:     testRelation.select(Explode(CreateArray(min($\"a\") :: max($\"a\") :: Nil)) + 1),",
          "514:   )",
          "516:   errorTest(",
          "517:     \"generator appears in operator which is not Project\",",
          "518:     listRelation.sortBy(Explode($\"list\").asc),",
          "520:   )",
          "522:   errorTest(",
          "",
          "[Removed Lines]",
          "497:     \"Generators are not supported when it's nested in expressions, but got: \" +",
          "498:       \"explode(explode(nestedList))\" :: Nil",
          "505:     \"Generators are not supported when it's nested in expressions, but got: \" +",
          "506:       \"explode(explode(array(array(min(a), max(a)))))\" :: Nil",
          "512:     \"Generators are not supported when it's nested in expressions, but got: \" +",
          "513:       \"(explode(array(min(a), max(a))) + 1)\" :: Nil",
          "519:     \"Generators are not supported outside the SELECT clause, but got: Sort\" :: Nil",
          "",
          "[Added Lines]",
          "497:     \"The generator is not supported: nested in expressions \" +",
          "498:     \"\"\"\"explode(explode(nestedList))\"\"\"\" :: Nil",
          "505:     \"The generator is not supported: nested in expressions \" +",
          "506:     \"\"\"\"explode(explode(array(array(min(a), max(a)))))\"\"\"\" :: Nil",
          "512:     \"The generator is not supported: nested in expressions \" +",
          "513:     \"\"\"\"(explode(array(min(a), max(a))) + 1)\"\"\"\" :: Nil",
          "519:     \"The generator is not supported: outside the SELECT clause, found: Sort\" :: Nil",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "534:   errorTest(",
          "535:     \"more than one generators in SELECT\",",
          "536:     listRelation.select(Explode($\"list\"), Explode($\"list\")),",
          "538:   )",
          "540:   errorTest(",
          "541:     \"more than one generators for aggregates in SELECT\",",
          "542:     testRelation.select(Explode(CreateArray(min($\"a\") :: Nil)),",
          "543:       Explode(CreateArray(max($\"a\") :: Nil))),",
          "546:   )",
          "548:   errorTest(",
          "",
          "[Removed Lines]",
          "537:     \"Only one generator allowed per select clause but found 2: explode(list), explode(list)\" :: Nil",
          "544:     \"Only one generator allowed per select clause but found 2: \" +",
          "545:       \"explode(array(min(a))), explode(array(max(a)))\" :: Nil",
          "",
          "[Added Lines]",
          "537:     \"The generator is not supported: only one generator allowed per select clause but found 2: \" +",
          "538:     \"\"\"\"explode(list)\", \"explode(list)\"\"\"\" :: Nil",
          "545:     \"The generator is not supported: only one generator allowed per select clause but found 2: \" +",
          "546:     \"\"\"\"explode(array(min(a)))\", \"explode(array(max(a)))\"\"\"\" :: Nil",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/GeneratorFunctionSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/GeneratorFunctionSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/GeneratorFunctionSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/GeneratorFunctionSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "332:       val msg1 = intercept[AnalysisException] {",
          "333:         sql(\"select 1 + explode(array(min(c2), max(c2))) from t1 group by c1\")",
          "334:       }.getMessage",
          "337:       val msg2 = intercept[AnalysisException] {",
          "338:         sql(",
          "",
          "[Removed Lines]",
          "335:       assert(msg1.contains(\"Generators are not supported when it's nested in expressions\"))",
          "",
          "[Added Lines]",
          "335:       assert(msg1.contains(\"The generator is not supported: nested in expressions\"))",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "342:             |from t1 group by c1",
          "343:           \"\"\".stripMargin)",
          "344:       }.getMessage",
          "346:     }",
          "347:   }",
          "",
          "[Removed Lines]",
          "345:       assert(msg2.contains(\"Only one generator allowed per aggregate clause\"))",
          "",
          "[Added Lines]",
          "345:       assert(msg2.contains(\"The generator is not supported: \" +",
          "346:         \"only one generator allowed per aggregate clause\"))",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "350:     val errMsg = intercept[AnalysisException] {",
          "351:       sql(\"SELECT array(array(1, 2), array(3)) v\").select(explode(explode($\"v\"))).collect",
          "352:     }.getMessage",
          "355:   }",
          "357:   test(\"SPARK-30997: generators in aggregate expressions for dataframe\") {",
          "",
          "[Removed Lines]",
          "353:     assert(errMsg.contains(\"Generators are not supported when it's nested in expressions, \" +",
          "354:       \"but got: explode(explode(v))\"))",
          "",
          "[Added Lines]",
          "354:     assert(errMsg.contains(\"The generator is not supported: \" +",
          "355:       \"nested in expressions \\\"explode(explode(v))\\\"\"))",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryCompilationErrorsSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/errors/QueryCompilationErrorsSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/errors/QueryCompilationErrorsSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/errors/QueryCompilationErrorsSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "205:       \"The deserializer is not supported: try to map \\\"STRUCT<a: STRING, b: INT>\\\" \" +",
          "206:       \"to Tuple1, but failed as the number of fields does not line up.\")",
          "207:   }",
          "208: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "209:   test(\"UNSUPPORTED_GENERATOR: \" +",
          "210:     \"generators are not supported when it's nested in expressions\") {",
          "211:     val e = intercept[AnalysisException](",
          "212:       sql(\"\"\"select explode(Array(1, 2, 3)) + 1\"\"\").collect()",
          "213:     )",
          "214:     assert(e.errorClass === Some(\"UNSUPPORTED_GENERATOR\"))",
          "215:     assert(e.message ===",
          "216:       \"\"\"The generator is not supported: \"\"\" +",
          "217:       \"\"\"nested in expressions \"(explode(array(1, 2, 3)) + 1)\"\"\"\")",
          "218:   }",
          "220:   test(\"UNSUPPORTED_GENERATOR: only one generator allowed\") {",
          "221:     val e = intercept[AnalysisException](",
          "222:       sql(\"\"\"select explode(Array(1, 2, 3)), explode(Array(1, 2, 3))\"\"\").collect()",
          "223:     )",
          "224:     assert(e.errorClass === Some(\"UNSUPPORTED_GENERATOR\"))",
          "225:     assert(e.message ===",
          "226:       \"The generator is not supported: only one generator allowed per select clause \" +",
          "227:       \"\"\"but found 2: \"explode(array(1, 2, 3))\", \"explode(array(1, 2, 3))\"\"\"\")",
          "228:   }",
          "230:   test(\"UNSUPPORTED_GENERATOR: generators are not supported outside the SELECT clause\") {",
          "231:     val e = intercept[AnalysisException](",
          "232:       sql(\"\"\"select 1 from t order by explode(Array(1, 2, 3))\"\"\").collect()",
          "233:     )",
          "234:     assert(e.errorClass === Some(\"UNSUPPORTED_GENERATOR\"))",
          "235:     assert(e.message ===",
          "236:       \"The generator is not supported: outside the SELECT clause, found: \" +",
          "237:       \"'Sort [explode(array(1, 2, 3)) ASC NULLS FIRST], true\")",
          "238:   }",
          "240:   test(\"UNSUPPORTED_GENERATOR: not a generator\") {",
          "241:     val e = intercept[AnalysisException](",
          "242:       sql(",
          "243:         \"\"\"",
          "244:           |SELECT explodedvalue.*",
          "245:           |FROM VALUES array(1, 2, 3) AS (value)",
          "246:           |LATERAL VIEW array_contains(value, 1) AS explodedvalue\"\"\".stripMargin).collect()",
          "247:     )",
          "248:     assert(e.errorClass === Some(\"UNSUPPORTED_GENERATOR\"))",
          "249:     assert(e.message ===",
          "250:       \"\"\"The generator is not supported: `array_contains` is expected to be a generator. \"\"\" +",
          "251:       \"However, its class is org.apache.spark.sql.catalyst.expressions.ArrayContains, \" +",
          "252:       \"which is not a generator.\")",
          "253:   }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "9353437ce11f646373d557f2c24aeb381b2aeec5",
      "candidate_info": {
        "commit_hash": "9353437ce11f646373d557f2c24aeb381b2aeec5",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/9353437ce11f646373d557f2c24aeb381b2aeec5",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/LimitPushDownThroughWindow.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/LimitPushdownThroughWindowSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/DataFrameWindowFunctionsSuite.scala"
        ],
        "message": "[SPARK-40002][SQL] Don't push down limit through window using ntile\n\n### What changes were proposed in this pull request?\n\nChange `LimitPushDownThroughWindow` so that it no longer supports pushing down a limit through a window using ntile.\n\n### Why are the changes needed?\n\nIn an unpartitioned window, the ntile function is currently applied to the result of the limit. This behavior produces results that conflict with Spark 3.1.3, Hive 2.3.9 and Prestodb 0.268\n\n#### Example\n\nAssume this data:\n```\ncreate table t1 stored as parquet as\nselect *\nfrom range(101);\n```\nAlso assume this query:\n```\nselect id, ntile(10) over (order by id) as nt\nfrom t1\nlimit 10;\n```\nWith Spark 3.2.2, Spark 3.3.0, and master, the limit is applied before the ntile function:\n```\n+---+---+\n|id |nt |\n+---+---+\n|0  |1  |\n|1  |2  |\n|2  |3  |\n|3  |4  |\n|4  |5  |\n|5  |6  |\n|6  |7  |\n|7  |8  |\n|8  |9  |\n|9  |10 |\n+---+---+\n```\nWith Spark 3.1.3, and Hive 2.3.9, and Prestodb 0.268, the limit is applied _after_ ntile.\n\nSpark 3.1.3:\n```\n+---+---+\n|id |nt |\n+---+---+\n|0  |1  |\n|1  |1  |\n|2  |1  |\n|3  |1  |\n|4  |1  |\n|5  |1  |\n|6  |1  |\n|7  |1  |\n|8  |1  |\n|9  |1  |\n+---+---+\n```\nHive 2.3.9:\n```\n+-----+-----+\n| id  | nt  |\n+-----+-----+\n| 0   | 1   |\n| 1   | 1   |\n| 2   | 1   |\n| 3   | 1   |\n| 4   | 1   |\n| 5   | 1   |\n| 6   | 1   |\n| 7   | 1   |\n| 8   | 1   |\n| 9   | 1   |\n+-----+-----+\n10 rows selected (1.72 seconds)\n```\nPrestodb 0.268:\n```\n id | nt\n----+----\n  0 |  1\n  1 |  1\n  2 |  1\n  3 |  1\n  4 |  1\n  5 |  1\n  6 |  1\n  7 |  1\n  8 |  1\n  9 |  1\n(10 rows)\n\n```\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nTwo new unit tests.\n\nCloses #37443 from bersprockets/pushdown_ntile.\n\nAuthored-by: Bruce Robbins <bersprockets@gmail.com>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>\n(cherry picked from commit c9156e5a3b9cb290c7cdda8db298c9875e67aa5e)\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/LimitPushDownThroughWindow.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/LimitPushDownThroughWindow.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/LimitPushdownThroughWindowSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/LimitPushdownThroughWindowSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/DataFrameWindowFunctionsSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/DataFrameWindowFunctionsSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/LimitPushDownThroughWindow.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/LimitPushDownThroughWindow.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/LimitPushDownThroughWindow.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/LimitPushDownThroughWindow.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: package org.apache.spark.sql.catalyst.optimizer",
          "21: import org.apache.spark.sql.catalyst.plans.logical.{Limit, LocalLimit, LogicalPlan, Project, Sort, Window}",
          "22: import org.apache.spark.sql.catalyst.rules.Rule",
          "23: import org.apache.spark.sql.catalyst.trees.TreePattern.{LIMIT, WINDOW}",
          "",
          "[Removed Lines]",
          "20: import org.apache.spark.sql.catalyst.expressions.{Alias, CurrentRow, DenseRank, IntegerLiteral, NamedExpression, NTile, Rank, RowFrame, RowNumber, SpecifiedWindowFrame, UnboundedPreceding, WindowExpression, WindowSpecDefinition}",
          "",
          "[Added Lines]",
          "20: import org.apache.spark.sql.catalyst.expressions.{Alias, CurrentRow, DenseRank, IntegerLiteral, NamedExpression, Rank, RowFrame, RowNumber, SpecifiedWindowFrame, UnboundedPreceding, WindowExpression, WindowSpecDefinition}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "34:   private def supportsPushdownThroughWindow(",
          "35:       windowExpressions: Seq[NamedExpression]): Boolean = windowExpressions.forall {",
          "38:         SpecifiedWindowFrame(RowFrame, UnboundedPreceding, CurrentRow))), _) => true",
          "39:     case _ => false",
          "40:   }",
          "",
          "[Removed Lines]",
          "36:     case Alias(WindowExpression(_: Rank | _: DenseRank | _: NTile | _: RowNumber,",
          "37:         WindowSpecDefinition(Nil, _,",
          "",
          "[Added Lines]",
          "36:     case Alias(WindowExpression(_: Rank | _: DenseRank | _: RowNumber, WindowSpecDefinition(Nil, _,",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/LimitPushdownThroughWindowSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/LimitPushdownThroughWindowSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/LimitPushdownThroughWindowSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/LimitPushdownThroughWindowSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: import org.apache.spark.sql.Row",
          "21: import org.apache.spark.sql.catalyst.dsl.expressions._",
          "22: import org.apache.spark.sql.catalyst.dsl.plans._",
          "24: import org.apache.spark.sql.catalyst.plans._",
          "25: import org.apache.spark.sql.catalyst.plans.logical._",
          "26: import org.apache.spark.sql.catalyst.rules._",
          "",
          "[Removed Lines]",
          "23: import org.apache.spark.sql.catalyst.expressions.{CurrentRow, PercentRank, Rank, RowFrame, RowNumber, SpecifiedWindowFrame, UnboundedPreceding}",
          "",
          "[Added Lines]",
          "23: import org.apache.spark.sql.catalyst.expressions.{CurrentRow, NTile, PercentRank, Rank, RowFrame, RowNumber, SpecifiedWindowFrame, UnboundedPreceding}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "198:       Optimize.execute(originalQuery.analyze),",
          "199:       WithoutOptimize.execute(originalQuery.analyze))",
          "200:   }",
          "201: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "202:   test(\"SPARK-40002: Should not push through ntile window function\") {",
          "203:     val originalQuery = testRelation",
          "204:       .select(a, b, c,",
          "205:         windowExpr(new NTile(), windowSpec(Nil, c.desc :: Nil, windowFrame)).as(\"nt\"))",
          "206:       .limit(2)",
          "208:     comparePlans(",
          "209:       Optimize.execute(originalQuery.analyze),",
          "210:       WithoutOptimize.execute(originalQuery.analyze))",
          "211:   }",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/DataFrameWindowFunctionsSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/DataFrameWindowFunctionsSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/DataFrameWindowFunctionsSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/DataFrameWindowFunctionsSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "1203:       )",
          "1204:     )",
          "1205:   }",
          "1206: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1207:   test(\"SPARK-40002: ntile should apply before limit\") {",
          "1208:     val df = Seq.tabulate(101)(identity).toDF(\"id\")",
          "1209:     val w = Window.orderBy(\"id\")",
          "1210:     checkAnswer(",
          "1211:       df.select($\"id\", ntile(10).over(w)).limit(3),",
          "1212:       Seq(",
          "1213:         Row(0, 1),",
          "1214:         Row(1, 1),",
          "1215:         Row(2, 1)",
          "1216:       )",
          "1217:     )",
          "1218:   }",
          "",
          "---------------"
        ]
      }
    }
  ]
}