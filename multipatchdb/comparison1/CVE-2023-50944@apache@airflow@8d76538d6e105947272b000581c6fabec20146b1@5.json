{
  "cve_id": "CVE-2023-50944",
  "cve_desc": "Apache Airflow, versions before 2.8.1, have a vulnerability that allows an authenticated user to access the source code of a DAG to which they don't have access.\u00a0This vulnerability is considered low since it requires an authenticated user to exploit it. Users are recommended to upgrade to version 2.8.1, which fixes this issue.",
  "repo": "apache/airflow",
  "patch_hash": "8d76538d6e105947272b000581c6fabec20146b1",
  "patch_info": {
    "commit_hash": "8d76538d6e105947272b000581c6fabec20146b1",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/8d76538d6e105947272b000581c6fabec20146b1",
    "files": [
      "airflow/api_connexion/endpoints/dag_source_endpoint.py",
      "airflow/models/dagcode.py",
      "tests/api_connexion/endpoints/test_dag_source_endpoint.py"
    ],
    "message": "Check DAG read permission before accessing DAG code (#36257)\n\n(cherry picked from commit 30ea37e0d247ce54c2d25b115e807fdb0074d795)",
    "before_after_code_files": [
      "airflow/api_connexion/endpoints/dag_source_endpoint.py||airflow/api_connexion/endpoints/dag_source_endpoint.py",
      "airflow/models/dagcode.py||airflow/models/dagcode.py",
      "tests/api_connexion/endpoints/test_dag_source_endpoint.py||tests/api_connexion/endpoints/test_dag_source_endpoint.py"
    ]
  },
  "patch_diff": {
    "airflow/api_connexion/endpoints/dag_source_endpoint.py||airflow/api_connexion/endpoints/dag_source_endpoint.py": [
      "File: airflow/api_connexion/endpoints/dag_source_endpoint.py -> airflow/api_connexion/endpoints/dag_source_endpoint.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "17: from __future__ import annotations",
      "19: from http import HTTPStatus",
      "21: from flask import Response, current_app, request",
      "22: from itsdangerous import BadSignature, URLSafeSerializer",
      "24: from airflow.api_connexion import security",
      "26: from airflow.api_connexion.schemas.dag_source_schema import dag_source_schema",
      "27: from airflow.auth.managers.models.resource_details import DagAccessEntity",
      "28: from airflow.models.dagcode import DagCode",
      "31: @security.requires_access_dag(\"GET\", DagAccessEntity.CODE)",
      "33:     \"\"\"Get source code using file token.\"\"\"",
      "34:     secret_key = current_app.config[\"SECRET_KEY\"]",
      "35:     auth_s = URLSafeSerializer(secret_key)",
      "36:     try:",
      "37:         path = auth_s.loads(file_token)",
      "39:     except (BadSignature, FileNotFoundError):",
      "40:         raise NotFound(\"Dag source not found\")",
      "",
      "[Removed Lines]",
      "25: from airflow.api_connexion.exceptions import NotFound",
      "32: def get_dag_source(*, file_token: str) -> Response:",
      "38:         dag_source = DagCode.code(path)",
      "",
      "[Added Lines]",
      "20: from typing import TYPE_CHECKING",
      "26: from airflow.api_connexion.exceptions import NotFound, PermissionDenied",
      "28: from airflow.api_connexion.security import get_readable_dags",
      "30: from airflow.models.dag import DagModel",
      "32: from airflow.utils.session import NEW_SESSION, provide_session",
      "34: if TYPE_CHECKING:",
      "35:     from sqlalchemy.orm import Session",
      "39: @provide_session",
      "40: def get_dag_source(*, file_token: str, session: Session = NEW_SESSION) -> Response:",
      "46:         dag_ids = session.query(DagModel.dag_id).filter(DagModel.fileloc == path).all()",
      "47:         readable_dags = get_readable_dags()",
      "48:         # Check if user has read access to all the DAGs defined in the file",
      "49:         if any(dag_id[0] not in readable_dags for dag_id in dag_ids):",
      "50:             raise PermissionDenied()",
      "51:         dag_source = DagCode.code(path, session=session)",
      "",
      "---------------"
    ],
    "airflow/models/dagcode.py||airflow/models/dagcode.py": [
      "File: airflow/models/dagcode.py -> airflow/models/dagcode.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "177:         return cls.code(fileloc)",
      "179:     @classmethod",
      "181:         \"\"\"Return source code for this DagCode object.",
      "183:         :return: source code as string",
      "184:         \"\"\"",
      "187:     @staticmethod",
      "188:     def _get_code_from_file(fileloc):",
      "",
      "[Removed Lines]",
      "180:     def code(cls, fileloc) -> str:",
      "185:         return cls._get_code_from_db(fileloc)",
      "",
      "[Added Lines]",
      "180:     @provide_session",
      "181:     def code(cls, fileloc, session: Session = NEW_SESSION) -> str:",
      "186:         return cls._get_code_from_db(fileloc, session)",
      "",
      "---------------"
    ],
    "tests/api_connexion/endpoints/test_dag_source_endpoint.py||tests/api_connexion/endpoints/test_dag_source_endpoint.py": [
      "File: tests/api_connexion/endpoints/test_dag_source_endpoint.py -> tests/api_connexion/endpoints/test_dag_source_endpoint.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "35: ROOT_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), os.pardir, os.pardir))",
      "36: EXAMPLE_DAG_FILE = os.path.join(\"airflow\", \"example_dags\", \"example_bash_operator.py\")",
      "39: @pytest.fixture(scope=\"module\")",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "37: EXAMPLE_DAG_ID = \"example_bash_operator\"",
      "38: TEST_DAG_ID = \"latest_only\"",
      "39: NOT_READABLE_DAG_ID = \"latest_only_with_trigger\"",
      "40: TEST_MULTIPLE_DAGS_ID = \"dataset_produces_1\"",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "45:         role_name=\"Test\",",
      "46:         permissions=[(permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG_CODE)],  # type: ignore",
      "47:     )",
      "48:     create_user(app, username=\"test_no_permissions\", role_name=\"TestNoPermissions\")  # type: ignore",
      "50:     yield app",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "52:     app.appbuilder.sm.sync_perm_for_dag(  # type: ignore",
      "53:         TEST_DAG_ID,",
      "54:         access_control={\"Test\": [permissions.ACTION_CAN_READ]},",
      "55:     )",
      "56:     app.appbuilder.sm.sync_perm_for_dag(  # type: ignore",
      "57:         EXAMPLE_DAG_ID,",
      "58:         access_control={\"Test\": [permissions.ACTION_CAN_READ]},",
      "59:     )",
      "60:     app.appbuilder.sm.sync_perm_for_dag(  # type: ignore",
      "61:         TEST_MULTIPLE_DAGS_ID,",
      "62:         access_control={\"Test\": [permissions.ACTION_CAN_READ]},",
      "63:     )",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "80:     def test_should_respond_200_text(self, url_safe_serializer):",
      "81:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
      "82:         dagbag.sync_to_db()",
      "87:         response = self.client.get(",
      "88:             url, headers={\"Accept\": \"text/plain\"}, environ_overrides={\"REMOTE_USER\": \"test\"}",
      "89:         )",
      "",
      "[Removed Lines]",
      "83:         first_dag: DAG = next(iter(dagbag.dags.values()))",
      "84:         dag_docstring = self._get_dag_file_docstring(first_dag.fileloc)",
      "86:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(first_dag.fileloc)}\"",
      "",
      "[Added Lines]",
      "99:         test_dag: DAG = dagbag.dags[TEST_DAG_ID]",
      "100:         dag_docstring = self._get_dag_file_docstring(test_dag.fileloc)",
      "102:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(test_dag.fileloc)}\"",
      "",
      "---------------",
      "--- Hunk 4 ---",
      "[Context before]",
      "95:     def test_should_respond_200_json(self, url_safe_serializer):",
      "96:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
      "97:         dagbag.sync_to_db()",
      "102:         response = self.client.get(",
      "103:             url, headers={\"Accept\": \"application/json\"}, environ_overrides={\"REMOTE_USER\": \"test\"}",
      "104:         )",
      "",
      "[Removed Lines]",
      "98:         first_dag: DAG = next(iter(dagbag.dags.values()))",
      "99:         dag_docstring = self._get_dag_file_docstring(first_dag.fileloc)",
      "101:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(first_dag.fileloc)}\"",
      "",
      "[Added Lines]",
      "114:         test_dag: DAG = dagbag.dags[TEST_DAG_ID]",
      "115:         dag_docstring = self._get_dag_file_docstring(test_dag.fileloc)",
      "117:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(test_dag.fileloc)}\"",
      "",
      "---------------",
      "--- Hunk 5 ---",
      "[Context before]",
      "110:     def test_should_respond_406(self, url_safe_serializer):",
      "111:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
      "112:         dagbag.sync_to_db()",
      "116:         response = self.client.get(",
      "117:             url, headers={\"Accept\": \"image/webp\"}, environ_overrides={\"REMOTE_USER\": \"test\"}",
      "118:         )",
      "",
      "[Removed Lines]",
      "113:         first_dag: DAG = next(iter(dagbag.dags.values()))",
      "115:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(first_dag.fileloc)}\"",
      "",
      "[Added Lines]",
      "129:         test_dag: DAG = dagbag.dags[TEST_DAG_ID]",
      "131:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(test_dag.fileloc)}\"",
      "",
      "---------------",
      "--- Hunk 6 ---",
      "[Context before]",
      "151:             environ_overrides={\"REMOTE_USER\": \"test_no_permissions\"},",
      "152:         )",
      "153:         assert response.status_code == 403",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "171:     def test_should_respond_403_not_readable(self, url_safe_serializer):",
      "172:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
      "173:         dagbag.sync_to_db()",
      "174:         dag: DAG = dagbag.dags[NOT_READABLE_DAG_ID]",
      "176:         response = self.client.get(",
      "177:             f\"/api/v1/dagSources/{url_safe_serializer.dumps(dag.fileloc)}\",",
      "178:             headers={\"Accept\": \"text/plain\"},",
      "179:             environ_overrides={\"REMOTE_USER\": \"test\"},",
      "180:         )",
      "181:         read_dag = self.client.get(",
      "182:             f\"/api/v1/dags/{NOT_READABLE_DAG_ID}\",",
      "183:             environ_overrides={\"REMOTE_USER\": \"test\"},",
      "184:         )",
      "185:         assert response.status_code == 403",
      "186:         assert read_dag.status_code == 403",
      "188:     def test_should_respond_403_some_dags_not_readable_in_the_file(self, url_safe_serializer):",
      "189:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
      "190:         dagbag.sync_to_db()",
      "191:         dag: DAG = dagbag.dags[TEST_MULTIPLE_DAGS_ID]",
      "193:         response = self.client.get(",
      "194:             f\"/api/v1/dagSources/{url_safe_serializer.dumps(dag.fileloc)}\",",
      "195:             headers={\"Accept\": \"text/plain\"},",
      "196:             environ_overrides={\"REMOTE_USER\": \"test\"},",
      "197:         )",
      "199:         read_dag = self.client.get(",
      "200:             f\"/api/v1/dags/{TEST_MULTIPLE_DAGS_ID}\",",
      "201:             environ_overrides={\"REMOTE_USER\": \"test\"},",
      "202:         )",
      "203:         assert response.status_code == 403",
      "204:         assert read_dag.status_code == 200",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "154a78dac9a89a54ce9de483c8eaa196f98862aa",
      "candidate_info": {
        "commit_hash": "154a78dac9a89a54ce9de483c8eaa196f98862aa",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/154a78dac9a89a54ce9de483c8eaa196f98862aa",
        "files": [
          "airflow/timetables/events.py",
          "tests/timetables/test_events_timetable.py"
        ],
        "message": "Do not let EventsTimetable schedule past events if catchup=False (#36134)\n\n* Fix the EventsTimetable schedules past events bug\n---------\n\nCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>\n(cherry picked from commit c01daf811925816e9ae09b78c37b9ff8d87ce691)",
        "before_after_code_files": [
          "airflow/timetables/events.py||airflow/timetables/events.py",
          "tests/timetables/test_events_timetable.py||tests/timetables/test_events_timetable.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/timetables/events.py||airflow/timetables/events.py": [
          "File: airflow/timetables/events.py -> airflow/timetables/events.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "22: import pendulum",
          "24: from airflow.timetables.base import DagRunInfo, DataInterval, Timetable",
          "26: if TYPE_CHECKING:",
          "27:     from pendulum import DateTime",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "25: from airflow.utils import timezone",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "58:             self.event_dates.sort()",
          "59:         self.restrict_to_events = restrict_to_events",
          "60:         if description is None:",
          "65:         else:",
          "66:             self._summary = description",
          "67:             self.description = description",
          "",
          "[Removed Lines]",
          "61:             self.description = (",
          "62:                 f\"{len(self.event_dates)} Events between {self.event_dates[0]} and {self.event_dates[-1]}\"",
          "63:             )",
          "64:             self._summary = f\"{len(self.event_dates)} Events\"",
          "",
          "[Added Lines]",
          "62:             if self.event_dates:",
          "63:                 self.description = (",
          "64:                     f\"{len(self.event_dates)} events between {self.event_dates[0]} and {self.event_dates[-1]}\"",
          "65:                 )",
          "66:             else:",
          "67:                 self.description = \"No events\"",
          "68:             self._summary = f\"{len(self.event_dates)} events\"",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "79:         last_automated_data_interval: DataInterval | None,",
          "80:         restriction: TimeRestriction,",
          "81:     ) -> DagRunInfo | None:",
          "84:         else:",
          "93:         return DagRunInfo.exact(next_event)",
          "95:     def infer_manual_data_interval(self, *, run_after: DateTime) -> DataInterval:",
          "96:         # If Timetable not restricted to events, run for the time specified",
          "98:             return DataInterval.exact(run_after)",
          "100:         # If restricted to events, run for the most recent past event",
          "",
          "[Removed Lines]",
          "82:         if last_automated_data_interval is None:",
          "83:             next_event = self.event_dates[0]",
          "85:             future_dates = itertools.dropwhile(",
          "86:                 lambda when: when <= last_automated_data_interval.end,  # type: ignore",
          "87:                 self.event_dates,",
          "88:             )",
          "89:             next_event = next(future_dates, None)  # type: ignore",
          "90:             if next_event is None:",
          "91:                 return None",
          "97:         if not self.restrict_to_events:",
          "",
          "[Added Lines]",
          "86:         earliest = restriction.earliest",
          "87:         if not restriction.catchup:",
          "88:             current_time = timezone.utcnow()",
          "89:             if earliest is None or current_time > earliest:",
          "90:                 earliest = pendulum.instance(current_time)",
          "92:         for next_event in self.event_dates:",
          "93:             if earliest and next_event < earliest:",
          "94:                 continue",
          "95:             if last_automated_data_interval and next_event <= last_automated_data_interval.end:",
          "96:                 continue",
          "97:             break",
          "99:             # We need to return None if self.event_dates is empty or,",
          "100:             # if not empty, when no suitable event can be found.",
          "101:             return None",
          "103:         if restriction.latest is not None and next_event > restriction.latest:",
          "104:             return None",
          "110:         if not self.restrict_to_events or not self.event_dates:",
          "",
          "---------------"
        ],
        "tests/timetables/test_events_timetable.py||tests/timetables/test_events_timetable.py": [
          "File: tests/timetables/test_events_timetable.py -> tests/timetables/test_events_timetable.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: import pendulum",
          "21: import pytest",
          "23: from airflow.timetables.base import DagRunInfo, DataInterval, TimeRestriction, Timetable",
          "24: from airflow.timetables.events import EventsTimetable",
          "25: from airflow.utils.timezone import utc",
          "29: EVENT_DATES = [",
          "30:     pendulum.DateTime(2021, 9, 6, tzinfo=utc),",
          "",
          "[Removed Lines]",
          "27: START_DATE = pendulum.DateTime(2021, 9, 4, tzinfo=utc)  # Precedes all events",
          "",
          "[Added Lines]",
          "22: import time_machine",
          "28: BEFORE_DATE = pendulum.DateTime(2021, 9, 4, tzinfo=utc)  # Precedes all events",
          "29: START_DATE = pendulum.DateTime(2021, 9, 7, tzinfo=utc)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "93:     Test that when using strict event dates, manual runs before the first event have the first event's date",
          "94:     as the start interval",
          "95:     \"\"\"",
          "97:     expected_data_interval = DataInterval.exact(EVENT_DATES[0])",
          "98:     assert expected_data_interval == manual_run_data_interval",
          "",
          "[Removed Lines]",
          "96:     manual_run_data_interval = restricted_timetable.infer_manual_data_interval(run_after=START_DATE)",
          "",
          "[Added Lines]",
          "98:     manual_run_data_interval = restricted_timetable.infer_manual_data_interval(run_after=BEFORE_DATE)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "101: @pytest.mark.parametrize(",
          "102:     \"last_automated_data_interval, expected_next_info\",",
          "103:     [",
          "104:         pytest.param(DataInterval(day1, day1), DagRunInfo.interval(day2, day2))",
          "106:     ]",
          "107:     + [pytest.param(DataInterval(EVENT_DATES_SORTED[-1], EVENT_DATES_SORTED[-1]), None)],",
          "108: )",
          "",
          "[Removed Lines]",
          "105:         for day1, day2 in zip(EVENT_DATES_SORTED, EVENT_DATES_SORTED[1:])",
          "",
          "[Added Lines]",
          "106:         pytest.param(None, DagRunInfo.interval(START_DATE, START_DATE)),",
          "107:         pytest.param(",
          "108:             DataInterval(EVENT_DATES_SORTED[0], EVENT_DATES_SORTED[0]),",
          "109:             DagRunInfo.interval(START_DATE, START_DATE),",
          "110:         ),",
          "111:     ]",
          "112:     + [",
          "114:         for day1, day2 in zip(EVENT_DATES_SORTED[1:], EVENT_DATES_SORTED[2:])",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "118:         restriction=restriction,",
          "119:     )",
          "120:     assert next_info == expected_next_info",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "132: @pytest.mark.parametrize(",
          "133:     \"current_date\",",
          "134:     [",
          "135:         pytest.param(pendulum.DateTime(2021, 9, 1, tzinfo=utc), id=\"when-current-date-is-before-first-event\"),",
          "136:         pytest.param(pendulum.DateTime(2021, 9, 8, tzinfo=utc), id=\"when-current-date-is-in-the-middle\"),",
          "137:         pytest.param(pendulum.DateTime(2021, 12, 9, tzinfo=utc), id=\"when-current-date-is-after-last-event\"),",
          "138:     ],",
          "139: )",
          "140: @pytest.mark.parametrize(",
          "141:     \"last_automated_data_interval\",",
          "142:     [",
          "143:         pytest.param(None, id=\"first-run\"),",
          "144:         pytest.param(DataInterval(start=BEFORE_DATE, end=BEFORE_DATE), id=\"subsequent-run\"),",
          "145:     ],",
          "146: )",
          "147: def test_no_catchup_first_starts(",
          "148:     last_automated_data_interval: DataInterval | None,",
          "149:     current_date,",
          "150:     unrestricted_timetable: Timetable,",
          "151: ) -> None:",
          "152:     # we don't use the last_automated_data_interval here because it's always less than the first event",
          "153:     expected_date = max(current_date, START_DATE, EVENT_DATES_SORTED[0])",
          "154:     expected_info = None",
          "155:     if expected_date <= EVENT_DATES_SORTED[-1]:",
          "156:         expected_info = DagRunInfo.interval(start=expected_date, end=expected_date)",
          "158:     with time_machine.travel(current_date):",
          "159:         next_info = unrestricted_timetable.next_dagrun_info(",
          "160:             last_automated_data_interval=last_automated_data_interval,",
          "161:             restriction=TimeRestriction(earliest=START_DATE, latest=None, catchup=False),",
          "162:         )",
          "163:     assert next_info == expected_info",
          "166: def test_empty_timetable() -> None:",
          "167:     empty_timetable = EventsTimetable(event_dates=[])",
          "168:     next_info = empty_timetable.next_dagrun_info(",
          "169:         last_automated_data_interval=None,",
          "170:         restriction=TimeRestriction(earliest=START_DATE, latest=None, catchup=False),",
          "171:     )",
          "172:     assert next_info is None",
          "175: def test_empty_timetable_manual_run() -> None:",
          "176:     empty_timetable = EventsTimetable(event_dates=[])",
          "177:     manual_run_data_interval = empty_timetable.infer_manual_data_interval(run_after=START_DATE)",
          "178:     assert manual_run_data_interval == DataInterval(start=START_DATE, end=START_DATE)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "6d47a857436257ef9a478ddc83b578ccba05c9d6",
      "candidate_info": {
        "commit_hash": "6d47a857436257ef9a478ddc83b578ccba05c9d6",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/6d47a857436257ef9a478ddc83b578ccba05c9d6",
        "files": [
          "CONTRIBUTING.rst",
          "Dockerfile",
          "INSTALL",
          "airflow/utils/dot_renderer.py",
          "dev/breeze/src/airflow_breeze/global_constants.py",
          "docs/apache-airflow/extra-packages-ref.rst",
          "docs/docker-stack/build-arg-ref.rst",
          "docs/spelling_wordlist.txt",
          "images/breeze/output_prod-image_build.txt",
          "newsfragments/36647.significant.rst",
          "setup.cfg",
          "setup.py"
        ],
        "message": "Make `graphviz` dependency optional (#36647)\n\nThe `graphviz` dependency has been problematic as Airflow required\ndependency - especially for ARM-based installations. Graphviz\npackages require binary graphviz libraries - which is already a\nlimitation, but they also require to install graphviz Python\nbindings to be build and installed. This does not work for older\nLinux installation but - more importantly - when you try\nto install Graphviz libraries for Python 3.8, 3.9 for ARM M1\nMacBooks, the packages fail to install because Python bindings\ncompilation for M1 can only work for Python 3.10+.\n\nThere is not an easy solution for that except commenting out\ngraphviz dependency from setup.py, when you want to install Airflow\nfor Python 3.8, 3.9 for MacBook M1.\n\nHowever Graphviz is really used in two places:\n\n* when you want to render DAGs wia airflow CLI - either to an image\n  or directly to terminal (for terminals/systems supporting imgcat)\n\n* when you want to render ER diagram after you modified Airflow\n  models\n\nThe latter is a development-only feature, the former is production\nfeature, however it is a very niche one.\n\nThis PR turns rendering of the images in Airflow in optional feature\n(only working when graphviz python bindings are installed) and\neffectively turns graphviz into an optional extra (and removes it\nfrom requirements).\n\nThis is not a breaking change technically - the CLIs to render the\nDAGs is still there and IF you already have graphviz installed, it\nwill continue working as it did before. The only problem when it\ndoes not work is where you do not have graphviz installed for\nfresh installation and it will raise an error and inform that you need it.\n\nGraphviz will remain to be installed for most users:\n\n* the Airflow Image will still contain graphviz library, because\n  it is added there as extra\n* when previous version of Airflow has been installed already, then\n  graphviz library is already installed there and Airflow will\n  continue working as it did\n\nThe only change will be a new installation of new version of Airflow\nfrom the scratch, where graphviz will need to be specified as extra\nor installed separately in order to enable DAG rendering option.\n\nTaking into account this behaviour (which only requires to install\na graphviz package), this should not be considered as a breaking\nchange.\n\nExtracted from: #36537\n\n(cherry picked from commit 89f1737afb27f6e708c2e83e3d8e751d9a36f91e)",
        "before_after_code_files": [
          "airflow/utils/dot_renderer.py||airflow/utils/dot_renderer.py",
          "dev/breeze/src/airflow_breeze/global_constants.py||dev/breeze/src/airflow_breeze/global_constants.py",
          "setup.cfg||setup.cfg",
          "setup.py||setup.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/utils/dot_renderer.py||airflow/utils/dot_renderer.py": [
          "File: airflow/utils/dot_renderer.py -> airflow/utils/dot_renderer.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "19: \"\"\"Renderer DAG (tasks and dependencies) to the graphviz object.\"\"\"",
          "20: from __future__ import annotations",
          "22: from typing import TYPE_CHECKING, Any",
          "26: from airflow.exceptions import AirflowException",
          "27: from airflow.models.baseoperator import BaseOperator",
          "",
          "[Removed Lines]",
          "24: import graphviz",
          "",
          "[Added Lines]",
          "22: import warnings",
          "25: try:",
          "26:     import graphviz",
          "27: except ImportError:",
          "28:     warnings.warn(\"Could not import graphviz. Rendering graph to the graphical format will not be possible.\")",
          "29:     graphviz = None",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "151:     :param deps: List of DAG dependencies",
          "152:     :return: Graphviz object",
          "153:     \"\"\"",
          "154:     dot = graphviz.Digraph(graph_attr={\"rankdir\": \"LR\"})",
          "156:     for dag, dependencies in deps.items():",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "159:     if not graphviz:",
          "160:         raise AirflowException(",
          "161:             \"Could not import graphviz. Install the graphviz python package to fix this error.\"",
          "162:         )",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "179:     :param tis: List of task instances",
          "180:     :return: Graphviz object",
          "181:     \"\"\"",
          "182:     dot = graphviz.Digraph(",
          "183:         dag.dag_id,",
          "184:         graph_attr={",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "191:     if not graphviz:",
          "192:         raise AirflowException(",
          "193:             \"Could not import graphviz. Install the graphviz python package to fix this error.\"",
          "194:         )",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/global_constants.py||dev/breeze/src/airflow_breeze/global_constants.py": [
          "File: dev/breeze/src/airflow_breeze/global_constants.py -> dev/breeze/src/airflow_breeze/global_constants.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "437:     \"ftp\",",
          "438:     \"google\",",
          "439:     \"google_auth\",",
          "440:     \"grpc\",",
          "441:     \"hashicorp\",",
          "442:     \"http\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "440:     \"graphviz\",",
          "",
          "---------------"
        ],
        "setup.cfg||setup.cfg": [
          "File: setup.cfg -> setup.cfg",
          "--- Hunk 1 ---",
          "[Context before]",
          "108:     flask-wtf>=0.15",
          "109:     fsspec>=2023.10.0",
          "110:     google-re2>=1.0",
          "112:     gunicorn>=20.1.0",
          "113:     httpx",
          "114:     importlib_metadata>=1.7;python_version<\"3.9\"",
          "",
          "[Removed Lines]",
          "111:     graphviz>=0.12",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "setup.py||setup.py": [
          "File: setup.py -> setup.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "318: ]",
          "319: doc_gen = [",
          "320:     \"eralchemy2\",",
          "321: ]",
          "322: flask_appbuilder_oauth = [",
          "323:     \"authlib>=1.0.0\",",
          "324:     # The version here should be upgraded at the same time as flask-appbuilder in setup.cfg",
          "325:     \"flask-appbuilder[oauth]==4.3.10\",",
          "326: ]",
          "327: kerberos = [",
          "328:     \"pykerberos>=1.1.13\",",
          "329:     \"requests_kerberos>=0.10.0\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "321:     \"graphviz>=0.12\",",
          "328: graphviz = [\"graphviz>=0.12\"]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "589:     \"deprecated_api\": deprecated_api,",
          "590:     \"github_enterprise\": flask_appbuilder_oauth,",
          "591:     \"google_auth\": flask_appbuilder_oauth,",
          "592:     \"kerberos\": kerberos,",
          "593:     \"ldap\": ldap,",
          "594:     \"leveldb\": leveldb,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "594:     \"graphviz\": graphviz,",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "5da2e0a4fd368e7cd520627e049d67b405c3b36f",
      "candidate_info": {
        "commit_hash": "5da2e0a4fd368e7cd520627e049d67b405c3b36f",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/5da2e0a4fd368e7cd520627e049d67b405c3b36f",
        "files": [
          "Dockerfile",
          "docker_tests/test_prod_image.py",
          "docs/docker-stack/changelog.rst"
        ],
        "message": "Use `mariadb` by default when build final prod image (#36716)\n\n(cherry picked from commit 11ec4100b3ffb86e15b43a8dde5f53f07d404508)",
        "before_after_code_files": [
          "docker_tests/test_prod_image.py||docker_tests/test_prod_image.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "docker_tests/test_prod_image.py||docker_tests/test_prod_image.py": [
          "File: docker_tests/test_prod_image.py -> docker_tests/test_prod_image.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "158:         \"grpc\": [\"grpc\", \"google.auth\", \"google_auth_httplib2\"],",
          "159:         \"hashicorp\": [\"hvac\"],",
          "160:         \"ldap\": [\"ldap\"],",
          "161:         \"postgres\": [\"psycopg2\"],",
          "162:         \"pyodbc\": [\"pyodbc\"],",
          "163:         \"redis\": [\"redis\"],",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "161:         \"mysql\": [\"MySQLdb\", *([\"mysql\"] if bool(find_spec(\"mysql\")) else [])],",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "167:         \"statsd\": [\"statsd\"],",
          "168:         \"virtualenv\": [\"virtualenv\"],",
          "169:     }",
          "173:     @pytest.mark.skipif(os.environ.get(\"TEST_SLIM_IMAGE\") == \"true\", reason=\"Skipped with slim image\")",
          "174:     @pytest.mark.parametrize(\"package_name,import_names\", PACKAGE_IMPORTS.items())",
          "",
          "[Removed Lines]",
          "170:     if bool(find_spec(\"mysql\")):",
          "171:         PACKAGE_IMPORTS[\"mysql\"] = [\"mysql\"]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "975ddce2ef214794befffadb430fdb1199aeabe0",
      "candidate_info": {
        "commit_hash": "975ddce2ef214794befffadb430fdb1199aeabe0",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/975ddce2ef214794befffadb430fdb1199aeabe0",
        "files": [
          "airflow/providers/apache/beam/provider.yaml",
          "generated/provider_dependencies.json",
          "setup.py"
        ],
        "message": "Get rid of pyarrow-hotfix for CVE-2023-47248 (#36697)\n\nThe #35650 introduced a hotfix for Pyarrow CVE-2023-47248. So far\nwe have been blocked from removing it by Apache Beam that limited\nAirflow from bumping pyarrow to a version that was not vulnerable.\n\nThis is now possible since Apache Beam relesed 2.53.0 version on\n4th of January 2023 that allows to use non-vulnerable pyarrow.\n\nWe are now bumping both Pyarrow and Beam minimum versions to\nreflect that and remove pyarrow hotfix.\n\n(cherry picked from commit d105c7115f56f88d48a2888484a0ed7d1c01576f)",
        "before_after_code_files": [
          "setup.py||setup.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "setup.py||setup.py": [
          "File: setup.py -> setup.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "349: otel = [\"opentelemetry-exporter-prometheus\"]",
          "350: pandas = [",
          "351:     \"pandas>=0.17.1\",",
          "356: ]",
          "357: password = [",
          "358:     \"bcrypt>=2.0.0\",",
          "",
          "[Removed Lines]",
          "352:     # Use pyarrow-hotfix to fix https://nvd.nist.gov/vuln/detail/CVE-2023-47248.",
          "353:     # We should remove it once Apache Beam frees us to upgrade to pyarrow 14.0.1",
          "354:     \"pyarrow-hotfix\",",
          "355:     \"pyarrow>=9.0.0\",",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "c836c4c6f7be5880700aa1df3a3439052c76dbb3",
      "candidate_info": {
        "commit_hash": "c836c4c6f7be5880700aa1df3a3439052c76dbb3",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/c836c4c6f7be5880700aa1df3a3439052c76dbb3",
        "files": [
          ".pre-commit-config.yaml",
          "STATIC_CODE_CHECKS.rst",
          "TESTING.rst",
          "dev/breeze/src/airflow_breeze/pre_commit_ids.py",
          "images/breeze/output_static-checks.svg",
          "images/breeze/output_static-checks.txt",
          "scripts/ci/docker-compose/integration-cassandra.yml",
          "scripts/ci/docker-compose/integration-celery.yml",
          "scripts/ci/docker-compose/integration-kafka.yml",
          "scripts/ci/docker-compose/integration-kerberos.yml",
          "scripts/ci/docker-compose/integration-mongo.yml",
          "scripts/ci/docker-compose/integration-openlineage.yml",
          "scripts/ci/docker-compose/integration-otel.yml",
          "scripts/ci/docker-compose/integration-pinot.yml",
          "scripts/ci/docker-compose/integration-statsd.yml",
          "scripts/ci/docker-compose/integration-trino.yml",
          "scripts/ci/pre_commit/pre_commit_check_integrations_list.py"
        ],
        "message": "Add pre-commit check to check integrations table. (#36497)\n\n(cherry picked from commit 9d1eba087b488d473f8e3a3b12df63d83c7364e8)",
        "before_after_code_files": [
          "dev/breeze/src/airflow_breeze/pre_commit_ids.py||dev/breeze/src/airflow_breeze/pre_commit_ids.py",
          "scripts/ci/pre_commit/pre_commit_check_integrations_list.py||scripts/ci/pre_commit/pre_commit_check_integrations_list.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/breeze/src/airflow_breeze/pre_commit_ids.py||dev/breeze/src/airflow_breeze/pre_commit_ids.py": [
          "File: dev/breeze/src/airflow_breeze/pre_commit_ids.py -> dev/breeze/src/airflow_breeze/pre_commit_ids.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "54:     \"check-hooks-apply\",",
          "55:     \"check-incorrect-use-of-LoggingMixin\",",
          "56:     \"check-init-decorator-arguments\",",
          "57:     \"check-lazy-logging\",",
          "58:     \"check-links-to-example-dags-do-not-use-hardcoded-versions\",",
          "59:     \"check-merge-conflict\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "57:     \"check-integrations-list-consistent\",",
          "",
          "---------------"
        ],
        "scripts/ci/pre_commit/pre_commit_check_integrations_list.py||scripts/ci/pre_commit/pre_commit_check_integrations_list.py": [
          "File: scripts/ci/pre_commit/pre_commit_check_integrations_list.py -> scripts/ci/pre_commit/pre_commit_check_integrations_list.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: #!/usr/bin/env python",
          "2: #",
          "3: # Licensed to the Apache Software Foundation (ASF) under one",
          "4: # or more contributor license agreements.  See the NOTICE file",
          "5: # distributed with this work for additional information",
          "6: # regarding copyright ownership.  The ASF licenses this file",
          "7: # to you under the Apache License, Version 2.0 (the",
          "8: # \"License\"); you may not use this file except in compliance",
          "9: # with the License.  You may obtain a copy of the License at",
          "10: #",
          "11: #   http://www.apache.org/licenses/LICENSE-2.0",
          "12: #",
          "13: # Unless required by applicable law or agreed to in writing,",
          "14: # software distributed under the License is distributed on an",
          "15: # \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
          "16: # KIND, either express or implied.  See the License for the",
          "17: # specific language governing permissions and limitations",
          "18: # under the License.",
          "19: \"\"\"",
          "20: Module to check integration tests are listed in documentation.",
          "22: Compare the contents of the integrations table and the docker-compose",
          "23: integration files, if there is a mismatch, the table is generated.",
          "24: \"\"\"",
          "25: from __future__ import annotations",
          "27: import re",
          "28: import sys",
          "29: from pathlib import Path",
          "30: from typing import Any",
          "32: import yaml",
          "34: # make sure common_precommit_utils is imported",
          "35: sys.path.insert(0, str(Path(__file__).parent.resolve()))",
          "36: from common_precommit_utils import (",
          "37:     AIRFLOW_SOURCES_ROOT_PATH,",
          "38:     console,",
          "39:     insert_documentation,",
          "40: )",
          "41: from tabulate import tabulate",
          "43: DOCUMENTATION_PATH = AIRFLOW_SOURCES_ROOT_PATH / \"TESTING.rst\"",
          "44: INTEGRATION_TESTS_PATH = AIRFLOW_SOURCES_ROOT_PATH / \"scripts\" / \"ci\" / \"docker-compose\"",
          "45: INTEGRATION_TEST_PREFIX = \"integration-*.yml\"",
          "46: DOCS_MARKER_START = \".. BEGIN AUTO-GENERATED INTEGRATION LIST\"",
          "47: DOCS_MARKER_END = \".. END AUTO-GENERATED INTEGRATION LIST\"",
          "48: _LIST_MATCH = r\"\\|[^|\\n]+\"",
          "51: def get_ci_integrations(",
          "52:     tests_path: Path = INTEGRATION_TESTS_PATH,",
          "53:     integration_prefix: str = INTEGRATION_TEST_PREFIX,",
          "54: ) -> dict[str, Path]:",
          "55:     \"\"\"Get list of integrations from matching filenames.\"\"\"",
          "56:     if not tests_path.is_dir() and tests_path.exists():",
          "57:         console.print(f\"[red]Bad tests path: {tests_path}. [/]\")",
          "58:         sys.exit(1)",
          "60:     integrations_files = [_i for _i in tests_path.glob(integration_prefix)]",
          "62:     if len(integrations_files) == 0:",
          "63:         console.print(",
          "64:             f\"[red]No integrations found.\"",
          "65:             f\"Pattern '{integration_prefix}' did not match any files under {tests_path}. [/]\"",
          "66:         )",
          "67:         sys.exit(1)",
          "69:     # parse into list of ids",
          "70:     integrations = {}",
          "71:     for _i in integrations_files:",
          "72:         try:",
          "73:             _key = _i.stem.split(\"-\")[1]",
          "74:             integrations[_key] = _i",
          "75:         except IndexError:",
          "76:             console.print(f\"[red]Tried to parse {_i.stem}, but did not contain '-' separator. [/]\")",
          "77:             continue",
          "79:     return integrations",
          "82: def get_docs_integrations(docs_path: Path = DOCUMENTATION_PATH):",
          "83:     \"\"\"Get integrations listed in docs.\"\"\"",
          "84:     table_lines = []",
          "85:     _list_start_line = None",
          "86:     with open(docs_path, encoding=\"utf8\") as f:",
          "87:         for line_n, line in enumerate(f):",
          "88:             if DOCS_MARKER_END in line:",
          "89:                 break",
          "90:             if DOCS_MARKER_START in line:",
          "91:                 _list_start_line = line_n",
          "92:             if _list_start_line is None:",
          "93:                 continue",
          "94:             if line_n > _list_start_line:",
          "95:                 table_lines.append(line)",
          "97:     if len(table_lines) == 0:",
          "98:         console.print(\"[red]No integrations table in docs.[/]\")",
          "99:         sys.exit(1)",
          "101:     table_cells = []",
          "102:     for line in table_lines:",
          "103:         m = re.findall(_LIST_MATCH, line)",
          "104:         if len(m) == 0:",
          "105:             continue",
          "106:         table_cells.append(m[0].strip(\"|\").strip())",
          "108:     def _list_matcher(j):",
          "109:         \"\"\"Filter callable to exclude header and empty cells.\"\"\"",
          "110:         if len(j) == 0:",
          "111:             return False",
          "112:         elif j in [\"Description\", \"Identifier\"]:",
          "113:             return False",
          "114:         else:",
          "115:             return True",
          "117:     table_cells = list(filter(_list_matcher, table_cells))",
          "118:     return table_cells",
          "121: def update_integration_tests_array(contents: dict[str, list[str]]):",
          "122:     \"\"\"Generate docs table.\"\"\"",
          "123:     rows = []",
          "124:     sorted_contents = dict(sorted(contents.items()))",
          "125:     for integration, description in sorted_contents.items():",
          "126:         formatted_hook_description = (",
          "127:             description[0] if len(description) == 1 else \"* \" + \"\\n* \".join(description)",
          "128:         )",
          "129:         rows.append((integration, formatted_hook_description))",
          "130:     formatted_table = \"\\n\" + tabulate(rows, tablefmt=\"grid\", headers=(\"Identifier\", \"Description\")) + \"\\n\\n\"",
          "131:     insert_documentation(",
          "132:         file_path=AIRFLOW_SOURCES_ROOT_PATH / \"TESTING.rst\",",
          "133:         content=formatted_table.splitlines(keepends=True),",
          "134:         header=DOCS_MARKER_START,",
          "135:         footer=DOCS_MARKER_END,",
          "136:     )",
          "139: def print_diff(source, target, msg):",
          "140:     difference = source - target",
          "141:     if difference:",
          "142:         console.print(msg)",
          "143:         for i in difference:",
          "144:             console.print(f\"[red]\\t- {i}[/]\")",
          "145:     return list(difference)",
          "148: def _get_breeze_description(parsed_compose: dict[str, Any], label_key: str = \"breeze.description\"):",
          "149:     \"\"\"Extract all breeze.description labels per image.\"\"\"",
          "150:     image_label_map = {}",
          "151:     # possible key error handled outside",
          "152:     for _img_name, img in parsed_compose[\"services\"].items():",
          "153:         try:",
          "154:             for _label_name, label in img[\"labels\"].items():",
          "155:                 if _label_name == label_key:",
          "156:                     image_label_map[_img_name] = label",
          "157:         except KeyError:",
          "158:             # service has no 'lables' entry",
          "159:             continue",
          "160:     return image_label_map",
          "163: def get_integration_descriptions(integrations: dict[str, Path]) -> dict[str, list[Any]]:",
          "164:     \"\"\"Pull breeze description from docker-compose files.\"\"\"",
          "165:     table = {}",
          "166:     for integration, path in integrations.items():",
          "167:         with open(path) as f:",
          "168:             _compose = yaml.safe_load(f)",
          "170:         try:",
          "171:             _labels = _get_breeze_description(_compose)",
          "172:         except KeyError:",
          "173:             console.print(f\"[red]No 'services' entry in compose file {path}.[/]\")",
          "174:             sys.exit(1)",
          "175:         table[integration] = list(_labels.values())",
          "176:     return table",
          "179: def main():",
          "180:     docs_integrations = get_docs_integrations()",
          "181:     ci_integrations = get_ci_integrations()",
          "183:     if len(ci_integrations) == 0:",
          "184:         console.print(\"[red]No integrations found.[/]\")",
          "185:         sys.exit(1)",
          "187:     _ci_items = set(ci_integrations)",
          "188:     _docs_items = set(docs_integrations)",
          "189:     diff = []",
          "190:     diff.append(print_diff(_ci_items, _docs_items, \"[red]Found in ci files, but not in docs: [/]\"))",
          "191:     diff.append(print_diff(_docs_items, _ci_items, \"[red]Found in docs, but not in ci files: [/]\"))",
          "192:     if diff:",
          "193:         console.print(",
          "194:             \"[yellow]Regenerating documentation table. Don't forget to review and commit possible changes.[/]\"",
          "195:         )",
          "197:     table_contents = get_integration_descriptions(ci_integrations)",
          "198:     update_integration_tests_array(table_contents)",
          "201: if __name__ == \"__main__\":",
          "202:     main()",
          "",
          "---------------"
        ]
      }
    }
  ]
}