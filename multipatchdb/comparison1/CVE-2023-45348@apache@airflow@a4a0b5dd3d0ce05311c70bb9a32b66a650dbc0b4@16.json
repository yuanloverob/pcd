{
  "cve_id": "CVE-2023-45348",
  "cve_desc": "Apache Airflow, versions 2.7.0 and 2.7.1, is affected by a vulnerability that allows an authenticated user to retrieve sensitive configuration information when the \"expose_config\" option is set to \"non-sensitive-only\". The `expose_config` option is False by default.\nIt is recommended to upgrade to a version that is not affected.",
  "repo": "apache/airflow",
  "patch_hash": "a4a0b5dd3d0ce05311c70bb9a32b66a650dbc0b4",
  "patch_info": {
    "commit_hash": "a4a0b5dd3d0ce05311c70bb9a32b66a650dbc0b4",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/a4a0b5dd3d0ce05311c70bb9a32b66a650dbc0b4",
    "files": [
      "airflow/api_connexion/endpoints/config_endpoint.py",
      "airflow/configuration.py",
      "tests/api_connexion/endpoints/test_config_endpoint.py"
    ],
    "message": "Check if the lower of provided values are sensitives in config endpoint (#34712)\n\n* Check if the lower of provided values are sensitives in config endpoint\n\n* update unit test\n\n* ensure that all values in sensitive dict are in lower characters\n\n(cherry picked from commit f044589b685855a8fce8f5376bea2564c5a001f7)",
    "before_after_code_files": [
      "airflow/api_connexion/endpoints/config_endpoint.py||airflow/api_connexion/endpoints/config_endpoint.py",
      "airflow/configuration.py||airflow/configuration.py",
      "tests/api_connexion/endpoints/test_config_endpoint.py||tests/api_connexion/endpoints/test_config_endpoint.py"
    ]
  },
  "patch_diff": {
    "airflow/api_connexion/endpoints/config_endpoint.py||airflow/api_connexion/endpoints/config_endpoint.py": [
      "File: airflow/api_connexion/endpoints/config_endpoint.py -> airflow/api_connexion/endpoints/config_endpoint.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "123:                 \"Config not found.\", detail=f\"The option [{section}/{option}] is not found in config.\"",
      "124:             )",
      "127:             value = \"< hidden >\"",
      "128:         else:",
      "129:             value = conf.get(section, option)",
      "",
      "[Removed Lines]",
      "126:         if (section, option) in conf.sensitive_config_values:",
      "",
      "[Added Lines]",
      "126:         if (section.lower(), option.lower()) in conf.sensitive_config_values:",
      "",
      "---------------"
    ],
    "airflow/configuration.py||airflow/configuration.py": [
      "File: airflow/configuration.py -> airflow/configuration.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "311:             for s, s_c in self.configuration_description.items()",
      "312:             for k, item in s_c.get(\"options\").items()  # type: ignore[union-attr]",
      "313:         }",
      "315:         depr_option = {self.deprecated_options[x][:-1] for x in sensitive if x in self.deprecated_options}",
      "316:         depr_section = {",
      "317:             (self.deprecated_sections[s][0], k) for s, k in sensitive if s in self.deprecated_sections",
      "",
      "[Removed Lines]",
      "314:         sensitive = {(section, key) for (section, key), v in flattened.items() if v.get(\"sensitive\") is True}",
      "",
      "[Added Lines]",
      "314:         sensitive = {",
      "315:             (section.lower(), key.lower())",
      "316:             for (section, key), v in flattened.items()",
      "317:             if v.get(\"sensitive\") is True",
      "318:         }",
      "",
      "---------------"
    ],
    "tests/api_connexion/endpoints/test_config_endpoint.py||tests/api_connexion/endpoints/test_config_endpoint.py": [
      "File: tests/api_connexion/endpoints/test_config_endpoint.py -> tests/api_connexion/endpoints/test_config_endpoint.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "247:         return_value=MOCK_CONF_WITH_SENSITIVE_VALUE,",
      "248:     )",
      "249:     @conf_vars({(\"webserver\", \"expose_config\"): \"non-sensitive-only\"})",
      "251:         response = self.client.get(",
      "253:             headers={\"Accept\": \"text/plain\"},",
      "254:             environ_overrides={\"REMOTE_USER\": \"test\"},",
      "255:         )",
      "256:         assert response.status_code == 200",
      "257:         expected = textwrap.dedent(",
      "261:         \"\"\"",
      "262:         )",
      "263:         assert expected == response.data.decode()",
      "",
      "[Removed Lines]",
      "250:     def test_should_respond_200_text_plain_with_non_sensitive_only(self, mock_as_dict):",
      "252:             \"/api/v1/config/section/core/option/sql_alchemy_conn\",",
      "258:             \"\"\"\\",
      "259:         [core]",
      "260:         sql_alchemy_conn = < hidden >",
      "",
      "[Added Lines]",
      "250:     @pytest.mark.parametrize(",
      "251:         \"section, option\",",
      "252:         [",
      "253:             (\"core\", \"sql_alchemy_conn\"),",
      "254:             (\"core\", \"SQL_ALCHEMY_CONN\"),",
      "255:             (\"corE\", \"sql_alchemy_conn\"),",
      "256:             (\"CORE\", \"sql_alchemy_conn\"),",
      "257:         ],",
      "258:     )",
      "259:     def test_should_respond_200_text_plain_with_non_sensitive_only(self, mock_as_dict, section, option):",
      "261:             f\"/api/v1/config/section/{section}/option/{option}\",",
      "267:             f\"\"\"\\",
      "268:         [{section}]",
      "269:         {option} = < hidden >",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "084839db3c80d959fb37cdd2868fdaaecb1b2631",
      "candidate_info": {
        "commit_hash": "084839db3c80d959fb37cdd2868fdaaecb1b2631",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/084839db3c80d959fb37cdd2868fdaaecb1b2631",
        "files": [
          "airflow/cli/cli_config.py",
          "airflow/cli/commands/connection_command.py",
          "airflow/www/views.py"
        ],
        "message": "E731: replace lambda by a def method in Airflow core (#33758)\n\n(cherry picked from commit a1d4a20548b18721aa7564a1e415ff866db2bebd)",
        "before_after_code_files": [
          "airflow/cli/cli_config.py||airflow/cli/cli_config.py",
          "airflow/cli/commands/connection_command.py||airflow/cli/commands/connection_command.py",
          "airflow/www/views.py||airflow/www/views.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/cli/cli_config.py||airflow/cli/cli_config.py": [
          "File: airflow/cli/cli_config.py -> airflow/cli/cli_config.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "103:         \"\"\"Add this argument to an ArgumentParser.\"\"\"",
          "104:         if \"metavar\" in self.kwargs and \"type\" not in self.kwargs:",
          "105:             if self.kwargs[\"metavar\"] == \"DIRPATH\":",
          "107:                 self.kwargs[\"type\"] = type",
          "108:         parser.add_argument(*self.flags, **self.kwargs)",
          "",
          "[Removed Lines]",
          "106:                 type = lambda x: self._is_valid_directory(parser, x)",
          "",
          "[Added Lines]",
          "107:                 def type(x):",
          "108:                     return self._is_valid_directory(parser, x)",
          "",
          "---------------"
        ],
        "airflow/cli/commands/connection_command.py||airflow/cli/commands/connection_command.py": [
          "File: airflow/cli/commands/connection_command.py -> airflow/cli/commands/connection_command.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "115: def _format_connections(conns: list[Connection], file_format: str, serialization_format: str) -> str:",
          "116:     if serialization_format == \"json\":",
          "118:     elif serialization_format == \"uri\":",
          "119:         serializer_func = Connection.get_uri",
          "120:     else:",
          "",
          "[Removed Lines]",
          "117:         serializer_func = lambda x: json.dumps(_connection_to_dict(x))",
          "",
          "[Added Lines]",
          "118:         def serializer_func(x):",
          "119:             return json.dumps(_connection_to_dict(x))",
          "",
          "---------------"
        ],
        "airflow/www/views.py||airflow/www/views.py": [
          "File: airflow/www/views.py -> airflow/www/views.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "320:     sort_order = conf.get(\"webserver\", \"grid_view_sorting_order\", fallback=\"topological\")",
          "321:     if sort_order == \"topological\":",
          "323:     elif sort_order == \"hierarchical_alphabetical\":",
          "325:     else:",
          "326:         raise AirflowConfigException(f\"Unsupported grid_view_sorting_order: {sort_order}\")",
          "",
          "[Removed Lines]",
          "322:         sort_children_fn = lambda task_group: task_group.topological_sort()",
          "324:         sort_children_fn = lambda task_group: task_group.hierarchical_alphabetical_sort()",
          "",
          "[Added Lines]",
          "323:         def sort_children_fn(task_group):",
          "324:             return task_group.topological_sort()",
          "328:         def sort_children_fn(task_group):",
          "329:             return task_group.hierarchical_alphabetical_sort()",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "6e739d63a85cc202858c06f684c986967262f6e9",
      "candidate_info": {
        "commit_hash": "6e739d63a85cc202858c06f684c986967262f6e9",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/6e739d63a85cc202858c06f684c986967262f6e9",
        "files": [
          "airflow/serialization/serializers/datetime.py",
          "tests/serialization/serializers/test_serializers.py"
        ],
        "message": "Fix non deterministic datetime deserialization (#34492)\n\ntzname() does not return full timezones and returned short hand notations are not deterministic. This changes the serialization to be deterministic and adds some logic to deal with serialized short-hand US Timezones and CEST.\n\n---------\n\nCo-authored-by: bolkedebruin <bolkedebruin@users.noreply.github.com>\n(cherry picked from commit a3c06c02e31cc77b2c19554892b72ed91b8387de)",
        "before_after_code_files": [
          "airflow/serialization/serializers/datetime.py||airflow/serialization/serializers/datetime.py",
          "tests/serialization/serializers/test_serializers.py||tests/serialization/serializers/test_serializers.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/serialization/serializers/datetime.py||airflow/serialization/serializers/datetime.py": [
          "File: airflow/serialization/serializers/datetime.py -> airflow/serialization/serializers/datetime.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: from typing import TYPE_CHECKING",
          "22: from airflow.utils.module_loading import qualname",
          "23: from airflow.utils.timezone import convert_to_utc, is_naive",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "22: from airflow.serialization.serializers.timezone import (",
          "23:     deserialize as deserialize_timezone,",
          "24:     serialize as serialize_timezone,",
          "25: )",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "28:     from airflow.serialization.serde import U",
          "32: serializers = [\"datetime.date\", \"datetime.datetime\", \"datetime.timedelta\", \"pendulum.datetime.DateTime\"]",
          "33: deserializers = serializers",
          "",
          "[Removed Lines]",
          "30: __version__ = 1",
          "",
          "[Added Lines]",
          "34: __version__ = 2",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "44:         if is_naive(o):",
          "45:             o = convert_to_utc(o)",
          "49:         return {TIMESTAMP: o.timestamp(), TIMEZONE: tz}, qn, __version__, True",
          "",
          "[Removed Lines]",
          "47:         tz = o.tzname()",
          "",
          "[Added Lines]",
          "51:         tz = serialize_timezone(o.tzinfo)",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "61:     import datetime",
          "63:     from pendulum import DateTime",
          "66:     if classname == qualname(datetime.datetime) and isinstance(data, dict):",
          "69:     if classname == qualname(DateTime) and isinstance(data, dict):",
          "72:     if classname == qualname(datetime.timedelta) and isinstance(data, (str, float)):",
          "73:         return datetime.timedelta(seconds=float(data))",
          "",
          "[Removed Lines]",
          "64:     from pendulum.tz import timezone",
          "67:         return datetime.datetime.fromtimestamp(float(data[TIMESTAMP]), tz=timezone(data[TIMEZONE]))",
          "70:         return DateTime.fromtimestamp(float(data[TIMESTAMP]), tz=timezone(data[TIMEZONE]))",
          "",
          "[Added Lines]",
          "68:     from pendulum.tz import fixed_timezone, timezone",
          "70:     tz: datetime.tzinfo | None = None",
          "71:     if isinstance(data, dict) and TIMEZONE in data:",
          "72:         if version == 1:",
          "73:             # try to deserialize unsupported timezones",
          "74:             timezone_mapping = {",
          "75:                 \"EDT\": fixed_timezone(-4 * 3600),",
          "76:                 \"CDT\": fixed_timezone(-5 * 3600),",
          "77:                 \"MDT\": fixed_timezone(-6 * 3600),",
          "78:                 \"PDT\": fixed_timezone(-7 * 3600),",
          "79:                 \"CEST\": timezone(\"CET\"),",
          "80:             }",
          "81:             if data[TIMEZONE] in timezone_mapping:",
          "82:                 tz = timezone_mapping[data[TIMEZONE]]",
          "83:             else:",
          "84:                 tz = timezone(data[TIMEZONE])",
          "85:         else:",
          "86:             tz = deserialize_timezone(data[TIMEZONE][1], data[TIMEZONE][2], data[TIMEZONE][0])",
          "89:         return datetime.datetime.fromtimestamp(float(data[TIMESTAMP]), tz=tz)",
          "92:         return DateTime.fromtimestamp(float(data[TIMESTAMP]), tz=tz)",
          "",
          "---------------"
        ],
        "tests/serialization/serializers/test_serializers.py||tests/serialization/serializers/test_serializers.py": [
          "File: tests/serialization/serializers/test_serializers.py -> tests/serialization/serializers/test_serializers.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "52:         d = deserialize(s)",
          "53:         assert i == d",
          "55:     @pytest.mark.parametrize(",
          "56:         \"expr, expected\",",
          "57:         [(\"1\", \"1\"), (\"52e4\", \"520000\"), (\"2e0\", \"2\"), (\"12e-2\", \"0.12\"), (\"12.34\", \"12.34\")],",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "54:         i = datetime.datetime(",
          "55:             2022, 7, 10, 22, 10, 43, microsecond=0, tzinfo=pendulum.timezone(\"America/New_York\")",
          "56:         )",
          "57:         s = serialize(i)",
          "58:         d = deserialize(s)",
          "59:         assert i.timestamp() == d.timestamp()",
          "61:         i = DateTime(2022, 7, 10, tzinfo=pendulum.timezone(\"America/New_York\"))",
          "62:         s = serialize(i)",
          "63:         d = deserialize(s)",
          "64:         assert i.timestamp() == d.timestamp()",
          "66:     def test_deserialize_datetime_v1(self):",
          "68:         s = {",
          "69:             \"__classname__\": \"pendulum.datetime.DateTime\",",
          "70:             \"__version__\": 1,",
          "71:             \"__data__\": {\"timestamp\": 1657505443.0, \"tz\": \"UTC\"},",
          "72:         }",
          "73:         d = deserialize(s)",
          "74:         assert d.timestamp() == 1657505443.0",
          "75:         assert d.tzinfo.name == \"UTC\"",
          "77:         s[\"__data__\"][\"tz\"] = \"Europe/Paris\"",
          "78:         d = deserialize(s)",
          "79:         assert d.timestamp() == 1657505443.0",
          "80:         assert d.tzinfo.name == \"Europe/Paris\"",
          "82:         s[\"__data__\"][\"tz\"] = \"America/New_York\"",
          "83:         d = deserialize(s)",
          "84:         assert d.timestamp() == 1657505443.0",
          "85:         assert d.tzinfo.name == \"America/New_York\"",
          "87:         s[\"__data__\"][\"tz\"] = \"EDT\"",
          "88:         d = deserialize(s)",
          "89:         assert d.timestamp() == 1657505443.0",
          "90:         assert d.tzinfo.name == \"-04:00\"",
          "91:         # assert that it's serializable with the new format",
          "92:         assert deserialize(serialize(d)) == d",
          "94:         s[\"__data__\"][\"tz\"] = \"CDT\"",
          "95:         d = deserialize(s)",
          "96:         assert d.timestamp() == 1657505443.0",
          "97:         assert d.tzinfo.name == \"-05:00\"",
          "98:         # assert that it's serializable with the new format",
          "99:         assert deserialize(serialize(d)) == d",
          "101:         s[\"__data__\"][\"tz\"] = \"MDT\"",
          "102:         d = deserialize(s)",
          "103:         assert d.timestamp() == 1657505443.0",
          "104:         assert d.tzinfo.name == \"-06:00\"",
          "105:         # assert that it's serializable with the new format",
          "106:         assert deserialize(serialize(d)) == d",
          "108:         s[\"__data__\"][\"tz\"] = \"PDT\"",
          "109:         d = deserialize(s)",
          "110:         assert d.timestamp() == 1657505443.0",
          "111:         assert d.tzinfo.name == \"-07:00\"",
          "112:         # assert that it's serializable with the new format",
          "113:         assert deserialize(serialize(d)) == d",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "ee06e416cbfee1507c09d1fa1e89ce272af9e988",
      "candidate_info": {
        "commit_hash": "ee06e416cbfee1507c09d1fa1e89ce272af9e988",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/ee06e416cbfee1507c09d1fa1e89ce272af9e988",
        "files": [
          "airflow/www/security.py"
        ],
        "message": "Set loglevel=DEBUG in 'Not syncing DAG-level permissions' (#34268)\n\n(cherry picked from commit 8035aee8da3c4bb7b9c01cfdc7236ca01d658bae)",
        "before_after_code_files": [
          "airflow/www/security.py||airflow/www/security.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/www/security.py||airflow/www/security.py": [
          "File: airflow/www/security.py -> airflow/www/security.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "704:             self.create_permission(dag_action_name, dag_resource_name)",
          "706:         if access_control is not None:",
          "708:             self._sync_dag_view_permissions(dag_resource_name, access_control)",
          "709:         else:",
          "711:                 \"Not syncing DAG-level permissions for DAG '%s' as access control is unset.\",",
          "712:                 dag_resource_name,",
          "713:             )",
          "",
          "[Removed Lines]",
          "707:             self.log.info(\"Syncing DAG-level permissions for DAG '%s'\", dag_resource_name)",
          "710:             self.log.info(",
          "",
          "[Added Lines]",
          "707:             self.log.debug(\"Syncing DAG-level permissions for DAG '%s'\", dag_resource_name)",
          "710:             self.log.debug(",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "672871b4d77037a1fa16c199dec2d424d19f0c0c",
      "candidate_info": {
        "commit_hash": "672871b4d77037a1fa16c199dec2d424d19f0c0c",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/672871b4d77037a1fa16c199dec2d424d19f0c0c",
        "files": [
          "airflow/sensors/bash.py",
          "airflow/utils/db.py"
        ],
        "message": "Use a single  statement with multiple contexts instead of nested  statements in core (#33769)\n\n(cherry picked from commit 60e6847c181959d95789266cc2712cdacaf18cf9)",
        "before_after_code_files": [
          "airflow/sensors/bash.py||airflow/sensors/bash.py",
          "airflow/utils/db.py||airflow/utils/db.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/sensors/bash.py||airflow/sensors/bash.py": [
          "File: airflow/sensors/bash.py -> airflow/sensors/bash.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "68:         \"\"\"Execute the bash command in a temporary directory.\"\"\"",
          "69:         bash_command = self.bash_command",
          "70:         self.log.info(\"Tmp dir root location: %s\", gettempdir())",
          "111:                         return False",
          "",
          "[Removed Lines]",
          "71:         with TemporaryDirectory(prefix=\"airflowtmp\") as tmp_dir:",
          "72:             with NamedTemporaryFile(dir=tmp_dir, prefix=self.task_id) as f:",
          "73:                 f.write(bytes(bash_command, \"utf_8\"))",
          "74:                 f.flush()",
          "75:                 fname = f.name",
          "76:                 script_location = tmp_dir + \"/\" + fname",
          "77:                 self.log.info(\"Temporary script location: %s\", script_location)",
          "78:                 self.log.info(\"Running command: %s\", bash_command)",
          "80:                 with Popen(",
          "81:                     [\"bash\", fname],",
          "82:                     stdout=PIPE,",
          "83:                     stderr=STDOUT,",
          "84:                     close_fds=True,",
          "85:                     cwd=tmp_dir,",
          "86:                     env=self.env,",
          "87:                     preexec_fn=os.setsid,",
          "88:                 ) as resp:",
          "89:                     if resp.stdout:",
          "90:                         self.log.info(\"Output:\")",
          "91:                         for line in iter(resp.stdout.readline, b\"\"):",
          "92:                             self.log.info(line.decode(self.output_encoding).strip())",
          "93:                     resp.wait()",
          "94:                     self.log.info(\"Command exited with return code %s\", resp.returncode)",
          "96:                     # zero code means success, the sensor can go green",
          "97:                     if resp.returncode == 0:",
          "98:                         return True",
          "100:                     # we have a retry exit code, sensor retries if return code matches, otherwise error",
          "101:                     elif self.retry_exit_code is not None:",
          "102:                         if resp.returncode == self.retry_exit_code:",
          "103:                             self.log.info(\"Return code matches retry code, will retry later\")",
          "104:                             return False",
          "105:                         else:",
          "106:                             raise AirflowFailException(f\"Command exited with return code {resp.returncode}\")",
          "108:                     # backwards compatibility: sensor retries no matter the error code",
          "109:                     else:",
          "110:                         self.log.info(\"Non-zero return code and no retry code set, will retry later\")",
          "",
          "[Added Lines]",
          "71:         with TemporaryDirectory(prefix=\"airflowtmp\") as tmp_dir, NamedTemporaryFile(",
          "72:             dir=tmp_dir, prefix=self.task_id",
          "73:         ) as f:",
          "74:             f.write(bytes(bash_command, \"utf_8\"))",
          "75:             f.flush()",
          "76:             fname = f.name",
          "77:             script_location = tmp_dir + \"/\" + fname",
          "78:             self.log.info(\"Temporary script location: %s\", script_location)",
          "79:             self.log.info(\"Running command: %s\", bash_command)",
          "81:             with Popen(",
          "82:                 [\"bash\", fname],",
          "83:                 stdout=PIPE,",
          "84:                 stderr=STDOUT,",
          "85:                 close_fds=True,",
          "86:                 cwd=tmp_dir,",
          "87:                 env=self.env,",
          "88:                 preexec_fn=os.setsid,",
          "89:             ) as resp:",
          "90:                 if resp.stdout:",
          "91:                     self.log.info(\"Output:\")",
          "92:                     for line in iter(resp.stdout.readline, b\"\"):",
          "93:                         self.log.info(line.decode(self.output_encoding).strip())",
          "94:                 resp.wait()",
          "95:                 self.log.info(\"Command exited with return code %s\", resp.returncode)",
          "97:                 # zero code means success, the sensor can go green",
          "98:                 if resp.returncode == 0:",
          "99:                     return True",
          "101:                 # we have a retry exit code, sensor retries if return code matches, otherwise error",
          "102:                 elif self.retry_exit_code is not None:",
          "103:                     if resp.returncode == self.retry_exit_code:",
          "104:                         self.log.info(\"Return code matches retry code, will retry later\")",
          "106:                     else:",
          "107:                         raise AirflowFailException(f\"Command exited with return code {resp.returncode}\")",
          "109:                 # backwards compatibility: sensor retries no matter the error code",
          "110:                 else:",
          "111:                     self.log.info(\"Non-zero return code and no retry code set, will retry later\")",
          "112:                     return False",
          "",
          "---------------"
        ],
        "airflow/utils/db.py||airflow/utils/db.py": [
          "File: airflow/utils/db.py -> airflow/utils/db.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1652:     connection = settings.engine.connect()",
          "1659:     if not skip_init:",
          "1660:         initdb(session=session)",
          "",
          "[Removed Lines]",
          "1654:     with create_global_lock(session=session, lock=DBLocks.MIGRATIONS):",
          "1655:         with connection.begin():",
          "1656:             drop_airflow_models(connection)",
          "1657:             drop_airflow_moved_tables(connection)",
          "",
          "[Added Lines]",
          "1654:     with create_global_lock(session=session, lock=DBLocks.MIGRATIONS), connection.begin():",
          "1655:         drop_airflow_models(connection)",
          "1656:         drop_airflow_moved_tables(connection)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "08381bb9ee5d7e088d30f44df3d996c971abe919",
      "candidate_info": {
        "commit_hash": "08381bb9ee5d7e088d30f44df3d996c971abe919",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/08381bb9ee5d7e088d30f44df3d996c971abe919",
        "files": [
          "airflow/models/dagrun.py",
          "airflow/models/taskinstance.py"
        ],
        "message": "Fix foreign key warning re ab_user.id (#34656)\n\nIntroduced in #34120.\n\n(cherry picked from commit 1fdc2311250fbae47749822b192a99066600f8ad)",
        "before_after_code_files": [
          "airflow/models/dagrun.py||airflow/models/dagrun.py",
          "airflow/models/taskinstance.py||airflow/models/taskinstance.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/models/dagrun.py||airflow/models/dagrun.py": [
          "File: airflow/models/dagrun.py -> airflow/models/dagrun.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1409:     user_id = Column(",
          "1410:         Integer,",
          "1411:         nullable=True,",
          "1413:     )",
          "1414:     dag_run_id = Column(Integer, primary_key=True, nullable=False)",
          "1415:     content = Column(String(1000).with_variant(Text(1000), \"mysql\"))",
          "",
          "[Removed Lines]",
          "1412:         foreign_key=ForeignKey(\"ab_user.id\", name=\"dag_run_note_user_fkey\"),",
          "",
          "[Added Lines]",
          "1411:         ForeignKey(\"ab_user.id\", name=\"dag_run_note_user_fkey\"),",
          "",
          "---------------"
        ],
        "airflow/models/taskinstance.py||airflow/models/taskinstance.py": [
          "File: airflow/models/taskinstance.py -> airflow/models/taskinstance.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "3017:     __tablename__ = \"task_instance_note\"",
          "3022:     task_id = Column(StringID(), primary_key=True, nullable=False)",
          "3023:     dag_id = Column(StringID(), primary_key=True, nullable=False)",
          "3024:     run_id = Column(StringID(), primary_key=True, nullable=False)",
          "",
          "[Removed Lines]",
          "3019:     user_id = Column(",
          "3020:         Integer, nullable=True, foreign_key=ForeignKey(\"ab_user.id\", name=\"task_instance_note_user_fkey\")",
          "3021:     )",
          "",
          "[Added Lines]",
          "3019:     user_id = Column(Integer, ForeignKey(\"ab_user.id\", name=\"task_instance_note_user_fkey\"), nullable=True)",
          "",
          "---------------"
        ]
      }
    }
  ]
}