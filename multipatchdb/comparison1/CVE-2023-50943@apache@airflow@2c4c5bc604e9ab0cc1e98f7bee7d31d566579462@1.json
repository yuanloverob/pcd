{
  "cve_id": "CVE-2023-50943",
  "cve_desc": "Apache Airflow, versions before 2.8.1, have a vulnerability that allows a potential attacker to poison the XCom data by bypassing the protection of \"enable_xcom_pickling=False\" configuration setting resulting in poisoned data after XCom deserialization. This vulnerability is considered low since it requires a DAG author to exploit it. Users are recommended to upgrade to version 2.8.1 or later, which fixes this issue.",
  "repo": "apache/airflow",
  "patch_hash": "2c4c5bc604e9ab0cc1e98f7bee7d31d566579462",
  "patch_info": {
    "commit_hash": "2c4c5bc604e9ab0cc1e98f7bee7d31d566579462",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/2c4c5bc604e9ab0cc1e98f7bee7d31d566579462",
    "files": [
      "airflow/models/xcom.py",
      "tests/api_connexion/schemas/test_xcom_schema.py",
      "tests/models/test_xcom.py"
    ],
    "message": "Stop deserializing pickle when enable_xcom_pickling is False (#36255)\n\n* Stop deserializing pickle when enable_xcom_pickling is False\n\n* Fix unit tests\n\n(cherry picked from commit 63e97abec5d56bc62a293c93f5227f364561e51c)",
    "before_after_code_files": [
      "airflow/models/xcom.py||airflow/models/xcom.py",
      "tests/api_connexion/schemas/test_xcom_schema.py||tests/api_connexion/schemas/test_xcom_schema.py",
      "tests/models/test_xcom.py||tests/models/test_xcom.py"
    ]
  },
  "patch_diff": {
    "airflow/models/xcom.py||airflow/models/xcom.py": [
      "File: airflow/models/xcom.py -> airflow/models/xcom.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "685:             except pickle.UnpicklingError:",
      "686:                 return json.loads(result.value.decode(\"UTF-8\"), cls=XComDecoder, object_hook=object_hook)",
      "687:         else:",
      "693:     @staticmethod",
      "694:     def deserialize_value(result: XCom) -> Any:",
      "",
      "[Removed Lines]",
      "688:             try:",
      "689:                 return json.loads(result.value.decode(\"UTF-8\"), cls=XComDecoder, object_hook=object_hook)",
      "690:             except (json.JSONDecodeError, UnicodeDecodeError):",
      "691:                 return pickle.loads(result.value)",
      "",
      "[Added Lines]",
      "688:             # Since xcom_pickling is disabled, we should only try to deserialize with JSON",
      "689:             return json.loads(result.value.decode(\"UTF-8\"), cls=XComDecoder, object_hook=object_hook)",
      "",
      "---------------"
    ],
    "tests/api_connexion/schemas/test_xcom_schema.py||tests/api_connexion/schemas/test_xcom_schema.py": [
      "File: tests/api_connexion/schemas/test_xcom_schema.py -> tests/api_connexion/schemas/test_xcom_schema.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "30: from airflow.models import DagRun, XCom",
      "31: from airflow.utils.dates import parse_execution_date",
      "32: from airflow.utils.session import create_session",
      "34: pytestmark = pytest.mark.db_test",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "33: from tests.test_utils.config import conf_vars",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "188:     default_time = \"2016-04-02T21:00:00+00:00\"",
      "189:     default_time_parsed = parse_execution_date(default_time)",
      "191:     def test_serialize(self, create_xcom, session):",
      "192:         create_xcom(",
      "193:             dag_id=\"test_dag\",",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "192:     @conf_vars({(\"core\", \"enable_xcom_pickling\"): \"True\"})",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "208:             \"map_index\": -1,",
      "209:         }",
      "211:     def test_deserialize(self):",
      "212:         xcom_dump = {",
      "213:             \"key\": \"test_key\",",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "213:     @conf_vars({(\"core\", \"enable_xcom_pickling\"): \"True\"})",
      "",
      "---------------"
    ],
    "tests/models/test_xcom.py||tests/models/test_xcom.py": [
      "File: tests/models/test_xcom.py -> tests/models/test_xcom.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "140:             ret_value = XCom.get_value(key=\"xcom_test3\", ti_key=ti_key, session=session)",
      "141:         assert ret_value == {\"key\": \"value\"}",
      "144:         with conf_vars({(\"core\", \"enable_xcom_pickling\"): \"True\"}):",
      "145:             XCom.set(",
      "146:                 key=\"xcom_test3\",",
      "",
      "[Removed Lines]",
      "143:     def test_xcom_deserialize_with_pickle_to_json_switch(self, task_instance, session):",
      "",
      "[Added Lines]",
      "143:     def test_xcom_deserialize_pickle_when_xcom_pickling_is_disabled(self, task_instance, session):",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "151:                 session=session,",
      "152:             )",
      "153:         with conf_vars({(\"core\", \"enable_xcom_pickling\"): \"False\"}):",
      "163:     @conf_vars({(\"core\", \"xcom_enable_pickling\"): \"False\"})",
      "164:     def test_xcom_disable_pickle_type_fail_on_non_json(self, task_instance, session):",
      "",
      "[Removed Lines]",
      "154:             ret_value = XCom.get_one(",
      "155:                 key=\"xcom_test3\",",
      "156:                 dag_id=task_instance.dag_id,",
      "157:                 task_id=task_instance.task_id,",
      "158:                 run_id=task_instance.run_id,",
      "159:                 session=session,",
      "160:             )",
      "161:         assert ret_value == {\"key\": \"value\"}",
      "",
      "[Added Lines]",
      "154:             with pytest.raises(UnicodeDecodeError):",
      "155:                 XCom.get_one(",
      "156:                     key=\"xcom_test3\",",
      "157:                     dag_id=task_instance.dag_id,",
      "158:                     task_id=task_instance.task_id,",
      "159:                     run_id=task_instance.run_id,",
      "160:                     session=session,",
      "161:                 )",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "154a78dac9a89a54ce9de483c8eaa196f98862aa",
      "candidate_info": {
        "commit_hash": "154a78dac9a89a54ce9de483c8eaa196f98862aa",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/154a78dac9a89a54ce9de483c8eaa196f98862aa",
        "files": [
          "airflow/timetables/events.py",
          "tests/timetables/test_events_timetable.py"
        ],
        "message": "Do not let EventsTimetable schedule past events if catchup=False (#36134)\n\n* Fix the EventsTimetable schedules past events bug\n---------\n\nCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>\n(cherry picked from commit c01daf811925816e9ae09b78c37b9ff8d87ce691)",
        "before_after_code_files": [
          "airflow/timetables/events.py||airflow/timetables/events.py",
          "tests/timetables/test_events_timetable.py||tests/timetables/test_events_timetable.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/timetables/events.py||airflow/timetables/events.py": [
          "File: airflow/timetables/events.py -> airflow/timetables/events.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "22: import pendulum",
          "24: from airflow.timetables.base import DagRunInfo, DataInterval, Timetable",
          "26: if TYPE_CHECKING:",
          "27:     from pendulum import DateTime",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "25: from airflow.utils import timezone",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "58:             self.event_dates.sort()",
          "59:         self.restrict_to_events = restrict_to_events",
          "60:         if description is None:",
          "65:         else:",
          "66:             self._summary = description",
          "67:             self.description = description",
          "",
          "[Removed Lines]",
          "61:             self.description = (",
          "62:                 f\"{len(self.event_dates)} Events between {self.event_dates[0]} and {self.event_dates[-1]}\"",
          "63:             )",
          "64:             self._summary = f\"{len(self.event_dates)} Events\"",
          "",
          "[Added Lines]",
          "62:             if self.event_dates:",
          "63:                 self.description = (",
          "64:                     f\"{len(self.event_dates)} events between {self.event_dates[0]} and {self.event_dates[-1]}\"",
          "65:                 )",
          "66:             else:",
          "67:                 self.description = \"No events\"",
          "68:             self._summary = f\"{len(self.event_dates)} events\"",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "79:         last_automated_data_interval: DataInterval | None,",
          "80:         restriction: TimeRestriction,",
          "81:     ) -> DagRunInfo | None:",
          "84:         else:",
          "93:         return DagRunInfo.exact(next_event)",
          "95:     def infer_manual_data_interval(self, *, run_after: DateTime) -> DataInterval:",
          "96:         # If Timetable not restricted to events, run for the time specified",
          "98:             return DataInterval.exact(run_after)",
          "100:         # If restricted to events, run for the most recent past event",
          "",
          "[Removed Lines]",
          "82:         if last_automated_data_interval is None:",
          "83:             next_event = self.event_dates[0]",
          "85:             future_dates = itertools.dropwhile(",
          "86:                 lambda when: when <= last_automated_data_interval.end,  # type: ignore",
          "87:                 self.event_dates,",
          "88:             )",
          "89:             next_event = next(future_dates, None)  # type: ignore",
          "90:             if next_event is None:",
          "91:                 return None",
          "97:         if not self.restrict_to_events:",
          "",
          "[Added Lines]",
          "86:         earliest = restriction.earliest",
          "87:         if not restriction.catchup:",
          "88:             current_time = timezone.utcnow()",
          "89:             if earliest is None or current_time > earliest:",
          "90:                 earliest = pendulum.instance(current_time)",
          "92:         for next_event in self.event_dates:",
          "93:             if earliest and next_event < earliest:",
          "94:                 continue",
          "95:             if last_automated_data_interval and next_event <= last_automated_data_interval.end:",
          "96:                 continue",
          "97:             break",
          "99:             # We need to return None if self.event_dates is empty or,",
          "100:             # if not empty, when no suitable event can be found.",
          "101:             return None",
          "103:         if restriction.latest is not None and next_event > restriction.latest:",
          "104:             return None",
          "110:         if not self.restrict_to_events or not self.event_dates:",
          "",
          "---------------"
        ],
        "tests/timetables/test_events_timetable.py||tests/timetables/test_events_timetable.py": [
          "File: tests/timetables/test_events_timetable.py -> tests/timetables/test_events_timetable.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: import pendulum",
          "21: import pytest",
          "23: from airflow.timetables.base import DagRunInfo, DataInterval, TimeRestriction, Timetable",
          "24: from airflow.timetables.events import EventsTimetable",
          "25: from airflow.utils.timezone import utc",
          "29: EVENT_DATES = [",
          "30:     pendulum.DateTime(2021, 9, 6, tzinfo=utc),",
          "",
          "[Removed Lines]",
          "27: START_DATE = pendulum.DateTime(2021, 9, 4, tzinfo=utc)  # Precedes all events",
          "",
          "[Added Lines]",
          "22: import time_machine",
          "28: BEFORE_DATE = pendulum.DateTime(2021, 9, 4, tzinfo=utc)  # Precedes all events",
          "29: START_DATE = pendulum.DateTime(2021, 9, 7, tzinfo=utc)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "93:     Test that when using strict event dates, manual runs before the first event have the first event's date",
          "94:     as the start interval",
          "95:     \"\"\"",
          "97:     expected_data_interval = DataInterval.exact(EVENT_DATES[0])",
          "98:     assert expected_data_interval == manual_run_data_interval",
          "",
          "[Removed Lines]",
          "96:     manual_run_data_interval = restricted_timetable.infer_manual_data_interval(run_after=START_DATE)",
          "",
          "[Added Lines]",
          "98:     manual_run_data_interval = restricted_timetable.infer_manual_data_interval(run_after=BEFORE_DATE)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "101: @pytest.mark.parametrize(",
          "102:     \"last_automated_data_interval, expected_next_info\",",
          "103:     [",
          "104:         pytest.param(DataInterval(day1, day1), DagRunInfo.interval(day2, day2))",
          "106:     ]",
          "107:     + [pytest.param(DataInterval(EVENT_DATES_SORTED[-1], EVENT_DATES_SORTED[-1]), None)],",
          "108: )",
          "",
          "[Removed Lines]",
          "105:         for day1, day2 in zip(EVENT_DATES_SORTED, EVENT_DATES_SORTED[1:])",
          "",
          "[Added Lines]",
          "106:         pytest.param(None, DagRunInfo.interval(START_DATE, START_DATE)),",
          "107:         pytest.param(",
          "108:             DataInterval(EVENT_DATES_SORTED[0], EVENT_DATES_SORTED[0]),",
          "109:             DagRunInfo.interval(START_DATE, START_DATE),",
          "110:         ),",
          "111:     ]",
          "112:     + [",
          "114:         for day1, day2 in zip(EVENT_DATES_SORTED[1:], EVENT_DATES_SORTED[2:])",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "118:         restriction=restriction,",
          "119:     )",
          "120:     assert next_info == expected_next_info",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "132: @pytest.mark.parametrize(",
          "133:     \"current_date\",",
          "134:     [",
          "135:         pytest.param(pendulum.DateTime(2021, 9, 1, tzinfo=utc), id=\"when-current-date-is-before-first-event\"),",
          "136:         pytest.param(pendulum.DateTime(2021, 9, 8, tzinfo=utc), id=\"when-current-date-is-in-the-middle\"),",
          "137:         pytest.param(pendulum.DateTime(2021, 12, 9, tzinfo=utc), id=\"when-current-date-is-after-last-event\"),",
          "138:     ],",
          "139: )",
          "140: @pytest.mark.parametrize(",
          "141:     \"last_automated_data_interval\",",
          "142:     [",
          "143:         pytest.param(None, id=\"first-run\"),",
          "144:         pytest.param(DataInterval(start=BEFORE_DATE, end=BEFORE_DATE), id=\"subsequent-run\"),",
          "145:     ],",
          "146: )",
          "147: def test_no_catchup_first_starts(",
          "148:     last_automated_data_interval: DataInterval | None,",
          "149:     current_date,",
          "150:     unrestricted_timetable: Timetable,",
          "151: ) -> None:",
          "152:     # we don't use the last_automated_data_interval here because it's always less than the first event",
          "153:     expected_date = max(current_date, START_DATE, EVENT_DATES_SORTED[0])",
          "154:     expected_info = None",
          "155:     if expected_date <= EVENT_DATES_SORTED[-1]:",
          "156:         expected_info = DagRunInfo.interval(start=expected_date, end=expected_date)",
          "158:     with time_machine.travel(current_date):",
          "159:         next_info = unrestricted_timetable.next_dagrun_info(",
          "160:             last_automated_data_interval=last_automated_data_interval,",
          "161:             restriction=TimeRestriction(earliest=START_DATE, latest=None, catchup=False),",
          "162:         )",
          "163:     assert next_info == expected_info",
          "166: def test_empty_timetable() -> None:",
          "167:     empty_timetable = EventsTimetable(event_dates=[])",
          "168:     next_info = empty_timetable.next_dagrun_info(",
          "169:         last_automated_data_interval=None,",
          "170:         restriction=TimeRestriction(earliest=START_DATE, latest=None, catchup=False),",
          "171:     )",
          "172:     assert next_info is None",
          "175: def test_empty_timetable_manual_run() -> None:",
          "176:     empty_timetable = EventsTimetable(event_dates=[])",
          "177:     manual_run_data_interval = empty_timetable.infer_manual_data_interval(run_after=START_DATE)",
          "178:     assert manual_run_data_interval == DataInterval(start=START_DATE, end=START_DATE)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "63c4546bb7e7d9b3c393755737796332184c6eb2",
      "candidate_info": {
        "commit_hash": "63c4546bb7e7d9b3c393755737796332184c6eb2",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/63c4546bb7e7d9b3c393755737796332184c6eb2",
        "files": [
          "airflow/providers/amazon/provider.yaml",
          "generated/provider_dependencies.json",
          "setup.py"
        ],
        "message": "Bump min version of amazon-provider related dependencies (#36660)\n\nThis is a regular bump of Amazon-provider related dependencies.\nThe way how botocore releases are done, they are putting a lot of\nstrain on `pip` to resolve the right set of dependencies, including\nlong backtracking, when there are too man versions available.\n\nTherefore, from time to time, we are bumping minimum version of\nAmazon-related dependencies to limit the impact frequent releases\nof boto and botocore has. Also it is generally fine to update min\nversion of dependencies for providers because at the very least\nusers can still use previously released providers in case they\nhave problem with those dependencies, also many of the updated\ndependencies contain fixes and feature we implicitly depend on and\nbumping them regulary is a good way to make sure all the functionalities\nof the Amazon provider are working as expected.\n\nAnother reason for the bump is that as of 1.33 version botocore and\nboto version stopped being shifted by 3 (previously boto3 1.28 was\nthe version corresponding to botocore 1.31). As of version 1.33 this\nproblem has been solved. See https://github.com/boto/boto3/issues/2702\n\nWatchtower min version is bumped to version 3 (which is 12 months old\neven if before we opted for much older (more than 2 years old) and again\nif users want to use older version of watchtower, they can opt for\nprevious provider version.\n\nThis change saves 5-6 minutes of backtracking when `pip` try to\nfind the right version of dependencies when upgrading to newer version.\n\nExtracted from #36537\n\n(cherry picked from commit 298c37d355eeadfccbd655efb2922d39ba17052c)",
        "before_after_code_files": [
          "setup.py||setup.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "setup.py||setup.py": [
          "File: setup.py -> setup.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "409: ]",
          "411: # make sure to update providers/amazon/provider.yaml botocore min version when you update it here",
          "414: _devel_only_amazon = [",
          "417:     f\"mypy-boto3-rds>={_MIN_BOTO3_VERSION}\",",
          "418:     f\"mypy-boto3-redshift-data>={_MIN_BOTO3_VERSION}\",",
          "419:     f\"mypy-boto3-s3>={_MIN_BOTO3_VERSION}\",",
          "421: ]",
          "423: _devel_only_azure = [",
          "",
          "[Removed Lines]",
          "412: _MIN_BOTO3_VERSION = \"1.28.0\"",
          "415:     \"aws_xray_sdk\",",
          "416:     \"moto[cloudformation,glue]>=4.2.9\",",
          "420:     f\"mypy-boto3-appflow>={_MIN_BOTO3_VERSION}\",",
          "",
          "[Added Lines]",
          "412: _MIN_BOTO3_VERSION = \"1.33.0\"",
          "415:     \"aws_xray_sdk>=2.12.0\",",
          "416:     \"moto[cloudformation,glue]>=4.2.12\",",
          "417:     f\"mypy-boto3-appflow>={_MIN_BOTO3_VERSION}\",",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "a161e6eb6e7f7ba8bfa57c80a83b83800d72e1ee",
      "candidate_info": {
        "commit_hash": "a161e6eb6e7f7ba8bfa57c80a83b83800d72e1ee",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/a161e6eb6e7f7ba8bfa57c80a83b83800d72e1ee",
        "files": [
          ".github/workflows/ci.yml",
          "Dockerfile",
          "Dockerfile.ci",
          "scripts/docker/install_os_dependencies.sh"
        ],
        "message": "Add zlib1g-dev package to Airflow images (#36493)\n\nSeems that when mysql repository is used to install mysql client,\nit induces libxml compilation for Python 3.8 and 3.9 and this\nlibrary requires devel version of zlib that is missing in the image.\n\nThis PR adds the devel version as dev apt dependency.\n\n(cherry picked from commit 2bc34ffcb5e830544e024e085f36481a33852f49)",
        "before_after_code_files": [
          "Dockerfile.ci||Dockerfile.ci",
          "scripts/docker/install_os_dependencies.sh||scripts/docker/install_os_dependencies.sh"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "Dockerfile.ci||Dockerfile.ci": [
          "File: Dockerfile.ci -> Dockerfile.ci",
          "--- Hunk 1 ---",
          "[Context before]",
          "58: freetds-bin freetds-dev git gosu graphviz graphviz-dev krb5-user ldap-utils libffi-dev libgeos-dev \\",
          "59: libkrb5-dev libldap2-dev libleveldb1d libleveldb-dev libsasl2-2 libsasl2-dev libsasl2-modules \\",
          "60: libssl-dev libxmlsec1 libxmlsec1-dev locales lsb-release openssh-client pkgconf sasl2-bin \\",
          "62:         export DEV_APT_DEPS",
          "63:     fi",
          "64: }",
          "",
          "[Removed Lines]",
          "61: software-properties-common sqlite3 sudo unixodbc unixodbc-dev\"",
          "",
          "[Added Lines]",
          "61: software-properties-common sqlite3 sudo unixodbc unixodbc-dev zlib1g-dev\"",
          "",
          "---------------"
        ],
        "scripts/docker/install_os_dependencies.sh||scripts/docker/install_os_dependencies.sh": [
          "File: scripts/docker/install_os_dependencies.sh -> scripts/docker/install_os_dependencies.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "40: freetds-bin freetds-dev git gosu graphviz graphviz-dev krb5-user ldap-utils libffi-dev libgeos-dev \\",
          "41: libkrb5-dev libldap2-dev libleveldb1d libleveldb-dev libsasl2-2 libsasl2-dev libsasl2-modules \\",
          "42: libssl-dev libxmlsec1 libxmlsec1-dev locales lsb-release openssh-client pkgconf sasl2-bin \\",
          "44:         export DEV_APT_DEPS",
          "45:     fi",
          "46: }",
          "",
          "[Removed Lines]",
          "43: software-properties-common sqlite3 sudo unixodbc unixodbc-dev\"",
          "",
          "[Added Lines]",
          "43: software-properties-common sqlite3 sudo unixodbc unixodbc-dev zlib1g-dev\"",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "0d12706dd9e8559942c303df8bb86d107ae9b039",
      "candidate_info": {
        "commit_hash": "0d12706dd9e8559942c303df8bb86d107ae9b039",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/0d12706dd9e8559942c303df8bb86d107ae9b039",
        "files": [
          "dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "dev/breeze/src/airflow_breeze/utils/docs_publisher.py",
          "dev/breeze/src/airflow_breeze/utils/publish_docs_builder.py",
          "docs/exts/docs_build/docs_builder.py"
        ],
        "message": "Remove rendundant and unused code in docs building/publishing (#36346)\n\nCurrently docs building happens insid of the container image and code\ndoing that sits in `docs` folder, while publishing has already been\nmoved to `breeze` code (and is executed in the Breeze venv, not in the\ncontainer). Both building and publishing code were present in both\n(copy&pasted) and the parts of it not relevant to the `other` function\nhas not been used.\n\nWhile eventually we will move docs building also to `breeze` the first\nstep of that is to remove the redundancy and clean-up unused code, so\nthat we can make the transition cleaner.\n\n(cherry picked from commit bf90992dd48bce7de9f2a687860479e95575cd24)",
        "before_after_code_files": [
          "dev/breeze/src/airflow_breeze/commands/release_management_commands.py||dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "dev/breeze/src/airflow_breeze/utils/docs_publisher.py||dev/breeze/src/airflow_breeze/utils/docs_publisher.py",
          "dev/breeze/src/airflow_breeze/utils/publish_docs_builder.py||dev/breeze/src/airflow_breeze/utils/publish_docs_builder.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/breeze/src/airflow_breeze/commands/release_management_commands.py||dev/breeze/src/airflow_breeze/commands/release_management_commands.py": [
          "File: dev/breeze/src/airflow_breeze/commands/release_management_commands.py -> dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "109:     fix_ownership_using_docker,",
          "110:     perform_environment_checks,",
          "111: )",
          "112: from airflow_breeze.utils.github import download_constraints_file, get_active_airflow_versions",
          "113: from airflow_breeze.utils.packages import (",
          "114:     PackageSuspendedException,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "112: from airflow_breeze.utils.docs_publisher import DocsPublisher",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "139:     generate_providers_metadata_for_package,",
          "140:     get_related_providers,",
          "141: )",
          "143: from airflow_breeze.utils.python_versions import get_python_version_list",
          "144: from airflow_breeze.utils.run_utils import (",
          "145:     clean_www_assets,",
          "",
          "[Removed Lines]",
          "142: from airflow_breeze.utils.publish_docs_builder import PublishDocsBuilder",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "1083:     verbose: bool,",
          "1084:     output: Output | None,",
          "1085: ) -> tuple[int, str]:",
          "1087:     builder.publish(override_versioned=override_versioned, airflow_site_dir=airflow_site_directory)",
          "1088:     return (",
          "1089:         0,",
          "",
          "[Removed Lines]",
          "1086:     builder = PublishDocsBuilder(package_name=package_name, output=output, verbose=verbose)",
          "",
          "[Added Lines]",
          "1086:     builder = DocsPublisher(package_name=package_name, output=output, verbose=verbose)",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/utils/docs_publisher.py||dev/breeze/src/airflow_breeze/utils/docs_publisher.py": [
          "File: dev/breeze/src/airflow_breeze/utils/docs_publisher.py -> dev/breeze/src/airflow_breeze/utils/docs_publisher.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: # Licensed to the Apache Software Foundation (ASF) under one",
          "2: # or more contributor license agreements.  See the NOTICE file",
          "3: # distributed with this work for additional information",
          "4: # regarding copyright ownership.  The ASF licenses this file",
          "5: # to you under the Apache License, Version 2.0 (the",
          "6: # \"License\"); you may not use this file except in compliance",
          "7: # with the License.  You may obtain a copy of the License at",
          "8: #",
          "9: #   http://www.apache.org/licenses/LICENSE-2.0",
          "10: #",
          "11: # Unless required by applicable law or agreed to in writing,",
          "12: # software distributed under the License is distributed on an",
          "13: # \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
          "14: # KIND, either express or implied.  See the License for the",
          "15: # specific language governing permissions and limitations",
          "16: # under the License.",
          "17: from __future__ import annotations",
          "19: import os",
          "20: import shutil",
          "21: from pathlib import Path",
          "23: from airflow_breeze.global_constants import get_airflow_version",
          "24: from airflow_breeze.utils.console import Output, get_console",
          "25: from airflow_breeze.utils.helm_chart_utils import chart_version",
          "26: from airflow_breeze.utils.packages import get_provider_packages_metadata, get_short_package_name",
          "27: from airflow_breeze.utils.publish_docs_helpers import pretty_format_path",
          "29: PROCESS_TIMEOUT = 15 * 60",
          "31: ROOT_PROJECT_DIR = Path(__file__).parents[5].resolve()",
          "32: DOCS_DIR = os.path.join(ROOT_PROJECT_DIR, \"docs\")",
          "35: class DocsPublisher:",
          "36:     \"\"\"Documentation builder for Airflow Docs Publishing.\"\"\"",
          "38:     def __init__(self, package_name: str, output: Output | None, verbose: bool):",
          "39:         self.package_name = package_name",
          "40:         self.output = output",
          "41:         self.verbose = verbose",
          "43:     @property",
          "44:     def is_versioned(self):",
          "45:         \"\"\"Is current documentation package versioned?\"\"\"",
          "46:         # Disable versioning. This documentation does not apply to any released product and we can update",
          "47:         # it as needed, i.e. with each new package of providers.",
          "48:         return self.package_name not in (\"apache-airflow-providers\", \"docker-stack\")",
          "50:     @property",
          "51:     def _build_dir(self) -> str:",
          "52:         if self.is_versioned:",
          "53:             version = \"stable\"",
          "54:             return f\"{DOCS_DIR}/_build/docs/{self.package_name}/{version}\"",
          "55:         else:",
          "56:             return f\"{DOCS_DIR}/_build/docs/{self.package_name}\"",
          "58:     @property",
          "59:     def _current_version(self):",
          "60:         if not self.is_versioned:",
          "61:             raise Exception(\"This documentation package is not versioned\")",
          "62:         if self.package_name == \"apache-airflow\":",
          "63:             return get_airflow_version()",
          "64:         if self.package_name.startswith(\"apache-airflow-providers-\"):",
          "65:             provider = get_provider_packages_metadata().get(get_short_package_name(self.package_name))",
          "66:             return provider[\"versions\"][0]",
          "67:         if self.package_name == \"helm-chart\":",
          "68:             return chart_version()",
          "69:         return Exception(f\"Unsupported package: {self.package_name}\")",
          "71:     @property",
          "72:     def _publish_dir(self) -> str:",
          "73:         if self.is_versioned:",
          "74:             return f\"docs-archive/{self.package_name}/{self._current_version}\"",
          "75:         else:",
          "76:             return f\"docs-archive/{self.package_name}\"",
          "78:     def publish(self, override_versioned: bool, airflow_site_dir: str):",
          "79:         \"\"\"Copy documentation packages files to airflow-site repository.\"\"\"",
          "80:         get_console(output=self.output).print(f\"Publishing docs for {self.package_name}\")",
          "81:         output_dir = os.path.join(airflow_site_dir, self._publish_dir)",
          "82:         pretty_source = pretty_format_path(self._build_dir, os.getcwd())",
          "83:         pretty_target = pretty_format_path(output_dir, airflow_site_dir)",
          "84:         get_console(output=self.output).print(f\"Copy directory: {pretty_source} => {pretty_target}\")",
          "85:         if os.path.exists(output_dir):",
          "86:             if self.is_versioned:",
          "87:                 if override_versioned:",
          "88:                     get_console(output=self.output).print(f\"Overriding previously existing {output_dir}! \")",
          "89:                 else:",
          "90:                     get_console(output=self.output).print(",
          "91:                         f\"Skipping previously existing {output_dir}! \"",
          "92:                         f\"Delete it manually if you want to regenerate it!\"",
          "93:                     )",
          "94:                     get_console(output=self.output).print()",
          "95:                     return",
          "96:             shutil.rmtree(output_dir)",
          "97:         shutil.copytree(self._build_dir, output_dir)",
          "98:         if self.is_versioned:",
          "99:             with open(os.path.join(output_dir, \"..\", \"stable.txt\"), \"w\") as stable_file:",
          "100:                 stable_file.write(self._current_version)",
          "101:         get_console(output=self.output).print()",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/utils/publish_docs_builder.py||dev/breeze/src/airflow_breeze/utils/publish_docs_builder.py": [
          "File: dev/breeze/src/airflow_breeze/utils/publish_docs_builder.py -> dev/breeze/src/airflow_breeze/utils/publish_docs_builder.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "842fd3b7fae758b53a2331f2bce87dc055bd174b",
      "candidate_info": {
        "commit_hash": "842fd3b7fae758b53a2331f2bce87dc055bd174b",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/842fd3b7fae758b53a2331f2bce87dc055bd174b",
        "files": [
          "airflow/jobs/backfill_job_runner.py"
        ],
        "message": "Refactor _manage_executor_state by refreshing TIs in batch (#36418)\n\n* Refactor _manage_executor_state by refreshing TIs in batch\n\n* Use a short key without retry number\n\n(cherry picked from commit 9d45db9e2cca2ad04db72f7e0712c478e5a8e1f1)",
        "before_after_code_files": [
          "airflow/jobs/backfill_job_runner.py||airflow/jobs/backfill_job_runner.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/jobs/backfill_job_runner.py||airflow/jobs/backfill_job_runner.py": [
          "File: airflow/jobs/backfill_job_runner.py -> airflow/jobs/backfill_job_runner.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "23: import attr",
          "24: import pendulum",
          "26: from sqlalchemy.exc import OperationalError",
          "27: from sqlalchemy.orm.session import make_transient",
          "28: from tabulate import tabulate",
          "",
          "[Removed Lines]",
          "25: from sqlalchemy import select, update",
          "",
          "[Added Lines]",
          "25: from sqlalchemy import select, tuple_, update",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "264:         :return: An iterable of expanded TaskInstance per MappedTask",
          "265:         \"\"\"",
          "266:         executor = self.job.executor",
          "270:             state, info = value",
          "272:                 self.log.warning(\"%s state %s not in running=%s\", key, state, running.values())",
          "273:                 continue",
          "278:             self.log.debug(\"Executor state: %s task %s\", state, ti)",
          "",
          "[Removed Lines]",
          "268:         # TODO: query all instead of refresh from db",
          "269:         for key, value in list(executor.get_event_buffer().items()):",
          "271:             if key not in running:",
          "275:             ti = running[key]",
          "276:             ti.refresh_from_db()",
          "",
          "[Added Lines]",
          "267:         # list of tuples (dag_id, task_id, execution_date, map_index) of running tasks in executor",
          "268:         buffered_events = list(executor.get_event_buffer().items())",
          "269:         running_tis_ids = [",
          "270:             (key.dag_id, key.task_id, key.run_id, key.map_index)",
          "271:             for key, _ in buffered_events",
          "272:             if key in running",
          "273:         ]",
          "274:         # list of TaskInstance of running tasks in executor (refreshed from db in batch)",
          "275:         refreshed_running_tis = session.scalars(",
          "276:             select(TaskInstance).where(",
          "277:                 tuple_(",
          "278:                     TaskInstance.dag_id,",
          "279:                     TaskInstance.task_id,",
          "280:                     TaskInstance.run_id,",
          "281:                     TaskInstance.map_index,",
          "282:                 ).in_(running_tis_ids)",
          "283:             )",
          "284:         ).all()",
          "285:         # dict of refreshed TaskInstance by key to easily find them",
          "286:         refreshed_running_tis_dict = {",
          "287:             (ti.dag_id, ti.task_id, ti.run_id, ti.map_index): ti for ti in refreshed_running_tis",
          "288:         }",
          "290:         for key, value in buffered_events:",
          "292:             ti_key = (key.dag_id, key.task_id, key.run_id, key.map_index)",
          "293:             if ti_key not in refreshed_running_tis_dict:",
          "297:             ti = refreshed_running_tis_dict[ti_key]",
          "",
          "---------------"
        ]
      }
    }
  ]
}