{
  "cve_id": "CVE-2023-42780",
  "cve_desc": "Apache Airflow, versions prior to 2.7.2, contains a security vulnerability that allows authenticated users of Airflow to list warnings for all DAGs, even if the user had no permission to see those DAGs. It would reveal the dag_ids and the stack-traces of import errors for those DAGs with import errors.\nUsers of Apache Airflow are advised to upgrade to version 2.7.2 or newer to mitigate the risk associated with this vulnerability.\n\n",
  "repo": "apache/airflow",
  "patch_hash": "cf4eb3fb9b5cf4a8369b890e39523d4c05eed161",
  "patch_info": {
    "commit_hash": "cf4eb3fb9b5cf4a8369b890e39523d4c05eed161",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/cf4eb3fb9b5cf4a8369b890e39523d4c05eed161",
    "files": [
      "airflow/api_connexion/endpoints/dag_warning_endpoint.py",
      "tests/api_connexion/endpoints/test_dag_warning_endpoint.py"
    ],
    "message": "Fix dag warning endpoint permissions (#34355)\n\n* Fix dag warning endpoint permissions\n\n* update the query to have an accurate result for total entries and pagination\n\n* add unit tests\n\n* Update test_dag_warning_endpoint.py\n\nCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>\n\n---------\n\nCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>\n(cherry picked from commit 3570bbfbea69e2965f91b9964ce28bc268c68129)",
    "before_after_code_files": [
      "airflow/api_connexion/endpoints/dag_warning_endpoint.py||airflow/api_connexion/endpoints/dag_warning_endpoint.py",
      "tests/api_connexion/endpoints/test_dag_warning_endpoint.py||tests/api_connexion/endpoints/test_dag_warning_endpoint.py"
    ]
  },
  "patch_diff": {
    "airflow/api_connexion/endpoints/dag_warning_endpoint.py||airflow/api_connexion/endpoints/dag_warning_endpoint.py": [
      "File: airflow/api_connexion/endpoints/dag_warning_endpoint.py -> airflow/api_connexion/endpoints/dag_warning_endpoint.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "16: # under the License.",
      "17: from __future__ import annotations",
      "19: from sqlalchemy import select",
      "20: from sqlalchemy.orm import Session",
      "22: from airflow.api_connexion import security",
      "23: from airflow.api_connexion.parameters import apply_sorting, check_limit, format_parameters",
      "24: from airflow.api_connexion.schemas.dag_warning_schema import (",
      "25:     DagWarningCollection,",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "19: from flask import g",
      "24: from airflow.api_connexion.exceptions import PermissionDenied",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "28: from airflow.api_connexion.types import APIResponse",
      "29: from airflow.models.dagwarning import DagWarning as DagWarningModel",
      "30: from airflow.security import permissions",
      "31: from airflow.utils.db import get_query_count",
      "32: from airflow.utils.session import NEW_SESSION, provide_session",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "33: from airflow.utils.airflow_flask_app import get_airflow_app",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "52:     allowed_filter_attrs = [\"dag_id\", \"warning_type\", \"message\", \"timestamp\"]",
      "53:     query = select(DagWarningModel)",
      "54:     if dag_id:",
      "55:         query = query.where(DagWarningModel.dag_id == dag_id)",
      "56:     if warning_type:",
      "57:         query = query.where(DagWarningModel.warning_type == warning_type)",
      "58:     total_entries = get_query_count(query, session=session)",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "58:         if not get_airflow_app().appbuilder.sm.can_read_dag(dag_id, g.user):",
      "59:             raise PermissionDenied(detail=f\"User not allowed to access this DAG: {dag_id}\")",
      "61:     else:",
      "62:         readable_dags = get_airflow_app().appbuilder.sm.get_accessible_dag_ids(g.user)",
      "63:         query = query.where(DagWarningModel.dag_id.in_(readable_dags))",
      "",
      "---------------"
    ],
    "tests/api_connexion/endpoints/test_dag_warning_endpoint.py||tests/api_connexion/endpoints/test_dag_warning_endpoint.py": [
      "File: tests/api_connexion/endpoints/test_dag_warning_endpoint.py -> tests/api_connexion/endpoints/test_dag_warning_endpoint.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "35:         app,  # type:ignore",
      "36:         username=\"test\",",
      "37:         role_name=\"Test\",",
      "39:     )",
      "40:     create_user(app, username=\"test_no_permissions\", role_name=\"TestNoPermissions\")  # type: ignore",
      "42:     yield minimal_app_for_api",
      "44:     delete_user(app, username=\"test\")  # type: ignore",
      "45:     delete_user(app, username=\"test_no_permissions\")  # type: ignore",
      "48: class TestBaseDagWarning:",
      "",
      "[Removed Lines]",
      "38:         permissions=[(permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG_WARNING)],  # type: ignore",
      "",
      "[Added Lines]",
      "38:         permissions=[",
      "39:             (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG_WARNING),",
      "40:             (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG),",
      "41:         ],  # type: ignore",
      "44:     create_user(",
      "45:         app,  # type:ignore",
      "46:         username=\"test_with_dag2_read\",",
      "47:         role_name=\"TestWithDag2Read\",",
      "48:         permissions=[",
      "49:             (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG_WARNING),",
      "50:             (permissions.ACTION_CAN_READ, f\"{permissions.RESOURCE_DAG_PREFIX}dag2\"),",
      "51:         ],  # type: ignore",
      "52:     )",
      "58:     delete_user(app, username=\"test_with_dag2_read\")  # type: ignore",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "147:             \"/api/v1/dagWarnings\", environ_overrides={\"REMOTE_USER\": \"test_no_permissions\"}",
      "148:         )",
      "149:         assert response.status_code == 403",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "164:     def test_should_raise_403_forbidden_when_user_has_no_dag_read_permission(self):",
      "165:         response = self.client.get(",
      "166:             \"/api/v1/dagWarnings\",",
      "167:             environ_overrides={\"REMOTE_USER\": \"test_with_dag2_read\"},",
      "168:             query_string={\"dag_id\": \"dag1\"},",
      "169:         )",
      "170:         assert response.status_code == 403",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "05e5ffd351c4928b87241f14750a0763d28352aa",
      "candidate_info": {
        "commit_hash": "05e5ffd351c4928b87241f14750a0763d28352aa",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/05e5ffd351c4928b87241f14750a0763d28352aa",
        "files": [
          ".github/workflows/ci.yml",
          "Dockerfile.ci",
          "dev/breeze/src/airflow_breeze/commands/developer_commands.py",
          "dev/breeze/src/airflow_breeze/commands/developer_commands_config.py",
          "dev/breeze/src/airflow_breeze/commands/testing_commands.py",
          "dev/breeze/src/airflow_breeze/commands/testing_commands_config.py",
          "dev/breeze/src/airflow_breeze/params/shell_params.py",
          "dev/breeze/src/airflow_breeze/utils/common_options.py",
          "dev/breeze/src/airflow_breeze/utils/docker_command_utils.py",
          "images/breeze/output-commands-hash.txt",
          "images/breeze/output_shell.svg",
          "images/breeze/output_testing_tests.svg",
          "scripts/ci/docker-compose/_docker.env",
          "scripts/ci/docker-compose/base.yml",
          "scripts/ci/docker-compose/devcontainer.env",
          "scripts/docker/entrypoint_ci.sh",
          "setup.cfg"
        ],
        "message": "Update min-sqlalchemy version to account for latest features used (#34293)\n\nSome of the recent sqlalchemy changes are not working with minimum\nversion of sqlalchemy of ours - for example `where` syntax does\nnot allow moe than one clause and we are already passing more\nin _do_delete_old_records (added in #33527). This syntax however\nwas added in SQL Alchemy 1.4.28 and our minimum version was\n1.4.27.\n\nThis change bumps the minimum SQLAlchemy version to 1.4.28 but it also\nadds a special test job that only runs on Postgres that downgrades\nthe SQLAlchemy to the minimum supported version (retrieved from\nsetup.cfg). This way, we will be able to detect such incompatible\nchanges at the PR time. This is a new flag `--downgrade-sqlalchemy`\non test command that works similar to earlier `--upgrade-boto`.\n\nWe also enable the `--upgrade-boto` and `--downgrade-sqlalchemy` flags\nto be used for `breeze shell` command - thanks to that we can\neasily test both flags with `breeze shell` command.\n\n(cherry picked from commit efbead9fe7462b3634b6d9c842bd9a7ac78a0207)",
        "before_after_code_files": [
          "Dockerfile.ci||Dockerfile.ci",
          "dev/breeze/src/airflow_breeze/commands/developer_commands.py||dev/breeze/src/airflow_breeze/commands/developer_commands.py",
          "dev/breeze/src/airflow_breeze/commands/developer_commands_config.py||dev/breeze/src/airflow_breeze/commands/developer_commands_config.py",
          "dev/breeze/src/airflow_breeze/commands/testing_commands.py||dev/breeze/src/airflow_breeze/commands/testing_commands.py",
          "dev/breeze/src/airflow_breeze/commands/testing_commands_config.py||dev/breeze/src/airflow_breeze/commands/testing_commands_config.py",
          "dev/breeze/src/airflow_breeze/params/shell_params.py||dev/breeze/src/airflow_breeze/params/shell_params.py",
          "dev/breeze/src/airflow_breeze/utils/common_options.py||dev/breeze/src/airflow_breeze/utils/common_options.py",
          "dev/breeze/src/airflow_breeze/utils/docker_command_utils.py||dev/breeze/src/airflow_breeze/utils/docker_command_utils.py",
          "scripts/ci/docker-compose/_docker.env||scripts/ci/docker-compose/_docker.env",
          "scripts/ci/docker-compose/devcontainer.env||scripts/ci/docker-compose/devcontainer.env",
          "scripts/docker/entrypoint_ci.sh||scripts/docker/entrypoint_ci.sh",
          "setup.cfg||setup.cfg"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "Dockerfile.ci||Dockerfile.ci": [
          "File: Dockerfile.ci -> Dockerfile.ci",
          "--- Hunk 1 ---",
          "[Context before]",
          "937: fi",
          "939: rm -f \"${AIRFLOW_SOURCES}/pytest.ini\"",
          "941: set +u",
          "942: if [[ \"${RUN_TESTS}\" != \"true\" ]]; then",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "940: if [[ ${UPGRADE_BOTO=} == \"true\" ]]; then",
          "941:     echo",
          "942:     echo \"${COLOR_BLUE}Upgrading boto3, botocore to latest version to run Amazon tests with them${COLOR_RESET}\"",
          "943:     echo",
          "944:     pip uninstall --root-user-action ignore aiobotocore -y || true",
          "945:     pip install --root-user-action ignore --upgrade boto3 botocore",
          "946:     pip check",
          "947: fi",
          "948: if [[ ${DOWNGRADE_SQLALCHEMY=} == \"true\" ]]; then",
          "949:     min_sqlalchemy_version=$(grep \"sqlalchemy>=\" setup.cfg | sed \"s/.*>=\\([0-9\\.]*\\).*/\\1/\")",
          "950:     echo",
          "951:     echo \"${COLOR_BLUE}Downgrading sqlalchemy to minimum supported version: ${min_sqlalchemy_version}${COLOR_RESET}\"",
          "952:     echo",
          "953:     pip install --root-user-action ignore \"sqlalchemy==${min_sqlalchemy_version}\"",
          "954:     pip check",
          "955: fi",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1166:         exit 1",
          "1167:     fi",
          "1168: fi",
          "1176: readonly SELECTED_TESTS CLI_TESTS API_TESTS PROVIDERS_TESTS CORE_TESTS WWW_TESTS \\",
          "1177:     ALL_TESTS ALL_PRESELECTED_TESTS",
          "",
          "[Removed Lines]",
          "1169: if [[ ${UPGRADE_BOTO=} == \"true\" ]]; then",
          "1170:     echo",
          "1171:     echo \"${COLOR_BLUE}Upgrading boto3, botocore to latest version to run Amazon tests with them${COLOR_RESET}\"",
          "1172:     echo",
          "1173:     pip uninstall aiobotocore -y || true",
          "1174:     pip install --upgrade boto3 botocore",
          "1175: fi",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/commands/developer_commands.py||dev/breeze/src/airflow_breeze/commands/developer_commands.py": [
          "File: dev/breeze/src/airflow_breeze/commands/developer_commands.py -> dev/breeze/src/airflow_breeze/commands/developer_commands.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "51:     option_celery_broker,",
          "52:     option_celery_flower,",
          "53:     option_db_reset,",
          "54:     option_dry_run,",
          "55:     option_executor,",
          "56:     option_force_build,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "54:     option_downgrade_sqlalchemy,",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "70:     option_platform_single,",
          "71:     option_postgres_version,",
          "72:     option_python,",
          "73:     option_use_airflow_version,",
          "74:     option_use_packages_from_dist,",
          "75:     option_verbose,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "74:     option_upgrade_boto,",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "162: @option_image_tag_for_running",
          "163: @option_max_time",
          "164: @option_include_mypy_volume",
          "165: @option_verbose",
          "166: @option_dry_run",
          "167: @option_github_repository",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "167: @option_upgrade_boto",
          "168: @option_downgrade_sqlalchemy",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "197:     celery_broker: str,",
          "198:     celery_flower: bool,",
          "199:     extra_args: tuple,",
          "200: ):",
          "201:     \"\"\"Enter breeze environment. this is the default command use when no other is selected.\"\"\"",
          "202:     if get_verbose() or get_dry_run():",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "204:     upgrade_boto: bool,",
          "205:     downgrade_sqlalchemy: bool,",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "234:         executor=executor,",
          "235:         celery_broker=celery_broker,",
          "236:         celery_flower=celery_flower,",
          "237:     )",
          "238:     sys.exit(result.returncode)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "243:         upgrade_boto=upgrade_boto,",
          "244:         downgrade_sqlalchemy=downgrade_sqlalchemy,",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/commands/developer_commands_config.py||dev/breeze/src/airflow_breeze/commands/developer_commands_config.py": [
          "File: dev/breeze/src/airflow_breeze/commands/developer_commands_config.py -> dev/breeze/src/airflow_breeze/commands/developer_commands_config.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "99:                 \"--package-format\",",
          "100:             ],",
          "101:         },",
          "102:     ],",
          "103:     \"breeze compile-www-assets\": [",
          "104:         {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "102:         {",
          "103:             \"name\": \"Upgrading/downgrading selected packages\",",
          "104:             \"options\": [",
          "105:                 \"--upgrade-boto\",",
          "106:                 \"--downgrade-sqlalchemy\",",
          "107:             ],",
          "108:         },",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/commands/testing_commands.py||dev/breeze/src/airflow_breeze/commands/testing_commands.py": [
          "File: dev/breeze/src/airflow_breeze/commands/testing_commands.py -> dev/breeze/src/airflow_breeze/commands/testing_commands.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "38:     option_backend,",
          "39:     option_db_reset,",
          "40:     option_debug_resources,",
          "41:     option_dry_run,",
          "42:     option_github_repository,",
          "43:     option_image_name,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "41:     option_downgrade_sqlalchemy,",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "52:     option_python,",
          "53:     option_run_in_parallel,",
          "54:     option_skip_cleanup,",
          "55:     option_use_airflow_version,",
          "56:     option_verbose,",
          "57: )",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "56:     option_upgrade_boto,",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "367:     show_default=True,",
          "368:     envvar=\"PARALLEL_TEST_TYPES\",",
          "369: )",
          "376: @click.option(",
          "377:     \"--collect-only\",",
          "378:     help=\"Collect tests only, do not run them.\",",
          "",
          "[Removed Lines]",
          "370: @click.option(",
          "371:     \"--upgrade-boto\",",
          "372:     help=\"Remove aiobotocore and upgrade botocore and boto to the latest version.\",",
          "373:     is_flag=True,",
          "374:     envvar=\"UPGRADE_BOTO\",",
          "375: )",
          "",
          "[Added Lines]",
          "372: @option_upgrade_boto",
          "373: @option_downgrade_sqlalchemy",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "416:     mount_sources: str,",
          "417:     extra_pytest_args: tuple,",
          "418:     upgrade_boto: bool,",
          "419:     collect_only: bool,",
          "420:     remove_arm_packages: bool,",
          "421:     github_repository: str,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "417:     downgrade_sqlalchemy: bool,",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "436:         forward_ports=False,",
          "437:         test_type=test_type,",
          "438:         upgrade_boto=upgrade_boto,",
          "439:         collect_only=collect_only,",
          "440:         remove_arm_packages=remove_arm_packages,",
          "441:         github_repository=github_repository,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "438:         downgrade_sqlalchemy=downgrade_sqlalchemy,",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/commands/testing_commands_config.py||dev/breeze/src/airflow_breeze/commands/testing_commands_config.py": [
          "File: dev/breeze/src/airflow_breeze/commands/testing_commands_config.py -> dev/breeze/src/airflow_breeze/commands/testing_commands_config.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "56:                 \"--use-airflow-version\",",
          "57:                 \"--mount-sources\",",
          "58:                 \"--upgrade-boto\",",
          "59:                 \"--remove-arm-packages\",",
          "60:                 \"--skip-docker-compose-down\",",
          "61:             ],",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "59:                 \"--downgrade-sqlalchemy\",",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/params/shell_params.py||dev/breeze/src/airflow_breeze/params/shell_params.py": [
          "File: dev/breeze/src/airflow_breeze/params/shell_params.py -> dev/breeze/src/airflow_breeze/params/shell_params.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "120:     dry_run: bool = False",
          "121:     verbose: bool = False",
          "122:     upgrade_boto: bool = False",
          "123:     executor: str = START_AIRFLOW_DEFAULT_ALLOWED_EXECUTORS",
          "124:     celery_broker: str = DEFAULT_CELERY_BROKER",
          "125:     celery_flower: bool = False",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "123:     downgrade_sqlalchemy: bool = False",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/utils/common_options.py||dev/breeze/src/airflow_breeze/utils/common_options.py": [
          "File: dev/breeze/src/airflow_breeze/utils/common_options.py -> dev/breeze/src/airflow_breeze/utils/common_options.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "601:     help=\"Optional additional requirements to upgrade eagerly to avoid backtracking \"",
          "602:     \"(see `breeze ci find-backtracking-candidates`).\",",
          "603: )",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "604: option_upgrade_boto = click.option(",
          "605:     \"--upgrade-boto\",",
          "606:     help=\"Remove aiobotocore and upgrade botocore and boto to the latest version.\",",
          "607:     is_flag=True,",
          "608:     envvar=\"UPGRADE_BOTO\",",
          "609: )",
          "610: option_downgrade_sqlalchemy = click.option(",
          "611:     \"--downgrade-sqlalchemy\",",
          "612:     help=\"Downgrade SQLAlchemy to minimum supported version.\",",
          "613:     is_flag=True,",
          "614:     envvar=\"DOWNGRADE_SQLALCHEMY\",",
          "615: )",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/utils/docker_command_utils.py||dev/breeze/src/airflow_breeze/utils/docker_command_utils.py": [
          "File: dev/breeze/src/airflow_breeze/utils/docker_command_utils.py -> dev/breeze/src/airflow_breeze/utils/docker_command_utils.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "597:     set_value_to_default_if_not_set(env, \"TEST_TYPE\", \"\")",
          "598:     set_value_to_default_if_not_set(env, \"TEST_TIMEOUT\", \"60\")",
          "599:     set_value_to_default_if_not_set(env, \"UPGRADE_BOTO\", \"false\")",
          "600:     set_value_to_default_if_not_set(env, \"UPGRADE_TO_NEWER_DEPENDENCIES\", \"false\")",
          "601:     set_value_to_default_if_not_set(env, \"USE_PACKAGES_FROM_DIST\", \"false\")",
          "602:     set_value_to_default_if_not_set(env, \"VERBOSE\", \"false\")",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "600:     set_value_to_default_if_not_set(env, \"DOWNGRADE_SQLALCHEMY\", \"false\")",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "644:     \"SQLITE_URL\": \"sqlite_url\",",
          "645:     \"START_AIRFLOW\": \"start_airflow\",",
          "646:     \"UPGRADE_BOTO\": \"upgrade_boto\",",
          "647:     \"USE_AIRFLOW_VERSION\": \"use_airflow_version\",",
          "648:     \"USE_PACKAGES_FROM_DIST\": \"use_packages_from_dist\",",
          "649:     \"VERSION_SUFFIX_FOR_PYPI\": \"version_suffix_for_pypi\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "648:     \"DOWNGRADE_SQLALCHEMY\": \"downgrade_sqlalchemy\",",
          "",
          "---------------"
        ],
        "scripts/ci/docker-compose/_docker.env||scripts/ci/docker-compose/_docker.env": [
          "File: scripts/ci/docker-compose/_docker.env -> scripts/ci/docker-compose/_docker.env",
          "--- Hunk 1 ---",
          "[Context before]",
          "75: TEST_TYPE",
          "76: UPGRADE_BOTO",
          "77: UPGRADE_TO_NEWER_DEPENDENCIES",
          "78: VERBOSE",
          "79: VERBOSE_COMMANDS",
          "80: VERSION_SUFFIX_FOR_PYPI",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "78: DOWNGRADE_SQLALCHEMY",
          "",
          "---------------"
        ],
        "scripts/ci/docker-compose/devcontainer.env||scripts/ci/docker-compose/devcontainer.env": [
          "File: scripts/ci/docker-compose/devcontainer.env -> scripts/ci/docker-compose/devcontainer.env",
          "--- Hunk 1 ---",
          "[Context before]",
          "69: SUSPENDED_PROVIDERS_FOLDERS=\"\"",
          "70: TEST_TYPE=",
          "71: UPGRADE_BOTO=\"false\"",
          "72: UPGRADE_TO_NEWER_DEPENDENCIES=\"false\"",
          "73: VERBOSE=\"false\"",
          "74: VERBOSE_COMMANDS=\"false\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "72: DOWNGRADE_SQLALCHEMY=\"false\"",
          "",
          "---------------"
        ],
        "scripts/docker/entrypoint_ci.sh||scripts/docker/entrypoint_ci.sh": [
          "File: scripts/docker/entrypoint_ci.sh -> scripts/docker/entrypoint_ci.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "326: # Remove pytest.ini from the current directory if it exists. It has been removed from the source tree",
          "327: # but may still be present in the local directory if the user has old breeze image",
          "328: rm -f \"${AIRFLOW_SOURCES}/pytest.ini\"",
          "330: set +u",
          "331: # If we do not want to run tests, we simply drop into bash",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "329: if [[ ${UPGRADE_BOTO=} == \"true\" ]]; then",
          "330:     echo",
          "331:     echo \"${COLOR_BLUE}Upgrading boto3, botocore to latest version to run Amazon tests with them${COLOR_RESET}\"",
          "332:     echo",
          "333:     pip uninstall --root-user-action ignore aiobotocore -y || true",
          "334:     pip install --root-user-action ignore --upgrade boto3 botocore",
          "335:     pip check",
          "336: fi",
          "337: if [[ ${DOWNGRADE_SQLALCHEMY=} == \"true\" ]]; then",
          "338:     min_sqlalchemy_version=$(grep \"sqlalchemy>=\" setup.cfg | sed \"s/.*>=\\([0-9\\.]*\\).*/\\1/\")",
          "339:     echo",
          "340:     echo \"${COLOR_BLUE}Downgrading sqlalchemy to minimum supported version: ${min_sqlalchemy_version}${COLOR_RESET}\"",
          "341:     echo",
          "342:     pip install --root-user-action ignore \"sqlalchemy==${min_sqlalchemy_version}\"",
          "343:     pip check",
          "344: fi",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "558:         exit 1",
          "559:     fi",
          "560: fi",
          "568: readonly SELECTED_TESTS CLI_TESTS API_TESTS PROVIDERS_TESTS CORE_TESTS WWW_TESTS \\",
          "569:     ALL_TESTS ALL_PRESELECTED_TESTS",
          "",
          "[Removed Lines]",
          "561: if [[ ${UPGRADE_BOTO=} == \"true\" ]]; then",
          "562:     echo",
          "563:     echo \"${COLOR_BLUE}Upgrading boto3, botocore to latest version to run Amazon tests with them${COLOR_RESET}\"",
          "564:     echo",
          "565:     pip uninstall aiobotocore -y || true",
          "566:     pip install --upgrade boto3 botocore",
          "567: fi",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "setup.cfg||setup.cfg": [
          "File: setup.cfg -> setup.cfg",
          "--- Hunk 1 ---",
          "[Context before]",
          "142:     # See https://sqlalche.me/e/b8d9 for details of deprecated features",
          "143:     # you can set environment variable SQLALCHEMY_WARN_20=1 to show all deprecation warnings.",
          "144:     # The issue tracking it is https://github.com/apache/airflow/issues/28723",
          "146:     sqlalchemy_jsonfield>=1.0",
          "147:     tabulate>=0.7.5",
          "148:     tenacity>=6.2.0,!=8.2.0",
          "",
          "[Removed Lines]",
          "145:     sqlalchemy>=1.4.24,<2.0",
          "",
          "[Added Lines]",
          "145:     sqlalchemy>=1.4.28,<2.0",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "b9b7a40168413cd3f4b5c09e2006ad9eab0d8282",
      "candidate_info": {
        "commit_hash": "b9b7a40168413cd3f4b5c09e2006ad9eab0d8282",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/b9b7a40168413cd3f4b5c09e2006ad9eab0d8282",
        "files": [
          "airflow/api_connexion/endpoints/dag_source_endpoint.py",
          "airflow/api_connexion/schemas/dag_schema.py",
          "airflow/cli/cli_config.py",
          "airflow/cli/commands/connection_command.py",
          "airflow/dag_processing/processor.py",
          "airflow/operators/python.py",
          "airflow/serialization/serialized_objects.py",
          "airflow/utils/db_cleanup.py",
          "airflow/utils/log/secrets_masker.py",
          "airflow/www/extensions/init_manifest_files.py"
        ],
        "message": "Use literal dict instead of calling dict() in Airflow core (#33762)\n\n(cherry picked from commit 1e81ed19997114cc7dc136e9fd64676a7710715a)",
        "before_after_code_files": [
          "airflow/api_connexion/endpoints/dag_source_endpoint.py||airflow/api_connexion/endpoints/dag_source_endpoint.py",
          "airflow/api_connexion/schemas/dag_schema.py||airflow/api_connexion/schemas/dag_schema.py",
          "airflow/cli/cli_config.py||airflow/cli/cli_config.py",
          "airflow/cli/commands/connection_command.py||airflow/cli/commands/connection_command.py",
          "airflow/dag_processing/processor.py||airflow/dag_processing/processor.py",
          "airflow/operators/python.py||airflow/operators/python.py",
          "airflow/serialization/serialized_objects.py||airflow/serialization/serialized_objects.py",
          "airflow/utils/db_cleanup.py||airflow/utils/db_cleanup.py",
          "airflow/utils/log/secrets_masker.py||airflow/utils/log/secrets_masker.py",
          "airflow/www/extensions/init_manifest_files.py||airflow/www/extensions/init_manifest_files.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/api_connexion/endpoints/dag_source_endpoint.py||airflow/api_connexion/endpoints/dag_source_endpoint.py": [
          "File: airflow/api_connexion/endpoints/dag_source_endpoint.py -> airflow/api_connexion/endpoints/dag_source_endpoint.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "43:     if return_type == \"text/plain\":",
          "44:         return Response(dag_source, headers={\"Content-Type\": return_type})",
          "45:     if return_type == \"application/json\":",
          "47:         return Response(content, headers={\"Content-Type\": return_type})",
          "48:     return Response(\"Not Allowed Accept Header\", status=HTTPStatus.NOT_ACCEPTABLE)",
          "",
          "[Removed Lines]",
          "46:         content = dag_source_schema.dumps(dict(content=dag_source))",
          "",
          "[Added Lines]",
          "46:         content = dag_source_schema.dumps({\"content\": dag_source})",
          "",
          "---------------"
        ],
        "airflow/api_connexion/schemas/dag_schema.py||airflow/api_connexion/schemas/dag_schema.py": [
          "File: airflow/api_connexion/schemas/dag_schema.py -> airflow/api_connexion/schemas/dag_schema.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "120:         \"\"\"Dump tags as objects.\"\"\"",
          "121:         tags = obj.tags",
          "122:         if tags:",
          "124:         return []",
          "126:     @staticmethod",
          "",
          "[Removed Lines]",
          "123:             return [DagTagSchema().dump(dict(name=tag)) for tag in tags]",
          "",
          "[Added Lines]",
          "123:             return [DagTagSchema().dump({\"name\": tag}) for tag in tags]",
          "",
          "---------------"
        ],
        "airflow/cli/cli_config.py||airflow/cli/cli_config.py": [
          "File: airflow/cli/cli_config.py -> airflow/cli/cli_config.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "2209:         name=\"standalone\",",
          "2210:         help=\"Run an all-in-one copy of Airflow\",",
          "2211:         func=lazy_load_command(\"airflow.cli.commands.standalone_command.standalone\"),",
          "2213:     ),",
          "2214: ]",
          "",
          "[Removed Lines]",
          "2212:         args=tuple(),",
          "",
          "[Added Lines]",
          "2212:         args=(),",
          "",
          "---------------"
        ],
        "airflow/cli/commands/connection_command.py||airflow/cli/commands/connection_command.py": [
          "File: airflow/cli/commands/connection_command.py -> airflow/cli/commands/connection_command.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "98: def _connection_to_dict(conn: Connection) -> dict:",
          "111: def create_default_connections(args):",
          "",
          "[Removed Lines]",
          "99:     return dict(",
          "100:         conn_type=conn.conn_type,",
          "101:         description=conn.description,",
          "102:         login=conn.login,",
          "103:         password=conn.password,",
          "104:         host=conn.host,",
          "105:         port=conn.port,",
          "106:         schema=conn.schema,",
          "107:         extra=conn.extra,",
          "108:     )",
          "",
          "[Added Lines]",
          "99:     return {",
          "100:         \"conn_type\": conn.conn_type,",
          "101:         \"description\": conn.description,",
          "102:         \"login\": conn.login,",
          "103:         \"password\": conn.password,",
          "104:         \"host\": conn.host,",
          "105:         \"port\": conn.port,",
          "106:         \"schema\": conn.schema,",
          "107:         \"extra\": conn.extra,",
          "108:     }",
          "",
          "---------------"
        ],
        "airflow/dag_processing/processor.py||airflow/dag_processing/processor.py": [
          "File: airflow/dag_processing/processor.py -> airflow/dag_processing/processor.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "628:         for filename, stacktrace in import_errors.items():",
          "629:             if filename in existing_import_error_files:",
          "630:                 session.query(errors.ImportError).filter(errors.ImportError.filename == filename).update(",
          "632:                     synchronize_session=\"fetch\",",
          "633:                 )",
          "634:             else:",
          "",
          "[Removed Lines]",
          "631:                     dict(filename=filename, timestamp=timezone.utcnow(), stacktrace=stacktrace),",
          "",
          "[Added Lines]",
          "631:                     {\"filename\": filename, \"timestamp\": timezone.utcnow(), \"stacktrace\": stacktrace},",
          "",
          "---------------"
        ],
        "airflow/operators/python.py||airflow/operators/python.py": [
          "File: airflow/operators/python.py -> airflow/operators/python.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "430:         self._write_args(input_path)",
          "431:         self._write_string_args(string_args_path)",
          "432:         write_python_script(",
          "441:             filename=os.fspath(script_path),",
          "442:             render_template_as_native_obj=self.dag.render_template_as_native_obj,",
          "443:         )",
          "",
          "[Removed Lines]",
          "433:             jinja_context=dict(",
          "434:                 op_args=self.op_args,",
          "435:                 op_kwargs=op_kwargs,",
          "436:                 expect_airflow=self.expect_airflow,",
          "437:                 pickling_library=self.pickling_library.__name__,",
          "438:                 python_callable=self.python_callable.__name__,",
          "439:                 python_callable_source=self.get_python_source(),",
          "440:             ),",
          "",
          "[Added Lines]",
          "433:             jinja_context={",
          "434:                 \"op_args\": self.op_args,",
          "435:                 \"op_kwargs\": op_kwargs,",
          "436:                 \"expect_airflow\": self.expect_airflow,",
          "437:                 \"pickling_library\": self.pickling_library.__name__,",
          "438:                 \"python_callable\": self.python_callable.__name__,",
          "439:                 \"python_callable_source\": self.get_python_source(),",
          "440:             },",
          "",
          "---------------"
        ],
        "airflow/serialization/serialized_objects.py||airflow/serialization/serialized_objects.py": [
          "File: airflow/serialization/serialized_objects.py -> airflow/serialization/serialized_objects.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "476:         elif isinstance(var, XComArg):",
          "477:             return cls._encode(serialize_xcom_arg(var), type_=DAT.XCOM_REF)",
          "478:         elif isinstance(var, Dataset):",
          "480:         elif isinstance(var, SimpleTaskInstance):",
          "481:             return cls._encode(",
          "482:                 cls.serialize(var.__dict__, strict=strict, use_pydantic_models=use_pydantic_models),",
          "",
          "[Removed Lines]",
          "479:             return cls._encode(dict(uri=var.uri, extra=var.extra), type_=DAT.DATASET)",
          "",
          "[Added Lines]",
          "479:             return cls._encode({\"uri\": var.uri, \"extra\": var.extra}, type_=DAT.DATASET)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "617:     @classmethod",
          "618:     def _serialize_param(cls, param: Param):",
          "626:     @classmethod",
          "627:     def _deserialize_param(cls, param_dict: dict):",
          "",
          "[Removed Lines]",
          "619:         return dict(",
          "620:             __class=f\"{param.__module__}.{param.__class__.__name__}\",",
          "621:             default=cls.serialize(param.value),",
          "622:             description=cls.serialize(param.description),",
          "623:             schema=cls.serialize(param.schema),",
          "624:         )",
          "",
          "[Added Lines]",
          "619:         return {",
          "620:             \"__class\": f\"{param.__module__}.{param.__class__.__name__}\",",
          "621:             \"default\": cls.serialize(param.value),",
          "622:             \"description\": cls.serialize(param.description),",
          "623:             \"schema\": cls.serialize(param.schema),",
          "624:         }",
          "",
          "---------------"
        ],
        "airflow/utils/db_cleanup.py||airflow/utils/db_cleanup.py": [
          "File: airflow/utils/db_cleanup.py -> airflow/utils/db_cleanup.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "84:     @property",
          "85:     def readable_config(self):",
          "95: config_list: list[_TableConfig] = [",
          "",
          "[Removed Lines]",
          "86:         return dict(",
          "87:             table=self.orm_model.name,",
          "88:             recency_column=str(self.recency_column),",
          "89:             keep_last=self.keep_last,",
          "90:             keep_last_filters=[str(x) for x in self.keep_last_filters] if self.keep_last_filters else None,",
          "91:             keep_last_group_by=str(self.keep_last_group_by),",
          "92:         )",
          "",
          "[Added Lines]",
          "86:         return {",
          "87:             \"table\": self.orm_model.name,",
          "88:             \"recency_column\": str(self.recency_column),",
          "89:             \"keep_last\": self.keep_last,",
          "90:             \"keep_last_filters\": [str(x) for x in self.keep_last_filters] if self.keep_last_filters else None,",
          "91:             \"keep_last_group_by\": str(self.keep_last_group_by),",
          "92:         }",
          "",
          "---------------"
        ],
        "airflow/utils/log/secrets_masker.py||airflow/utils/log/secrets_masker.py": [
          "File: airflow/utils/log/secrets_masker.py -> airflow/utils/log/secrets_masker.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "176:             __file__,",
          "177:             1,",
          "178:             \"\",",
          "180:             exc_info=None,",
          "181:             func=\"funcname\",",
          "182:         )",
          "",
          "[Removed Lines]",
          "179:             tuple(),",
          "",
          "[Added Lines]",
          "179:             (),",
          "",
          "---------------"
        ],
        "airflow/www/extensions/init_manifest_files.py||airflow/www/extensions/init_manifest_files.py": [
          "File: airflow/www/extensions/init_manifest_files.py -> airflow/www/extensions/init_manifest_files.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "56:         static/dist folder. This template tag reads the asset name in",
          "57:         ``manifest.json`` and returns the appropriate file.",
          "58:         \"\"\"",
          "",
          "[Removed Lines]",
          "59:         return dict(url_for_asset=get_asset_url)",
          "",
          "[Added Lines]",
          "59:         return {\"url_for_asset\": get_asset_url}",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "de5ec9b58bee9b7c9478ecce3153935815c43563",
      "candidate_info": {
        "commit_hash": "de5ec9b58bee9b7c9478ecce3153935815c43563",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/de5ec9b58bee9b7c9478ecce3153935815c43563",
        "files": [
          "airflow/www/api/experimental/endpoints.py",
          "airflow/www/extensions/init_appbuilder.py",
          "airflow/www/extensions/init_views.py",
          "airflow/www/extensions/init_wsgi_middlewares.py",
          "airflow/www/fab_security/manager.py",
          "airflow/www/security.py",
          "airflow/www/utils.py",
          "airflow/www/views.py"
        ],
        "message": "Improve importing the modules in Airflow www package (#33810)\n\n(cherry picked from commit b470c6bdcc801bcd57c4008a823bd768405d1736)",
        "before_after_code_files": [
          "airflow/www/api/experimental/endpoints.py||airflow/www/api/experimental/endpoints.py",
          "airflow/www/extensions/init_appbuilder.py||airflow/www/extensions/init_appbuilder.py",
          "airflow/www/extensions/init_views.py||airflow/www/extensions/init_views.py",
          "airflow/www/extensions/init_wsgi_middlewares.py||airflow/www/extensions/init_wsgi_middlewares.py",
          "airflow/www/fab_security/manager.py||airflow/www/fab_security/manager.py",
          "airflow/www/security.py||airflow/www/security.py",
          "airflow/www/utils.py||airflow/www/utils.py",
          "airflow/www/views.py||airflow/www/views.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/www/api/experimental/endpoints.py||airflow/www/api/experimental/endpoints.py": [
          "File: airflow/www/api/experimental/endpoints.py -> airflow/www/api/experimental/endpoints.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: import logging",
          "21: from functools import wraps",
          "26: from airflow import models",
          "27: from airflow.api.common.experimental import delete_dag as delete, pool as pool_api, trigger_dag as trigger",
          "",
          "[Removed Lines]",
          "22: from typing import Callable, TypeVar, cast",
          "24: from flask import Blueprint, Response, current_app, g, jsonify, request, url_for",
          "",
          "[Added Lines]",
          "22: from typing import TYPE_CHECKING, Callable, TypeVar, cast",
          "24: from flask import Blueprint, current_app, g, jsonify, request, url_for",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "37: from airflow.utils.strings import to_boolean",
          "38: from airflow.version import version",
          "40: log = logging.getLogger(__name__)",
          "42: T = TypeVar(\"T\", bound=Callable)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "40: if TYPE_CHECKING:",
          "41:     from flask import Response",
          "",
          "---------------"
        ],
        "airflow/www/extensions/init_appbuilder.py||airflow/www/extensions/init_appbuilder.py": [
          "File: airflow/www/extensions/init_appbuilder.py -> airflow/www/extensions/init_appbuilder.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "21: import logging",
          "22: from functools import reduce",
          "24: from flask import Blueprint, current_app, url_for",
          "26: from flask_appbuilder.babel.manager import BabelManager",
          "27: from flask_appbuilder.const import (",
          "28:     LOGMSG_ERR_FAB_ADD_PERMISSION_MENU,",
          "",
          "[Removed Lines]",
          "25: from flask_appbuilder import BaseView, __version__",
          "",
          "[Added Lines]",
          "23: from typing import TYPE_CHECKING",
          "26: from flask_appbuilder import __version__",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "35: )",
          "36: from flask_appbuilder.filters import TemplateFilters",
          "37: from flask_appbuilder.menu import Menu",
          "39: from flask_appbuilder.views import IndexView, UtilView",
          "42: from airflow import settings",
          "43: from airflow.configuration import conf",
          "44: from airflow.www.extensions.init_auth_manager import get_auth_manager",
          "46: # This product contains a modified portion of 'Flask App Builder' developed by Daniel Vaz Gaspar.",
          "47: # (https://github.com/dpgaspar/Flask-AppBuilder).",
          "48: # Copyright 2013, Daniel Vaz Gaspar",
          "",
          "[Removed Lines]",
          "38: from flask_appbuilder.security.manager import BaseSecurityManager",
          "40: from sqlalchemy.orm import Session",
          "",
          "[Added Lines]",
          "45: if TYPE_CHECKING:",
          "46:     from flask_appbuilder import BaseView",
          "47:     from flask_appbuilder.security.manager import BaseSecurityManager",
          "48:     from sqlalchemy.orm import Session",
          "",
          "---------------"
        ],
        "airflow/www/extensions/init_views.py||airflow/www/extensions/init_views.py": [
          "File: airflow/www/extensions/init_views.py -> airflow/www/extensions/init_views.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: import warnings",
          "21: from functools import cached_property",
          "22: from os import path",
          "24: from connexion import FlaskApi, ProblemException, Resolver",
          "25: from connexion.decorators.validation import RequestBodyValidator",
          "26: from connexion.exceptions import BadRequestProblem",
          "29: from airflow.api_connexion.exceptions import common_error_handler",
          "30: from airflow.configuration import conf",
          "",
          "[Removed Lines]",
          "27: from flask import Flask, request",
          "",
          "[Added Lines]",
          "23: from typing import TYPE_CHECKING",
          "28: from flask import request",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "32: from airflow.security import permissions",
          "33: from airflow.utils.yaml import safe_load",
          "35: log = logging.getLogger(__name__)",
          "37: # airflow/www/extensions/init_views.py => airflow/",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "36: if TYPE_CHECKING:",
          "37:     from flask import Flask",
          "",
          "---------------"
        ],
        "airflow/www/extensions/init_wsgi_middlewares.py||airflow/www/extensions/init_wsgi_middlewares.py": [
          "File: airflow/www/extensions/init_wsgi_middlewares.py -> airflow/www/extensions/init_wsgi_middlewares.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: from typing import TYPE_CHECKING, Iterable",
          "21: from urllib.parse import urlsplit",
          "24: from werkzeug.middleware.dispatcher import DispatcherMiddleware",
          "25: from werkzeug.middleware.proxy_fix import ProxyFix",
          "",
          "[Removed Lines]",
          "23: from flask import Flask",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "30: if TYPE_CHECKING:",
          "31:     from _typeshed.wsgi import StartResponse, WSGIEnvironment",
          "34: def _root_app(env: WSGIEnvironment, resp: StartResponse) -> Iterable[bytes]:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "31:     from flask import Flask",
          "",
          "---------------"
        ],
        "airflow/www/fab_security/manager.py||airflow/www/fab_security/manager.py": [
          "File: airflow/www/fab_security/manager.py -> airflow/www/fab_security/manager.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "22: import datetime",
          "23: import json",
          "24: import logging",
          "26: from uuid import uuid4",
          "28: import re2",
          "31: from flask_appbuilder.const import (",
          "32:     AUTH_DB,",
          "33:     AUTH_LDAP,",
          "",
          "[Removed Lines]",
          "25: from typing import Any",
          "29: from flask import Flask, g, session, url_for",
          "30: from flask_appbuilder import AppBuilder",
          "",
          "[Added Lines]",
          "25: from typing import TYPE_CHECKING, Any",
          "29: from flask import g, session, url_for",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "69: from flask_limiter.util import get_remote_address",
          "70: from werkzeug.security import check_password_hash",
          "73: from airflow.configuration import conf",
          "74: from airflow.www.extensions.init_auth_manager import get_auth_manager",
          "76: # This product contains a modified portion of 'Flask App Builder' developed by Daniel Vaz Gaspar.",
          "77: # (https://github.com/dpgaspar/Flask-AppBuilder).",
          "78: # Copyright 2013, Daniel Vaz Gaspar",
          "",
          "[Removed Lines]",
          "72: from airflow.auth.managers.fab.models import Action, Permission, RegisterUser, Resource, Role, User",
          "",
          "[Added Lines]",
          "74: if TYPE_CHECKING:",
          "75:     from flask import Flask",
          "76:     from flask_appbuilder import AppBuilder",
          "78:     from airflow.auth.managers.fab.models import Action, Permission, RegisterUser, Resource, Role, User",
          "",
          "---------------"
        ],
        "airflow/www/security.py||airflow/www/security.py": [
          "File: airflow/www/security.py -> airflow/www/security.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "22: from flask import g",
          "23: from sqlalchemy import or_",
          "26: from airflow.auth.managers.fab.models import Permission, Resource, Role, User",
          "27: from airflow.auth.managers.fab.views.user_details import CustomUserDBModelView",
          "",
          "[Removed Lines]",
          "24: from sqlalchemy.orm import Session, joinedload",
          "",
          "[Added Lines]",
          "24: from sqlalchemy.orm import joinedload",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "57: }",
          "59: if TYPE_CHECKING:",
          "60:     SecurityManagerOverride: type = object",
          "61: else:",
          "62:     # Fetch the security manager override from the auth manager",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "60:     from sqlalchemy.orm import Session",
          "",
          "---------------"
        ],
        "airflow/www/utils.py||airflow/www/utils.py": [
          "File: airflow/www/utils.py -> airflow/www/utils.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "27: from flask.helpers import flash",
          "28: from flask_appbuilder.forms import FieldConverter",
          "29: from flask_appbuilder.models.filters import BaseFilter",
          "31: from flask_appbuilder.models.sqla.filters import get_field_setup_query, set_value_to_type",
          "32: from flask_appbuilder.models.sqla.interface import SQLAInterface",
          "33: from flask_babel import lazy_gettext",
          "34: from markdown_it import MarkdownIt",
          "35: from markupsafe import Markup",
          "37: from pygments import highlight, lexers",
          "38: from pygments.formatters import HtmlFormatter",
          "40: from sqlalchemy import delete, func, select, types",
          "41: from sqlalchemy.ext.associationproxy import AssociationProxy",
          "44: from airflow.exceptions import RemovedInAirflow3Warning",
          "45: from airflow.models import errors",
          "",
          "[Removed Lines]",
          "30: from flask_appbuilder.models.sqla import Model, filters as fab_sqlafilters",
          "36: from pendulum.datetime import DateTime",
          "39: from pygments.lexer import Lexer",
          "42: from sqlalchemy.sql import Select",
          "",
          "[Added Lines]",
          "30: from flask_appbuilder.models.sqla import filters as fab_sqlafilters",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "56: from airflow.www.widgets import AirflowDateTimePickerWidget",
          "58: if TYPE_CHECKING:",
          "59:     from sqlalchemy.orm.session import Session",
          "60:     from sqlalchemy.sql.operators import ColumnOperators",
          "62:     from airflow.www.fab_security.sqla.manager import SecurityManager",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "56:     from flask_appbuilder.models.sqla import Model",
          "57:     from pendulum.datetime import DateTime",
          "58:     from pygments.lexer import Lexer",
          "60:     from sqlalchemy.sql import Select",
          "",
          "---------------"
        ],
        "airflow/www/views.py||airflow/www/views.py": [
          "File: airflow/www/views.py -> airflow/www/views.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "31: from collections import defaultdict",
          "32: from functools import cached_property, wraps",
          "33: from json import JSONDecodeError",
          "35: from urllib.parse import unquote, urljoin, urlsplit",
          "37: import configupdater",
          "",
          "[Removed Lines]",
          "34: from typing import Any, Callable, Collection, Iterator, Mapping, MutableMapping, Sequence",
          "",
          "[Added Lines]",
          "34: from typing import TYPE_CHECKING, Any, Callable, Collection, Iterator, Mapping, MutableMapping, Sequence",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "69: from pendulum.parsing.exceptions import ParserError",
          "70: from sqlalchemy import Date, and_, case, desc, func, inspect, select, union_all",
          "71: from sqlalchemy.exc import IntegrityError",
          "73: from wtforms import BooleanField, validators",
          "75: import airflow",
          "",
          "[Removed Lines]",
          "72: from sqlalchemy.orm import Session, joinedload",
          "",
          "[Added Lines]",
          "72: from sqlalchemy.orm import joinedload",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "96: from airflow.jobs.scheduler_job_runner import SchedulerJobRunner",
          "97: from airflow.jobs.triggerer_job_runner import TriggererJobRunner",
          "98: from airflow.models import Connection, DagModel, DagTag, Log, SlaMiss, TaskFail, Trigger, XCom, errors",
          "101: from airflow.models.dagrun import RUN_ID_REGEX, DagRun, DagRunType",
          "102: from airflow.models.dataset import DagScheduleDatasetReference, DatasetDagRunQueue, DatasetEvent, DatasetModel",
          "103: from airflow.models.mappedoperator import MappedOperator",
          "105: from airflow.models.serialized_dag import SerializedDagModel",
          "106: from airflow.models.taskinstance import TaskInstance, TaskInstanceNote",
          "107: from airflow.providers_manager import ProvidersManager",
          "",
          "[Removed Lines]",
          "99: from airflow.models.abstractoperator import AbstractOperator",
          "100: from airflow.models.dag import DAG, get_dataset_triggered_next_run_info",
          "104: from airflow.models.operator import Operator",
          "",
          "[Added Lines]",
          "99: from airflow.models.dag import get_dataset_triggered_next_run_info",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "138: )",
          "139: from airflow.www.widgets import AirflowModelListWidget, AirflowVariableShowWidget",
          "141: PAGE_SIZE = conf.getint(\"webserver\", \"page_size\")",
          "142: FILTER_TAGS_COOKIE = \"tags_filter\"",
          "143: FILTER_STATUS_COOKIE = \"dag_status_filter\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "139: if TYPE_CHECKING:",
          "140:     from sqlalchemy.orm import Session",
          "142:     from airflow.models.abstractoperator import AbstractOperator",
          "143:     from airflow.models.dag import DAG",
          "144:     from airflow.models.operator import Operator",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "836:             is_paused_count = dict(",
          "837:                 session.execute(",
          "841:                 ).all()",
          "842:             )",
          "",
          "[Removed Lines]",
          "838:                     all_dags.with_only_columns([DagModel.is_paused, func.count()]).group_by(",
          "839:                         DagModel.is_paused",
          "840:                     )",
          "",
          "[Added Lines]",
          "843:                     select(DagModel.is_paused, func.count(DagModel.dag_id)).group_by(DagModel.is_paused)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "3188f511bd92a530435ad05b8d3d0346a96a1dc3",
      "candidate_info": {
        "commit_hash": "3188f511bd92a530435ad05b8d3d0346a96a1dc3",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/3188f511bd92a530435ad05b8d3d0346a96a1dc3",
        "files": [
          "airflow/kubernetes/pre_7_4_0_compatibility/pod_generator.py",
          "scripts/ci/testing/summarize_junit_failures.py",
          "scripts/in_container/run_migration_reference.py",
          "scripts/in_container/verify_providers.py",
          "tests/providers/cncf/kubernetes/test_pod_generator.py"
        ],
        "message": "Refactor: Simplify string generation (#34118)\n\n(cherry picked from commit a1fe77bc820a0dccf170fc123ec7e7280747e145)",
        "before_after_code_files": [
          "airflow/kubernetes/pre_7_4_0_compatibility/pod_generator.py||airflow/kubernetes/pre_7_4_0_compatibility/pod_generator.py",
          "scripts/ci/testing/summarize_junit_failures.py||scripts/ci/testing/summarize_junit_failures.py",
          "scripts/in_container/run_migration_reference.py||scripts/in_container/run_migration_reference.py",
          "scripts/in_container/verify_providers.py||scripts/in_container/verify_providers.py",
          "tests/providers/cncf/kubernetes/test_pod_generator.py||tests/providers/cncf/kubernetes/test_pod_generator.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/kubernetes/pre_7_4_0_compatibility/pod_generator.py||airflow/kubernetes/pre_7_4_0_compatibility/pod_generator.py": [
          "File: airflow/kubernetes/pre_7_4_0_compatibility/pod_generator.py -> airflow/kubernetes/pre_7_4_0_compatibility/pod_generator.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "492:             airflow_worker=airflow_worker,",
          "493:         )",
          "494:         label_strings = [f\"{label_id}={label}\" for label_id, label in sorted(labels.items())]",
          "496:         if not airflow_worker:  # this filters out KPO pods even when we don't know the scheduler job id",
          "498:         return selector",
          "500:     @classmethod",
          "",
          "[Removed Lines]",
          "495:         selector = \",\".join(label_strings)",
          "497:             selector += \",airflow-worker\"",
          "",
          "[Added Lines]",
          "496:             label_strings.append(\"airflow-worker\")",
          "497:         selector = \",\".join(label_strings)",
          "",
          "---------------"
        ],
        "scripts/ci/testing/summarize_junit_failures.py||scripts/ci/testing/summarize_junit_failures.py": [
          "File: scripts/ci/testing/summarize_junit_failures.py -> scripts/ci/testing/summarize_junit_failures.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "84:     testsuite = root.find(\".//testsuite\")",
          "103:         return",
          "106:     for testcase in testsuite.findall(\".//testcase[error]\"):",
          "107:         case_name = translate_name(testcase)",
          "",
          "[Removed Lines]",
          "86:     fail_message = \"\"",
          "88:     num = int(testsuite.get(\"failures\"))",
          "89:     if num:",
          "90:         fail_message = f\"{num} failure\"",
          "91:         if num != 1:",
          "92:             fail_message += \"s\"",
          "94:     num = int(testsuite.get(\"errors\"))",
          "95:     if num:",
          "96:         if fail_message:",
          "97:             fail_message += \", \"",
          "98:         fail_message += f\"{num} error\"",
          "99:         if num != 1:",
          "100:             fail_message += \"s\"",
          "102:     if not fail_message:",
          "104:     print(f\"\\n{TEXT_RED}==== {test_type} {backend}: {fail_message} ===={TEXT_RESET}\\n\")",
          "",
          "[Added Lines]",
          "86:     fail_message_parts = []",
          "88:     num_failures = int(testsuite.get(\"failures\"))",
          "89:     if num_failures:",
          "90:         fail_message_parts.append(f\"{num_failures} failure{'' if num_failures == 1 else 's'}\")",
          "92:     num_errors = int(testsuite.get(\"errors\"))",
          "93:     if num_errors:",
          "94:         fail_message_parts.append(f\"{num_errors} error{'' if num_errors == 1 else 's'}\")",
          "96:     if not fail_message_parts:",
          "98:     print(f\"\\n{TEXT_RED}==== {test_type} {backend}: {', '.join(fail_message_parts)} ===={TEXT_RESET}\\n\")",
          "",
          "---------------"
        ],
        "scripts/in_container/run_migration_reference.py||scripts/in_container/run_migration_reference.py": [
          "File: scripts/in_container/run_migration_reference.py -> scripts/in_container/run_migration_reference.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "135:     )",
          "142: def ensure_mod_prefix(mod_name, idx, version):",
          "144:     match = re.match(r\"([0-9]+)_([0-9]+)_([0-9]+)_([0-9]+)_(.+)\", mod_name)",
          "145:     if match:",
          "146:         # previously standardized file, rebuild the name",
          "148:     else:",
          "149:         # new migration file, standard format",
          "150:         match = re.match(r\"([a-z0-9]+)_(.+)\", mod_name)",
          "151:         if match:",
          "156: def ensure_filenames_are_sorted(revisions):",
          "",
          "[Removed Lines]",
          "138: def num_to_prefix(idx: int) -> str:",
          "139:     return f\"000{idx+1}\"[-4:] + \"_\"",
          "143:     prefix = num_to_prefix(idx) + \"_\".join(version) + \"_\"",
          "147:         mod_name = match.group(5)",
          "152:             mod_name = match.group(2)",
          "153:     return prefix + mod_name",
          "",
          "[Added Lines]",
          "139:     parts = [f\"{idx + 1:04}\", *version]",
          "143:         parts.append(match.group(5))",
          "148:             parts.append(match.group(2))",
          "149:     return \"_\".join(parts)",
          "",
          "---------------"
        ],
        "scripts/in_container/verify_providers.py||scripts/in_container/verify_providers.py": [
          "File: scripts/in_container/verify_providers.py -> scripts/in_container/verify_providers.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "548:         s = s[1:]",
          "549:     if not s:",
          "550:         return True",
          "554: def check_if_classes_are_properly_named(",
          "",
          "[Removed Lines]",
          "551:     return s != s.lower() and s != s.upper() and \"_\" not in s and s[0].upper() == s[0]",
          "",
          "[Added Lines]",
          "551:     return s[0].isupper() and not (s.islower() or s.isupper() or \"_\" in s)",
          "",
          "---------------"
        ],
        "tests/providers/cncf/kubernetes/test_pod_generator.py||tests/providers/cncf/kubernetes/test_pod_generator.py": [
          "File: tests/providers/cncf/kubernetes/test_pod_generator.py -> tests/providers/cncf/kubernetes/test_pod_generator.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "818:         )",
          "819:         labels = PodGenerator.build_labels_for_k8s_executor_pod(**kwargs, **extra)",
          "820:         assert labels == {**expected, **extra_expected}",
          "822:         if \"airflow_worker\" not in extra:",
          "824:         assert PodGenerator.build_selector_for_k8s_executor_pod(**kwargs, **extra) == exp_selector",
          "",
          "[Removed Lines]",
          "821:         exp_selector = \",\".join([f\"{k}={v}\" for k, v in sorted(labels.items())])",
          "823:             exp_selector += \",airflow-worker\"",
          "",
          "[Added Lines]",
          "821:         items = [f\"{k}={v}\" for k, v in sorted(labels.items())]",
          "823:             items.append(\"airflow-worker\")",
          "824:         exp_selector = \",\".join(items)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "cff99284e8f9bccad6e2e3be024d98996ac639c6",
      "candidate_info": {
        "commit_hash": "cff99284e8f9bccad6e2e3be024d98996ac639c6",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/cff99284e8f9bccad6e2e3be024d98996ac639c6",
        "files": [
          "airflow/utils/dag_edges.py",
          "airflow/utils/db.py",
          "airflow/utils/db_cleanup.py",
          "airflow/utils/email.py",
          "airflow/utils/file.py",
          "airflow/utils/log/file_task_handler.py",
          "airflow/utils/log/secrets_masker.py"
        ],
        "message": "Refactor unneeded 'continue' jumps in utils (#33836)\n\n(cherry picked from commit ca4cd3b2eceaaaa870dd3d3911217e0ed2060e2f)",
        "before_after_code_files": [
          "airflow/utils/dag_edges.py||airflow/utils/dag_edges.py",
          "airflow/utils/db.py||airflow/utils/db.py",
          "airflow/utils/db_cleanup.py||airflow/utils/db_cleanup.py",
          "airflow/utils/email.py||airflow/utils/email.py",
          "airflow/utils/file.py||airflow/utils/file.py",
          "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py",
          "airflow/utils/log/secrets_masker.py||airflow/utils/log/secrets_masker.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/utils/dag_edges.py||airflow/utils/dag_edges.py": [
          "File: airflow/utils/dag_edges.py -> airflow/utils/dag_edges.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "111:                 edge = (task.task_id, child.task_id)",
          "112:                 if task.is_setup and child.is_teardown:",
          "113:                     setup_teardown_edges.add(edge)",
          "118:         tasks_to_trace = tasks_to_trace_next",
          "120:     result = []",
          "",
          "[Removed Lines]",
          "114:                 if edge in edges:",
          "115:                     continue",
          "116:                 edges.add(edge)",
          "117:                 tasks_to_trace_next.append(child)",
          "",
          "[Added Lines]",
          "114:                 if edge not in edges:",
          "115:                     edges.add(edge)",
          "116:                     tasks_to_trace_next.append(child)",
          "",
          "---------------"
        ],
        "airflow/utils/db.py||airflow/utils/db.py": [
          "File: airflow/utils/db.py -> airflow/utils/db.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1447:         dangling_table_name = _format_airflow_moved_table_name(source_table.name, change_version, \"dangling\")",
          "1448:         if dangling_table_name in existing_table_names:",
          "1449:             invalid_row_count = bad_rows_query.count()",
          "1453:                 yield _format_dangling_error(",
          "1454:                     source_table=source_table.name,",
          "1455:                     target_table=dangling_table_name,",
          "",
          "[Removed Lines]",
          "1450:             if invalid_row_count <= 0:",
          "1451:                 continue",
          "1452:             else:",
          "",
          "[Added Lines]",
          "1450:             if invalid_row_count:",
          "",
          "---------------"
        ],
        "airflow/utils/db_cleanup.py||airflow/utils/db_cleanup.py": [
          "File: airflow/utils/db_cleanup.py -> airflow/utils/db_cleanup.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "431:         _confirm_delete(date=clean_before_timestamp, tables=sorted(effective_table_names))",
          "432:     existing_tables = reflect_tables(tables=None, session=session).tables",
          "433:     for table_name, table_config in effective_config_dict.items():",
          "435:             logger.warning(\"Table %s not found.  Skipping.\", table_name)",
          "449: @provide_session",
          "",
          "[Removed Lines]",
          "434:         if table_name not in existing_tables:",
          "436:             continue",
          "437:         with _suppress_with_logging(table_name, session):",
          "438:             _cleanup_table(",
          "439:                 clean_before_timestamp=clean_before_timestamp,",
          "440:                 dry_run=dry_run,",
          "441:                 verbose=verbose,",
          "443:                 skip_archive=skip_archive,",
          "444:                 session=session,",
          "445:             )",
          "446:             session.commit()",
          "",
          "[Added Lines]",
          "434:         if table_name in existing_tables:",
          "435:             with _suppress_with_logging(table_name, session):",
          "436:                 _cleanup_table(",
          "437:                     clean_before_timestamp=clean_before_timestamp,",
          "438:                     dry_run=dry_run,",
          "439:                     verbose=verbose,",
          "441:                     skip_archive=skip_archive,",
          "442:                     session=session,",
          "443:                 )",
          "444:                 session.commit()",
          "445:         else:",
          "",
          "---------------"
        ],
        "airflow/utils/email.py||airflow/utils/email.py": [
          "File: airflow/utils/email.py -> airflow/utils/email.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "271:             try:",
          "272:                 smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)",
          "273:             except smtplib.SMTPServerDisconnected:",
          "288: def get_email_address_list(addresses: str | Iterable[str]) -> list[str]:",
          "",
          "[Removed Lines]",
          "274:                 if attempt < smtp_retry_limit:",
          "275:                     continue",
          "276:                 raise",
          "278:             if smtp_starttls:",
          "279:                 smtp_conn.starttls()",
          "280:             if smtp_user and smtp_password:",
          "281:                 smtp_conn.login(smtp_user, smtp_password)",
          "282:             log.info(\"Sent an alert email to %s\", e_to)",
          "283:             smtp_conn.sendmail(e_from, e_to, mime_msg.as_string())",
          "284:             smtp_conn.quit()",
          "285:             break",
          "",
          "[Added Lines]",
          "274:                 if attempt == smtp_retry_limit:",
          "275:                     raise",
          "276:             else:",
          "277:                 if smtp_starttls:",
          "278:                     smtp_conn.starttls()",
          "279:                 if smtp_user and smtp_password:",
          "280:                     smtp_conn.login(smtp_user, smtp_password)",
          "281:                 log.info(\"Sent an alert email to %s\", e_to)",
          "282:                 smtp_conn.sendmail(e_from, e_to, mime_msg.as_string())",
          "283:                 smtp_conn.quit()",
          "284:                 break",
          "",
          "---------------"
        ],
        "airflow/utils/file.py||airflow/utils/file.py": [
          "File: airflow/utils/file.py -> airflow/utils/file.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "244:             patterns_by_dir.update({dirpath: patterns.copy()})",
          "246:         for file in files:",
          "255: def find_path_from_directory(",
          "",
          "[Removed Lines]",
          "247:             if file == ignore_file_name:",
          "248:                 continue",
          "249:             abs_file_path = Path(root) / file",
          "250:             if ignore_rule_type.match(abs_file_path, patterns):",
          "251:                 continue",
          "252:             yield str(abs_file_path)",
          "",
          "[Added Lines]",
          "247:             if file != ignore_file_name:",
          "248:                 abs_file_path = Path(root) / file",
          "249:                 if not ignore_rule_type.match(abs_file_path, patterns):",
          "250:                     yield str(abs_file_path)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "310:     file_paths = []",
          "312:     for file_path in find_path_from_directory(directory, \".airflowignore\"):",
          "313:         try:",
          "323:         except Exception:",
          "324:             log.exception(\"Error while examining %s\", file_path)",
          "",
          "[Removed Lines]",
          "314:             if not os.path.isfile(file_path):",
          "315:                 continue",
          "316:             _, file_ext = os.path.splitext(os.path.split(file_path)[-1])",
          "317:             if file_ext != \".py\" and not zipfile.is_zipfile(file_path):",
          "318:                 continue",
          "319:             if not might_contain_dag(file_path, safe_mode):",
          "320:                 continue",
          "322:             file_paths.append(file_path)",
          "",
          "[Added Lines]",
          "311:         path = Path(file_path)",
          "313:             if path.is_file() and (path.suffix == \".py\" or zipfile.is_zipfile(path)):",
          "314:                 if might_contain_dag(file_path, safe_mode):",
          "315:                     file_paths.append(file_path)",
          "",
          "---------------"
        ],
        "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py": [
          "File: airflow/utils/log/file_task_handler.py -> airflow/utils/log/file_task_handler.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "109:     timestamp = None",
          "110:     next_timestamp = None",
          "111:     for idx, line in enumerate(lines):",
          "122: def _interleave_logs(*logs):",
          "",
          "[Removed Lines]",
          "112:         if not line:",
          "113:             continue",
          "114:         with suppress(Exception):",
          "115:             # next_timestamp unchanged if line can't be parsed",
          "116:             next_timestamp = _parse_timestamp(line)",
          "117:         if next_timestamp:",
          "118:             timestamp = next_timestamp",
          "119:         yield timestamp, idx, line",
          "",
          "[Added Lines]",
          "112:         if line:",
          "113:             with suppress(Exception):",
          "114:                 # next_timestamp unchanged if line can't be parsed",
          "115:                 next_timestamp = _parse_timestamp(line)",
          "116:             if next_timestamp:",
          "117:                 timestamp = next_timestamp",
          "118:             yield timestamp, idx, line",
          "",
          "---------------"
        ],
        "airflow/utils/log/secrets_masker.py||airflow/utils/log/secrets_masker.py": [
          "File: airflow/utils/log/secrets_masker.py -> airflow/utils/log/secrets_masker.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "206:         if self.replacer:",
          "207:             for k, v in record.__dict__.items():",
          "211:             if record.exc_info and record.exc_info[1] is not None:",
          "212:                 exc = record.exc_info[1]",
          "213:                 self._redact_exception_with_context(exc)",
          "",
          "[Removed Lines]",
          "208:                 if k in self._record_attrs_to_ignore:",
          "209:                     continue",
          "210:                 record.__dict__[k] = self.redact(v)",
          "",
          "[Added Lines]",
          "208:                 if k not in self._record_attrs_to_ignore:",
          "209:                     record.__dict__[k] = self.redact(v)",
          "",
          "---------------"
        ]
      }
    }
  ]
}