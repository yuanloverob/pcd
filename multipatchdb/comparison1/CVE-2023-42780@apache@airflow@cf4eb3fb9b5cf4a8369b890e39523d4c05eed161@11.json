{
  "cve_id": "CVE-2023-42780",
  "cve_desc": "Apache Airflow, versions prior to 2.7.2, contains a security vulnerability that allows authenticated users of Airflow to list warnings for all DAGs, even if the user had no permission to see those DAGs. It would reveal the dag_ids and the stack-traces of import errors for those DAGs with import errors.\nUsers of Apache Airflow are advised to upgrade to version 2.7.2 or newer to mitigate the risk associated with this vulnerability.\n\n",
  "repo": "apache/airflow",
  "patch_hash": "cf4eb3fb9b5cf4a8369b890e39523d4c05eed161",
  "patch_info": {
    "commit_hash": "cf4eb3fb9b5cf4a8369b890e39523d4c05eed161",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/cf4eb3fb9b5cf4a8369b890e39523d4c05eed161",
    "files": [
      "airflow/api_connexion/endpoints/dag_warning_endpoint.py",
      "tests/api_connexion/endpoints/test_dag_warning_endpoint.py"
    ],
    "message": "Fix dag warning endpoint permissions (#34355)\n\n* Fix dag warning endpoint permissions\n\n* update the query to have an accurate result for total entries and pagination\n\n* add unit tests\n\n* Update test_dag_warning_endpoint.py\n\nCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>\n\n---------\n\nCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>\n(cherry picked from commit 3570bbfbea69e2965f91b9964ce28bc268c68129)",
    "before_after_code_files": [
      "airflow/api_connexion/endpoints/dag_warning_endpoint.py||airflow/api_connexion/endpoints/dag_warning_endpoint.py",
      "tests/api_connexion/endpoints/test_dag_warning_endpoint.py||tests/api_connexion/endpoints/test_dag_warning_endpoint.py"
    ]
  },
  "patch_diff": {
    "airflow/api_connexion/endpoints/dag_warning_endpoint.py||airflow/api_connexion/endpoints/dag_warning_endpoint.py": [
      "File: airflow/api_connexion/endpoints/dag_warning_endpoint.py -> airflow/api_connexion/endpoints/dag_warning_endpoint.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "16: # under the License.",
      "17: from __future__ import annotations",
      "19: from sqlalchemy import select",
      "20: from sqlalchemy.orm import Session",
      "22: from airflow.api_connexion import security",
      "23: from airflow.api_connexion.parameters import apply_sorting, check_limit, format_parameters",
      "24: from airflow.api_connexion.schemas.dag_warning_schema import (",
      "25:     DagWarningCollection,",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "19: from flask import g",
      "24: from airflow.api_connexion.exceptions import PermissionDenied",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "28: from airflow.api_connexion.types import APIResponse",
      "29: from airflow.models.dagwarning import DagWarning as DagWarningModel",
      "30: from airflow.security import permissions",
      "31: from airflow.utils.db import get_query_count",
      "32: from airflow.utils.session import NEW_SESSION, provide_session",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "33: from airflow.utils.airflow_flask_app import get_airflow_app",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "52:     allowed_filter_attrs = [\"dag_id\", \"warning_type\", \"message\", \"timestamp\"]",
      "53:     query = select(DagWarningModel)",
      "54:     if dag_id:",
      "55:         query = query.where(DagWarningModel.dag_id == dag_id)",
      "56:     if warning_type:",
      "57:         query = query.where(DagWarningModel.warning_type == warning_type)",
      "58:     total_entries = get_query_count(query, session=session)",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "58:         if not get_airflow_app().appbuilder.sm.can_read_dag(dag_id, g.user):",
      "59:             raise PermissionDenied(detail=f\"User not allowed to access this DAG: {dag_id}\")",
      "61:     else:",
      "62:         readable_dags = get_airflow_app().appbuilder.sm.get_accessible_dag_ids(g.user)",
      "63:         query = query.where(DagWarningModel.dag_id.in_(readable_dags))",
      "",
      "---------------"
    ],
    "tests/api_connexion/endpoints/test_dag_warning_endpoint.py||tests/api_connexion/endpoints/test_dag_warning_endpoint.py": [
      "File: tests/api_connexion/endpoints/test_dag_warning_endpoint.py -> tests/api_connexion/endpoints/test_dag_warning_endpoint.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "35:         app,  # type:ignore",
      "36:         username=\"test\",",
      "37:         role_name=\"Test\",",
      "39:     )",
      "40:     create_user(app, username=\"test_no_permissions\", role_name=\"TestNoPermissions\")  # type: ignore",
      "42:     yield minimal_app_for_api",
      "44:     delete_user(app, username=\"test\")  # type: ignore",
      "45:     delete_user(app, username=\"test_no_permissions\")  # type: ignore",
      "48: class TestBaseDagWarning:",
      "",
      "[Removed Lines]",
      "38:         permissions=[(permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG_WARNING)],  # type: ignore",
      "",
      "[Added Lines]",
      "38:         permissions=[",
      "39:             (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG_WARNING),",
      "40:             (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG),",
      "41:         ],  # type: ignore",
      "44:     create_user(",
      "45:         app,  # type:ignore",
      "46:         username=\"test_with_dag2_read\",",
      "47:         role_name=\"TestWithDag2Read\",",
      "48:         permissions=[",
      "49:             (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG_WARNING),",
      "50:             (permissions.ACTION_CAN_READ, f\"{permissions.RESOURCE_DAG_PREFIX}dag2\"),",
      "51:         ],  # type: ignore",
      "52:     )",
      "58:     delete_user(app, username=\"test_with_dag2_read\")  # type: ignore",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "147:             \"/api/v1/dagWarnings\", environ_overrides={\"REMOTE_USER\": \"test_no_permissions\"}",
      "148:         )",
      "149:         assert response.status_code == 403",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "164:     def test_should_raise_403_forbidden_when_user_has_no_dag_read_permission(self):",
      "165:         response = self.client.get(",
      "166:             \"/api/v1/dagWarnings\",",
      "167:             environ_overrides={\"REMOTE_USER\": \"test_with_dag2_read\"},",
      "168:             query_string={\"dag_id\": \"dag1\"},",
      "169:         )",
      "170:         assert response.status_code == 403",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "67e49b6fa575a93ee55e4ed82647bbbea8ed2ec3",
      "candidate_info": {
        "commit_hash": "67e49b6fa575a93ee55e4ed82647bbbea8ed2ec3",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/67e49b6fa575a93ee55e4ed82647bbbea8ed2ec3",
        "files": [
          "airflow/www/views.py",
          "tests/www/views/test_views_dataset.py"
        ],
        "message": "Fix query bug in next_run_datasets_summary endpoint (#34143)\n\nThe query to get the dataset_triggered_dag_ids in the next_run_datasets_summary endpoint\nerrors with `str` object has no attribute dag_id. This issue is from a recent refactor on\nsqlalchemy queries and the view has no unit test to detect it.\nI added a fix with a unit test\n\n(cherry picked from commit 79abcfacbec07b9e4ee374f7f4f7dc2c05b72692)",
        "before_after_code_files": [
          "airflow/www/views.py||airflow/www/views.py",
          "tests/www/views/test_views_dataset.py||tests/www/views/test_views_dataset.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/www/views.py||airflow/www/views.py": [
          "File: airflow/www/views.py -> airflow/www/views.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1089:             filter_dag_ids = allowed_dag_ids",
          "1091:         dataset_triggered_dag_ids = [",
          "1094:                 session.scalars(",
          "1095:                     select(DagModel.dag_id)",
          "1096:                     .where(DagModel.dag_id.in_(filter_dag_ids))",
          "",
          "[Removed Lines]",
          "1092:             dag.dag_id",
          "1093:             for dag in (",
          "",
          "[Added Lines]",
          "1092:             dag_id",
          "1093:             for dag_id in (",
          "",
          "---------------"
        ],
        "tests/www/views/test_views_dataset.py||tests/www/views/test_views_dataset.py": [
          "File: tests/www/views/test_views_dataset.py -> tests/www/views/test_views_dataset.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "441:         assert response.status_code == 200",
          "442:         assert len(response.json[\"datasets\"]) == 50",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "445: class TestGetDatasetNextRunSummary(TestDatasetEndpoint):",
          "446:     def test_next_run_dataset_summary(self, dag_maker, admin_client):",
          "447:         with dag_maker(dag_id=\"upstream\", schedule=[Dataset(uri=\"s3://bucket/key/1\")], serialized=True):",
          "448:             EmptyOperator(task_id=\"task1\")",
          "450:         response = admin_client.post(\"/next_run_datasets_summary\", data={\"dag_ids\": [\"upstream\"]})",
          "452:         assert response.status_code == 200",
          "453:         assert response.json == {\"upstream\": {\"ready\": 0, \"total\": 1, \"uri\": \"s3://bucket/key/1\"}}",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "3035e8d41142b9d76a3e8bf4501432a4244d5bd8",
      "candidate_info": {
        "commit_hash": "3035e8d41142b9d76a3e8bf4501432a4244d5bd8",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/3035e8d41142b9d76a3e8bf4501432a4244d5bd8",
        "files": [
          "airflow/api_connexion/endpoints/task_endpoint.py",
          "airflow/dag_processing/manager.py",
          "airflow/timetables/events.py",
          "dev/validate_version_added_fields_in_config.py",
          "scripts/ci/pre_commit/pre_commit_sort_in_the_wild.py",
          "scripts/tools/generate-integrations-json.py",
          "tests/jobs/test_scheduler_job.py"
        ],
        "message": "Refactor: Use inplace .sort() (#33743)\n\n(cherry picked from commit 660386b5d962a77d895be34a644616a67a0e49c8)",
        "before_after_code_files": [
          "airflow/api_connexion/endpoints/task_endpoint.py||airflow/api_connexion/endpoints/task_endpoint.py",
          "airflow/dag_processing/manager.py||airflow/dag_processing/manager.py",
          "airflow/timetables/events.py||airflow/timetables/events.py",
          "dev/validate_version_added_fields_in_config.py||dev/validate_version_added_fields_in_config.py",
          "scripts/ci/pre_commit/pre_commit_sort_in_the_wild.py||scripts/ci/pre_commit/pre_commit_sort_in_the_wild.py",
          "scripts/tools/generate-integrations-json.py||scripts/tools/generate-integrations-json.py",
          "tests/jobs/test_scheduler_job.py||tests/jobs/test_scheduler_job.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/api_connexion/endpoints/task_endpoint.py||airflow/api_connexion/endpoints/task_endpoint.py": [
          "File: airflow/api_connexion/endpoints/task_endpoint.py -> airflow/api_connexion/endpoints/task_endpoint.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "61:     tasks = dag.tasks",
          "63:     try:",
          "65:     except AttributeError as err:",
          "66:         raise BadRequest(detail=str(err))",
          "67:     task_collection = TaskCollection(tasks=tasks, total_entries=len(tasks))",
          "",
          "[Removed Lines]",
          "64:         tasks = sorted(tasks, key=attrgetter(order_by.lstrip(\"-\")), reverse=(order_by[0:1] == \"-\"))",
          "",
          "[Added Lines]",
          "64:         tasks.sort(key=attrgetter(order_by.lstrip(\"-\")), reverse=(order_by[0:1] == \"-\"))",
          "",
          "---------------"
        ],
        "airflow/dag_processing/manager.py||airflow/dag_processing/manager.py": [
          "File: airflow/dag_processing/manager.py -> airflow/dag_processing/manager.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "865:             rows.append((file_path, processor_pid, runtime, num_dags, num_errors, last_runtime, last_run))",
          "867:         # Sort by longest last runtime. (Can't sort None values in python3)",
          "870:         formatted_rows = []",
          "871:         for file_path, pid, runtime, num_dags, num_errors, last_runtime, last_run in rows:",
          "",
          "[Removed Lines]",
          "868:         rows = sorted(rows, key=lambda x: x[3] or 0.0)",
          "",
          "[Added Lines]",
          "868:         rows.sort(key=lambda x: x[3] or 0.0)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1167:         if is_mtime_mode:",
          "1168:             file_paths = sorted(files_with_mtime, key=files_with_mtime.get, reverse=True)",
          "1169:         elif list_mode == \"alphabetical\":",
          "1171:         elif list_mode == \"random_seeded_by_host\":",
          "1172:             # Shuffle the list seeded by hostname so multiple schedulers can work on different",
          "1173:             # set of files. Since we set the seed, the sort order will remain same per host",
          "",
          "[Removed Lines]",
          "1170:             file_paths = sorted(file_paths)",
          "",
          "[Added Lines]",
          "1170:             file_paths.sort()",
          "",
          "---------------"
        ],
        "airflow/timetables/events.py||airflow/timetables/events.py": [
          "File: airflow/timetables/events.py -> airflow/timetables/events.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "52:         self.event_dates = list(event_dates)  # Must be reversible and indexable",
          "53:         if not presorted:",
          "54:             # For long lists this could take a while, so only want to do it once",
          "56:         self.restrict_to_events = restrict_to_events",
          "57:         if description is None:",
          "58:             self.description = (",
          "",
          "[Removed Lines]",
          "55:             self.event_dates = sorted(self.event_dates)",
          "",
          "[Added Lines]",
          "55:             self.event_dates.sort()",
          "",
          "---------------"
        ],
        "dev/validate_version_added_fields_in_config.py||dev/validate_version_added_fields_in_config.py": [
          "File: dev/validate_version_added_fields_in_config.py -> dev/validate_version_added_fields_in_config.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "114:     computed_option_new_section.update(options)",
          "116: # 1. Prepare versions to checks",
          "121: # 2. Compute expected options set with version added fields",
          "122: expected_computed_options: set[tuple[str, str, str]] = set()",
          "",
          "[Removed Lines]",
          "117: airflow_version = fetch_pypi_versions()",
          "118: airflow_version = sorted(airflow_version, key=semver.VersionInfo.parse)",
          "119: to_check_versions: list[str] = [d for d in airflow_version if d.startswith(\"2.\")]",
          "",
          "[Added Lines]",
          "117: to_check_versions: list[str] = [d for d in fetch_pypi_versions() if d.startswith(\"2.\")]",
          "118: to_check_versions.sort(key=semver.VersionInfo.parse)",
          "",
          "---------------"
        ],
        "scripts/ci/pre_commit/pre_commit_sort_in_the_wild.py||scripts/ci/pre_commit/pre_commit_sort_in_the_wild.py": [
          "File: scripts/ci/pre_commit/pre_commit_sort_in_the_wild.py -> scripts/ci/pre_commit/pre_commit_sort_in_the_wild.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "68:                 line = \"1.\" + line.split(\".\", maxsplit=1)[1]",
          "69:                 print(f\"{old_line.strip()} => {line.strip()}\")",
          "70:             companies.append(line)",
          "72:     inthewild_path.write_text(\"\".join(header) + \"\\n\" + \"\".join(companies))",
          "",
          "[Removed Lines]",
          "71:     companies = sorted(companies, key=stable_sort)",
          "",
          "[Added Lines]",
          "71:     companies.sort(key=stable_sort)",
          "",
          "---------------"
        ],
        "scripts/tools/generate-integrations-json.py||scripts/tools/generate-integrations-json.py": [
          "File: scripts/tools/generate-integrations-json.py -> scripts/tools/generate-integrations-json.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "67:             result[\"logo\"] = logo",
          "68:         result_integrations.append(result)",
          "71: with open(os.path.join(AIRFLOW_SITE_DIR, \"landing-pages/site/static/integrations.json\"), \"w\") as f:",
          "72:     f.write(",
          "73:         json.dumps(",
          "",
          "[Removed Lines]",
          "70: result_integrations = sorted(result_integrations, key=lambda x: x[\"name\"].lower())",
          "",
          "[Added Lines]",
          "70: result_integrations.sort(key=lambda x: x[\"name\"].lower())",
          "",
          "---------------"
        ],
        "tests/jobs/test_scheduler_job.py||tests/jobs/test_scheduler_job.py": [
          "File: tests/jobs/test_scheduler_job.py -> tests/jobs/test_scheduler_job.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "601:             dr2.get_task_instance(task_id_1, session=session),",
          "602:             dr2.get_task_instance(task_id_2, session=session),",
          "603:         ]",
          "605:         for ti in tis:",
          "606:             ti.state = State.SCHEDULED",
          "607:             session.merge(ti)",
          "",
          "[Removed Lines]",
          "604:         tis = sorted(tis, key=lambda ti: ti.key)",
          "",
          "[Added Lines]",
          "604:         tis.sort(key=lambda ti: ti.key)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "270e2190b8bd15594af5a936fd54d35cb3a3b8ac",
      "candidate_info": {
        "commit_hash": "270e2190b8bd15594af5a936fd54d35cb3a3b8ac",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/270e2190b8bd15594af5a936fd54d35cb3a3b8ac",
        "files": [
          "airflow/api/common/delete_dag.py",
          "airflow/api/common/mark_tasks.py"
        ],
        "message": "Refactor unneeded 'continue' jumps in api (#33842)\n\n(cherry picked from commit 3b58a38e67c6645b04b8c2a1cc3bd95f32da6f12)",
        "before_after_code_files": [
          "airflow/api/common/delete_dag.py||airflow/api/common/delete_dag.py",
          "airflow/api/common/mark_tasks.py||airflow/api/common/mark_tasks.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/api/common/delete_dag.py||airflow/api/common/delete_dag.py": [
          "File: airflow/api/common/delete_dag.py -> airflow/api/common/delete_dag.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "79:     count = 0",
          "81:     for model in get_sqla_model_classes():",
          "85:             count += session.execute(",
          "86:                 delete(model)",
          "87:                 .where(model.dag_id.in_(dags_to_delete))",
          "",
          "[Removed Lines]",
          "82:         if hasattr(model, \"dag_id\"):",
          "83:             if keep_records_in_log and model.__name__ == \"Log\":",
          "84:                 continue",
          "",
          "[Added Lines]",
          "82:         if hasattr(model, \"dag_id\") and (not keep_records_in_log or model.__name__ != \"Log\"):",
          "",
          "---------------"
        ],
        "airflow/api/common/mark_tasks.py||airflow/api/common/mark_tasks.py": [
          "File: airflow/api/common/mark_tasks.py -> airflow/api/common/mark_tasks.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "62:     }",
          "64:     for info in infos:",
          "75:     return dag_runs.values()",
          "",
          "[Removed Lines]",
          "65:         if info.logical_date in dag_runs:",
          "66:             continue",
          "67:         dag_runs[info.logical_date] = dag.create_dagrun(",
          "68:             execution_date=info.logical_date,",
          "69:             data_interval=info.data_interval,",
          "70:             start_date=timezone.utcnow(),",
          "71:             external_trigger=False,",
          "72:             state=state,",
          "73:             run_type=run_type,",
          "74:         )",
          "",
          "[Added Lines]",
          "65:         if info.logical_date not in dag_runs:",
          "66:             dag_runs[info.logical_date] = dag.create_dagrun(",
          "67:                 execution_date=info.logical_date,",
          "68:                 data_interval=info.data_interval,",
          "69:                 start_date=timezone.utcnow(),",
          "70:                 external_trigger=False,",
          "71:                 state=state,",
          "72:                 run_type=run_type,",
          "73:             )",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "489:     tasks = []",
          "490:     for task in dag.tasks:",
          "496:     # Mark non-finished tasks as SKIPPED.",
          "497:     tis = session.scalars(",
          "",
          "[Removed Lines]",
          "491:         if task.task_id not in task_ids_of_running_tis:",
          "492:             continue",
          "493:         task.dag = dag",
          "494:         tasks.append(task)",
          "",
          "[Added Lines]",
          "490:         if task.task_id in task_ids_of_running_tis:",
          "491:             task.dag = dag",
          "492:             tasks.append(task)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "16ab2c212546a7925a6fcf65b28de645a10ad336",
      "candidate_info": {
        "commit_hash": "16ab2c212546a7925a6fcf65b28de645a10ad336",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/16ab2c212546a7925a6fcf65b28de645a10ad336",
        "files": [
          "airflow/cli/commands/standalone_command.py",
          "airflow/utils/code_utils.py",
          "dev/assign_cherry_picked_prs_with_milestone.py",
          "dev/check_files.py",
          "dev/prepare_release_issue.py",
          "dev/provider_packages/prepare_provider_packages.py",
          "docs/exts/docs_build/code_utils.py",
          "docs/exts/docs_build/errors.py",
          "docs/exts/docs_build/spelling_checks.py",
          "kubernetes_tests/test_base.py",
          "tests/cli/commands/test_config_command.py",
          "tests/cli/commands/test_dag_command.py",
          "tests/models/test_dag.py",
          "tests/test_utils/terraform.py"
        ],
        "message": "Use str.splitlines() to split lines (#33592)\n\n(cherry picked from commit 0e005643280883afe88416c7f3d3425dff8f02b7)",
        "before_after_code_files": [
          "airflow/cli/commands/standalone_command.py||airflow/cli/commands/standalone_command.py",
          "airflow/utils/code_utils.py||airflow/utils/code_utils.py",
          "dev/assign_cherry_picked_prs_with_milestone.py||dev/assign_cherry_picked_prs_with_milestone.py",
          "dev/check_files.py||dev/check_files.py",
          "dev/prepare_release_issue.py||dev/prepare_release_issue.py",
          "dev/provider_packages/prepare_provider_packages.py||dev/provider_packages/prepare_provider_packages.py",
          "kubernetes_tests/test_base.py||kubernetes_tests/test_base.py",
          "tests/cli/commands/test_config_command.py||tests/cli/commands/test_config_command.py",
          "tests/cli/commands/test_dag_command.py||tests/cli/commands/test_dag_command.py",
          "tests/models/test_dag.py||tests/models/test_dag.py",
          "tests/test_utils/terraform.py||tests/test_utils/terraform.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/cli/commands/standalone_command.py||airflow/cli/commands/standalone_command.py": [
          "File: airflow/cli/commands/standalone_command.py -> airflow/cli/commands/standalone_command.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "140:             \"standalone\": \"white\",",
          "141:         }.get(name, \"white\")",
          "142:         colorised_name = colored(\"%10s\" % name, color)",
          "144:             print(f\"{colorised_name} | {line.strip()}\")",
          "146:     def print_error(self, name: str, output):",
          "",
          "[Removed Lines]",
          "143:         for line in output.split(\"\\n\"):",
          "",
          "[Added Lines]",
          "143:         for line in output.splitlines():",
          "",
          "---------------"
        ],
        "airflow/utils/code_utils.py||airflow/utils/code_utils.py": [
          "File: airflow/utils/code_utils.py -> airflow/utils/code_utils.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "19: import functools",
          "20: import inspect",
          "21: import os",
          "22: from typing import Any",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "22: from pathlib import Path",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "61:     :param context_lines_count: The number of lines that will be cut before and after.",
          "62:     :return: str",
          "63:     \"\"\"",
          "79:     return code",
          "",
          "[Removed Lines]",
          "64:     with open(file_path) as text_file:",
          "65:         # Highlight code",
          "66:         code = text_file.read()",
          "67:         code_lines = code.split(\"\\n\")",
          "68:         # Prepend line number",
          "69:         code_lines = [",
          "70:             f\">{lno:3} | {line}\" if line_no == lno else f\"{lno:4} | {line}\"",
          "71:             for lno, line in enumerate(code_lines, 1)",
          "72:         ]",
          "73:         # # Cut out the snippet",
          "74:         start_line_no = max(0, line_no - context_lines_count - 1)",
          "75:         end_line_no = line_no + context_lines_count",
          "76:         code_lines = code_lines[start_line_no:end_line_no]",
          "77:         # Join lines",
          "78:         code = \"\\n\".join(code_lines)",
          "",
          "[Added Lines]",
          "65:     code_lines = Path(file_path).read_text().splitlines()",
          "66:     # Prepend line number",
          "67:     code_lines = [",
          "68:         f\">{lno:3} | {line}\" if line_no == lno else f\"{lno:4} | {line}\"",
          "69:         for lno, line in enumerate(code_lines, 1)",
          "70:     ]",
          "71:     # # Cut out the snippet",
          "72:     start_line_no = max(0, line_no - context_lines_count - 1)",
          "73:     end_line_no = line_no + context_lines_count",
          "74:     code_lines = code_lines[start_line_no:end_line_no]",
          "75:     # Join lines",
          "76:     code = \"\\n\".join(code_lines)",
          "",
          "---------------"
        ],
        "dev/assign_cherry_picked_prs_with_milestone.py||dev/assign_cherry_picked_prs_with_milestone.py": [
          "File: dev/assign_cherry_picked_prs_with_milestone.py -> dev/assign_cherry_picked_prs_with_milestone.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "226:         cwd=SOURCE_DIR_PATH,",
          "227:         text=True,",
          "228:     )",
          "232: def update_milestone(r: Repository, pr: PullRequest, m: Milestone):",
          "",
          "[Removed Lines]",
          "229:     return [get_change_from_line(line) for line in change_strings.split(\"\\n\")]",
          "",
          "[Added Lines]",
          "229:     return [get_change_from_line(line) for line in change_strings.splitlines()]",
          "",
          "---------------"
        ],
        "dev/check_files.py||dev/check_files.py": [
          "File: dev/check_files.py -> dev/check_files.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "65:     # e.g. https://pypi.org/project/apache-airflow-providers-airbyte/3.1.0rc1/",
          "67:     packages = []",
          "69:         if not line:",
          "70:             continue",
          "71:         name, version = line.rstrip(\"/\").split(\"/\")[-2:]",
          "",
          "[Removed Lines]",
          "68:     for line in content.split(\"\\n\"):",
          "",
          "[Added Lines]",
          "68:     for line in content.splitlines():",
          "",
          "---------------"
        ],
        "dev/prepare_release_issue.py||dev/prepare_release_issue.py": [
          "File: dev/prepare_release_issue.py -> dev/prepare_release_issue.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "169:         cwd=SOURCE_DIR_PATH,",
          "170:         text=True,",
          "171:     )",
          "175: def render_template(",
          "",
          "[Removed Lines]",
          "172:     return [get_change_from_line(line) for line in change_strings.split(\"\\n\")]",
          "",
          "[Added Lines]",
          "172:     return [get_change_from_line(line) for line in change_strings.splitlines()]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "301:             # GitHub does not have linked issues in PR - but we quite rigorously add Fixes/Closes",
          "302:             # Relate so we can find those from the body",
          "303:             if pr.body:",
          "305:                 linked_issue_numbers = {",
          "306:                     int(issue_match.group(1)) for issue_match in ISSUE_MATCH_IN_BODY.finditer(body)",
          "307:                 }",
          "",
          "[Removed Lines]",
          "304:                 body = pr.body.replace(\"\\n\", \" \").replace(\"\\r\", \" \")",
          "",
          "[Added Lines]",
          "304:                 body = \" \".join(pr.body.splitlines())",
          "",
          "---------------"
        ],
        "dev/provider_packages/prepare_provider_packages.py||dev/provider_packages/prepare_provider_packages.py": [
          "File: dev/provider_packages/prepare_provider_packages.py -> dev/provider_packages/prepare_provider_packages.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "483:     \"\"\"",
          "484:     from tabulate import tabulate",
          "487:     headers = [\"Commit\", \"Committed\", \"Subject\"]",
          "488:     table_data = []",
          "489:     changes_list: list[Change] = []",
          "",
          "[Removed Lines]",
          "486:     lines = changes.split(\"\\n\")",
          "",
          "[Added Lines]",
          "486:     lines = changes.splitlines()",
          "",
          "---------------"
        ],
        "kubernetes_tests/test_base.py||kubernetes_tests/test_base.py": [
          "File: kubernetes_tests/test_base.py -> kubernetes_tests/test_base.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "102:     @staticmethod",
          "103:     def _num_pods_in_namespace(namespace):",
          "104:         air_pod = check_output([\"kubectl\", \"get\", \"pods\", \"-n\", namespace]).decode()",
          "106:         names = [re2.compile(r\"\\s+\").split(x)[0] for x in air_pod if \"airflow\" in x]",
          "107:         return len(names)",
          "",
          "[Removed Lines]",
          "105:         air_pod = air_pod.split(\"\\n\")",
          "",
          "[Added Lines]",
          "105:         air_pod = air_pod.splitlines()",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "110:     def _delete_airflow_pod(name=\"\"):",
          "111:         suffix = \"-\" + name if name else \"\"",
          "112:         air_pod = check_output([\"kubectl\", \"get\", \"pods\"]).decode()",
          "114:         names = [re2.compile(r\"\\s+\").split(x)[0] for x in air_pod if \"airflow\" + suffix in x]",
          "115:         if names:",
          "116:             check_call([\"kubectl\", \"delete\", \"pod\", names[0]])",
          "",
          "[Removed Lines]",
          "113:         air_pod = air_pod.split(\"\\n\")",
          "",
          "[Added Lines]",
          "113:         air_pod = air_pod.splitlines()",
          "",
          "---------------"
        ],
        "tests/cli/commands/test_config_command.py||tests/cli/commands/test_config_command.py": [
          "File: tests/cli/commands/test_config_command.py -> tests/cli/commands/test_config_command.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "78:         with contextlib.redirect_stdout(io.StringIO()) as temp_stdout:",
          "79:             config_command.show_config(self.parser.parse_args([\"config\", \"list\", \"--color\", \"off\"]))",
          "80:         output = temp_stdout.getvalue()",
          "82:         assert all(not line.startswith(\"#\") or line.endswith(\"= \") for line in lines if line)",
          "84:     def test_cli_show_config_shows_descriptions(self):",
          "",
          "[Removed Lines]",
          "81:         lines = output.split(\"\\n\")",
          "",
          "[Added Lines]",
          "81:         lines = output.splitlines()",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "87:                 self.parser.parse_args([\"config\", \"list\", \"--color\", \"off\", \"--include-descriptions\"])",
          "88:             )",
          "89:         output = temp_stdout.getvalue()",
          "91:         # comes from metrics description",
          "92:         assert all(not line.startswith(\"# Source: \") for line in lines if line)",
          "93:         assert any(line.startswith(\"# StatsD\") for line in lines if line)",
          "",
          "[Removed Lines]",
          "90:         lines = output.split(\"\\n\")",
          "",
          "[Added Lines]",
          "90:         lines = output.splitlines()",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "100:                 self.parser.parse_args([\"config\", \"list\", \"--color\", \"off\", \"--include-examples\"])",
          "101:             )",
          "102:         output = temp_stdout.getvalue()",
          "104:         assert all(not line.startswith(\"# Source: \") for line in lines if line)",
          "105:         assert all(not line.startswith(\"# StatsD\") for line in lines if line)",
          "106:         assert any(line.startswith(\"# Example:\") for line in lines if line)",
          "",
          "[Removed Lines]",
          "103:         lines = output.split(\"\\n\")",
          "",
          "[Added Lines]",
          "103:         lines = output.splitlines()",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "112:                 self.parser.parse_args([\"config\", \"list\", \"--color\", \"off\", \"--include-env-vars\"])",
          "113:             )",
          "114:         output = temp_stdout.getvalue()",
          "116:         assert all(not line.startswith(\"# Source: \") for line in lines if line)",
          "117:         assert all(not line.startswith(\"# StatsD\") for line in lines if line)",
          "118:         assert all(not line.startswith(\"# Example:\") for line in lines if line)",
          "",
          "[Removed Lines]",
          "115:         lines = output.split(\"\\n\")",
          "",
          "[Added Lines]",
          "115:         lines = output.splitlines()",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "124:                 self.parser.parse_args([\"config\", \"list\", \"--color\", \"off\", \"--include-sources\"])",
          "125:             )",
          "126:         output = temp_stdout.getvalue()",
          "128:         assert any(line.startswith(\"# Source: \") for line in lines if line)",
          "129:         assert all(not line.startswith(\"# StatsD\") for line in lines if line)",
          "130:         assert all(not line.startswith(\"# Example:\") for line in lines if line)",
          "",
          "[Removed Lines]",
          "127:         lines = output.split(\"\\n\")",
          "",
          "[Added Lines]",
          "127:         lines = output.splitlines()",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "136:                 self.parser.parse_args([\"config\", \"list\", \"--color\", \"off\", \"--defaults\"])",
          "137:             )",
          "138:         output = temp_stdout.getvalue()",
          "140:         assert all(not line.startswith(\"# Source: \") for line in lines if line)",
          "141:         assert any(line.startswith(\"# StatsD\") for line in lines if line)",
          "142:         assert any(not line.startswith(\"# Example:\") for line in lines if line)",
          "",
          "[Removed Lines]",
          "139:         lines = output.split(\"\\n\")",
          "",
          "[Added Lines]",
          "139:         lines = output.splitlines()",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "151:                 self.parser.parse_args([\"config\", \"list\", \"--color\", \"off\", \"--defaults\"])",
          "152:             )",
          "153:         output = temp_stdout.getvalue()",
          "155:         assert any(line.startswith(\"# task_runner = StandardTaskRunner\") for line in lines if line)",
          "157:     @mock.patch(\"os.environ\", {\"AIRFLOW__CORE__TASK_RUNNER\": \"test-env-runner\"})",
          "",
          "[Removed Lines]",
          "154:         lines = output.split(\"\\n\")",
          "",
          "[Added Lines]",
          "154:         lines = output.splitlines()",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "161:                 self.parser.parse_args([\"config\", \"list\", \"--color\", \"off\", \"--defaults\"])",
          "162:             )",
          "163:         output = temp_stdout.getvalue()",
          "165:         assert any(line.startswith(\"# task_runner = StandardTaskRunner\") for line in lines if line)",
          "167:     @conf_vars({(\"core\", \"task_runner\"): \"test-runner\"})",
          "",
          "[Removed Lines]",
          "164:         lines = output.split(\"\\n\")",
          "",
          "[Added Lines]",
          "164:         lines = output.splitlines()",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "169:         with contextlib.redirect_stdout(io.StringIO()) as temp_stdout:",
          "170:             config_command.show_config(self.parser.parse_args([\"config\", \"list\", \"--color\", \"off\"]))",
          "171:         output = temp_stdout.getvalue()",
          "173:         assert any(line.startswith(\"task_runner = test-runner\") for line in lines if line)",
          "175:     @mock.patch(\"os.environ\", {\"AIRFLOW__CORE__TASK_RUNNER\": \"test-env-runner\"})",
          "",
          "[Removed Lines]",
          "172:         lines = output.split(\"\\n\")",
          "",
          "[Added Lines]",
          "172:         lines = output.splitlines()",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "177:         with contextlib.redirect_stdout(io.StringIO()) as temp_stdout:",
          "178:             config_command.show_config(self.parser.parse_args([\"config\", \"list\", \"--color\", \"off\"]))",
          "179:         output = temp_stdout.getvalue()",
          "181:         assert any(line.startswith(\"task_runner = test-env-runner\") for line in lines if line)",
          "183:     def test_cli_has_providers(self):",
          "184:         with contextlib.redirect_stdout(io.StringIO()) as temp_stdout:",
          "185:             config_command.show_config(self.parser.parse_args([\"config\", \"list\", \"--color\", \"off\"]))",
          "186:         output = temp_stdout.getvalue()",
          "188:         assert any(line.startswith(\"celery_config_options\") for line in lines if line)",
          "190:     def test_cli_comment_out_everything(self):",
          "",
          "[Removed Lines]",
          "180:         lines = output.split(\"\\n\")",
          "187:         lines = output.split(\"\\n\")",
          "",
          "[Added Lines]",
          "180:         lines = output.splitlines()",
          "187:         lines = output.splitlines()",
          "",
          "---------------",
          "--- Hunk 11 ---",
          "[Context before]",
          "193:                 self.parser.parse_args([\"config\", \"list\", \"--color\", \"off\", \"--comment-out-everything\"])",
          "194:             )",
          "195:         output = temp_stdout.getvalue()",
          "197:         assert all(not line.strip() or line.startswith((\"#\", \"[\")) for line in lines if line)",
          "",
          "[Removed Lines]",
          "196:         lines = output.split(\"\\n\")",
          "",
          "[Added Lines]",
          "196:         lines = output.splitlines()",
          "",
          "---------------"
        ],
        "tests/cli/commands/test_dag_command.py||tests/cli/commands/test_dag_command.py": [
          "File: tests/cli/commands/test_dag_command.py -> tests/cli/commands/test_dag_command.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "680:         with contextlib.redirect_stdout(io.StringIO()) as temp_stdout:",
          "681:             dag_command.dag_trigger(args)",
          "682:             # get the last line from the logs ignoring all logging lines",
          "684:         parsed_out = json.loads(out)",
          "686:         assert 1 == len(parsed_out)",
          "",
          "[Removed Lines]",
          "683:             out = temp_stdout.getvalue().strip().split(\"\\n\")[-1]",
          "",
          "[Added Lines]",
          "683:             out = temp_stdout.getvalue().strip().splitlines()[-1]",
          "",
          "---------------"
        ],
        "tests/models/test_dag.py||tests/models/test_dag.py": [
          "File: tests/models/test_dag.py -> tests/models/test_dag.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1368:                 dag.tree_view()",
          "1369:                 stdout = stdout.getvalue()",
          "1372:             assert \"t1\" in stdout_lines[0]",
          "1373:             assert \"t2\" in stdout_lines[1]",
          "1374:             assert \"t3\" in stdout_lines[2]",
          "",
          "[Removed Lines]",
          "1371:             stdout_lines = stdout.split(\"\\n\")",
          "",
          "[Added Lines]",
          "1371:             stdout_lines = stdout.splitlines()",
          "",
          "---------------"
        ],
        "tests/test_utils/terraform.py||tests/test_utils/terraform.py": [
          "File: tests/test_utils/terraform.py -> tests/test_utils/terraform.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "28:         self.execute_cmd([\"terraform\", \"apply\", \"-input=false\", \"-auto-approve\", self.TERRAFORM_DIR])",
          "30:     def get_tf_output(self, name):",
          "33:     def teardown_method(self) -> None:",
          "34:         self.execute_cmd([\"terraform\", \"plan\", \"-destroy\", \"-input=false\", self.TERRAFORM_DIR])",
          "",
          "[Removed Lines]",
          "31:         return self.check_output([\"terraform\", \"output\", name]).decode(\"utf-8\").replace(\"\\r\\n\", \"\")",
          "",
          "[Added Lines]",
          "31:         return \"\".join(self.check_output([\"terraform\", \"output\", name]).decode(\"utf-8\").splitlines())",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "8e618a25816ca189b0c078b0a94676d3d3baf718",
      "candidate_info": {
        "commit_hash": "8e618a25816ca189b0c078b0a94676d3d3baf718",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/8e618a25816ca189b0c078b0a94676d3d3baf718",
        "files": [
          "tests/jobs/test_triggerer_job.py",
          "tests/providers/amazon/aws/secrets/test_systems_manager.py",
          "tests/providers/cncf/kubernetes/executors/test_kubernetes_executor.py",
          "tests/providers/google/cloud/operators/test_bigquery.py",
          "tests/system/providers/amazon/aws/utils/__init__.py",
          "tests/www/views/test_views_base.py"
        ],
        "message": "Replace single element slice by next() in Airflow tests (#33938)\n\n* Replace single element slice by next() in Airflow tests\n\n* revert qubole change\n\n* Update test_views_base.py\n\nCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>\n\n---------\n\nCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>\n(cherry picked from commit 528459bbe932ff6318b629c618f878b280df2867)",
        "before_after_code_files": [
          "tests/jobs/test_triggerer_job.py||tests/jobs/test_triggerer_job.py",
          "tests/providers/amazon/aws/secrets/test_systems_manager.py||tests/providers/amazon/aws/secrets/test_systems_manager.py",
          "tests/providers/cncf/kubernetes/executors/test_kubernetes_executor.py||tests/providers/cncf/kubernetes/executors/test_kubernetes_executor.py",
          "tests/providers/google/cloud/operators/test_bigquery.py||tests/providers/google/cloud/operators/test_bigquery.py",
          "tests/system/providers/amazon/aws/utils/__init__.py||tests/system/providers/amazon/aws/utils/__init__.py",
          "tests/www/views/test_views_base.py||tests/www/views/test_views_base.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "tests/jobs/test_triggerer_job.py||tests/jobs/test_triggerer_job.py": [
          "File: tests/jobs/test_triggerer_job.py -> tests/jobs/test_triggerer_job.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "557:         for _ in range(30):",
          "558:             if job_runner.trigger_runner.failed_triggers:",
          "559:                 assert len(job_runner.trigger_runner.failed_triggers) == 1",
          "561:                 assert trigger_id == 1",
          "562:                 assert isinstance(exc, ValueError)",
          "563:                 assert exc.args[0] == \"Deliberate trigger failure\"",
          "",
          "[Removed Lines]",
          "560:                 trigger_id, exc = list(job_runner.trigger_runner.failed_triggers)[0]",
          "",
          "[Added Lines]",
          "560:                 trigger_id, exc = next(iter(job_runner.trigger_runner.failed_triggers))",
          "",
          "---------------"
        ],
        "tests/providers/amazon/aws/secrets/test_systems_manager.py||tests/providers/amazon/aws/secrets/test_systems_manager.py": [
          "File: tests/providers/amazon/aws/secrets/test_systems_manager.py -> tests/providers/amazon/aws/secrets/test_systems_manager.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "157:     @mock.patch(\"airflow.providers.amazon.aws.hooks.base_aws.SessionFactory\")",
          "158:     def test_passing_client_kwargs(self, mock_session_factory):",
          "159:         backends = initialize_secrets_backends()",
          "161:             backend",
          "162:             for backend in backends",
          "163:             if backend.__class__.__name__ == \"SystemsManagerParameterStoreBackend\"",
          "166:         # Mock SessionFactory, session and client",
          "167:         mock_session_factory_instance = mock_session_factory.return_value",
          "",
          "[Removed Lines]",
          "160:         systems_manager = [",
          "164:         ][0]",
          "",
          "[Added Lines]",
          "160:         systems_manager = next(",
          "164:         )",
          "",
          "---------------"
        ],
        "tests/providers/cncf/kubernetes/executors/test_kubernetes_executor.py||tests/providers/cncf/kubernetes/executors/test_kubernetes_executor.py": [
          "File: tests/providers/cncf/kubernetes/executors/test_kubernetes_executor.py -> tests/providers/cncf/kubernetes/executors/test_kubernetes_executor.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "437:                 ),",
          "438:             )",
          "441:         finally:",
          "442:             executor.end()",
          "",
          "[Removed Lines]",
          "440:             assert list(executor.event_buffer.values())[0][1] == \"Invalid executor_config passed\"",
          "",
          "[Added Lines]",
          "440:             assert next(iter(executor.event_buffer.values()))[1] == \"Invalid executor_config passed\"",
          "",
          "---------------"
        ],
        "tests/providers/google/cloud/operators/test_bigquery.py||tests/providers/google/cloud/operators/test_bigquery.py": [
          "File: tests/providers/google/cloud/operators/test_bigquery.py -> tests/providers/google/cloud/operators/test_bigquery.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "685:         ]",
          "687:         # Check DeSerialized version of operator link",
          "690:         ti.xcom_push(\"job_id_path\", TEST_FULL_JOB_ID)",
          "",
          "[Removed Lines]",
          "688:         assert isinstance(list(simple_task.operator_extra_links)[0], BigQueryConsoleLink)",
          "",
          "[Added Lines]",
          "688:         assert isinstance(next(iter(simple_task.operator_extra_links)), BigQueryConsoleLink)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "723:         ]",
          "725:         # Check DeSerialized version of operator link",
          "728:         ti.xcom_push(key=\"job_id_path\", value=[TEST_FULL_JOB_ID, TEST_FULL_JOB_ID_2])",
          "",
          "[Removed Lines]",
          "726:         assert isinstance(list(simple_task.operator_extra_links)[0], BigQueryConsoleIndexableLink)",
          "",
          "[Added Lines]",
          "726:         assert isinstance(next(iter(simple_task.operator_extra_links)), BigQueryConsoleIndexableLink)",
          "",
          "---------------"
        ],
        "tests/system/providers/amazon/aws/utils/__init__.py||tests/system/providers/amazon/aws/utils/__init__.py": [
          "File: tests/system/providers/amazon/aws/utils/__init__.py -> tests/system/providers/amazon/aws/utils/__init__.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "64:     \"\"\"",
          "65:     # The exact layer of the stack will depend on if this is called directly",
          "66:     # or from another helper, but the test will always contain the identifier.",
          "68:         frame.filename for frame in inspect.stack() if TEST_FILE_IDENTIFIER in frame.filename",
          "70:     return splitext(basename(test_filename))[0]",
          "",
          "[Removed Lines]",
          "67:     test_filename: str = [",
          "69:     ][0]",
          "",
          "[Added Lines]",
          "67:     test_filename: str = next(",
          "69:     )",
          "",
          "---------------"
        ],
        "tests/www/views/test_views_base.py||tests/www/views/test_views_base.py": [
          "File: tests/www/views/test_views_base.py -> tests/www/views/test_views_base.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "253: def _check_task_stats_json(resp):",
          "257: @pytest.mark.parametrize(",
          "",
          "[Removed Lines]",
          "254:     return set(list(resp.json.items())[0][1][0].keys()) == {\"state\", \"count\"}",
          "",
          "[Added Lines]",
          "254:     return set(next(iter(resp.json.items()))[1][0]) == {\"state\", \"count\"}",
          "",
          "---------------"
        ]
      }
    }
  ]
}