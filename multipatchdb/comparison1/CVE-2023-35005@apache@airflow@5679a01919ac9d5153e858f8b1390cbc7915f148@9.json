{
  "cve_id": "CVE-2023-35005",
  "cve_desc": "In Apache Airflow, some potentially sensitive values were being shown to the user in certain situations.\n\nThis vulnerability is mitigated by the fact configuration is not shown in the UI by default (only if `[webserver] expose_config` is set to `non-sensitive-only`), and not all uncensored values are actually sentitive.\n\n\nThis issue affects Apache Airflow: from 2.5.0 before 2.6.2. Users are recommended to update to version 2.6.2 or later.\n\n\n",
  "repo": "apache/airflow",
  "patch_hash": "5679a01919ac9d5153e858f8b1390cbc7915f148",
  "patch_info": {
    "commit_hash": "5679a01919ac9d5153e858f8b1390cbc7915f148",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/5679a01919ac9d5153e858f8b1390cbc7915f148",
    "files": [
      "airflow/config_templates/config.yml",
      "airflow/config_templates/default_airflow.cfg",
      "airflow/configuration.py",
      "airflow/www/views.py",
      "tests/core/test_configuration.py",
      "tests/www/views/test_views_configuration.py"
    ],
    "message": "Use single source of truth for sensitive config items (#31820)\n\nPreviously we had them defined both in constant and in config.yml.\n\nNow just config.yml\n\n(cherry picked from commit cab342ee010bfd048006ca458c760b37470b6ea5)",
    "before_after_code_files": [
      "airflow/config_templates/default_airflow.cfg||airflow/config_templates/default_airflow.cfg",
      "airflow/configuration.py||airflow/configuration.py",
      "airflow/www/views.py||airflow/www/views.py",
      "tests/core/test_configuration.py||tests/core/test_configuration.py",
      "tests/www/views/test_views_configuration.py||tests/www/views/test_views_configuration.py"
    ]
  },
  "patch_diff": {
    "airflow/config_templates/default_airflow.cfg||airflow/config_templates/default_airflow.cfg": [
      "File: airflow/config_templates/default_airflow.cfg -> airflow/config_templates/default_airflow.cfg",
      "--- Hunk 1 ---",
      "[Context before]",
      "995: # Example: result_backend = db+postgresql://postgres:airflow@postgres/airflow",
      "996: # result_backend =",
      "998: # Celery Flower is a sweet UI for Celery. Airflow has a shortcut to start",
      "999: # it ``airflow celery flower``. This defines the IP that Celery Flower runs on",
      "1000: flower_host = 0.0.0.0",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "998: # Optional configuration dictionary to pass to the Celery result backend SQLAlchemy engine.",
      "999: # Example: result_backend_sqlalchemy_engine_options = {{\"pool_recycle\": 1800}}",
      "1000: result_backend_sqlalchemy_engine_options =",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "1018: # Import path for celery configuration options",
      "1019: celery_config_options = airflow.config_templates.default_celery.DEFAULT_CELERY_CONFIG",
      "1020: ssl_active = False",
      "1021: ssl_key =",
      "1022: ssl_cert =",
      "1023: ssl_cacert =",
      "1025: # Celery Pool implementation.",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "1026: # Path to the client key.",
      "1029: # Path to the client certificate.",
      "1032: # Path to the CA certificate.",
      "",
      "---------------"
    ],
    "airflow/configuration.py||airflow/configuration.py": [
      "File: airflow/configuration.py -> airflow/configuration.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "37: from contextlib import contextmanager, suppress",
      "38: from json.decoder import JSONDecodeError",
      "39: from re import Pattern",
      "41: from urllib.parse import urlsplit",
      "43: from typing_extensions import overload",
      "",
      "[Removed Lines]",
      "40: from typing import IO, Any, Dict, Iterable, Tuple, Union",
      "",
      "[Added Lines]",
      "40: from typing import IO, Any, Dict, Iterable, Set, Tuple, Union",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "147:         return yaml.safe_load(config_file)",
      "165: class AirflowConfigParser(ConfigParser):",
      "166:     \"\"\"Custom Airflow Configparser supporting defaults and deprecated options.\"\"\"",
      "",
      "[Removed Lines]",
      "150: SENSITIVE_CONFIG_VALUES = {",
      "151:     (\"database\", \"sql_alchemy_conn\"),",
      "152:     (\"core\", \"fernet_key\"),",
      "153:     (\"celery\", \"broker_url\"),",
      "154:     (\"celery\", \"flower_basic_auth\"),",
      "155:     (\"celery\", \"result_backend\"),",
      "156:     (\"atlas\", \"password\"),",
      "157:     (\"smtp\", \"smtp_password\"),",
      "158:     (\"webserver\", \"secret_key\"),",
      "159:     (\"secrets\", \"backend_kwargs\"),",
      "160:     # The following options are deprecated",
      "161:     (\"core\", \"sql_alchemy_conn\"),",
      "162: }",
      "",
      "[Added Lines]",
      "[None]",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "171:     # These configs can also be fetched from Secrets backend",
      "172:     # following the \"{section}__{name}__secret\" pattern",
      "176:     # A mapping of (new section, new option) -> (old section, old option, since_version).",
      "177:     # When reading new option, the old option will be checked to see if it exists. If it does a",
      "",
      "[Removed Lines]",
      "174:     sensitive_config_values: set[tuple[str, str]] = SENSITIVE_CONFIG_VALUES",
      "",
      "[Added Lines]",
      "159:     @cached_property",
      "160:     def sensitive_config_values(self) -> Set[tuple[str, str]]:  # noqa: UP006",
      "161:         default_config = default_config_yaml()",
      "162:         flattened = {",
      "163:             (s, k): item for s, s_c in default_config.items() for k, item in s_c.get(\"options\").items()",
      "164:         }",
      "165:         sensitive = {(section, key) for (section, key), v in flattened.items() if v.get(\"sensitive\") is True}",
      "166:         depr_option = {self.deprecated_options[x][:-1] for x in sensitive if x in self.deprecated_options}",
      "167:         depr_section = {",
      "168:             (self.deprecated_sections[s][0], k) for s, k in sensitive if s in self.deprecated_sections",
      "169:         }",
      "170:         sensitive.update(depr_section, depr_option)",
      "171:         return sensitive",
      "",
      "---------------"
    ],
    "airflow/www/views.py||airflow/www/views.py": [
      "File: airflow/www/views.py -> airflow/www/views.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "3951:         # TODO remove \"if raw\" usage in Airflow 3.0. Configuration can be fetched via the REST API.",
      "3952:         if raw:",
      "3953:             if expose_config == \"non-sensitive-only\":",
      "3956:                 updater = configupdater.ConfigUpdater()",
      "3957:                 updater.read(AIRFLOW_CONFIG)",
      "3959:                     if updater.has_option(sect, key):",
      "3960:                         updater[sect][key].value = \"< hidden >\"",
      "3961:                 config = str(updater)",
      "",
      "[Removed Lines]",
      "3954:                 from airflow.configuration import SENSITIVE_CONFIG_VALUES",
      "3958:                 for sect, key in SENSITIVE_CONFIG_VALUES:",
      "",
      "[Added Lines]",
      "3956:                 for sect, key in conf.sensitive_config_values:",
      "",
      "---------------"
    ],
    "tests/core/test_configuration.py||tests/core/test_configuration.py": [
      "File: tests/core/test_configuration.py -> tests/core/test_configuration.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "36:     AirflowConfigException,",
      "37:     AirflowConfigParser,",
      "38:     conf,",
      "39:     expand_env_var,",
      "40:     get_airflow_config,",
      "41:     get_airflow_home,",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "39:     default_config_yaml,",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "1447:             w = captured.pop()",
      "1448:             assert \"your `conf.get*` call to use the new name\" in str(w.message)",
      "1449:             assert w.category == FutureWarning",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "1453: def test_sensitive_values():",
      "1454:     from airflow.settings import conf",
      "1456:     # this list was hardcoded prior to 2.6.2",
      "1457:     # included here to avoid regression in refactor",
      "1458:     # inclusion of keys ending in \"password\" or \"kwargs\" is automated from 2.6.2",
      "1459:     # items not matching this pattern must be added here manually",
      "1460:     sensitive_values = {",
      "1461:         (\"database\", \"sql_alchemy_conn\"),",
      "1462:         (\"core\", \"fernet_key\"),",
      "1463:         (\"celery\", \"broker_url\"),",
      "1464:         (\"celery\", \"flower_basic_auth\"),",
      "1465:         (\"celery\", \"result_backend\"),",
      "1466:         (\"atlas\", \"password\"),",
      "1467:         (\"smtp\", \"smtp_password\"),",
      "1468:         (\"webserver\", \"secret_key\"),",
      "1469:         (\"secrets\", \"backend_kwargs\"),",
      "1470:         (\"sentry\", \"sentry_dsn\"),",
      "1471:         (\"database\", \"sql_alchemy_engine_args\"),",
      "1472:         (\"core\", \"sql_alchemy_conn\"),",
      "1473:     }",
      "1474:     default_config = default_config_yaml()",
      "1475:     all_keys = {(s, k) for s, v in default_config.items() for k in v.get(\"options\")}",
      "1476:     suspected_sensitive = {(s, k) for (s, k) in all_keys if k.endswith((\"password\", \"kwargs\"))}",
      "1477:     exclude_list = {",
      "1478:         (\"kubernetes_executor\", \"delete_option_kwargs\"),",
      "1479:     }",
      "1480:     suspected_sensitive -= exclude_list",
      "1481:     sensitive_values.update(suspected_sensitive)",
      "1482:     assert sensitive_values == conf.sensitive_config_values",
      "",
      "---------------"
    ],
    "tests/www/views/test_views_configuration.py||tests/www/views/test_views_configuration.py": [
      "File: tests/www/views/test_views_configuration.py -> tests/www/views/test_views_configuration.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "19: import html",
      "22: from tests.test_utils.config import conf_vars",
      "23: from tests.test_utils.www import check_content_in_response, check_content_not_in_response",
      "",
      "[Removed Lines]",
      "21: from airflow.configuration import SENSITIVE_CONFIG_VALUES, conf",
      "",
      "[Added Lines]",
      "21: from airflow.configuration import conf",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "36: @conf_vars({(\"webserver\", \"expose_config\"): \"True\"})",
      "37: def test_user_can_view_configuration(admin_client):",
      "38:     resp = admin_client.get(\"configuration\", follow_redirects=True)",
      "40:         value = conf.get(section, key, fallback=\"\")",
      "41:         if not value:",
      "42:             continue",
      "",
      "[Removed Lines]",
      "39:     for section, key in SENSITIVE_CONFIG_VALUES:",
      "",
      "[Added Lines]",
      "39:     for section, key in conf.sensitive_config_values:",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "46: @conf_vars({(\"webserver\", \"expose_config\"): \"non-sensitive-only\"})",
      "47: def test_configuration_redacted(admin_client):",
      "48:     resp = admin_client.get(\"configuration\", follow_redirects=True)",
      "50:         value = conf.get(section, key, fallback=\"\")",
      "51:         if not value or value == \"airflow\":",
      "52:             continue",
      "",
      "[Removed Lines]",
      "49:     for section, key in SENSITIVE_CONFIG_VALUES:",
      "",
      "[Added Lines]",
      "49:     for section, key in conf.sensitive_config_values:",
      "",
      "---------------",
      "--- Hunk 4 ---",
      "[Context before]",
      "58: @conf_vars({(\"webserver\", \"expose_config\"): \"non-sensitive-only\"})",
      "59: def test_configuration_redacted_in_running_configuration(admin_client):",
      "60:     resp = admin_client.get(\"configuration\", follow_redirects=True)",
      "62:         value = conf.get(section, key, fallback=\"\")",
      "63:         if not value or value == \"airflow\":",
      "64:             continue",
      "",
      "[Removed Lines]",
      "61:     for section, key in SENSITIVE_CONFIG_VALUES:",
      "",
      "[Added Lines]",
      "61:     for section, key in conf.sensitive_config_values:",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "99241948e8f07e4ff1b9ef335d0c51e2c0e22815",
      "candidate_info": {
        "commit_hash": "99241948e8f07e4ff1b9ef335d0c51e2c0e22815",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/99241948e8f07e4ff1b9ef335d0c51e2c0e22815",
        "files": [
          "airflow/sensors/external_task.py",
          "tests/sensors/test_external_task_sensor.py"
        ],
        "message": "Fix ExternalTaskSensor to work correctly with task groups (#30742)\n\n* Fix ExternalTaskSensor to work correctly with task groups that have mapped tasks\n\n* Add tests for ExternalTaskSensor with task group that have mapped tasks\n\n* Fix working for multiple runs\n\n* Use tuple_in_condition instead of tuple_\n\n---------\n\nCo-authored-by: Zhyhimont Dmitry <dzhigimont@gmail.com>\n(cherry picked from commit 3c30e54de3b8a6fe793b0ff1ed8225562779d96c)",
        "before_after_code_files": [
          "airflow/sensors/external_task.py||airflow/sensors/external_task.py",
          "tests/sensors/test_external_task_sensor.py||tests/sensors/test_external_task_sensor.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/31796"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/sensors/external_task.py||airflow/sensors/external_task.py": [
          "File: airflow/sensors/external_task.py -> airflow/sensors/external_task.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "36: from airflow.utils.file import correct_maybe_zipped",
          "37: from airflow.utils.helpers import build_airflow_url_with_query",
          "38: from airflow.utils.session import NEW_SESSION, provide_session",
          "39: from airflow.utils.state import State",
          "41: if TYPE_CHECKING:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "39: from airflow.utils.sqlalchemy import tuple_in_condition",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "364:                 .scalar()",
          "365:             ) / len(self.external_task_ids)",
          "366:         elif self.external_task_group_id:",
          "368:             count = (",
          "369:                 self._count_query(TI, session, states, dttm_filter)",
          "371:                 .scalar()",
          "372:             ) / len(external_task_group_task_ids)",
          "373:         else:",
          "",
          "[Removed Lines]",
          "367:             external_task_group_task_ids = self.get_external_task_group_task_ids(session)",
          "370:                 .filter(TI.task_id.in_(external_task_group_task_ids))",
          "",
          "[Added Lines]",
          "368:             external_task_group_task_ids = self.get_external_task_group_task_ids(session, dttm_filter)",
          "371:                 .filter(tuple_in_condition((TI.task_id, TI.map_index), external_task_group_task_ids))",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "382:         )",
          "383:         return query",
          "386:         refreshed_dag_info = DagBag(read_dags_from_db=True).get_dag(self.external_dag_id, session)",
          "387:         task_group = refreshed_dag_info.task_group_dict.get(self.external_task_group_id)",
          "389:         if task_group:",
          "392:         # returning default task_id as group_id itself, this will avoid any failure in case of",
          "393:         # 'check_existence=False' and will fail on timeout",
          "396:     def _handle_execution_date_fn(self, context) -> Any:",
          "397:         \"\"\"",
          "",
          "[Removed Lines]",
          "385:     def get_external_task_group_task_ids(self, session):",
          "390:             return [task.task_id for task in task_group]",
          "394:         return [self.external_task_group_id]",
          "",
          "[Added Lines]",
          "386:     def get_external_task_group_task_ids(self, session, dttm_filter):",
          "391:             group_tasks = session.query(TaskInstance).filter(",
          "392:                 TaskInstance.dag_id == self.external_dag_id,",
          "393:                 TaskInstance.task_id.in_(task.task_id for task in task_group),",
          "394:                 TaskInstance.execution_date.in_(dttm_filter),",
          "395:             )",
          "397:             return [(t.task_id, t.map_index) for t in group_tasks]",
          "401:         return [(self.external_task_group_id, -1)]",
          "",
          "---------------"
        ],
        "tests/sensors/test_external_task_sensor.py||tests/sensors/test_external_task_sensor.py": [
          "File: tests/sensors/test_external_task_sensor.py -> tests/sensors/test_external_task_sensor.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "112:             ti.run(ignore_ti_state=True, mark_success=True)",
          "113:             ti.set_state(target_states[idx])",
          "115:     def test_external_task_sensor(self):",
          "116:         self.add_time_sensor()",
          "117:         op = ExternalTaskSensor(",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "115:     def add_dummy_task_group_with_dynamic_tasks(self, target_state=State.SUCCESS):",
          "116:         map_indexes = range(5)",
          "117:         with self.dag as dag:",
          "118:             with TaskGroup(group_id=TEST_TASK_GROUP_ID) as task_group:",
          "120:                 @task_deco",
          "121:                 def dummy_task():",
          "122:                     pass",
          "124:                 @task_deco",
          "125:                 def dummy_mapped_task(x: int):",
          "126:                     return x",
          "128:                 dummy_task()",
          "129:                 dummy_mapped_task.expand(x=[i for i in map_indexes])",
          "131:         SerializedDagModel.write_dag(dag)",
          "133:         for task in task_group:",
          "134:             if task.task_id == \"dummy_mapped_task\":",
          "135:                 for map_index in map_indexes:",
          "136:                     ti = TaskInstance(task=task, execution_date=DEFAULT_DATE, map_index=map_index)",
          "137:                     ti.run(ignore_ti_state=True, mark_success=True)",
          "138:                     ti.set_state(target_state)",
          "139:             else:",
          "140:                 ti = TaskInstance(task=task, execution_date=DEFAULT_DATE)",
          "141:                 ti.run(ignore_ti_state=True, mark_success=True)",
          "142:                 ti.set_state(target_state)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "751:         with pytest.raises(AirflowException):",
          "752:             op.run(start_date=DEFAULT_DATE, end_date=DEFAULT_DATE, ignore_ti_state=True)",
          "755: def test_external_task_sensor_check_zipped_dag_existence(dag_zip_maker):",
          "756:     with dag_zip_maker(\"test_external_task_sensor_check_existense.py\") as dagbag:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "783:     def test_external_task_group_with_mapped_tasks_sensor_success(self):",
          "784:         self.add_time_sensor()",
          "785:         self.add_dummy_task_group_with_dynamic_tasks()",
          "786:         op = ExternalTaskSensor(",
          "787:             task_id=\"test_external_task_sensor_check\",",
          "788:             external_dag_id=TEST_DAG_ID,",
          "789:             external_task_group_id=TEST_TASK_GROUP_ID,",
          "790:             failed_states=[State.FAILED],",
          "791:             dag=self.dag,",
          "792:         )",
          "793:         op.run(start_date=DEFAULT_DATE, end_date=DEFAULT_DATE, ignore_ti_state=True)",
          "795:     def test_external_task_group_with_mapped_tasks_failed_states(self):",
          "796:         self.add_time_sensor()",
          "797:         self.add_dummy_task_group_with_dynamic_tasks(State.FAILED)",
          "798:         op = ExternalTaskSensor(",
          "799:             task_id=\"test_external_task_sensor_check\",",
          "800:             external_dag_id=TEST_DAG_ID,",
          "801:             external_task_group_id=TEST_TASK_GROUP_ID,",
          "802:             failed_states=[State.FAILED],",
          "803:             dag=self.dag,",
          "804:         )",
          "805:         with pytest.raises(",
          "806:             AirflowException,",
          "807:             match=f\"The external task_group '{TEST_TASK_GROUP_ID}' in DAG '{TEST_DAG_ID}' failed.\",",
          "808:         ):",
          "809:             op.run(start_date=DEFAULT_DATE, end_date=DEFAULT_DATE, ignore_ti_state=True)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "bbc2f0becf8e4d342211bd2c8f027b5f4e8a3735",
      "candidate_info": {
        "commit_hash": "bbc2f0becf8e4d342211bd2c8f027b5f4e8a3735",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/bbc2f0becf8e4d342211bd2c8f027b5f4e8a3735",
        "files": [
          "airflow/api_connexion/openapi/v1.yaml",
          "airflow/www/static/js/types/api-generated.ts"
        ],
        "message": "Add TriggererStatus to OpenAPI spec (#31579)\n\n* Add TriggererStatus to OpenAPI spec to health endpoint\n\n* Update spec\n\n* Update description\n\n(cherry picked from commit fc36d002fccaad65718f42994747600c14f10263)",
        "before_after_code_files": [
          "airflow/www/static/js/types/api-generated.ts||airflow/www/static/js/types/api-generated.ts"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/31796"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/www/static/js/types/api-generated.ts||airflow/www/static/js/types/api-generated.ts": [
          "File: airflow/www/static/js/types/api-generated.ts -> airflow/www/static/js/types/api-generated.ts",
          "--- Hunk 1 ---",
          "[Context before]",
          "1160:     HealthInfo: {",
          "1161:       metadatabase?: components[\"schemas\"][\"MetadatabaseStatus\"];",
          "1162:       scheduler?: components[\"schemas\"][\"SchedulerStatus\"];",
          "1163:     };",
          "1165:     MetadatabaseStatus: {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1163:       triggerer?: components[\"schemas\"][\"TriggererStatus\"];",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1170:       status?: components[\"schemas\"][\"HealthStatus\"];",
          "1175:       latest_scheduler_heartbeat?: string | null;",
          "1176:     };",
          "1178:     Pool: {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1183:     TriggererStatus: {",
          "1184:       status?: components[\"schemas\"][\"HealthStatus\"];",
          "1189:       latest_triggerer_heartbeat?: string | null;",
          "1190:     };",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "2123:     WeightRule: \"downstream\" | \"upstream\" | \"absolute\";",
          "2129:   };",
          "2130:   responses: {",
          "",
          "[Removed Lines]",
          "2128:     HealthStatus: \"healthy\" | \"unhealthy\";",
          "",
          "[Added Lines]",
          "2142:     HealthStatus: (\"healthy\" | \"unhealthy\") | null;",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "4564: export type SchedulerStatus = CamelCasedPropertiesDeep<",
          "4565:   components[\"schemas\"][\"SchedulerStatus\"]",
          "4566: >;",
          "4567: export type Pool = CamelCasedPropertiesDeep<components[\"schemas\"][\"Pool\"]>;",
          "4568: export type PoolCollection = CamelCasedPropertiesDeep<",
          "4569:   components[\"schemas\"][\"PoolCollection\"]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "4581: export type TriggererStatus = CamelCasedPropertiesDeep<",
          "4582:   components[\"schemas\"][\"TriggererStatus\"]",
          "4583: >;",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "17392bcd63d1e24503a064cc6f9cfc57663a4a59",
      "candidate_info": {
        "commit_hash": "17392bcd63d1e24503a064cc6f9cfc57663a4a59",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/17392bcd63d1e24503a064cc6f9cfc57663a4a59",
        "files": [
          ".github/workflows/ci.yml",
          "dev/breeze/src/airflow_breeze/utils/selective_checks.py",
          "dev/breeze/tests/test_selective_checks.py"
        ],
        "message": "Fix selective checks skipping provider tests in non main branch (#31821)\n\nWhen CI runs in non-main branch, some provider precommits and provider\ntests should be disabled. There was a typo that prevented this from\nhappening, also names of the pre-commits changed.\n\n(cherry picked from commit bc7e471cdae73a3c46ef89d17dbc1a54212b9291)",
        "before_after_code_files": [
          "dev/breeze/src/airflow_breeze/utils/selective_checks.py||dev/breeze/src/airflow_breeze/utils/selective_checks.py",
          "dev/breeze/tests/test_selective_checks.py||dev/breeze/tests/test_selective_checks.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/31796"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/breeze/src/airflow_breeze/utils/selective_checks.py||dev/breeze/src/airflow_breeze/utils/selective_checks.py": [
          "File: dev/breeze/src/airflow_breeze/utils/selective_checks.py -> dev/breeze/src/airflow_breeze/utils/selective_checks.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "574:         return \"allow suspended provider changes\" not in self._pr_labels",
          "576:     def _get_test_types_to_run(self) -> list[str]:",
          "577:         candidate_test_types: set[str] = {\"Always\"}",
          "578:         matched_files: set[str] = set()",
          "579:         matched_files.update(",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "577:         if self.full_tests_needed:",
          "578:             return list(all_selective_test_types())",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "656:     def parallel_test_types_list_as_string(self) -> str | None:",
          "657:         if not self.run_tests:",
          "658:             return None",
          "663:         if self._default_branch != \"main\":",
          "664:             test_types_to_remove: set[str] = set()",
          "665:             for test_type in current_test_types:",
          "",
          "[Removed Lines]",
          "659:         if self.full_tests_needed:",
          "660:             current_test_types = set(all_selective_test_types())",
          "661:         else:",
          "662:             current_test_types = set(self._get_test_types_to_run())",
          "",
          "[Added Lines]",
          "662:         current_test_types = set(self._get_test_types_to_run())",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "736:     @cached_property",
          "737:     def skip_pre_commits(self) -> str:",
          "740:     @cached_property",
          "741:     def skip_provider_tests(self) -> bool:",
          "746:     @cached_property",
          "747:     def cache_directive(self) -> str:",
          "",
          "[Removed Lines]",
          "738:         return \"identity\" if self._default_branch == \"main\" else \"identity,check-airflow-2-2-compatibility\"",
          "742:         return self._default_branch != \"main\" or not any(",
          "743:             test_type.startswith(\"Providers\") for test_type in self._get_test_types_to_run()",
          "744:         )",
          "",
          "[Added Lines]",
          "738:         return (",
          "739:             \"identity\"",
          "740:             if self._default_branch == \"main\"",
          "741:             else \"identity,check-airflow-provider-compatibility,\"",
          "742:             \"check-extra-packages-references,check-provider-yaml-valid\"",
          "743:         )",
          "747:         if self._default_branch != \"main\":",
          "748:             return True",
          "749:         if self.full_tests_needed:",
          "750:             return False",
          "751:         if any(test_type.startswith(\"Providers\") for test_type in self._get_test_types_to_run()):",
          "752:             return False",
          "753:         return True",
          "",
          "---------------"
        ],
        "dev/breeze/tests/test_selective_checks.py||dev/breeze/tests/test_selective_checks.py": [
          "File: dev/breeze/tests/test_selective_checks.py -> dev/breeze/tests/test_selective_checks.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "485:                     \"docs-filter-list-as-string\": \"--package-filter apache-airflow \"",
          "486:                     \"--package-filter docker-stack\",",
          "487:                     \"full-tests-needed\": \"true\",",
          "488:                     \"upgrade-to-newer-dependencies\": \"false\",",
          "489:                     \"parallel-test-types-list-as-string\": \"Core Other WWW API Always CLI\",",
          "490:                 },",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "488:                     \"skip-provider-tests\": \"true\",",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "271b33af26d9ab88a016a8a1ba3dce718db6c41a",
      "candidate_info": {
        "commit_hash": "271b33af26d9ab88a016a8a1ba3dce718db6c41a",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/271b33af26d9ab88a016a8a1ba3dce718db6c41a",
        "files": [
          "airflow/sensors/base.py",
          "tests/sensors/test_base.py"
        ],
        "message": "Ensure min backoff in base sensor is at least 1 (#31412)\n\n(cherry picked from commit a98621f4facabc207b4d6b6968e6863845e1f90f)",
        "before_after_code_files": [
          "airflow/sensors/base.py||airflow/sensors/base.py",
          "tests/sensors/test_base.py||tests/sensors/test_base.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/31796"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/sensors/base.py||airflow/sensors/base.py": [
          "File: airflow/sensors/base.py -> airflow/sensors/base.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "265:         if not self.exponential_backoff:",
          "266:             return self.poke_interval",
          "270:         run_hash = int(",
          "271:             hashlib.sha1(f\"{self.dag_id}#{self.task_id}#{started_at}#{try_number}\".encode()).hexdigest(),",
          "",
          "[Removed Lines]",
          "268:         min_backoff = int(self.poke_interval * (2 ** (try_number - 2)))",
          "",
          "[Added Lines]",
          "268:         # The value of min_backoff should always be greater than or equal to 1.",
          "269:         min_backoff = max(int(self.poke_interval * (2 ** (try_number - 2))), 1)",
          "",
          "---------------"
        ],
        "tests/sensors/test_base.py||tests/sensors/test_base.py": [
          "File: tests/sensors/test_base.py -> tests/sensors/test_base.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "523:             assert interval2 >= sensor.poke_interval",
          "524:             assert interval2 > interval1",
          "526:     def test_sensor_with_exponential_backoff_on_and_max_wait(self):",
          "528:         sensor = DummySensor(",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "526:     @pytest.mark.parametrize(\"poke_interval\", [0, 0.1, 0.9, 1, 2, 3])",
          "527:     def test_sensor_with_exponential_backoff_on_and_small_poke_interval(self, poke_interval):",
          "528:         \"\"\"Test that sensor works correctly when poke_interval is small and exponential_backoff is on\"\"\"",
          "530:         sensor = DummySensor(",
          "531:             task_id=SENSOR_OP,",
          "532:             return_value=None,",
          "533:             poke_interval=poke_interval,",
          "534:             timeout=60,",
          "535:             exponential_backoff=True,",
          "536:         )",
          "538:         with patch(\"airflow.utils.timezone.utcnow\") as mock_utctime:",
          "539:             mock_utctime.return_value = DEFAULT_DATE",
          "541:             started_at = timezone.utcnow() - timedelta(seconds=10)",
          "543:             def run_duration():",
          "544:                 return (timezone.utcnow - started_at).total_seconds()",
          "546:             intervals = [",
          "547:                 sensor._get_next_poke_interval(started_at, run_duration, retry_number)",
          "548:                 for retry_number in range(1, 10)",
          "549:             ]",
          "551:             for i in range(0, len(intervals) - 1):",
          "552:                 # intervals should be increasing or equals",
          "553:                 assert intervals[i] <= intervals[i + 1]",
          "554:             if poke_interval > 0:",
          "555:                 # check if the intervals are increasing after some retries when poke_interval > 0",
          "556:                 assert intervals[0] < intervals[-1]",
          "557:             else:",
          "558:                 # check if the intervals are equal after some retries when poke_interval == 0",
          "559:                 assert intervals[0] == intervals[-1]",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "948a0ce56a0619f1ed80cbf26c34428942c1d77f",
      "candidate_info": {
        "commit_hash": "948a0ce56a0619f1ed80cbf26c34428942c1d77f",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/948a0ce56a0619f1ed80cbf26c34428942c1d77f",
        "files": [
          "airflow/api_connexion/endpoints/connection_endpoint.py",
          "airflow/cli/commands/connection_command.py",
          "tests/api_connexion/endpoints/test_connection_endpoint.py",
          "tests/cli/commands/test_connection_command.py"
        ],
        "message": "Make connection id validation consistent across interface (#31282)\n\n* Make connection id validation consistent across interface\n\nrecently, we added the connection id validation\nif the user creates a connection from UI but\nthis improvement is missing in CLI and API.\nThis PR make sure that connection id validation is consistent\nacross interfaces like CLI, API, and UI.\n\n(cherry picked from commit 7bf53395f2802b9952b0e90ffe548f70c50907ad)",
        "before_after_code_files": [
          "airflow/api_connexion/endpoints/connection_endpoint.py||airflow/api_connexion/endpoints/connection_endpoint.py",
          "airflow/cli/commands/connection_command.py||airflow/cli/commands/connection_command.py",
          "tests/api_connexion/endpoints/test_connection_endpoint.py||tests/api_connexion/endpoints/test_connection_endpoint.py",
          "tests/cli/commands/test_connection_command.py||tests/cli/commands/test_connection_command.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/31796"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/api_connexion/endpoints/connection_endpoint.py||airflow/api_connexion/endpoints/connection_endpoint.py": [
          "File: airflow/api_connexion/endpoints/connection_endpoint.py -> airflow/api_connexion/endpoints/connection_endpoint.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "39: from airflow.models import Connection",
          "40: from airflow.secrets.environment_variables import CONN_ENV_PREFIX",
          "41: from airflow.security import permissions",
          "42: from airflow.utils.log.action_logger import action_event_from_permission",
          "43: from airflow.utils.session import NEW_SESSION, provide_session",
          "44: from airflow.utils.strings import get_random_string",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "42: from airflow.utils import helpers",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "157:     except ValidationError as err:",
          "158:         raise BadRequest(detail=str(err.messages))",
          "159:     conn_id = data[\"conn_id\"]",
          "160:     query = session.query(Connection)",
          "161:     connection = query.filter_by(conn_id=conn_id).first()",
          "162:     if not connection:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "161:     try:",
          "162:         helpers.validate_key(conn_id, max_length=200)",
          "163:     except Exception as e:",
          "164:         raise BadRequest(detail=str(e))",
          "",
          "---------------"
        ],
        "airflow/cli/commands/connection_command.py||airflow/cli/commands/connection_command.py": [
          "File: airflow/cli/commands/connection_command.py -> airflow/cli/commands/connection_command.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "35: from airflow.models import Connection",
          "36: from airflow.providers_manager import ProvidersManager",
          "37: from airflow.secrets.local_filesystem import load_connections_dict",
          "39: from airflow.utils.cli import suppress_logs_and_warning",
          "40: from airflow.utils.session import create_session",
          "",
          "[Removed Lines]",
          "38: from airflow.utils import cli as cli_utils, yaml",
          "",
          "[Added Lines]",
          "38: from airflow.utils import cli as cli_utils, helpers, yaml",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "203:     has_json = bool(args.conn_json)",
          "204:     has_type = bool(args.conn_type)",
          "206:     if not has_type and not (has_json or has_uri):",
          "207:         raise SystemExit(\"Must supply either conn-uri or conn-json if not supplying conn-type\")",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "206:     # Validate connection-id",
          "207:     try:",
          "208:         helpers.validate_key(args.conn_id, max_length=200)",
          "209:     except Exception as e:",
          "210:         raise SystemExit(f\"Could not create connection. {e}\")",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "313:     connections_dict = load_connections_dict(file_path)",
          "314:     with create_session() as session:",
          "315:         for conn_id, conn in connections_dict.items():",
          "316:             existing_conn_id = session.query(Connection.id).filter(Connection.conn_id == conn_id).scalar()",
          "317:             if existing_conn_id is not None:",
          "318:                 if not overwrite:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "322:             try:",
          "323:                 helpers.validate_key(conn_id, max_length=200)",
          "324:             except Exception as e:",
          "325:                 print(f\"Could not import connection. {e}\")",
          "326:                 continue",
          "",
          "---------------"
        ],
        "tests/api_connexion/endpoints/test_connection_endpoint.py||tests/api_connexion/endpoints/test_connection_endpoint.py": [
          "File: tests/api_connexion/endpoints/test_connection_endpoint.py -> tests/api_connexion/endpoints/test_connection_endpoint.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "562:             \"type\": EXCEPTIONS_LINK_MAP[400],",
          "563:         }",
          "565:     def test_post_should_respond_409_already_exist(self):",
          "566:         payload = {\"connection_id\": \"test-connection-id\", \"conn_type\": \"test_type\"}",
          "567:         response = self.client.post(",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "565:     def test_post_should_respond_400_for_invalid_conn_id(self):",
          "566:         payload = {\"connection_id\": \"****\", \"conn_type\": \"test_type\"}",
          "567:         response = self.client.post(",
          "568:             \"/api/v1/connections\", json=payload, environ_overrides={\"REMOTE_USER\": \"test\"}",
          "569:         )",
          "570:         assert response.status_code == 400",
          "571:         assert response.json == {",
          "572:             \"detail\": \"The key '****' has to be made of \"",
          "573:             \"alphanumeric characters, dashes, dots and underscores exclusively\",",
          "574:             \"status\": 400,",
          "575:             \"title\": \"Bad Request\",",
          "576:             \"type\": EXCEPTIONS_LINK_MAP[400],",
          "577:         }",
          "",
          "---------------"
        ],
        "tests/cli/commands/test_connection_command.py||tests/cli/commands/test_connection_command.py": [
          "File: tests/cli/commands/test_connection_command.py -> tests/cli/commands/test_connection_command.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "635:                 )",
          "636:             )",
          "639: class TestCliDeleteConnections:",
          "640:     parser = cli_parser.get_parser()",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "638:     def test_cli_connections_add_invalid_conn_id(self):",
          "639:         with pytest.raises(SystemExit) as e:",
          "640:             connection_command.connections_add(",
          "641:                 self.parser.parse_args([\"connections\", \"add\", \"Test$\", f\"--conn-uri={TEST_URL}\"])",
          "642:             )",
          "643:         assert (",
          "644:             e.value.args[0] == \"Could not create connection. The key 'Test$' has to be made of \"",
          "645:             \"alphanumeric characters, dashes, dots and underscores exclusively\"",
          "646:         )",
          "",
          "---------------"
        ]
      }
    }
  ]
}