{
  "cve_id": "CVE-2021-45229",
  "cve_desc": "It was discovered that the \"Trigger DAG with config\" screen was susceptible to XSS attacks via the `origin` query argument. This issue affects Apache Airflow versions 2.2.3 and below.",
  "repo": "apache/airflow",
  "patch_hash": "628aa1f99c865d97d0b1c7c76e630e43a7b8d319",
  "patch_info": {
    "commit_hash": "628aa1f99c865d97d0b1c7c76e630e43a7b8d319",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/628aa1f99c865d97d0b1c7c76e630e43a7b8d319",
    "files": [
      "airflow/www/templates/airflow/trigger.html",
      "tests/www/views/test_views_trigger_dag.py"
    ],
    "message": "Simplify trigger cancel button (#21591)\n\nCo-authored-by: Jed Cunningham <jedcunningham@apache.org>\n(cherry picked from commit 65297673a318660fba76797e50d0c06804dfcafc)",
    "before_after_code_files": [
      "airflow/www/templates/airflow/trigger.html||airflow/www/templates/airflow/trigger.html",
      "tests/www/views/test_views_trigger_dag.py||tests/www/views/test_views_trigger_dag.py"
    ]
  },
  "patch_diff": {
    "airflow/www/templates/airflow/trigger.html||airflow/www/templates/airflow/trigger.html": [
      "File: airflow/www/templates/airflow/trigger.html -> airflow/www/templates/airflow/trigger.html",
      "--- Hunk 1 ---",
      "[Context before]",
      "63:       </label>",
      "64:     </div>",
      "65:     <button type=\"submit\" class=\"btn btn-primary\">Trigger</button>",
      "67:   </form>",
      "68: {% endblock %}",
      "",
      "[Removed Lines]",
      "66:     <button type=\"button\" class=\"btn\" onclick=\"location.href = '{{ origin }}'; return false\">Cancel</button>",
      "",
      "[Added Lines]",
      "66:     <a class=\"btn\" href=\"{{ origin }}\">Cancel</a>",
      "",
      "---------------"
    ],
    "tests/www/views/test_views_trigger_dag.py||tests/www/views/test_views_trigger_dag.py": [
      "File: tests/www/views/test_views_trigger_dag.py -> tests/www/views/test_views_trigger_dag.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "133:         (\"javascript:alert(1)\", \"/home\"),",
      "134:         (\"http://google.com\", \"/home\"),",
      "135:         (\"36539'%3balert(1)%2f%2f166\", \"/home\"),",
      "136:         (",
      "137:             \"%2Ftree%3Fdag_id%3Dexample_bash_operator';alert(33)//\",",
      "138:             \"/home\",",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "136:         (",
      "137:             '\"><script>alert(99)</script><a href=\"',",
      "138:             \"&#34;&gt;&lt;script&gt;alert(99)&lt;/script&gt;&lt;a href=&#34;\",",
      "139:         ),",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "145:     test_dag_id = \"example_bash_operator\"",
      "147:     resp = admin_client.get(f'trigger?dag_id={test_dag_id}&origin={test_origin}')",
      "156: @pytest.mark.parametrize(",
      "",
      "[Removed Lines]",
      "148:     check_content_in_response(",
      "149:         '<button type=\"button\" class=\"btn\" onclick=\"location.href = \\'{}\\'; return false\">'.format(",
      "150:             expected_origin",
      "151:         ),",
      "152:         resp,",
      "153:     )",
      "",
      "[Added Lines]",
      "152:     check_content_in_response(f'<a class=\"btn\" href=\"{expected_origin}\">Cancel</a>', resp)",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "6aacc6647ead714891da2ad5cef8a51b953af44a",
      "candidate_info": {
        "commit_hash": "6aacc6647ead714891da2ad5cef8a51b953af44a",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/6aacc6647ead714891da2ad5cef8a51b953af44a",
        "files": [
          "airflow/hooks/dbapi.py",
          "airflow/operators/generic_transfer.py",
          "airflow/providers/google/cloud/hooks/workflows.py",
          "airflow/providers/google/cloud/operators/workflows.py",
          "airflow/providers/postgres/hooks/postgres.py",
          "airflow/providers/sqlite/hooks/sqlite.py",
          "scripts/in_container/run_generate_constraints.sh"
        ],
        "message": "Misc. documentation typos and language improvements (#19599)\n\n(cherry picked from commit 355dec8fea5e2ef1a9b88363f201fce4f022fef3)",
        "before_after_code_files": [
          "airflow/hooks/dbapi.py||airflow/hooks/dbapi.py",
          "airflow/operators/generic_transfer.py||airflow/operators/generic_transfer.py",
          "airflow/providers/google/cloud/hooks/workflows.py||airflow/providers/google/cloud/hooks/workflows.py",
          "airflow/providers/google/cloud/operators/workflows.py||airflow/providers/google/cloud/operators/workflows.py",
          "airflow/providers/postgres/hooks/postgres.py||airflow/providers/postgres/hooks/postgres.py",
          "airflow/providers/sqlite/hooks/sqlite.py||airflow/providers/sqlite/hooks/sqlite.py",
          "scripts/in_container/run_generate_constraints.sh||scripts/in_container/run_generate_constraints.sh"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/hooks/dbapi.py||airflow/hooks/dbapi.py": [
          "File: airflow/hooks/dbapi.py -> airflow/hooks/dbapi.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "262:     @staticmethod",
          "263:     def _generate_insert_sql(table, values, target_fields, replace, **kwargs):",
          "264:         \"\"\"",
          "266:         The REPLACE variant is specific to MySQL syntax.",
          "268:         :param table: Name of the target table",
          "",
          "[Removed Lines]",
          "265:         Static helper method that generate the INSERT SQL statement.",
          "",
          "[Added Lines]",
          "265:         Static helper method that generates the INSERT SQL statement.",
          "",
          "---------------"
        ],
        "airflow/operators/generic_transfer.py||airflow/operators/generic_transfer.py": [
          "File: airflow/operators/generic_transfer.py -> airflow/operators/generic_transfer.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "36:     :type destination_table: str",
          "37:     :param source_conn_id: source connection",
          "38:     :type source_conn_id: str",
          "40:     :type destination_conn_id: str",
          "41:     :param preoperator: sql statement or list of statements to be",
          "42:         executed prior to loading the data. (templated)",
          "",
          "[Removed Lines]",
          "39:     :param destination_conn_id: source connection",
          "",
          "[Added Lines]",
          "39:     :param destination_conn_id: destination connection",
          "",
          "---------------"
        ],
        "airflow/providers/google/cloud/hooks/workflows.py||airflow/providers/google/cloud/hooks/workflows.py": [
          "File: airflow/providers/google/cloud/hooks/workflows.py -> airflow/providers/google/cloud/hooks/workflows.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "213:         :param filter_: Filter to restrict results to specific workflows.",
          "214:         :type filter_: str",
          "217:             To specify descending order for a field, append a \"desc\" suffix.",
          "218:             If not specified, the results will be returned in an unspecified order.",
          "219:         :type order_by: str",
          "",
          "[Removed Lines]",
          "215:         :param order_by: Comma-separated list of fields that that",
          "216:             specify the order of the results. Default sorting order for a field is ascending.",
          "",
          "[Added Lines]",
          "215:         :param order_by: Comma-separated list of fields that",
          "216:             specifies the order of the results. Default sorting order for a field is ascending.",
          "",
          "---------------"
        ],
        "airflow/providers/google/cloud/operators/workflows.py||airflow/providers/google/cloud/operators/workflows.py": [
          "File: airflow/providers/google/cloud/operators/workflows.py -> airflow/providers/google/cloud/operators/workflows.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "300:     :param filter_: Filter to restrict results to specific workflows.",
          "301:     :type filter_: str",
          "304:         To specify descending order for a field, append a \"desc\" suffix.",
          "305:         If not specified, the results will be returned in an unspecified order.",
          "306:     :type order_by: str",
          "",
          "[Removed Lines]",
          "302:     :param order_by: Comma-separated list of fields that that",
          "303:         specify the order of the results. Default sorting order for a field is ascending.",
          "",
          "[Added Lines]",
          "302:     :param order_by: Comma-separated list of fields that",
          "303:         specifies the order of the results. Default sorting order for a field is ascending.",
          "",
          "---------------"
        ],
        "airflow/providers/postgres/hooks/postgres.py||airflow/providers/postgres/hooks/postgres.py": [
          "File: airflow/providers/postgres/hooks/postgres.py -> airflow/providers/postgres/hooks/postgres.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "227:         table: str, values: Tuple[str, ...], target_fields: Iterable[str], replace: bool, **kwargs",
          "228:     ) -> str:",
          "229:         \"\"\"",
          "233:         :param table: Name of the target table",
          "234:         :type table: str",
          "",
          "[Removed Lines]",
          "230:         Static helper method that generate the INSERT SQL statement.",
          "231:         The REPLACE variant is specific to MySQL syntax.",
          "",
          "[Added Lines]",
          "230:         Static helper method that generates the INSERT SQL statement.",
          "231:         The REPLACE variant is specific to PostgreSQL syntax.",
          "",
          "---------------"
        ],
        "airflow/providers/sqlite/hooks/sqlite.py||airflow/providers/sqlite/hooks/sqlite.py": [
          "File: airflow/providers/sqlite/hooks/sqlite.py -> airflow/providers/sqlite/hooks/sqlite.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "39:     @staticmethod",
          "40:     def _generate_insert_sql(table, values, target_fields, replace, **kwargs):",
          "41:         \"\"\"",
          "43:         The REPLACE variant is specific to MySQL syntax.",
          "45:         :param table: Name of the target table",
          "",
          "[Removed Lines]",
          "42:         Static helper method that generate the INSERT SQL statement.",
          "",
          "[Added Lines]",
          "42:         Static helper method that generates the INSERT SQL statement.",
          "",
          "---------------"
        ],
        "scripts/in_container/run_generate_constraints.sh||scripts/in_container/run_generate_constraints.sh": [
          "File: scripts/in_container/run_generate_constraints.sh -> scripts/in_container/run_generate_constraints.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "79: # This variant of constraints install uses the HEAD of the branch version for 'apache-airflow' but installs",
          "80: # the providers from PIP-released packages at the moment of the constraint generation.",
          "81: #",
          "83: # We also use those constraints after \"apache-airflow\" is released and the constraints are tagged with",
          "84: # \"constraints-X.Y.Z\" tag to build the production image for that version.",
          "85: #",
          "",
          "[Removed Lines]",
          "82: # Those constraints are actually those that that regular users use to install released version of Airflow.",
          "",
          "[Added Lines]",
          "82: # Those constraints are actually those that regular users use to install released version of Airflow.",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "c7236c7389ba254f8d3ad3285ccab0062c561d5c",
      "candidate_info": {
        "commit_hash": "c7236c7389ba254f8d3ad3285ccab0062c561d5c",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/c7236c7389ba254f8d3ad3285ccab0062c561d5c",
        "files": [
          "scripts/ci/testing/ci_run_single_airflow_test_in_docker.sh"
        ],
        "message": "Fix failing CI phase with unhealthy container issue (#19633)\n\nFix failing CI phase with unhealthy container issue\n\n* Add post cleanup\n* Pin pinot to stable version\n* Pin grafana to stable version\n\nCo-authored-by: Jarek Potiuk <jarek@potiuk.com>\n(cherry picked from commit fcf90c5970aaf7043b1a57d58296d7fd80d6ebf9)",
        "before_after_code_files": [
          "scripts/ci/testing/ci_run_single_airflow_test_in_docker.sh||scripts/ci/testing/ci_run_single_airflow_test_in_docker.sh"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "scripts/ci/testing/ci_run_single_airflow_test_in_docker.sh||scripts/ci/testing/ci_run_single_airflow_test_in_docker.sh": [
          "File: scripts/ci/testing/ci_run_single_airflow_test_in_docker.sh -> scripts/ci/testing/ci_run_single_airflow_test_in_docker.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "125:       \"${DOCKER_COMPOSE_LOCAL[@]}\" \\",
          "126:       --project-name \"airflow-${TEST_TYPE}-${BACKEND}\" \\",
          "127:          run airflow \"${@}\"",
          "128:     exit_code=$?",
          "129:     docker ps",
          "130:     if [[ ${exit_code} != \"0\" && ${CI} == \"true\" ]]; then",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "128:     docker ps",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "c8f492bfd81f694c0c4b66f73bcda56c1f92a4f5",
      "candidate_info": {
        "commit_hash": "c8f492bfd81f694c0c4b66f73bcda56c1f92a4f5",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/c8f492bfd81f694c0c4b66f73bcda56c1f92a4f5",
        "files": [
          "LOCAL_VIRTUALENV.rst",
          "breeze",
          "scripts/ci/libraries/_initialization.sh"
        ],
        "message": "Allow specifying extras when using breeze initialize_local_virtualenv (#19178)\n\n(cherry picked from commit f47d7b95fe68fe4c0c9db6503ddbf2ed13ea43dd)",
        "before_after_code_files": [
          "scripts/ci/libraries/_initialization.sh||scripts/ci/libraries/_initialization.sh"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "scripts/ci/libraries/_initialization.sh||scripts/ci/libraries/_initialization.sh": [
          "File: scripts/ci/libraries/_initialization.sh -> scripts/ci/libraries/_initialization.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "564:     readonly API_SERVER_PORT",
          "565: }",
          "567: function initialization::initialize_git_variables() {",
          "568:     # SHA of the commit for the current sources",
          "569:     COMMIT_SHA=\"$(git rev-parse HEAD 2>/dev/null || echo \"Unknown\")\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "567: function initialization::initialize_virtualenv_variables() {",
          "568:     # The extras to install when initializing a virtual env with breeze",
          "569:     export VIRTUALENV_EXTRAS=${VIRTUALENV_EXTRAS:=\"devel\"}",
          "570: }",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "638:     initialization::initialize_image_build_variables",
          "639:     initialization::initialize_provider_package_building",
          "640:     initialization::initialize_kubernetes_variables",
          "641:     initialization::initialize_git_variables",
          "642:     initialization::initialize_github_variables",
          "643:     initialization::initialize_test_variables",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "646:     initialization::initialize_virtualenv_variables",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "8a79d7ecfeefa946567675a3c9ee8568bcf7d64c",
      "candidate_info": {
        "commit_hash": "8a79d7ecfeefa946567675a3c9ee8568bcf7d64c",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/8a79d7ecfeefa946567675a3c9ee8568bcf7d64c",
        "files": [
          "scripts/ci/testing/ci_run_single_airflow_test_in_docker.sh"
        ],
        "message": "Fix CI tests so they correctly fail in case of error! (#19678)\n\n(cherry picked from commit 889f1571259ae5ce83fb8723ac2d10cd21dc9d50)",
        "before_after_code_files": [
          "scripts/ci/testing/ci_run_single_airflow_test_in_docker.sh||scripts/ci/testing/ci_run_single_airflow_test_in_docker.sh"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "scripts/ci/testing/ci_run_single_airflow_test_in_docker.sh||scripts/ci/testing/ci_run_single_airflow_test_in_docker.sh": [
          "File: scripts/ci/testing/ci_run_single_airflow_test_in_docker.sh -> scripts/ci/testing/ci_run_single_airflow_test_in_docker.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "125:       \"${DOCKER_COMPOSE_LOCAL[@]}\" \\",
          "126:       --project-name \"airflow-${TEST_TYPE}-${BACKEND}\" \\",
          "127:          run airflow \"${@}\"",
          "129:     exit_code=$?",
          "130:     if [[ ${exit_code} != \"0\" && ${CI} == \"true\" ]]; then",
          "131:         docker ps --all",
          "132:         local container",
          "",
          "[Removed Lines]",
          "128:     docker ps",
          "",
          "[Added Lines]",
          "129:     docker ps",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "9b5e913ef6ec8857c40546345d2fc71bac02a8a0",
      "candidate_info": {
        "commit_hash": "9b5e913ef6ec8857c40546345d2fc71bac02a8a0",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/9b5e913ef6ec8857c40546345d2fc71bac02a8a0",
        "files": [
          "Dockerfile",
          "Dockerfile.ci",
          "breeze",
          "dev/README_RELEASE_PROVIDER_PACKAGES.md",
          "docs/docker-stack/build-arg-ref.rst",
          "scripts/ci/libraries/_build_images.sh",
          "scripts/ci/libraries/_initialization.sh",
          "scripts/docker/compile_www_assets.sh",
          "scripts/docker/install_airflow.sh",
          "scripts/docker/prepare_node_modules.sh"
        ],
        "message": "Optimize dockerfiles for local rebuilds (#20238)\n\nWhen you build dockerfiles locally for development the layer\ninvalidation could happen earlier than you wanted - some of the\nvariables (like COMMIT_SHA) were affecting the cache of Docker\nin the way that they forced either invalidation of the pre-cached\npackages installed or forced to recreate assets when they were\nnot touched.\n\nSimilarly when no webpack/yarn/packages/static are modified,\nthe node asset compilation should not happen. It makes\nno sense to compile all the assets on docker rebuild when\nnone of the www files changed.\n\nIn case of CI build we can also separate node modules\npreparation and asset compilation, because node modules\nshould remain in the image anyway for incremental changes.\n\nFixes: #20259\n\nThis PR improves the experience of iterating over docker image\nbuilding by decreasing unnecesary layer invalidations.\n\n(cherry picked from commit 4620770af4550251b5139bb99185656227335f67)",
        "before_after_code_files": [
          "Dockerfile.ci||Dockerfile.ci",
          "scripts/ci/libraries/_build_images.sh||scripts/ci/libraries/_build_images.sh",
          "scripts/ci/libraries/_initialization.sh||scripts/ci/libraries/_initialization.sh",
          "scripts/docker/compile_www_assets.sh||scripts/docker/compile_www_assets.sh",
          "scripts/docker/install_airflow.sh||scripts/docker/install_airflow.sh",
          "scripts/docker/prepare_node_modules.sh||scripts/docker/prepare_node_modules.sh"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "Dockerfile.ci||Dockerfile.ci": [
          "File: Dockerfile.ci -> Dockerfile.ci",
          "--- Hunk 1 ---",
          "[Context before]",
          "99: # Only copy mysql/mssql installation scripts for now - so that changing the other",
          "100: # scripts which are needed much later will not invalidate the docker layer here",
          "101: COPY scripts/docker/install_mysql.sh scripts/docker/install_mssql.sh /scripts/docker/",
          "104:     && adduser --gecos \"First Last,RoomNumber,WorkPhone,HomePhone\" --disabled-password \\",
          "105:               --quiet \"airflow\" --home \"/home/airflow\" \\",
          "106:     && echo -e \"airflow\\nairflow\" | passwd airflow 2>&1 \\",
          "",
          "[Removed Lines]",
          "102: RUN bash -o pipefail -o errexit -o nounset -o nolog /scripts/docker/install_mysql.sh dev \\",
          "103:     && bash -o pipefail -o errexit -o nounset -o nolog /scripts/docker/install_mssql.sh \\",
          "",
          "[Added Lines]",
          "102: RUN /scripts/docker/install_mysql.sh dev && /scripts/docker/install_mssql.sh \\",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "190: RUN curl -sSL https://github.com/bats-core/bats-core/archive/v${BATS_VERSION}.tar.gz -o /tmp/bats.tgz \\",
          "191:     && tar -zxf /tmp/bats.tgz -C /tmp \\",
          "193:     && mkdir -p /opt/bats/lib/bats-support \\",
          "194:     && curl -sSL https://github.com/bats-core/bats-support/archive/v${BATS_SUPPORT_VERSION}.tar.gz -o /tmp/bats-support.tgz \\",
          "195:     && tar -zxf /tmp/bats-support.tgz -C /opt/bats/lib/bats-support --strip 1 && rm -rf /tmp/* \\",
          "",
          "[Removed Lines]",
          "192:     && bash -o pipefail -o errexit -o nounset -o nolog /tmp/bats-core-${BATS_VERSION}/install.sh /opt/bats && rm -rf \\",
          "",
          "[Added Lines]",
          "191:     && /tmp/bats-core-${BATS_VERSION}/install.sh /opt/bats && rm -rf \\",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "271: ENV EAGER_UPGRADE_ADDITIONAL_REQUIREMENTS=${EAGER_UPGRADE_ADDITIONAL_REQUIREMENTS} \\",
          "272:     UPGRADE_TO_NEWER_DEPENDENCIES=${UPGRADE_TO_NEWER_DEPENDENCIES}",
          "276: # In case of CI builds we want to pre-install main version of airflow dependencies so that",
          "277: # We do not have to always reinstall it from the scratch.",
          "278: # And is automatically reinstalled from the scratch every time patch release of python gets released",
          "",
          "[Removed Lines]",
          "274: COPY scripts/docker/*.sh scripts/docker/install_pip_version.sh /scripts/docker/",
          "",
          "[Added Lines]",
          "273: # Copy all scripts required for installation - changing any of those should lead to",
          "274: # rebuilding from here",
          "275: COPY scripts/docker/install_pip_version.sh scripts/docker/install_airflow_dependencies_from_branch_tip.sh \\",
          "276:      scripts/docker/common.sh \\",
          "277:      /scripts/docker/",
          "279: # We are first creating a venv where all python packages and .so binaries needed by those are",
          "280: # installed.",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "282: # account for removed dependencies (we do not install them in the first place)",
          "283: RUN echo -e \"\\n\\e[32mThe 'Running pip as the root user' warnings below are not valid but we can't disable them :(\\e[0m\\n\"; \\",
          "284:     echo -e \"\\n\\e[34mSee https://github.com/pypa/pip/issues/10556 for details.\\e[0m\\n\" ; \\",
          "286:     if [[ ${AIRFLOW_PRE_CACHED_PIP_PACKAGES} == \"true\" && \\",
          "287:           ${UPGRADE_TO_NEWER_DEPENDENCIES} == \"false\" ]]; then \\",
          "289:     fi",
          "291: # Generate random hex dump file so that we can determine whether it's faster to rebuild the image",
          "",
          "[Removed Lines]",
          "285:     bash -o pipefail -o errexit -o nounset -o nolog /scripts/docker/install_pip_version.sh; \\",
          "288:         bash -o pipefail -o errexit -o nounset -o nolog /scripts/docker/install_airflow_dependencies_from_branch_tip.sh; \\",
          "",
          "[Added Lines]",
          "290:     /scripts/docker/install_pip_version.sh; \\",
          "293:         /scripts/docker/install_airflow_dependencies_from_branch_tip.sh; \\",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "293: # the new image (when it is different)",
          "294: RUN head -c 30 /dev/urandom | xxd -ps >/build-cache-hash",
          "302: # Airflow sources change frequently but dependency configuration won't change that often",
          "303: # We copy setup.py and other files needed to perform setup of dependencies",
          "",
          "[Removed Lines]",
          "296: # Link dumb-init for backwards compatibility (so that older images also work)",
          "297: RUN ln -sf /usr/bin/dumb-init /usr/local/bin/dumb-init",
          "299: # Note! We are copying everything with airflow:airflow user:group even if we use root to run the scripts",
          "300: # This is fine as root user will be able to use those dirs anyway.",
          "",
          "[Added Lines]",
          "301: # Copy package.json and yarn.lock to install node modules",
          "302: # this way even if other static check files change, node modules will not need to be installed",
          "303: # we want to keep node_modules so we can do this step separately from compiling assets",
          "304: COPY airflow/www/package.json airflow/www/yarn.lock ${AIRFLOW_SOURCES}/airflow/www/",
          "305: COPY scripts/docker/prepare_node_modules.sh /scripts/docker/",
          "307: # Package JS/css for production",
          "308: RUN /scripts/docker/prepare_node_modules.sh",
          "310: # Copy all the needed www/ for assets compilation. Done as two separate COPY",
          "311: # commands so as otherwise it copies the _contents_ of static/ in to www/",
          "312: COPY airflow/www/webpack.config.js ${AIRFLOW_SOURCES}/airflow/www/",
          "313: COPY airflow/www/static ${AIRFLOW_SOURCES}/airflow/www/static/",
          "314: COPY scripts/docker/compile_www_assets.sh /scripts/docker/",
          "316: # Build artifacts without removing temporary artifacts (we will need them for incremental changes)",
          "317: # in build  mode",
          "318: RUN REMOVE_ARTIFACTS=\"false\" BUILD_TYPE=\"build\" /scripts/docker/compile_www_assets.sh",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "308: COPY airflow/__init__.py ${AIRFLOW_SOURCES}/airflow/__init__.py",
          "310: # The goal of this line is to install the dependencies from the most current setup.py from sources",
          "311: # This will be usually incremental small set of packages in CI optimized build, so it will be very fast",
          "312: # In non-CI optimized build this will install all dependencies before installing sources.",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "328: COPY scripts/docker/install_airflow.sh /scripts/docker/",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "314: # But in cron job we will install latest versions matching setup.py to see if there is no breaking change",
          "315: # and push the constraints if everything is successful",
          "316: RUN if [[ ${INSTALL_FROM_PYPI} == \"true\" ]]; then \\",
          "318:     fi",
          "328: COPY scripts/in_container/entrypoint_ci.sh /entrypoint",
          "329: RUN chmod a+x /entrypoint",
          "331: COPY scripts/docker/load.bash /opt/bats/lib/",
          "333: # Additional python deps to install",
          "334: ARG ADDITIONAL_PYTHON_DEPS=\"\"",
          "337:     if [[ -n \"${ADDITIONAL_PYTHON_DEPS}\" ]]; then \\",
          "339:     fi",
          "341: # Install autocomplete for airflow",
          "",
          "[Removed Lines]",
          "317:         bash -o pipefail -o errexit -o nounset -o nolog /scripts/docker/install_airflow.sh; \\",
          "320: # Copy all the www/ files we need to compile assets. Done as two separate COPY",
          "321: # commands so as otherwise it copies the _contents_ of static/ in to www/",
          "322: COPY airflow/www/webpack.config.js airflow/www/package.json airflow/www/yarn.lock ${AIRFLOW_SOURCES}/airflow/www/",
          "323: COPY airflow/www/static ${AIRFLOW_SOURCES}/airflow/www/static/",
          "325: # Package JS/css for production",
          "326: RUN bash -o pipefail -o errexit -o nounset -o nolog /scripts/docker/compile_www_assets.sh",
          "336: RUN bash -o pipefail -o errexit -o nounset -o nolog /scripts/docker/install_pip_version.sh; \\",
          "338:             bash -o pipefail -o errexit -o nounset -o nolog /scripts/docker/install_additional_dependencies.sh; \\",
          "",
          "[Added Lines]",
          "337:         /scripts/docker/install_airflow.sh; \\",
          "344: COPY scripts/docker/install_pip_version.sh scripts/docker/install_additional_dependencies.sh /scripts/docker/",
          "350: RUN /scripts/docker/install_pip_version.sh; \\",
          "352:         /scripts/docker/install_additional_dependencies.sh; \\",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "358: ARG COMMIT_SHA",
          "359: ARG AIRFLOW_IMAGE_DATE_CREATED",
          "362:     GUNICORN_CMD_ARGS=\"--worker-tmp-dir /dev/shm/\" \\",
          "363:     BUILD_ID=${BUILD_ID} \\",
          "364:     COMMIT_SHA=${COMMIT_SHA}",
          "366: LABEL org.apache.airflow.distro=\"debian\" \\",
          "367:   org.apache.airflow.distro.version=\"buster\" \\",
          "368:   org.apache.airflow.module=\"airflow\" \\",
          "",
          "[Removed Lines]",
          "361: ENV PATH=\"/files/bin/:/opt/airflow/scripts/in_container/bin/:${HOME}:${PATH}\" \\",
          "",
          "[Added Lines]",
          "375: ENV PATH=\"/files/bin/:/opt/airflow/scripts/in_container/bin/:${PATH}\" \\",
          "380: # This one is to workaround https://github.com/apache/airflow/issues/17546",
          "381: # issue with /usr/lib/x86_64-linux-gnu/libstdc++.so.6: cannot allocate memory in static TLS block",
          "382: # We do not yet a more \"correct\" solution to the problem but in order to avoid raising new issues",
          "383: # by users of the prod image, we implement the workaround now.",
          "384: # The side effect of this is slightly (in the range of 100s of milliseconds) slower load for any",
          "385: # binary started and a little memory used for Heap allocated by initialization of libstdc++",
          "386: # This overhead is not happening for binaries that already link dynamically libstdc++",
          "387: ENV LD_PRELOAD=\"/usr/lib/x86_64-linux-gnu/libstdc++.so.6\"",
          "389: # Link dumb-init for backwards compatibility (so that older images also work)",
          "390: RUN ln -sf /usr/bin/dumb-init /usr/local/bin/dumb-init",
          "392: EXPOSE 8080",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "387:   org.opencontainers.image.title=\"Continuous Integration Airflow Image\" \\",
          "388:   org.opencontainers.image.description=\"Installed Apache Airflow with Continuous Integration dependencies\"",
          "402: ENTRYPOINT [\"/usr/bin/dumb-init\", \"--\", \"/entrypoint\"]",
          "",
          "[Removed Lines]",
          "390: # This one is to workaround https://github.com/apache/airflow/issues/17546",
          "391: # issue with /usr/lib/x86_64-linux-gnu/libstdc++.so.6: cannot allocate memory in static TLS block",
          "392: # We do not yet a more \"correct\" solution to the problem but in order to avoid raising new issues",
          "393: # by users of the prod image, we implement the workaround now.",
          "394: # The side effect of this is slightly (in the range of 100s of milliseconds) slower load for any",
          "395: # binary started and a little memory used for Heap allocated by initialization of libstdc++",
          "396: # This overhead is not happening for binaries that already link dynamically libstdc++",
          "397: ENV LD_PRELOAD=\"/usr/lib/x86_64-linux-gnu/libstdc++.so.6\"",
          "400: EXPOSE 8080",
          "",
          "[Added Lines]",
          "419: CMD []",
          "",
          "---------------"
        ],
        "scripts/ci/libraries/_build_images.sh||scripts/ci/libraries/_build_images.sh": [
          "File: scripts/ci/libraries/_build_images.sh -> scripts/ci/libraries/_build_images.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: # pass build flags depending on the version and method of the installation (for example to",
          "21: # get proper requirement constraint files)",
          "22: function build_images::add_build_args_for_remote_install() {",
          "24:     # Airflow - those are not needed for remote install at all. Entrypoint is later overwritten by",
          "25:     EXTRA_DOCKER_PROD_BUILD_FLAGS+=(",
          "26:         \"--build-arg\" \"AIRFLOW_SOURCES_FROM=empty\"",
          "27:         \"--build-arg\" \"AIRFLOW_SOURCES_TO=/empty\"",
          "28:     )",
          "",
          "[Removed Lines]",
          "23:     # entrypoint is used as AIRFLOW_SOURCES_FROM/TO in order to avoid costly copying of all sources of",
          "",
          "[Added Lines]",
          "23:     # entrypoint is used as AIRFLOW_SOURCES_(WWW)_FROM/TO in order to avoid costly copying of all sources of",
          "26:         \"--build-arg\" \"AIRFLOW_SOURCES_WWW_FROM=empty\"",
          "27:         \"--build-arg\" \"AIRFLOW_SOURCES_WWW_TO=/empty\"",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "738:         EXTRA_DOCKER_PROD_BUILD_FLAGS=(",
          "739:             \"--build-arg\" \"AIRFLOW_SOURCES_FROM=${AIRFLOW_SOURCES_FROM}\"",
          "740:             \"--build-arg\" \"AIRFLOW_SOURCES_TO=${AIRFLOW_SOURCES_TO}\"",
          "741:             \"--build-arg\" \"AIRFLOW_INSTALLATION_METHOD=${AIRFLOW_INSTALLATION_METHOD}\"",
          "742:             \"--build-arg\" \"AIRFLOW_CONSTRAINTS_REFERENCE=${DEFAULT_CONSTRAINTS_BRANCH}\"",
          "743:         )",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "743:             \"--build-arg\" \"AIRFLOW_SOURCES_WWW_FROM=${AIRFLOW_SOURCES_WWW_FROM}\"",
          "744:             \"--build-arg\" \"AIRFLOW_SOURCES_WWW_TO=${AIRFLOW_SOURCES_WWW_TO}\"",
          "",
          "---------------"
        ],
        "scripts/ci/libraries/_initialization.sh||scripts/ci/libraries/_initialization.sh": [
          "File: scripts/ci/libraries/_initialization.sh -> scripts/ci/libraries/_initialization.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "439:     AIRFLOW_SOURCES_TO=${AIRFLOW_SOURCES_TO:=\"/empty\"}",
          "440:     export AIRFLOW_SOURCES_TO",
          "442:     # By default in scripts production docker image is installed from PyPI package",
          "443:     export AIRFLOW_INSTALLATION_METHOD=${AIRFLOW_INSTALLATION_METHOD:=\"apache-airflow\"}",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "442:     # By default no sources are copied to image",
          "443:     AIRFLOW_SOURCES_WWW_FROM=${AIRFLOW_SOURCES_WWW_FROM:=\"empty\"}",
          "444:     export AIRFLOW_SOURCES_WWW_FROM",
          "446:     AIRFLOW_SOURCES_WWW_TO=${AIRFLOW_SOURCES_WWW_TO:=\"/empty\"}",
          "447:     export AIRFLOW_SOURCES_WWW_TO",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "711:     AIRFLOW_VERSION_SPECIFICATION: '${AIRFLOW_VERSION_SPECIFICATION}'",
          "712:     AIRFLOW_SOURCES_FROM: '${AIRFLOW_SOURCES_FROM}'",
          "713:     AIRFLOW_SOURCES_TO: '${AIRFLOW_SOURCES_TO}'",
          "715: Detected GitHub environment:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "721:     AIRFLOW_SOURCES_WWW_FROM: '${AIRFLOW_SOURCES_WWW_FROM}'",
          "722:     AIRFLOW_SOURCES_WWW_TO: '${AIRFLOW_SOURCES_WWW_TO}'",
          "",
          "---------------"
        ],
        "scripts/docker/compile_www_assets.sh||scripts/docker/compile_www_assets.sh": [
          "File: scripts/docker/compile_www_assets.sh -> scripts/docker/compile_www_assets.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "19: set -euo pipefail",
          "21: BUILD_TYPE=${BUILD_TYPE=\"prod\"}",
          "23: COLOR_BLUE=$'\\e[34m'",
          "24: readonly COLOR_BLUE",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "22: REMOVE_ARTIFACTS=${REMOVE_ARTIFACTS=\"true\"}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "28: # Installs additional dependencies passed as Argument to the Docker build command",
          "29: function compile_www_assets() {",
          "30:     echo",
          "32:     echo",
          "36:     local www_dir",
          "37:     if [[ ${AIRFLOW_INSTALLATION_METHOD=} == \".\" ]]; then",
          "38:         # In case we are building from sources in production image, we should build the assets",
          "",
          "[Removed Lines]",
          "31:     echo \"${COLOR_BLUE}Compiling www assets${COLOR_RESET}\"",
          "33:     local md5sum_file",
          "34:     md5sum_file=\"static/dist/sum.md5\"",
          "35:     readonly md5sum_file",
          "",
          "[Added Lines]",
          "32:     echo \"${COLOR_BLUE}Compiling www assets: running yarn ${BUILD_TYPE}${COLOR_RESET}\"",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "42:     fi",
          "43:     pushd ${www_dir} || exit 1",
          "44:     set +e",
          "55:     yarn run \"${BUILD_TYPE}\" 2>/tmp/out-yarn-run.txt",
          "56:     res=$?",
          "57:     if [[ ${res} != 0 ]]; then",
          "58:         >&2 echo",
          "60:         >&2 echo",
          "61:         >&2 cat /tmp/out-yarn-run.txt && rm -rf /tmp/out-yarn-run.txt",
          "62:         exit 1",
          "63:     fi",
          "64:     rm -f /tmp/out-yarn-run.txt",
          "65:     set -e",
          "66:     find package.json yarn.lock static/css static/js -type f | sort | xargs md5sum > \"${md5sum_file}\"",
          "69:     popd || exit 1",
          "70: }",
          "",
          "[Removed Lines]",
          "45:     yarn install --frozen-lockfile --no-cache 2>/tmp/out-yarn-install.txt",
          "46:     local res=$?",
          "47:     if [[ ${res} != 0 ]]; then",
          "48:         >&2 echo",
          "49:         >&2 echo \"Error when running yarn install:\"",
          "50:         >&2 echo",
          "51:         >&2 cat /tmp/out-yarn-install.txt && rm -f /tmp/out-yarn-install.txt",
          "52:         exit 1",
          "53:     fi",
          "54:     rm -f /tmp/out-yarn-install.txt",
          "59:         >&2 echo \"Error when running yarn install:\"",
          "67:     rm -rf \"${www_dir}/node_modules\"",
          "68:     rm -vf \"${www_dir}\"/{package.json,yarn.lock,.eslintignore,.eslintrc,.stylelintignore,.stylelintrc,compile_assets.sh,webpack.config.js}",
          "",
          "[Added Lines]",
          "47:         >&2 echo \"Error when running yarn run:\"",
          "54:     local md5sum_file",
          "55:     md5sum_file=\"static/dist/sum.md5\"",
          "56:     readonly md5sum_file",
          "58:     if [[ ${REMOVE_ARTIFACTS} == \"true\" ]]; then",
          "59:         echo",
          "60:         echo \"${COLOR_BLUE}Removing generated node modules${COLOR_RESET}\"",
          "61:         echo",
          "62:         rm -rf \"${www_dir}/node_modules\"",
          "63:         rm -vf \"${www_dir}\"/{package.json,yarn.lock,.eslintignore,.eslintrc,.stylelintignore,.stylelintrc,compile_assets.sh,webpack.config.js}",
          "64:     else",
          "65:         echo",
          "66:         echo \"${COLOR_BLUE}Leaving generated node modules${COLOR_RESET}\"",
          "67:         echo",
          "68:     fi",
          "",
          "---------------"
        ],
        "scripts/docker/install_airflow.sh||scripts/docker/install_airflow.sh": [
          "File: scripts/docker/install_airflow.sh -> scripts/docker/install_airflow.sh"
        ],
        "scripts/docker/prepare_node_modules.sh||scripts/docker/prepare_node_modules.sh": [
          "File: scripts/docker/prepare_node_modules.sh -> scripts/docker/prepare_node_modules.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: #!/usr/bin/env bash",
          "2: # Licensed to the Apache Software Foundation (ASF) under one",
          "3: # or more contributor license agreements.  See the NOTICE file",
          "4: # distributed with this work for additional information",
          "5: # regarding copyright ownership.  The ASF licenses this file",
          "6: # to you under the Apache License, Version 2.0 (the",
          "7: # \"License\"); you may not use this file except in compliance",
          "8: # with the License.  You may obtain a copy of the License at",
          "9: #",
          "10: #   http://www.apache.org/licenses/LICENSE-2.0",
          "11: #",
          "12: # Unless required by applicable law or agreed to in writing,",
          "13: # software distributed under the License is distributed on an",
          "14: # \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
          "15: # KIND, either express or implied.  See the License for the",
          "16: # specific language governing permissions and limitations",
          "17: # under the License.",
          "18: # shellcheck disable=SC2086",
          "19: set -euo pipefail",
          "21: COLOR_BLUE=$'\\e[34m'",
          "22: readonly COLOR_BLUE",
          "23: COLOR_RESET=$'\\e[0m'",
          "24: readonly COLOR_RESET",
          "26: # Prepares node modules needed to compile WWW assets",
          "27: function prepare_node_modules() {",
          "28:     echo",
          "29:     echo \"${COLOR_BLUE}Preparing node modules${COLOR_RESET}\"",
          "30:     echo",
          "31:     local www_dir",
          "32:     if [[ ${AIRFLOW_INSTALLATION_METHOD=} == \".\" ]]; then",
          "33:         # In case we are building from sources in production image, we should build the assets",
          "34:         www_dir=\"${AIRFLOW_SOURCES_TO=${AIRFLOW_SOURCES}}/airflow/www\"",
          "35:     else",
          "36:         www_dir=\"$(python -m site --user-site)/airflow/www\"",
          "37:     fi",
          "38:     pushd ${www_dir} || exit 1",
          "39:     set +e",
          "40:     yarn install --frozen-lockfile --no-cache 2>/tmp/out-yarn-install.txt",
          "41:     local res=$?",
          "42:     if [[ ${res} != 0 ]]; then",
          "43:         >&2 echo",
          "44:         >&2 echo \"Error when running yarn install:\"",
          "45:         >&2 echo",
          "46:         >&2 cat /tmp/out-yarn-install.txt && rm -f /tmp/out-yarn-install.txt",
          "47:         exit 1",
          "48:     fi",
          "49:     rm -f /tmp/out-yarn-install.txt",
          "50:     popd || exit 1",
          "51: }",
          "53: prepare_node_modules",
          "",
          "---------------"
        ]
      }
    }
  ]
}