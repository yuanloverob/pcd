{
  "cve_id": "CVE-2023-50944",
  "cve_desc": "Apache Airflow, versions before 2.8.1, have a vulnerability that allows an authenticated user to access the source code of a DAG to which they don't have access.\u00a0This vulnerability is considered low since it requires an authenticated user to exploit it. Users are recommended to upgrade to version 2.8.1, which fixes this issue.",
  "repo": "apache/airflow",
  "patch_hash": "8d76538d6e105947272b000581c6fabec20146b1",
  "patch_info": {
    "commit_hash": "8d76538d6e105947272b000581c6fabec20146b1",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/8d76538d6e105947272b000581c6fabec20146b1",
    "files": [
      "airflow/api_connexion/endpoints/dag_source_endpoint.py",
      "airflow/models/dagcode.py",
      "tests/api_connexion/endpoints/test_dag_source_endpoint.py"
    ],
    "message": "Check DAG read permission before accessing DAG code (#36257)\n\n(cherry picked from commit 30ea37e0d247ce54c2d25b115e807fdb0074d795)",
    "before_after_code_files": [
      "airflow/api_connexion/endpoints/dag_source_endpoint.py||airflow/api_connexion/endpoints/dag_source_endpoint.py",
      "airflow/models/dagcode.py||airflow/models/dagcode.py",
      "tests/api_connexion/endpoints/test_dag_source_endpoint.py||tests/api_connexion/endpoints/test_dag_source_endpoint.py"
    ]
  },
  "patch_diff": {
    "airflow/api_connexion/endpoints/dag_source_endpoint.py||airflow/api_connexion/endpoints/dag_source_endpoint.py": [
      "File: airflow/api_connexion/endpoints/dag_source_endpoint.py -> airflow/api_connexion/endpoints/dag_source_endpoint.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "17: from __future__ import annotations",
      "19: from http import HTTPStatus",
      "21: from flask import Response, current_app, request",
      "22: from itsdangerous import BadSignature, URLSafeSerializer",
      "24: from airflow.api_connexion import security",
      "26: from airflow.api_connexion.schemas.dag_source_schema import dag_source_schema",
      "27: from airflow.auth.managers.models.resource_details import DagAccessEntity",
      "28: from airflow.models.dagcode import DagCode",
      "31: @security.requires_access_dag(\"GET\", DagAccessEntity.CODE)",
      "33:     \"\"\"Get source code using file token.\"\"\"",
      "34:     secret_key = current_app.config[\"SECRET_KEY\"]",
      "35:     auth_s = URLSafeSerializer(secret_key)",
      "36:     try:",
      "37:         path = auth_s.loads(file_token)",
      "39:     except (BadSignature, FileNotFoundError):",
      "40:         raise NotFound(\"Dag source not found\")",
      "",
      "[Removed Lines]",
      "25: from airflow.api_connexion.exceptions import NotFound",
      "32: def get_dag_source(*, file_token: str) -> Response:",
      "38:         dag_source = DagCode.code(path)",
      "",
      "[Added Lines]",
      "20: from typing import TYPE_CHECKING",
      "26: from airflow.api_connexion.exceptions import NotFound, PermissionDenied",
      "28: from airflow.api_connexion.security import get_readable_dags",
      "30: from airflow.models.dag import DagModel",
      "32: from airflow.utils.session import NEW_SESSION, provide_session",
      "34: if TYPE_CHECKING:",
      "35:     from sqlalchemy.orm import Session",
      "39: @provide_session",
      "40: def get_dag_source(*, file_token: str, session: Session = NEW_SESSION) -> Response:",
      "46:         dag_ids = session.query(DagModel.dag_id).filter(DagModel.fileloc == path).all()",
      "47:         readable_dags = get_readable_dags()",
      "48:         # Check if user has read access to all the DAGs defined in the file",
      "49:         if any(dag_id[0] not in readable_dags for dag_id in dag_ids):",
      "50:             raise PermissionDenied()",
      "51:         dag_source = DagCode.code(path, session=session)",
      "",
      "---------------"
    ],
    "airflow/models/dagcode.py||airflow/models/dagcode.py": [
      "File: airflow/models/dagcode.py -> airflow/models/dagcode.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "177:         return cls.code(fileloc)",
      "179:     @classmethod",
      "181:         \"\"\"Return source code for this DagCode object.",
      "183:         :return: source code as string",
      "184:         \"\"\"",
      "187:     @staticmethod",
      "188:     def _get_code_from_file(fileloc):",
      "",
      "[Removed Lines]",
      "180:     def code(cls, fileloc) -> str:",
      "185:         return cls._get_code_from_db(fileloc)",
      "",
      "[Added Lines]",
      "180:     @provide_session",
      "181:     def code(cls, fileloc, session: Session = NEW_SESSION) -> str:",
      "186:         return cls._get_code_from_db(fileloc, session)",
      "",
      "---------------"
    ],
    "tests/api_connexion/endpoints/test_dag_source_endpoint.py||tests/api_connexion/endpoints/test_dag_source_endpoint.py": [
      "File: tests/api_connexion/endpoints/test_dag_source_endpoint.py -> tests/api_connexion/endpoints/test_dag_source_endpoint.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "35: ROOT_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), os.pardir, os.pardir))",
      "36: EXAMPLE_DAG_FILE = os.path.join(\"airflow\", \"example_dags\", \"example_bash_operator.py\")",
      "39: @pytest.fixture(scope=\"module\")",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "37: EXAMPLE_DAG_ID = \"example_bash_operator\"",
      "38: TEST_DAG_ID = \"latest_only\"",
      "39: NOT_READABLE_DAG_ID = \"latest_only_with_trigger\"",
      "40: TEST_MULTIPLE_DAGS_ID = \"dataset_produces_1\"",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "45:         role_name=\"Test\",",
      "46:         permissions=[(permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG_CODE)],  # type: ignore",
      "47:     )",
      "48:     create_user(app, username=\"test_no_permissions\", role_name=\"TestNoPermissions\")  # type: ignore",
      "50:     yield app",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "52:     app.appbuilder.sm.sync_perm_for_dag(  # type: ignore",
      "53:         TEST_DAG_ID,",
      "54:         access_control={\"Test\": [permissions.ACTION_CAN_READ]},",
      "55:     )",
      "56:     app.appbuilder.sm.sync_perm_for_dag(  # type: ignore",
      "57:         EXAMPLE_DAG_ID,",
      "58:         access_control={\"Test\": [permissions.ACTION_CAN_READ]},",
      "59:     )",
      "60:     app.appbuilder.sm.sync_perm_for_dag(  # type: ignore",
      "61:         TEST_MULTIPLE_DAGS_ID,",
      "62:         access_control={\"Test\": [permissions.ACTION_CAN_READ]},",
      "63:     )",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "80:     def test_should_respond_200_text(self, url_safe_serializer):",
      "81:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
      "82:         dagbag.sync_to_db()",
      "87:         response = self.client.get(",
      "88:             url, headers={\"Accept\": \"text/plain\"}, environ_overrides={\"REMOTE_USER\": \"test\"}",
      "89:         )",
      "",
      "[Removed Lines]",
      "83:         first_dag: DAG = next(iter(dagbag.dags.values()))",
      "84:         dag_docstring = self._get_dag_file_docstring(first_dag.fileloc)",
      "86:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(first_dag.fileloc)}\"",
      "",
      "[Added Lines]",
      "99:         test_dag: DAG = dagbag.dags[TEST_DAG_ID]",
      "100:         dag_docstring = self._get_dag_file_docstring(test_dag.fileloc)",
      "102:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(test_dag.fileloc)}\"",
      "",
      "---------------",
      "--- Hunk 4 ---",
      "[Context before]",
      "95:     def test_should_respond_200_json(self, url_safe_serializer):",
      "96:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
      "97:         dagbag.sync_to_db()",
      "102:         response = self.client.get(",
      "103:             url, headers={\"Accept\": \"application/json\"}, environ_overrides={\"REMOTE_USER\": \"test\"}",
      "104:         )",
      "",
      "[Removed Lines]",
      "98:         first_dag: DAG = next(iter(dagbag.dags.values()))",
      "99:         dag_docstring = self._get_dag_file_docstring(first_dag.fileloc)",
      "101:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(first_dag.fileloc)}\"",
      "",
      "[Added Lines]",
      "114:         test_dag: DAG = dagbag.dags[TEST_DAG_ID]",
      "115:         dag_docstring = self._get_dag_file_docstring(test_dag.fileloc)",
      "117:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(test_dag.fileloc)}\"",
      "",
      "---------------",
      "--- Hunk 5 ---",
      "[Context before]",
      "110:     def test_should_respond_406(self, url_safe_serializer):",
      "111:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
      "112:         dagbag.sync_to_db()",
      "116:         response = self.client.get(",
      "117:             url, headers={\"Accept\": \"image/webp\"}, environ_overrides={\"REMOTE_USER\": \"test\"}",
      "118:         )",
      "",
      "[Removed Lines]",
      "113:         first_dag: DAG = next(iter(dagbag.dags.values()))",
      "115:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(first_dag.fileloc)}\"",
      "",
      "[Added Lines]",
      "129:         test_dag: DAG = dagbag.dags[TEST_DAG_ID]",
      "131:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(test_dag.fileloc)}\"",
      "",
      "---------------",
      "--- Hunk 6 ---",
      "[Context before]",
      "151:             environ_overrides={\"REMOTE_USER\": \"test_no_permissions\"},",
      "152:         )",
      "153:         assert response.status_code == 403",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "171:     def test_should_respond_403_not_readable(self, url_safe_serializer):",
      "172:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
      "173:         dagbag.sync_to_db()",
      "174:         dag: DAG = dagbag.dags[NOT_READABLE_DAG_ID]",
      "176:         response = self.client.get(",
      "177:             f\"/api/v1/dagSources/{url_safe_serializer.dumps(dag.fileloc)}\",",
      "178:             headers={\"Accept\": \"text/plain\"},",
      "179:             environ_overrides={\"REMOTE_USER\": \"test\"},",
      "180:         )",
      "181:         read_dag = self.client.get(",
      "182:             f\"/api/v1/dags/{NOT_READABLE_DAG_ID}\",",
      "183:             environ_overrides={\"REMOTE_USER\": \"test\"},",
      "184:         )",
      "185:         assert response.status_code == 403",
      "186:         assert read_dag.status_code == 403",
      "188:     def test_should_respond_403_some_dags_not_readable_in_the_file(self, url_safe_serializer):",
      "189:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
      "190:         dagbag.sync_to_db()",
      "191:         dag: DAG = dagbag.dags[TEST_MULTIPLE_DAGS_ID]",
      "193:         response = self.client.get(",
      "194:             f\"/api/v1/dagSources/{url_safe_serializer.dumps(dag.fileloc)}\",",
      "195:             headers={\"Accept\": \"text/plain\"},",
      "196:             environ_overrides={\"REMOTE_USER\": \"test\"},",
      "197:         )",
      "199:         read_dag = self.client.get(",
      "200:             f\"/api/v1/dags/{TEST_MULTIPLE_DAGS_ID}\",",
      "201:             environ_overrides={\"REMOTE_USER\": \"test\"},",
      "202:         )",
      "203:         assert response.status_code == 403",
      "204:         assert read_dag.status_code == 200",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "2c2d71c43b2d6f1de9efe011223c5fefceb68621",
      "candidate_info": {
        "commit_hash": "2c2d71c43b2d6f1de9efe011223c5fefceb68621",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/2c2d71c43b2d6f1de9efe011223c5fefceb68621",
        "files": [
          "tests/models/test_mappedoperator.py"
        ],
        "message": "Fix tests to adopt changes in Jinja 3.1.3 (#36731)\n\n(cherry picked from commit 8b33e25e502c18f42dd3f76c95fefd78fb3a04a3)",
        "before_after_code_files": [
          "tests/models/test_mappedoperator.py||tests/models/test_mappedoperator.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "tests/models/test_mappedoperator.py||tests/models/test_mappedoperator.py": [
          "File: tests/models/test_mappedoperator.py -> tests/models/test_mappedoperator.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "21: from collections import defaultdict",
          "22: from datetime import timedelta",
          "23: from typing import TYPE_CHECKING",
          "25: from unittest.mock import patch",
          "27: import pendulum",
          "",
          "[Removed Lines]",
          "24: from unittest import mock",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "405:     assert t.expand_input.value == {\"params\": [{\"c\": \"x\"}, {\"d\": 1}]}",
          "409:     with set_current_task_instance_session(session=session):",
          "411:         class MyOperator(BaseOperator):",
          "",
          "[Removed Lines]",
          "408: def test_mapped_render_template_fields_validating_operator(dag_maker, session):",
          "",
          "[Added Lines]",
          "407: def test_mapped_render_template_fields_validating_operator(dag_maker, session, tmp_path):",
          "408:     file_template_dir = tmp_path / \"path\" / \"to\"",
          "409:     file_template_dir.mkdir(parents=True, exist_ok=True)",
          "410:     file_template = file_template_dir / \"file.ext\"",
          "411:     file_template.write_text(\"loaded data\")",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "427:         def execute(self, context):",
          "428:             pass",
          "431:             task1 = BaseOperator(task_id=\"op1\")",
          "432:             output1 = task1.output",
          "433:             mapped = MyOperator.partial(",
          "",
          "[Removed Lines]",
          "430:         with dag_maker(session=session):",
          "",
          "[Added Lines]",
          "434:         with dag_maker(session=session, template_searchpath=tmp_path.__fspath__()):",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "455:         mapped_ti.map_index = 0",
          "457:         assert isinstance(mapped_ti.task, MappedOperator)",
          "462:         assert isinstance(mapped_ti.task, MyOperator)",
          "464:         assert mapped_ti.task.partial_template == \"a\", \"Should be templated!\"",
          "",
          "[Removed Lines]",
          "458:         with patch(\"builtins.open\", mock.mock_open(read_data=b\"loaded data\")), patch(",
          "459:             \"os.path.isfile\", return_value=True",
          "460:         ), patch(\"os.path.getmtime\", return_value=0):",
          "461:             mapped.render_template_fields(context=mapped_ti.get_template_context(session=session))",
          "",
          "[Added Lines]",
          "462:         mapped.render_template_fields(context=mapped_ti.get_template_context(session=session))",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "468:         assert mapped_ti.task.file_template == \"loaded data\", \"Should be templated!\"",
          "472:     with set_current_task_instance_session(session=session):",
          "474:         class MyOperator(BaseOperator):",
          "",
          "[Removed Lines]",
          "471: def test_mapped_expand_kwargs_render_template_fields_validating_operator(dag_maker, session):",
          "",
          "[Added Lines]",
          "472: def test_mapped_expand_kwargs_render_template_fields_validating_operator(dag_maker, session, tmp_path):",
          "473:     file_template_dir = tmp_path / \"path\" / \"to\"",
          "474:     file_template_dir.mkdir(parents=True, exist_ok=True)",
          "475:     file_template = file_template_dir / \"file.ext\"",
          "476:     file_template.write_text(\"loaded data\")",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "490:             def execute(self, context):",
          "491:                 pass",
          "494:             mapped = MyOperator.partial(",
          "495:                 task_id=\"a\", partial_template=\"{{ ti.task_id }}\", partial_static=\"{{ ti.task_id }}\"",
          "496:             ).expand_kwargs(",
          "",
          "[Removed Lines]",
          "493:         with dag_maker(session=session):",
          "",
          "[Added Lines]",
          "499:         with dag_maker(session=session, template_searchpath=tmp_path.__fspath__()):",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "502:         mapped_ti: TaskInstance = dr.get_task_instance(mapped.task_id, session=session, map_index=0)",
          "504:         assert isinstance(mapped_ti.task, MappedOperator)",
          "509:         assert isinstance(mapped_ti.task, MyOperator)",
          "511:         assert mapped_ti.task.partial_template == \"a\", \"Should be templated!\"",
          "",
          "[Removed Lines]",
          "505:         with patch(\"builtins.open\", mock.mock_open(read_data=b\"loaded data\")), patch(",
          "506:             \"os.path.isfile\", return_value=True",
          "507:         ), patch(\"os.path.getmtime\", return_value=0):",
          "508:             mapped.render_template_fields(context=mapped_ti.get_template_context(session=session))",
          "",
          "[Added Lines]",
          "511:         mapped.render_template_fields(context=mapped_ti.get_template_context(session=session))",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "dd344fd961bdc81efb7698652a3c148fb918eafb",
      "candidate_info": {
        "commit_hash": "dd344fd961bdc81efb7698652a3c148fb918eafb",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/dd344fd961bdc81efb7698652a3c148fb918eafb",
        "files": [
          "README.md",
          "dev/breeze/src/airflow_breeze/global_constants.py",
          "generated/PYPI_README.md",
          "images/breeze/output_k8s_configure-cluster.svg",
          "images/breeze/output_k8s_configure-cluster.txt",
          "images/breeze/output_k8s_create-cluster.svg",
          "images/breeze/output_k8s_create-cluster.txt",
          "images/breeze/output_k8s_delete-cluster.svg",
          "images/breeze/output_k8s_delete-cluster.txt",
          "images/breeze/output_k8s_deploy-airflow.svg",
          "images/breeze/output_k8s_deploy-airflow.txt",
          "images/breeze/output_k8s_k9s.svg",
          "images/breeze/output_k8s_k9s.txt",
          "images/breeze/output_k8s_logs.svg",
          "images/breeze/output_k8s_logs.txt",
          "images/breeze/output_k8s_run-complete-tests.svg",
          "images/breeze/output_k8s_run-complete-tests.txt",
          "images/breeze/output_k8s_shell.svg",
          "images/breeze/output_k8s_shell.txt",
          "images/breeze/output_k8s_status.svg",
          "images/breeze/output_k8s_status.txt",
          "images/breeze/output_k8s_tests.svg",
          "images/breeze/output_k8s_tests.txt",
          "images/breeze/output_k8s_upload-k8s-image.svg",
          "images/breeze/output_k8s_upload-k8s-image.txt"
        ],
        "message": "feat: K8S 1.29 support (#36527)\n\n(cherry picked from commit 7e26f79d4b9f0dccf0d39db3d40efbf08aa8d083)",
        "before_after_code_files": [
          "dev/breeze/src/airflow_breeze/global_constants.py||dev/breeze/src/airflow_breeze/global_constants.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/breeze/src/airflow_breeze/global_constants.py||dev/breeze/src/airflow_breeze/global_constants.py": [
          "File: dev/breeze/src/airflow_breeze/global_constants.py -> dev/breeze/src/airflow_breeze/global_constants.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "74: #   - https://endoflife.date/amazon-eks",
          "75: #   - https://endoflife.date/azure-kubernetes-service",
          "76: #   - https://endoflife.date/google-kubernetes-engine",
          "78: ALLOWED_EXECUTORS = [",
          "79:     \"LocalExecutor\",",
          "80:     \"KubernetesExecutor\",",
          "",
          "[Removed Lines]",
          "77: ALLOWED_KUBERNETES_VERSIONS = [\"v1.25.11\", \"v1.26.6\", \"v1.27.3\", \"v1.28.0\"]",
          "",
          "[Added Lines]",
          "77: ALLOWED_KUBERNETES_VERSIONS = [\"v1.25.11\", \"v1.26.6\", \"v1.27.3\", \"v1.28.0\", \"v1.29.0\"]",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "874227c2cf8b61dfc3d7d4a91d5441411404266c",
      "candidate_info": {
        "commit_hash": "874227c2cf8b61dfc3d7d4a91d5441411404266c",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/874227c2cf8b61dfc3d7d4a91d5441411404266c",
        "files": [
          "airflow/providers/amazon/provider.yaml",
          "airflow/providers/apache/hive/provider.yaml",
          "airflow/providers/common/sql/provider.yaml",
          "airflow/providers/exasol/provider.yaml",
          "airflow/providers/google/provider.yaml",
          "airflow/providers/presto/provider.yaml",
          "airflow/providers/salesforce/provider.yaml",
          "airflow/providers/trino/provider.yaml",
          "airflow/providers/weaviate/provider.yaml",
          "generated/provider_dependencies.json",
          "setup.py"
        ],
        "message": "Set min pandas dependency to 1.2.5 for all providers and airflow (#36698)\n\nWe had some REALLY old minimum version of Pandas set for all our\npandas dependency - Pandas 0.17.1 has been released in 2015 (!)\n\nLooking at the dependency tree - most of our dependencies had\n> 1.2.5 set - which is more than reasonable limit as Pandas 1.2.5\nhad been released in June 2021 - so more than 2.5 years ago.\n\nThis limit bump further helps us to limit the pip backtracking\nthat starts happening in certain situations.\n\nExtracted from: #36537\n\n(cherry picked from commit ecb2c9f24d1364642604c14f0deb681ab4894135)",
        "before_after_code_files": [
          "setup.py||setup.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "setup.py||setup.py": [
          "File: setup.py -> setup.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "348: leveldb = [\"plyvel\"]",
          "349: otel = [\"opentelemetry-exporter-prometheus\"]",
          "350: pandas = [",
          "352: ]",
          "353: password = [",
          "354:     \"bcrypt>=2.0.0\",",
          "",
          "[Removed Lines]",
          "351:     \"pandas>=0.17.1\",",
          "",
          "[Added Lines]",
          "351:     \"pandas>=1.2.5\",",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "f0b18881a6917b292617a66c1efcfdc7119fe447",
      "candidate_info": {
        "commit_hash": "f0b18881a6917b292617a66c1efcfdc7119fe447",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/f0b18881a6917b292617a66c1efcfdc7119fe447",
        "files": [
          ".pre-commit-config.yaml",
          ".rat-excludes",
          "STATIC_CODE_CHECKS.rst",
          "airflow/reproducible_build.yaml",
          "dev/README_RELEASE_AIRFLOW.md",
          "dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "dev/breeze/src/airflow_breeze/pre_commit_ids.py",
          "images/breeze/output_static-checks.svg",
          "images/breeze/output_static-checks.txt",
          "scripts/ci/pre_commit/pre_commit_update_source_date_epoch.py",
          "scripts/in_container/run_prepare_airflow_packages.py"
        ],
        "message": "Add support for reproducible build date epoch for Airflow releases (#36726)\n\nHatch has built-in support for reproducible builds, however it\nuses a hard-coded 2020 date to generate the reproducible binaries,\nwhich produces whl, tar.gz files that contain file dates that are\npretty old. This might be confusing for anyone who is looking at\nthe file contents and timestamp inside.\n\nThis PR adds support (similar to provider approach) to store current\nreproducible date in the repository - so that it can be committed\nand tagged together with Airflow sources. It is updated fully\nautomaticallly by pre-commit whenever release notes change, which\nbasically means that whenever release notes are update just\nbefore release, the reproducible date is updated to current date.\n\nFor now we only check if the packages produced by hatchling\nbuild are reproducible.\n\n(cherry picked from commit a2d6c389f69034c526554b3291874dc4d66c4529)",
        "before_after_code_files": [
          "dev/breeze/src/airflow_breeze/commands/release_management_commands.py||dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "dev/breeze/src/airflow_breeze/pre_commit_ids.py||dev/breeze/src/airflow_breeze/pre_commit_ids.py",
          "scripts/ci/pre_commit/pre_commit_update_source_date_epoch.py||scripts/ci/pre_commit/pre_commit_update_source_date_epoch.py",
          "scripts/in_container/run_prepare_airflow_packages.py||scripts/in_container/run_prepare_airflow_packages.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/breeze/src/airflow_breeze/commands/release_management_commands.py||dev/breeze/src/airflow_breeze/commands/release_management_commands.py": [
          "File: dev/breeze/src/airflow_breeze/commands/release_management_commands.py -> dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "209: RICH_VERSION = \"13.7.0\"",
          "210: NODE_VERSION = \"21.2.0\"",
          "211: PRE_COMMIT_VERSION = \"3.5.0\"",
          "213: AIRFLOW_BUILD_DOCKERFILE = f\"\"\"",
          "214: FROM python:{DEFAULT_PYTHON_MAJOR_MINOR_VERSION}-slim-{ALLOWED_DEBIAN_VERSIONS[0]}",
          "215: RUN apt-get update && apt-get install -y --no-install-recommends git",
          "218: COPY . /opt/airflow",
          "219: \"\"\"",
          "",
          "[Removed Lines]",
          "216: RUN pip install pip=={AIRFLOW_PIP_VERSION} hatch==1.9.1 \\",
          "217:   gitpython=={GITPYTHON_VERSION} rich=={RICH_VERSION} pre-commit=={PRE_COMMIT_VERSION}",
          "",
          "[Added Lines]",
          "212: PYYAML_VERSION = \"6.0.1\"",
          "217: RUN pip install pip=={AIRFLOW_PIP_VERSION} hatch==1.9.1 pyyaml=={PYYAML_VERSION}\\",
          "218:  gitpython=={GITPYTHON_VERSION} rich=={RICH_VERSION} pre-commit=={PRE_COMMIT_VERSION}",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/pre_commit_ids.py||dev/breeze/src/airflow_breeze/pre_commit_ids.py": [
          "File: dev/breeze/src/airflow_breeze/pre_commit_ids.py -> dev/breeze/src/airflow_breeze/pre_commit_ids.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "129:     \"update-local-yml-file\",",
          "130:     \"update-migration-references\",",
          "131:     \"update-providers-dependencies\",",
          "132:     \"update-spelling-wordlist-to-be-sorted\",",
          "133:     \"update-supported-versions\",",
          "134:     \"update-vendored-in-k8s-json-schema\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "132:     \"update-reproducible-source-date-epoch\",",
          "",
          "---------------"
        ],
        "scripts/ci/pre_commit/pre_commit_update_source_date_epoch.py||scripts/ci/pre_commit/pre_commit_update_source_date_epoch.py": [
          "File: scripts/ci/pre_commit/pre_commit_update_source_date_epoch.py -> scripts/ci/pre_commit/pre_commit_update_source_date_epoch.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: #!/usr/bin/env python",
          "2: # Licensed to the Apache Software Foundation (ASF) under one",
          "3: # or more contributor license agreements.  See the NOTICE file",
          "4: # distributed with this work for additional information",
          "5: # regarding copyright ownership.  The ASF licenses this file",
          "6: # to you under the Apache License, Version 2.0 (the",
          "7: # \"License\"); you may not use this file except in compliance",
          "8: # with the License.  You may obtain a copy of the License at",
          "9: #",
          "10: #   http://www.apache.org/licenses/LICENSE-2.0",
          "11: #",
          "12: # Unless required by applicable law or agreed to in writing,",
          "13: # software distributed under the License is distributed on an",
          "14: # \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
          "15: # KIND, either express or implied.  See the License for the",
          "16: # specific language governing permissions and limitations",
          "17: # under the License.",
          "18: from __future__ import annotations",
          "20: import sys",
          "21: from hashlib import md5",
          "22: from pathlib import Path",
          "23: from time import time",
          "25: import yaml",
          "27: sys.path.insert(0, str(Path(__file__).parent.resolve()))  # make sure common_precommit_utils is importable",
          "29: from common_precommit_utils import AIRFLOW_SOURCES_ROOT_PATH",
          "31: RELEASE_NOTES_FILE_PATH = AIRFLOW_SOURCES_ROOT_PATH / \"RELEASE_NOTES.rst\"",
          "32: REPRODUCIBLE_BUILD_FILE = AIRFLOW_SOURCES_ROOT_PATH / \"airflow\" / \"reproducible_build.yaml\"",
          "34: if __name__ == \"__main__\":",
          "35:     hash_md5 = md5()",
          "36:     hash_md5.update(RELEASE_NOTES_FILE_PATH.read_bytes())",
          "37:     release_notes_hash = hash_md5.hexdigest()",
          "38:     reproducible_build_text = REPRODUCIBLE_BUILD_FILE.read_text()",
          "39:     reproducible_build = yaml.safe_load(reproducible_build_text)",
          "40:     old_hash = reproducible_build[\"release-notes-hash\"]",
          "41:     if release_notes_hash != old_hash:",
          "42:         # Replace the hash in the file",
          "43:         reproducible_build[\"release-notes-hash\"] = release_notes_hash",
          "44:         reproducible_build[\"source-date-epoch\"] = int(time())",
          "45:     REPRODUCIBLE_BUILD_FILE.write_text(yaml.dump(reproducible_build))",
          "",
          "---------------"
        ],
        "scripts/in_container/run_prepare_airflow_packages.py||scripts/in_container/run_prepare_airflow_packages.py": [
          "File: scripts/in_container/run_prepare_airflow_packages.py -> scripts/in_container/run_prepare_airflow_packages.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "26: from pathlib import Path",
          "27: from shutil import rmtree",
          "29: from rich.console import Console",
          "31: console = Console(color_system=\"standard\", width=200)",
          "33: AIRFLOW_SOURCES_ROOT = Path(__file__).parents[2].resolve()",
          "34: AIRFLOW_INIT_FILE = AIRFLOW_SOURCES_ROOT / \"airflow\" / \"__init__.py\"",
          "35: WWW_DIRECTORY = AIRFLOW_SOURCES_ROOT / \"airflow\" / \"www\"",
          "36: VERSION_SUFFIX = os.environ.get(\"VERSION_SUFFIX_FOR_PYPI\", \"\")",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "29: import yaml",
          "35: REPRODUCIBLE_BUILD_FILE = AIRFLOW_SOURCES_ROOT / \"airflow\" / \"reproducible_build.yaml\"",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "81:     if package_format in [\"both\", \"sdist\"]:",
          "82:         build_command.extend([\"-t\", \"sdist\"])",
          "84:     console.print(f\"[bright_blue]Building packages: {package_format}\\n\")",
          "87:     if build_process.returncode != 0:",
          "88:         console.print(\"[red]Error building Airflow packages\")",
          "",
          "[Removed Lines]",
          "85:     build_process = subprocess.run(build_command, capture_output=False, cwd=AIRFLOW_SOURCES_ROOT)",
          "",
          "[Added Lines]",
          "86:     reproducible_date = yaml.safe_load(REPRODUCIBLE_BUILD_FILE.read_text())[\"source-date-epoch\"]",
          "88:     envcopy = os.environ.copy()",
          "89:     envcopy[\"SOURCE_DATE_EPOCH\"] = str(reproducible_date)",
          "91:     build_process = subprocess.run(",
          "92:         build_command,",
          "93:         capture_output=False,",
          "94:         cwd=AIRFLOW_SOURCES_ROOT,",
          "95:         env=envcopy,",
          "96:     )",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "c730881000f479f6b5f2844e3e2beebf75980bf3",
      "candidate_info": {
        "commit_hash": "c730881000f479f6b5f2844e3e2beebf75980bf3",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/c730881000f479f6b5f2844e3e2beebf75980bf3",
        "files": [
          "BREEZE.rst",
          "dev/breeze/README.md",
          "dev/breeze/pyproject.toml",
          "scripts/ci/install_breeze.sh"
        ],
        "message": "Upgrade to latest versions of `pip` and `pipx` in CI runners (#36646)\n\nThe CI runners did not have latest version of `pip` and `pipx`. This\nchange updates the installation scripts to fix `pip` to the same\nversion as in the CI image and down-binds pipx to 1.4.1 which is\nrecently released bugfix version with better logging and installation\ninstructions.\n\n(cherry picked from commit 75bc05ce1f53de112f7eee7be524a26f2a3f6845)",
        "before_after_code_files": [
          "scripts/ci/install_breeze.sh||scripts/ci/install_breeze.sh"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "scripts/ci/install_breeze.sh||scripts/ci/install_breeze.sh": [
          "File: scripts/ci/install_breeze.sh -> scripts/ci/install_breeze.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: cd \"$( dirname \"${BASH_SOURCE[0]}\" )/../../\"",
          "23: python -m pipx install --editable ./dev/breeze/ --force",
          "24: echo '/home/runner/.local/bin' >> \"${GITHUB_PATH}\"",
          "",
          "[Removed Lines]",
          "22: python -m pip install \"pipx>=1.2.1\"",
          "",
          "[Added Lines]",
          "22: python -m pip install --upgrade pip==23.3.2",
          "23: python -m pip install \"pipx>=1.4.1\"",
          "",
          "---------------"
        ]
      }
    }
  ]
}