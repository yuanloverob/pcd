{
  "cve_id": "CVE-2022-31777",
  "cve_desc": "A stored cross-site scripting (XSS) vulnerability in Apache Spark 3.2.1 and earlier, and 3.3.0, allows remote attackers to execute arbitrary JavaScript in the web browser of a user, by including a malicious payload into the logs which would be returned in logs rendered in the UI.",
  "repo": "apache/spark",
  "patch_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
  "patch_info": {
    "commit_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
    "repo": "apache/spark",
    "commit_url": "https://github.com/apache/spark/commit/ad90195de56688ce0898691eb9d04297ab0871ad",
    "files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ],
    "message": "[SPARK-39505][UI] Escape log content rendered in UI\n\n### What changes were proposed in this pull request?\n\nEscape log content rendered to the UI.\n\n### Why are the changes needed?\n\nLog content may contain reserved characters or other code in the log and be misinterpreted in the UI as HTML.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nExisting tests\n\nCloses #36902 from srowen/LogViewEscape.\n\nAuthored-by: Sean Owen <srowen@gmail.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
    "before_after_code_files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ]
  },
  "patch_diff": {
    "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js": [
      "File: core/src/main/resources/org/apache/spark/ui/static/log-view.js -> core/src/main/resources/org/apache/spark/ui/static/log-view.js",
      "--- Hunk 1 ---",
      "[Context before]",
      "85:       if (retStartByte == 0) {",
      "86:         disableMoreButton();",
      "87:       }",
      "90:       curLogLength = curLogLength + (startByte - retStartByte);",
      "91:       startByte = retStartByte;",
      "",
      "[Removed Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(cleanData);",
      "",
      "[Added Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(document.createTextNode(cleanData));",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "115:             var retLogLength = dataInfo[2];",
      "117:             var cleanData = data.substring(newlineIndex + 1);",
      "120:             curLogLength = curLogLength + (retEndByte - retStartByte);",
      "121:             endByte = retEndByte;",
      "",
      "[Removed Lines]",
      "118:             $(\"pre\", \".log-content\").append(cleanData);",
      "",
      "[Added Lines]",
      "118:             $(\"pre\", \".log-content\").append(document.createTextNode(cleanData));",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "dfb668d2541c5e15c7b41dfa74c9dea7291fe9e1",
      "candidate_info": {
        "commit_hash": "dfb668d2541c5e15c7b41dfa74c9dea7291fe9e1",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/dfb668d2541c5e15c7b41dfa74c9dea7291fe9e1",
        "files": [
          "dev/deps/spark-deps-hadoop-2-hive-2.3",
          "dev/deps/spark-deps-hadoop-3-hive-2.3",
          "pom.xml"
        ],
        "message": "[SPARK-38866][BUILD] Update ORC to 1.7.4\n\n### What changes were proposed in this pull request?\nThis PR aims to update ORC to version 1.7.4.\n\n### Why are the changes needed?\nThis will bring the following bug fixes.\n- https://github.com/apache/orc/milestone/7?closed=1\n\n### Does this PR introduce _any_ user-facing change?\nNo.\n\n### How was this patch tested?\nPass the CIs.\n\nCloses #36153 from williamhyun/orc174RC0.\n\nAuthored-by: William Hyun <william@apache.org>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>\n(cherry picked from commit 7caf487c76abfdc76fc79a3bd4787d2e6c8034eb)\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
        "before_after_code_files": [
          "dev/deps/spark-deps-hadoop-2-hive-2.3||dev/deps/spark-deps-hadoop-2-hive-2.3",
          "dev/deps/spark-deps-hadoop-3-hive-2.3||dev/deps/spark-deps-hadoop-3-hive-2.3"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/deps/spark-deps-hadoop-2-hive-2.3||dev/deps/spark-deps-hadoop-2-hive-2.3": [
          "File: dev/deps/spark-deps-hadoop-2-hive-2.3 -> dev/deps/spark-deps-hadoop-2-hive-2.3",
          "--- Hunk 1 ---",
          "[Context before]",
          "219: okhttp/3.12.12//okhttp-3.12.12.jar",
          "220: okio/1.14.0//okio-1.14.0.jar",
          "221: opencsv/2.3//opencsv-2.3.jar",
          "225: oro/2.0.8//oro-2.0.8.jar",
          "226: osgi-resource-locator/1.0.3//osgi-resource-locator-1.0.3.jar",
          "227: paranamer/2.8//paranamer-2.8.jar",
          "",
          "[Removed Lines]",
          "222: orc-core/1.7.3//orc-core-1.7.3.jar",
          "223: orc-mapreduce/1.7.3//orc-mapreduce-1.7.3.jar",
          "224: orc-shims/1.7.3//orc-shims-1.7.3.jar",
          "",
          "[Added Lines]",
          "222: orc-core/1.7.4//orc-core-1.7.4.jar",
          "223: orc-mapreduce/1.7.4//orc-mapreduce-1.7.4.jar",
          "224: orc-shims/1.7.4//orc-shims-1.7.4.jar",
          "",
          "---------------"
        ],
        "dev/deps/spark-deps-hadoop-3-hive-2.3||dev/deps/spark-deps-hadoop-3-hive-2.3": [
          "File: dev/deps/spark-deps-hadoop-3-hive-2.3 -> dev/deps/spark-deps-hadoop-3-hive-2.3",
          "--- Hunk 1 ---",
          "[Context before]",
          "208: opentracing-api/0.33.0//opentracing-api-0.33.0.jar",
          "209: opentracing-noop/0.33.0//opentracing-noop-0.33.0.jar",
          "210: opentracing-util/0.33.0//opentracing-util-0.33.0.jar",
          "214: oro/2.0.8//oro-2.0.8.jar",
          "215: osgi-resource-locator/1.0.3//osgi-resource-locator-1.0.3.jar",
          "216: paranamer/2.8//paranamer-2.8.jar",
          "",
          "[Removed Lines]",
          "211: orc-core/1.7.3//orc-core-1.7.3.jar",
          "212: orc-mapreduce/1.7.3//orc-mapreduce-1.7.3.jar",
          "213: orc-shims/1.7.3//orc-shims-1.7.3.jar",
          "",
          "[Added Lines]",
          "211: orc-core/1.7.4//orc-core-1.7.4.jar",
          "212: orc-mapreduce/1.7.4//orc-mapreduce-1.7.4.jar",
          "213: orc-shims/1.7.4//orc-shims-1.7.4.jar",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "b25276f438543082ea2bb5841f75ac49fde73f81",
      "candidate_info": {
        "commit_hash": "b25276f438543082ea2bb5841f75ac49fde73f81",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/b25276f438543082ea2bb5841f75ac49fde73f81",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/complexTypeExtractors.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/DateTimeUtils.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/IntervalUtils.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryErrorsBase.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/types/Decimal.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/types/numerics.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/AnsiCastSuiteBase.scala",
          "sql/core/src/test/resources/sql-tests/inputs/ansi/map.sql",
          "sql/core/src/test/resources/sql-tests/results/ansi/map.sql.out"
        ],
        "message": "[SPARK-39015][SQL][3.3] Remove the usage of toSQLValue(v) without an explicit type\n\n### What changes were proposed in this pull request?\n\nThis PR is a backport of https://github.com/apache/spark/pull/36351\n\nThis PR proposes to remove the the usage of `toSQLValue(v)` without an explicit type.\n\n`Literal(v)` is intended to be used from end-users so it cannot handle our internal types such as `UTF8String` and `ArrayBasedMapData`. Using this method can lead to unexpected error messages such as:\n\n```\nCaused by: org.apache.spark.SparkRuntimeException: [UNSUPPORTED_FEATURE] The feature is not supported: literal for 'hair' of class org.apache.spark.unsafe.types.UTF8String.\n  at org.apache.spark.sql.errors.QueryExecutionErrors$.literalTypeUnsupportedError(QueryExecutionErrors.scala:241)\n  at org.apache.spark.sql.catalyst.expressions.Literal$.apply(literals.scala:99)\n  at org.apache.spark.sql.errors.QueryErrorsBase.toSQLValue(QueryErrorsBase.scala:45)\n  ...\n```\n\nSince It is impossible to have the corresponding data type from the internal types as one type can map to multiple external types (e.g., `Long` for `Timestamp`, `TimestampNTZ`, and `LongType`), the removal approach was taken.\n\n### Why are the changes needed?\n\nTo provide the error messages as intended.\n\n### Does this PR introduce _any_ user-facing change?\n\nYes.\n\n```scala\nimport org.apache.spark.sql.Row\nimport org.apache.spark.sql.types.StructType\nimport org.apache.spark.sql.types.StringType\nimport org.apache.spark.sql.types.DataTypes\n\nval arrayStructureData = Seq(\nRow(Map(\"hair\"->\"black\", \"eye\"->\"brown\")),\nRow(Map(\"hair\"->\"blond\", \"eye\"->\"blue\")),\nRow(Map()))\n\nval mapType  = DataTypes.createMapType(StringType, StringType)\n\nval arrayStructureSchema = new StructType().add(\"properties\", mapType)\n\nval mapTypeDF = spark.createDataFrame(\n    spark.sparkContext.parallelize(arrayStructureData),arrayStructureSchema)\n\nspark.conf.set(\"spark.sql.ansi.enabled\", true)\nmapTypeDF.selectExpr(\"element_at(properties, 'hair')\").show\n```\n\nBefore:\n\n```\nCaused by: org.apache.spark.SparkRuntimeException: [UNSUPPORTED_FEATURE] The feature is not supported: literal for 'hair' of class org.apache.spark.unsafe.types.UTF8String.\n  at org.apache.spark.sql.errors.QueryExecutionErrors$.literalTypeUnsupportedError(QueryExecutionErrors.scala:241)\n  at org.apache.spark.sql.catalyst.expressions.Literal$.apply(literals.scala:99)\n  at org.apache.spark.sql.errors.QueryErrorsBase.toSQLValue(QueryErrorsBase.scala:45)\n  ...\n```\n\nAfter:\n\n```\nCaused by: org.apache.spark.SparkNoSuchElementException: [MAP_KEY_DOES_NOT_EXIST] Key 'hair' does not exist. To return NULL instead, use 'try_element_at'. If necessary set spark.sql.ansi.enabled to false to bypass this error.\n== SQL(line 1, position 0) ==\nelement_at(properties, 'hair')\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n```\n\n### How was this patch tested?\n\nUnittest was added. Otherwise, existing test cases should cover.\n\nCloses #36375 from HyukjinKwon/SPARK-39015-3.3.\n\nLead-authored-by: Hyukjin Kwon <gurwls223@apache.org>\nCo-authored-by: Hyukjin Kwon <gurwls223@gmail.com>\nSigned-off-by: Max Gekk <max.gekk@gmail.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/complexTypeExtractors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/complexTypeExtractors.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/DateTimeUtils.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/DateTimeUtils.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/IntervalUtils.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/IntervalUtils.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryErrorsBase.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryErrorsBase.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/types/Decimal.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/types/Decimal.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/types/numerics.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/types/numerics.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/AnsiCastSuiteBase.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/AnsiCastSuiteBase.scala",
          "sql/core/src/test/resources/sql-tests/inputs/ansi/map.sql||sql/core/src/test/resources/sql-tests/inputs/ansi/map.sql"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "668:         if (longValue == longValue.toInt) {",
          "669:           longValue.toInt",
          "670:         } else {",
          "672:         }",
          "673:       })",
          "674:     case TimestampType =>",
          "",
          "[Removed Lines]",
          "671:           throw QueryExecutionErrors.castingCauseOverflowError(t, IntegerType)",
          "",
          "[Added Lines]",
          "671:           throw QueryExecutionErrors.castingCauseOverflowError(t, from, IntegerType)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "704:         if (longValue == longValue.toShort) {",
          "705:           longValue.toShort",
          "706:         } else {",
          "708:         }",
          "709:       })",
          "710:     case TimestampType =>",
          "",
          "[Removed Lines]",
          "707:           throw QueryExecutionErrors.castingCauseOverflowError(t, ShortType)",
          "",
          "[Added Lines]",
          "707:           throw QueryExecutionErrors.castingCauseOverflowError(t, from, ShortType)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "715:           x.exactNumeric.asInstanceOf[Numeric[Any]].toInt(b)",
          "716:         } catch {",
          "717:           case _: ArithmeticException =>",
          "719:         }",
          "720:         if (intValue == intValue.toShort) {",
          "721:           intValue.toShort",
          "722:         } else {",
          "724:         }",
          "725:     case x: NumericType =>",
          "726:       b => x.numeric.asInstanceOf[Numeric[Any]].toInt(b).toShort",
          "",
          "[Removed Lines]",
          "718:             throw QueryExecutionErrors.castingCauseOverflowError(b, ShortType)",
          "723:           throw QueryExecutionErrors.castingCauseOverflowError(b, ShortType)",
          "",
          "[Added Lines]",
          "718:             throw QueryExecutionErrors.castingCauseOverflowError(b, from, ShortType)",
          "723:           throw QueryExecutionErrors.castingCauseOverflowError(b, from, ShortType)",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "751:         if (longValue == longValue.toByte) {",
          "752:           longValue.toByte",
          "753:         } else {",
          "755:         }",
          "756:       })",
          "757:     case TimestampType =>",
          "",
          "[Removed Lines]",
          "754:           throw QueryExecutionErrors.castingCauseOverflowError(t, ByteType)",
          "",
          "[Added Lines]",
          "754:           throw QueryExecutionErrors.castingCauseOverflowError(t, from, ByteType)",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "762:           x.exactNumeric.asInstanceOf[Numeric[Any]].toInt(b)",
          "763:         } catch {",
          "764:           case _: ArithmeticException =>",
          "766:         }",
          "767:         if (intValue == intValue.toByte) {",
          "768:           intValue.toByte",
          "769:         } else {",
          "771:         }",
          "772:     case x: NumericType =>",
          "773:       b => x.numeric.asInstanceOf[Numeric[Any]].toInt(b).toByte",
          "",
          "[Removed Lines]",
          "765:             throw QueryExecutionErrors.castingCauseOverflowError(b, ByteType)",
          "770:           throw QueryExecutionErrors.castingCauseOverflowError(b, ByteType)",
          "",
          "[Added Lines]",
          "765:             throw QueryExecutionErrors.castingCauseOverflowError(b, from, ByteType)",
          "770:           throw QueryExecutionErrors.castingCauseOverflowError(b, from, ByteType)",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "1656:   private[this] def castTimestampToIntegralTypeCode(",
          "1657:       ctx: CodegenContext,",
          "1658:       integralType: String,",
          "1660:     if (ansiEnabled) {",
          "1661:       val longValue = ctx.freshName(\"longValue\")",
          "1663:       (c, evPrim, _) =>",
          "1664:         code\"\"\"",
          "1665:           long $longValue = ${timestampToLongCode(c)};",
          "1666:           if ($longValue == ($integralType) $longValue) {",
          "1667:             $evPrim = ($integralType) $longValue;",
          "1668:           } else {",
          "1670:           }",
          "1671:         \"\"\"",
          "1672:     } else {",
          "",
          "[Removed Lines]",
          "1659:       dataType: DataType): CastFunction = {",
          "1662:       val dt = ctx.addReferenceObj(\"dataType\", dataType, dataType.getClass.getName)",
          "1669:             throw QueryExecutionErrors.castingCauseOverflowError($c, $dt);",
          "",
          "[Added Lines]",
          "1659:       from: DataType,",
          "1660:       to: DataType): CastFunction = {",
          "1663:       val fromDt = ctx.addReferenceObj(\"from\", from, from.getClass.getName)",
          "1664:       val toDt = ctx.addReferenceObj(\"to\", to, to.getClass.getName)",
          "1671:             throw QueryExecutionErrors.castingCauseOverflowError($c, $fromDt, $toDt);",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "1707:   private[this] def castIntegralTypeToIntegralTypeExactCode(",
          "1708:       ctx: CodegenContext,",
          "1709:       integralType: String,",
          "1711:     assert(ansiEnabled)",
          "1713:     (c, evPrim, _) =>",
          "1714:       code\"\"\"",
          "1715:         if ($c == ($integralType) $c) {",
          "1716:           $evPrim = ($integralType) $c;",
          "1717:         } else {",
          "1719:         }",
          "1720:       \"\"\"",
          "1721:   }",
          "",
          "[Removed Lines]",
          "1710:       dataType: DataType): CastFunction = {",
          "1712:     val dt = ctx.addReferenceObj(\"dataType\", dataType, dataType.getClass.getName)",
          "1718:           throw QueryExecutionErrors.castingCauseOverflowError($c, $dt);",
          "",
          "[Added Lines]",
          "1712:       from: DataType,",
          "1713:       to: DataType): CastFunction = {",
          "1715:     val fromDt = ctx.addReferenceObj(\"from\", from, from.getClass.getName)",
          "1716:     val toDt = ctx.addReferenceObj(\"to\", to, to.getClass.getName)",
          "1722:           throw QueryExecutionErrors.castingCauseOverflowError($c, $fromDt, $toDt);",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "1734:   private[this] def castFractionToIntegralTypeCode(",
          "1735:       ctx: CodegenContext,",
          "1736:       integralType: String,",
          "1738:     assert(ansiEnabled)",
          "1739:     val (min, max) = lowerAndUpperBound(integralType)",
          "1740:     val mathClass = classOf[Math].getName",
          "",
          "[Removed Lines]",
          "1737:       dataType: DataType): CastFunction = {",
          "1741:     val dt = ctx.addReferenceObj(\"dataType\", dataType, dataType.getClass.getName)",
          "",
          "[Added Lines]",
          "1741:       from: DataType,",
          "1742:       to: DataType): CastFunction = {",
          "1746:     val fromDt = ctx.addReferenceObj(\"from\", from, from.getClass.getName)",
          "1747:     val toDt = ctx.addReferenceObj(\"to\", to, to.getClass.getName)",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "1749:         if ($mathClass.floor($c) <= $max && $mathClass.ceil($c) >= $min) {",
          "1750:           $evPrim = ($integralType) $c;",
          "1751:         } else {",
          "1753:         }",
          "1754:       \"\"\"",
          "1755:   }",
          "",
          "[Removed Lines]",
          "1752:           throw QueryExecutionErrors.castingCauseOverflowError($c, $dt);",
          "",
          "[Added Lines]",
          "1758:           throw QueryExecutionErrors.castingCauseOverflowError($c, $fromDt, $toDt);",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "1775:       (c, evPrim, evNull) => code\"$evPrim = $c ? (byte) 1 : (byte) 0;\"",
          "1776:     case DateType =>",
          "1777:       (c, evPrim, evNull) => code\"$evNull = true;\"",
          "1779:     case DecimalType() => castDecimalToIntegralTypeCode(\"byte\")",
          "1780:     case ShortType | IntegerType | LongType if ansiEnabled =>",
          "1782:     case FloatType | DoubleType if ansiEnabled =>",
          "1784:     case x: NumericType =>",
          "1785:       (c, evPrim, evNull) => code\"$evPrim = (byte) $c;\"",
          "1786:     case x: DayTimeIntervalType =>",
          "",
          "[Removed Lines]",
          "1778:     case TimestampType => castTimestampToIntegralTypeCode(ctx, \"byte\", ByteType)",
          "1781:       castIntegralTypeToIntegralTypeExactCode(ctx, \"byte\", ByteType)",
          "1783:       castFractionToIntegralTypeCode(ctx, \"byte\", ByteType)",
          "",
          "[Added Lines]",
          "1784:     case TimestampType => castTimestampToIntegralTypeCode(ctx, \"byte\", from, ByteType)",
          "1787:       castIntegralTypeToIntegralTypeExactCode(ctx, \"byte\", from, ByteType)",
          "1789:       castFractionToIntegralTypeCode(ctx, \"byte\", from, ByteType)",
          "",
          "---------------",
          "--- Hunk 11 ---",
          "[Context before]",
          "1812:       (c, evPrim, evNull) => code\"$evPrim = $c ? (short) 1 : (short) 0;\"",
          "1813:     case DateType =>",
          "1814:       (c, evPrim, evNull) => code\"$evNull = true;\"",
          "1816:     case DecimalType() => castDecimalToIntegralTypeCode(\"short\")",
          "1817:     case IntegerType | LongType if ansiEnabled =>",
          "1819:     case FloatType | DoubleType if ansiEnabled =>",
          "1821:     case x: NumericType =>",
          "1822:       (c, evPrim, evNull) => code\"$evPrim = (short) $c;\"",
          "1823:     case x: DayTimeIntervalType =>",
          "",
          "[Removed Lines]",
          "1815:     case TimestampType => castTimestampToIntegralTypeCode(ctx, \"short\", ShortType)",
          "1818:       castIntegralTypeToIntegralTypeExactCode(ctx, \"short\", ShortType)",
          "1820:       castFractionToIntegralTypeCode(ctx, \"short\", ShortType)",
          "",
          "[Added Lines]",
          "1821:     case TimestampType => castTimestampToIntegralTypeCode(ctx, \"short\", from, ShortType)",
          "1824:       castIntegralTypeToIntegralTypeExactCode(ctx, \"short\", from, ShortType)",
          "1826:       castFractionToIntegralTypeCode(ctx, \"short\", from, ShortType)",
          "",
          "---------------",
          "--- Hunk 12 ---",
          "[Context before]",
          "1847:       (c, evPrim, evNull) => code\"$evPrim = $c ? 1 : 0;\"",
          "1848:     case DateType =>",
          "1849:       (c, evPrim, evNull) => code\"$evNull = true;\"",
          "1851:     case DecimalType() => castDecimalToIntegralTypeCode(\"int\")",
          "1852:     case LongType if ansiEnabled =>",
          "1854:     case FloatType | DoubleType if ansiEnabled =>",
          "1856:     case x: NumericType =>",
          "1857:       (c, evPrim, evNull) => code\"$evPrim = (int) $c;\"",
          "1858:     case x: DayTimeIntervalType =>",
          "",
          "[Removed Lines]",
          "1850:     case TimestampType => castTimestampToIntegralTypeCode(ctx, \"int\", IntegerType)",
          "1853:       castIntegralTypeToIntegralTypeExactCode(ctx, \"int\", IntegerType)",
          "1855:       castFractionToIntegralTypeCode(ctx, \"int\", IntegerType)",
          "",
          "[Added Lines]",
          "1856:     case TimestampType => castTimestampToIntegralTypeCode(ctx, \"int\", from, IntegerType)",
          "1859:       castIntegralTypeToIntegralTypeExactCode(ctx, \"int\", from, IntegerType)",
          "1861:       castFractionToIntegralTypeCode(ctx, \"int\", from, IntegerType)",
          "",
          "---------------",
          "--- Hunk 13 ---",
          "[Context before]",
          "1886:       (c, evPrim, evNull) => code\"$evPrim = (long) ${timestampToLongCode(c)};\"",
          "1887:     case DecimalType() => castDecimalToIntegralTypeCode(\"long\")",
          "1888:     case FloatType | DoubleType if ansiEnabled =>",
          "1890:     case x: NumericType =>",
          "1891:       (c, evPrim, evNull) => code\"$evPrim = (long) $c;\"",
          "1892:     case x: DayTimeIntervalType =>",
          "",
          "[Removed Lines]",
          "1889:       castFractionToIntegralTypeCode(ctx, \"long\", LongType)",
          "",
          "[Added Lines]",
          "1895:       castFractionToIntegralTypeCode(ctx, \"long\", from, LongType)",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/complexTypeExtractors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/complexTypeExtractors.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/complexTypeExtractors.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/complexTypeExtractors.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "366:     if (!found) {",
          "367:       if (failOnError) {",
          "369:       } else {",
          "370:         null",
          "371:       }",
          "",
          "[Removed Lines]",
          "368:         throw QueryExecutionErrors.mapKeyNotExistError(ordinal, origin.context)",
          "",
          "[Added Lines]",
          "368:         throw QueryExecutionErrors.mapKeyNotExistError(ordinal, keyType, origin.context)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "400:     val keyJavaType = CodeGenerator.javaType(keyType)",
          "401:     lazy val errorContext = ctx.addReferenceObj(\"errCtx\", origin.context)",
          "402:     nullSafeCodeGen(ctx, ev, (eval1, eval2) => {",
          "403:       val keyNotFoundBranch = if (failOnError) {",
          "405:       } else {",
          "406:         s\"${ev.isNull} = true;\"",
          "407:       }",
          "",
          "[Removed Lines]",
          "404:         s\"throw QueryExecutionErrors.mapKeyNotExistError($eval2, $errorContext);\"",
          "",
          "[Added Lines]",
          "402:     val keyDt = ctx.addReferenceObj(\"keyType\", keyType, keyType.getClass.getName)",
          "405:         s\"throw QueryExecutionErrors.mapKeyNotExistError($eval2, $keyDt, $errorContext);\"",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/DateTimeUtils.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/DateTimeUtils.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/DateTimeUtils.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/DateTimeUtils.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "30: import org.apache.spark.sql.catalyst.util.DateTimeConstants._",
          "31: import org.apache.spark.sql.catalyst.util.RebaseDateTime._",
          "32: import org.apache.spark.sql.errors.QueryExecutionErrors",
          "34: import org.apache.spark.unsafe.types.{CalendarInterval, UTF8String}",
          "",
          "[Removed Lines]",
          "33: import org.apache.spark.sql.types.{DateType, Decimal, DoubleExactNumeric, TimestampNTZType, TimestampType}",
          "",
          "[Added Lines]",
          "33: import org.apache.spark.sql.types.{DateType, Decimal, DoubleExactNumeric, DoubleType, StringType, TimestampNTZType, TimestampType}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "451:   def stringToTimestampAnsi(s: UTF8String, timeZoneId: ZoneId, errorContext: String = \"\"): Long = {",
          "452:     stringToTimestamp(s, timeZoneId).getOrElse {",
          "454:     }",
          "455:   }",
          "457:   def doubleToTimestampAnsi(d: Double, errorContext: String): Long = {",
          "458:     if (d.isNaN || d.isInfinite) {",
          "460:     } else {",
          "461:       DoubleExactNumeric.toLong(d * MICROS_PER_SECOND)",
          "462:     }",
          "",
          "[Removed Lines]",
          "453:       throw QueryExecutionErrors.cannotCastToDateTimeError(s, TimestampType, errorContext)",
          "459:       throw QueryExecutionErrors.cannotCastToDateTimeError(d, TimestampType, errorContext)",
          "",
          "[Added Lines]",
          "453:       throw QueryExecutionErrors.cannotCastToDateTimeError(",
          "454:         s, StringType, TimestampType, errorContext)",
          "460:       throw QueryExecutionErrors.cannotCastToDateTimeError(",
          "461:         d, DoubleType, TimestampType, errorContext)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "506:   def stringToTimestampWithoutTimeZoneAnsi(s: UTF8String, errorContext: String): Long = {",
          "507:     stringToTimestampWithoutTimeZone(s, true).getOrElse {",
          "509:     }",
          "510:   }",
          "",
          "[Removed Lines]",
          "508:       throw QueryExecutionErrors.cannotCastToDateTimeError(s, TimestampNTZType, errorContext)",
          "",
          "[Added Lines]",
          "510:       throw QueryExecutionErrors.cannotCastToDateTimeError(",
          "511:         s, StringType, TimestampNTZType, errorContext)",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "624:   def stringToDateAnsi(s: UTF8String, errorContext: String = \"\"): Int = {",
          "625:     stringToDate(s).getOrElse {",
          "627:     }",
          "628:   }",
          "",
          "[Removed Lines]",
          "626:       throw QueryExecutionErrors.cannotCastToDateTimeError(s, DateType, errorContext)",
          "",
          "[Added Lines]",
          "629:       throw QueryExecutionErrors.cannotCastToDateTimeError(",
          "630:         s, StringType, DateType, errorContext)",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/IntervalUtils.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/IntervalUtils.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/IntervalUtils.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/IntervalUtils.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "1263:           Math.multiplyExact(v, MONTHS_PER_YEAR)",
          "1264:         } catch {",
          "1265:           case _: ArithmeticException =>",
          "1267:         }",
          "1268:       case MONTH => v",
          "1269:     }",
          "",
          "[Removed Lines]",
          "1266:             throw QueryExecutionErrors.castingCauseOverflowError(v, YM(endField))",
          "",
          "[Added Lines]",
          "1266:             throw QueryExecutionErrors.castingCauseOverflowError(v, IntegerType, YM(endField))",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1272:   def longToYearMonthInterval(v: Long, endField: Byte): Int = {",
          "1273:     val vInt = v.toInt",
          "1274:     if (v != vInt) {",
          "1276:     }",
          "1277:     intToYearMonthInterval(vInt, endField)",
          "1278:   }",
          "",
          "[Removed Lines]",
          "1275:       throw QueryExecutionErrors.castingCauseOverflowError(v, YM(endField))",
          "",
          "[Added Lines]",
          "1275:       throw QueryExecutionErrors.castingCauseOverflowError(v, LongType, YM(endField))",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "1289:     val vShort = vInt.toShort",
          "1290:     if (vInt != vShort) {",
          "1291:       throw QueryExecutionErrors.castingCauseOverflowError(",
          "1293:         ShortType)",
          "1294:     }",
          "1295:     vShort",
          "",
          "[Removed Lines]",
          "1292:         Literal(v, YearMonthIntervalType(startField, endField)),",
          "",
          "[Added Lines]",
          "1292:         v,",
          "1293:         YearMonthIntervalType(startField, endField),",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "1300:     val vByte = vInt.toByte",
          "1301:     if (vInt != vByte) {",
          "1302:       throw QueryExecutionErrors.castingCauseOverflowError(",
          "1304:         ByteType)",
          "1305:     }",
          "1306:     vByte",
          "",
          "[Removed Lines]",
          "1303:         Literal(v, YearMonthIntervalType(startField, endField)),",
          "",
          "[Added Lines]",
          "1304:         v,",
          "1305:         YearMonthIntervalType(startField, endField),",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "1313:           Math.multiplyExact(v, MICROS_PER_DAY)",
          "1314:         } catch {",
          "1315:           case _: ArithmeticException =>",
          "1317:         }",
          "1318:       case HOUR => v * MICROS_PER_HOUR",
          "1319:       case MINUTE => v * MICROS_PER_MINUTE",
          "",
          "[Removed Lines]",
          "1316:             throw QueryExecutionErrors.castingCauseOverflowError(v, DT(endField))",
          "",
          "[Added Lines]",
          "1318:             throw QueryExecutionErrors.castingCauseOverflowError(v, IntegerType, DT(endField))",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "1331:       }",
          "1332:     } catch {",
          "1333:       case _: ArithmeticException =>",
          "1335:     }",
          "1336:   }",
          "",
          "[Removed Lines]",
          "1334:         throw QueryExecutionErrors.castingCauseOverflowError(v, DT(endField))",
          "",
          "[Added Lines]",
          "1336:         throw QueryExecutionErrors.castingCauseOverflowError(v, LongType, DT(endField))",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "1349:     val vInt = vLong.toInt",
          "1350:     if (vLong != vInt) {",
          "1351:       throw QueryExecutionErrors.castingCauseOverflowError(",
          "1353:         IntegerType)",
          "1354:     }",
          "1355:     vInt",
          "",
          "[Removed Lines]",
          "1352:         Literal(v, DayTimeIntervalType(startField, endField)),",
          "",
          "[Added Lines]",
          "1354:         v,",
          "1355:         DayTimeIntervalType(startField, endField),",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "1360:     val vShort = vLong.toShort",
          "1361:     if (vLong != vShort) {",
          "1362:       throw QueryExecutionErrors.castingCauseOverflowError(",
          "1364:         ShortType)",
          "1365:     }",
          "1366:     vShort",
          "",
          "[Removed Lines]",
          "1363:         Literal(v, DayTimeIntervalType(startField, endField)),",
          "",
          "[Added Lines]",
          "1366:         v,",
          "1367:         DayTimeIntervalType(startField, endField),",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "1371:     val vByte = vLong.toByte",
          "1372:     if (vLong != vByte) {",
          "1373:       throw QueryExecutionErrors.castingCauseOverflowError(",
          "1375:         ByteType)",
          "1376:     }",
          "1377:     vByte",
          "",
          "[Removed Lines]",
          "1374:         Literal(v, DayTimeIntervalType(startField, endField)),",
          "",
          "[Added Lines]",
          "1378:         v,",
          "1379:         DayTimeIntervalType(startField, endField),",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryErrorsBase.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryErrorsBase.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryErrorsBase.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryErrorsBase.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "24: import org.apache.spark.sql.types.{DataType, DoubleType, FloatType}",
          "26: trait QueryErrorsBase {",
          "28:     case Literal(null, _) => \"NULL\"",
          "29:     case Literal(v: Float, FloatType) =>",
          "30:       if (v.isNaN) \"NaN\"",
          "31:       else if (v.isPosInfinity) \"Infinity\"",
          "32:       else if (v.isNegInfinity) \"-Infinity\"",
          "33:       else v.toString",
          "35:       if (v.isNaN) \"NaN\"",
          "36:       else if (v.isPosInfinity) \"Infinity\"",
          "37:       else if (v.isNegInfinity) \"-Infinity\"",
          "",
          "[Removed Lines]",
          "27:   private def litToErrorValue(l: Literal): String = l match {",
          "34:     case Literal(v: Double, DoubleType) =>",
          "",
          "[Added Lines]",
          "28:   def toSQLValue(v: Any, t: DataType): String = Literal.create(v, t) match {",
          "35:     case l @ Literal(v: Double, DoubleType) =>",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "39:     case l => l.sql",
          "40:   }",
          "51:   private def quoteByDefault(elem: String): String = {",
          "52:     \"\\\"\" + elem + \"\\\"\"",
          "53:   }",
          "",
          "[Removed Lines]",
          "43:   def toSQLValue(v: Any): String = {",
          "44:     litToErrorValue(Literal(v))",
          "45:   }",
          "47:   def toSQLValue(v: Any, t: DataType): String = {",
          "48:     litToErrorValue(Literal.create(v, t))",
          "49:   }",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "89:       messageParameters = Array(s\"Cannot terminate expression: $generator\"))",
          "90:   }",
          "93:     new SparkArithmeticException(errorClass = \"CAST_CAUSES_OVERFLOW\",",
          "94:       messageParameters = Array(",
          "96:   }",
          "98:   def cannotChangeDecimalPrecisionError(",
          "",
          "[Removed Lines]",
          "92:   def castingCauseOverflowError(t: Any, dataType: DataType): ArithmeticException = {",
          "95:         toSQLValue(t), toSQLType(dataType), toSQLConf(SQLConf.ANSI_ENABLED.key)))",
          "",
          "[Added Lines]",
          "92:   def castingCauseOverflowError(t: Any, from: DataType, to: DataType): ArithmeticException = {",
          "95:         toSQLValue(t, from), toSQLType(to), toSQLConf(SQLConf.ANSI_ENABLED.key)))",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "172:       key: String): ArrayIndexOutOfBoundsException = {",
          "173:     new SparkArrayIndexOutOfBoundsException(",
          "174:       errorClass = \"INVALID_ARRAY_INDEX\",",
          "176:   }",
          "178:   def invalidElementAtIndexError(",
          "",
          "[Removed Lines]",
          "175:       messageParameters = Array(toSQLValue(index), toSQLValue(numElements), toSQLConf(key)))",
          "",
          "[Added Lines]",
          "175:       messageParameters = Array(",
          "176:         toSQLValue(index, IntegerType), toSQLValue(numElements, IntegerType), toSQLConf(key)))",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "181:     new SparkArrayIndexOutOfBoundsException(",
          "182:       errorClass = \"INVALID_ARRAY_INDEX_IN_ELEMENT_AT\",",
          "183:       messageParameters =",
          "185:   }",
          "188:     new SparkNoSuchElementException(errorClass = \"MAP_KEY_DOES_NOT_EXIST\",",
          "190:   }",
          "192:   def inputTypeUnsupportedError(dataType: DataType): Throwable = {",
          "",
          "[Removed Lines]",
          "184:         Array(toSQLValue(index), toSQLValue(numElements), toSQLConf(SQLConf.ANSI_ENABLED.key)))",
          "187:   def mapKeyNotExistError(key: Any, context: String): NoSuchElementException = {",
          "189:       messageParameters = Array(toSQLValue(key), SQLConf.ANSI_ENABLED.key, context))",
          "",
          "[Added Lines]",
          "185:         Array(",
          "186:           toSQLValue(index, IntegerType),",
          "187:           toSQLValue(numElements, IntegerType),",
          "188:           toSQLConf(SQLConf.ANSI_ENABLED.key)))",
          "191:   def mapKeyNotExistError(key: Any, dataType: DataType, context: String): NoSuchElementException = {",
          "193:       messageParameters = Array(toSQLValue(key, dataType), SQLConf.ANSI_ENABLED.key, context))",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "464:       Array(message, alternative, SQLConf.ANSI_ENABLED.key, errorContext))",
          "465:   }",
          "469:   }",
          "471:   def binaryArithmeticCauseOverflowError(",
          "472:       eval1: Short, symbol: String, eval2: Short): ArithmeticException = {",
          "474:   }",
          "476:   def failedSplitSubExpressionMsg(length: Int): String = {",
          "",
          "[Removed Lines]",
          "467:   def unaryMinusCauseOverflowError(originValue: AnyVal): ArithmeticException = {",
          "468:     arithmeticOverflowError(s\"- ${toSQLValue(originValue)} caused overflow\")",
          "473:     arithmeticOverflowError(s\"${toSQLValue(eval1)} $symbol ${toSQLValue(eval2)} caused overflow\")",
          "",
          "[Added Lines]",
          "471:   def unaryMinusCauseOverflowError(originValue: Int): ArithmeticException = {",
          "472:     arithmeticOverflowError(s\"- ${toSQLValue(originValue, IntegerType)} caused overflow\")",
          "477:     arithmeticOverflowError(",
          "478:       s\"${toSQLValue(eval1, ShortType)} $symbol ${toSQLValue(eval2, ShortType)} caused overflow\")",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "1050:       e)",
          "1051:   }",
          "1059:     new DateTimeException(s\"Invalid input syntax for type ${toSQLType(to)}: $valueString. \" +",
          "1060:       s\"To return NULL instead, use 'try_cast'. If necessary set ${SQLConf.ANSI_ENABLED.key} \" +",
          "1061:       s\"to false to bypass this error.\" + errorContext)",
          "",
          "[Removed Lines]",
          "1053:   def cannotCastToDateTimeError(value: Any, to: DataType, errorContext: String): Throwable = {",
          "1054:     val valueString = if (value.isInstanceOf[UTF8String]) {",
          "1055:       toSQLValue(value, StringType)",
          "1056:     } else {",
          "1057:       toSQLValue(value)",
          "1058:     }",
          "",
          "[Added Lines]",
          "1058:   def cannotCastToDateTimeError(",
          "1059:       value: Any, from: DataType, to: DataType, errorContext: String): Throwable = {",
          "1060:     val valueString = toSQLValue(value, from)",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "1084:   def cannotParseStringAsDataTypeError(pattern: String, value: String, dataType: DataType)",
          "1085:   : Throwable = {",
          "1086:     new RuntimeException(",
          "1088:         s\"as target spark data type [$dataType].\")",
          "1089:   }",
          "",
          "[Removed Lines]",
          "1087:       s\"Cannot parse field value ${toSQLValue(value)} for pattern ${toSQLValue(pattern)} \" +",
          "",
          "[Added Lines]",
          "1089:       s\"Cannot parse field value ${toSQLValue(value, StringType)} \" +",
          "1090:         s\"for pattern ${toSQLValue(pattern, StringType)} \" +",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "1149:   }",
          "1151:   def paramIsNotIntegerError(paramName: String, value: String): Throwable = {",
          "1153:   }",
          "1155:   def paramIsNotBooleanValueError(paramName: String): Throwable = {",
          "",
          "[Removed Lines]",
          "1152:     new RuntimeException(s\"$paramName should be an integer. Found ${toSQLValue(value)}\")",
          "",
          "[Added Lines]",
          "1155:     new RuntimeException(s\"$paramName should be an integer. Found ${toSQLValue(value, StringType)}\")",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "1357:   }",
          "1359:   def indexOutOfBoundsOfArrayDataError(idx: Int): Throwable = {",
          "1361:   }",
          "1363:   def malformedRecordsDetectedInRecordParsingError(e: BadRecordException): Throwable = {",
          "",
          "[Removed Lines]",
          "1360:     new SparkIndexOutOfBoundsException(errorClass = \"INDEX_OUT_OF_BOUNDS\", Array(toSQLValue(idx)))",
          "",
          "[Added Lines]",
          "1363:     new SparkIndexOutOfBoundsException(",
          "1364:       errorClass = \"INDEX_OUT_OF_BOUNDS\", Array(toSQLValue(idx, IntegerType)))",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "1396:   def dynamicPartitionKeyNotAmongWrittenPartitionPathsError(key: String): Throwable = {",
          "1397:     new SparkException(",
          "1399:   }",
          "1401:   def cannotRemovePartitionDirError(partitionPath: Path): Throwable = {",
          "",
          "[Removed Lines]",
          "1398:       s\"Dynamic partition key ${toSQLValue(key)} is not among written partition paths.\")",
          "",
          "[Added Lines]",
          "1402:       s\"Dynamic partition key ${toSQLValue(key, StringType)} is not among written partition paths.\")",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "1678:   }",
          "1680:   def valueIsNullError(index: Int): Throwable = {",
          "1682:   }",
          "1684:   def onlySupportDataSourcesProvidingFileFormatError(providingClass: String): Throwable = {",
          "",
          "[Removed Lines]",
          "1681:     new NullPointerException(s\"Value at index ${toSQLValue(index)} is null\")",
          "",
          "[Added Lines]",
          "1685:     new NullPointerException(s\"Value at index ${toSQLValue(index, IntegerType)} is null\")",
          "",
          "---------------",
          "--- Hunk 11 ---",
          "[Context before]",
          "2028:     new SparkArithmeticException(",
          "2029:       errorClass = \"DATETIME_OVERFLOW\",",
          "2030:       messageParameters = Array(",
          "2032:   }",
          "2033: }",
          "",
          "[Removed Lines]",
          "2031:         s\"add ${toSQLValue(amount)} $unit to ${toSQLValue(DateTimeUtils.microsToInstant(micros))}\"))",
          "",
          "[Added Lines]",
          "2035:         s\"add ${toSQLValue(amount, IntegerType)} $unit to \" +",
          "2036:         s\"${toSQLValue(DateTimeUtils.microsToInstant(micros), TimestampType)}\"))",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/types/Decimal.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/types/Decimal.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/types/Decimal.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/types/Decimal.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "261:       if (actualLongVal == actualLongVal.toByte) {",
          "262:         actualLongVal.toByte",
          "263:       } else {",
          "265:       }",
          "266:     } else {",
          "267:       val doubleVal = decimalVal.toDouble",
          "268:       if (Math.floor(doubleVal) <= Byte.MaxValue && Math.ceil(doubleVal) >= Byte.MinValue) {",
          "269:         doubleVal.toByte",
          "270:       } else {",
          "272:       }",
          "273:     }",
          "274:   }",
          "",
          "[Removed Lines]",
          "264:         throw QueryExecutionErrors.castingCauseOverflowError(this, ByteType)",
          "271:         throw QueryExecutionErrors.castingCauseOverflowError(this, ByteType)",
          "",
          "[Added Lines]",
          "264:         throw QueryExecutionErrors.castingCauseOverflowError(",
          "265:           this, DecimalType(this.precision, this.scale), ByteType)",
          "272:         throw QueryExecutionErrors.castingCauseOverflowError(",
          "273:           this, DecimalType(this.precision, this.scale), ByteType)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "283:       if (actualLongVal == actualLongVal.toShort) {",
          "284:         actualLongVal.toShort",
          "285:       } else {",
          "287:       }",
          "288:     } else {",
          "289:       val doubleVal = decimalVal.toDouble",
          "290:       if (Math.floor(doubleVal) <= Short.MaxValue && Math.ceil(doubleVal) >= Short.MinValue) {",
          "291:         doubleVal.toShort",
          "292:       } else {",
          "294:       }",
          "295:     }",
          "296:   }",
          "",
          "[Removed Lines]",
          "286:         throw QueryExecutionErrors.castingCauseOverflowError(this, ShortType)",
          "293:         throw QueryExecutionErrors.castingCauseOverflowError(this, ShortType)",
          "",
          "[Added Lines]",
          "288:         throw QueryExecutionErrors.castingCauseOverflowError(",
          "289:           this, DecimalType(this.precision, this.scale), ShortType)",
          "296:         throw QueryExecutionErrors.castingCauseOverflowError(",
          "297:           this, DecimalType(this.precision, this.scale), ShortType)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "305:       if (actualLongVal == actualLongVal.toInt) {",
          "306:         actualLongVal.toInt",
          "307:       } else {",
          "309:       }",
          "310:     } else {",
          "311:       val doubleVal = decimalVal.toDouble",
          "312:       if (Math.floor(doubleVal) <= Int.MaxValue && Math.ceil(doubleVal) >= Int.MinValue) {",
          "313:         doubleVal.toInt",
          "314:       } else {",
          "316:       }",
          "317:     }",
          "318:   }",
          "",
          "[Removed Lines]",
          "308:         throw QueryExecutionErrors.castingCauseOverflowError(this, IntegerType)",
          "315:         throw QueryExecutionErrors.castingCauseOverflowError(this, IntegerType)",
          "",
          "[Added Lines]",
          "312:         throw QueryExecutionErrors.castingCauseOverflowError(",
          "313:           this, DecimalType(this.precision, this.scale), IntegerType)",
          "320:         throw QueryExecutionErrors.castingCauseOverflowError(",
          "321:           this, DecimalType(this.precision, this.scale), IntegerType)",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "332:         decimalVal.bigDecimal.toBigInteger.longValueExact()",
          "333:       } catch {",
          "334:         case _: ArithmeticException =>",
          "336:       }",
          "337:     }",
          "338:   }",
          "",
          "[Removed Lines]",
          "335:           throw QueryExecutionErrors.castingCauseOverflowError(this, LongType)",
          "",
          "[Added Lines]",
          "341:           throw QueryExecutionErrors.castingCauseOverflowError(",
          "342:             this, DecimalType(this.precision, this.scale), LongType)",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/types/numerics.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/types/numerics.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/types/numerics.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/types/numerics.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "115:     if (x == x.toInt) {",
          "116:       x.toInt",
          "117:     } else {",
          "119:     }",
          "120: }",
          "",
          "[Removed Lines]",
          "118:       throw QueryExecutionErrors.castingCauseOverflowError(x, IntegerType)",
          "",
          "[Added Lines]",
          "118:       throw QueryExecutionErrors.castingCauseOverflowError(",
          "119:         x, LongType, IntegerType)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "135:     if (Math.floor(x) <= intUpperBound && Math.ceil(x) >= intLowerBound) {",
          "136:       x.toInt",
          "137:     } else {",
          "139:     }",
          "140:   }",
          "",
          "[Removed Lines]",
          "138:       throw QueryExecutionErrors.castingCauseOverflowError(x, IntegerType)",
          "",
          "[Added Lines]",
          "139:       throw QueryExecutionErrors.castingCauseOverflowError(",
          "140:         x, FloatType, IntegerType)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "143:     if (Math.floor(x) <= longUpperBound && Math.ceil(x) >= longLowerBound) {",
          "144:       x.toLong",
          "145:     } else {",
          "147:     }",
          "148:   }",
          "",
          "[Removed Lines]",
          "146:       throw QueryExecutionErrors.castingCauseOverflowError(x, LongType)",
          "",
          "[Added Lines]",
          "148:       throw QueryExecutionErrors.castingCauseOverflowError(",
          "149:         x, FloatType, LongType)",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "160:     if (Math.floor(x) <= intUpperBound && Math.ceil(x) >= intLowerBound) {",
          "161:       x.toInt",
          "162:     } else {",
          "164:     }",
          "165:   }",
          "",
          "[Removed Lines]",
          "163:       throw QueryExecutionErrors.castingCauseOverflowError(x, IntegerType)",
          "",
          "[Added Lines]",
          "166:       throw QueryExecutionErrors.castingCauseOverflowError(x, DoubleType, IntegerType)",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "168:     if (Math.floor(x) <= longUpperBound && Math.ceil(x) >= longLowerBound) {",
          "169:       x.toLong",
          "170:     } else {",
          "172:     }",
          "173:   }",
          "",
          "[Removed Lines]",
          "171:       throw QueryExecutionErrors.castingCauseOverflowError(x, LongType)",
          "",
          "[Added Lines]",
          "174:       throw QueryExecutionErrors.castingCauseOverflowError(x, DoubleType, LongType)",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/AnsiCastSuiteBase.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/AnsiCastSuiteBase.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/AnsiCastSuiteBase.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/AnsiCastSuiteBase.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "261:   protected def checkCastToTimestampError(l: Literal, to: DataType): Unit = {",
          "262:     checkExceptionInExpression[DateTimeException](",
          "264:   }",
          "266:   test(\"cast from timestamp II\") {",
          "",
          "[Removed Lines]",
          "263:       cast(l, to), s\"\"\"Invalid input syntax for type \"TIMESTAMP\": ${toSQLValue(l)}\"\"\")",
          "",
          "[Added Lines]",
          "263:       cast(l, to),",
          "264:       s\"\"\"Invalid input syntax for type \"TIMESTAMP\": ${toSQLValue(l.eval(), l.dataType)}\"\"\")",
          "",
          "---------------"
        ],
        "sql/core/src/test/resources/sql-tests/inputs/ansi/map.sql||sql/core/src/test/resources/sql-tests/inputs/ansi/map.sql": [
          "File: sql/core/src/test/resources/sql-tests/inputs/ansi/map.sql -> sql/core/src/test/resources/sql-tests/inputs/ansi/map.sql",
          "--- Hunk 1 ---",
          "[Context before]",
          "6: select map(1, 'a', 2, 'b')[5];",
          "7: -- the configuration spark.sql.ansi.strictIndexOperator doesn't affect the function element_at",
          "8: select element_at(map(1, 'a', 2, 'b'), 5);",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "9: select element_at(map('a', 1, 'b', 2), 'c');",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "ba6d17288c3287e8dc1f7cb95db0233a45732dc0",
      "candidate_info": {
        "commit_hash": "ba6d17288c3287e8dc1f7cb95db0233a45732dc0",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/ba6d17288c3287e8dc1f7cb95db0233a45732dc0",
        "files": [
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileSourceStrategy.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala"
        ],
        "message": "[SPARK-40169][SQL] Don't pushdown Parquet filters with no reference to data schema\n\n### What changes were proposed in this pull request?\n\nCurrently in Parquet V1 read path, Spark will pushdown data filters even if they have no reference in the Parquet read schema. This can cause correctness issues as described in [SPARK-39833](https://issues.apache.org/jira/browse/SPARK-39833).\n\nThe root cause, it seems, is because in the V1 path, we first use `AttributeReference` equality to filter out data columns without partition columns, and then use `AttributeSet` equality to filter out filters with only references to data columns.\nThere's inconsistency in the two steps, when case sensitive check is false.\n\nTake the following scenario as example:\n- data column: `[COL, a]`\n- partition column: `[col]`\n- filter: `col > 10`\n\nWith `AttributeReference` equality, `COL` is not considered equal to `col` (because their names are different), and thus the filtered out data column set is still `[COL, a]`. However, when calculating filters with only reference to data columns, `COL` is **considered equal** to `col`. Consequently, the filter `col > 10`, when checking with `[COL, a]`, is considered to have reference to data columns, and thus will be pushed down to Parquet as data filter.\n\nOn the Parquet side, since `col` doesn't exist in the file schema (it only has `COL`), when column index enabled, it will incorrectly return wrong number of rows. See [PARQUET-2170](https://issues.apache.org/jira/browse/PARQUET-2170) for more detail.\n\nIn general, where data columns overlap with partition columns and case sensitivity is false, partition filters will not be filter out before we calculate filters with only reference to data columns, which is incorrect.\n\n### Why are the changes needed?\n\nThis fixes the correctness bug described in [SPARK-39833](https://issues.apache.org/jira/browse/SPARK-39833).\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nThere are existing test cases for this issue from [SPARK-39833](https://issues.apache.org/jira/browse/SPARK-39833). This also modified them to test the scenarios when case sensitivity is on or off.\n\nCloses #37881 from sunchao/SPARK-40169.\n\nAuthored-by: Chao Sun <sunchao@apple.com>\nSigned-off-by: Chao Sun <sunchao@apple.com>",
        "before_after_code_files": [
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileSourceStrategy.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileSourceStrategy.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileSourceStrategy.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileSourceStrategy.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileSourceStrategy.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileSourceStrategy.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "188:       val dataFilters = normalizedFiltersWithoutSubqueries.flatMap { f =>",
          "189:         if (f.references.intersect(partitionSet).nonEmpty) {",
          "190:           extractPredicatesWithinOutputSet(f, AttributeSet(dataColumnsWithoutPartitionCols))",
          "",
          "[Removed Lines]",
          "187:       val dataColumnsWithoutPartitionCols = dataColumns.filterNot(partitionColumns.contains)",
          "",
          "[Added Lines]",
          "187:       val dataColumnsWithoutPartitionCols = dataColumns.filterNot(partitionSet.contains)",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "230:       SQLConf.PARQUET_INT96_AS_TIMESTAMP.key,",
          "231:       sparkSession.sessionState.conf.isParquetINT96AsTimestamp)",
          "238:     val broadcastedHadoopConf =",
          "239:       sparkSession.sparkContext.broadcast(new SerializableConfiguration(hadoopConf))",
          "",
          "[Removed Lines]",
          "236:     hadoopConf.setBooleanIfUnset(ParquetInputFormat.COLUMN_INDEX_FILTERING_ENABLED, false)",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "1067:   }",
          "1069:   test(\"SPARK-39833: pushed filters with count()\") {",
          "1077:     }",
          "1078:   }",
          "1080:   test(\"SPARK-39833: pushed filters with project without filter columns\") {",
          "1088:     }",
          "1089:   }",
          "1090: }",
          "",
          "[Removed Lines]",
          "1070:     withTempPath { path =>",
          "1071:       val p = s\"${path.getCanonicalPath}${File.separator}col=0${File.separator}\"",
          "1072:       Seq(0).toDF(\"COL\").coalesce(1).write.save(p)",
          "1073:       val df = spark.read.parquet(path.getCanonicalPath)",
          "1074:       checkAnswer(df.filter(\"col = 0\"), Seq(Row(0)))",
          "1075:       assert(df.filter(\"col = 0\").count() == 1, \"col\")",
          "1076:       assert(df.filter(\"COL = 0\").count() == 1, \"COL\")",
          "1081:     withTempPath { path =>",
          "1082:       val p = s\"${path.getCanonicalPath}${File.separator}col=0${File.separator}\"",
          "1083:       Seq((0, 1)).toDF(\"COL\", \"a\").coalesce(1).write.save(p)",
          "1084:       val df = spark.read.parquet(path.getCanonicalPath)",
          "1085:       checkAnswer(df.filter(\"col = 0\"), Seq(Row(0, 1)))",
          "1086:       assert(df.filter(\"col = 0\").select(\"a\").collect().toSeq == Row(1) :: Nil)",
          "1087:       assert(df.filter(\"col = 0 and a = 1\").select(\"a\").collect().toSeq == Row(1) :: Nil)",
          "",
          "[Added Lines]",
          "1070:     Seq(true, false).foreach { caseSensitive =>",
          "1071:       withSQLConf(SQLConf.CASE_SENSITIVE.key -> caseSensitive.toString) {",
          "1072:         withTempPath { path =>",
          "1073:           val p = s\"${path.getCanonicalPath}${File.separator}col=0${File.separator}\"",
          "1074:           Seq(0).toDF(\"COL\").coalesce(1).write.save(p)",
          "1075:           val df = spark.read.parquet(path.getCanonicalPath)",
          "1076:           val expected = if (caseSensitive) Seq(Row(0, 0)) else Seq(Row(0))",
          "1077:           checkAnswer(df.filter(\"col = 0\"), expected)",
          "1078:           assert(df.filter(\"col = 0\").count() == 1, \"col\")",
          "1079:           assert(df.filter(\"COL = 0\").count() == 1, \"COL\")",
          "1080:         }",
          "1081:       }",
          "1086:     Seq(true, false).foreach { caseSensitive =>",
          "1087:       withSQLConf(SQLConf.CASE_SENSITIVE.key -> caseSensitive.toString) {",
          "1088:         withTempPath { path =>",
          "1089:           val p = s\"${path.getCanonicalPath}${File.separator}col=0${File.separator}\"",
          "1090:           Seq((0, 1)).toDF(\"COL\", \"a\").coalesce(1).write.save(p)",
          "1091:           val df = spark.read.parquet(path.getCanonicalPath)",
          "1092:           val expected = if (caseSensitive) Seq(Row(0, 1, 0)) else Seq(Row(0, 1))",
          "1093:           checkAnswer(df.filter(\"col = 0\"), expected)",
          "1094:           assert(df.filter(\"col = 0\").select(\"a\").collect().toSeq == Row(1) :: Nil)",
          "1095:           assert(df.filter(\"col = 0 and a = 1\").select(\"a\").collect().toSeq == Row(1) :: Nil)",
          "1096:         }",
          "1097:       }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "7ed30443a09dde842424165283d45c0c54d86a81",
      "candidate_info": {
        "commit_hash": "7ed30443a09dde842424165283d45c0c54d86a81",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/7ed30443a09dde842424165283d45c0c54d86a81",
        "files": [
          "core/src/main/scala/org/apache/spark/rpc/RpcEnv.scala"
        ],
        "message": "[SPARK-38807][CORE] Fix the startup error of spark shell on Windows\n\n### What changes were proposed in this pull request?\nThe File.getCanonicalPath method will return the drive letter in the windows system. The RpcEnvFileServer.validateDirectoryUri method uses the File.getCanonicalPath method to process the baseuri, which will cause the baseuri not to comply with the URI verification rules. For example, the / classes is processed into F: \\ classes.This causes the sparkcontext to fail to start on windows.\nThis PR modifies the RpcEnvFileServer.validateDirectoryUri  method and replaces `new File(baseUri).getCanonicalPath`   with\n `new URI(baseUri).normalize().getPath`. This method can work normally in windows.\n\n### Why are the changes needed?\nFix the startup error of spark shell on Windows system\n[[SPARK-35691](https://issues.apache.org/jira/browse/SPARK-35691)] introduced this regression.\n\n### Does this PR introduce any user-facing change?\nNo\n\n### How was this patch tested?\nCI\n\nCloses #36447 from 1104056452/master.\n\nLead-authored-by: Ming Li <1104056452@qq.com>\nCo-authored-by: ming li <1104056452@qq.com>\nSigned-off-by: Sean Owen <srowen@gmail.com>\n(cherry picked from commit a760975083ea0696e8fd834ecfe3fb877b7f7449)\nSigned-off-by: Sean Owen <srowen@gmail.com>",
        "before_after_code_files": [
          "core/src/main/scala/org/apache/spark/rpc/RpcEnv.scala||core/src/main/scala/org/apache/spark/rpc/RpcEnv.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "core/src/main/scala/org/apache/spark/rpc/RpcEnv.scala||core/src/main/scala/org/apache/spark/rpc/RpcEnv.scala": [
          "File: core/src/main/scala/org/apache/spark/rpc/RpcEnv.scala -> core/src/main/scala/org/apache/spark/rpc/RpcEnv.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: package org.apache.spark.rpc",
          "20: import java.io.File",
          "21: import java.nio.channels.ReadableByteChannel",
          "23: import scala.concurrent.Future",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "21: import java.net.URI",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "189:   protected def validateDirectoryUri(baseUri: String): String = {",
          "191:     val fixedBaseUri = \"/\" + baseCanonicalUri.stripPrefix(\"/\").stripSuffix(\"/\")",
          "192:     require(fixedBaseUri != \"/files\" && fixedBaseUri != \"/jars\",",
          "193:       \"Directory URI cannot be /files nor /jars.\")",
          "",
          "[Removed Lines]",
          "190:     val baseCanonicalUri = new File(baseUri).getCanonicalPath",
          "",
          "[Added Lines]",
          "191:     val baseCanonicalUri = new URI(baseUri).normalize().getPath",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "96c8b4f47c2d0df249efb088882b248b5c230188",
      "candidate_info": {
        "commit_hash": "96c8b4f47c2d0df249efb088882b248b5c230188",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/96c8b4f47c2d0df249efb088882b248b5c230188",
        "files": [
          "sql/catalyst/src/main/java/org/apache/spark/sql/connector/expressions/GeneralScalarExpression.java",
          "sql/catalyst/src/main/java/org/apache/spark/sql/connector/util/V2ExpressionSQLBuilder.java",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/catalyst/util/V2ExpressionBuilder.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/jdbc/H2Dialect.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCV2Suite.scala"
        ],
        "message": "[SPARK-38855][SQL] DS V2 supports push down math functions\n\n### What changes were proposed in this pull request?\nCurrently, Spark have some math functions of ANSI standard. Please refer https://github.com/apache/spark/blob/2f8613f22c0750c00cf1dcfb2f31c431d8dc1be7/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/FunctionRegistry.scala#L388\nThese functions show below:\n`LN`,\n`EXP`,\n`POWER`,\n`SQRT`,\n`FLOOR`,\n`CEIL`,\n`WIDTH_BUCKET`\n\nThe mainstream databases support these functions show below.\n\n|  \u51fd\u6570   | PostgreSQL  | ClickHouse  | H2  | MySQL  | Oracle  | Redshift  | Presto  | Teradata  | Snowflake  | DB2  | Vertica  | Exasol  | SqlServer  | Yellowbrick  | Impala  | Mariadb | Druid | Pig | SQLite | Influxdata | Singlestore | ElasticSearch |\n|  ----  | ----  | ----  | ----  | ----  | ----  | ----  | ----  | ----  | ----  | ----  | ----  | ----  | ----  | ----  | ----  | ----  | ----  | ----  | ----  | ----  | ----  | ----  |\n| `LN` | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes |\n| `EXP` | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes |\n| `POWER` | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes | No | Yes | Yes | Yes | Yes |\n| `SQRT` | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes |\n| `FLOOR` | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes |\n| `CEIL` | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes |\n| `WIDTH_BUCKET` | Yes | No | No | No | Yes | No | Yes | Yes | Yes | Yes | Yes | No | No | No | Yes | No | No | No | No | No | No | No |\n\nDS V2 should supports push down these math functions.\n\n### Why are the changes needed?\nDS V2 supports push down math functions\n\n### Does this PR introduce _any_ user-facing change?\n'No'.\nNew feature.\n\n### How was this patch tested?\nNew tests.\n\nCloses #36140 from beliefer/SPARK-38855.\n\nAuthored-by: Jiaan Geng <beliefer@163.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\n(cherry picked from commit bf75b495e18ed87d0c118bfd5f1ceb52d720cad9)\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/java/org/apache/spark/sql/connector/expressions/GeneralScalarExpression.java||sql/catalyst/src/main/java/org/apache/spark/sql/connector/expressions/GeneralScalarExpression.java",
          "sql/catalyst/src/main/java/org/apache/spark/sql/connector/util/V2ExpressionSQLBuilder.java||sql/catalyst/src/main/java/org/apache/spark/sql/connector/util/V2ExpressionSQLBuilder.java",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/catalyst/util/V2ExpressionBuilder.scala||sql/core/src/main/scala/org/apache/spark/sql/catalyst/util/V2ExpressionBuilder.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/jdbc/H2Dialect.scala||sql/core/src/main/scala/org/apache/spark/sql/jdbc/H2Dialect.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCV2Suite.scala||sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCV2Suite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/java/org/apache/spark/sql/connector/expressions/GeneralScalarExpression.java||sql/catalyst/src/main/java/org/apache/spark/sql/connector/expressions/GeneralScalarExpression.java": [
          "File: sql/catalyst/src/main/java/org/apache/spark/sql/connector/expressions/GeneralScalarExpression.java -> sql/catalyst/src/main/java/org/apache/spark/sql/connector/expressions/GeneralScalarExpression.java"
        ],
        "sql/catalyst/src/main/java/org/apache/spark/sql/connector/util/V2ExpressionSQLBuilder.java||sql/catalyst/src/main/java/org/apache/spark/sql/connector/util/V2ExpressionSQLBuilder.java": [
          "File: sql/catalyst/src/main/java/org/apache/spark/sql/connector/util/V2ExpressionSQLBuilder.java -> sql/catalyst/src/main/java/org/apache/spark/sql/connector/util/V2ExpressionSQLBuilder.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "95:           return visitUnaryArithmetic(name, inputToSQL(e.children()[0]));",
          "96:         case \"ABS\":",
          "97:         case \"COALESCE\":",
          "98:           return visitSQLFunction(name,",
          "99:             Arrays.stream(e.children()).map(c -> build(c)).toArray(String[]::new));",
          "100:         case \"CASE_WHEN\": {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "98:         case \"LN\":",
          "99:         case \"EXP\":",
          "100:         case \"POWER\":",
          "101:         case \"SQRT\":",
          "102:         case \"FLOOR\":",
          "103:         case \"CEIL\":",
          "104:         case \"WIDTH_BUCKET\":",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "2392:     new AnalysisException(",
          "2393:       \"Sinks cannot request distribution and ordering in continuous execution mode\")",
          "2394:   }",
          "2395: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "2396:   def noSuchFunctionError(database: String, funcInfo: String): Throwable = {",
          "2397:     new AnalysisException(s\"$database does not support function: $funcInfo\")",
          "2398:   }",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/catalyst/util/V2ExpressionBuilder.scala||sql/core/src/main/scala/org/apache/spark/sql/catalyst/util/V2ExpressionBuilder.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/catalyst/util/V2ExpressionBuilder.scala -> sql/core/src/main/scala/org/apache/spark/sql/catalyst/util/V2ExpressionBuilder.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: package org.apache.spark.sql.catalyst.util",
          "21: import org.apache.spark.sql.connector.expressions.{Cast => V2Cast, Expression => V2Expression, FieldReference, GeneralScalarExpression, LiteralValue}",
          "22: import org.apache.spark.sql.connector.expressions.filter.{AlwaysFalse, AlwaysTrue, And => V2And, Not => V2Not, Or => V2Or, Predicate => V2Predicate}",
          "23: import org.apache.spark.sql.execution.datasources.PushableColumn",
          "",
          "[Removed Lines]",
          "20: import org.apache.spark.sql.catalyst.expressions.{Abs, Add, And, BinaryComparison, BinaryOperator, BitwiseAnd, BitwiseNot, BitwiseOr, BitwiseXor, CaseWhen, Cast, Coalesce, Contains, Divide, EndsWith, EqualTo, Expression, In, InSet, IsNotNull, IsNull, Literal, Multiply, Not, Or, Predicate, Remainder, StartsWith, StringPredicate, Subtract, UnaryMinus}",
          "",
          "[Added Lines]",
          "20: import org.apache.spark.sql.catalyst.expressions.{Abs, Add, And, BinaryComparison, BinaryOperator, BitwiseAnd, BitwiseNot, BitwiseOr, BitwiseXor, CaseWhen, Cast, Ceil, Coalesce, Contains, Divide, EndsWith, EqualTo, Exp, Expression, Floor, In, InSet, IsNotNull, IsNull, Literal, Log, Multiply, Not, Or, Pow, Predicate, Remainder, Sqrt, StartsWith, StringPredicate, Subtract, UnaryMinus, WidthBucket}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "104:       } else {",
          "105:         None",
          "106:       }",
          "107:     case and: And =>",
          "109:       val l = generateExpression(and.left, true)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "105:         None",
          "106:       }",
          "107:     case Log(child) => generateExpression(child)",
          "108:       .map(v => new GeneralScalarExpression(\"LN\", Array[V2Expression](v)))",
          "109:     case Exp(child) => generateExpression(child)",
          "110:       .map(v => new GeneralScalarExpression(\"EXP\", Array[V2Expression](v)))",
          "111:     case Pow(left, right) =>",
          "112:       val l = generateExpression(left)",
          "113:       val r = generateExpression(right)",
          "114:       if (l.isDefined && r.isDefined) {",
          "115:         Some(new GeneralScalarExpression(\"POWER\", Array[V2Expression](l.get, r.get)))",
          "116:       } else {",
          "117:         None",
          "118:       }",
          "119:     case Sqrt(child) => generateExpression(child)",
          "120:       .map(v => new GeneralScalarExpression(\"SQRT\", Array[V2Expression](v)))",
          "121:     case Floor(child) => generateExpression(child)",
          "122:       .map(v => new GeneralScalarExpression(\"FLOOR\", Array[V2Expression](v)))",
          "123:     case Ceil(child) => generateExpression(child)",
          "124:       .map(v => new GeneralScalarExpression(\"CEIL\", Array[V2Expression](v)))",
          "125:     case wb: WidthBucket =>",
          "126:       val childrenExpressions = wb.children.flatMap(generateExpression(_))",
          "127:       if (childrenExpressions.length == wb.children.length) {",
          "128:         Some(new GeneralScalarExpression(\"WIDTH_BUCKET\",",
          "129:           childrenExpressions.toArray[V2Expression]))",
          "130:       } else {",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/jdbc/H2Dialect.scala||sql/core/src/main/scala/org/apache/spark/sql/jdbc/H2Dialect.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/jdbc/H2Dialect.scala -> sql/core/src/main/scala/org/apache/spark/sql/jdbc/H2Dialect.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: import java.sql.SQLException",
          "21: import java.util.Locale",
          "23: import org.apache.spark.sql.AnalysisException",
          "24: import org.apache.spark.sql.catalyst.analysis.{NoSuchNamespaceException, NoSuchTableException, TableAlreadyExistsException}",
          "25: import org.apache.spark.sql.connector.expressions.aggregate.{AggregateFunc, GeneralAggregateFunc}",
          "27: private object H2Dialect extends JdbcDialect {",
          "28:   override def canHandle(url: String): Boolean =",
          "29:     url.toLowerCase(Locale.ROOT).startsWith(\"jdbc:h2\")",
          "31:   override def compileAggregate(aggFunction: AggregateFunc): Option[String] = {",
          "32:     super.compileAggregate(aggFunction).orElse(",
          "33:       aggFunction match {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "23: import scala.util.control.NonFatal",
          "27: import org.apache.spark.sql.connector.expressions.Expression",
          "29: import org.apache.spark.sql.errors.QueryCompilationErrors",
          "35:   class H2SQLBuilder extends JDBCSQLBuilder {",
          "36:     override def visitSQLFunction(funcName: String, inputs: Array[String]): String = {",
          "37:       funcName match {",
          "38:         case \"WIDTH_BUCKET\" =>",
          "39:           val functionInfo = super.visitSQLFunction(funcName, inputs)",
          "40:           throw QueryCompilationErrors.noSuchFunctionError(\"H2\", functionInfo)",
          "41:         case _ => super.visitSQLFunction(funcName, inputs)",
          "42:       }",
          "43:     }",
          "44:   }",
          "46:   override def compileExpression(expr: Expression): Option[String] = {",
          "47:     val h2SQLBuilder = new H2SQLBuilder()",
          "48:     try {",
          "49:       Some(h2SQLBuilder.build(expr))",
          "50:     } catch {",
          "51:       case NonFatal(e) =>",
          "52:         logWarning(\"Error occurs while compiling V2 expression\", e)",
          "53:         None",
          "54:     }",
          "55:   }",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCV2Suite.scala||sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCV2Suite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCV2Suite.scala -> sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCV2Suite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "26: import org.apache.spark.sql.catalyst.plans.logical.{Aggregate, Filter, Sort}",
          "27: import org.apache.spark.sql.execution.datasources.v2.{DataSourceV2ScanRelation, V1ScanWrapper}",
          "28: import org.apache.spark.sql.execution.datasources.v2.jdbc.JDBCTableCatalog",
          "30: import org.apache.spark.sql.internal.SQLConf",
          "31: import org.apache.spark.sql.test.SharedSparkSession",
          "32: import org.apache.spark.util.Utils",
          "",
          "[Removed Lines]",
          "29: import org.apache.spark.sql.functions.{abs, avg, coalesce, count, count_distinct, lit, not, sum, udf, when}",
          "",
          "[Added Lines]",
          "29: import org.apache.spark.sql.functions.{abs, avg, ceil, coalesce, count, count_distinct, exp, floor, lit, log => ln, not, pow, sqrt, sum, udf, when}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "440:         checkPushedInfo(df5, expectedPlanFragment5)",
          "441:         checkAnswer(df5, Seq(Row(1, \"amy\", 10000, 1000, true),",
          "442:           Row(1, \"cathy\", 9000, 1200, false), Row(6, \"jen\", 12000, 1200, true)))",
          "443:       }",
          "444:     }",
          "445:   }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "444:         val df6 = spark.table(\"h2.test.employee\")",
          "445:           .filter(ln($\"dept\") > 1)",
          "446:           .filter(exp($\"salary\") > 2000)",
          "447:           .filter(pow($\"dept\", 2) > 4)",
          "448:           .filter(sqrt($\"salary\") > 100)",
          "449:           .filter(floor($\"dept\") > 1)",
          "450:           .filter(ceil($\"dept\") > 1)",
          "451:         checkFiltersRemoved(df6, ansiMode)",
          "452:         val expectedPlanFragment6 = if (ansiMode) {",
          "453:           \"PushedFilters: [DEPT IS NOT NULL, SALARY IS NOT NULL, \" +",
          "454:             \"LN(CAST(DEPT AS double)) > 1.0, EXP(CAST(SALARY AS double)...,\"",
          "455:         } else {",
          "456:           \"PushedFilters: [DEPT IS NOT NULL, SALARY IS NOT NULL]\"",
          "457:         }",
          "458:         checkPushedInfo(df6, expectedPlanFragment6)",
          "459:         checkAnswer(df6, Seq(Row(6, \"jen\", 12000, 1200, true)))",
          "462:         val df7 = sql(\"\"\"",
          "463:                         |SELECT * FROM h2.test.employee",
          "464:                         |WHERE width_bucket(dept, 1, 6, 3) > 1",
          "465:                         |\"\"\".stripMargin)",
          "466:         checkFiltersRemoved(df7, false)",
          "467:         checkPushedInfo(df7, \"PushedFilters: [DEPT IS NOT NULL]\")",
          "468:         checkAnswer(df7, Seq(Row(6, \"jen\", 12000, 1200, true)))",
          "",
          "---------------"
        ]
      }
    }
  ]
}