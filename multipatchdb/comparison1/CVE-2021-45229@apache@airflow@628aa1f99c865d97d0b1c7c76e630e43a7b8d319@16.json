{
  "cve_id": "CVE-2021-45229",
  "cve_desc": "It was discovered that the \"Trigger DAG with config\" screen was susceptible to XSS attacks via the `origin` query argument. This issue affects Apache Airflow versions 2.2.3 and below.",
  "repo": "apache/airflow",
  "patch_hash": "628aa1f99c865d97d0b1c7c76e630e43a7b8d319",
  "patch_info": {
    "commit_hash": "628aa1f99c865d97d0b1c7c76e630e43a7b8d319",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/628aa1f99c865d97d0b1c7c76e630e43a7b8d319",
    "files": [
      "airflow/www/templates/airflow/trigger.html",
      "tests/www/views/test_views_trigger_dag.py"
    ],
    "message": "Simplify trigger cancel button (#21591)\n\nCo-authored-by: Jed Cunningham <jedcunningham@apache.org>\n(cherry picked from commit 65297673a318660fba76797e50d0c06804dfcafc)",
    "before_after_code_files": [
      "airflow/www/templates/airflow/trigger.html||airflow/www/templates/airflow/trigger.html",
      "tests/www/views/test_views_trigger_dag.py||tests/www/views/test_views_trigger_dag.py"
    ]
  },
  "patch_diff": {
    "airflow/www/templates/airflow/trigger.html||airflow/www/templates/airflow/trigger.html": [
      "File: airflow/www/templates/airflow/trigger.html -> airflow/www/templates/airflow/trigger.html",
      "--- Hunk 1 ---",
      "[Context before]",
      "63:       </label>",
      "64:     </div>",
      "65:     <button type=\"submit\" class=\"btn btn-primary\">Trigger</button>",
      "67:   </form>",
      "68: {% endblock %}",
      "",
      "[Removed Lines]",
      "66:     <button type=\"button\" class=\"btn\" onclick=\"location.href = '{{ origin }}'; return false\">Cancel</button>",
      "",
      "[Added Lines]",
      "66:     <a class=\"btn\" href=\"{{ origin }}\">Cancel</a>",
      "",
      "---------------"
    ],
    "tests/www/views/test_views_trigger_dag.py||tests/www/views/test_views_trigger_dag.py": [
      "File: tests/www/views/test_views_trigger_dag.py -> tests/www/views/test_views_trigger_dag.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "133:         (\"javascript:alert(1)\", \"/home\"),",
      "134:         (\"http://google.com\", \"/home\"),",
      "135:         (\"36539'%3balert(1)%2f%2f166\", \"/home\"),",
      "136:         (",
      "137:             \"%2Ftree%3Fdag_id%3Dexample_bash_operator';alert(33)//\",",
      "138:             \"/home\",",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "136:         (",
      "137:             '\"><script>alert(99)</script><a href=\"',",
      "138:             \"&#34;&gt;&lt;script&gt;alert(99)&lt;/script&gt;&lt;a href=&#34;\",",
      "139:         ),",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "145:     test_dag_id = \"example_bash_operator\"",
      "147:     resp = admin_client.get(f'trigger?dag_id={test_dag_id}&origin={test_origin}')",
      "156: @pytest.mark.parametrize(",
      "",
      "[Removed Lines]",
      "148:     check_content_in_response(",
      "149:         '<button type=\"button\" class=\"btn\" onclick=\"location.href = \\'{}\\'; return false\">'.format(",
      "150:             expected_origin",
      "151:         ),",
      "152:         resp,",
      "153:     )",
      "",
      "[Added Lines]",
      "152:     check_content_in_response(f'<a class=\"btn\" href=\"{expected_origin}\">Cancel</a>', resp)",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "2dbbe2bbac35a77c9e3dd16ea1fc6ed861ef5db1",
      "candidate_info": {
        "commit_hash": "2dbbe2bbac35a77c9e3dd16ea1fc6ed861ef5db1",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/2dbbe2bbac35a77c9e3dd16ea1fc6ed861ef5db1",
        "files": [
          ".pre-commit-config.yaml",
          "BREEZE.rst",
          "STATIC_CODE_CHECKS.rst",
          "breeze-complete",
          "scripts/ci/pre_commit/pre_commit_checkout_no_credentials.py"
        ],
        "message": "Add pre-commit that checks credentials are not persisted in CI (#20430)\n\nFor security reason we should not persist credentials on checking\nout code during GitHub actions. This pre-commit prevents this\nfrom happening.\n\n(cherry picked from commit 3ccbd4f4ee4f27c08ab39aa61aa0cf1e631bd154)",
        "before_after_code_files": [
          "scripts/ci/pre_commit/pre_commit_checkout_no_credentials.py||scripts/ci/pre_commit/pre_commit_checkout_no_credentials.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "scripts/ci/pre_commit/pre_commit_checkout_no_credentials.py||scripts/ci/pre_commit/pre_commit_checkout_no_credentials.py": [
          "File: scripts/ci/pre_commit/pre_commit_checkout_no_credentials.py -> scripts/ci/pre_commit/pre_commit_checkout_no_credentials.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: #!/usr/bin/env python3",
          "2: # Licensed to the Apache Software Foundation (ASF) under one",
          "3: # or more contributor license agreements.  See the NOTICE file",
          "4: # distributed with this work for additional information",
          "5: # regarding copyright ownership.  The ASF licenses this file",
          "6: # to you under the Apache License, Version 2.0 (the",
          "7: # \"License\"); you may not use this file except in compliance",
          "8: # with the License.  You may obtain a copy of the License at",
          "9: #",
          "10: #   http://www.apache.org/licenses/LICENSE-2.0",
          "11: #",
          "12: # Unless required by applicable law or agreed to in writing,",
          "13: # software distributed under the License is distributed on an",
          "14: # \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
          "15: # KIND, either express or implied.  See the License for the",
          "16: # specific language governing permissions and limitations",
          "17: # under the License.",
          "18: import sys",
          "19: from pathlib import Path",
          "21: import yaml",
          "22: from rich.console import Console",
          "24: if __name__ not in (\"__main__\", \"__mp_main__\"):",
          "25:     raise SystemExit(",
          "26:         \"This file is intended to be executed as an executable program. You cannot use it as a module.\"",
          "27:         f\"To run this script, run the ./{__file__} command [FILE] ...\"",
          "28:     )",
          "31: console = Console(color_system=\"standard\", width=200)",
          "34: def check_file(the_file: Path) -> int:",
          "35:     \"\"\"Returns number of wrong checkout instructions in the workflow file\"\"\"",
          "36:     error_num = 0",
          "37:     res = yaml.safe_load(the_file.read_text())",
          "38:     console.print(f\"Checking file [yellow]{the_file}[/]\")",
          "39:     for job in res['jobs'].values():",
          "40:         for step in job['steps']:",
          "41:             uses = step.get('uses')",
          "42:             pretty_step = yaml.safe_dump(step, indent=2)",
          "43:             if uses is not None and uses.startswith('actions/checkout'):",
          "44:                 with_clause = step.get('with')",
          "45:                 if with_clause is None:",
          "46:                     console.print(f\"\\n[red]The `with` clause is missing in step:[/]\\n\\n{pretty_step}\")",
          "47:                     error_num += 1",
          "48:                     continue",
          "49:                 persist_credentials = with_clause.get(\"persist-credentials\")",
          "50:                 if persist_credentials is None:",
          "51:                     console.print(",
          "52:                         \"\\n[red]The `with` clause does not have persist-credentials in step:[/]\"",
          "53:                         f\"\\n\\n{pretty_step}\"",
          "54:                     )",
          "55:                     error_num += 1",
          "56:                     continue",
          "57:                 else:",
          "58:                     if persist_credentials:",
          "59:                         console.print(",
          "60:                             \"\\n[red]The `with` clause have persist-credentials=True in step:[/]\"",
          "61:                             f\"\\n\\n{pretty_step}\"",
          "62:                         )",
          "63:                         error_num += 1",
          "64:                         continue",
          "65:     return error_num",
          "68: if __name__ == '__main__':",
          "69:     total_err_num = 0",
          "70:     for a_file in sys.argv[1:]:",
          "71:         total_err_num += check_file(Path(a_file))",
          "72:     if total_err_num:",
          "73:         console.print(",
          "74:             \"\"\"",
          "75: [red]There are are some checkout instructions in github workflows that have no \"persist_credentials\"",
          "76: set to False.[/]",
          "78: For security reasons - make sure all of the checkout actions have persist_credentials set, similar to:",
          "80:   - name: \"Checkout ${{ github.ref }} ( ${{ github.sha }} )\"",
          "81:     uses: actions/checkout@v2",
          "82:     with:",
          "83:       persist-credentials: false",
          "85: \"\"\"",
          "86:         )",
          "87:         sys.exit(1)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "a5ee60e5f1f3e74b7f809c50b85ecb1b3d44f303",
      "candidate_info": {
        "commit_hash": "a5ee60e5f1f3e74b7f809c50b85ecb1b3d44f303",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/a5ee60e5f1f3e74b7f809c50b85ecb1b3d44f303",
        "files": [
          "airflow/providers/google/cloud/hooks/dataproc.py",
          "airflow/providers/google/cloud/operators/dataproc.py",
          "airflow/providers/google/cloud/sensors/dataproc.py",
          "setup.py",
          "tests/providers/google/cloud/hooks/test_dataproc.py",
          "tests/providers/google/cloud/sensors/test_dataproc.py"
        ],
        "message": "Upgrade the Dataproc package to 3.0.0 and migrate from v1beta2 to v1 api (#18879)\n\n(cherry picked from commit 4fae04a47119c9f2319ae5e533edcf457e4df003)",
        "before_after_code_files": [
          "airflow/providers/google/cloud/hooks/dataproc.py||airflow/providers/google/cloud/hooks/dataproc.py",
          "airflow/providers/google/cloud/operators/dataproc.py||airflow/providers/google/cloud/operators/dataproc.py",
          "airflow/providers/google/cloud/sensors/dataproc.py||airflow/providers/google/cloud/sensors/dataproc.py",
          "setup.py||setup.py",
          "tests/providers/google/cloud/hooks/test_dataproc.py||tests/providers/google/cloud/hooks/test_dataproc.py",
          "tests/providers/google/cloud/sensors/test_dataproc.py||tests/providers/google/cloud/sensors/test_dataproc.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/providers/google/cloud/hooks/dataproc.py||airflow/providers/google/cloud/hooks/dataproc.py": [
          "File: airflow/providers/google/cloud/hooks/dataproc.py -> airflow/providers/google/cloud/hooks/dataproc.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "26: from google.api_core.exceptions import ServerError",
          "27: from google.api_core.retry import Retry",
          "29:     Cluster,",
          "30:     ClusterControllerClient,",
          "31:     Job,",
          "",
          "[Removed Lines]",
          "28: from google.cloud.dataproc_v1beta2 import (",
          "",
          "[Added Lines]",
          "28: from google.cloud.dataproc_v1 import (",
          "",
          "---------------"
        ],
        "airflow/providers/google/cloud/operators/dataproc.py||airflow/providers/google/cloud/operators/dataproc.py": [
          "File: airflow/providers/google/cloud/operators/dataproc.py -> airflow/providers/google/cloud/operators/dataproc.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "31: from google.api_core.exceptions import AlreadyExists, NotFound",
          "32: from google.api_core.retry import Retry, exponential_sleep_generator",
          "34: from google.protobuf.duration_pb2 import Duration",
          "35: from google.protobuf.field_mask_pb2 import FieldMask",
          "",
          "[Removed Lines]",
          "33: from google.cloud.dataproc_v1beta2 import Cluster",
          "",
          "[Added Lines]",
          "33: from google.cloud.dataproc_v1 import Cluster",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1909:     :type location: str",
          "1910:     :param job: Required. The job resource.",
          "1911:         If a dict is provided, it must be of the same form as the protobuf message",
          "1913:     :type job: Dict",
          "1914:     :param request_id: Optional. A unique id used to identify the request. If the server receives two",
          "1915:         ``SubmitJobRequest`` requests with the same id, then the second request will be ignored and the first",
          "",
          "[Removed Lines]",
          "1912:         :class:`~google.cloud.dataproc_v1beta2.types.Job`",
          "",
          "[Added Lines]",
          "1912:         :class:`~google.cloud.dataproc_v1.types.Job`",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "2050:     :param cluster: Required. The changes to the cluster.",
          "2052:         If a dict is provided, it must be of the same form as the protobuf message",
          "2055:     :param update_mask: Required. Specifies the path, relative to ``Cluster``, of the field to update. For",
          "2056:         example, to change the number of workers in a cluster to 5, the ``update_mask`` parameter would be",
          "2057:         specified as ``config.worker_config.num_instances``, and the ``PATCH`` request body would specify the",
          "",
          "[Removed Lines]",
          "2053:         :class:`~google.cloud.dataproc_v1beta2.types.Cluster`",
          "2054:     :type cluster: Union[Dict, google.cloud.dataproc_v1beta2.types.Cluster]",
          "",
          "[Added Lines]",
          "2053:         :class:`~google.cloud.dataproc_v1.types.Cluster`",
          "2054:     :type cluster: Union[Dict, google.cloud.dataproc_v1.types.Cluster]",
          "",
          "---------------"
        ],
        "airflow/providers/google/cloud/sensors/dataproc.py||airflow/providers/google/cloud/sensors/dataproc.py": [
          "File: airflow/providers/google/cloud/sensors/dataproc.py -> airflow/providers/google/cloud/sensors/dataproc.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: import warnings",
          "21: from typing import Optional",
          "25: from airflow.exceptions import AirflowException",
          "26: from airflow.providers.google.cloud.hooks.dataproc import DataprocHook",
          "",
          "[Removed Lines]",
          "23: from google.cloud.dataproc_v1beta2.types import JobStatus",
          "",
          "[Added Lines]",
          "23: from google.cloud.dataproc_v1.types import JobStatus",
          "",
          "---------------"
        ],
        "setup.py||setup.py": [
          "File: setup.py -> setup.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "303:     'google-cloud-build>=3.0.0,<4.0.0',",
          "304:     'google-cloud-container>=0.1.1,<2.0.0',",
          "305:     'google-cloud-datacatalog>=3.0.0,<4.0.0',",
          "307:     'google-cloud-dlp>=0.11.0,<2.0.0',",
          "308:     'google-cloud-kms>=2.0.0,<3.0.0',",
          "309:     'google-cloud-language>=1.1.1,<2.0.0',",
          "",
          "[Removed Lines]",
          "306:     'google-cloud-dataproc>=2.2.0,<2.6.0',",
          "",
          "[Added Lines]",
          "306:     'google-cloud-dataproc>=2.2.0,<4.0.0',",
          "",
          "---------------"
        ],
        "tests/providers/google/cloud/hooks/test_dataproc.py||tests/providers/google/cloud/hooks/test_dataproc.py": [
          "File: tests/providers/google/cloud/hooks/test_dataproc.py -> tests/providers/google/cloud/hooks/test_dataproc.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: from unittest import mock",
          "22: import pytest",
          "25: from airflow.exceptions import AirflowException",
          "26: from airflow.providers.google.cloud.hooks.dataproc import DataprocHook, DataProcJobBuilder",
          "",
          "[Removed Lines]",
          "23: from google.cloud.dataproc_v1beta2 import JobStatus",
          "",
          "[Added Lines]",
          "23: from google.cloud.dataproc_v1 import JobStatus",
          "",
          "---------------"
        ],
        "tests/providers/google/cloud/sensors/test_dataproc.py||tests/providers/google/cloud/sensors/test_dataproc.py": [
          "File: tests/providers/google/cloud/sensors/test_dataproc.py -> tests/providers/google/cloud/sensors/test_dataproc.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "19: from unittest import mock",
          "21: import pytest",
          "24: from airflow import AirflowException",
          "25: from airflow.providers.google.cloud.sensors.dataproc import DataprocJobSensor",
          "",
          "[Removed Lines]",
          "22: from google.cloud.dataproc_v1beta2.types import JobStatus",
          "",
          "[Added Lines]",
          "22: from google.cloud.dataproc_v1.types import JobStatus",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "8a432bbf6feb57d4da7a4f7300e423af209a6d70",
      "candidate_info": {
        "commit_hash": "8a432bbf6feb57d4da7a4f7300e423af209a6d70",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/8a432bbf6feb57d4da7a4f7300e423af209a6d70",
        "files": [
          ".github/workflows/ci.yml",
          "breeze",
          "scripts/ci/docker-compose/_docker.env",
          "scripts/ci/docker-compose/base.yml",
          "scripts/ci/libraries/_initialization.sh",
          "scripts/in_container/_in_container_utils.sh",
          "scripts/in_container/run_install_and_test_provider_packages.sh",
          "setup.py"
        ],
        "message": "Add twine check for provider packages (#20619)\n\nTwine (which we use to upload packages to PyPI) has the\nability to run checks of packages before uploading them.\n\nThis allows to detect cases like when we are using forbidden\ndirectives in README.rst (which delayed slightly preparing the\nDecember 2021 provider packages and resulted in #20614\n\nWith this PR Twine check will be run for all packages in CI\nbefore we even attempt to merge such change that could break\nthem.\n\n(cherry picked from commit f011f66f763cb9bfcccea085dbd2cb2b44614d20)",
        "before_after_code_files": [
          "scripts/ci/docker-compose/_docker.env||scripts/ci/docker-compose/_docker.env",
          "scripts/ci/libraries/_initialization.sh||scripts/ci/libraries/_initialization.sh",
          "scripts/in_container/_in_container_utils.sh||scripts/in_container/_in_container_utils.sh",
          "scripts/in_container/run_install_and_test_provider_packages.sh||scripts/in_container/run_install_and_test_provider_packages.sh",
          "setup.py||setup.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "scripts/ci/docker-compose/_docker.env||scripts/ci/docker-compose/_docker.env": [
          "File: scripts/ci/docker-compose/_docker.env -> scripts/ci/docker-compose/_docker.env",
          "--- Hunk 1 ---",
          "[Context before]",
          "57: LIST_OF_INTEGRATION_TESTS_TO_RUN",
          "58: RUN_SYSTEM_TESTS",
          "59: START_AIRFLOW",
          "60: TEST_TYPE",
          "61: UPGRADE_TO_NEWER_DEPENDENCIES",
          "62: VERBOSE",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "60: SKIP_TWINE_CHECK",
          "",
          "---------------"
        ],
        "scripts/ci/libraries/_initialization.sh||scripts/ci/libraries/_initialization.sh": [
          "File: scripts/ci/libraries/_initialization.sh -> scripts/ci/libraries/_initialization.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "416:     INSTALL_PROVIDERS_FROM_SOURCES=${INSTALL_PROVIDERS_FROM_SOURCES:=\"true\"}",
          "417:     export INSTALL_PROVIDERS_FROM_SOURCES",
          "419:     export INSTALLED_EXTRAS=\"async,amazon,celery,cncf.kubernetes,docker,dask,elasticsearch,ftp,grpc,hashicorp,http,imap,ldap,google,microsoft.azure,mysql,postgres,redis,sendgrid,sftp,slack,ssh,statsd,virtualenv\"",
          "421:     AIRFLOW_PIP_VERSION=${AIRFLOW_PIP_VERSION:=\"21.2.4\"}",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "419:     SKIP_TWINE_CHECK=${SKIP_TWINE_CHECK:=\"\"}",
          "420:     export SKIP_TWINE_CHECK",
          "",
          "---------------"
        ],
        "scripts/in_container/_in_container_utils.sh||scripts/in_container/_in_container_utils.sh": [
          "File: scripts/in_container/_in_container_utils.sh -> scripts/in_container/_in_container_utils.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "301:     pip install /dist/apache-airflow-*providers-*.tar.gz",
          "302: }",
          "304: function setup_provider_packages() {",
          "305:     export PACKAGE_TYPE=\"regular\"",
          "306:     export PACKAGE_PREFIX_UPPERCASE=\"\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "304: function twine_check_provider_packages_from_wheels() {",
          "305:     echo",
          "306:     echo \"Twine check of all provider packages from wheels\"",
          "307:     echo",
          "308:     twine check /dist/apache_airflow*providers_*.whl",
          "309: }",
          "311: function twine_check_provider_packages_from_sdist() {",
          "312:     echo",
          "313:     echo \"Twine check all provider packages from sdist\"",
          "314:     echo",
          "315:     twine check /dist/apache-airflow-*providers-*.tar.gz",
          "316: }",
          "",
          "---------------"
        ],
        "scripts/in_container/run_install_and_test_provider_packages.sh||scripts/in_container/run_install_and_test_provider_packages.sh": [
          "File: scripts/in_container/run_install_and_test_provider_packages.sh -> scripts/in_container/run_install_and_test_provider_packages.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "90:     group_end",
          "91: }",
          "93: function discover_all_provider_packages() {",
          "94:     group_start \"Listing available providers via 'airflow providers list'\"",
          "95:     # Columns is to force it wider, so it doesn't wrap at 80 characters",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "93: function twine_check_provider_packages() {",
          "94:     group_start \"Twine check provider packages\"",
          "95:     if [[ ${PACKAGE_FORMAT} == \"wheel\" ]]; then",
          "96:        twine_check_provider_packages_from_wheels",
          "97:     elif [[ ${PACKAGE_FORMAT} == \"sdist\" ]]; then",
          "98:        twine_check_provider_packages_from_sdist",
          "99:     else",
          "100:         echo",
          "101:         echo \"${COLOR_RED}ERROR: Wrong package format ${PACKAGE_FORMAT}. Should be wheel or sdist${COLOR_RESET}\"",
          "102:         echo",
          "103:         exit 1",
          "104:     fi",
          "105:     group_end",
          "106: }",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "225: setup_provider_packages",
          "226: verify_parameters",
          "227: install_airflow_as_specified",
          "228: install_provider_packages",
          "229: import_all_provider_classes",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "245: if [[ ${SKIP_TWINE_CHECK=\"\"} != \"true\" ]]; then",
          "246:     # Airflow 2.1.0 installs importlib_metadata version that does not work well with twine",
          "247:     # So we should skip twine check in this case",
          "248:     twine_check_provider_packages",
          "249: fi",
          "",
          "---------------"
        ],
        "setup.py||setup.py": [
          "File: setup.py -> setup.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "556:     'pytest-httpx',",
          "557:     'requests_mock',",
          "558:     'semver',",
          "559:     'wheel',",
          "560:     'yamllint',",
          "561: ]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "559:     'twine',",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "96f88a6ed9b3ea8bc331e384a716bd81f95594e1",
      "candidate_info": {
        "commit_hash": "96f88a6ed9b3ea8bc331e384a716bd81f95594e1",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/96f88a6ed9b3ea8bc331e384a716bd81f95594e1",
        "files": [
          "airflow/models/dag.py",
          "airflow/providers/google/cloud/utils/credentials_provider.py",
          "airflow/providers/google/cloud/utils/mlengine_prediction_summary.py",
          "airflow/providers/google/common/utils/id_token_credentials.py",
          "airflow/sensors/base.py",
          "docs/apache-airflow-providers-cncf-kubernetes/index.rst",
          "docs/apache-airflow/executor/kubernetes.rst",
          "docs/conf.py",
          "docs/exts/docs_build/docs_builder.py",
          "docs/exts/docs_build/run_patched_sphinx.py",
          "docs/exts/exampleinclude.py",
          "docs/spelling_wordlist.txt",
          "setup.py"
        ],
        "message": "Update Sphinx and Sphinx-AutoAPI (#20079)\n\nWe were stuck on an old version of Sphinx AutoAPI for a long while as\nmore recent versions wouldn't build Airflow's docs, but that seems to\nhave finally been resolved.\n\nWe can remove the run_patched_sphinx.py as that was included in\nsphinx-autoapi 1.1\n\n* Fix doc rendering glitch in Google provider utils\n\n* Remove duplicated link from cncf-kubernetes provider index\n\n(cherry picked from commit fa96b093952f96449d6d328a2b9e9300b81cf08e)",
        "before_after_code_files": [
          "airflow/models/dag.py||airflow/models/dag.py",
          "airflow/providers/google/cloud/utils/credentials_provider.py||airflow/providers/google/cloud/utils/credentials_provider.py",
          "airflow/providers/google/cloud/utils/mlengine_prediction_summary.py||airflow/providers/google/cloud/utils/mlengine_prediction_summary.py",
          "airflow/providers/google/common/utils/id_token_credentials.py||airflow/providers/google/common/utils/id_token_credentials.py",
          "airflow/sensors/base.py||airflow/sensors/base.py",
          "setup.py||setup.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/models/dag.py||airflow/models/dag.py": [
          "File: airflow/models/dag.py -> airflow/models/dag.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "2938: def dag(*dag_args, **dag_kwargs):",
          "2939:     \"\"\"",
          "2940:     Python dag decorator. Wraps a function into an Airflow DAG.",
          "2943:     :param dag_args: Arguments for DAG object",
          "2944:     :type dag_args: Any",
          "",
          "[Removed Lines]",
          "2941:     Accepts kwargs for operator kwarg. Can be used to parametrize DAGs.",
          "",
          "[Added Lines]",
          "2941:     Accepts kwargs for operator kwarg. Can be used to parameterize DAGs.",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "2998:     from airflow.models.serialized_dag import SerializedDagModel",
          "3000:     DagModel.serialized_dag = relationship(SerializedDagModel)",
          "3003: class DagContext:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "3001:     \"\"\":sphinx-autoapi-skip:\"\"\"",
          "",
          "---------------"
        ],
        "airflow/providers/google/cloud/utils/credentials_provider.py||airflow/providers/google/cloud/utils/credentials_provider.py": [
          "File: airflow/providers/google/cloud/utils/credentials_provider.py -> airflow/providers/google/cloud/utils/credentials_provider.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "78: @contextmanager",
          "79: def provide_gcp_credentials(key_file_path: Optional[str] = None, key_file_dict: Optional[Dict] = None):",
          "80:     \"\"\"",
          "84:     It can be used to provide credentials for external programs (e.g. gcloud) that expect authorization",
          "85:     file in ``GOOGLE_APPLICATION_CREDENTIALS`` environment variable.",
          "",
          "[Removed Lines]",
          "81:     Context manager that provides a Google Cloud credentials for application supporting `Application",
          "82:     Default Credentials (ADC) strategy <https://cloud.google.com/docs/authentication/production>`__.",
          "",
          "[Added Lines]",
          "81:     Context manager that provides a Google Cloud credentials for application supporting",
          "82:     `Application Default Credentials (ADC) strategy`__.",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "88:     :type key_file_path: str",
          "89:     :param key_file_dict: Dictionary with credentials.",
          "90:     :type key_file_dict: Dict",
          "91:     \"\"\"",
          "92:     if not key_file_path and not key_file_dict:",
          "93:         raise ValueError(\"Please provide `key_file_path` or `key_file_dict`.\")",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "92:     __ https://cloud.google.com/docs/authentication/production",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "145:     Context manager that provides both:",
          "147:     - Google Cloud credentials for application supporting `Application Default Credentials (ADC)",
          "149:     - temporary value of :envvar:`AIRFLOW_CONN_GOOGLE_CLOUD_DEFAULT` connection",
          "151:     :param key_file_path: Path to file with Google Cloud Service Account .json file.",
          "",
          "[Removed Lines]",
          "148:       strategy <https://cloud.google.com/docs/authentication/production>`__.",
          "",
          "[Added Lines]",
          "150:       strategy`__.",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "154:     :type scopes: Sequence",
          "155:     :param project_id: The id of Google Cloud project for the connection.",
          "156:     :type project_id: str",
          "157:     \"\"\"",
          "158:     with ExitStack() as stack:",
          "159:         if key_file_path:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "160:     __ https://cloud.google.com/docs/authentication/production",
          "",
          "---------------"
        ],
        "airflow/providers/google/cloud/utils/mlengine_prediction_summary.py||airflow/providers/google/cloud/utils/mlengine_prediction_summary.py": [
          "File: airflow/providers/google/cloud/utils/mlengine_prediction_summary.py -> airflow/providers/google/cloud/utils/mlengine_prediction_summary.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "101:             \"--temp_location=gs://...\",",
          "102:         ]",
          "103:     )",
          "104: \"\"\"",
          "106: import argparse",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "105: .. spelling::",
          "107:     pcoll",
          "",
          "---------------"
        ],
        "airflow/providers/google/common/utils/id_token_credentials.py||airflow/providers/google/common/utils/id_token_credentials.py": [
          "File: airflow/providers/google/common/utils/id_token_credentials.py -> airflow/providers/google/common/utils/id_token_credentials.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "24:     ID_TOKEN=\"$(python -m airflow.providers.google.common.utils.id_token_credentials)\"",
          "25:     curl \"https://www.googleapis.com/oauth2/v3/tokeninfo?id_token=${ID_TOKEN}\" -v",
          "26: \"\"\"",
          "28: import json",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "27: .. spelling::",
          "29:     RefreshError",
          "",
          "---------------"
        ],
        "airflow/sensors/base.py||airflow/sensors/base.py": [
          "File: airflow/sensors/base.py -> airflow/sensors/base.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "19: import datetime",
          "20: import hashlib",
          "22: import time",
          "23: from datetime import timedelta",
          "24: from typing import Any, Callable, Dict, Iterable",
          "",
          "[Removed Lines]",
          "21: import os",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "39: # We need to keep the import here because GCSToLocalFilesystemOperator released in",
          "40: # Google Provider before 3.0.0 imported apply_defaults from here.",
          "41: # See  https://github.com/apache/airflow/issues/16035",
          "45: class BaseSensorOperator(BaseOperator, SkipMixin):",
          "",
          "[Removed Lines]",
          "42: from airflow.utils.decorators import apply_defaults",
          "",
          "[Added Lines]",
          "41: from airflow.utils.decorators import apply_defaults  # noqa: F401",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "122:             raise AirflowException(\"The timeout must be a non-negative number\")",
          "123:         if self.mode not in self.valid_modes:",
          "124:             raise AirflowException(",
          "132:             )",
          "134:     def poke(self, context: Dict) -> bool:",
          "",
          "[Removed Lines]",
          "125:                 \"The mode must be one of {valid_modes},\"",
          "126:                 \"'{d}.{t}'; received '{m}'.\".format(",
          "127:                     valid_modes=self.valid_modes,",
          "128:                     d=self.dag.dag_id if self.has_dag() else \"\",",
          "129:                     t=self.task_id,",
          "130:                     m=self.mode,",
          "131:                 )",
          "",
          "[Added Lines]",
          "124:                 f\"The mode must be one of {self.valid_modes},'{self.dag.dag_id if self.has_dag() else ''} \"",
          "125:                 f\".{self.task_id}'; received '{self.mode}'.\"",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "324:         return cls_type",
          "326:     return decorate(cls)",
          "",
          "[Removed Lines]",
          "329: if 'BUILDING_AIRFLOW_DOCS' in os.environ:",
          "330:     # flake8: noqa: F811",
          "331:     # Monkey patch hook to get good function headers while building docs",
          "332:     apply_defaults = lambda x: x",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "setup.py||setup.py": [
          "File: setup.py -> setup.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "251: doc = [",
          "252:     'click>=7.1,<9',",
          "253:     # Sphinx is limited to < 3.5.0 because of https://github.com/sphinx-doc/sphinx/issues/8880",
          "255:     'sphinx-airflow-theme',",
          "256:     'sphinx-argparse>=0.1.13',",
          "258:     'sphinx-copybutton',",
          "259:     'sphinx-jinja~=1.1',",
          "260:     'sphinx-rtd-theme>=0.1.6',",
          "261:     'sphinxcontrib-httpdomain>=1.7.0',",
          "262:     'sphinxcontrib-redoc>=1.6.0',",
          "264: ]",
          "265: docker = [",
          "266:     'docker>=5.0.3',",
          "",
          "[Removed Lines]",
          "254:     'sphinx>=2.1.2, <3.5.0',",
          "257:     'sphinx-autoapi==1.0.0',",
          "263:     'sphinxcontrib-spelling==7.2.1',",
          "",
          "[Added Lines]",
          "254:     'sphinx>=3.5.0, <5.0.0',",
          "257:     'sphinx-autoapi==1.8.0',",
          "263:     'sphinxcontrib-spelling~=7.3',",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "436f452ab8e32bfd5997e9650d1cfc490a41b0e4",
      "candidate_info": {
        "commit_hash": "436f452ab8e32bfd5997e9650d1cfc490a41b0e4",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/436f452ab8e32bfd5997e9650d1cfc490a41b0e4",
        "files": [
          "airflow/jobs/base_job.py",
          "airflow/migrations/versions/587bdf053233_adding_index_for_dag_id_in_job.py",
          "docs/apache-airflow/migrations-ref.rst"
        ],
        "message": "Fix slow DAG deletion due to missing ``dag_id`` index for job table (#20282)\n\nFixes #20249\n\n(cherry picked from commit ac9f29da200c208bb52d412186c5a1b936eb0b5a)",
        "before_after_code_files": [
          "airflow/jobs/base_job.py||airflow/jobs/base_job.py",
          "airflow/migrations/versions/587bdf053233_adding_index_for_dag_id_in_job.py||airflow/migrations/versions/587bdf053233_adding_index_for_dag_id_in_job.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/jobs/base_job.py||airflow/jobs/base_job.py": [
          "File: airflow/jobs/base_job.py -> airflow/jobs/base_job.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "71:     __table_args__ = (",
          "72:         Index('job_type_heart', job_type, latest_heartbeat),",
          "73:         Index('idx_job_state_heartbeat', state, latest_heartbeat),",
          "74:     )",
          "76:     task_instances_enqueued = relationship(",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "74:         Index('idx_job_dag_id', dag_id),",
          "",
          "---------------"
        ],
        "airflow/migrations/versions/587bdf053233_adding_index_for_dag_id_in_job.py||airflow/migrations/versions/587bdf053233_adding_index_for_dag_id_in_job.py": [
          "File: airflow/migrations/versions/587bdf053233_adding_index_for_dag_id_in_job.py -> airflow/migrations/versions/587bdf053233_adding_index_for_dag_id_in_job.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: #",
          "2: # Licensed to the Apache Software Foundation (ASF) under one",
          "3: # or more contributor license agreements.  See the NOTICE file",
          "4: # distributed with this work for additional information",
          "5: # regarding copyright ownership.  The ASF licenses this file",
          "6: # to you under the Apache License, Version 2.0 (the",
          "7: # \"License\"); you may not use this file except in compliance",
          "8: # with the License.  You may obtain a copy of the License at",
          "9: #",
          "10: #   http://www.apache.org/licenses/LICENSE-2.0",
          "11: #",
          "12: # Unless required by applicable law or agreed to in writing,",
          "13: # software distributed under the License is distributed on an",
          "14: # \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
          "15: # KIND, either express or implied.  See the License for the",
          "16: # specific language governing permissions and limitations",
          "17: # under the License.",
          "19: \"\"\"adding index for dag_id in job",
          "21: Revision ID: 587bdf053233",
          "22: Revises: f9da662e7089",
          "23: Create Date: 2021-12-14 10:20:12.482940",
          "25: \"\"\"",
          "27: from alembic import op",
          "29: # revision identifiers, used by Alembic.",
          "30: revision = '587bdf053233'",
          "31: down_revision = 'f9da662e7089'",
          "32: branch_labels = None",
          "33: depends_on = None",
          "36: def upgrade():",
          "37:     \"\"\"Apply adding index for dag_id in job\"\"\"",
          "38:     op.create_index('idx_job_dag_id', 'job', ['dag_id'], unique=False)",
          "41: def downgrade():",
          "42:     \"\"\"Unapply adding index for dag_id in job\"\"\"",
          "43:     op.drop_index('idx_job_dag_id', table_name='job')",
          "",
          "---------------"
        ]
      }
    }
  ]
}