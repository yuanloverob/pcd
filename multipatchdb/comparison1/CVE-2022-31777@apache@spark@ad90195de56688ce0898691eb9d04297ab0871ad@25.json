{
  "cve_id": "CVE-2022-31777",
  "cve_desc": "A stored cross-site scripting (XSS) vulnerability in Apache Spark 3.2.1 and earlier, and 3.3.0, allows remote attackers to execute arbitrary JavaScript in the web browser of a user, by including a malicious payload into the logs which would be returned in logs rendered in the UI.",
  "repo": "apache/spark",
  "patch_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
  "patch_info": {
    "commit_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
    "repo": "apache/spark",
    "commit_url": "https://github.com/apache/spark/commit/ad90195de56688ce0898691eb9d04297ab0871ad",
    "files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ],
    "message": "[SPARK-39505][UI] Escape log content rendered in UI\n\n### What changes were proposed in this pull request?\n\nEscape log content rendered to the UI.\n\n### Why are the changes needed?\n\nLog content may contain reserved characters or other code in the log and be misinterpreted in the UI as HTML.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nExisting tests\n\nCloses #36902 from srowen/LogViewEscape.\n\nAuthored-by: Sean Owen <srowen@gmail.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
    "before_after_code_files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ]
  },
  "patch_diff": {
    "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js": [
      "File: core/src/main/resources/org/apache/spark/ui/static/log-view.js -> core/src/main/resources/org/apache/spark/ui/static/log-view.js",
      "--- Hunk 1 ---",
      "[Context before]",
      "85:       if (retStartByte == 0) {",
      "86:         disableMoreButton();",
      "87:       }",
      "90:       curLogLength = curLogLength + (startByte - retStartByte);",
      "91:       startByte = retStartByte;",
      "",
      "[Removed Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(cleanData);",
      "",
      "[Added Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(document.createTextNode(cleanData));",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "115:             var retLogLength = dataInfo[2];",
      "117:             var cleanData = data.substring(newlineIndex + 1);",
      "120:             curLogLength = curLogLength + (retEndByte - retStartByte);",
      "121:             endByte = retEndByte;",
      "",
      "[Removed Lines]",
      "118:             $(\"pre\", \".log-content\").append(cleanData);",
      "",
      "[Added Lines]",
      "118:             $(\"pre\", \".log-content\").append(document.createTextNode(cleanData));",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "3f77be288ffee792ef6bb49c65132f48b269e142",
      "candidate_info": {
        "commit_hash": "3f77be288ffee792ef6bb49c65132f48b269e142",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/3f77be288ffee792ef6bb49c65132f48b269e142",
        "files": [
          "dev/create-release/release-build.sh"
        ],
        "message": "[SPARK-39240][INFRA][BUILD] Source and binary releases using different tool to generate hashes for integrity\n\n### What changes were proposed in this pull request?\n\nunify the hash generator for release files.\n\n### Why are the changes needed?\n\nCurrently, we use `shasum` for source but `gpg` for binary, since https://github.com/apache/spark/pull/30123\n\nthis confuses me when validating the integrities of spark 3.3.0 RC https://dist.apache.org/repos/dist/dev/spark/v3.3.0-rc2-bin/\n\n### Does this PR introduce _any_ user-facing change?\n\nno\n\n### How was this patch tested?\n\ntest script manually\n\nCloses #36619 from yaooqinn/SPARK-39240.\n\nAuthored-by: Kent Yao <yao@apache.org>\nSigned-off-by: Sean Owen <srowen@gmail.com>\n(cherry picked from commit 3e783375097d14f1c28eb9b0e08075f1f8daa4a2)\nSigned-off-by: Sean Owen <srowen@gmail.com>",
        "before_after_code_files": [
          "dev/create-release/release-build.sh||dev/create-release/release-build.sh"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/create-release/release-build.sh||dev/create-release/release-build.sh": [
          "File: dev/create-release/release-build.sh -> dev/create-release/release-build.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "283:       echo $GPG_PASSPHRASE | $GPG --passphrase-fd 0 --armour \\",
          "284:         --output $R_DIST_NAME.asc \\",
          "285:         --detach-sig $R_DIST_NAME",
          "289:     fi",
          "291:     if [[ -n $PIP_FLAG ]]; then",
          "",
          "[Removed Lines]",
          "286:       echo $GPG_PASSPHRASE | $GPG --passphrase-fd 0 --print-md \\",
          "287:         SHA512 $R_DIST_NAME > \\",
          "288:         $R_DIST_NAME.sha512",
          "",
          "[Added Lines]",
          "286:       shasum -a 512 $R_DIST_NAME > $R_DIST_NAME.sha512",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "296:       echo $GPG_PASSPHRASE | $GPG --passphrase-fd 0 --armour \\",
          "297:         --output $PYTHON_DIST_NAME.asc \\",
          "298:         --detach-sig $PYTHON_DIST_NAME",
          "302:     fi",
          "304:     echo \"Copying and signing regular binary distribution\"",
          "",
          "[Removed Lines]",
          "299:       echo $GPG_PASSPHRASE | $GPG --passphrase-fd 0 --print-md \\",
          "300:         SHA512 $PYTHON_DIST_NAME > \\",
          "301:         $PYTHON_DIST_NAME.sha512",
          "",
          "[Added Lines]",
          "297:       shasum -a 512 $PYTHON_DIST_NAME > $PYTHON_DIST_NAME.sha512",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "306:     echo $GPG_PASSPHRASE | $GPG --passphrase-fd 0 --armour \\",
          "307:       --output spark-$SPARK_VERSION-bin-$NAME.tgz.asc \\",
          "308:       --detach-sig spark-$SPARK_VERSION-bin-$NAME.tgz",
          "312:   }",
          "314:   # List of binary packages built. Populates two associative arrays, where the key is the \"name\" of",
          "",
          "[Removed Lines]",
          "309:     echo $GPG_PASSPHRASE | $GPG --passphrase-fd 0 --print-md \\",
          "310:       SHA512 spark-$SPARK_VERSION-bin-$NAME.tgz > \\",
          "311:       spark-$SPARK_VERSION-bin-$NAME.tgz.sha512",
          "",
          "[Added Lines]",
          "305:     shasum -a 512 spark-$SPARK_VERSION-bin-$NAME.tgz > spark-$SPARK_VERSION-bin-$NAME.tgz.sha512",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "4f69c98ae95681cf972fa6701c94dbbb28e40d80",
      "candidate_info": {
        "commit_hash": "4f69c98ae95681cf972fa6701c94dbbb28e40d80",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/4f69c98ae95681cf972fa6701c94dbbb28e40d80",
        "files": [
          "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/orc/OrcQuerySuite.scala"
        ],
        "message": "[SPARK-39830][SQL][TESTS][3.3] Add a test case to read ORC table that requires type promotion\n\n### What changes were proposed in this pull request?\nIncrease ORC test coverage.\n[ORC-1205](https://issues.apache.org/jira/browse/ORC-1205) Size of batches in some ConvertTreeReaders should be ensured before using\n\n### Why are the changes needed?\n\nWhen spark reads an orc with type promotion, an `ArrayIndexOutOfBoundsException` may be thrown, which has been fixed in version 1.7.6 and 1.8.0.\n\n```java\njava.lang.ArrayIndexOutOfBoundsException: 1\n        at org.apache.orc.impl.TreeReaderFactory$TreeReader.nextVector(TreeReaderFactory.java:387)\n        at org.apache.orc.impl.TreeReaderFactory$LongTreeReader.nextVector(TreeReaderFactory.java:740)\n        at org.apache.orc.impl.ConvertTreeReaderFactory$StringGroupFromAnyIntegerTreeReader.nextVector(ConvertTreeReaderFactory.java:1069)\n        at org.apache.orc.impl.reader.tree.StructBatchReader.readBatchColumn(StructBatchReader.java:65)\n```\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\nadd UT\n\nCloses #37808 from cxzl25/SPARK-39830-3.3.\n\nAuthored-by: sychen <sychen@ctrip.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
        "before_after_code_files": [
          "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/orc/OrcQuerySuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/orc/OrcQuerySuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/orc/OrcQuerySuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/orc/OrcQuerySuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/orc/OrcQuerySuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/orc/OrcQuerySuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "832:       }",
          "833:     }",
          "834:   }",
          "835: }",
          "837: class OrcV1QuerySuite extends OrcQuerySuite {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "836:   test(\"SPARK-39830: Reading ORC table that requires type promotion may throw AIOOBE\") {",
          "837:     withSQLConf(\"orc.stripe.size\" -> \"20480\",",
          "838:       \"orc.rows.between.memory.checks\" -> \"1\") {",
          "839:       withTempPath { dir =>",
          "840:         val path = dir.getCanonicalPath",
          "841:         val df = spark.range(1, 3072, 1, 1).map { i =>",
          "842:           if (i < 1024) {",
          "843:             (i, Array.fill[Byte](1024)('X'))",
          "844:           } else {",
          "845:             (i, Array.fill[Byte](1)('X'))",
          "846:           }",
          "847:         }.toDF(\"c1\", \"c2\")",
          "848:         df.write.format(\"orc\").save(path)",
          "849:         withSQLConf(SQLConf.ORC_VECTORIZED_READER_BATCH_SIZE.key -> \"1025\") {",
          "850:           withTable(\"t1\") {",
          "851:             spark.sql(s\"create table t1 (c1 string,c2 binary) using orc location '$path'\")",
          "852:             spark.sql(\"select * from t1\").collect()",
          "853:           }",
          "854:         }",
          "855:       }",
          "856:     }",
          "857:   }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "6c4e07dbe38ade7be9051cc16667f2e75cac6b3e",
      "candidate_info": {
        "commit_hash": "6c4e07dbe38ade7be9051cc16667f2e75cac6b3e",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/6c4e07dbe38ade7be9051cc16667f2e75cac6b3e",
        "files": [
          "core/src/main/resources/error/error-classes.json",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryErrorsBase.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/types/StructTypeSuite.scala",
          "sql/core/src/test/resources/sql-tests/results/ansi/cast.sql.out",
          "sql/core/src/test/resources/sql-tests/results/ansi/date.sql.out",
          "sql/core/src/test/resources/sql-tests/results/ansi/datetime-parsing-invalid.sql.out",
          "sql/core/src/test/resources/sql-tests/results/ansi/interval.sql.out",
          "sql/core/src/test/resources/sql-tests/results/ansi/string-functions.sql.out",
          "sql/core/src/test/resources/sql-tests/results/pivot.sql.out",
          "sql/core/src/test/resources/sql-tests/results/postgreSQL/boolean.sql.out",
          "sql/core/src/test/resources/sql-tests/results/postgreSQL/float4.sql.out",
          "sql/core/src/test/resources/sql-tests/results/postgreSQL/float8.sql.out",
          "sql/core/src/test/resources/sql-tests/results/postgreSQL/text.sql.out",
          "sql/core/src/test/resources/sql-tests/results/postgreSQL/window_part2.sql.out",
          "sql/core/src/test/resources/sql-tests/results/postgreSQL/window_part3.sql.out",
          "sql/core/src/test/resources/sql-tests/results/postgreSQL/window_part4.sql.out",
          "sql/core/src/test/resources/sql-tests/results/timestampNTZ/timestamp-ansi.sql.out",
          "sql/core/src/test/resources/sql-tests/results/udf/udf-pivot.sql.out",
          "sql/core/src/test/scala/org/apache/spark/sql/connector/InsertIntoTests.scala"
        ],
        "message": "[SPARK-39255][SQL][3.3] Improve error messages\n\n### What changes were proposed in this pull request?\nIn the PR, I propose to improve errors of the following error classes:\n1. NON_PARTITION_COLUMN - `a non-partition column name` -> `the non-partition column`\n2. UNSUPPORTED_SAVE_MODE - `a not existent path` -> `a non existent path`.\n3. INVALID_FIELD_NAME. Quote ids to follow the rules https://github.com/apache/spark/pull/36621.\n4. FAILED_SET_ORIGINAL_PERMISSION_BACK. It is renamed to RESET_PERMISSION_TO_ORIGINAL.\n5. NON_LITERAL_PIVOT_VALUES - Wrap error's expression by double quotes. The PR adds new helper method `toSQLExpr()` for that.\n6. CAST_INVALID_INPUT - Add the recommendation: `... Correct the syntax for the value before casting it, or change the type to one appropriate for the value.`\n\nThis is a backport of https://github.com/apache/spark/pull/36635.\n\n### Why are the changes needed?\nTo improve user experience with Spark SQL by making error message more clear.\n\n### Does this PR introduce _any_ user-facing change?\nYes, it changes user-facing error messages.\n\n### How was this patch tested?\nBy running the affected test suites:\n```\n$ build/sbt \"sql/testOnly org.apache.spark.sql.SQLQueryTestSuite\"\n$ build/sbt \"sql/testOnly *QueryCompilationErrorsDSv2Suite\"\n$ build/sbt \"sql/testOnly *QueryCompilationErrorsSuite\"\n$ build/sbt \"sql/testOnly *QueryExecutionAnsiErrorsSuite\"\n$ build/sbt \"sql/testOnly *QueryExecutionErrorsSuite\"\n$ build/sbt \"sql/testOnly *QueryParsingErrorsSuite*\"\n```\n\nLead-authored-by: Max Gekk <max.gekkgmail.com>\nCo-authored-by: Maxim Gekk <max.gekkgmail.com>\nSigned-off-by: Max Gekk <max.gekkgmail.com>\n(cherry picked from commit 625afb4e1aefda59191d79b31f8c94941aedde1e)\nSigned-off-by: Max Gekk <max.gekkgmail.com>\n\nCloses #36655 from MaxGekk/error-class-improve-msg-3-3.3.\n\nAuthored-by: Max Gekk <max.gekk@gmail.com>\nSigned-off-by: Max Gekk <max.gekk@gmail.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryErrorsBase.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryErrorsBase.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/types/StructTypeSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/types/StructTypeSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/connector/InsertIntoTests.scala||sql/core/src/test/scala/org/apache/spark/sql/connector/InsertIntoTests.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "82:   def nonLiteralPivotValError(pivotVal: Expression): Throwable = {",
          "83:     new AnalysisException(",
          "84:       errorClass = \"NON_LITERAL_PIVOT_VALUES\",",
          "86:   }",
          "88:   def pivotValDataTypeMismatchError(pivotVal: Expression, pivotCol: Expression): Throwable = {",
          "",
          "[Removed Lines]",
          "85:       messageParameters = Array(pivotVal.toString))",
          "",
          "[Added Lines]",
          "85:       messageParameters = Array(toSQLExpr(pivotVal)))",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "2371:   def invalidFieldName(fieldName: Seq[String], path: Seq[String], context: Origin): Throwable = {",
          "2372:     new AnalysisException(",
          "2373:       errorClass = \"INVALID_FIELD_NAME\",",
          "2375:       origin = context)",
          "2376:   }",
          "",
          "[Removed Lines]",
          "2374:       messageParameters = Array(fieldName.quoted, path.quoted),",
          "",
          "[Added Lines]",
          "2374:       messageParameters = Array(toSQLId(fieldName), toSQLId(path)),",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryErrorsBase.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryErrorsBase.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryErrorsBase.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryErrorsBase.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: import java.util.Locale",
          "24: import org.apache.spark.sql.types.{DataType, DoubleType, FloatType}",
          "",
          "[Removed Lines]",
          "22: import org.apache.spark.sql.catalyst.expressions.Literal",
          "23: import org.apache.spark.sql.catalyst.util.quoteIdentifier",
          "",
          "[Added Lines]",
          "22: import org.apache.spark.sql.catalyst.expressions.{Expression, Literal}",
          "23: import org.apache.spark.sql.catalyst.util.{quoteIdentifier, toPrettySQL}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "84:   def toDSOption(option: String): String = {",
          "85:     quoteByDefault(option)",
          "86:   }",
          "87: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "90:   def toSQLExpr(e: Expression): String = {",
          "91:     quoteByDefault(toPrettySQL(e))",
          "92:   }",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "1720:       permission: FsPermission,",
          "1721:       path: Path,",
          "1722:       e: Throwable): Throwable = {",
          "1724:       Array(permission.toString, path.toString, e.getMessage))",
          "1725:   }",
          "",
          "[Removed Lines]",
          "1723:     new SparkSecurityException(errorClass = \"FAILED_SET_ORIGINAL_PERMISSION_BACK\",",
          "",
          "[Added Lines]",
          "1723:     new SparkSecurityException(errorClass = \"RESET_PERMISSION_TO_ORIGINAL\",",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/types/StructTypeSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/types/StructTypeSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/types/StructTypeSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/types/StructTypeSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "319:     var e = intercept[AnalysisException] {",
          "320:       check(Seq(\"S1\", \"S12\", \"S123\"), None)",
          "321:     }",
          "325:     e = intercept[AnalysisException] {",
          "",
          "[Removed Lines]",
          "322:     assert(e.getMessage.contains(\"Field name S1.S12.S123 is invalid: s1.s12 is not a struct\"))",
          "",
          "[Added Lines]",
          "322:     assert(e.getMessage.contains(",
          "323:       \"Field name `S1`.`S12`.`S123` is invalid: `s1`.`s12` is not a struct\"))",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "333:     e = intercept[AnalysisException] {",
          "334:       check(Seq(\"m1\", \"key\"), None)",
          "335:     }",
          "337:     checkCollection(Seq(\"m1\", \"key\"), Some(Seq(\"m1\") -> StructField(\"key\", IntegerType, false)))",
          "338:     checkCollection(Seq(\"M1\", \"value\"), Some(Seq(\"m1\") -> StructField(\"value\", IntegerType)))",
          "339:     e = intercept[AnalysisException] {",
          "340:       checkCollection(Seq(\"M1\", \"key\", \"name\"), None)",
          "341:     }",
          "343:     e = intercept[AnalysisException] {",
          "344:       checkCollection(Seq(\"M1\", \"value\", \"name\"), None)",
          "345:     }",
          "349:     checkCollection(Seq(\"M2\", \"key\", \"A\"),",
          "",
          "[Removed Lines]",
          "336:     assert(e.getMessage.contains(\"Field name m1.key is invalid: m1 is not a struct\"))",
          "342:     assert(e.getMessage.contains(\"Field name M1.key.name is invalid: m1.key is not a struct\"))",
          "346:     assert(e.getMessage.contains(\"Field name M1.value.name is invalid: m1.value is not a struct\"))",
          "",
          "[Added Lines]",
          "337:     assert(e.getMessage.contains(\"Field name `m1`.`key` is invalid: `m1` is not a struct\"))",
          "343:     assert(e.getMessage.contains(",
          "344:       \"Field name `M1`.`key`.`name` is invalid: `m1`.`key` is not a struct\"))",
          "348:     assert(e.getMessage.contains(",
          "349:       \"Field name `M1`.`value`.`name` is invalid: `m1`.`value` is not a struct\"))",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "355:     e = intercept[AnalysisException] {",
          "356:       checkCollection(Seq(\"m2\", \"key\", \"A\", \"name\"), None)",
          "357:     }",
          "359:     e = intercept[AnalysisException] {",
          "360:       checkCollection(Seq(\"M2\", \"value\", \"b\", \"name\"), None)",
          "361:     }",
          "362:     assert(e.getMessage.contains(",
          "366:     e = intercept[AnalysisException] {",
          "367:       check(Seq(\"A1\", \"element\"), None)",
          "368:     }",
          "370:     checkCollection(Seq(\"A1\", \"element\"), Some(Seq(\"a1\") -> StructField(\"element\", IntegerType)))",
          "371:     e = intercept[AnalysisException] {",
          "372:       checkCollection(Seq(\"A1\", \"element\", \"name\"), None)",
          "373:     }",
          "374:     assert(e.getMessage.contains(",
          "378:     checkCollection(Seq(\"A2\", \"element\", \"C\"),",
          "",
          "[Removed Lines]",
          "358:     assert(e.getMessage.contains(\"Field name m2.key.A.name is invalid: m2.key.a is not a struct\"))",
          "363:       \"Field name M2.value.b.name is invalid: m2.value.b is not a struct\"))",
          "369:     assert(e.getMessage.contains(\"Field name A1.element is invalid: a1 is not a struct\"))",
          "375:       \"Field name A1.element.name is invalid: a1.element is not a struct\"))",
          "",
          "[Added Lines]",
          "361:     assert(e.getMessage.contains(",
          "362:       \"Field name `m2`.`key`.`A`.`name` is invalid: `m2`.`key`.`a` is not a struct\"))",
          "367:       \"Field name `M2`.`value`.`b`.`name` is invalid: `m2`.`value`.`b` is not a struct\"))",
          "373:     assert(e.getMessage.contains(\"Field name `A1`.`element` is invalid: `a1` is not a struct\"))",
          "379:       \"Field name `A1`.`element`.`name` is invalid: `a1`.`element` is not a struct\"))",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "382:       checkCollection(Seq(\"a2\", \"element\", \"C\", \"name\"), None)",
          "383:     }",
          "384:     assert(e.getMessage.contains(",
          "386:   }",
          "388:   test(\"SPARK-36807: Merge ANSI interval types to a tightest common type\") {",
          "",
          "[Removed Lines]",
          "385:       \"Field name a2.element.C.name is invalid: a2.element.c is not a struct\"))",
          "",
          "[Added Lines]",
          "389:       \"Field name `a2`.`element`.`C`.`name` is invalid: `a2`.`element`.`c` is not a struct\"))",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/connector/InsertIntoTests.scala||sql/core/src/test/scala/org/apache/spark/sql/connector/InsertIntoTests.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/connector/InsertIntoTests.scala -> sql/core/src/test/scala/org/apache/spark/sql/connector/InsertIntoTests.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "260:         verifyTable(t1, spark.emptyDataFrame)",
          "261:         assert(exc.getMessage.contains(",
          "263:         assert(exc.getMessage.contains(\"id\"))",
          "264:         assert(exc.getErrorClass == \"NON_PARTITION_COLUMN\")",
          "265:       }",
          "",
          "[Removed Lines]",
          "262:           \"PARTITION clause cannot contain a non-partition column name\"))",
          "",
          "[Added Lines]",
          "262:           \"PARTITION clause cannot contain the non-partition column\"))",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "277:         verifyTable(t1, spark.emptyDataFrame)",
          "278:         assert(exc.getMessage.contains(",
          "280:         assert(exc.getMessage.contains(\"data\"))",
          "281:         assert(exc.getErrorClass == \"NON_PARTITION_COLUMN\")",
          "282:       }",
          "",
          "[Removed Lines]",
          "279:           \"PARTITION clause cannot contain a non-partition column name\"))",
          "",
          "[Added Lines]",
          "279:           \"PARTITION clause cannot contain the non-partition column\"))",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "0e998d31234f08be956c5bd2dec0b086952c2e18",
      "candidate_info": {
        "commit_hash": "0e998d31234f08be956c5bd2dec0b086952c2e18",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/0e998d31234f08be956c5bd2dec0b086952c2e18",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/csv/CSVInferSchema.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/json/JsonInferSchema.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/TimestampFormatter.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/TimestampFormatterSuite.scala"
        ],
        "message": "[SPARK-39193][SQL] Fasten Timestamp type inference of JSON/CSV data sources\n\n### What changes were proposed in this pull request?\n\nWhen reading JSON/CSV files with inferring timestamp types (`.option(\"inferTimestamp\", true)`), the Timestamp conversion will throw and catch exceptions.\nAs we are putting decent error messages in the exception:\n```\n  def cannotCastToDateTimeError(\n      value: Any, from: DataType, to: DataType, errorContext: String): Throwable = {\n    val valueString = toSQLValue(value, from)\n    new SparkDateTimeException(\"INVALID_SYNTAX_FOR_CAST\",\n      Array(toSQLType(to), valueString, SQLConf.ANSI_ENABLED.key, errorContext))\n  }\n```\nThrowing and catching the timestamp parsing exceptions is actually not cheap. It consumes more than 90% of the type inference time.\n\nThis PR improves the default timestamp parsing by returning optional results instead of throwing/catching the exceptions. With this PR, the schema inference time is reduced by 90% in a local benchmark.\n\nNote this PR is for the default timestamp parser. It doesn't cover the scenarios of\n* users provide a customized timestamp format via option\n* users enable legacy timestamp formatter\nWe can have follow-ups for it.\n\n### Why are the changes needed?\n\nPerformance improvement\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nExisting UT\nAlso manual test the runtime to inferring a JSON file of 624MB with inferring timestamp enabled:\n```\nspark.read.option(\"inferTimestamp\", true).json(file)\n```\n\nBefore the change, it takes 166 seconds\nAfter the change, it only 16 seconds.\n\nCloses #36562 from gengliangwang/improveInferTS.\n\nLead-authored-by: Gengliang Wang <gengliang@apache.org>\nCo-authored-by: Gengliang Wang <ltnwgl@gmail.com>\nSigned-off-by: Gengliang Wang <gengliang@apache.org>\n(cherry picked from commit 888bf1b2ef44a27c3d4be716a72175bbaa8c6453)\nSigned-off-by: Gengliang Wang <gengliang@apache.org>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/csv/CSVInferSchema.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/csv/CSVInferSchema.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/json/JsonInferSchema.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/json/JsonInferSchema.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/TimestampFormatter.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/TimestampFormatter.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/TimestampFormatterSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/TimestampFormatterSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/csv/CSVInferSchema.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/csv/CSVInferSchema.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/csv/CSVInferSchema.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/csv/CSVInferSchema.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "182:       SQLConf.get.timestampType",
          "183:     } else {",
          "184:       tryParseTimestamp(field)",
          "",
          "[Removed Lines]",
          "181:     if ((allCatch opt timestampNTZFormatter.parseWithoutTimeZone(field, false)).isDefined) {",
          "",
          "[Added Lines]",
          "181:     if (timestampNTZFormatter.parseWithoutTimeZoneOptional(field, false).isDefined) {",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "188:   private def tryParseTimestamp(field: String): DataType = {",
          "191:       TimestampType",
          "192:     } else {",
          "193:       tryParseBoolean(field)",
          "",
          "[Removed Lines]",
          "190:     if ((allCatch opt timestampParser.parse(field)).isDefined) {",
          "",
          "[Added Lines]",
          "190:     if (timestampParser.parseOptional(field).isDefined) {",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/json/JsonInferSchema.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/json/JsonInferSchema.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/json/JsonInferSchema.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/json/JsonInferSchema.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "151:         if (options.prefersDecimal && decimalTry.isDefined) {",
          "152:           decimalTry.get",
          "153:         } else if (options.inferTimestamp &&",
          "155:           SQLConf.get.timestampType",
          "156:         } else if (options.inferTimestamp &&",
          "158:           TimestampType",
          "159:         } else {",
          "160:           StringType",
          "",
          "[Removed Lines]",
          "154:             (allCatch opt timestampNTZFormatter.parseWithoutTimeZone(field, false)).isDefined) {",
          "157:             (allCatch opt timestampFormatter.parse(field)).isDefined) {",
          "",
          "[Added Lines]",
          "154:             timestampNTZFormatter.parseWithoutTimeZoneOptional(field, false).isDefined) {",
          "157:             timestampFormatter.parseOptional(field).isDefined) {",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/TimestampFormatter.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/TimestampFormatter.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/TimestampFormatter.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/TimestampFormatter.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "52:   @throws(classOf[DateTimeException])",
          "53:   def parse(s: String): Long",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "64:   @throws(classOf[ParseException])",
          "65:   @throws(classOf[DateTimeParseException])",
          "66:   @throws(classOf[DateTimeException])",
          "67:   def parseOptional(s: String): Option[Long] =",
          "68:     try {",
          "69:       Some(parse(s))",
          "70:     } catch {",
          "71:       case _: Exception => None",
          "72:     }",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "73:       s\"The method `parseWithoutTimeZone(s: String, allowTimeZone: Boolean)` should be \" +",
          "74:         \"implemented in the formatter of timestamp without time zone\")",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "108:   @throws(classOf[ParseException])",
          "109:   @throws(classOf[DateTimeParseException])",
          "110:   @throws(classOf[DateTimeException])",
          "111:   @throws(classOf[IllegalStateException])",
          "112:   def parseWithoutTimeZoneOptional(s: String, allowTimeZone: Boolean): Option[Long] =",
          "113:     try {",
          "114:       Some(parseWithoutTimeZone(s, allowTimeZone))",
          "115:     } catch {",
          "116:       case _: Exception => None",
          "117:     }",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "204:     } catch checkParsedDiff(s, legacyFormatter.parse)",
          "205:   }",
          "207:   override def parseWithoutTimeZone(s: String, allowTimeZone: Boolean): Long = {",
          "208:     try {",
          "209:       val utf8Value = UTF8String.fromString(s)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "250:   override def parseOptional(s: String): Option[Long] =",
          "251:     DateTimeUtils.stringToTimestamp(UTF8String.fromString(s), zoneId)",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "213:       }",
          "214:     } catch checkParsedDiff(s, legacyFormatter.parse)",
          "215:   }",
          "216: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "263:   override def parseWithoutTimeZoneOptional(s: String, allowTimeZone: Boolean): Option[Long] = {",
          "264:     val utf8Value = UTF8String.fromString(s)",
          "265:     DateTimeUtils.stringToTimestampWithoutTimeZone(utf8Value, allowTimeZone)",
          "266:   }",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/TimestampFormatterSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/TimestampFormatterSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/TimestampFormatterSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/TimestampFormatterSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "456:       assert(errMsg.contains(\"\"\"Invalid input syntax for type \"TIMESTAMP\": 'x123'\"\"\"))",
          "457:     }",
          "458:   }",
          "459: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "460:   test(\"SPARK-39193: support returning optional parse results in the default formatter\") {",
          "461:     val formatter = new DefaultTimestampFormatter(",
          "462:       DateTimeTestUtils.LA,",
          "463:       locale = DateFormatter.defaultLocale,",
          "464:       legacyFormat = LegacyDateFormats.SIMPLE_DATE_FORMAT,",
          "465:       isParsing = true)",
          "466:     assert(formatter.parseOptional(\"2021-01-01T00:00:00\").contains(1609488000000000L))",
          "467:     assert(",
          "468:       formatter.parseWithoutTimeZoneOptional(\"2021-01-01T00:00:00\", false)",
          "469:         .contains(1609459200000000L))",
          "470:     assert(formatter.parseOptional(\"abc\").isEmpty)",
          "471:     assert(",
          "472:       formatter.parseWithoutTimeZoneOptional(\"abc\", false).isEmpty)",
          "473:   }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "109904e8fd1a7543e23c1cdb6c86af490bc75a4e",
      "candidate_info": {
        "commit_hash": "109904e8fd1a7543e23c1cdb6c86af490bc75a4e",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/109904e8fd1a7543e23c1cdb6c86af490bc75a4e",
        "files": [
          "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/ExecutorRollPlugin.scala",
          "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/scheduler/cluster/k8s/ExecutorRollPluginSuite.scala"
        ],
        "message": "[SPARK-39327][K8S] ExecutorRollPolicy.ID should consider ID as a numerical value\n\nThis PR aims to make `ExecutorRollPolicy.ID` should consider ID as a numerical value.\n\nCurrently, the ExecutorRollPolicy chooses the smallest ID from string sorting.\n\nNo, 3.3.0 is not released yet.\n\nPass the CIs.\n\nCloses #36715 from williamhyun/SPARK-39327.\n\nAuthored-by: William Hyun <william@apache.org>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>\n(cherry picked from commit 97f4b0cc1b20ca641d0e968e0b0fb45557085115)\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
        "before_after_code_files": [
          "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/ExecutorRollPlugin.scala||resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/ExecutorRollPlugin.scala",
          "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/scheduler/cluster/k8s/ExecutorRollPluginSuite.scala||resource-managers/kubernetes/core/src/test/scala/org/apache/spark/scheduler/cluster/k8s/ExecutorRollPluginSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/ExecutorRollPlugin.scala||resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/ExecutorRollPlugin.scala": [
          "File: resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/ExecutorRollPlugin.scala -> resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/ExecutorRollPlugin.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "106:       .filter(_.totalTasks >= minTasks)",
          "107:     val sortedList = policy match {",
          "108:       case ExecutorRollPolicy.ID =>",
          "110:       case ExecutorRollPolicy.ADD_TIME =>",
          "111:         listWithoutDriver.sortBy(_.addTime)",
          "112:       case ExecutorRollPolicy.TOTAL_GC_TIME =>",
          "",
          "[Removed Lines]",
          "109:         listWithoutDriver.sortBy(_.id)",
          "",
          "[Added Lines]",
          "110:         listWithoutDriver.sortBy(_.id.toInt)",
          "",
          "---------------"
        ],
        "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/scheduler/cluster/k8s/ExecutorRollPluginSuite.scala||resource-managers/kubernetes/core/src/test/scala/org/apache/spark/scheduler/cluster/k8s/ExecutorRollPluginSuite.scala": [
          "File: resource-managers/kubernetes/core/src/test/scala/org/apache/spark/scheduler/cluster/k8s/ExecutorRollPluginSuite.scala -> resource-managers/kubernetes/core/src/test/scala/org/apache/spark/scheduler/cluster/k8s/ExecutorRollPluginSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "110:     Option.empty, Option.empty, Map(), Option.empty, Set(), Option.empty, Map(), Map(), 1,",
          "111:     false, Set())",
          "113:   val list = Seq(driverSummary, execWithSmallestID, execWithSmallestAddTime,",
          "114:     execWithBiggestTotalGCTime, execWithBiggestTotalDuration, execWithBiggestFailedTasks,",
          "117:   override def beforeEach(): Unit = {",
          "118:     super.beforeEach()",
          "",
          "[Removed Lines]",
          "115:     execWithBiggestAverageDuration, execWithoutTasks, execNormal)",
          "",
          "[Added Lines]",
          "113:   val execWithTwoDigitID = new ExecutorSummary(\"10\", \"host:port\", true, 1,",
          "114:     10, 10, 1, 1, 1,",
          "115:     4, 0, 2, 280,",
          "116:     30, 100, 100,",
          "117:     10, false, 20, new Date(1639300001000L),",
          "118:     Option.empty, Option.empty, Map(), Option.empty, Set(), Option.empty, Map(), Map(), 1,",
          "119:     false, Set())",
          "123:     execWithBiggestAverageDuration, execWithoutTasks, execNormal, execWithTwoDigitID)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "149:   test(\"Policy: ID\") {",
          "150:     assertEquals(Some(\"1\"), plugin.invokePrivate(_choose(list, ExecutorRollPolicy.ID)))",
          "151:   }",
          "153:   test(\"Policy: ADD_TIME\") {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "159:     assertEquals(Some(\"2\"), plugin.invokePrivate(_choose(list.filter(_.id != \"1\"),",
          "160:       ExecutorRollPolicy.ID)))",
          "",
          "---------------"
        ]
      }
    }
  ]
}