{
  "cve_id": "CVE-2023-39441",
  "cve_desc": "Apache Airflow SMTP Provider before 1.3.0, Apache Airflow IMAP Provider before 3.3.0, and\u00a0Apache Airflow before 2.7.0 are affected by the\u00a0Validation of OpenSSL Certificate vulnerability.\n\nThe default SSL context with SSL library did not check a server's X.509\u00a0certificate.\u00a0 Instead, the code accepted any certificate, which could\u00a0result in the disclosure of mail server credentials or mail contents\u00a0when the client connects to an attacker in a MITM position.\n\nUsers are strongly advised to upgrade to Apache Airflow version 2.7.0 or newer, Apache Airflow IMAP Provider version 3.3.0 or newer, and Apache Airflow SMTP Provider version 1.3.0 or newer to mitigate the risk associated with this vulnerability",
  "repo": "apache/airflow",
  "patch_hash": "38fc9cd823feafd8ec61d5d5c7eddb9e9162f755",
  "patch_info": {
    "commit_hash": "38fc9cd823feafd8ec61d5d5c7eddb9e9162f755",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/38fc9cd823feafd8ec61d5d5c7eddb9e9162f755",
    "files": [
      "airflow/providers/imap/CHANGELOG.rst",
      "airflow/providers/imap/hooks/imap.py",
      "airflow/providers/imap/provider.yaml",
      "docs/apache-airflow-providers-imap/configurations-ref.rst",
      "docs/apache-airflow-providers-imap/index.rst",
      "docs/apache-airflow/configurations-ref.rst",
      "tests/providers/imap/hooks/test_imap.py"
    ],
    "message": "Allows to choose SSL context for IMAP provider (#33108)\n\n* Allows to choose SSL context for IMAP provider\n\nThis change add two options to choose from when SSL IMAP connection is created:\n\n* default - for balance between compatibility and security\n* none - in case compatibility with existing infrastructure is preferred\n\nThe fallback is:\n\n* The Airflow \"email\", \"ssl_context\"\n* \"default\"\n\nCo-authored-by: Ephraim Anierobi <splendidzigy24@gmail.com>\n(cherry picked from commit 52ca7bfc988f4c9b608f544bc3e9524fd6564639)",
    "before_after_code_files": [
      "airflow/providers/imap/hooks/imap.py||airflow/providers/imap/hooks/imap.py",
      "tests/providers/imap/hooks/test_imap.py||tests/providers/imap/hooks/test_imap.py"
    ]
  },
  "patch_diff": {
    "airflow/providers/imap/hooks/imap.py||airflow/providers/imap/hooks/imap.py": [
      "File: airflow/providers/imap/hooks/imap.py -> airflow/providers/imap/hooks/imap.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "26: import imaplib",
      "27: import os",
      "28: import re",
      "29: from typing import Any, Iterable",
      "31: from airflow.exceptions import AirflowException",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "29: import ssl",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "78:         return self",
      "80:     def _build_client(self, conn: Connection) -> imaplib.IMAP4_SSL | imaplib.IMAP4:",
      "84:         else:",
      "92:         return mail_client",
      "",
      "[Removed Lines]",
      "81:         IMAP: type[imaplib.IMAP4_SSL] | type[imaplib.IMAP4]",
      "82:         if conn.extra_dejson.get(\"use_ssl\", True):",
      "83:             IMAP = imaplib.IMAP4_SSL",
      "85:             IMAP = imaplib.IMAP4",
      "87:         if conn.port:",
      "88:             mail_client = IMAP(conn.host, conn.port)",
      "89:         else:",
      "90:             mail_client = IMAP(conn.host)",
      "",
      "[Added Lines]",
      "82:         mail_client: imaplib.IMAP4_SSL | imaplib.IMAP4",
      "83:         use_ssl = conn.extra_dejson.get(\"use_ssl\", True)",
      "84:         if use_ssl:",
      "85:             from airflow.configuration import conf",
      "87:             ssl_context_string = conf.get(\"imap\", \"SSL_CONTEXT\", fallback=None)",
      "88:             if ssl_context_string is None:",
      "89:                 ssl_context_string = conf.get(\"email\", \"SSL_CONTEXT\", fallback=None)",
      "90:             if ssl_context_string is None:",
      "91:                 ssl_context_string = \"default\"",
      "92:             if ssl_context_string == \"default\":",
      "93:                 ssl_context = ssl.create_default_context()",
      "94:             elif ssl_context_string == \"none\":",
      "95:                 ssl_context = None",
      "96:             else:",
      "97:                 raise RuntimeError(",
      "98:                     f\"The email.ssl_context configuration variable must \"",
      "99:                     f\"be set to 'default' or 'none' and is '{ssl_context_string}'.\"",
      "100:                 )",
      "101:             if conn.port:",
      "102:                 mail_client = imaplib.IMAP4_SSL(conn.host, conn.port, ssl_context=ssl_context)",
      "103:             else:",
      "104:                 mail_client = imaplib.IMAP4_SSL(conn.host, ssl_context=ssl_context)",
      "106:             if conn.port:",
      "107:                 mail_client = imaplib.IMAP4(conn.host, conn.port)",
      "108:             else:",
      "109:                 mail_client = imaplib.IMAP4(conn.host)",
      "",
      "---------------"
    ],
    "tests/providers/imap/hooks/test_imap.py||tests/providers/imap/hooks/test_imap.py": [
      "File: tests/providers/imap/hooks/test_imap.py -> tests/providers/imap/hooks/test_imap.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "27: from airflow.models import Connection",
      "28: from airflow.providers.imap.hooks.imap import ImapHook",
      "29: from airflow.utils import db",
      "31: imaplib_string = \"airflow.providers.imap.hooks.imap.imaplib\"",
      "32: open_string = \"airflow.providers.imap.hooks.imap.open\"",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "30: from tests.test_utils.config import conf_vars",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "85:         )",
      "87:     @patch(imaplib_string)",
      "89:         mock_conn = _create_fake_imap(mock_imaplib)",
      "91:         with ImapHook():",
      "92:             pass",
      "95:         mock_conn.login.assert_called_once_with(\"imap_user\", \"imap_password\")",
      "96:         assert mock_conn.logout.call_count == 1",
      "",
      "[Removed Lines]",
      "88:     def test_connect_and_disconnect(self, mock_imaplib):",
      "94:         mock_imaplib.IMAP4_SSL.assert_called_once_with(\"imap_server_address\", 1993)",
      "",
      "[Added Lines]",
      "89:     @patch(\"ssl.create_default_context\")",
      "90:     def test_connect_and_disconnect(self, create_default_context, mock_imaplib):",
      "96:         assert create_default_context.called",
      "97:         mock_imaplib.IMAP4_SSL.assert_called_once_with(",
      "98:             \"imap_server_address\", 1993, ssl_context=create_default_context.return_value",
      "99:         )",
      "100:         mock_conn.login.assert_called_once_with(\"imap_user\", \"imap_password\")",
      "101:         assert mock_conn.logout.call_count == 1",
      "103:     @patch(imaplib_string)",
      "104:     @patch(\"ssl.create_default_context\")",
      "105:     def test_connect_and_disconnect_imap_ssl_context_none(self, create_default_context, mock_imaplib):",
      "106:         mock_conn = _create_fake_imap(mock_imaplib)",
      "108:         with conf_vars({(\"imap\", \"ssl_context\"): \"none\"}):",
      "109:             with ImapHook():",
      "110:                 pass",
      "112:         assert not create_default_context.called",
      "113:         mock_imaplib.IMAP4_SSL.assert_called_once_with(\"imap_server_address\", 1993, ssl_context=None)",
      "114:         mock_conn.login.assert_called_once_with(\"imap_user\", \"imap_password\")",
      "115:         assert mock_conn.logout.call_count == 1",
      "117:     @patch(imaplib_string)",
      "118:     @patch(\"ssl.create_default_context\")",
      "119:     def test_connect_and_disconnect_imap_ssl_context_default(self, create_default_context, mock_imaplib):",
      "120:         mock_conn = _create_fake_imap(mock_imaplib)",
      "122:         with conf_vars({(\"imap\", \"ssl_context\"): \"default\"}):",
      "123:             with ImapHook():",
      "124:                 pass",
      "126:         assert create_default_context.called",
      "127:         mock_imaplib.IMAP4_SSL.assert_called_once_with(",
      "128:             \"imap_server_address\", 1993, ssl_context=create_default_context.return_value",
      "129:         )",
      "130:         mock_conn.login.assert_called_once_with(\"imap_user\", \"imap_password\")",
      "131:         assert mock_conn.logout.call_count == 1",
      "133:     @patch(imaplib_string)",
      "134:     @patch(\"ssl.create_default_context\")",
      "135:     def test_connect_and_disconnect_email_ssl_context_none(self, create_default_context, mock_imaplib):",
      "136:         mock_conn = _create_fake_imap(mock_imaplib)",
      "138:         with conf_vars({(\"email\", \"ssl_context\"): \"none\"}):",
      "139:             with ImapHook():",
      "140:                 pass",
      "142:         assert not create_default_context.called",
      "143:         mock_imaplib.IMAP4_SSL.assert_called_once_with(\"imap_server_address\", 1993, ssl_context=None)",
      "144:         mock_conn.login.assert_called_once_with(\"imap_user\", \"imap_password\")",
      "145:         assert mock_conn.logout.call_count == 1",
      "147:     @patch(imaplib_string)",
      "148:     @patch(\"ssl.create_default_context\")",
      "149:     def test_connect_and_disconnect_imap_ssl_context_override(self, create_default_context, mock_imaplib):",
      "150:         mock_conn = _create_fake_imap(mock_imaplib)",
      "152:         with conf_vars({(\"email\", \"ssl_context\"): \"none\", (\"imap\", \"ssl_context\"): \"default\"}):",
      "153:             with ImapHook():",
      "154:                 pass",
      "156:         assert create_default_context.called",
      "157:         mock_imaplib.IMAP4_SSL.assert_called_once_with(",
      "158:             \"imap_server_address\", 1993, ssl_context=create_default_context.return_value",
      "159:         )",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "b501446a2db1c474f5a303b3ea6897da3a60bf77",
      "candidate_info": {
        "commit_hash": "b501446a2db1c474f5a303b3ea6897da3a60bf77",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/b501446a2db1c474f5a303b3ea6897da3a60bf77",
        "files": [
          "airflow/www/app.py",
          "docs/apache-airflow/howto/set-config.rst",
          "docs/apache-airflow/security/webserver.rst"
        ],
        "message": "Pass app context to webserver_config.py (#32759)\n\n(cherry picked from commit 7847b6ead3c039726bb82e0de3a39e5ef5eb00aa)",
        "before_after_code_files": [
          "airflow/www/app.py||airflow/www/app.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/33247"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/www/app.py||airflow/www/app.py": [
          "File: airflow/www/app.py -> airflow/www/app.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "83:     flask_app.config[\"PERMANENT_SESSION_LIFETIME\"] = timedelta(minutes=settings.get_session_lifetime_config())",
          "85:     webserver_config = conf.get_mandatory_value(\"webserver\", \"config_file\")",
          "88:     flask_app.config[\"TESTING\"] = testing",
          "89:     flask_app.config[\"SQLALCHEMY_DATABASE_URI\"] = conf.get(\"database\", \"SQL_ALCHEMY_CONN\")",
          "",
          "[Removed Lines]",
          "86:     flask_app.config.from_pyfile(webserver_config, silent=True)",
          "",
          "[Added Lines]",
          "86:     # Enable customizations in webserver_config.py to be applied via Flask.current_app.",
          "87:     with flask_app.app_context():",
          "88:         flask_app.config.from_pyfile(webserver_config, silent=True)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "0253f6aa1b1837883abdb7387ae1bc703ec352cc",
      "candidate_info": {
        "commit_hash": "0253f6aa1b1837883abdb7387ae1bc703ec352cc",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/0253f6aa1b1837883abdb7387ae1bc703ec352cc",
        "files": [
          "dev/breeze/src/airflow_breeze/commands/developer_commands.py",
          "dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "dev/breeze/src/airflow_breeze/global_constants.py",
          "dev/breeze/src/airflow_breeze/utils/add_back_references.py",
          "dev/breeze/src/airflow_breeze/utils/common_options.py",
          "dev/breeze/tests/test_global_constants.py",
          "images/breeze/output-commands-hash.txt",
          "images/breeze/output_release-management_add-back-references.svg"
        ],
        "message": "Improve back-reference generation (#33149)\n\n* better command line support with better auto-complete\n* allows to run back-referance generation for individual provider packages\n* better typing including using Pathlib everywhere\n* handling of redirection between providers - when an operator is\n  moved from one provider to the other - instead of generating invalid\n  \"stable\" link we generate back reference for all versions of the\n  provider.\n\n(cherry picked from commit 14faf196e901e0a6fdaeb7d55e6e293b7071b501)",
        "before_after_code_files": [
          "dev/breeze/src/airflow_breeze/commands/developer_commands.py||dev/breeze/src/airflow_breeze/commands/developer_commands.py",
          "dev/breeze/src/airflow_breeze/commands/release_management_commands.py||dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "dev/breeze/src/airflow_breeze/global_constants.py||dev/breeze/src/airflow_breeze/global_constants.py",
          "dev/breeze/src/airflow_breeze/utils/add_back_references.py||dev/breeze/src/airflow_breeze/utils/add_back_references.py",
          "dev/breeze/src/airflow_breeze/utils/common_options.py||dev/breeze/src/airflow_breeze/utils/common_options.py",
          "dev/breeze/tests/test_global_constants.py||dev/breeze/tests/test_global_constants.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/33247"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/breeze/src/airflow_breeze/commands/developer_commands.py||dev/breeze/src/airflow_breeze/commands/developer_commands.py": [
          "File: dev/breeze/src/airflow_breeze/commands/developer_commands.py -> dev/breeze/src/airflow_breeze/commands/developer_commands.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "32:     DEFAULT_PYTHON_MAJOR_MINOR_VERSION,",
          "33:     DOCKER_DEFAULT_PLATFORM,",
          "34:     MOUNT_SELECTED,",
          "36: )",
          "37: from airflow_breeze.params.build_ci_params import BuildCiParams",
          "38: from airflow_breeze.params.doc_build_params import DocBuildParams",
          "",
          "[Removed Lines]",
          "35:     get_available_documentation_packages,",
          "",
          "[Added Lines]",
          "35:     get_available_documentation_provider_packages,",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "331: @click.option(",
          "332:     \"--package-filter\",",
          "333:     help=\"List of packages to consider.\",",
          "335:     multiple=True,",
          "336: )",
          "337: @click.option(",
          "",
          "[Removed Lines]",
          "334:     type=NotVerifiedBetterChoice(get_available_documentation_packages()),",
          "",
          "[Added Lines]",
          "334:     type=NotVerifiedBetterChoice(get_available_documentation_provider_packages()),",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/commands/release_management_commands.py||dev/breeze/src/airflow_breeze/commands/release_management_commands.py": [
          "File: dev/breeze/src/airflow_breeze/commands/release_management_commands.py -> dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "43:     MOUNT_ALL,",
          "44:     MOUNT_SELECTED,",
          "45:     MULTI_PLATFORM,",
          "47: )",
          "48: from airflow_breeze.params.shell_params import ShellParams",
          "49: from airflow_breeze.utils.add_back_references import (",
          "",
          "[Removed Lines]",
          "46:     get_available_documentation_packages,",
          "",
          "[Added Lines]",
          "46:     get_available_documentation_provider_packages,",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "783: @click.option(",
          "784:     \"--package-filter\",",
          "785:     help=\"List of packages to consider.\",",
          "787:     multiple=True,",
          "788: )",
          "789: @option_verbose",
          "",
          "[Removed Lines]",
          "786:     type=NotVerifiedBetterChoice(get_available_documentation_packages()),",
          "",
          "[Added Lines]",
          "786:     type=NotVerifiedBetterChoice(get_available_documentation_provider_packages()),",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "821:     \"-a\",",
          "822:     \"--airflow-site-directory\",",
          "823:     envvar=\"AIRFLOW_SITE_DIRECTORY\",",
          "824:     help=\"Local directory path of cloned airflow-site repo.\",",
          "825:     required=True,",
          "826: )",
          "827: @click.option(",
          "828:     \"-g\",",
          "829:     \"--gen-type\",",
          "833: )",
          "834: @option_verbose",
          "835: @option_dry_run",
          "836: def add_back_references(",
          "838:     gen_type: str,",
          "839: ):",
          "840:     \"\"\"Adds back references for documentation generated by build-docs and publish-docs\"\"\"",
          "842:         get_console().print(",
          "843:             \"\\n[error]location pointed by airflow_site_dir is not valid. \"",
          "844:             \"Provide the path of cloned airflow-site repo\\n\"",
          "845:         )",
          "846:         sys.exit(1)",
          "848:     gen = GenerationType[gen_type]",
          "849:     if gen not in GenerationType:",
          "850:         get_console().print(",
          "",
          "[Removed Lines]",
          "830:     help=\"Type of back references to generate, supports: [airflow | providers | helm]\",",
          "831:     type=str,",
          "832:     required=True,",
          "837:     airflow_site_directory: bool,",
          "841:     if not os.path.isdir(airflow_site_directory):",
          "",
          "[Added Lines]",
          "824:     type=click.Path(exists=True, file_okay=False, dir_okay=True, resolve_path=True),",
          "831:     show_default=True,",
          "832:     help=\"Type of back references to generate. Forced to providers if providers specified as arguments.\",",
          "833:     type=BetterChoice(",
          "834:         [e.name for e in GenerationType],",
          "835:     ),",
          "836:     default=GenerationType.airflow.name,",
          "838: @argument_packages",
          "842:     airflow_site_directory: str,",
          "844:     packages: list[str],",
          "847:     site_path = Path(airflow_site_directory)",
          "848:     if not site_path.is_dir():",
          "854:     if len(packages) != 0 and gen_type != GenerationType.providers.name:",
          "855:         get_console().print(",
          "856:             [",
          "857:                 f\"[warning]Forcing gen type to \"",
          "858:                 f\"{GenerationType.providers} as some provider_packages are selected.\"",
          "859:             ]",
          "860:         )",
          "861:         gen_type = GenerationType.providers.name",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "852:         )",
          "853:         sys.exit(1)",
          "858: @release_management.command(",
          "",
          "[Removed Lines]",
          "855:     start_generating_back_references(gen, airflow_site_directory)",
          "",
          "[Added Lines]",
          "869:     start_generating_back_references(gen, site_path, packages)",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/global_constants.py||dev/breeze/src/airflow_breeze/global_constants.py": [
          "File: dev/breeze/src/airflow_breeze/global_constants.py -> dev/breeze/src/airflow_breeze/global_constants.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "145: ALL_HISTORICAL_PYTHON_VERSIONS = [\"3.6\", \"3.7\", \"3.8\", \"3.9\", \"3.10\", \"3.11\"]",
          "149:     provider_names: list[str] = list(json.loads(PROVIDER_DEPENDENCIES_JSON_FILE_PATH.read_text()).keys())",
          "150:     doc_provider_names = [provider_name.replace(\".\", \"-\") for provider_name in provider_names]",
          "151:     available_packages = [f\"apache-airflow-providers-{doc_provider}\" for doc_provider in doc_provider_names]",
          "",
          "[Removed Lines]",
          "148: def get_available_documentation_packages(short_version=False) -> list[str]:",
          "",
          "[Added Lines]",
          "148: def get_available_documentation_provider_packages(short_version=False) -> list[str]:",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/utils/add_back_references.py||dev/breeze/src/airflow_breeze/utils/add_back_references.py": [
          "File: dev/breeze/src/airflow_breeze/utils/add_back_references.py -> dev/breeze/src/airflow_breeze/utils/add_back_references.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "19: import enum",
          "20: import os",
          "21: import tempfile",
          "22: from pathlib import Path",
          "23: from urllib.error import URLError",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "21: import re",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "26: from rich import print",
          "28: airflow_redirects_link = (",
          "29:     \"https://raw.githubusercontent.com/apache/airflow/main/docs/apache-airflow/redirects.txt\"",
          "30: )",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "29: from airflow_breeze.global_constants import get_available_documentation_provider_packages",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "89:     return f\"https://raw.githubusercontent.com/apache/airflow/main/docs/{provider_name}/redirects.txt\"",
          "97:     content = get_redirect_content(back_ref_url)",
          "110:     is_downloaded, file_name = download_file(link)",
          "111:     if not is_downloaded:",
          "112:         old_to_new: list[tuple[str, str]] = []",
          "",
          "[Removed Lines]",
          "92: def get_provider_docs_path(docs_archive_path, provider_name: str):",
          "93:     return docs_archive_path + \"/\" + provider_name",
          "96: def create_back_reference_html(back_ref_url, path):",
          "99:     if Path(path).exists():",
          "100:         print(f\"Skipping file:{path}, redirects already exist\")",
          "101:         return",
          "103:     # creating a back reference html file",
          "104:     with open(path, \"w\") as f:",
          "105:         f.write(content)",
          "106:     print(f\"[green]Created back reference redirect: {path}\")",
          "109: def generate_back_references(link: str, base_path: str):",
          "",
          "[Added Lines]",
          "95: def crete_redirect_html_if_not_exist(path: Path, content: str):",
          "96:     if not path.exists():",
          "97:         path.parent.mkdir(parents=True, exist_ok=True)",
          "98:         path.write_text(content)",
          "99:         print(f\"[green]Created back reference redirect: {path}\")",
          "100:     else:",
          "101:         print(f\"Skipping file:{path}, redirects already exist\")",
          "104: def create_back_reference_html(back_ref_url: str, target_path: Path):",
          "107:     version_match = re.compile(r\"[0-9]+.[0-9]+.[0-9]+\")",
          "108:     target_path_as_posix = target_path.as_posix()",
          "109:     if \"/stable/\" in target_path_as_posix:",
          "110:         prefix, postfix = target_path_as_posix.split(\"/stable/\")",
          "111:         base_folder = Path(prefix)",
          "112:         for folder in base_folder.iterdir():",
          "113:             if folder.is_dir() and version_match.match(folder.name):",
          "114:                 crete_redirect_html_if_not_exist(folder / postfix, content)",
          "115:     else:",
          "116:         crete_redirect_html_if_not_exist(Path(target_path), content)",
          "119: def generate_back_references(link: str, base_path: Path):",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "118:     old_to_new.append((\"security.html\", \"security/security-model.html\"))",
          "120:     versions = [f.path.split(\"/\")[-1] for f in os.scandir(base_path) if f.is_dir()]",
          "122:     for version in versions:",
          "123:         print(f\"Processing {base_path}, version: {version}\")",
          "126:         for old, new in old_to_new:",
          "127:             # only if old file exists, add the back reference",
          "129:                 split_new_path = new.split(\"/\")",
          "130:                 file_name = new.split(\"/\")[-1]",
          "133:                 # finds relative path of old file with respect to new and handles case of different file",
          "134:                 # names also",
          "135:                 relative_path = os.path.relpath(old, new)",
          "136:                 # remove one directory level because file path was used above",
          "137:                 relative_path = relative_path.replace(\"../\", \"\", 1)",
          "139:                 os.makedirs(dest_dir, exist_ok=True)",
          "141:                 create_back_reference_html(relative_path, dest_file_path)",
          "149:     if gen_type == GenerationType.airflow:",
          "150:         generate_back_references(airflow_redirects_link, airflow_docs_path)",
          "151:     elif gen_type == GenerationType.helm:",
          "152:         generate_back_references(helm_redirects_link, helm_docs_path)",
          "153:     elif gen_type == GenerationType.providers:",
          "159:         for p in all_providers:",
          "160:             print(f\"Processing airflow provider: {p}\")",
          "",
          "[Removed Lines]",
          "124:         versioned_provider_path = base_path + \"/\" + version",
          "128:             if os.path.exists(versioned_provider_path + \"/\" + old):",
          "131:                 dest_dir = versioned_provider_path + \"/\" + \"/\".join(split_new_path[: len(split_new_path) - 1])",
          "140:                 dest_file_path = dest_dir + \"/\" + file_name",
          "144: def start_generating_back_references(gen_type, airflow_site_directory):",
          "145:     docs_archive_path = airflow_site_directory + \"/docs-archive\"",
          "146:     airflow_docs_path = docs_archive_path + \"/apache-airflow\"",
          "147:     helm_docs_path = docs_archive_path + \"/helm-chart\"",
          "154:         all_providers = [",
          "155:             f.path.split(\"/\")[-1]",
          "156:             for f in os.scandir(docs_archive_path)",
          "157:             if f.is_dir() and \"providers\" in f.name",
          "158:         ]",
          "161:             generate_back_references(",
          "162:                 get_github_redirects_url(p), get_provider_docs_path(docs_archive_path, p)",
          "163:             )",
          "",
          "[Added Lines]",
          "133:         versioned_provider_path = base_path / version",
          "137:             if os.path.exists(versioned_provider_path / old):",
          "140:                 dest_dir = versioned_provider_path.joinpath(*split_new_path[: len(split_new_path) - 1])",
          "148:                 dest_file_path = dest_dir / file_name",
          "152: def start_generating_back_references(",
          "153:     gen_type: GenerationType, airflow_site_directory: Path, short_provider_package_ids: list[str]",
          "154: ):",
          "155:     # Either packages or gen_type should be provided",
          "156:     docs_archive_path = airflow_site_directory / \"docs-archive\"",
          "157:     airflow_docs_path = docs_archive_path / \"apache-airflow\"",
          "158:     helm_docs_path = docs_archive_path / \"helm-chart\"",
          "165:         if short_provider_package_ids:",
          "166:             all_providers = [",
          "167:                 f\"apache-airflow-providers-{package.replace('.','-')}\"",
          "168:                 for package in short_provider_package_ids",
          "169:             ]",
          "170:         else:",
          "171:             all_providers = get_available_documentation_provider_packages()",
          "174:             generate_back_references(get_github_redirects_url(p), docs_archive_path / p)",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/utils/common_options.py||dev/breeze/src/airflow_breeze/utils/common_options.py": [
          "File: dev/breeze/src/airflow_breeze/utils/common_options.py -> dev/breeze/src/airflow_breeze/utils/common_options.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "43:     SINGLE_PLATFORMS,",
          "44:     START_AIRFLOW_ALLOWED_EXECUTORS,",
          "45:     START_AIRFLOW_DEFAULT_ALLOWED_EXECUTORS,",
          "47: )",
          "48: from airflow_breeze.utils.custom_param_types import (",
          "49:     AnswerChoice,",
          "",
          "[Removed Lines]",
          "46:     get_available_documentation_packages,",
          "",
          "[Added Lines]",
          "46:     get_available_documentation_provider_packages,",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "448:     \"packages\",",
          "449:     nargs=-1,",
          "450:     required=False,",
          "452: )",
          "453: option_airflow_constraints_reference = click.option(",
          "454:     \"--airflow-constraints-reference\",",
          "",
          "[Removed Lines]",
          "451:     type=BetterChoice(get_available_documentation_packages(short_version=True)),",
          "",
          "[Added Lines]",
          "451:     type=BetterChoice(get_available_documentation_provider_packages(short_version=True)),",
          "",
          "---------------"
        ],
        "dev/breeze/tests/test_global_constants.py||dev/breeze/tests/test_global_constants.py": [
          "File: dev/breeze/tests/test_global_constants.py -> dev/breeze/tests/test_global_constants.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "16: # under the License.",
          "17: from __future__ import annotations",
          "21: AVAILABLE_PACKAGES_STARTING_LIST = (\"apache-airflow\", \"helm-chart\", \"docker-stack\")",
          "24: def test_get_available_packages():",
          "27:         assert package.startswith(AVAILABLE_PACKAGES_STARTING_LIST)",
          "",
          "[Removed Lines]",
          "19: from airflow_breeze.global_constants import get_available_documentation_packages",
          "25:     assert len(get_available_documentation_packages()) > 70",
          "26:     for package in get_available_documentation_packages():",
          "",
          "[Added Lines]",
          "19: from airflow_breeze.global_constants import get_available_documentation_provider_packages",
          "25:     assert len(get_available_documentation_provider_packages()) > 70",
          "26:     for package in get_available_documentation_provider_packages():",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "78f4344e0b5ccb90b28a2b9e6aa34cd5aa0698b8",
      "candidate_info": {
        "commit_hash": "78f4344e0b5ccb90b28a2b9e6aa34cd5aa0698b8",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/78f4344e0b5ccb90b28a2b9e6aa34cd5aa0698b8",
        "files": [
          "airflow/models/base.py",
          "airflow/models/baseoperator.py",
          "airflow/models/dag.py",
          "airflow/models/dagbag.py",
          "airflow/models/expandinput.py",
          "airflow/models/taskmixin.py"
        ],
        "message": "Refactor: Simplify code in models (#33181)\n\n(cherry picked from commit 5a0494f83e8ad0e5cbf0d3dcad3022a3ea89d789)",
        "before_after_code_files": [
          "airflow/models/base.py||airflow/models/base.py",
          "airflow/models/baseoperator.py||airflow/models/baseoperator.py",
          "airflow/models/dag.py||airflow/models/dag.py",
          "airflow/models/dagbag.py||airflow/models/dagbag.py",
          "airflow/models/expandinput.py||airflow/models/expandinput.py",
          "airflow/models/taskmixin.py||airflow/models/taskmixin.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/33247"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/models/base.py||airflow/models/base.py": [
          "File: airflow/models/base.py -> airflow/models/base.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "69:         # We cannot use session/dialect as at this point we are trying to determine the right connection",
          "70:         # parameters, so we use the connection",
          "71:         conn = conf.get(\"database\", \"sql_alchemy_conn\", fallback=\"\")",
          "73:             return {\"collation\": \"utf8mb3_bin\"}",
          "74:         return {}",
          "",
          "[Removed Lines]",
          "72:         if conn.startswith(\"mysql\") or conn.startswith(\"mariadb\"):",
          "",
          "[Added Lines]",
          "72:         if conn.startswith((\"mysql\", \"mariadb\")):",
          "",
          "---------------"
        ],
        "airflow/models/baseoperator.py||airflow/models/baseoperator.py": [
          "File: airflow/models/baseoperator.py -> airflow/models/baseoperator.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "414:                 if arg not in kwargs and arg in default_args:",
          "415:                     kwargs[arg] = default_args[arg]",
          "418:             if len(missing_args) == 1:",
          "419:                 raise AirflowException(f\"missing keyword argument {missing_args.pop()!r}\")",
          "420:             elif missing_args:",
          "",
          "[Removed Lines]",
          "417:             missing_args = non_optional_args - set(kwargs)",
          "",
          "[Added Lines]",
          "417:             missing_args = non_optional_args.difference(kwargs)",
          "",
          "---------------"
        ],
        "airflow/models/dag.py||airflow/models/dag.py": [
          "File: airflow/models/dag.py -> airflow/models/dag.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "740:         for c in self._comps:",
          "741:             # task_ids returns a list and lists can't be hashed",
          "742:             if c == \"task_ids\":",
          "744:             else:",
          "745:                 val = getattr(self, c, None)",
          "746:             try:",
          "",
          "[Removed Lines]",
          "743:                 val = tuple(self.task_dict.keys())",
          "",
          "[Added Lines]",
          "743:                 val = tuple(self.task_dict)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1257:     @property",
          "1258:     def task_ids(self) -> list[str]:",
          "1261:     @property",
          "1262:     def teardowns(self) -> list[Operator]:",
          "",
          "[Removed Lines]",
          "1259:         return list(self.task_dict.keys())",
          "",
          "[Added Lines]",
          "1259:         return list(self.task_dict)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "2897:         log.info(\"Sync %s DAGs\", len(dags))",
          "2898:         dag_by_ids = {dag.dag_id: dag for dag in dags}",
          "2901:         query = (",
          "2902:             select(DagModel)",
          "2903:             .options(joinedload(DagModel.tags, innerjoin=False))",
          "",
          "[Removed Lines]",
          "2900:         dag_ids = set(dag_by_ids.keys())",
          "",
          "[Added Lines]",
          "2900:         dag_ids = set(dag_by_ids)",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "3235:                 \"auto_register\",",
          "3236:                 \"fail_stop\",",
          "3237:             }",
          "3239:         return cls.__serialized_fields",
          "3241:     def get_edge_info(self, upstream_task_id: str, downstream_task_id: str) -> EdgeInfoType:",
          "",
          "[Removed Lines]",
          "3238:             cls.__serialized_fields = frozenset(vars(DAG(dag_id=\"test\")).keys()) - exclusion_list",
          "",
          "[Added Lines]",
          "3238:             cls.__serialized_fields = frozenset(vars(DAG(dag_id=\"test\"))) - exclusion_list",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "3594:                 .having(func.count() == func.sum(case((DDRQ.target_dag_id.is_not(None), 1), else_=0)))",
          "3595:             )",
          "3596:         }",
          "3598:         if dataset_triggered_dag_ids:",
          "3610:                 )",
          "3612:             if exclusion_list:",
          "3613:                 dataset_triggered_dag_ids -= exclusion_list",
          "3614:                 dataset_triggered_dag_info = {",
          "",
          "[Removed Lines]",
          "3597:         dataset_triggered_dag_ids = set(dataset_triggered_dag_info.keys())",
          "3599:             exclusion_list = {",
          "3600:                 x",
          "3601:                 for x in (",
          "3602:                     session.scalars(",
          "3603:                         select(DagModel.dag_id)",
          "3604:                         .join(DagRun.dag_model)",
          "3605:                         .where(DagRun.state.in_((DagRunState.QUEUED, DagRunState.RUNNING)))",
          "3606:                         .where(DagModel.dag_id.in_(dataset_triggered_dag_ids))",
          "3607:                         .group_by(DagModel.dag_id)",
          "3608:                         .having(func.count() >= func.max(DagModel.max_active_runs))",
          "3609:                     )",
          "3611:             }",
          "",
          "[Added Lines]",
          "3597:         dataset_triggered_dag_ids = set(dataset_triggered_dag_info)",
          "3599:             exclusion_list = set(",
          "3600:                 session.scalars(",
          "3601:                     select(DagModel.dag_id)",
          "3602:                     .join(DagRun.dag_model)",
          "3603:                     .where(DagRun.state.in_((DagRunState.QUEUED, DagRunState.RUNNING)))",
          "3604:                     .where(DagModel.dag_id.in_(dataset_triggered_dag_ids))",
          "3605:                     .group_by(DagModel.dag_id)",
          "3606:                     .having(func.count() >= func.max(DagModel.max_active_runs))",
          "3608:             )",
          "",
          "---------------"
        ],
        "airflow/models/dagbag.py||airflow/models/dagbag.py": [
          "File: airflow/models/dagbag.py -> airflow/models/dagbag.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "170:         :return: a list of DAG IDs in this bag",
          "171:         \"\"\"",
          "174:     @provide_session",
          "175:     def get_dag(self, dag_id, session: Session = None):",
          "",
          "[Removed Lines]",
          "172:         return list(self.dags.keys())",
          "",
          "[Added Lines]",
          "172:         return list(self.dags)",
          "",
          "---------------"
        ],
        "airflow/models/expandinput.py||airflow/models/expandinput.py": [
          "File: airflow/models/expandinput.py -> airflow/models/expandinput.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "169:         def _find_index_for_this_field(index: int) -> int:",
          "170:             # Need to use the original user input to retain argument order.",
          "172:                 mapped_length = all_lengths[mapped_key]",
          "173:                 if mapped_length < 1:",
          "174:                     raise RuntimeError(f\"cannot expand field mapped to length {mapped_length!r}\")",
          "",
          "[Removed Lines]",
          "171:             for mapped_key in reversed(list(self.value)):",
          "",
          "[Added Lines]",
          "171:             for mapped_key in reversed(self.value):",
          "",
          "---------------"
        ],
        "airflow/models/taskmixin.py||airflow/models/taskmixin.py": [
          "File: airflow/models/taskmixin.py -> airflow/models/taskmixin.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "39: class DependencyMixin:",
          "42:     @property",
          "43:     def roots(self) -> Sequence[DependencyMixin]:",
          "",
          "[Removed Lines]",
          "40:     \"\"\"Mixing implementing common dependency setting methods methods like >> and <<.\"\"\"",
          "",
          "[Added Lines]",
          "40:     \"\"\"Mixing implementing common dependency setting methods like >> and <<.\"\"\"",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "0655d883ae391c52ad0b9126cf6932cf641faa41",
      "candidate_info": {
        "commit_hash": "0655d883ae391c52ad0b9126cf6932cf641faa41",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/0655d883ae391c52ad0b9126cf6932cf641faa41",
        "files": [
          "airflow/utils/log/file_task_handler.py",
          "tests/utils/test_log_handlers.py"
        ],
        "message": "Skip served logs for non-running task try (#32561)\n\nCo-authored-by: eladkal <45845474+eladkal@users.noreply.github.com>\n(cherry picked from commit 29d5e955fca5e6bee30b14ac9fcf85eebc94ae6d)",
        "before_after_code_files": [
          "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py",
          "tests/utils/test_log_handlers.py||tests/utils/test_log_handlers.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/33247"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py": [
          "File: airflow/utils/log/file_task_handler.py -> airflow/utils/log/file_task_handler.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "306:         executor_messages: list[str] = []",
          "307:         executor_logs: list[str] = []",
          "308:         served_logs: list[str] = []",
          "309:         with suppress(NotImplementedError):",
          "310:             remote_messages, remote_logs = self._read_remote_logs(ti, try_number, metadata)",
          "311:             messages_list.extend(remote_messages)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "309:         is_running = ti.try_number == try_number and ti.state in (",
          "310:             TaskInstanceState.RUNNING,",
          "311:             TaskInstanceState.DEFERRED,",
          "312:         )",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "320:             worker_log_full_path = Path(self.local_base, worker_log_rel_path)",
          "321:             local_messages, local_logs = self._read_from_local(worker_log_full_path)",
          "322:             messages_list.extend(local_messages)",
          "324:             served_messages, served_logs = self._read_from_logs_server(ti, worker_log_rel_path)",
          "325:             messages_list.extend(served_messages)",
          "326:         elif ti.state not in State.unfinished and not (local_logs or remote_logs):",
          "",
          "[Removed Lines]",
          "323:         if ti.state in (TaskInstanceState.RUNNING, TaskInstanceState.DEFERRED) and not executor_messages:",
          "",
          "[Added Lines]",
          "327:         if is_running and not executor_messages:",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "340:         )",
          "341:         log_pos = len(logs)",
          "342:         messages = \"\".join([f\"*** {x}\\n\" for x in messages_list])",
          "347:         if metadata and \"log_pos\" in metadata:",
          "348:             previous_chars = metadata[\"log_pos\"]",
          "349:             logs = logs[previous_chars:]  # Cut off previously passed log test as new tail",
          "350:         out_message = logs if \"log_pos\" in (metadata or {}) else messages + logs",
          "353:     @staticmethod",
          "354:     def _get_pod_namespace(ti: TaskInstance):",
          "",
          "[Removed Lines]",
          "343:         end_of_log = ti.try_number != try_number or ti.state not in (",
          "344:             TaskInstanceState.RUNNING,",
          "345:             TaskInstanceState.DEFERRED,",
          "346:         )",
          "351:         return out_message, {\"end_of_log\": end_of_log, \"log_pos\": log_pos}",
          "",
          "[Added Lines]",
          "351:         return out_message, {\"end_of_log\": not is_running, \"log_pos\": log_pos}",
          "",
          "---------------"
        ],
        "tests/utils/test_log_handlers.py||tests/utils/test_log_handlers.py": [
          "File: tests/utils/test_log_handlers.py -> tests/utils/test_log_handlers.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "299:     def test__read_for_celery_executor_fallbacks_to_worker(self, create_task_instance):",
          "300:         \"\"\"Test for executors which do not have `get_task_log` method, it fallbacks to reading",
          "302:         executor_name = \"CeleryExecutor\"",
          "304:         ti = create_task_instance(",
          "",
          "[Removed Lines]",
          "301:         log from worker\"\"\"",
          "",
          "[Added Lines]",
          "301:         log from worker. But it happens only for the latest try_number.\"\"\"",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "308:             execution_date=DEFAULT_DATE,",
          "309:         )",
          "310:         ti.state = TaskInstanceState.RUNNING",
          "311:         with conf_vars({(\"core\", \"executor\"): executor_name}):",
          "312:             fth = FileTaskHandler(\"\")",
          "314:             fth._read_from_logs_server = mock.Mock()",
          "315:             fth._read_from_logs_server.return_value = [\"this message\"], [\"this\\nlog\\ncontent\"]",
          "317:             fth._read_from_logs_server.assert_called_once()",
          "320:     @pytest.mark.parametrize(",
          "321:         \"remote_logs, local_logs, served_logs_checked\",",
          "",
          "[Removed Lines]",
          "316:             actual = fth._read(ti=ti, try_number=1)",
          "318:         assert actual == (\"*** this message\\nthis\\nlog\\ncontent\", {\"end_of_log\": True, \"log_pos\": 16})",
          "",
          "[Added Lines]",
          "311:         ti.try_number = 2",
          "317:             actual = fth._read(ti=ti, try_number=2)",
          "319:             assert actual == (\"*** this message\\nthis\\nlog\\ncontent\", {\"end_of_log\": False, \"log_pos\": 16})",
          "321:             # Previous try_number is from remote logs without reaching worker server",
          "322:             fth._read_from_logs_server.reset_mock()",
          "323:             fth._read_remote_logs = mock.Mock()",
          "324:             fth._read_remote_logs.return_value = [\"remote logs\"], [\"remote\\nlog\\ncontent\"]",
          "325:             actual = fth._read(ti=ti, try_number=1)",
          "326:             fth._read_remote_logs.assert_called_once()",
          "327:             fth._read_from_logs_server.assert_not_called()",
          "328:             assert actual == (\"*** remote logs\\nremote\\nlog\\ncontent\", {\"end_of_log\": True, \"log_pos\": 18})",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "b54f5030371ba4fcdc8437f06fc7916362e2a1a8",
      "candidate_info": {
        "commit_hash": "b54f5030371ba4fcdc8437f06fc7916362e2a1a8",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/b54f5030371ba4fcdc8437f06fc7916362e2a1a8",
        "files": [
          "Dockerfile.ci",
          "airflow/cli/cli_config.py",
          "airflow/cli/commands/db_command.py",
          "airflow/config_templates/config.yml",
          "airflow/config_templates/config.yml.schema.json",
          "airflow/config_templates/unit_tests.cfg",
          "airflow/configuration.py",
          "airflow/utils/db.py",
          "dev/perf/sql_queries.py",
          "docs/apache-airflow-providers-amazon/connections/aws.rst",
          "docs/exts/includes/sections-and-options.rst",
          "docs/helm-chart/airflow-configuration.rst",
          "newsfragments/33136.significant.rst",
          "scripts/docker/entrypoint_ci.sh",
          "scripts/in_container/check_environment.sh",
          "tests/core/test_configuration.py"
        ],
        "message": "Fix edge cases of \"migrate/create-default-connections\" (#33136)\n\nIn #32810, \"airflow db migrate\" command has been added and it\nis used by `start-airflow` command. There were a few edge cases\nnot covered and this PR completes it.\n\n* We can move the \"database/load_default_connections\" configuration\n  to a new \"deprecated\" section. This is the first time we remove\n  the option completely as it lost its meaning, but we likely\n  still want to explain that the option was there and what it does\n  when deprecated \"db init\" command is used.\n\nIt has no meaning when you use \"airflow db migrate\" or when you run the\nnew \"airflow connections create-default-connections\" commands. So we can\nnow remove it completely from configuraiton. It will still work\nin the \"airflow db init\" which is deprecated, as long as we provide an\nexplicit fallback. Also if someone had it defined in their config or\nenv variable, it will continue to work even if it is not defined.\n\n* We need to explain the change in a significant newsfragment.\n\n* The ``start-airflow`` command supports creating default connections\n  with ``--load-default-connections`` flag. This was lost after\n  the change so this PR brings it back by running the new\n  \"airflow connections create-default-connections\" command if the\n  flag is used.\n\n* The `start-airflow` breeze command can be used to start older\n  versions of airflow - with ``--use-airflow-version\" - those that do not\n  support `airflow db migrate` command. In this case the old behaviour is\n  used with setting the \"AIRFLOW__DATABASE__LOAD_DEFAULT_CONNECTIONS\"\n  based on the flag passed and running \"airflow db init\" instead.\n\n(cherry picked from commit b672ba478cdf579d159d5ddd4c823a36e606f168)",
        "before_after_code_files": [
          "Dockerfile.ci||Dockerfile.ci",
          "airflow/cli/cli_config.py||airflow/cli/cli_config.py",
          "airflow/cli/commands/db_command.py||airflow/cli/commands/db_command.py",
          "airflow/config_templates/unit_tests.cfg||airflow/config_templates/unit_tests.cfg",
          "airflow/configuration.py||airflow/configuration.py",
          "airflow/utils/db.py||airflow/utils/db.py",
          "dev/perf/sql_queries.py||dev/perf/sql_queries.py",
          "scripts/docker/entrypoint_ci.sh||scripts/docker/entrypoint_ci.sh",
          "scripts/in_container/check_environment.sh||scripts/in_container/check_environment.sh",
          "tests/core/test_configuration.py||tests/core/test_configuration.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/33247"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "Dockerfile.ci||Dockerfile.ci": [
          "File: Dockerfile.ci -> Dockerfile.ci",
          "--- Hunk 1 ---",
          "[Context before]",
          "929:     cd \"${AIRFLOW_SOURCES}\"",
          "931:     if [[ ${START_AIRFLOW:=\"false\"} == \"true\" || ${START_AIRFLOW} == \"True\" ]]; then",
          "933:         export AIRFLOW__CORE__LOAD_EXAMPLES=${LOAD_EXAMPLES}",
          "934:         wait_for_asset_compilation",
          "935:         # shellcheck source=scripts/in_container/bin/run_tmux",
          "",
          "[Removed Lines]",
          "932:         export AIRFLOW__DATABASE__LOAD_DEFAULT_CONNECTIONS=${LOAD_DEFAULT_CONNECTIONS}",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "airflow/cli/cli_config.py||airflow/cli/cli_config.py": [
          "File: airflow/cli/cli_config.py -> airflow/cli/cli_config.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1520:         name=\"init\",",
          "1521:         help=(",
          "1522:             \"Deprecated -- use `migrate` instead. \"",
          "1524:             \"Initialize the metadata database\"",
          "1525:         ),",
          "1526:         func=lazy_load_command(\"airflow.cli.commands.db_command.initdb\"),",
          "",
          "[Removed Lines]",
          "1523:             \"To create default connections use `connections create-default-connections`. \"",
          "",
          "[Added Lines]",
          "1523:             \"To create default connections use `airflow connections create-default-connections`. \"",
          "",
          "---------------"
        ],
        "airflow/cli/commands/db_command.py||airflow/cli/commands/db_command.py": [
          "File: airflow/cli/commands/db_command.py -> airflow/cli/commands/db_command.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "42:     \"\"\"Initializes the metadata database.\"\"\"",
          "43:     warnings.warn(",
          "44:         \"`db init` is deprecated.  Use `db migrate` instead to migrate the db and/or \"",
          "46:         DeprecationWarning,",
          "47:     )",
          "48:     print(\"DB: \" + repr(settings.engine.url))",
          "",
          "[Removed Lines]",
          "45:         \"create-default-connections to create the default connections\",",
          "",
          "[Added Lines]",
          "45:         \"airflow connections create-default-connections to create the default connections\",",
          "",
          "---------------"
        ],
        "airflow/config_templates/unit_tests.cfg||airflow/config_templates/unit_tests.cfg": [
          "File: airflow/config_templates/unit_tests.cfg -> airflow/config_templates/unit_tests.cfg",
          "--- Hunk 1 ---",
          "[Context before]",
          "59: killed_task_cleanup_time = 5",
          "60: # We only allow our own classes to be deserialized in tests",
          "61: allowed_deserialization_classes = airflow\\..* tests\\..*",
          "62: [database]",
          "66: [logging]",
          "67: # celery tests rely on it being set",
          "",
          "[Removed Lines]",
          "63: # we want to have default connections loaded in unit tests",
          "64: load_default_connections = True",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "airflow/configuration.py||airflow/configuration.py": [
          "File: airflow/configuration.py -> airflow/configuration.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "960:         deprecated_section: str | None",
          "961:         deprecated_key: str | None",
          "963:         # For when we rename whole sections",
          "964:         if section in self.inversed_deprecated_sections:",
          "965:             deprecated_section, deprecated_key = (section, key)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "963:         option_description = self.configuration_description.get(section, {}).get(key, {})",
          "964:         if option_description.get(\"deprecated\"):",
          "965:             deprecation_reason = option_description.get(\"deprecation_reason\", \"\")",
          "966:             warnings.warn(",
          "967:                 f\"The '{key}' option in section {section} is deprecated. {deprecation_reason}\",",
          "968:                 DeprecationWarning,",
          "969:                 stacklevel=2 + _extra_stacklevel,",
          "970:             )",
          "",
          "---------------"
        ],
        "airflow/utils/db.py||airflow/utils/db.py": [
          "File: airflow/utils/db.py -> airflow/utils/db.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "748:         upgradedb(session=session)",
          "749:     else:",
          "750:         _create_db_from_orm(session=session)",
          "752:     if conf.getboolean(\"database\", \"LOAD_DEFAULT_CONNECTIONS\") and load_connections:",
          "753:         create_default_connections(session=session)",
          "754:     # Add default pool & sync log_template",
          "",
          "[Removed Lines]",
          "751:     # Load default connections",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "dev/perf/sql_queries.py||dev/perf/sql_queries.py": [
          "File: dev/perf/sql_queries.py -> dev/perf/sql_queries.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "30: os.environ[\"AIRFLOW__CORE__DAGS_FOLDER\"] = DAG_FOLDER",
          "31: os.environ[\"AIRFLOW__DEBUG__SQLALCHEMY_STATS\"] = \"True\"",
          "32: os.environ[\"AIRFLOW__CORE__LOAD_EXAMPLES\"] = \"False\"",
          "35: # Here we setup simpler logger to avoid any code changes in",
          "36: # Airflow core code base",
          "",
          "[Removed Lines]",
          "33: os.environ[\"AIRFLOW__DATABASE__LOAD_DEFAULT_CONNECTIONS\"] = \"True\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "scripts/docker/entrypoint_ci.sh||scripts/docker/entrypoint_ci.sh": [
          "File: scripts/docker/entrypoint_ci.sh -> scripts/docker/entrypoint_ci.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "316:     cd \"${AIRFLOW_SOURCES}\"",
          "318:     if [[ ${START_AIRFLOW:=\"false\"} == \"true\" || ${START_AIRFLOW} == \"True\" ]]; then",
          "320:         export AIRFLOW__CORE__LOAD_EXAMPLES=${LOAD_EXAMPLES}",
          "321:         wait_for_asset_compilation",
          "322:         # shellcheck source=scripts/in_container/bin/run_tmux",
          "",
          "[Removed Lines]",
          "319:         export AIRFLOW__DATABASE__LOAD_DEFAULT_CONNECTIONS=${LOAD_DEFAULT_CONNECTIONS}",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "scripts/in_container/check_environment.sh||scripts/in_container/check_environment.sh": [
          "File: scripts/in_container/check_environment.sh -> scripts/in_container/check_environment.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "110:         echo",
          "111:         echo \"Starting Airflow\"",
          "112:         echo",
          "114:         export AIRFLOW__CORE__LOAD_EXAMPLES=${LOAD_EXAMPLES}",
          "116:         . \"$( dirname \"${BASH_SOURCE[0]}\" )/configure_environment.sh\"",
          "119:         airflow users create -u admin -p admin -f Thor -l Adminstra -r Admin -e admin@email.domain",
          "121:         . \"$( dirname \"${BASH_SOURCE[0]}\" )/run_init_script.sh\"",
          "",
          "[Removed Lines]",
          "113:         export AIRFLOW__DATABASE__LOAD_DEFAULT_CONNECTIONS=${LOAD_DEFAULT_CONNECTIONS}",
          "118:         airflow db migrate",
          "",
          "[Added Lines]",
          "117:         if airflow db migrate",
          "118:         then",
          "119:             if [[ ${LOAD_DEFAULT_CONNECTIONS=} == \"true\" || ${LOAD_DEFAULT_CONNECTIONS=} == \"True\" ]]; then",
          "120:                 echo",
          "121:                 echo \"${COLOR_BLUE}Creating default connections${COLOR_RESET}\"",
          "122:                 echo",
          "123:                 airflow connections create-default-connections",
          "124:             fi",
          "125:         else",
          "126:             # For Airflow versions that do not support db migrate, we should run airflow db init and",
          "127:             # set the removed AIRFLOW__DATABASE__LOAD_DEFAULT_CONNECTIONS",
          "128:             AIRFLOW__DATABASE__LOAD_DEFAULT_CONNECTIONS=${LOAD_DEFAULT_CONNECTIONS} airflow db init",
          "129:         fi",
          "",
          "---------------"
        ],
        "tests/core/test_configuration.py||tests/core/test_configuration.py": [
          "File: tests/core/test_configuration.py -> tests/core/test_configuration.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "157:         # test display_source",
          "158:         cfg_dict = conf.as_dict(display_source=True)",
          "159:         assert cfg_dict[\"core\"][\"load_examples\"][1] == \"airflow.cfg\"",
          "161:         assert cfg_dict[\"testsection\"][\"testkey\"] == (\"testvalue\", \"env var\")",
          "162:         assert cfg_dict[\"core\"][\"fernet_key\"] == (\"< hidden >\", \"env var\")",
          "",
          "[Removed Lines]",
          "160:         assert cfg_dict[\"database\"][\"load_default_connections\"][1] == \"airflow.cfg\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ]
      }
    }
  ]
}