{
  "cve_id": "CVE-2023-35005",
  "cve_desc": "In Apache Airflow, some potentially sensitive values were being shown to the user in certain situations.\n\nThis vulnerability is mitigated by the fact configuration is not shown in the UI by default (only if `[webserver] expose_config` is set to `non-sensitive-only`), and not all uncensored values are actually sentitive.\n\n\nThis issue affects Apache Airflow: from 2.5.0 before 2.6.2. Users are recommended to update to version 2.6.2 or later.\n\n\n",
  "repo": "apache/airflow",
  "patch_hash": "5679a01919ac9d5153e858f8b1390cbc7915f148",
  "patch_info": {
    "commit_hash": "5679a01919ac9d5153e858f8b1390cbc7915f148",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/5679a01919ac9d5153e858f8b1390cbc7915f148",
    "files": [
      "airflow/config_templates/config.yml",
      "airflow/config_templates/default_airflow.cfg",
      "airflow/configuration.py",
      "airflow/www/views.py",
      "tests/core/test_configuration.py",
      "tests/www/views/test_views_configuration.py"
    ],
    "message": "Use single source of truth for sensitive config items (#31820)\n\nPreviously we had them defined both in constant and in config.yml.\n\nNow just config.yml\n\n(cherry picked from commit cab342ee010bfd048006ca458c760b37470b6ea5)",
    "before_after_code_files": [
      "airflow/config_templates/default_airflow.cfg||airflow/config_templates/default_airflow.cfg",
      "airflow/configuration.py||airflow/configuration.py",
      "airflow/www/views.py||airflow/www/views.py",
      "tests/core/test_configuration.py||tests/core/test_configuration.py",
      "tests/www/views/test_views_configuration.py||tests/www/views/test_views_configuration.py"
    ]
  },
  "patch_diff": {
    "airflow/config_templates/default_airflow.cfg||airflow/config_templates/default_airflow.cfg": [
      "File: airflow/config_templates/default_airflow.cfg -> airflow/config_templates/default_airflow.cfg",
      "--- Hunk 1 ---",
      "[Context before]",
      "995: # Example: result_backend = db+postgresql://postgres:airflow@postgres/airflow",
      "996: # result_backend =",
      "998: # Celery Flower is a sweet UI for Celery. Airflow has a shortcut to start",
      "999: # it ``airflow celery flower``. This defines the IP that Celery Flower runs on",
      "1000: flower_host = 0.0.0.0",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "998: # Optional configuration dictionary to pass to the Celery result backend SQLAlchemy engine.",
      "999: # Example: result_backend_sqlalchemy_engine_options = {{\"pool_recycle\": 1800}}",
      "1000: result_backend_sqlalchemy_engine_options =",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "1018: # Import path for celery configuration options",
      "1019: celery_config_options = airflow.config_templates.default_celery.DEFAULT_CELERY_CONFIG",
      "1020: ssl_active = False",
      "1021: ssl_key =",
      "1022: ssl_cert =",
      "1023: ssl_cacert =",
      "1025: # Celery Pool implementation.",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "1026: # Path to the client key.",
      "1029: # Path to the client certificate.",
      "1032: # Path to the CA certificate.",
      "",
      "---------------"
    ],
    "airflow/configuration.py||airflow/configuration.py": [
      "File: airflow/configuration.py -> airflow/configuration.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "37: from contextlib import contextmanager, suppress",
      "38: from json.decoder import JSONDecodeError",
      "39: from re import Pattern",
      "41: from urllib.parse import urlsplit",
      "43: from typing_extensions import overload",
      "",
      "[Removed Lines]",
      "40: from typing import IO, Any, Dict, Iterable, Tuple, Union",
      "",
      "[Added Lines]",
      "40: from typing import IO, Any, Dict, Iterable, Set, Tuple, Union",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "147:         return yaml.safe_load(config_file)",
      "165: class AirflowConfigParser(ConfigParser):",
      "166:     \"\"\"Custom Airflow Configparser supporting defaults and deprecated options.\"\"\"",
      "",
      "[Removed Lines]",
      "150: SENSITIVE_CONFIG_VALUES = {",
      "151:     (\"database\", \"sql_alchemy_conn\"),",
      "152:     (\"core\", \"fernet_key\"),",
      "153:     (\"celery\", \"broker_url\"),",
      "154:     (\"celery\", \"flower_basic_auth\"),",
      "155:     (\"celery\", \"result_backend\"),",
      "156:     (\"atlas\", \"password\"),",
      "157:     (\"smtp\", \"smtp_password\"),",
      "158:     (\"webserver\", \"secret_key\"),",
      "159:     (\"secrets\", \"backend_kwargs\"),",
      "160:     # The following options are deprecated",
      "161:     (\"core\", \"sql_alchemy_conn\"),",
      "162: }",
      "",
      "[Added Lines]",
      "[None]",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "171:     # These configs can also be fetched from Secrets backend",
      "172:     # following the \"{section}__{name}__secret\" pattern",
      "176:     # A mapping of (new section, new option) -> (old section, old option, since_version).",
      "177:     # When reading new option, the old option will be checked to see if it exists. If it does a",
      "",
      "[Removed Lines]",
      "174:     sensitive_config_values: set[tuple[str, str]] = SENSITIVE_CONFIG_VALUES",
      "",
      "[Added Lines]",
      "159:     @cached_property",
      "160:     def sensitive_config_values(self) -> Set[tuple[str, str]]:  # noqa: UP006",
      "161:         default_config = default_config_yaml()",
      "162:         flattened = {",
      "163:             (s, k): item for s, s_c in default_config.items() for k, item in s_c.get(\"options\").items()",
      "164:         }",
      "165:         sensitive = {(section, key) for (section, key), v in flattened.items() if v.get(\"sensitive\") is True}",
      "166:         depr_option = {self.deprecated_options[x][:-1] for x in sensitive if x in self.deprecated_options}",
      "167:         depr_section = {",
      "168:             (self.deprecated_sections[s][0], k) for s, k in sensitive if s in self.deprecated_sections",
      "169:         }",
      "170:         sensitive.update(depr_section, depr_option)",
      "171:         return sensitive",
      "",
      "---------------"
    ],
    "airflow/www/views.py||airflow/www/views.py": [
      "File: airflow/www/views.py -> airflow/www/views.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "3951:         # TODO remove \"if raw\" usage in Airflow 3.0. Configuration can be fetched via the REST API.",
      "3952:         if raw:",
      "3953:             if expose_config == \"non-sensitive-only\":",
      "3956:                 updater = configupdater.ConfigUpdater()",
      "3957:                 updater.read(AIRFLOW_CONFIG)",
      "3959:                     if updater.has_option(sect, key):",
      "3960:                         updater[sect][key].value = \"< hidden >\"",
      "3961:                 config = str(updater)",
      "",
      "[Removed Lines]",
      "3954:                 from airflow.configuration import SENSITIVE_CONFIG_VALUES",
      "3958:                 for sect, key in SENSITIVE_CONFIG_VALUES:",
      "",
      "[Added Lines]",
      "3956:                 for sect, key in conf.sensitive_config_values:",
      "",
      "---------------"
    ],
    "tests/core/test_configuration.py||tests/core/test_configuration.py": [
      "File: tests/core/test_configuration.py -> tests/core/test_configuration.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "36:     AirflowConfigException,",
      "37:     AirflowConfigParser,",
      "38:     conf,",
      "39:     expand_env_var,",
      "40:     get_airflow_config,",
      "41:     get_airflow_home,",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "39:     default_config_yaml,",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "1447:             w = captured.pop()",
      "1448:             assert \"your `conf.get*` call to use the new name\" in str(w.message)",
      "1449:             assert w.category == FutureWarning",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "1453: def test_sensitive_values():",
      "1454:     from airflow.settings import conf",
      "1456:     # this list was hardcoded prior to 2.6.2",
      "1457:     # included here to avoid regression in refactor",
      "1458:     # inclusion of keys ending in \"password\" or \"kwargs\" is automated from 2.6.2",
      "1459:     # items not matching this pattern must be added here manually",
      "1460:     sensitive_values = {",
      "1461:         (\"database\", \"sql_alchemy_conn\"),",
      "1462:         (\"core\", \"fernet_key\"),",
      "1463:         (\"celery\", \"broker_url\"),",
      "1464:         (\"celery\", \"flower_basic_auth\"),",
      "1465:         (\"celery\", \"result_backend\"),",
      "1466:         (\"atlas\", \"password\"),",
      "1467:         (\"smtp\", \"smtp_password\"),",
      "1468:         (\"webserver\", \"secret_key\"),",
      "1469:         (\"secrets\", \"backend_kwargs\"),",
      "1470:         (\"sentry\", \"sentry_dsn\"),",
      "1471:         (\"database\", \"sql_alchemy_engine_args\"),",
      "1472:         (\"core\", \"sql_alchemy_conn\"),",
      "1473:     }",
      "1474:     default_config = default_config_yaml()",
      "1475:     all_keys = {(s, k) for s, v in default_config.items() for k in v.get(\"options\")}",
      "1476:     suspected_sensitive = {(s, k) for (s, k) in all_keys if k.endswith((\"password\", \"kwargs\"))}",
      "1477:     exclude_list = {",
      "1478:         (\"kubernetes_executor\", \"delete_option_kwargs\"),",
      "1479:     }",
      "1480:     suspected_sensitive -= exclude_list",
      "1481:     sensitive_values.update(suspected_sensitive)",
      "1482:     assert sensitive_values == conf.sensitive_config_values",
      "",
      "---------------"
    ],
    "tests/www/views/test_views_configuration.py||tests/www/views/test_views_configuration.py": [
      "File: tests/www/views/test_views_configuration.py -> tests/www/views/test_views_configuration.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "19: import html",
      "22: from tests.test_utils.config import conf_vars",
      "23: from tests.test_utils.www import check_content_in_response, check_content_not_in_response",
      "",
      "[Removed Lines]",
      "21: from airflow.configuration import SENSITIVE_CONFIG_VALUES, conf",
      "",
      "[Added Lines]",
      "21: from airflow.configuration import conf",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "36: @conf_vars({(\"webserver\", \"expose_config\"): \"True\"})",
      "37: def test_user_can_view_configuration(admin_client):",
      "38:     resp = admin_client.get(\"configuration\", follow_redirects=True)",
      "40:         value = conf.get(section, key, fallback=\"\")",
      "41:         if not value:",
      "42:             continue",
      "",
      "[Removed Lines]",
      "39:     for section, key in SENSITIVE_CONFIG_VALUES:",
      "",
      "[Added Lines]",
      "39:     for section, key in conf.sensitive_config_values:",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "46: @conf_vars({(\"webserver\", \"expose_config\"): \"non-sensitive-only\"})",
      "47: def test_configuration_redacted(admin_client):",
      "48:     resp = admin_client.get(\"configuration\", follow_redirects=True)",
      "50:         value = conf.get(section, key, fallback=\"\")",
      "51:         if not value or value == \"airflow\":",
      "52:             continue",
      "",
      "[Removed Lines]",
      "49:     for section, key in SENSITIVE_CONFIG_VALUES:",
      "",
      "[Added Lines]",
      "49:     for section, key in conf.sensitive_config_values:",
      "",
      "---------------",
      "--- Hunk 4 ---",
      "[Context before]",
      "58: @conf_vars({(\"webserver\", \"expose_config\"): \"non-sensitive-only\"})",
      "59: def test_configuration_redacted_in_running_configuration(admin_client):",
      "60:     resp = admin_client.get(\"configuration\", follow_redirects=True)",
      "62:         value = conf.get(section, key, fallback=\"\")",
      "63:         if not value or value == \"airflow\":",
      "64:             continue",
      "",
      "[Removed Lines]",
      "61:     for section, key in SENSITIVE_CONFIG_VALUES:",
      "",
      "[Added Lines]",
      "61:     for section, key in conf.sensitive_config_values:",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "c3e388a8f5ac93756f978ab1bc22f9478450ed07",
      "candidate_info": {
        "commit_hash": "c3e388a8f5ac93756f978ab1bc22f9478450ed07",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/c3e388a8f5ac93756f978ab1bc22f9478450ed07",
        "files": [
          "airflow/utils/file.py",
          "tests/dag_processing/test_processor.py"
        ],
        "message": "Fix error handling when pre-importing modules in DAGs (#31401)\n\n(cherry picked from commit 24a94bbb603c5308a2e8817dc356492287f7b174)",
        "before_after_code_files": [
          "airflow/utils/file.py||airflow/utils/file.py",
          "tests/dag_processing/test_processor.py||tests/dag_processing/test_processor.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/31796"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/utils/file.py||airflow/utils/file.py": [
          "File: airflow/utils/file.py -> airflow/utils/file.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "387:     \"\"\"Find Airflow modules imported in the given file.\"\"\"",
          "388:     try:",
          "389:         parsed = ast.parse(Path(file_path).read_bytes())",
          "391:         return",
          "392:     for m in _find_imported_modules(parsed):",
          "393:         if m.startswith(\"airflow.\"):",
          "",
          "[Removed Lines]",
          "390:     except (OSError, SyntaxError, UnicodeDecodeError):",
          "",
          "[Added Lines]",
          "390:     except Exception:",
          "",
          "---------------"
        ],
        "tests/dag_processing/test_processor.py||tests/dag_processing/test_processor.py": [
          "File: tests/dag_processing/test_processor.py -> tests/dag_processing/test_processor.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "977:         )",
          "978:         processor.start()",
          "981: class TestProcessorAgent:",
          "982:     @pytest.fixture(autouse=True)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "980:     @mock.patch(\"airflow.dag_processing.processor.settings.dispose_orm\", MagicMock)",
          "981:     @mock.patch.object(DagFileProcessorProcess, \"_get_multiprocessing_context\")",
          "982:     def test_nullbyte_exception_handling_when_preimporting_airflow(self, mock_context, tmpdir):",
          "983:         mock_context.return_value.Pipe.return_value = (MagicMock(), MagicMock())",
          "984:         dag_filename = os.path.join(tmpdir, \"test_dag.py\")",
          "985:         with open(dag_filename, \"wb\") as file:",
          "986:             file.write(b\"hello\\x00world\")",
          "988:         processor = DagFileProcessorProcess(",
          "989:             file_path=dag_filename,",
          "990:             pickle_dags=False,",
          "991:             dag_ids=[],",
          "992:             dag_directory=[],",
          "993:             callback_requests=[],",
          "994:         )",
          "995:         processor.start()",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "0b9ca46fdb940d7973d424f7991f2fa821e2419f",
      "candidate_info": {
        "commit_hash": "0b9ca46fdb940d7973d424f7991f2fa821e2419f",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/0b9ca46fdb940d7973d424f7991f2fa821e2419f",
        "files": [
          "airflow/models/taskinstance.py",
          "tests/models/test_dagrun.py",
          "tests/models/test_taskinstance.py"
        ],
        "message": "Fix crash when clearing run with task from normal to mapped (#31352)\n\nCo-authored-by: Jed Cunningham <66968678+jedcunningham@users.noreply.github.com>\nCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>\n(cherry picked from commit f82246abe9491a49701abdb647be001d95db7e9f)",
        "before_after_code_files": [
          "airflow/models/taskinstance.py||airflow/models/taskinstance.py",
          "tests/models/test_dagrun.py||tests/models/test_dagrun.py",
          "tests/models/test_taskinstance.py||tests/models/test_taskinstance.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/31796"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/models/taskinstance.py||airflow/models/taskinstance.py": [
          "File: airflow/models/taskinstance.py -> airflow/models/taskinstance.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "2742:     def clear_db_references(self, session):",
          "2743:         \"\"\"",
          "2746:         :param session: ORM Session",
          "",
          "[Removed Lines]",
          "2744:         Clear DB references to XCom, TaskFail and RenderedTaskInstanceFields.",
          "",
          "[Added Lines]",
          "2744:         Clear db tables that have a reference to this instance.",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "2749:         \"\"\"",
          "2750:         from airflow.models.renderedtifields import RenderedTaskInstanceFields",
          "2753:         for table in tables:",
          "2754:             session.query(table).filter(",
          "2755:                 table.dag_id == self.dag_id,",
          "",
          "[Removed Lines]",
          "2752:         tables = [TaskFail, XCom, RenderedTaskInstanceFields]",
          "",
          "[Added Lines]",
          "2752:         tables = [TaskFail, TaskInstanceNote, TaskReschedule, XCom, RenderedTaskInstanceFields]",
          "",
          "---------------"
        ],
        "tests/models/test_dagrun.py||tests/models/test_dagrun.py": [
          "File: tests/models/test_dagrun.py -> tests/models/test_dagrun.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "40: )",
          "41: from airflow.models.baseoperator import BaseOperator",
          "42: from airflow.models.dagrun import DagRunNote",
          "43: from airflow.models.taskmap import TaskMap",
          "44: from airflow.operators.empty import EmptyOperator",
          "45: from airflow.operators.python import ShortCircuitOperator",
          "46: from airflow.serialization.serialized_objects import SerializedDAG",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "43: from airflow.models.taskinstance import TaskInstanceNote",
          "45: from airflow.models.taskreschedule import TaskReschedule",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "2255: def test_clearing_task_and_moving_from_non_mapped_to_mapped(dag_maker, session):",
          "2256:     \"\"\"",
          "2257:     Test that clearing a task and moving from non-mapped to mapped clears existing",
          "2261:     \"\"\"",
          "2263:     from airflow.models.taskfail import TaskFail",
          "",
          "[Removed Lines]",
          "2258:     references in XCom, TaskFail, and RenderedTaskInstanceFields",
          "2259:     To be able to test this, RenderedTaskInstanceFields was not used in the test",
          "2260:     since it would require that the task is expanded first.",
          "",
          "[Added Lines]",
          "2260:     references in XCom, TaskFail, TaskInstanceNote, TaskReschedule and",
          "2261:     RenderedTaskInstanceFields. To be able to test this, RenderedTaskInstanceFields",
          "2262:     was not used in the test since it would require that the task is expanded first.",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "2273:     dr1: DagRun = dag_maker.create_dagrun(run_type=DagRunType.SCHEDULED)",
          "2274:     ti = dr1.get_task_instances()[0]",
          "2275:     # mimicking a case where task moved from non-mapped to mapped",
          "2276:     # in that case, it would have map_index of -1 even though mapped",
          "2277:     ti.map_index = -1",
          "2278:     session.merge(ti)",
          "2279:     session.flush()",
          "2280:     # Purposely omitted RenderedTaskInstanceFields because the ti need",
          "2281:     # to be expanded but here we are mimicking and made it map_index -1",
          "2282:     session.add(TaskFail(ti))",
          "2283:     XCom.set(key=\"test\", value=\"value\", task_id=ti.task_id, dag_id=dag.dag_id, run_id=ti.run_id)",
          "2284:     session.commit()",
          "2286:         assert session.query(table).count() == 1",
          "2287:     dr1.task_instance_scheduling_decisions(session)",
          "2289:         assert session.query(table).count() == 0",
          "",
          "[Removed Lines]",
          "2285:     for table in [TaskFail, XCom]:",
          "2288:     for table in [TaskFail, XCom]:",
          "",
          "[Added Lines]",
          "2277:     filter_kwargs = dict(dag_id=ti.dag_id, task_id=ti.task_id, run_id=ti.run_id, map_index=ti.map_index)",
          "2278:     ti = session.query(TaskInstance).filter_by(**filter_kwargs).one()",
          "2280:     tr = TaskReschedule(",
          "2281:         task=ti,",
          "2282:         run_id=ti.run_id,",
          "2283:         try_number=ti.try_number,",
          "2284:         start_date=timezone.datetime(2017, 1, 1),",
          "2285:         end_date=timezone.datetime(2017, 1, 2),",
          "2286:         reschedule_date=timezone.datetime(2017, 1, 1),",
          "2287:     )",
          "2292:     ti.note = \"sample note\"",
          "2297:     session.add(tr)",
          "2301:     for table in [TaskFail, TaskInstanceNote, TaskReschedule, XCom]:",
          "2304:     for table in [TaskFail, TaskInstanceNote, TaskReschedule, XCom]:",
          "",
          "---------------"
        ],
        "tests/models/test_taskinstance.py||tests/models/test_taskinstance.py": [
          "File: tests/models/test_taskinstance.py -> tests/models/test_taskinstance.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "2888:     def test_clear_db_references(self, session, create_task_instance):",
          "2889:         tables = [TaskFail, RenderedTaskInstanceFields, XCom]",
          "2890:         ti = create_task_instance()",
          "2891:         session.merge(ti)",
          "2892:         session.commit()",
          "2893:         for table in [TaskFail, RenderedTaskInstanceFields]:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "2891:         ti.note = \"sample note\"",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "2896:         session.commit()",
          "2897:         for table in tables:",
          "2898:             assert session.query(table).count() == 1",
          "2899:         ti.clear_db_references(session)",
          "2900:         for table in tables:",
          "2901:             assert session.query(table).count() == 0",
          "2904: @pytest.mark.parametrize(\"pool_override\", [None, \"test_pool2\"])",
          "2905: def test_refresh_from_task(pool_override):",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "2902:         filter_kwargs = dict(dag_id=ti.dag_id, task_id=ti.task_id, run_id=ti.run_id, map_index=ti.map_index)",
          "2903:         ti_note = session.query(TaskInstanceNote).filter_by(**filter_kwargs).one()",
          "2904:         assert ti_note.content == \"sample note\"",
          "2910:         assert session.query(TaskInstanceNote).filter_by(**filter_kwargs).one_or_none() is None",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "186df9f1bf1b47c5f3fd1f721f630f137862fdff",
      "candidate_info": {
        "commit_hash": "186df9f1bf1b47c5f3fd1f721f630f137862fdff",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/186df9f1bf1b47c5f3fd1f721f630f137862fdff",
        "files": [
          "tests/utils/test_db.py"
        ],
        "message": "Add back accidentally removed `create_session` in utils/db.py (#31705)\n\nThe create_session function was accidentally removed in commit\nhttps://github.com/apache/airflow/commit/522661b6ad4479e3c8243b2d2c8a793d1af82c17\nand it's a breaking change.\n\n(cherry picked from commit 5f3a46427bd8fca6514b0080270f93600b033851)",
        "before_after_code_files": [
          "tests/utils/test_db.py||tests/utils/test_db.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/31796"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "tests/utils/test_db.py||tests/utils/test_db.py": [
          "File: tests/utils/test_db.py -> tests/utils/test_db.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "40:     compare_server_default,",
          "41:     compare_type,",
          "42:     create_default_connections,",
          "43:     downgrade,",
          "44:     resetdb,",
          "45:     upgradedb,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "43:     # The create_session is not used. It is imported here to",
          "44:     # guard against removing it from utils.db accidentally",
          "45:     create_session,  # noqa: F401",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "d26144bb2071b74920273588dc0c1cb118db8ca3",
      "candidate_info": {
        "commit_hash": "d26144bb2071b74920273588dc0c1cb118db8ca3",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/d26144bb2071b74920273588dc0c1cb118db8ca3",
        "files": [
          "airflow/models/taskmixin.py"
        ],
        "message": "Remove dependency already registered for this task warning (#31502)\n\n(cherry picked from commit abcfc6f76def56c56c0bb15423eab00a837c099a)",
        "before_after_code_files": [
          "airflow/models/taskmixin.py||airflow/models/taskmixin.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/31796"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/models/taskmixin.py||airflow/models/taskmixin.py": [
          "File: airflow/models/taskmixin.py -> airflow/models/taskmixin.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "205:             # If this task does not yet have a dag, add it to the same dag as the other task.",
          "206:             self.dag = dag",
          "215:         for task in task_list:",
          "216:             if dag and not task.has_dag():",
          "217:                 # If the other task does not yet have a dag, add it to the same dag as this task and",
          "218:                 dag.add_task(task)",
          "219:             if upstream:",
          "222:                 if edge_modifier:",
          "223:                     edge_modifier.add_edge_info(self.dag, task.node_id, self.node_id)",
          "224:             else:",
          "227:                 if edge_modifier:",
          "228:                     edge_modifier.add_edge_info(self.dag, self.node_id, task.node_id)",
          "",
          "[Removed Lines]",
          "208:         def add_only_new(obj, item_set: set[str], item: str) -> None:",
          "209:             \"\"\"Adds only new items to item set\"\"\"",
          "210:             if item in item_set:",
          "211:                 self.log.warning(\"Dependency %s, %s already registered for DAG: %s\", obj, item, dag.dag_id)",
          "212:             else:",
          "213:                 item_set.add(item)",
          "220:                 add_only_new(task, task.downstream_task_ids, self.node_id)",
          "221:                 add_only_new(self, self.upstream_task_ids, task.node_id)",
          "225:                 add_only_new(self, self.downstream_task_ids, task.node_id)",
          "226:                 add_only_new(task, task.upstream_task_ids, self.node_id)",
          "",
          "[Added Lines]",
          "213:                 task.downstream_task_ids.add(self.node_id)",
          "214:                 self.upstream_task_ids.add(task.node_id)",
          "218:                 self.downstream_task_ids.add(task.node_id)",
          "219:                 task.upstream_task_ids.add(self.node_id)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "b1e5021c9b256975c249b42cfd659b280a31a359",
      "candidate_info": {
        "commit_hash": "b1e5021c9b256975c249b42cfd659b280a31a359",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/b1e5021c9b256975c249b42cfd659b280a31a359",
        "files": [
          "setup.py"
        ],
        "message": "Rephrase comment in setup.py (#31312)\n\nFollowing up on my late comment in PR #31309, this is a try to rephrase\nthe comment in `setup.py` which indicates how to trigger CI build to use\n\"upgrade to newer dependencies\".\n\n(cherry picked from commit 492acc274b278728069da246c88162f938cb0fb0)",
        "before_after_code_files": [
          "setup.py||setup.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/31796"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "setup.py||setup.py": [
          "File: setup.py -> setup.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "16: # specific language governing permissions and limitations",
          "17: # under the License.",
          "18: \"\"\"Setup.py for the Airflow project.\"\"\"",
          "19: from __future__ import annotations",
          "21: import glob",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "19: # To make sure the CI build is using \"upgrade to newer dependencies\", which is useful when you want to check",
          "20: # if the dependencies are still compatible with the latest versions as they seem to break some unrelated",
          "21: # tests in main, you can modify this file. The modification can be simply modifying this particular comment.",
          "22: # e.g. you can modify the following number \"00001\" to something else to trigger it.",
          "",
          "---------------"
        ]
      }
    }
  ]
}