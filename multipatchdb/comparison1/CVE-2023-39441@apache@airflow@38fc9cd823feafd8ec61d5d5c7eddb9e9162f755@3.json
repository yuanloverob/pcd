{
  "cve_id": "CVE-2023-39441",
  "cve_desc": "Apache Airflow SMTP Provider before 1.3.0, Apache Airflow IMAP Provider before 3.3.0, and\u00a0Apache Airflow before 2.7.0 are affected by the\u00a0Validation of OpenSSL Certificate vulnerability.\n\nThe default SSL context with SSL library did not check a server's X.509\u00a0certificate.\u00a0 Instead, the code accepted any certificate, which could\u00a0result in the disclosure of mail server credentials or mail contents\u00a0when the client connects to an attacker in a MITM position.\n\nUsers are strongly advised to upgrade to Apache Airflow version 2.7.0 or newer, Apache Airflow IMAP Provider version 3.3.0 or newer, and Apache Airflow SMTP Provider version 1.3.0 or newer to mitigate the risk associated with this vulnerability",
  "repo": "apache/airflow",
  "patch_hash": "38fc9cd823feafd8ec61d5d5c7eddb9e9162f755",
  "patch_info": {
    "commit_hash": "38fc9cd823feafd8ec61d5d5c7eddb9e9162f755",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/38fc9cd823feafd8ec61d5d5c7eddb9e9162f755",
    "files": [
      "airflow/providers/imap/CHANGELOG.rst",
      "airflow/providers/imap/hooks/imap.py",
      "airflow/providers/imap/provider.yaml",
      "docs/apache-airflow-providers-imap/configurations-ref.rst",
      "docs/apache-airflow-providers-imap/index.rst",
      "docs/apache-airflow/configurations-ref.rst",
      "tests/providers/imap/hooks/test_imap.py"
    ],
    "message": "Allows to choose SSL context for IMAP provider (#33108)\n\n* Allows to choose SSL context for IMAP provider\n\nThis change add two options to choose from when SSL IMAP connection is created:\n\n* default - for balance between compatibility and security\n* none - in case compatibility with existing infrastructure is preferred\n\nThe fallback is:\n\n* The Airflow \"email\", \"ssl_context\"\n* \"default\"\n\nCo-authored-by: Ephraim Anierobi <splendidzigy24@gmail.com>\n(cherry picked from commit 52ca7bfc988f4c9b608f544bc3e9524fd6564639)",
    "before_after_code_files": [
      "airflow/providers/imap/hooks/imap.py||airflow/providers/imap/hooks/imap.py",
      "tests/providers/imap/hooks/test_imap.py||tests/providers/imap/hooks/test_imap.py"
    ]
  },
  "patch_diff": {
    "airflow/providers/imap/hooks/imap.py||airflow/providers/imap/hooks/imap.py": [
      "File: airflow/providers/imap/hooks/imap.py -> airflow/providers/imap/hooks/imap.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "26: import imaplib",
      "27: import os",
      "28: import re",
      "29: from typing import Any, Iterable",
      "31: from airflow.exceptions import AirflowException",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "29: import ssl",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "78:         return self",
      "80:     def _build_client(self, conn: Connection) -> imaplib.IMAP4_SSL | imaplib.IMAP4:",
      "84:         else:",
      "92:         return mail_client",
      "",
      "[Removed Lines]",
      "81:         IMAP: type[imaplib.IMAP4_SSL] | type[imaplib.IMAP4]",
      "82:         if conn.extra_dejson.get(\"use_ssl\", True):",
      "83:             IMAP = imaplib.IMAP4_SSL",
      "85:             IMAP = imaplib.IMAP4",
      "87:         if conn.port:",
      "88:             mail_client = IMAP(conn.host, conn.port)",
      "89:         else:",
      "90:             mail_client = IMAP(conn.host)",
      "",
      "[Added Lines]",
      "82:         mail_client: imaplib.IMAP4_SSL | imaplib.IMAP4",
      "83:         use_ssl = conn.extra_dejson.get(\"use_ssl\", True)",
      "84:         if use_ssl:",
      "85:             from airflow.configuration import conf",
      "87:             ssl_context_string = conf.get(\"imap\", \"SSL_CONTEXT\", fallback=None)",
      "88:             if ssl_context_string is None:",
      "89:                 ssl_context_string = conf.get(\"email\", \"SSL_CONTEXT\", fallback=None)",
      "90:             if ssl_context_string is None:",
      "91:                 ssl_context_string = \"default\"",
      "92:             if ssl_context_string == \"default\":",
      "93:                 ssl_context = ssl.create_default_context()",
      "94:             elif ssl_context_string == \"none\":",
      "95:                 ssl_context = None",
      "96:             else:",
      "97:                 raise RuntimeError(",
      "98:                     f\"The email.ssl_context configuration variable must \"",
      "99:                     f\"be set to 'default' or 'none' and is '{ssl_context_string}'.\"",
      "100:                 )",
      "101:             if conn.port:",
      "102:                 mail_client = imaplib.IMAP4_SSL(conn.host, conn.port, ssl_context=ssl_context)",
      "103:             else:",
      "104:                 mail_client = imaplib.IMAP4_SSL(conn.host, ssl_context=ssl_context)",
      "106:             if conn.port:",
      "107:                 mail_client = imaplib.IMAP4(conn.host, conn.port)",
      "108:             else:",
      "109:                 mail_client = imaplib.IMAP4(conn.host)",
      "",
      "---------------"
    ],
    "tests/providers/imap/hooks/test_imap.py||tests/providers/imap/hooks/test_imap.py": [
      "File: tests/providers/imap/hooks/test_imap.py -> tests/providers/imap/hooks/test_imap.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "27: from airflow.models import Connection",
      "28: from airflow.providers.imap.hooks.imap import ImapHook",
      "29: from airflow.utils import db",
      "31: imaplib_string = \"airflow.providers.imap.hooks.imap.imaplib\"",
      "32: open_string = \"airflow.providers.imap.hooks.imap.open\"",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "30: from tests.test_utils.config import conf_vars",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "85:         )",
      "87:     @patch(imaplib_string)",
      "89:         mock_conn = _create_fake_imap(mock_imaplib)",
      "91:         with ImapHook():",
      "92:             pass",
      "95:         mock_conn.login.assert_called_once_with(\"imap_user\", \"imap_password\")",
      "96:         assert mock_conn.logout.call_count == 1",
      "",
      "[Removed Lines]",
      "88:     def test_connect_and_disconnect(self, mock_imaplib):",
      "94:         mock_imaplib.IMAP4_SSL.assert_called_once_with(\"imap_server_address\", 1993)",
      "",
      "[Added Lines]",
      "89:     @patch(\"ssl.create_default_context\")",
      "90:     def test_connect_and_disconnect(self, create_default_context, mock_imaplib):",
      "96:         assert create_default_context.called",
      "97:         mock_imaplib.IMAP4_SSL.assert_called_once_with(",
      "98:             \"imap_server_address\", 1993, ssl_context=create_default_context.return_value",
      "99:         )",
      "100:         mock_conn.login.assert_called_once_with(\"imap_user\", \"imap_password\")",
      "101:         assert mock_conn.logout.call_count == 1",
      "103:     @patch(imaplib_string)",
      "104:     @patch(\"ssl.create_default_context\")",
      "105:     def test_connect_and_disconnect_imap_ssl_context_none(self, create_default_context, mock_imaplib):",
      "106:         mock_conn = _create_fake_imap(mock_imaplib)",
      "108:         with conf_vars({(\"imap\", \"ssl_context\"): \"none\"}):",
      "109:             with ImapHook():",
      "110:                 pass",
      "112:         assert not create_default_context.called",
      "113:         mock_imaplib.IMAP4_SSL.assert_called_once_with(\"imap_server_address\", 1993, ssl_context=None)",
      "114:         mock_conn.login.assert_called_once_with(\"imap_user\", \"imap_password\")",
      "115:         assert mock_conn.logout.call_count == 1",
      "117:     @patch(imaplib_string)",
      "118:     @patch(\"ssl.create_default_context\")",
      "119:     def test_connect_and_disconnect_imap_ssl_context_default(self, create_default_context, mock_imaplib):",
      "120:         mock_conn = _create_fake_imap(mock_imaplib)",
      "122:         with conf_vars({(\"imap\", \"ssl_context\"): \"default\"}):",
      "123:             with ImapHook():",
      "124:                 pass",
      "126:         assert create_default_context.called",
      "127:         mock_imaplib.IMAP4_SSL.assert_called_once_with(",
      "128:             \"imap_server_address\", 1993, ssl_context=create_default_context.return_value",
      "129:         )",
      "130:         mock_conn.login.assert_called_once_with(\"imap_user\", \"imap_password\")",
      "131:         assert mock_conn.logout.call_count == 1",
      "133:     @patch(imaplib_string)",
      "134:     @patch(\"ssl.create_default_context\")",
      "135:     def test_connect_and_disconnect_email_ssl_context_none(self, create_default_context, mock_imaplib):",
      "136:         mock_conn = _create_fake_imap(mock_imaplib)",
      "138:         with conf_vars({(\"email\", \"ssl_context\"): \"none\"}):",
      "139:             with ImapHook():",
      "140:                 pass",
      "142:         assert not create_default_context.called",
      "143:         mock_imaplib.IMAP4_SSL.assert_called_once_with(\"imap_server_address\", 1993, ssl_context=None)",
      "144:         mock_conn.login.assert_called_once_with(\"imap_user\", \"imap_password\")",
      "145:         assert mock_conn.logout.call_count == 1",
      "147:     @patch(imaplib_string)",
      "148:     @patch(\"ssl.create_default_context\")",
      "149:     def test_connect_and_disconnect_imap_ssl_context_override(self, create_default_context, mock_imaplib):",
      "150:         mock_conn = _create_fake_imap(mock_imaplib)",
      "152:         with conf_vars({(\"email\", \"ssl_context\"): \"none\", (\"imap\", \"ssl_context\"): \"default\"}):",
      "153:             with ImapHook():",
      "154:                 pass",
      "156:         assert create_default_context.called",
      "157:         mock_imaplib.IMAP4_SSL.assert_called_once_with(",
      "158:             \"imap_server_address\", 1993, ssl_context=create_default_context.return_value",
      "159:         )",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "855d4684e9a966801da921d7a145a52a2a645014",
      "candidate_info": {
        "commit_hash": "855d4684e9a966801da921d7a145a52a2a645014",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/855d4684e9a966801da921d7a145a52a2a645014",
        "files": [
          "tests/jobs/test_backfill_job.py",
          "tests/providers/openlineage/extractors/test_bash_extractor.py",
          "tests/providers/openlineage/extractors/test_python_extractor.py",
          "tests/system/providers/apache/kafka/example_dag_hello_kafka.py",
          "tests/system/providers/ftp/example_ftp.py",
          "tests/system/providers/google/cloud/compute/example_compute.py",
          "tests/system/providers/google/cloud/compute/example_compute_igm.py",
          "tests/system/providers/google/cloud/compute/example_compute_ssh.py",
          "tests/system/providers/google/cloud/dataproc/example_dataproc_spark_deferrable.py",
          "tests/ti_deps/deps/test_runnable_exec_date_dep.py"
        ],
        "message": "Clean `schedule_interval` usages from example dags (#33131)\n\n* Clean `schedule_interval` usages from example dags\n\n* Fix Pytest parameter name to match test argument\n\n---------\n\nCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>\n(cherry picked from commit cdea9f176ca8f9882bd41c9c198b03421dbfc298)",
        "before_after_code_files": [
          "tests/jobs/test_backfill_job.py||tests/jobs/test_backfill_job.py",
          "tests/providers/openlineage/extractors/test_bash_extractor.py||tests/providers/openlineage/extractors/test_bash_extractor.py",
          "tests/providers/openlineage/extractors/test_python_extractor.py||tests/providers/openlineage/extractors/test_python_extractor.py",
          "tests/system/providers/apache/kafka/example_dag_hello_kafka.py||tests/system/providers/apache/kafka/example_dag_hello_kafka.py",
          "tests/system/providers/ftp/example_ftp.py||tests/system/providers/ftp/example_ftp.py",
          "tests/system/providers/google/cloud/compute/example_compute.py||tests/system/providers/google/cloud/compute/example_compute.py",
          "tests/system/providers/google/cloud/compute/example_compute_igm.py||tests/system/providers/google/cloud/compute/example_compute_igm.py",
          "tests/system/providers/google/cloud/compute/example_compute_ssh.py||tests/system/providers/google/cloud/compute/example_compute_ssh.py",
          "tests/system/providers/google/cloud/dataproc/example_dataproc_spark_deferrable.py||tests/system/providers/google/cloud/dataproc/example_dataproc_spark_deferrable.py",
          "tests/ti_deps/deps/test_runnable_exec_date_dep.py||tests/ti_deps/deps/test_runnable_exec_date_dep.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/33247"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "tests/jobs/test_backfill_job.py||tests/jobs/test_backfill_job.py": [
          "File: tests/jobs/test_backfill_job.py -> tests/jobs/test_backfill_job.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "2054:     def test_backfill_disable_retry(self, dag_maker, disable_retry, try_number, exception):",
          "2055:         with dag_maker(",
          "2056:             dag_id=\"test_disable_retry\",",
          "2058:             default_args={",
          "2059:                 \"retries\": 2,",
          "2060:                 \"retry_delay\": datetime.timedelta(seconds=3),",
          "",
          "[Removed Lines]",
          "2057:             schedule_interval=\"@daily\",",
          "",
          "[Added Lines]",
          "2057:             schedule=\"@daily\",",
          "",
          "---------------"
        ],
        "tests/providers/openlineage/extractors/test_bash_extractor.py||tests/providers/openlineage/extractors/test_bash_extractor.py": [
          "File: tests/providers/openlineage/extractors/test_bash_extractor.py -> tests/providers/openlineage/extractors/test_bash_extractor.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "33: with DAG(",
          "34:     dag_id=\"test_dummy_dag\",",
          "35:     description=\"Test dummy DAG\",",
          "37:     start_date=datetime(2020, 1, 8),",
          "38:     catchup=False,",
          "39:     max_active_runs=1,",
          "",
          "[Removed Lines]",
          "36:     schedule_interval=\"*/2 * * * *\",",
          "",
          "[Added Lines]",
          "36:     schedule=\"*/2 * * * *\",",
          "",
          "---------------"
        ],
        "tests/providers/openlineage/extractors/test_python_extractor.py||tests/providers/openlineage/extractors/test_python_extractor.py": [
          "File: tests/providers/openlineage/extractors/test_python_extractor.py -> tests/providers/openlineage/extractors/test_python_extractor.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "35: dag = DAG(",
          "36:     dag_id=\"test_dummy_dag\",",
          "37:     description=\"Test dummy DAG\",",
          "39:     start_date=datetime(2020, 1, 8),",
          "40:     catchup=False,",
          "41:     max_active_runs=1,",
          "",
          "[Removed Lines]",
          "38:     schedule_interval=\"*/2 * * * *\",",
          "",
          "[Added Lines]",
          "38:     schedule=\"*/2 * * * *\",",
          "",
          "---------------"
        ],
        "tests/system/providers/apache/kafka/example_dag_hello_kafka.py||tests/system/providers/apache/kafka/example_dag_hello_kafka.py": [
          "File: tests/system/providers/apache/kafka/example_dag_hello_kafka.py -> tests/system/providers/apache/kafka/example_dag_hello_kafka.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "151:     \"kafka-example\",",
          "152:     default_args=default_args,",
          "153:     description=\"Examples of Kafka Operators\",",
          "155:     start_date=datetime(2021, 1, 1),",
          "156:     catchup=False,",
          "157:     tags=[\"example\"],",
          "",
          "[Removed Lines]",
          "154:     schedule_interval=timedelta(days=1),",
          "",
          "[Added Lines]",
          "154:     schedule=timedelta(days=1),",
          "",
          "---------------"
        ],
        "tests/system/providers/ftp/example_ftp.py||tests/system/providers/ftp/example_ftp.py": [
          "File: tests/system/providers/ftp/example_ftp.py -> tests/system/providers/ftp/example_ftp.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "35: with DAG(",
          "36:     DAG_ID,",
          "38:     start_date=datetime(2021, 1, 1),",
          "39:     catchup=False,",
          "40:     tags=[\"example\", \"Ftp\", \"FtpFileTransmit\", \"Ftps\", \"FtpsFileTransmit\"],",
          "",
          "[Removed Lines]",
          "37:     schedule_interval=\"@once\",",
          "",
          "[Added Lines]",
          "37:     schedule=\"@once\",",
          "",
          "---------------"
        ],
        "tests/system/providers/google/cloud/compute/example_compute.py||tests/system/providers/google/cloud/compute/example_compute.py": [
          "File: tests/system/providers/google/cloud/compute/example_compute.py -> tests/system/providers/google/cloud/compute/example_compute.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "101: with models.DAG(",
          "102:     DAG_ID,",
          "104:     start_date=datetime(2021, 1, 1),",
          "105:     catchup=False,",
          "106:     tags=[\"example\"],",
          "",
          "[Removed Lines]",
          "103:     schedule_interval=\"@once\",",
          "",
          "[Added Lines]",
          "103:     schedule=\"@once\",",
          "",
          "---------------"
        ],
        "tests/system/providers/google/cloud/compute/example_compute_igm.py||tests/system/providers/google/cloud/compute/example_compute_igm.py": [
          "File: tests/system/providers/google/cloud/compute/example_compute_igm.py -> tests/system/providers/google/cloud/compute/example_compute_igm.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "111: with models.DAG(",
          "112:     DAG_ID,",
          "114:     start_date=datetime(2021, 1, 1),",
          "115:     catchup=False,",
          "116:     tags=[\"example\"],",
          "",
          "[Removed Lines]",
          "113:     schedule_interval=\"@once\",",
          "",
          "[Added Lines]",
          "113:     schedule=\"@once\",",
          "",
          "---------------"
        ],
        "tests/system/providers/google/cloud/compute/example_compute_ssh.py||tests/system/providers/google/cloud/compute/example_compute_ssh.py": [
          "File: tests/system/providers/google/cloud/compute/example_compute_ssh.py -> tests/system/providers/google/cloud/compute/example_compute_ssh.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "72: with models.DAG(",
          "73:     DAG_ID,",
          "75:     start_date=datetime(2021, 1, 1),",
          "76:     catchup=False,",
          "77:     tags=[\"example\", \"compute-ssh\"],",
          "",
          "[Removed Lines]",
          "74:     schedule_interval=\"@once\",",
          "",
          "[Added Lines]",
          "74:     schedule=\"@once\",",
          "",
          "---------------"
        ],
        "tests/system/providers/google/cloud/dataproc/example_dataproc_spark_deferrable.py||tests/system/providers/google/cloud/dataproc/example_dataproc_spark_deferrable.py": [
          "File: tests/system/providers/google/cloud/dataproc/example_dataproc_spark_deferrable.py -> tests/system/providers/google/cloud/dataproc/example_dataproc_spark_deferrable.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "69: with models.DAG(",
          "70:     DAG_ID,",
          "72:     start_date=datetime(2021, 1, 1),",
          "73:     catchup=False,",
          "74:     tags=[\"example\", \"dataproc\", \"spark\", \"deferrable\"],",
          "",
          "[Removed Lines]",
          "71:     schedule_interval=\"@once\",",
          "",
          "[Added Lines]",
          "71:     schedule=\"@once\",",
          "",
          "---------------"
        ],
        "tests/ti_deps/deps/test_runnable_exec_date_dep.py||tests/ti_deps/deps/test_runnable_exec_date_dep.py": [
          "File: tests/ti_deps/deps/test_runnable_exec_date_dep.py -> tests/ti_deps/deps/test_runnable_exec_date_dep.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "39: @time_machine.travel(\"2016-11-01\")",
          "40: @pytest.mark.parametrize(",
          "42:     [",
          "43:         (True, None, datetime(2016, 11, 3), True),",
          "44:         (True, \"@daily\", datetime(2016, 11, 3), False),",
          "",
          "[Removed Lines]",
          "41:     \"allow_trigger_in_future,schedule_interval,execution_date,is_met\",",
          "",
          "[Added Lines]",
          "41:     \"allow_trigger_in_future,schedule,execution_date,is_met\",",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "53:     session,",
          "54:     create_dummy_dag,",
          "55:     allow_trigger_in_future,",
          "57:     execution_date,",
          "58:     is_met,",
          "59: ):",
          "60:     \"\"\"",
          "62:     this dep should fail",
          "63:     \"\"\"",
          "64:     with patch.object(settings, \"ALLOW_FUTURE_EXEC_DATES\", allow_trigger_in_future):",
          "",
          "[Removed Lines]",
          "56:     schedule_interval,",
          "61:     If the dag's execution date is in the future but (allow_trigger_in_future=False or not schedule_interval)",
          "",
          "[Added Lines]",
          "56:     schedule,",
          "61:     If the dag's execution date is in the future but (allow_trigger_in_future=False or not schedule)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "66:             \"test_localtaskjob_heartbeat\",",
          "67:             start_date=datetime(2015, 1, 1),",
          "68:             end_date=datetime(2016, 11, 5),",
          "70:             with_dagrun_type=DagRunType.MANUAL,",
          "71:             session=session,",
          "72:         )",
          "",
          "[Removed Lines]",
          "69:             schedule=schedule_interval,",
          "",
          "[Added Lines]",
          "69:             schedule=schedule,",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "c80d91be62b643fe21efca8fdc5a94d724cf3f58",
      "candidate_info": {
        "commit_hash": "c80d91be62b643fe21efca8fdc5a94d724cf3f58",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/c80d91be62b643fe21efca8fdc5a94d724cf3f58",
        "files": [
          "tests/models/test_xcom_arg_map.py"
        ],
        "message": "Attempt to stabilise tests for xcom_arg_map (#33150)\n\nSimilarly to #33145 - this is an attempt to stabilise flaky tests\nfor the test_xcom_arg_map.\n\nEven if the mechanism is not entirely clear (provide_session should\nalso close the connection) seems like using pytest-fixture provided\nsession works better than relying on a new session created in run()\nmethods.\n\n(cherry picked from commit 3dd0c999f1159a2fefbf32d9f10208a274a79a62)",
        "before_after_code_files": [
          "tests/models/test_xcom_arg_map.py||tests/models/test_xcom_arg_map.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/33247"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "tests/models/test_xcom_arg_map.py||tests/models/test_xcom_arg_map.py": [
          "File: tests/models/test_xcom_arg_map.py -> tests/models/test_xcom_arg_map.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "84:     # Run \"push\".",
          "85:     decision = dr.task_instance_scheduling_decisions(session=session)",
          "86:     for ti in decision.schedulable_tis:",
          "89:     # Run \"pull\". This should automatically convert \"c\" to None.",
          "90:     decision = dr.task_instance_scheduling_decisions(session=session)",
          "91:     for ti in decision.schedulable_tis:",
          "93:     assert results == {\"a\", \"b\", None}",
          "",
          "[Removed Lines]",
          "87:         ti.run()",
          "92:         ti.run()",
          "",
          "[Added Lines]",
          "87:         ti.run(session=session)",
          "92:         ti.run(session=session)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "118:     # Run \"push\".",
          "119:     decision = dr.task_instance_scheduling_decisions(session=session)",
          "120:     for ti in decision.schedulable_tis:",
          "123:     # Prepare to run \"pull\"...",
          "124:     decision = dr.task_instance_scheduling_decisions(session=session)",
          "125:     tis = {(ti.task_id, ti.map_index): ti for ti in decision.schedulable_tis}",
          "127:     # The first two \"pull\" tis should also succeed.",
          "131:     # But the third one fails because the map() result cannot be used as kwargs.",
          "132:     with pytest.raises(ValueError) as ctx:",
          "134:     assert str(ctx.value) == \"expand_kwargs() expects a list[dict], not list[None]\"",
          "136:     assert [tis[(\"pull\", i)].state for i in range(3)] == [",
          "",
          "[Removed Lines]",
          "121:         ti.run()",
          "128:     tis[(\"pull\", 0)].run()",
          "129:     tis[(\"pull\", 1)].run()",
          "133:         tis[(\"pull\", 2)].run()",
          "",
          "[Added Lines]",
          "121:         ti.run(session=session)",
          "128:     tis[(\"pull\", 0)].run(session=session)",
          "129:     tis[(\"pull\", 1)].run(session=session)",
          "133:         tis[(\"pull\", 2)].run(session=session)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "163:     # The \"push\" task should not fail.",
          "164:     decision = dr.task_instance_scheduling_decisions(session=session)",
          "165:     for ti in decision.schedulable_tis:",
          "167:     assert [ti.state for ti in decision.schedulable_tis] == [TaskInstanceState.SUCCESS]",
          "169:     # Prepare to run \"pull\"...",
          "",
          "[Removed Lines]",
          "166:         ti.run()",
          "",
          "[Added Lines]",
          "166:         ti.run(session=session)",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "171:     tis = {(ti.task_id, ti.map_index): ti for ti in decision.schedulable_tis}",
          "173:     # The first two \"pull\" tis should also succeed.",
          "177:     # But the third one (for \"c\") will fail.",
          "178:     with pytest.raises(ValueError) as ctx:",
          "180:     assert str(ctx.value) == \"nope\"",
          "182:     assert [tis[(\"pull\", i)].state for i in range(3)] == [",
          "",
          "[Removed Lines]",
          "174:     tis[(\"pull\", 0)].run()",
          "175:     tis[(\"pull\", 1)].run()",
          "179:         tis[(\"pull\", 2)].run()",
          "",
          "[Added Lines]",
          "174:     tis[(\"pull\", 0)].run(session=session)",
          "175:     tis[(\"pull\", 1)].run(session=session)",
          "179:         tis[(\"pull\", 2)].run(session=session)",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "216:     # Run \"push\".",
          "217:     decision = dr.task_instance_scheduling_decisions(session=session)",
          "218:     for ti in decision.schedulable_tis:",
          "221:     # Run \"forward\". This should automatically skip \"c\".",
          "222:     decision = dr.task_instance_scheduling_decisions(session=session)",
          "223:     for ti in decision.schedulable_tis:",
          "226:     # Now \"collect\" should only get \"a\" and \"b\".",
          "227:     decision = dr.task_instance_scheduling_decisions(session=session)",
          "228:     for ti in decision.schedulable_tis:",
          "230:     assert result == [\"a\", \"b\"]",
          "",
          "[Removed Lines]",
          "219:         ti.run()",
          "224:         ti.run()",
          "229:         ti.run()",
          "",
          "[Added Lines]",
          "219:         ti.run(session=session)",
          "224:         ti.run(session=session)",
          "229:         ti.run(session=session)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "7463380047347e8fe98202d3b272a1b60e8db5b9",
      "candidate_info": {
        "commit_hash": "7463380047347e8fe98202d3b272a1b60e8db5b9",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/7463380047347e8fe98202d3b272a1b60e8db5b9",
        "files": [
          "airflow/www/views.py",
          "tests/www/views/test_views_tasks.py"
        ],
        "message": "Fix xcom view returning bytes as xcom value (#33202)\n\n(cherry picked from commit 36c2735ca48d4c2e2d239c210d3732fd8918fed2)",
        "before_after_code_files": [
          "airflow/www/views.py||airflow/www/views.py",
          "tests/www/views/test_views_tasks.py||tests/www/views/test_views_tasks.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/33247"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/www/views.py||airflow/www/views.py": [
          "File: airflow/www/views.py -> airflow/www/views.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1926:             flash(f\"Task [{dag_id}.{task_id}] doesn't seem to exist at the moment\", \"error\")",
          "1927:             return redirect(url_for(\"Airflow.index\"))",
          "1931:                 XCom.dag_id == dag_id,",
          "1932:                 XCom.task_id == task_id,",
          "1933:                 XCom.execution_date == dttm,",
          "1934:                 XCom.map_index == map_index,",
          "1935:             )",
          "1936:         )",
          "1939:         title = \"XCom\"",
          "1940:         return self.render_template(",
          "",
          "[Removed Lines]",
          "1929:         xcom_query = session.execute(",
          "1930:             select(XCom.key, XCom.value).where(",
          "1937:         attributes = [(k, v) for k, v in xcom_query if not k.startswith(\"_\")]",
          "",
          "[Added Lines]",
          "1929:         xcom_query = session.scalars(",
          "1930:             select(XCom).where(",
          "1937:         attributes = [(xcom.key, xcom.value) for xcom in xcom_query if not xcom.key.startswith(\"_\")]",
          "",
          "---------------"
        ],
        "tests/www/views/test_views_tasks.py||tests/www/views/test_views_tasks.py": [
          "File: tests/www/views/test_views_tasks.py -> tests/www/views/test_views_tasks.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "30: from airflow import settings",
          "31: from airflow.exceptions import AirflowException",
          "33: from airflow.models.dagcode import DagCode",
          "34: from airflow.operators.bash import BashOperator",
          "35: from airflow.providers.celery.executors.celery_executor import CeleryExecutor",
          "",
          "[Removed Lines]",
          "32: from airflow.models import DAG, DagBag, DagModel, TaskFail, TaskInstance, TaskReschedule",
          "",
          "[Added Lines]",
          "32: from airflow.models import DAG, DagBag, DagModel, TaskFail, TaskInstance, TaskReschedule, XCom",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "42: from airflow.www.views import TaskInstanceModelView",
          "43: from tests.test_utils.api_connexion_utils import create_user, delete_roles, delete_user",
          "44: from tests.test_utils.config import conf_vars",
          "46: from tests.test_utils.www import check_content_in_response, check_content_not_in_response, client_with_login",
          "48: DEFAULT_DATE = timezone.utcnow().replace(hour=0, minute=0, second=0, microsecond=0)",
          "",
          "[Removed Lines]",
          "45: from tests.test_utils.db import clear_db_runs",
          "",
          "[Added Lines]",
          "45: from tests.test_utils.db import clear_db_runs, clear_db_xcom",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "69:             start_date=timezone.utcnow(),",
          "70:             state=State.RUNNING,",
          "71:         )",
          "72:         app.dag_bag.get_dag(\"example_subdag_operator\").create_dagrun(",
          "73:             run_id=DEFAULT_DAGRUN,",
          "74:             run_type=DagRunType.SCHEDULED,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "72:         XCom.set(",
          "73:             key=\"return_value\",",
          "74:             value=\"{'x':1}\",",
          "75:             task_id=\"runme_0\",",
          "76:             dag_id=\"example_bash_operator\",",
          "77:             execution_date=DEFAULT_DATE,",
          "78:         )",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "103:         )",
          "104:     yield",
          "105:     clear_db_runs()",
          "108: @pytest.fixture(scope=\"module\")",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "113:     clear_db_xcom()",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "331:         check_content_in_response(content, resp)",
          "334: def test_rendered_task_view(admin_client):",
          "335:     url = f\"task?task_id=runme_0&dag_id=example_bash_operator&execution_date={DEFAULT_VAL}\"",
          "336:     resp = admin_client.get(url, follow_redirects=True)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "342: def test_xcom_return_value_is_not_bytes(admin_client):",
          "343:     url = f\"xcom?dag_id=example_bash_operator&task_id=runme_0&execution_date={DEFAULT_VAL}&map_index=-1\"",
          "344:     resp = admin_client.get(url, follow_redirects=True)",
          "345:     # check that {\"x\":1} is in the response",
          "346:     content = \"{&#39;x&#39;:1}\"",
          "347:     check_content_in_response(content, resp)",
          "348:     # check that b'{\"x\":1}' is not in the response",
          "349:     content = \"b&#39;&#34;{\\\\&#39;x\\\\&#39;:1}&#34;&#39;\"",
          "350:     check_content_not_in_response(content, resp)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "127445f577e34b7b0506a39c898bc335a32cd53f",
      "candidate_info": {
        "commit_hash": "127445f577e34b7b0506a39c898bc335a32cd53f",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/127445f577e34b7b0506a39c898bc335a32cd53f",
        "files": [
          ".pre-commit-config.yaml",
          "airflow/models/dag.py",
          "airflow/providers/qubole/hooks/qubole.py",
          "dev/breeze/src/airflow_breeze/commands/release_candidate_command.py",
          "scripts/ci/pre_commit/pre_commit_insert_extras.py",
          "scripts/ci/pre_commit/pre_commit_local_yml_mounts.py"
        ],
        "message": "Upgrade ruff to latest 0.0.282 version in pre-commits (#33152)\n\n(cherry picked from commit 6b892bf21f69a41caf670e2c498aea1c83086848)",
        "before_after_code_files": [
          "airflow/models/dag.py||airflow/models/dag.py",
          "airflow/providers/qubole/hooks/qubole.py||airflow/providers/qubole/hooks/qubole.py",
          "dev/breeze/src/airflow_breeze/commands/release_candidate_command.py||dev/breeze/src/airflow_breeze/commands/release_candidate_command.py",
          "scripts/ci/pre_commit/pre_commit_insert_extras.py||scripts/ci/pre_commit/pre_commit_insert_extras.py",
          "scripts/ci/pre_commit/pre_commit_local_yml_mounts.py||scripts/ci/pre_commit/pre_commit_local_yml_mounts.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/33247"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/models/dag.py||airflow/models/dag.py": [
          "File: airflow/models/dag.py -> airflow/models/dag.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "2219:             return 0",
          "2220:         if confirm_prompt:",
          "2221:             ti_list = \"\\n\".join(str(t) for t in tis)",
          "2225:             do_it = utils.helpers.ask_yesno(question)",
          "2227:         if do_it:",
          "",
          "[Removed Lines]",
          "2222:             question = (",
          "2223:                 \"You are about to delete these {count} tasks:\\n{ti_list}\\n\\nAre you sure? [y/n]\"",
          "2224:             ).format(count=count, ti_list=ti_list)",
          "",
          "[Added Lines]",
          "2222:             question = f\"You are about to delete these {count} tasks:\\n{ti_list}\\n\\nAre you sure? [y/n]\"",
          "",
          "---------------"
        ],
        "airflow/providers/qubole/hooks/qubole.py||airflow/providers/qubole/hooks/qubole.py": [
          "File: airflow/providers/qubole/hooks/qubole.py -> airflow/providers/qubole/hooks/qubole.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "185:             )",
          "187:         if self.cmd.status != \"done\":  # type: ignore[attr-defined]",
          "194:     def kill(self, ti):",
          "195:         \"\"\"",
          "",
          "[Removed Lines]",
          "188:             raise AirflowException(",
          "189:                 \"Command Id: {} failed with Status: {}\".format(",
          "190:                     self.cmd.id, self.cmd.status  # type: ignore[attr-defined]",
          "191:                 )",
          "192:             )",
          "",
          "[Added Lines]",
          "188:             raise AirflowException(f\"Command Id: {self.cmd.id} failed with Status: {self.cmd.status}\")",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/commands/release_candidate_command.py||dev/breeze/src/airflow_breeze/commands/release_candidate_command.py": [
          "File: dev/breeze/src/airflow_breeze/commands/release_candidate_command.py -> dev/breeze/src/airflow_breeze/commands/release_candidate_command.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "203:             \"environment. The package download link is available at: \"",
          "204:             \"https://test.pypi.org/project/apache-airflow/#files \"",
          "205:             \"Install it with the appropriate constraint file, adapt python version: \"",
          "207:         )",
          "",
          "[Removed Lines]",
          "206:             f\"pip install -i https://test.pypi.org/simple/ --extra-index-url https://pypi.org/simple/apache-airflow=={version} --constraint https://raw.githubusercontent.com/apache/airflow/constraints-{version}/constraints-3.8.txt\"  # noqa: 501",
          "",
          "[Added Lines]",
          "206:             f\"pip install -i https://test.pypi.org/simple/ --extra-index-url \"",
          "207:             f\"https://pypi.org/simple/apache-airflow=={version} --constraint \"",
          "208:             f\"https://raw.githubusercontent.com/apache/airflow/\"",
          "209:             f\"constraints-{version}/constraints-3.8.txt\"",
          "",
          "---------------"
        ],
        "scripts/ci/pre_commit/pre_commit_insert_extras.py||scripts/ci/pre_commit/pre_commit_insert_extras.py": [
          "File: scripts/ci/pre_commit/pre_commit_insert_extras.py -> scripts/ci/pre_commit/pre_commit_insert_extras.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "31: os.environ[\"_SKIP_PYTHON_VERSION_CHECK\"] = \"true\"",
          "36: sys.path.append(str(AIRFLOW_SOURCES_DIR))",
          "",
          "[Removed Lines]",
          "33: from common_precommit_utils import insert_documentation  # isort: skip",
          "34: from setup import EXTRAS_DEPENDENCIES  # isort:skip",
          "",
          "[Added Lines]",
          "33: from common_precommit_utils import insert_documentation  # isort: skip  # noqa: E402",
          "34: from setup import EXTRAS_DEPENDENCIES  # isort:skip  # noqa: E402",
          "",
          "---------------"
        ],
        "scripts/ci/pre_commit/pre_commit_local_yml_mounts.py||scripts/ci/pre_commit/pre_commit_local_yml_mounts.py": [
          "File: scripts/ci/pre_commit/pre_commit_local_yml_mounts.py -> scripts/ci/pre_commit/pre_commit_local_yml_mounts.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "23: sys.path.insert(0, str(Path(__file__).parent.resolve()))  # make sure common_precommit_utils is imported",
          "27: sys.path.insert(0, str(AIRFLOW_SOURCES_ROOT_PATH))  # make sure setup is imported from Airflow",
          "28: sys.path.insert(",
          "29:     0, str(AIRFLOW_SOURCES_ROOT_PATH / \"dev\" / \"breeze\" / \"src\")",
          "30: )  # make sure setup is imported from Airflow",
          "31: # flake8: noqa: F401",
          "36: sys.path.append(str(AIRFLOW_SOURCES_ROOT_PATH))",
          "",
          "[Removed Lines]",
          "25: from common_precommit_utils import AIRFLOW_SOURCES_ROOT_PATH  # isort: skip",
          "32: from airflow_breeze.utils.docker_command_utils import VOLUMES_FOR_SELECTED_MOUNTS  # isort: skip",
          "34: from common_precommit_utils import insert_documentation  # isort: skip",
          "",
          "[Added Lines]",
          "25: from common_precommit_utils import AIRFLOW_SOURCES_ROOT_PATH  # isort: skip  # noqa: E402",
          "32: from airflow_breeze.utils.docker_command_utils import VOLUMES_FOR_SELECTED_MOUNTS  # isort: skip  # noqa: E402",
          "34: from common_precommit_utils import insert_documentation  # isort: skip # noqa: E402",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "29dbd3265748feb49d94c12410392999618ab3f9",
      "candidate_info": {
        "commit_hash": "29dbd3265748feb49d94c12410392999618ab3f9",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/29dbd3265748feb49d94c12410392999618ab3f9",
        "files": [
          "airflow/utils/task_group.py",
          "tests/utils/test_task_group.py"
        ],
        "message": "Don't ignore setups when arrowing from group (#33097)\n\nThis enables us to have a group with just setups in it.\n\n(cherry picked from commit cd7e7bcb2310dea19f7ee946716a7c91ed610c68)",
        "before_after_code_files": [
          "airflow/utils/task_group.py||airflow/utils/task_group.py",
          "tests/utils/test_task_group.py||tests/utils/test_task_group.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/33247"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/utils/task_group.py||airflow/utils/task_group.py": [
          "File: airflow/utils/task_group.py -> airflow/utils/task_group.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "370:         tasks = list(self)",
          "371:         ids = {x.task_id for x in tasks}",
          "374:             for upstream_task in task.upstream_list:",
          "375:                 if upstream_task.task_id not in ids:",
          "376:                     continue",
          "379:                 else:",
          "380:                     yield upstream_task",
          "382:         for task in tasks:",
          "383:             if task.downstream_task_ids.isdisjoint(ids):",
          "385:                     yield task",
          "386:                 else:",
          "389:     def child_id(self, label):",
          "390:         \"\"\"Prefix label with group_id if prefix_group_id is True. Otherwise return the label as-is.\"\"\"",
          "",
          "[Removed Lines]",
          "373:         def recurse_for_first_non_setup_teardown(task):",
          "377:                 if upstream_task.is_setup or upstream_task.is_teardown:",
          "378:                     yield from recurse_for_first_non_setup_teardown(upstream_task)",
          "384:                 if not (task.is_teardown or task.is_setup):",
          "387:                     yield from recurse_for_first_non_setup_teardown(task)",
          "",
          "[Added Lines]",
          "373:         def recurse_for_first_non_teardown(task):",
          "376:                     # upstream task is not in task group",
          "377:                     continue",
          "378:                 elif upstream_task.is_teardown:",
          "379:                     yield from recurse_for_first_non_teardown(upstream_task)",
          "380:                 elif task.is_teardown and upstream_task.is_setup:",
          "381:                     # don't go through the teardown-to-setup path",
          "388:                 if not task.is_teardown:",
          "391:                     yield from recurse_for_first_non_teardown(task)",
          "",
          "---------------"
        ],
        "tests/utils/test_task_group.py||tests/utils/test_task_group.py": [
          "File: tests/utils/test_task_group.py -> tests/utils/test_task_group.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "22: import pendulum",
          "23: import pytest",
          "26: from airflow.exceptions import TaskAlreadyInTaskGroup",
          "27: from airflow.models.baseoperator import BaseOperator",
          "28: from airflow.models.dag import DAG",
          "",
          "[Removed Lines]",
          "25: from airflow.decorators import dag, task as task_decorator, task_group as task_group_decorator",
          "",
          "[Added Lines]",
          "25: from airflow.decorators import (",
          "26:     dag,",
          "27:     setup,",
          "28:     task as task_decorator,",
          "29:     task_group as task_group_decorator,",
          "30:     teardown,",
          "31: )",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1479:         tg1 >> w2",
          "1480:     assert t1.downstream_task_ids == set()",
          "1481:     assert w1.downstream_task_ids == {\"tg1.t1\", \"w2\"}",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1490: def test_task_group_arrow_with_setup_group():",
          "1491:     with DAG(dag_id=\"setup_group_teardown_group\", start_date=pendulum.now()):",
          "1492:         with TaskGroup(\"group_1\") as g1:",
          "1494:             @setup",
          "1495:             def setup_1():",
          "1496:                 ...",
          "1498:             @setup",
          "1499:             def setup_2():",
          "1500:                 ...",
          "1502:             s1 = setup_1()",
          "1503:             s2 = setup_2()",
          "1505:         with TaskGroup(\"group_2\") as g2:",
          "1507:             @teardown",
          "1508:             def teardown_1():",
          "1509:                 ...",
          "1511:             @teardown",
          "1512:             def teardown_2():",
          "1513:                 ...",
          "1515:             t1 = teardown_1()",
          "1516:             t2 = teardown_2()",
          "1518:         @task_decorator",
          "1519:         def work():",
          "1520:             ...",
          "1522:         w1 = work()",
          "1523:         g1 >> w1 >> g2",
          "1524:         t1.as_teardown(setups=s1)",
          "1525:         t2.as_teardown(setups=s2)",
          "1526:     assert set(s1.operator.downstream_task_ids) == {\"work\", \"group_2.teardown_1\"}",
          "1527:     assert set(s2.operator.downstream_task_ids) == {\"work\", \"group_2.teardown_2\"}",
          "1528:     assert set(w1.operator.downstream_task_ids) == {\"group_2.teardown_1\", \"group_2.teardown_2\"}",
          "1529:     assert set(t1.operator.downstream_task_ids) == set()",
          "1530:     assert set(t2.operator.downstream_task_ids) == set()",
          "1532:     def get_nodes(group):",
          "1533:         d = task_group_to_dict(group)",
          "1534:         new_d = {}",
          "1535:         new_d[\"id\"] = d[\"id\"]",
          "1536:         new_d[\"children\"] = [{\"id\": x[\"id\"]} for x in d[\"children\"]]",
          "1537:         return new_d",
          "1539:     assert get_nodes(g1) == {",
          "1540:         \"id\": \"group_1\",",
          "1541:         \"children\": [",
          "1542:             {\"id\": \"group_1.setup_1\"},",
          "1543:             {\"id\": \"group_1.setup_2\"},",
          "1544:             {\"id\": \"group_1.downstream_join_id\"},",
          "1545:         ],",
          "1546:     }",
          "1549: def test_task_group_arrow_with_setup_group_deeper_setup():",
          "1550:     \"\"\"",
          "1551:     When recursing upstream for a non-teardown leaf, we should ignore setups that",
          "1552:     are direct upstream of a teardown.",
          "1553:     \"\"\"",
          "1554:     with DAG(dag_id=\"setup_group_teardown_group_2\", start_date=pendulum.now()):",
          "1555:         with TaskGroup(\"group_1\") as g1:",
          "1557:             @setup",
          "1558:             def setup_1():",
          "1559:                 ...",
          "1561:             @setup",
          "1562:             def setup_2():",
          "1563:                 ...",
          "1565:             @teardown",
          "1566:             def teardown_0():",
          "1567:                 ...",
          "1569:             s1 = setup_1()",
          "1570:             s2 = setup_2()",
          "1571:             t0 = teardown_0()",
          "1572:             s2 >> t0",
          "1574:         with TaskGroup(\"group_2\") as g2:",
          "1576:             @teardown",
          "1577:             def teardown_1():",
          "1578:                 ...",
          "1580:             @teardown",
          "1581:             def teardown_2():",
          "1582:                 ...",
          "1584:             t1 = teardown_1()",
          "1585:             t2 = teardown_2()",
          "1587:         @task_decorator",
          "1588:         def work():",
          "1589:             ...",
          "1591:         w1 = work()",
          "1592:         g1 >> w1 >> g2",
          "1593:         t1.as_teardown(setups=s1)",
          "1594:         t2.as_teardown(setups=s2)",
          "1595:     assert set(s1.operator.downstream_task_ids) == {\"work\", \"group_2.teardown_1\"}",
          "1596:     assert set(s2.operator.downstream_task_ids) == {\"group_1.teardown_0\", \"group_2.teardown_2\"}",
          "1597:     assert set(w1.operator.downstream_task_ids) == {\"group_2.teardown_1\", \"group_2.teardown_2\"}",
          "1598:     assert set(t1.operator.downstream_task_ids) == set()",
          "1599:     assert set(t2.operator.downstream_task_ids) == set()",
          "",
          "---------------"
        ]
      }
    }
  ]
}