{
  "cve_id": "CVE-2022-31777",
  "cve_desc": "A stored cross-site scripting (XSS) vulnerability in Apache Spark 3.2.1 and earlier, and 3.3.0, allows remote attackers to execute arbitrary JavaScript in the web browser of a user, by including a malicious payload into the logs which would be returned in logs rendered in the UI.",
  "repo": "apache/spark",
  "patch_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
  "patch_info": {
    "commit_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
    "repo": "apache/spark",
    "commit_url": "https://github.com/apache/spark/commit/ad90195de56688ce0898691eb9d04297ab0871ad",
    "files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ],
    "message": "[SPARK-39505][UI] Escape log content rendered in UI\n\n### What changes were proposed in this pull request?\n\nEscape log content rendered to the UI.\n\n### Why are the changes needed?\n\nLog content may contain reserved characters or other code in the log and be misinterpreted in the UI as HTML.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nExisting tests\n\nCloses #36902 from srowen/LogViewEscape.\n\nAuthored-by: Sean Owen <srowen@gmail.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
    "before_after_code_files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ]
  },
  "patch_diff": {
    "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js": [
      "File: core/src/main/resources/org/apache/spark/ui/static/log-view.js -> core/src/main/resources/org/apache/spark/ui/static/log-view.js",
      "--- Hunk 1 ---",
      "[Context before]",
      "85:       if (retStartByte == 0) {",
      "86:         disableMoreButton();",
      "87:       }",
      "90:       curLogLength = curLogLength + (startByte - retStartByte);",
      "91:       startByte = retStartByte;",
      "",
      "[Removed Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(cleanData);",
      "",
      "[Added Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(document.createTextNode(cleanData));",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "115:             var retLogLength = dataInfo[2];",
      "117:             var cleanData = data.substring(newlineIndex + 1);",
      "120:             curLogLength = curLogLength + (retEndByte - retStartByte);",
      "121:             endByte = retEndByte;",
      "",
      "[Removed Lines]",
      "118:             $(\"pre\", \".log-content\").append(cleanData);",
      "",
      "[Added Lines]",
      "118:             $(\"pre\", \".log-content\").append(document.createTextNode(cleanData));",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "ef521d30a3b023213bbc3076911a93c0c0c425dc",
      "candidate_info": {
        "commit_hash": "ef521d30a3b023213bbc3076911a93c0c0c425dc",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/ef521d30a3b023213bbc3076911a93c0c0c425dc",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/AnalysisSuite.scala"
        ],
        "message": "[SPARK-39354][SQL] Ensure show `Table or view not found` even if there are `dataTypeMismatchError` related to `Filter` at the same time\n\n### What changes were proposed in this pull request?\nAfter SPARK-38118,  `dataTypeMismatchError` related to `Filter` will be checked and throw in `RemoveTempResolvedColumn`,  this will cause compatibility issue with exception message presentation.\n\nFor example, the following case:\n\n```\nspark.sql(\"create table t1(user_id int, auct_end_dt date) using parquet;\")\nspark.sql(\"select * from t1 join t2 on t1.user_id = t2.user_id where t1.auct_end_dt >= Date_sub('2020-12-27', 90)\").show\n```\n\nThe expected message is\n\n```\nTable or view not found: t2\n```\n\nBut the actual message is\n```\norg.apache.spark.sql.AnalysisException: cannot resolve 'date_sub('2020-12-27', 90)' due to data type mismatch: argument 1 requires date type, however, ''2020-12-27'' is of string type.; line 1 pos 76\n```\n\nFor forward compatibility, this pr change to only records `DATA_TYPE_MISMATCH_ERROR_MESSAGE` in the `RemoveTempResolvedColumn` check  process , and move `failAnalysis` to `CheckAnalysis#checkAnalysis`\n\n### Why are the changes needed?\nFix analysis exception message compatibility.\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\nPass Github Actions and add a new test case\n\nCloses #36746 from LuciferYang/SPARK-39354.\n\nAuthored-by: yangjie01 <yangjie01@baidu.com>\nSigned-off-by: Max Gekk <max.gekk@gmail.com>\n(cherry picked from commit 89fdb8a6fb6a669c458891b3abeba236e64b1e89)\nSigned-off-by: Max Gekk <max.gekk@gmail.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/AnalysisSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/AnalysisSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "29: import org.apache.spark.sql.AnalysisException",
          "30: import org.apache.spark.sql.catalyst._",
          "32: import org.apache.spark.sql.catalyst.catalog._",
          "33: import org.apache.spark.sql.catalyst.encoders.OuterScopes",
          "34: import org.apache.spark.sql.catalyst.expressions.{Expression, FrameLessOffsetWindowFunction, _}",
          "",
          "[Removed Lines]",
          "31: import org.apache.spark.sql.catalyst.analysis.SimpleAnalyzer.{extraHintForAnsiTypeCoercionExpression, DATA_TYPE_MISMATCH_ERROR}",
          "",
          "[Added Lines]",
          "31: import org.apache.spark.sql.catalyst.analysis.SimpleAnalyzer.DATA_TYPE_MISMATCH_ERROR_MESSAGE",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "4328:           case e: Expression if e.childrenResolved && e.checkInputDataTypes().isFailure =>",
          "4329:             e.checkInputDataTypes() match {",
          "4330:               case TypeCheckResult.TypeCheckFailure(message) =>",
          "4335:             }",
          "4336:           case _ =>",
          "4337:         })",
          "",
          "[Removed Lines]",
          "4331:                 e.setTagValue(DATA_TYPE_MISMATCH_ERROR, true)",
          "4332:                 e.failAnalysis(",
          "4333:                   s\"cannot resolve '${e.sql}' due to data type mismatch: $message\" +",
          "4334:                     extraHintForAnsiTypeCoercionExpression(plan))",
          "",
          "[Added Lines]",
          "4331:                 e.setTagValue(DATA_TYPE_MISMATCH_ERROR_MESSAGE, message)",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "51:   val DATA_TYPE_MISMATCH_ERROR = TreeNodeTag[Boolean](\"dataTypeMismatchError\")",
          "53:   protected def failAnalysis(msg: String): Nothing = {",
          "54:     throw new AnalysisException(msg)",
          "55:   }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "53:   val DATA_TYPE_MISMATCH_ERROR_MESSAGE = TreeNodeTag[String](\"dataTypeMismatchError\")",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "174:             }",
          "175:         }",
          "178:           case a: Attribute if !a.resolved =>",
          "179:             val missingCol = a.sql",
          "180:             val candidates = operator.inputSet.toSeq.map(_.qualifiedName)",
          "",
          "[Removed Lines]",
          "177:         getAllExpressions(operator).foreach(_.foreachUp {",
          "",
          "[Added Lines]",
          "179:         val expressions = getAllExpressions(operator)",
          "181:         expressions.foreach(_.foreachUp {",
          "182:           case e: Expression =>",
          "183:             e.getTagValue(DATA_TYPE_MISMATCH_ERROR_MESSAGE) match {",
          "184:               case Some(message) =>",
          "185:                 e.failAnalysis(s\"cannot resolve '${e.sql}' due to data type mismatch: $message\" +",
          "186:                   extraHintForAnsiTypeCoercionExpression(operator))",
          "187:               case _ =>",
          "188:             }",
          "189:           case _ =>",
          "190:         })",
          "192:         expressions.foreach(_.foreachUp {",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/AnalysisSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/AnalysisSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/AnalysisSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/AnalysisSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "1170:            |WITH t as (SELECT true c, false d)",
          "1171:            |SELECT (t.c AND t.d) c",
          "1172:            |FROM t",
          "1174:            |HAVING ${func}(c) > 0d\"\"\".stripMargin),",
          "1176:         false)",
          "1177:     }",
          "1178:   }",
          "1180:   test(\"SPARK-39144: nested subquery expressions deduplicate relations should be done bottom up\") {",
          "1181:     val innerRelation = SubqueryAlias(\"src1\", testRelation)",
          "1182:     val outerRelation = SubqueryAlias(\"src2\", testRelation)",
          "",
          "[Removed Lines]",
          "1173:            |GROUP BY t.c",
          "1175:         Seq(s\"cannot resolve '$func(t.c)' due to data type mismatch\"),",
          "",
          "[Added Lines]",
          "1173:            |GROUP BY t.c, t.d",
          "1175:         Seq(s\"cannot resolve '$func(c)' due to data type mismatch\"),",
          "1180:   test(\"SPARK-39354: should be `Table or view not found`\") {",
          "1181:     assertAnalysisError(parsePlan(",
          "1182:       s\"\"\"",
          "1183:          |WITH t1 as (SELECT 1 user_id, CAST(\"2022-06-02\" AS DATE) dt)",
          "1184:          |SELECT *",
          "1185:          |FROM t1",
          "1186:          |JOIN t2 ON t1.user_id = t2.user_id",
          "1187:          |WHERE t1.dt >= DATE_SUB('2020-12-27', 90)\"\"\".stripMargin),",
          "1188:       Seq(s\"Table or view not found: t2\"),",
          "1189:       false)",
          "1190:   }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "f74867bddfbcdd4d08076db36851e88b15e66556",
      "candidate_info": {
        "commit_hash": "f74867bddfbcdd4d08076db36851e88b15e66556",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/f74867bddfbcdd4d08076db36851e88b15e66556",
        "files": [
          "R/pkg/DESCRIPTION",
          "assembly/pom.xml",
          "common/kvstore/pom.xml",
          "common/network-common/pom.xml",
          "common/network-shuffle/pom.xml",
          "common/network-yarn/pom.xml",
          "common/sketch/pom.xml",
          "common/tags/pom.xml",
          "common/unsafe/pom.xml",
          "core/pom.xml",
          "docs/_config.yml",
          "examples/pom.xml",
          "external/avro/pom.xml",
          "external/docker-integration-tests/pom.xml",
          "external/kafka-0-10-assembly/pom.xml",
          "external/kafka-0-10-sql/pom.xml",
          "external/kafka-0-10-token-provider/pom.xml",
          "external/kafka-0-10/pom.xml",
          "external/kinesis-asl-assembly/pom.xml",
          "external/kinesis-asl/pom.xml",
          "external/spark-ganglia-lgpl/pom.xml",
          "graphx/pom.xml",
          "hadoop-cloud/pom.xml",
          "launcher/pom.xml",
          "mllib-local/pom.xml",
          "mllib/pom.xml",
          "pom.xml",
          "python/pyspark/version.py",
          "repl/pom.xml",
          "resource-managers/kubernetes/core/pom.xml",
          "resource-managers/kubernetes/integration-tests/pom.xml",
          "resource-managers/mesos/pom.xml",
          "resource-managers/yarn/pom.xml",
          "sql/catalyst/pom.xml",
          "sql/core/pom.xml",
          "sql/hive-thriftserver/pom.xml",
          "sql/hive/pom.xml",
          "streaming/pom.xml",
          "tools/pom.xml"
        ],
        "message": "Preparing Spark release v3.3.0-rc6",
        "before_after_code_files": [
          "python/pyspark/version.py||python/pyspark/version.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "python/pyspark/version.py||python/pyspark/version.py": [
          "File: python/pyspark/version.py -> python/pyspark/version.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "16: # See the License for the specific language governing permissions and",
          "17: # limitations under the License.",
          "",
          "[Removed Lines]",
          "19: __version__: str = \"3.3.0.dev0\"",
          "",
          "[Added Lines]",
          "19: __version__: str = \"3.3.0\"",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "d59f11816b7e1d9195cc08806820469f78c3e0aa",
      "candidate_info": {
        "commit_hash": "d59f11816b7e1d9195cc08806820469f78c3e0aa",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/d59f11816b7e1d9195cc08806820469f78c3e0aa",
        "files": [
          "python/pyspark/sql/functions.py"
        ],
        "message": "[SPARK-39032][PYTHON][DOCS] Examples' tag for pyspark.sql.functions.when()\n\n### What changes were proposed in this pull request?\nFix missing keyword for `pyspark.sql.functions.when()` documentation.\n\n### Why are the changes needed?\n[Documentation](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.when.html) is not formatted correctly\n\n### Does this PR introduce _any_ user-facing change?\nNo.\n\n### How was this patch tested?\nAll tests passed.\n\nCloses #36369 from vadim/SPARK-39032.\n\nAuthored-by: vadim <86705+vadim@users.noreply.github.com>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>\n(cherry picked from commit 5b53bdfa83061c160652e07b999f996fc8bd2ece)\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>",
        "before_after_code_files": [
          "python/pyspark/sql/functions.py||python/pyspark/sql/functions.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "python/pyspark/sql/functions.py||python/pyspark/sql/functions.py": [
          "File: python/pyspark/sql/functions.py -> python/pyspark/sql/functions.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1716:     value :",
          "1717:         a literal value, or a :class:`~pyspark.sql.Column` expression.",
          "1719:     >>> df.select(when(df['age'] == 2, 3).otherwise(4).alias(\"age\")).collect()",
          "1720:     [Row(age=3), Row(age=4)]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1719:     Examples",
          "1720:     --------",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "16788ee8905b94e51e00ccd8a48894f8824077b0",
      "candidate_info": {
        "commit_hash": "16788ee8905b94e51e00ccd8a48894f8824077b0",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/16788ee8905b94e51e00ccd8a48894f8824077b0",
        "files": [
          "core/src/main/scala/org/apache/spark/ExecutorAllocationManager.scala"
        ],
        "message": "[SPARK-39322][CORE][FOLLOWUP] Revise log messages for dynamic allocation and shuffle decommission\n\n### What changes were proposed in this pull request?\n\nThis PR is a follow-up for #36705 to revise the missed log message change.\n\n### Why are the changes needed?\n\nLike the documentation, this PR updates the log message correspondingly.\n- Lower log level to `INFO` from `WARN`\n- Provide a specific message according to the configurations.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo. This is a log-message-only change.\n\n### How was this patch tested?\n\nPass the CIs.\n\nCloses #36725 from dongjoon-hyun/SPARK-39322-2.\n\nAuthored-by: Dongjoon Hyun <dongjoon@apache.org>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>\n(cherry picked from commit be2c8c0115861e6975b658a7b0455bae828b7553)\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
        "before_after_code_files": [
          "core/src/main/scala/org/apache/spark/ExecutorAllocationManager.scala||core/src/main/scala/org/apache/spark/ExecutorAllocationManager.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "core/src/main/scala/org/apache/spark/ExecutorAllocationManager.scala||core/src/main/scala/org/apache/spark/ExecutorAllocationManager.scala": [
          "File: core/src/main/scala/org/apache/spark/ExecutorAllocationManager.scala -> core/src/main/scala/org/apache/spark/ExecutorAllocationManager.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "204:         s\"s${DYN_ALLOCATION_SUSTAINED_SCHEDULER_BACKLOG_TIMEOUT.key} must be > 0!\")",
          "205:     }",
          "206:     if (!conf.get(config.SHUFFLE_SERVICE_ENABLED)) {",
          "214:       } else if (!testing) {",
          "215:         throw new SparkException(\"Dynamic allocation of executors requires the external \" +",
          "216:           \"shuffle service. You may enable this through spark.shuffle.service.enabled.\")",
          "",
          "[Removed Lines]",
          "210:       if (conf.get(config.DYN_ALLOCATION_SHUFFLE_TRACKING_ENABLED) ||",
          "211:           (decommissionEnabled &&",
          "212:             conf.get(config.STORAGE_DECOMMISSION_SHUFFLE_BLOCKS_ENABLED))) {",
          "213:         logWarning(\"Dynamic allocation without a shuffle service is an experimental feature.\")",
          "",
          "[Added Lines]",
          "207:       if (conf.get(config.DYN_ALLOCATION_SHUFFLE_TRACKING_ENABLED)) {",
          "208:         logInfo(\"Dynamic allocation is enabled without a shuffle service.\")",
          "209:       } else if (decommissionEnabled &&",
          "210:           conf.get(config.STORAGE_DECOMMISSION_SHUFFLE_BLOCKS_ENABLED)) {",
          "211:         logInfo(\"Shuffle data decommission is enabled without a shuffle service.\")",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "c87180a760c04c8508c8d5da4d15723efc1671f9",
      "candidate_info": {
        "commit_hash": "c87180a760c04c8508c8d5da4d15723efc1671f9",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/c87180a760c04c8508c8d5da4d15723efc1671f9",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/EquivalentExpressions.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/SubexpressionEliminationSuite.scala",
          "sql/core/src/test/resources/sql-tests/inputs/ansi/conditional-functions.sql",
          "sql/core/src/test/resources/sql-tests/results/ansi/conditional-functions.sql.out"
        ],
        "message": "[SPARK-39040][SQL] Respect NaNvl in EquivalentExpressions for expression elimination\n\n### What changes were proposed in this pull request?\n\nRespect NaNvl in EquivalentExpressions for expression elimination.\n\n### Why are the changes needed?\n\nFor example the query will fail:\n```sql\nset spark.sql.ansi.enabled=true;\nset spark.sql.optimizer.excludedRules=org.apache.spark.sql.catalyst.optimizer.ConstantFolding;\nSELECT nanvl(1, 1/0 + 1/0);\n```\n```sql\norg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 1 times, most recent failure: Lost task 0.0 in stage 4.0 (TID 4) (10.221.98.68 executor driver): org.apache.spark.SparkArithmeticException: divide by zero. To return NULL instead, use 'try_divide'. If necessary set spark.sql.ansi.enabled to false (except for ANSI interval type) to bypass this error.\n== SQL(line 1, position 17) ==\nselect nanvl(1 , 1/0 + 1/0)\n                 ^^^    at org.apache.spark.sql.errors.QueryExecutionErrors$.divideByZeroError(QueryExecutionErrors.scala:151)\n ```\nWe should respect the ordering of conditional expression that always evaluate the predicate branch first, so the query above should not fail.\n\n### Does this PR introduce _any_ user-facing change?\n\nyes, bug fix\n\n### How was this patch tested?\n\nadd test\n\nCloses #36376 from ulysses-you/SPARK-39040.\n\nAuthored-by: ulysses-you <ulyssesyou18@gmail.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\n(cherry picked from commit f6b43f0384f9681b963f52a53759c521f6ac11d5)\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/EquivalentExpressions.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/EquivalentExpressions.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/SubexpressionEliminationSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/SubexpressionEliminationSuite.scala",
          "sql/core/src/test/resources/sql-tests/inputs/ansi/conditional-functions.sql||sql/core/src/test/resources/sql-tests/inputs/ansi/conditional-functions.sql"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/EquivalentExpressions.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/EquivalentExpressions.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/EquivalentExpressions.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/EquivalentExpressions.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "138:   private def childrenToRecurse(expr: Expression): Seq[Expression] = expr match {",
          "139:     case _: CodegenFallback => Nil",
          "140:     case i: If => i.predicate :: Nil",
          "141:     case c: CaseWhen => c.children.head :: Nil",
          "142:     case c: Coalesce => c.children.head :: Nil",
          "143:     case other => other.children",
          "144:   }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "145:     case n: NaNvl => n.left :: Nil",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "175:     case c: Coalesce if c.children.length > 1 => Seq(c.children)",
          "176:     case _ => Nil",
          "177:   }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "179:     case n: NaNvl => Seq(n.children)",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/SubexpressionEliminationSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/SubexpressionEliminationSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/SubexpressionEliminationSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/SubexpressionEliminationSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "448:     assert(code.value.toString == \"((Decimal) references[0] /* literal */)\")",
          "449:   }",
          "450: }",
          "452: case class CodegenFallbackExpression(child: Expression)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "451:   test(\"SPARK-39040: Respect NaNvl in EquivalentExpressions for expression elimination\") {",
          "452:     val add = Add(Literal(1), Literal(0))",
          "453:     val n1 = NaNvl(Literal(1.0d), Add(add, add))",
          "454:     val e1 = new EquivalentExpressions",
          "455:     e1.addExprTree(n1)",
          "456:     assert(e1.getCommonSubexpressions.isEmpty)",
          "458:     val n2 = NaNvl(add, add)",
          "459:     val e2 = new EquivalentExpressions",
          "460:     e2.addExprTree(n2)",
          "461:     assert(e2.getCommonSubexpressions.size == 1)",
          "462:     assert(e2.getCommonSubexpressions.head == add)",
          "463:   }",
          "",
          "---------------"
        ],
        "sql/core/src/test/resources/sql-tests/inputs/ansi/conditional-functions.sql||sql/core/src/test/resources/sql-tests/inputs/ansi/conditional-functions.sql": [
          "File: sql/core/src/test/resources/sql-tests/inputs/ansi/conditional-functions.sql -> sql/core/src/test/resources/sql-tests/inputs/ansi/conditional-functions.sql",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: -- Tests for conditional functions",
          "2: CREATE TABLE t USING PARQUET AS SELECT c1, c2 FROM VALUES(1, 0),(2, 1) AS t(c1, c2);",
          "4: SELECT nanvl(c1, c1/c2 + c1/c2) FROM t;",
          "6: DROP TABLE IF EXISTS t;",
          "",
          "---------------"
        ]
      }
    }
  ]
}