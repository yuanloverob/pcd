{
  "cve_id": "CVE-2022-31777",
  "cve_desc": "A stored cross-site scripting (XSS) vulnerability in Apache Spark 3.2.1 and earlier, and 3.3.0, allows remote attackers to execute arbitrary JavaScript in the web browser of a user, by including a malicious payload into the logs which would be returned in logs rendered in the UI.",
  "repo": "apache/spark",
  "patch_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
  "patch_info": {
    "commit_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
    "repo": "apache/spark",
    "commit_url": "https://github.com/apache/spark/commit/ad90195de56688ce0898691eb9d04297ab0871ad",
    "files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ],
    "message": "[SPARK-39505][UI] Escape log content rendered in UI\n\n### What changes were proposed in this pull request?\n\nEscape log content rendered to the UI.\n\n### Why are the changes needed?\n\nLog content may contain reserved characters or other code in the log and be misinterpreted in the UI as HTML.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nExisting tests\n\nCloses #36902 from srowen/LogViewEscape.\n\nAuthored-by: Sean Owen <srowen@gmail.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
    "before_after_code_files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ]
  },
  "patch_diff": {
    "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js": [
      "File: core/src/main/resources/org/apache/spark/ui/static/log-view.js -> core/src/main/resources/org/apache/spark/ui/static/log-view.js",
      "--- Hunk 1 ---",
      "[Context before]",
      "85:       if (retStartByte == 0) {",
      "86:         disableMoreButton();",
      "87:       }",
      "90:       curLogLength = curLogLength + (startByte - retStartByte);",
      "91:       startByte = retStartByte;",
      "",
      "[Removed Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(cleanData);",
      "",
      "[Added Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(document.createTextNode(cleanData));",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "115:             var retLogLength = dataInfo[2];",
      "117:             var cleanData = data.substring(newlineIndex + 1);",
      "120:             curLogLength = curLogLength + (retEndByte - retStartByte);",
      "121:             endByte = retEndByte;",
      "",
      "[Removed Lines]",
      "118:             $(\"pre\", \".log-content\").append(cleanData);",
      "",
      "[Added Lines]",
      "118:             $(\"pre\", \".log-content\").append(document.createTextNode(cleanData));",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "bebfecb81da2de33240d8ab37bd641985281844e",
      "candidate_info": {
        "commit_hash": "bebfecb81da2de33240d8ab37bd641985281844e",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/bebfecb81da2de33240d8ab37bd641985281844e",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/LimitPushDownThroughWindow.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/LimitPushdownThroughWindowSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/DataFrameWindowFunctionsSuite.scala"
        ],
        "message": "[SPARK-38614][SQL] Don't push down limit through window that's using percent_rank\n\n### What changes were proposed in this pull request?\n\nChange `LimitPushDownThroughWindow` so that it no longer supports pushing down a limit through a window using percent_rank.\n\n### Why are the changes needed?\n\nGiven a query with a limit of _n_ rows, and a window whose child produces _m_ rows, percent_rank will label the _nth_ row as 100% rather than the _mth_ row.\n\nThis behavior conflicts with Spark 3.1.3, Hive 2.3.9 and Prestodb 0.268.\n\n#### Example\n\nAssume this data:\n```\ncreate table t1 stored as parquet as\nselect *\nfrom range(101);\n```\nAnd also assume this query:\n```\nselect id, percent_rank() over (order by id) as pr\nfrom t1\nlimit 3;\n```\nWith Spark 3.2.1, 3.3.0, and master, the limit is applied before the percent_rank:\n```\n0\t0.0\n1\t0.5\n2\t1.0\n```\nWith Spark 3.1.3, and Hive 2.3.9, and Prestodb 0.268, the limit is applied _after_ percent_rank:\n\nSpark 3.1.3:\n```\n0\t0.0\n1\t0.01\n2\t0.02\n```\nHive 2.3.9:\n```\n0: jdbc:hive2://localhost:10000> select id, percent_rank() over (order by id) as pr\nfrom t1\nlimit 3;\n. . . . . . . . . . . . . . . .> . . . . . . . . . . . . . . . .> WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n+-----+-------+\n| id  |  pr   |\n+-----+-------+\n| 0   | 0.0   |\n| 1   | 0.01  |\n| 2   | 0.02  |\n+-----+-------+\n3 rows selected (4.621 seconds)\n0: jdbc:hive2://localhost:10000>\n```\n\nPrestodb 0.268:\n```\n id |  pr\n----+------\n  0 |  0.0\n  1 | 0.01\n  2 | 0.02\n(3 rows)\n```\nWith this PR, Spark will apply the limit after percent_rank.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo (besides changing percent_rank's behavior to be more like Spark 3.1.3, Hive, and Prestodb).\n\n### How was this patch tested?\n\nNew unit tests.\n\nCloses #36951 from bersprockets/percent_rank_issue.\n\nAuthored-by: Bruce Robbins <bersprockets@gmail.com>\nSigned-off-by: Yuming Wang <yumwang@ebay.com>\n(cherry picked from commit a63ce5676e79f15903e9fd533a26a6c3ec9bf7a8)\nSigned-off-by: Yuming Wang <yumwang@ebay.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/LimitPushDownThroughWindow.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/LimitPushDownThroughWindow.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/LimitPushdownThroughWindowSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/LimitPushdownThroughWindowSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/DataFrameWindowFunctionsSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/DataFrameWindowFunctionsSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/LimitPushDownThroughWindow.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/LimitPushDownThroughWindow.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/LimitPushDownThroughWindow.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/LimitPushDownThroughWindow.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: package org.apache.spark.sql.catalyst.optimizer",
          "21: import org.apache.spark.sql.catalyst.plans.logical.{Limit, LocalLimit, LogicalPlan, Project, Sort, Window}",
          "22: import org.apache.spark.sql.catalyst.rules.Rule",
          "23: import org.apache.spark.sql.catalyst.trees.TreePattern.{LIMIT, WINDOW}",
          "",
          "[Removed Lines]",
          "20: import org.apache.spark.sql.catalyst.expressions.{Alias, CurrentRow, IntegerLiteral, NamedExpression, RankLike, RowFrame, RowNumberLike, SpecifiedWindowFrame, UnboundedPreceding, WindowExpression, WindowSpecDefinition}",
          "",
          "[Added Lines]",
          "20: import org.apache.spark.sql.catalyst.expressions.{Alias, CurrentRow, DenseRank, IntegerLiteral, NamedExpression, NTile, Rank, RowFrame, RowNumber, SpecifiedWindowFrame, UnboundedPreceding, WindowExpression, WindowSpecDefinition}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "34:   private def supportsPushdownThroughWindow(",
          "35:       windowExpressions: Seq[NamedExpression]): Boolean = windowExpressions.forall {",
          "37:         SpecifiedWindowFrame(RowFrame, UnboundedPreceding, CurrentRow))), _) => true",
          "38:     case _ => false",
          "39:   }",
          "",
          "[Removed Lines]",
          "36:     case Alias(WindowExpression(_: RankLike | _: RowNumberLike, WindowSpecDefinition(Nil, _,",
          "",
          "[Added Lines]",
          "36:     case Alias(WindowExpression(_: Rank | _: DenseRank | _: NTile | _: RowNumber,",
          "37:         WindowSpecDefinition(Nil, _,",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/LimitPushdownThroughWindowSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/LimitPushdownThroughWindowSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/LimitPushdownThroughWindowSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/LimitPushdownThroughWindowSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: import org.apache.spark.sql.Row",
          "21: import org.apache.spark.sql.catalyst.dsl.expressions._",
          "22: import org.apache.spark.sql.catalyst.dsl.plans._",
          "24: import org.apache.spark.sql.catalyst.plans._",
          "25: import org.apache.spark.sql.catalyst.plans.logical._",
          "26: import org.apache.spark.sql.catalyst.rules._",
          "",
          "[Removed Lines]",
          "23: import org.apache.spark.sql.catalyst.expressions.{CurrentRow, Rank, RowFrame, RowNumber, SpecifiedWindowFrame, UnboundedPreceding}",
          "",
          "[Added Lines]",
          "23: import org.apache.spark.sql.catalyst.expressions.{CurrentRow, PercentRank, Rank, RowFrame, RowNumber, SpecifiedWindowFrame, UnboundedPreceding}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "187:       Optimize.execute(originalQuery.analyze),",
          "188:       WithoutOptimize.execute(originalQuery.analyze))",
          "189:   }",
          "190: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "191:   test(\"SPARK-38614: Should not push through percent_rank window function\") {",
          "192:     val originalQuery = testRelation",
          "193:       .select(a, b, c,",
          "194:         windowExpr(new PercentRank(), windowSpec(Nil, c.desc :: Nil, windowFrame)).as(\"rn\"))",
          "195:       .limit(2)",
          "197:     comparePlans(",
          "198:       Optimize.execute(originalQuery.analyze),",
          "199:       WithoutOptimize.execute(originalQuery.analyze))",
          "200:   }",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/DataFrameWindowFunctionsSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/DataFrameWindowFunctionsSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/DataFrameWindowFunctionsSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/DataFrameWindowFunctionsSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "1190:       )",
          "1191:     )",
          "1192:   }",
          "1193: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1194:   test(\"SPARK-38614: percent_rank should apply before limit\") {",
          "1195:     val df = Seq.tabulate(101)(identity).toDF(\"id\")",
          "1196:     val w = Window.orderBy(\"id\")",
          "1197:     checkAnswer(",
          "1198:       df.select($\"id\", percent_rank().over(w)).limit(3),",
          "1199:       Seq(",
          "1200:         Row(0, 0.0d),",
          "1201:         Row(1, 0.01d),",
          "1202:         Row(2, 0.02d)",
          "1203:       )",
          "1204:     )",
          "1205:   }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "d8e157d0347f51c54e334fabe76072fc95332671",
      "candidate_info": {
        "commit_hash": "d8e157d0347f51c54e334fabe76072fc95332671",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/d8e157d0347f51c54e334fabe76072fc95332671",
        "files": [
          "python/pyspark/sql/functions.py",
          "sql/core/src/main/scala/org/apache/spark/sql/functions.scala"
        ],
        "message": "[SPARK-38017][FOLLOWUP][3.3] Hide TimestampNTZ in the doc\n\n### What changes were proposed in this pull request?\n\nThis PR removes `TimestampNTZ` from the doc about `TimeWindow` and `SessionWIndow`.\n\n### Why are the changes needed?\n\nAs we discussed, it's better to hide `TimestampNTZ` from the doc.\nhttps://github.com/apache/spark/pull/35313#issuecomment-1185192162\n\n### Does this PR introduce _any_ user-facing change?\n\nThe document will be changed, but there is no compatibility problem.\n\n### How was this patch tested?\n\nBuilt the doc with `SKIP_RDOC=1 SKIP_SQLDOC=1 bundle exec jekyll build` at `doc` directory.\nThen, confirmed the generated HTML.\n\nCloses #37882 from sarutak/fix-window-doc-3.3.\n\nAuthored-by: Kousuke Saruta <sarutak@oss.nttdata.com>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>",
        "before_after_code_files": [
          "python/pyspark/sql/functions.py||python/pyspark/sql/functions.py",
          "sql/core/src/main/scala/org/apache/spark/sql/functions.scala||sql/core/src/main/scala/org/apache/spark/sql/functions.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "python/pyspark/sql/functions.py||python/pyspark/sql/functions.py": [
          "File: python/pyspark/sql/functions.py -> python/pyspark/sql/functions.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "2557:     ----------",
          "2558:     timeColumn : :class:`~pyspark.sql.Column`",
          "2559:         The column or the expression to use as the timestamp for windowing by time.",
          "2561:     windowDuration : str",
          "2562:         A string specifying the width of the window, e.g. `10 minutes`,",
          "2563:         `1 second`. Check `org.apache.spark.unsafe.types.CalendarInterval` for",
          "",
          "[Removed Lines]",
          "2560:         The time column must be of TimestampType or TimestampNTZType.",
          "",
          "[Added Lines]",
          "2560:         The time column must be of TimestampType.",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "2632:     ----------",
          "2633:     timeColumn : :class:`~pyspark.sql.Column` or str",
          "2634:         The column name or column to use as the timestamp for windowing by time.",
          "2636:     gapDuration : :class:`~pyspark.sql.Column` or str",
          "2637:         A Python string literal or column specifying the timeout of the session. It could be",
          "2638:         static value, e.g. `10 minutes`, `1 second`, or an expression/UDF that specifies gap",
          "",
          "[Removed Lines]",
          "2635:         The time column must be of TimestampType or TimestampNTZType.",
          "",
          "[Added Lines]",
          "2635:         The time column must be of TimestampType.",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/functions.scala||sql/core/src/main/scala/org/apache/spark/sql/functions.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/functions.scala -> sql/core/src/main/scala/org/apache/spark/sql/functions.scala"
        ]
      }
    },
    {
      "candidate_hash": "376c14ac8cfb6d51c29755b5ee951e5e41981a1a",
      "candidate_info": {
        "commit_hash": "376c14ac8cfb6d51c29755b5ee951e5e41981a1a",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/376c14ac8cfb6d51c29755b5ee951e5e41981a1a",
        "files": [
          "dev/create-release/release-build.sh",
          "dev/create-release/release-tag.sh"
        ],
        "message": "[SPARK-39411][BUILD] Fix release script to address type hint in pyspark/version.py\n\nThis PR proposes to address type hints `__version__: str` correctly in each release. The type hint was added from Spark 3.3.0 at https://github.com/apache/spark/commit/f59e1d548e2e7c97195697910c40c5383a76ca48.\n\nFor PySpark to have the correct version in releases.\n\nNo, dev-only.\n\nManually tested by setting environment variables and running the changed shall commands locally.\n\nCloses #36803 from HyukjinKwon/SPARK-39411.\n\nAuthored-by: Hyukjin Kwon <gurwls223@apache.org>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>\n(cherry picked from commit 87b0a41cfb46ba9389c6f5abb9628415a72c4f93)\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>",
        "before_after_code_files": [
          "dev/create-release/release-build.sh||dev/create-release/release-build.sh",
          "dev/create-release/release-tag.sh||dev/create-release/release-tag.sh"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/create-release/release-build.sh||dev/create-release/release-build.sh": [
          "File: dev/create-release/release-build.sh -> dev/create-release/release-build.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "265:     # Write out the VERSION to PySpark version info we rewrite the - into a . and SNAPSHOT",
          "266:     # to dev0 to be closer to PEP440.",
          "267:     PYSPARK_VERSION=`echo \"$SPARK_VERSION\" |  sed -e \"s/-/./\" -e \"s/SNAPSHOT/dev0/\" -e \"s/preview/dev/\"`",
          "270:     # Get maven home set by MVN",
          "271:     MVN_HOME=`$MVN -version 2>&1 | grep 'Maven home' | awk '{print $NF}'`",
          "",
          "[Removed Lines]",
          "268:     echo \"__version__='$PYSPARK_VERSION'\" > python/pyspark/version.py",
          "",
          "[Added Lines]",
          "269:     if [[ $SPARK_VERSION == 3.0* ]] || [[ $SPARK_VERSION == 3.1* ]] || [[ $SPARK_VERSION == 3.2* ]]; then",
          "270:       echo \"__version__ = '$PYSPARK_VERSION'\" > python/pyspark/version.py",
          "271:     else",
          "272:       echo \"__version__: str = '$PYSPARK_VERSION'\" > python/pyspark/version.py",
          "273:     fi",
          "",
          "---------------"
        ],
        "dev/create-release/release-tag.sh||dev/create-release/release-tag.sh": [
          "File: dev/create-release/release-tag.sh -> dev/create-release/release-tag.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "85: sed -i\".tmp1\" 's/SPARK_VERSION:.*$/SPARK_VERSION: '\"$RELEASE_VERSION\"'/g' docs/_config.yml",
          "86: sed -i\".tmp2\" 's/SPARK_VERSION_SHORT:.*$/SPARK_VERSION_SHORT: '\"$RELEASE_VERSION\"'/g' docs/_config.yml",
          "87: sed -i\".tmp3\" \"s/'facetFilters':.*$/'facetFilters': [\\\"version:$RELEASE_VERSION\\\"]/g\" docs/_config.yml",
          "90: git commit -a -m \"Preparing Spark release $RELEASE_TAG\"",
          "91: echo \"Creating tag $RELEASE_TAG at the head of $GIT_BRANCH\"",
          "",
          "[Removed Lines]",
          "88: sed -i\".tmp4\" 's/__version__ = .*$/__version__ = \"'\"$RELEASE_VERSION\"'\"/' python/pyspark/version.py",
          "",
          "[Added Lines]",
          "88: if [[ $RELEASE_VERSION == 3.0* ]] || [[ $RELEASE_VERSION == 3.1* ]] || [[ $RELEASE_VERSION == 3.2* ]]; then",
          "89:   sed -i\".tmp4\" 's/__version__ = .*$/__version__ = \"'\"$RELEASE_VERSION\"'\"/' python/pyspark/version.py",
          "90: else",
          "91:   sed -i\".tmp4\" 's/__version__: str = .*$/__version__: str = \"'\"$RELEASE_VERSION\"'\"/' python/pyspark/version.py",
          "92: fi",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "98: sed -i\".tmp5\" 's/Version.*$/Version: '\"$R_NEXT_VERSION\"'/g' R/pkg/DESCRIPTION",
          "99: # Write out the R_NEXT_VERSION to PySpark version info we use dev0 instead of SNAPSHOT to be closer",
          "100: # to PEP440.",
          "104: # Update docs with next version",
          "105: sed -i\".tmp7\" 's/SPARK_VERSION:.*$/SPARK_VERSION: '\"$NEXT_VERSION\"'/g' docs/_config.yml",
          "",
          "[Removed Lines]",
          "101: sed -i\".tmp6\" 's/__version__ = .*$/__version__ = \"'\"$R_NEXT_VERSION.dev0\"'\"/' python/pyspark/version.py",
          "",
          "[Added Lines]",
          "105: if [[ $RELEASE_VERSION == 3.0* ]] || [[ $RELEASE_VERSION == 3.1* ]] || [[ $RELEASE_VERSION == 3.2* ]]; then",
          "106:   sed -i\".tmp6\" 's/__version__ = .*$/__version__ = \"'\"$R_NEXT_VERSION.dev0\"'\"/' python/pyspark/version.py",
          "107: else",
          "108:   sed -i\".tmp6\" 's/__version__: str = .*$/__version__: str = \"'\"$R_NEXT_VERSION.dev0\"'\"/' python/pyspark/version.py",
          "109: fi",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "ffa8152912d83ff0f6251891b097319d28572890",
      "candidate_info": {
        "commit_hash": "ffa8152912d83ff0f6251891b097319d28572890",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/ffa8152912d83ff0f6251891b097319d28572890",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/joins.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/OuterJoinEliminationSuite.scala"
        ],
        "message": "[SPARK-38868][SQL] Don't propagate exceptions from filter predicate when optimizing outer joins\n\n### What changes were proposed in this pull request?\n\nChange `EliminateOuterJoin#canFilterOutNull` to return `false` when a `where` condition throws an exception.\n\n### Why are the changes needed?\n\nConsider this query:\n```\nselect *\nfrom (select id, id as b from range(0, 10)) l\nleft outer join (select id, id + 1 as c from range(0, 10)) r\non l.id = r.id\nwhere assert_true(c > 0) is null;\n```\nThe query should succeed, but instead fails with\n```\njava.lang.RuntimeException: '(c#1L > cast(0 as bigint))' is not true!\n```\nThis happens even though there is no row where `c > 0` is false.\n\nThe `EliminateOuterJoin` rule checks if it can convert the outer join to a inner join based on the expression in the where clause, which in this case is\n```\nassert_true(c > 0) is null\n```\n`EliminateOuterJoin#canFilterOutNull` evaluates that expression with `c` set to `null` to see if the result is `null` or `false`. That rule doesn't expect the result to be a `RuntimeException`, but in this case it always is.\n\nThat is, the assertion is failing during optimization, not at run time.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nNew unit test.\n\nCloses #36230 from bersprockets/outer_join_eval_assert_issue.\n\nAuthored-by: Bruce Robbins <bersprockets@gmail.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\n(cherry picked from commit e2930b8dc087e5a284b451c4cac6c1a2459b456d)\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/joins.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/joins.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/OuterJoinEliminationSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/OuterJoinEliminationSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/joins.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/joins.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/joins.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/joins.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: package org.apache.spark.sql.catalyst.optimizer",
          "20: import scala.annotation.tailrec",
          "22: import org.apache.spark.sql.catalyst.expressions._",
          "23: import org.apache.spark.sql.catalyst.planning.ExtractFiltersAndInnerJoins",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "21: import scala.util.control.NonFatal",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "144:     val emptyRow = new GenericInternalRow(attributes.length)",
          "145:     val boundE = BindReferences.bindReference(e, attributes)",
          "146:     if (boundE.exists(_.isInstanceOf[Unevaluable])) return false",
          "149:   }",
          "151:   private def buildNewJoinType(filter: Filter, join: Join): JoinType = {",
          "",
          "[Removed Lines]",
          "147:     val v = boundE.eval(emptyRow)",
          "148:     v == null || v == false",
          "",
          "[Added Lines]",
          "151:     try {",
          "152:       val v = boundE.eval(emptyRow)",
          "153:       v == null || v == false",
          "154:     } catch {",
          "155:       case NonFatal(e) =>",
          "157:         false",
          "158:     }",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/OuterJoinEliminationSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/OuterJoinEliminationSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/OuterJoinEliminationSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/OuterJoinEliminationSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: import org.apache.spark.sql.catalyst.analysis.EliminateSubqueryAliases",
          "21: import org.apache.spark.sql.catalyst.dsl.expressions._",
          "22: import org.apache.spark.sql.catalyst.dsl.plans._",
          "24: import org.apache.spark.sql.catalyst.plans._",
          "25: import org.apache.spark.sql.catalyst.plans.logical._",
          "26: import org.apache.spark.sql.catalyst.rules._",
          "27: import org.apache.spark.sql.internal.SQLConf",
          "29: class OuterJoinEliminationSuite extends PlanTest {",
          "30:   object Optimize extends RuleExecutor[LogicalPlan] {",
          "",
          "[Removed Lines]",
          "23: import org.apache.spark.sql.catalyst.expressions.{Coalesce, IsNotNull}",
          "",
          "[Added Lines]",
          "23: import org.apache.spark.sql.catalyst.expressions.{Coalesce, If, IsNotNull, Literal, RaiseError}",
          "28: import org.apache.spark.sql.types.StringType",
          "29: import org.apache.spark.unsafe.types.UTF8String",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "251:       comparePlans(optimized, originalQuery.analyze)",
          "252:     }",
          "253:   }",
          "254: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "257:   test(\"SPARK-38868: exception thrown from filter predicate does not propagate\") {",
          "258:     val x = testRelation.subquery(Symbol(\"x\"))",
          "259:     val y = testRelation1.subquery(Symbol(\"y\"))",
          "261:     val message = Literal(UTF8String.fromString(\"Bad value\"), StringType)",
          "262:     val originalQuery =",
          "263:       x.join(y, LeftOuter, Option(\"x.a\".attr === \"y.d\".attr))",
          "264:         .where(If(\"y.d\".attr > 0, true, RaiseError(message)).isNull)",
          "266:     val optimized = Optimize.execute(originalQuery.analyze)",
          "268:     comparePlans(optimized, originalQuery.analyze)",
          "269:   }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "b7e95bad882482168b7dd301fcfa3daf80477a7a",
      "candidate_info": {
        "commit_hash": "b7e95bad882482168b7dd301fcfa3daf80477a7a",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/b7e95bad882482168b7dd301fcfa3daf80477a7a",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/finishAnalysis.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/QueryPlan.scala"
        ],
        "message": "[SPARK-39259][SQL][FOLLOWUP] Fix source and binary incompatibilities in transformDownWithSubqueries\n\n### What changes were proposed in this pull request?\n\nThis is a followup to #36654. That PR modified the existing `QueryPlan.transformDownWithSubqueries` to add additional arguments for tree pattern pruning.\n\nIn this PR, I roll back the change to that method's signature and instead add a new `transformDownWithSubqueriesAndPruning` method.\n\n### Why are the changes needed?\n\nThe original change breaks binary and source compatibility in Catalyst. Technically speaking, Catalyst APIs are considered internal to Spark and are subject to change between minor releases (see [source](https://github.com/apache/spark/blob/bb51add5c79558df863d37965603387d40cc4387/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/package.scala#L20-L24)), but I think it's nice to try to avoid API breakage when possible.\n\nWhile trying to compile some custom Catalyst code, I ran into issues when trying to call the `transformDownWithSubqueries` method without supplying a tree pattern filter condition. If I do `transformDownWithSubqueries() { f} ` then I get a compilation error. I think this is due to the first parameter group containing all default parameters.\n\nMy PR's solution of adding a new `transformDownWithSubqueriesAndPruning` method solves this problem. It's also more consistent with the naming convention used for other pruning-enabled tree transformation methods.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nExisting tests.\n\nCloses #36765 from JoshRosen/SPARK-39259-binary-compatibility-followup.\n\nAuthored-by: Josh Rosen <joshrosen@databricks.com>\nSigned-off-by: Max Gekk <max.gekk@gmail.com>\n(cherry picked from commit eda6c4b9987f0515cb0aae4686c8a0ae0a3987d4)\nSigned-off-by: Max Gekk <max.gekk@gmail.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/finishAnalysis.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/finishAnalysis.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/QueryPlan.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/QueryPlan.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/finishAnalysis.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/finishAnalysis.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/finishAnalysis.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/finishAnalysis.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "84:       treePatternbits.containsPattern(CURRENT_LIKE)",
          "85:     }",
          "88:       case subQuery =>",
          "89:         subQuery.transformAllExpressionsWithPruning(transformCondition) {",
          "90:           case cd: CurrentDate =>",
          "",
          "[Removed Lines]",
          "87:     plan.transformDownWithSubqueries(transformCondition) {",
          "",
          "[Added Lines]",
          "87:     plan.transformDownWithSubqueriesAndPruning(transformCondition) {",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/QueryPlan.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/QueryPlan.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/QueryPlan.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/QueryPlan.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "456:   def transformWithSubqueries(f: PartialFunction[PlanType, PlanType]): PlanType =",
          "",
          "[Removed Lines]",
          "457:     transformDownWithSubqueries(AlwaysProcess.fn, UnknownRuleId)(f)",
          "",
          "[Added Lines]",
          "457:     transformDownWithSubqueries(f)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "486:     val g: PartialFunction[PlanType, PlanType] = new PartialFunction[PlanType, PlanType] {",
          "487:       override def isDefinedAt(x: PlanType): Boolean = true",
          "",
          "[Removed Lines]",
          "482:   def transformDownWithSubqueries(",
          "483:     cond: TreePatternBits => Boolean = AlwaysProcess.fn, ruleId: RuleId = UnknownRuleId)",
          "484:     (f: PartialFunction[PlanType, PlanType])",
          "485: : PlanType = {",
          "",
          "[Added Lines]",
          "482:   def transformDownWithSubqueries(f: PartialFunction[PlanType, PlanType]): PlanType = {",
          "483:     transformDownWithSubqueriesAndPruning(AlwaysProcess.fn, UnknownRuleId)(f)",
          "484:   }",
          "492:   def transformDownWithSubqueriesAndPruning(",
          "493:       cond: TreePatternBits => Boolean,",
          "494:       ruleId: RuleId = UnknownRuleId)",
          "495:       (f: PartialFunction[PlanType, PlanType]): PlanType = {",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "490:         val transformed = f.applyOrElse[PlanType, PlanType](plan, identity)",
          "491:         transformed transformExpressionsDown {",
          "492:           case planExpression: PlanExpression[PlanType] =>",
          "494:             planExpression.withNewPlan(newPlan)",
          "495:         }",
          "496:       }",
          "",
          "[Removed Lines]",
          "493:             val newPlan = planExpression.plan.transformDownWithSubqueries(cond, ruleId)(f)",
          "",
          "[Added Lines]",
          "503:             val newPlan = planExpression.plan.transformDownWithSubqueriesAndPruning(cond, ruleId)(f)",
          "",
          "---------------"
        ]
      }
    }
  ]
}