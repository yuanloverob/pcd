{
  "cve_id": "CVE-2023-50944",
  "cve_desc": "Apache Airflow, versions before 2.8.1, have a vulnerability that allows an authenticated user to access the source code of a DAG to which they don't have access.\u00a0This vulnerability is considered low since it requires an authenticated user to exploit it. Users are recommended to upgrade to version 2.8.1, which fixes this issue.",
  "repo": "apache/airflow",
  "patch_hash": "8d76538d6e105947272b000581c6fabec20146b1",
  "patch_info": {
    "commit_hash": "8d76538d6e105947272b000581c6fabec20146b1",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/8d76538d6e105947272b000581c6fabec20146b1",
    "files": [
      "airflow/api_connexion/endpoints/dag_source_endpoint.py",
      "airflow/models/dagcode.py",
      "tests/api_connexion/endpoints/test_dag_source_endpoint.py"
    ],
    "message": "Check DAG read permission before accessing DAG code (#36257)\n\n(cherry picked from commit 30ea37e0d247ce54c2d25b115e807fdb0074d795)",
    "before_after_code_files": [
      "airflow/api_connexion/endpoints/dag_source_endpoint.py||airflow/api_connexion/endpoints/dag_source_endpoint.py",
      "airflow/models/dagcode.py||airflow/models/dagcode.py",
      "tests/api_connexion/endpoints/test_dag_source_endpoint.py||tests/api_connexion/endpoints/test_dag_source_endpoint.py"
    ]
  },
  "patch_diff": {
    "airflow/api_connexion/endpoints/dag_source_endpoint.py||airflow/api_connexion/endpoints/dag_source_endpoint.py": [
      "File: airflow/api_connexion/endpoints/dag_source_endpoint.py -> airflow/api_connexion/endpoints/dag_source_endpoint.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "17: from __future__ import annotations",
      "19: from http import HTTPStatus",
      "21: from flask import Response, current_app, request",
      "22: from itsdangerous import BadSignature, URLSafeSerializer",
      "24: from airflow.api_connexion import security",
      "26: from airflow.api_connexion.schemas.dag_source_schema import dag_source_schema",
      "27: from airflow.auth.managers.models.resource_details import DagAccessEntity",
      "28: from airflow.models.dagcode import DagCode",
      "31: @security.requires_access_dag(\"GET\", DagAccessEntity.CODE)",
      "33:     \"\"\"Get source code using file token.\"\"\"",
      "34:     secret_key = current_app.config[\"SECRET_KEY\"]",
      "35:     auth_s = URLSafeSerializer(secret_key)",
      "36:     try:",
      "37:         path = auth_s.loads(file_token)",
      "39:     except (BadSignature, FileNotFoundError):",
      "40:         raise NotFound(\"Dag source not found\")",
      "",
      "[Removed Lines]",
      "25: from airflow.api_connexion.exceptions import NotFound",
      "32: def get_dag_source(*, file_token: str) -> Response:",
      "38:         dag_source = DagCode.code(path)",
      "",
      "[Added Lines]",
      "20: from typing import TYPE_CHECKING",
      "26: from airflow.api_connexion.exceptions import NotFound, PermissionDenied",
      "28: from airflow.api_connexion.security import get_readable_dags",
      "30: from airflow.models.dag import DagModel",
      "32: from airflow.utils.session import NEW_SESSION, provide_session",
      "34: if TYPE_CHECKING:",
      "35:     from sqlalchemy.orm import Session",
      "39: @provide_session",
      "40: def get_dag_source(*, file_token: str, session: Session = NEW_SESSION) -> Response:",
      "46:         dag_ids = session.query(DagModel.dag_id).filter(DagModel.fileloc == path).all()",
      "47:         readable_dags = get_readable_dags()",
      "48:         # Check if user has read access to all the DAGs defined in the file",
      "49:         if any(dag_id[0] not in readable_dags for dag_id in dag_ids):",
      "50:             raise PermissionDenied()",
      "51:         dag_source = DagCode.code(path, session=session)",
      "",
      "---------------"
    ],
    "airflow/models/dagcode.py||airflow/models/dagcode.py": [
      "File: airflow/models/dagcode.py -> airflow/models/dagcode.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "177:         return cls.code(fileloc)",
      "179:     @classmethod",
      "181:         \"\"\"Return source code for this DagCode object.",
      "183:         :return: source code as string",
      "184:         \"\"\"",
      "187:     @staticmethod",
      "188:     def _get_code_from_file(fileloc):",
      "",
      "[Removed Lines]",
      "180:     def code(cls, fileloc) -> str:",
      "185:         return cls._get_code_from_db(fileloc)",
      "",
      "[Added Lines]",
      "180:     @provide_session",
      "181:     def code(cls, fileloc, session: Session = NEW_SESSION) -> str:",
      "186:         return cls._get_code_from_db(fileloc, session)",
      "",
      "---------------"
    ],
    "tests/api_connexion/endpoints/test_dag_source_endpoint.py||tests/api_connexion/endpoints/test_dag_source_endpoint.py": [
      "File: tests/api_connexion/endpoints/test_dag_source_endpoint.py -> tests/api_connexion/endpoints/test_dag_source_endpoint.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "35: ROOT_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), os.pardir, os.pardir))",
      "36: EXAMPLE_DAG_FILE = os.path.join(\"airflow\", \"example_dags\", \"example_bash_operator.py\")",
      "39: @pytest.fixture(scope=\"module\")",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "37: EXAMPLE_DAG_ID = \"example_bash_operator\"",
      "38: TEST_DAG_ID = \"latest_only\"",
      "39: NOT_READABLE_DAG_ID = \"latest_only_with_trigger\"",
      "40: TEST_MULTIPLE_DAGS_ID = \"dataset_produces_1\"",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "45:         role_name=\"Test\",",
      "46:         permissions=[(permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG_CODE)],  # type: ignore",
      "47:     )",
      "48:     create_user(app, username=\"test_no_permissions\", role_name=\"TestNoPermissions\")  # type: ignore",
      "50:     yield app",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "52:     app.appbuilder.sm.sync_perm_for_dag(  # type: ignore",
      "53:         TEST_DAG_ID,",
      "54:         access_control={\"Test\": [permissions.ACTION_CAN_READ]},",
      "55:     )",
      "56:     app.appbuilder.sm.sync_perm_for_dag(  # type: ignore",
      "57:         EXAMPLE_DAG_ID,",
      "58:         access_control={\"Test\": [permissions.ACTION_CAN_READ]},",
      "59:     )",
      "60:     app.appbuilder.sm.sync_perm_for_dag(  # type: ignore",
      "61:         TEST_MULTIPLE_DAGS_ID,",
      "62:         access_control={\"Test\": [permissions.ACTION_CAN_READ]},",
      "63:     )",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "80:     def test_should_respond_200_text(self, url_safe_serializer):",
      "81:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
      "82:         dagbag.sync_to_db()",
      "87:         response = self.client.get(",
      "88:             url, headers={\"Accept\": \"text/plain\"}, environ_overrides={\"REMOTE_USER\": \"test\"}",
      "89:         )",
      "",
      "[Removed Lines]",
      "83:         first_dag: DAG = next(iter(dagbag.dags.values()))",
      "84:         dag_docstring = self._get_dag_file_docstring(first_dag.fileloc)",
      "86:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(first_dag.fileloc)}\"",
      "",
      "[Added Lines]",
      "99:         test_dag: DAG = dagbag.dags[TEST_DAG_ID]",
      "100:         dag_docstring = self._get_dag_file_docstring(test_dag.fileloc)",
      "102:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(test_dag.fileloc)}\"",
      "",
      "---------------",
      "--- Hunk 4 ---",
      "[Context before]",
      "95:     def test_should_respond_200_json(self, url_safe_serializer):",
      "96:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
      "97:         dagbag.sync_to_db()",
      "102:         response = self.client.get(",
      "103:             url, headers={\"Accept\": \"application/json\"}, environ_overrides={\"REMOTE_USER\": \"test\"}",
      "104:         )",
      "",
      "[Removed Lines]",
      "98:         first_dag: DAG = next(iter(dagbag.dags.values()))",
      "99:         dag_docstring = self._get_dag_file_docstring(first_dag.fileloc)",
      "101:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(first_dag.fileloc)}\"",
      "",
      "[Added Lines]",
      "114:         test_dag: DAG = dagbag.dags[TEST_DAG_ID]",
      "115:         dag_docstring = self._get_dag_file_docstring(test_dag.fileloc)",
      "117:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(test_dag.fileloc)}\"",
      "",
      "---------------",
      "--- Hunk 5 ---",
      "[Context before]",
      "110:     def test_should_respond_406(self, url_safe_serializer):",
      "111:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
      "112:         dagbag.sync_to_db()",
      "116:         response = self.client.get(",
      "117:             url, headers={\"Accept\": \"image/webp\"}, environ_overrides={\"REMOTE_USER\": \"test\"}",
      "118:         )",
      "",
      "[Removed Lines]",
      "113:         first_dag: DAG = next(iter(dagbag.dags.values()))",
      "115:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(first_dag.fileloc)}\"",
      "",
      "[Added Lines]",
      "129:         test_dag: DAG = dagbag.dags[TEST_DAG_ID]",
      "131:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(test_dag.fileloc)}\"",
      "",
      "---------------",
      "--- Hunk 6 ---",
      "[Context before]",
      "151:             environ_overrides={\"REMOTE_USER\": \"test_no_permissions\"},",
      "152:         )",
      "153:         assert response.status_code == 403",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "171:     def test_should_respond_403_not_readable(self, url_safe_serializer):",
      "172:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
      "173:         dagbag.sync_to_db()",
      "174:         dag: DAG = dagbag.dags[NOT_READABLE_DAG_ID]",
      "176:         response = self.client.get(",
      "177:             f\"/api/v1/dagSources/{url_safe_serializer.dumps(dag.fileloc)}\",",
      "178:             headers={\"Accept\": \"text/plain\"},",
      "179:             environ_overrides={\"REMOTE_USER\": \"test\"},",
      "180:         )",
      "181:         read_dag = self.client.get(",
      "182:             f\"/api/v1/dags/{NOT_READABLE_DAG_ID}\",",
      "183:             environ_overrides={\"REMOTE_USER\": \"test\"},",
      "184:         )",
      "185:         assert response.status_code == 403",
      "186:         assert read_dag.status_code == 403",
      "188:     def test_should_respond_403_some_dags_not_readable_in_the_file(self, url_safe_serializer):",
      "189:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
      "190:         dagbag.sync_to_db()",
      "191:         dag: DAG = dagbag.dags[TEST_MULTIPLE_DAGS_ID]",
      "193:         response = self.client.get(",
      "194:             f\"/api/v1/dagSources/{url_safe_serializer.dumps(dag.fileloc)}\",",
      "195:             headers={\"Accept\": \"text/plain\"},",
      "196:             environ_overrides={\"REMOTE_USER\": \"test\"},",
      "197:         )",
      "199:         read_dag = self.client.get(",
      "200:             f\"/api/v1/dags/{TEST_MULTIPLE_DAGS_ID}\",",
      "201:             environ_overrides={\"REMOTE_USER\": \"test\"},",
      "202:         )",
      "203:         assert response.status_code == 403",
      "204:         assert read_dag.status_code == 200",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "4c613f827aa992ae5aff2d074eff16338ff5de54",
      "candidate_info": {
        "commit_hash": "4c613f827aa992ae5aff2d074eff16338ff5de54",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/4c613f827aa992ae5aff2d074eff16338ff5de54",
        "files": [
          "airflow/hooks/filesystem.py",
          "airflow/hooks/package_index.py",
          "airflow/hooks/subprocess.py"
        ],
        "message": "Provide the logger_name param to base hook in order to override the logger name (#36674)\n\n(cherry picked from commit 8e8c080050c374b47fd0e6ffb8cb0a27adbca055)",
        "before_after_code_files": [
          "airflow/hooks/filesystem.py||airflow/hooks/filesystem.py",
          "airflow/hooks/package_index.py||airflow/hooks/package_index.py",
          "airflow/hooks/subprocess.py||airflow/hooks/subprocess.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/hooks/filesystem.py||airflow/hooks/filesystem.py": [
          "File: airflow/hooks/filesystem.py -> airflow/hooks/filesystem.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "59:             \"placeholders\": {},",
          "60:         }",
          "64:         conn = self.get_connection(fs_conn_id)",
          "65:         self.basepath = conn.extra_dejson.get(\"path\", \"\")",
          "66:         self.conn = conn",
          "",
          "[Removed Lines]",
          "62:     def __init__(self, fs_conn_id: str = default_conn_name):",
          "63:         super().__init__()",
          "",
          "[Added Lines]",
          "62:     def __init__(self, fs_conn_id: str = default_conn_name, **kwargs):",
          "63:         super().__init__(**kwargs)",
          "",
          "---------------"
        ],
        "airflow/hooks/package_index.py||airflow/hooks/package_index.py": [
          "File: airflow/hooks/package_index.py -> airflow/hooks/package_index.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "33:     conn_type = \"package_index\"",
          "34:     hook_name = \"Package Index (Python)\"",
          "38:         self.pi_conn_id = pi_conn_id",
          "39:         self.conn = None",
          "",
          "[Removed Lines]",
          "36:     def __init__(self, pi_conn_id: str = default_conn_name) -> None:",
          "37:         super().__init__()",
          "",
          "[Added Lines]",
          "36:     def __init__(self, pi_conn_id: str = default_conn_name, **kwargs) -> None:",
          "37:         super().__init__(**kwargs)",
          "",
          "---------------"
        ],
        "airflow/hooks/subprocess.py||airflow/hooks/subprocess.py": [
          "File: airflow/hooks/subprocess.py -> airflow/hooks/subprocess.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "31: class SubprocessHook(BaseHook):",
          "32:     \"\"\"Hook for running processes with the ``subprocess`` module.\"\"\"",
          "35:         self.sub_process: Popen[bytes] | None = None",
          "38:     def run_command(",
          "39:         self,",
          "",
          "[Removed Lines]",
          "34:     def __init__(self) -> None:",
          "36:         super().__init__()",
          "",
          "[Added Lines]",
          "34:     def __init__(self, **kwargs) -> None:",
          "36:         super().__init__(**kwargs)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "4452e301ebbfd2995c3751c2b0d4c8e2d3455756",
      "candidate_info": {
        "commit_hash": "4452e301ebbfd2995c3751c2b0d4c8e2d3455756",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/4452e301ebbfd2995c3751c2b0d4c8e2d3455756",
        "files": [
          "airflow/decorators/base.py",
          "docs/apache-airflow/tutorial/taskflow.rst",
          "tests/decorators/test_python.py"
        ],
        "message": "Make sure `multiple_outputs` is inferred correctly even when using `TypedDict` (#36652)\n\n* Use `issubclass()` to check if return type is a dictionary\n\n* Compare type to `typing.Mapping` instead of `typing.Dict`\n\n* Add documentation\n\n(cherry picked from commit e11b91c8e01b38023f209983a81aee23439a34a3)",
        "before_after_code_files": [
          "airflow/decorators/base.py||airflow/decorators/base.py",
          "tests/decorators/test_python.py||tests/decorators/test_python.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/decorators/base.py||airflow/decorators/base.py": [
          "File: airflow/decorators/base.py -> airflow/decorators/base.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "27:     Callable,",
          "28:     ClassVar,",
          "29:     Collection,",
          "31:     Generic,",
          "32:     Iterator,",
          "33:     Mapping,",
          "",
          "[Removed Lines]",
          "30:     Dict,",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "351:         except TypeError:  # Can't evaluate return type.",
          "352:             return False",
          "353:         ttype = getattr(return_type, \"__origin__\", return_type)",
          "356:     def __attrs_post_init__(self):",
          "357:         if \"self\" in self.function_signature.parameters:",
          "",
          "[Removed Lines]",
          "354:         return ttype is dict or ttype is Dict",
          "",
          "[Added Lines]",
          "353:         return issubclass(ttype, Mapping)",
          "",
          "---------------"
        ],
        "tests/decorators/test_python.py||tests/decorators/test_python.py": [
          "File: tests/decorators/test_python.py -> tests/decorators/test_python.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "98:         assert identity_dict_with_decorator_call(5, 5).operator.multiple_outputs is True",
          "100:     def test_infer_multiple_outputs_forward_annotation(self):",
          "101:         if TYPE_CHECKING:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "100:     @pytest.mark.skipif(sys.version_info < (3, 8), reason=\"PEP 589 is implemented in Python 3.8\")",
          "101:     def test_infer_multiple_outputs_typed_dict(self):",
          "102:         from typing import TypedDict",
          "104:         class TypeDictClass(TypedDict):",
          "105:             pass",
          "107:         @task_decorator",
          "108:         def t1() -> TypeDictClass:",
          "109:             return {}",
          "111:         assert t1().operator.multiple_outputs is True",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "1718b2bc7c19b260071ce8988da3e86806b56792",
      "candidate_info": {
        "commit_hash": "1718b2bc7c19b260071ce8988da3e86806b56792",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/1718b2bc7c19b260071ce8988da3e86806b56792",
        "files": [
          "airflow/www/static/js/dag/details/index.tsx"
        ],
        "message": "Fix details tab not showing when using dynamic task mapping (#36522)\n\n* Fix details tab not showing when using dynamic task mapping\n\n* Remove unnecessary variable\n\n(cherry picked from commit 18b701358183762b0b3609627c198c263680eb6e)",
        "before_after_code_files": [
          "airflow/www/static/js/dag/details/index.tsx||airflow/www/static/js/dag/details/index.tsx"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/www/static/js/dag/details/index.tsx||airflow/www/static/js/dag/details/index.tsx": [
          "File: airflow/www/static/js/dag/details/index.tsx -> airflow/www/static/js/dag/details/index.tsx",
          "--- Hunk 1 ---",
          "[Context before]",
          "301:           <TabPanel height=\"100%\">",
          "302:             {isDag && <DagContent />}",
          "303:             {isDagRun && <DagRunContent runId={runId} />}",
          "305:               <>",
          "306:                 <BackToTaskSummary",
          "307:                   isMapIndexDefined={mapIndex !== undefined && mapIndex > -1}",
          "",
          "[Removed Lines]",
          "304:             {isTaskInstance && (",
          "",
          "[Added Lines]",
          "304:             {!!runId && !!taskId && (",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "eefc0f88d0454871fb22a1f6deff584dd1f9b83a",
      "candidate_info": {
        "commit_hash": "eefc0f88d0454871fb22a1f6deff584dd1f9b83a",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/eefc0f88d0454871fb22a1f6deff584dd1f9b83a",
        "files": [
          "Dockerfile.ci",
          "airflow/providers/apache/spark/provider.yaml",
          "generated/provider_dependencies.json"
        ],
        "message": "Bump min version for grpcio-status in spark provider (#36662)\n\nPreviously we limited grpcio minimum version to stop backtracking\nof `pip` from happening and we could not do it in the limits of\nspark provider, becaue some google dependencies used it and\nconflicted with it. This problem is now gone as we have newer\nversions of google dependencies and we can not only safely move\nit to spark provider but also bump it slightly higher to limit\nthe amount of backtracking we need to do.\n\nExtracted from #36537\n\n(cherry picked from commit ded01a5aba337882fb19e03c24d7736c7154fdd8)",
        "before_after_code_files": [
          "Dockerfile.ci||Dockerfile.ci"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "Dockerfile.ci||Dockerfile.ci": [
          "File: Dockerfile.ci -> Dockerfile.ci",
          "--- Hunk 1 ---",
          "[Context before]",
          "1116: # force them on the main Airflow package. Currently we need no extra limits as PIP 23.1+ has much better",
          "1117: # dependency resolution and we do not need to limit the versions of the dependencies",
          "1118: #",
          "1122: # Aiobotocore is limited for eager upgrade because it either causes a long backtracking or",
          "1123: # conflict when we do not limit it. It seems that `pip` has a hard time figuring the right",
          "1124: # combination of dependencies for aiobotocore, botocore, boto3 and s3fs together",
          "1125: #",
          "1127: ARG UPGRADE_TO_NEWER_DEPENDENCIES=\"false\"",
          "1128: ARG VERSION_SUFFIX_FOR_PYPI=\"\"",
          "",
          "[Removed Lines]",
          "1119: # Without grpcio-status limit, pip gets into very long backtracking",
          "1120: # We should attempt to remove it in the future",
          "1121: #",
          "1126: ARG EAGER_UPGRADE_ADDITIONAL_REQUIREMENTS=\"grpcio-status>=1.55.0 aiobotocore>=2.5.4\"",
          "",
          "[Added Lines]",
          "1123: ARG EAGER_UPGRADE_ADDITIONAL_REQUIREMENTS=\"aiobotocore>=2.5.4\"",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "986f6d8ea2b6cc47613bbcbb226eeb2c0ba48e7b",
      "candidate_info": {
        "commit_hash": "986f6d8ea2b6cc47613bbcbb226eeb2c0ba48e7b",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/986f6d8ea2b6cc47613bbcbb226eeb2c0ba48e7b",
        "files": [
          "tests/models/test_mappedoperator.py"
        ],
        "message": "Add few tests on the mapped task group. (#36149)\n\n(cherry picked from commit bed2789c246c71656bad1cf45374ebef4d28fb2d)",
        "before_after_code_files": [
          "tests/models/test_mappedoperator.py||tests/models/test_mappedoperator.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "tests/models/test_mappedoperator.py||tests/models/test_mappedoperator.py": [
          "File: tests/models/test_mappedoperator.py -> tests/models/test_mappedoperator.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1568:             \"tg_2.my_work\": \"skipped\",",
          "1569:         }",
          "1570:         assert states == expected",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1572:     def test_skip_one_mapped_task_from_task_group_with_generator(self, dag_maker):",
          "1573:         with dag_maker() as dag:",
          "1575:             @task",
          "1576:             def make_list():",
          "1577:                 return [1, 2, 3]",
          "1579:             @task",
          "1580:             def double(n):",
          "1581:                 if n == 2:",
          "1582:                     raise AirflowSkipException()",
          "1583:                 return n * 2",
          "1585:             @task",
          "1586:             def last(n):",
          "1587:                 ...",
          "1589:             @task_group",
          "1590:             def group(n: int) -> None:",
          "1591:                 last(double(n))",
          "1593:             group.expand(n=make_list())",
          "1595:         dr = dag.test()",
          "1596:         states = self.get_states(dr)",
          "1597:         expected = {",
          "1598:             \"group.double\": {0: \"success\", 1: \"skipped\", 2: \"success\"},",
          "1599:             \"group.last\": {0: \"success\", 1: \"skipped\", 2: \"success\"},",
          "1600:             \"make_list\": \"success\",",
          "1601:         }",
          "1602:         assert states == expected",
          "1604:     def test_skip_one_mapped_task_from_task_group(self, dag_maker):",
          "1605:         with dag_maker() as dag:",
          "1607:             @task",
          "1608:             def double(n):",
          "1609:                 if n == 2:",
          "1610:                     raise AirflowSkipException()",
          "1611:                 return n * 2",
          "1613:             @task",
          "1614:             def last(n):",
          "1615:                 ...",
          "1617:             @task_group",
          "1618:             def group(n: int) -> None:",
          "1619:                 last(double(n))",
          "1621:             group.expand(n=[1, 2, 3])",
          "1623:         dr = dag.test()",
          "1624:         states = self.get_states(dr)",
          "1625:         expected = {",
          "1626:             \"group.double\": {0: \"success\", 1: \"skipped\", 2: \"success\"},",
          "1627:             \"group.last\": {0: \"success\", 1: \"skipped\", 2: \"success\"},",
          "1628:         }",
          "1629:         assert states == expected",
          "",
          "---------------"
        ]
      }
    }
  ]
}