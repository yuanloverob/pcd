{
  "cve_id": "CVE-2023-42780",
  "cve_desc": "Apache Airflow, versions prior to 2.7.2, contains a security vulnerability that allows authenticated users of Airflow to list warnings for all DAGs, even if the user had no permission to see those DAGs. It would reveal the dag_ids and the stack-traces of import errors for those DAGs with import errors.\nUsers of Apache Airflow are advised to upgrade to version 2.7.2 or newer to mitigate the risk associated with this vulnerability.\n\n",
  "repo": "apache/airflow",
  "patch_hash": "cf4eb3fb9b5cf4a8369b890e39523d4c05eed161",
  "patch_info": {
    "commit_hash": "cf4eb3fb9b5cf4a8369b890e39523d4c05eed161",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/cf4eb3fb9b5cf4a8369b890e39523d4c05eed161",
    "files": [
      "airflow/api_connexion/endpoints/dag_warning_endpoint.py",
      "tests/api_connexion/endpoints/test_dag_warning_endpoint.py"
    ],
    "message": "Fix dag warning endpoint permissions (#34355)\n\n* Fix dag warning endpoint permissions\n\n* update the query to have an accurate result for total entries and pagination\n\n* add unit tests\n\n* Update test_dag_warning_endpoint.py\n\nCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>\n\n---------\n\nCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>\n(cherry picked from commit 3570bbfbea69e2965f91b9964ce28bc268c68129)",
    "before_after_code_files": [
      "airflow/api_connexion/endpoints/dag_warning_endpoint.py||airflow/api_connexion/endpoints/dag_warning_endpoint.py",
      "tests/api_connexion/endpoints/test_dag_warning_endpoint.py||tests/api_connexion/endpoints/test_dag_warning_endpoint.py"
    ]
  },
  "patch_diff": {
    "airflow/api_connexion/endpoints/dag_warning_endpoint.py||airflow/api_connexion/endpoints/dag_warning_endpoint.py": [
      "File: airflow/api_connexion/endpoints/dag_warning_endpoint.py -> airflow/api_connexion/endpoints/dag_warning_endpoint.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "16: # under the License.",
      "17: from __future__ import annotations",
      "19: from sqlalchemy import select",
      "20: from sqlalchemy.orm import Session",
      "22: from airflow.api_connexion import security",
      "23: from airflow.api_connexion.parameters import apply_sorting, check_limit, format_parameters",
      "24: from airflow.api_connexion.schemas.dag_warning_schema import (",
      "25:     DagWarningCollection,",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "19: from flask import g",
      "24: from airflow.api_connexion.exceptions import PermissionDenied",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "28: from airflow.api_connexion.types import APIResponse",
      "29: from airflow.models.dagwarning import DagWarning as DagWarningModel",
      "30: from airflow.security import permissions",
      "31: from airflow.utils.db import get_query_count",
      "32: from airflow.utils.session import NEW_SESSION, provide_session",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "33: from airflow.utils.airflow_flask_app import get_airflow_app",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "52:     allowed_filter_attrs = [\"dag_id\", \"warning_type\", \"message\", \"timestamp\"]",
      "53:     query = select(DagWarningModel)",
      "54:     if dag_id:",
      "55:         query = query.where(DagWarningModel.dag_id == dag_id)",
      "56:     if warning_type:",
      "57:         query = query.where(DagWarningModel.warning_type == warning_type)",
      "58:     total_entries = get_query_count(query, session=session)",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "58:         if not get_airflow_app().appbuilder.sm.can_read_dag(dag_id, g.user):",
      "59:             raise PermissionDenied(detail=f\"User not allowed to access this DAG: {dag_id}\")",
      "61:     else:",
      "62:         readable_dags = get_airflow_app().appbuilder.sm.get_accessible_dag_ids(g.user)",
      "63:         query = query.where(DagWarningModel.dag_id.in_(readable_dags))",
      "",
      "---------------"
    ],
    "tests/api_connexion/endpoints/test_dag_warning_endpoint.py||tests/api_connexion/endpoints/test_dag_warning_endpoint.py": [
      "File: tests/api_connexion/endpoints/test_dag_warning_endpoint.py -> tests/api_connexion/endpoints/test_dag_warning_endpoint.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "35:         app,  # type:ignore",
      "36:         username=\"test\",",
      "37:         role_name=\"Test\",",
      "39:     )",
      "40:     create_user(app, username=\"test_no_permissions\", role_name=\"TestNoPermissions\")  # type: ignore",
      "42:     yield minimal_app_for_api",
      "44:     delete_user(app, username=\"test\")  # type: ignore",
      "45:     delete_user(app, username=\"test_no_permissions\")  # type: ignore",
      "48: class TestBaseDagWarning:",
      "",
      "[Removed Lines]",
      "38:         permissions=[(permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG_WARNING)],  # type: ignore",
      "",
      "[Added Lines]",
      "38:         permissions=[",
      "39:             (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG_WARNING),",
      "40:             (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG),",
      "41:         ],  # type: ignore",
      "44:     create_user(",
      "45:         app,  # type:ignore",
      "46:         username=\"test_with_dag2_read\",",
      "47:         role_name=\"TestWithDag2Read\",",
      "48:         permissions=[",
      "49:             (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG_WARNING),",
      "50:             (permissions.ACTION_CAN_READ, f\"{permissions.RESOURCE_DAG_PREFIX}dag2\"),",
      "51:         ],  # type: ignore",
      "52:     )",
      "58:     delete_user(app, username=\"test_with_dag2_read\")  # type: ignore",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "147:             \"/api/v1/dagWarnings\", environ_overrides={\"REMOTE_USER\": \"test_no_permissions\"}",
      "148:         )",
      "149:         assert response.status_code == 403",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "164:     def test_should_raise_403_forbidden_when_user_has_no_dag_read_permission(self):",
      "165:         response = self.client.get(",
      "166:             \"/api/v1/dagWarnings\",",
      "167:             environ_overrides={\"REMOTE_USER\": \"test_with_dag2_read\"},",
      "168:             query_string={\"dag_id\": \"dag1\"},",
      "169:         )",
      "170:         assert response.status_code == 403",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "6e739d63a85cc202858c06f684c986967262f6e9",
      "candidate_info": {
        "commit_hash": "6e739d63a85cc202858c06f684c986967262f6e9",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/6e739d63a85cc202858c06f684c986967262f6e9",
        "files": [
          "airflow/serialization/serializers/datetime.py",
          "tests/serialization/serializers/test_serializers.py"
        ],
        "message": "Fix non deterministic datetime deserialization (#34492)\n\ntzname() does not return full timezones and returned short hand notations are not deterministic. This changes the serialization to be deterministic and adds some logic to deal with serialized short-hand US Timezones and CEST.\n\n---------\n\nCo-authored-by: bolkedebruin <bolkedebruin@users.noreply.github.com>\n(cherry picked from commit a3c06c02e31cc77b2c19554892b72ed91b8387de)",
        "before_after_code_files": [
          "airflow/serialization/serializers/datetime.py||airflow/serialization/serializers/datetime.py",
          "tests/serialization/serializers/test_serializers.py||tests/serialization/serializers/test_serializers.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/serialization/serializers/datetime.py||airflow/serialization/serializers/datetime.py": [
          "File: airflow/serialization/serializers/datetime.py -> airflow/serialization/serializers/datetime.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: from typing import TYPE_CHECKING",
          "22: from airflow.utils.module_loading import qualname",
          "23: from airflow.utils.timezone import convert_to_utc, is_naive",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "22: from airflow.serialization.serializers.timezone import (",
          "23:     deserialize as deserialize_timezone,",
          "24:     serialize as serialize_timezone,",
          "25: )",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "28:     from airflow.serialization.serde import U",
          "32: serializers = [\"datetime.date\", \"datetime.datetime\", \"datetime.timedelta\", \"pendulum.datetime.DateTime\"]",
          "33: deserializers = serializers",
          "",
          "[Removed Lines]",
          "30: __version__ = 1",
          "",
          "[Added Lines]",
          "34: __version__ = 2",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "44:         if is_naive(o):",
          "45:             o = convert_to_utc(o)",
          "49:         return {TIMESTAMP: o.timestamp(), TIMEZONE: tz}, qn, __version__, True",
          "",
          "[Removed Lines]",
          "47:         tz = o.tzname()",
          "",
          "[Added Lines]",
          "51:         tz = serialize_timezone(o.tzinfo)",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "61:     import datetime",
          "63:     from pendulum import DateTime",
          "66:     if classname == qualname(datetime.datetime) and isinstance(data, dict):",
          "69:     if classname == qualname(DateTime) and isinstance(data, dict):",
          "72:     if classname == qualname(datetime.timedelta) and isinstance(data, (str, float)):",
          "73:         return datetime.timedelta(seconds=float(data))",
          "",
          "[Removed Lines]",
          "64:     from pendulum.tz import timezone",
          "67:         return datetime.datetime.fromtimestamp(float(data[TIMESTAMP]), tz=timezone(data[TIMEZONE]))",
          "70:         return DateTime.fromtimestamp(float(data[TIMESTAMP]), tz=timezone(data[TIMEZONE]))",
          "",
          "[Added Lines]",
          "68:     from pendulum.tz import fixed_timezone, timezone",
          "70:     tz: datetime.tzinfo | None = None",
          "71:     if isinstance(data, dict) and TIMEZONE in data:",
          "72:         if version == 1:",
          "73:             # try to deserialize unsupported timezones",
          "74:             timezone_mapping = {",
          "75:                 \"EDT\": fixed_timezone(-4 * 3600),",
          "76:                 \"CDT\": fixed_timezone(-5 * 3600),",
          "77:                 \"MDT\": fixed_timezone(-6 * 3600),",
          "78:                 \"PDT\": fixed_timezone(-7 * 3600),",
          "79:                 \"CEST\": timezone(\"CET\"),",
          "80:             }",
          "81:             if data[TIMEZONE] in timezone_mapping:",
          "82:                 tz = timezone_mapping[data[TIMEZONE]]",
          "83:             else:",
          "84:                 tz = timezone(data[TIMEZONE])",
          "85:         else:",
          "86:             tz = deserialize_timezone(data[TIMEZONE][1], data[TIMEZONE][2], data[TIMEZONE][0])",
          "89:         return datetime.datetime.fromtimestamp(float(data[TIMESTAMP]), tz=tz)",
          "92:         return DateTime.fromtimestamp(float(data[TIMESTAMP]), tz=tz)",
          "",
          "---------------"
        ],
        "tests/serialization/serializers/test_serializers.py||tests/serialization/serializers/test_serializers.py": [
          "File: tests/serialization/serializers/test_serializers.py -> tests/serialization/serializers/test_serializers.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "52:         d = deserialize(s)",
          "53:         assert i == d",
          "55:     @pytest.mark.parametrize(",
          "56:         \"expr, expected\",",
          "57:         [(\"1\", \"1\"), (\"52e4\", \"520000\"), (\"2e0\", \"2\"), (\"12e-2\", \"0.12\"), (\"12.34\", \"12.34\")],",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "54:         i = datetime.datetime(",
          "55:             2022, 7, 10, 22, 10, 43, microsecond=0, tzinfo=pendulum.timezone(\"America/New_York\")",
          "56:         )",
          "57:         s = serialize(i)",
          "58:         d = deserialize(s)",
          "59:         assert i.timestamp() == d.timestamp()",
          "61:         i = DateTime(2022, 7, 10, tzinfo=pendulum.timezone(\"America/New_York\"))",
          "62:         s = serialize(i)",
          "63:         d = deserialize(s)",
          "64:         assert i.timestamp() == d.timestamp()",
          "66:     def test_deserialize_datetime_v1(self):",
          "68:         s = {",
          "69:             \"__classname__\": \"pendulum.datetime.DateTime\",",
          "70:             \"__version__\": 1,",
          "71:             \"__data__\": {\"timestamp\": 1657505443.0, \"tz\": \"UTC\"},",
          "72:         }",
          "73:         d = deserialize(s)",
          "74:         assert d.timestamp() == 1657505443.0",
          "75:         assert d.tzinfo.name == \"UTC\"",
          "77:         s[\"__data__\"][\"tz\"] = \"Europe/Paris\"",
          "78:         d = deserialize(s)",
          "79:         assert d.timestamp() == 1657505443.0",
          "80:         assert d.tzinfo.name == \"Europe/Paris\"",
          "82:         s[\"__data__\"][\"tz\"] = \"America/New_York\"",
          "83:         d = deserialize(s)",
          "84:         assert d.timestamp() == 1657505443.0",
          "85:         assert d.tzinfo.name == \"America/New_York\"",
          "87:         s[\"__data__\"][\"tz\"] = \"EDT\"",
          "88:         d = deserialize(s)",
          "89:         assert d.timestamp() == 1657505443.0",
          "90:         assert d.tzinfo.name == \"-04:00\"",
          "91:         # assert that it's serializable with the new format",
          "92:         assert deserialize(serialize(d)) == d",
          "94:         s[\"__data__\"][\"tz\"] = \"CDT\"",
          "95:         d = deserialize(s)",
          "96:         assert d.timestamp() == 1657505443.0",
          "97:         assert d.tzinfo.name == \"-05:00\"",
          "98:         # assert that it's serializable with the new format",
          "99:         assert deserialize(serialize(d)) == d",
          "101:         s[\"__data__\"][\"tz\"] = \"MDT\"",
          "102:         d = deserialize(s)",
          "103:         assert d.timestamp() == 1657505443.0",
          "104:         assert d.tzinfo.name == \"-06:00\"",
          "105:         # assert that it's serializable with the new format",
          "106:         assert deserialize(serialize(d)) == d",
          "108:         s[\"__data__\"][\"tz\"] = \"PDT\"",
          "109:         d = deserialize(s)",
          "110:         assert d.timestamp() == 1657505443.0",
          "111:         assert d.tzinfo.name == \"-07:00\"",
          "112:         # assert that it's serializable with the new format",
          "113:         assert deserialize(serialize(d)) == d",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "ee06e416cbfee1507c09d1fa1e89ce272af9e988",
      "candidate_info": {
        "commit_hash": "ee06e416cbfee1507c09d1fa1e89ce272af9e988",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/ee06e416cbfee1507c09d1fa1e89ce272af9e988",
        "files": [
          "airflow/www/security.py"
        ],
        "message": "Set loglevel=DEBUG in 'Not syncing DAG-level permissions' (#34268)\n\n(cherry picked from commit 8035aee8da3c4bb7b9c01cfdc7236ca01d658bae)",
        "before_after_code_files": [
          "airflow/www/security.py||airflow/www/security.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/www/security.py||airflow/www/security.py": [
          "File: airflow/www/security.py -> airflow/www/security.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "704:             self.create_permission(dag_action_name, dag_resource_name)",
          "706:         if access_control is not None:",
          "708:             self._sync_dag_view_permissions(dag_resource_name, access_control)",
          "709:         else:",
          "711:                 \"Not syncing DAG-level permissions for DAG '%s' as access control is unset.\",",
          "712:                 dag_resource_name,",
          "713:             )",
          "",
          "[Removed Lines]",
          "707:             self.log.info(\"Syncing DAG-level permissions for DAG '%s'\", dag_resource_name)",
          "710:             self.log.info(",
          "",
          "[Added Lines]",
          "707:             self.log.debug(\"Syncing DAG-level permissions for DAG '%s'\", dag_resource_name)",
          "710:             self.log.debug(",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "672871b4d77037a1fa16c199dec2d424d19f0c0c",
      "candidate_info": {
        "commit_hash": "672871b4d77037a1fa16c199dec2d424d19f0c0c",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/672871b4d77037a1fa16c199dec2d424d19f0c0c",
        "files": [
          "airflow/sensors/bash.py",
          "airflow/utils/db.py"
        ],
        "message": "Use a single  statement with multiple contexts instead of nested  statements in core (#33769)\n\n(cherry picked from commit 60e6847c181959d95789266cc2712cdacaf18cf9)",
        "before_after_code_files": [
          "airflow/sensors/bash.py||airflow/sensors/bash.py",
          "airflow/utils/db.py||airflow/utils/db.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/sensors/bash.py||airflow/sensors/bash.py": [
          "File: airflow/sensors/bash.py -> airflow/sensors/bash.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "68:         \"\"\"Execute the bash command in a temporary directory.\"\"\"",
          "69:         bash_command = self.bash_command",
          "70:         self.log.info(\"Tmp dir root location: %s\", gettempdir())",
          "111:                         return False",
          "",
          "[Removed Lines]",
          "71:         with TemporaryDirectory(prefix=\"airflowtmp\") as tmp_dir:",
          "72:             with NamedTemporaryFile(dir=tmp_dir, prefix=self.task_id) as f:",
          "73:                 f.write(bytes(bash_command, \"utf_8\"))",
          "74:                 f.flush()",
          "75:                 fname = f.name",
          "76:                 script_location = tmp_dir + \"/\" + fname",
          "77:                 self.log.info(\"Temporary script location: %s\", script_location)",
          "78:                 self.log.info(\"Running command: %s\", bash_command)",
          "80:                 with Popen(",
          "81:                     [\"bash\", fname],",
          "82:                     stdout=PIPE,",
          "83:                     stderr=STDOUT,",
          "84:                     close_fds=True,",
          "85:                     cwd=tmp_dir,",
          "86:                     env=self.env,",
          "87:                     preexec_fn=os.setsid,",
          "88:                 ) as resp:",
          "89:                     if resp.stdout:",
          "90:                         self.log.info(\"Output:\")",
          "91:                         for line in iter(resp.stdout.readline, b\"\"):",
          "92:                             self.log.info(line.decode(self.output_encoding).strip())",
          "93:                     resp.wait()",
          "94:                     self.log.info(\"Command exited with return code %s\", resp.returncode)",
          "96:                     # zero code means success, the sensor can go green",
          "97:                     if resp.returncode == 0:",
          "98:                         return True",
          "100:                     # we have a retry exit code, sensor retries if return code matches, otherwise error",
          "101:                     elif self.retry_exit_code is not None:",
          "102:                         if resp.returncode == self.retry_exit_code:",
          "103:                             self.log.info(\"Return code matches retry code, will retry later\")",
          "104:                             return False",
          "105:                         else:",
          "106:                             raise AirflowFailException(f\"Command exited with return code {resp.returncode}\")",
          "108:                     # backwards compatibility: sensor retries no matter the error code",
          "109:                     else:",
          "110:                         self.log.info(\"Non-zero return code and no retry code set, will retry later\")",
          "",
          "[Added Lines]",
          "71:         with TemporaryDirectory(prefix=\"airflowtmp\") as tmp_dir, NamedTemporaryFile(",
          "72:             dir=tmp_dir, prefix=self.task_id",
          "73:         ) as f:",
          "74:             f.write(bytes(bash_command, \"utf_8\"))",
          "75:             f.flush()",
          "76:             fname = f.name",
          "77:             script_location = tmp_dir + \"/\" + fname",
          "78:             self.log.info(\"Temporary script location: %s\", script_location)",
          "79:             self.log.info(\"Running command: %s\", bash_command)",
          "81:             with Popen(",
          "82:                 [\"bash\", fname],",
          "83:                 stdout=PIPE,",
          "84:                 stderr=STDOUT,",
          "85:                 close_fds=True,",
          "86:                 cwd=tmp_dir,",
          "87:                 env=self.env,",
          "88:                 preexec_fn=os.setsid,",
          "89:             ) as resp:",
          "90:                 if resp.stdout:",
          "91:                     self.log.info(\"Output:\")",
          "92:                     for line in iter(resp.stdout.readline, b\"\"):",
          "93:                         self.log.info(line.decode(self.output_encoding).strip())",
          "94:                 resp.wait()",
          "95:                 self.log.info(\"Command exited with return code %s\", resp.returncode)",
          "97:                 # zero code means success, the sensor can go green",
          "98:                 if resp.returncode == 0:",
          "99:                     return True",
          "101:                 # we have a retry exit code, sensor retries if return code matches, otherwise error",
          "102:                 elif self.retry_exit_code is not None:",
          "103:                     if resp.returncode == self.retry_exit_code:",
          "104:                         self.log.info(\"Return code matches retry code, will retry later\")",
          "106:                     else:",
          "107:                         raise AirflowFailException(f\"Command exited with return code {resp.returncode}\")",
          "109:                 # backwards compatibility: sensor retries no matter the error code",
          "110:                 else:",
          "111:                     self.log.info(\"Non-zero return code and no retry code set, will retry later\")",
          "112:                     return False",
          "",
          "---------------"
        ],
        "airflow/utils/db.py||airflow/utils/db.py": [
          "File: airflow/utils/db.py -> airflow/utils/db.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1652:     connection = settings.engine.connect()",
          "1659:     if not skip_init:",
          "1660:         initdb(session=session)",
          "",
          "[Removed Lines]",
          "1654:     with create_global_lock(session=session, lock=DBLocks.MIGRATIONS):",
          "1655:         with connection.begin():",
          "1656:             drop_airflow_models(connection)",
          "1657:             drop_airflow_moved_tables(connection)",
          "",
          "[Added Lines]",
          "1654:     with create_global_lock(session=session, lock=DBLocks.MIGRATIONS), connection.begin():",
          "1655:         drop_airflow_models(connection)",
          "1656:         drop_airflow_moved_tables(connection)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "08381bb9ee5d7e088d30f44df3d996c971abe919",
      "candidate_info": {
        "commit_hash": "08381bb9ee5d7e088d30f44df3d996c971abe919",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/08381bb9ee5d7e088d30f44df3d996c971abe919",
        "files": [
          "airflow/models/dagrun.py",
          "airflow/models/taskinstance.py"
        ],
        "message": "Fix foreign key warning re ab_user.id (#34656)\n\nIntroduced in #34120.\n\n(cherry picked from commit 1fdc2311250fbae47749822b192a99066600f8ad)",
        "before_after_code_files": [
          "airflow/models/dagrun.py||airflow/models/dagrun.py",
          "airflow/models/taskinstance.py||airflow/models/taskinstance.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/models/dagrun.py||airflow/models/dagrun.py": [
          "File: airflow/models/dagrun.py -> airflow/models/dagrun.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1409:     user_id = Column(",
          "1410:         Integer,",
          "1411:         nullable=True,",
          "1413:     )",
          "1414:     dag_run_id = Column(Integer, primary_key=True, nullable=False)",
          "1415:     content = Column(String(1000).with_variant(Text(1000), \"mysql\"))",
          "",
          "[Removed Lines]",
          "1412:         foreign_key=ForeignKey(\"ab_user.id\", name=\"dag_run_note_user_fkey\"),",
          "",
          "[Added Lines]",
          "1411:         ForeignKey(\"ab_user.id\", name=\"dag_run_note_user_fkey\"),",
          "",
          "---------------"
        ],
        "airflow/models/taskinstance.py||airflow/models/taskinstance.py": [
          "File: airflow/models/taskinstance.py -> airflow/models/taskinstance.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "3017:     __tablename__ = \"task_instance_note\"",
          "3022:     task_id = Column(StringID(), primary_key=True, nullable=False)",
          "3023:     dag_id = Column(StringID(), primary_key=True, nullable=False)",
          "3024:     run_id = Column(StringID(), primary_key=True, nullable=False)",
          "",
          "[Removed Lines]",
          "3019:     user_id = Column(",
          "3020:         Integer, nullable=True, foreign_key=ForeignKey(\"ab_user.id\", name=\"task_instance_note_user_fkey\")",
          "3021:     )",
          "",
          "[Added Lines]",
          "3019:     user_id = Column(Integer, ForeignKey(\"ab_user.id\", name=\"task_instance_note_user_fkey\"), nullable=True)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "c9f84c6644a04240769b16217fbd14c44735e9a8",
      "candidate_info": {
        "commit_hash": "c9f84c6644a04240769b16217fbd14c44735e9a8",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/c9f84c6644a04240769b16217fbd14c44735e9a8",
        "files": [
          "airflow/providers/amazon/aws/hooks/batch_client.py",
          "airflow/utils/strings.py",
          "dev/breeze/src/airflow_breeze/utils/docker_command_utils.py",
          "dev/provider_packages/prepare_provider_packages.py",
          "tests/models/test_cleartasks.py",
          "tests/providers/amazon/aws/hooks/test_batch_client.py",
          "tests/providers/oracle/operators/test_oracle.py",
          "tests/providers/ssh/operators/test_ssh.py",
          "tests/system/providers/google/cloud/dataplex/resources/spark_example_pi.py"
        ],
        "message": "Refactor: Consolidate import and usage of random (#34108)\n\n(cherry picked from commit 4fa66d17003f10d03a13eda659bca8670bdf5052)",
        "before_after_code_files": [
          "airflow/providers/amazon/aws/hooks/batch_client.py||airflow/providers/amazon/aws/hooks/batch_client.py",
          "airflow/utils/strings.py||airflow/utils/strings.py",
          "dev/breeze/src/airflow_breeze/utils/docker_command_utils.py||dev/breeze/src/airflow_breeze/utils/docker_command_utils.py",
          "dev/provider_packages/prepare_provider_packages.py||dev/provider_packages/prepare_provider_packages.py",
          "tests/models/test_cleartasks.py||tests/models/test_cleartasks.py",
          "tests/providers/amazon/aws/hooks/test_batch_client.py||tests/providers/amazon/aws/hooks/test_batch_client.py",
          "tests/providers/oracle/operators/test_oracle.py||tests/providers/oracle/operators/test_oracle.py",
          "tests/providers/ssh/operators/test_ssh.py||tests/providers/ssh/operators/test_ssh.py",
          "tests/system/providers/google/cloud/dataplex/resources/spark_example_pi.py||tests/system/providers/google/cloud/dataplex/resources/spark_example_pi.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/providers/amazon/aws/hooks/batch_client.py||airflow/providers/amazon/aws/hooks/batch_client.py": [
          "File: airflow/providers/amazon/aws/hooks/batch_client.py -> airflow/providers/amazon/aws/hooks/batch_client.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "27: from __future__ import annotations",
          "29: import itertools",
          "31: from time import sleep",
          "34: import botocore.client",
          "35: import botocore.exceptions",
          "",
          "[Removed Lines]",
          "30: from random import uniform",
          "32: from typing import Callable",
          "",
          "[Added Lines]",
          "30: import random",
          "32: from typing import TYPE_CHECKING, Callable",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "38: from airflow.exceptions import AirflowException",
          "39: from airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook",
          "41: from airflow.typing_compat import Protocol, runtime_checkable",
          "44: @runtime_checkable",
          "45: class BatchProtocol(Protocol):",
          "",
          "[Removed Lines]",
          "40: from airflow.providers.amazon.aws.utils.task_log_fetcher import AwsTaskLogFetcher",
          "",
          "[Added Lines]",
          "42: if TYPE_CHECKING:",
          "43:     from airflow.providers.amazon.aws.utils.task_log_fetcher import AwsTaskLogFetcher",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "527:         minima = abs(minima)",
          "528:         lower = max(minima, delay - width)",
          "529:         upper = delay + width",
          "532:     @staticmethod",
          "533:     def delay(delay: int | float | None = None) -> None:",
          "",
          "[Removed Lines]",
          "530:         return uniform(lower, upper)",
          "",
          "[Added Lines]",
          "532:         return random.uniform(lower, upper)",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "544:             when many concurrent tasks request job-descriptions.",
          "545:         \"\"\"",
          "546:         if delay is None:",
          "548:         else:",
          "549:             delay = BatchClientHook.add_jitter(delay)",
          "550:         sleep(delay)",
          "",
          "[Removed Lines]",
          "547:             delay = uniform(BatchClientHook.DEFAULT_DELAY_MIN, BatchClientHook.DEFAULT_DELAY_MAX)",
          "",
          "[Added Lines]",
          "549:             delay = random.uniform(BatchClientHook.DEFAULT_DELAY_MIN, BatchClientHook.DEFAULT_DELAY_MAX)",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "593:         max_interval = 600.0  # results in 3 to 10 minute delay",
          "594:         delay = 1 + pow(tries * 0.6, 2)",
          "595:         delay = min(max_interval, delay)",
          "",
          "[Removed Lines]",
          "596:         return uniform(delay / 3, delay)",
          "",
          "[Added Lines]",
          "598:         return random.uniform(delay / 3, delay)",
          "",
          "---------------"
        ],
        "airflow/utils/strings.py||airflow/utils/strings.py": [
          "File: airflow/utils/strings.py -> airflow/utils/strings.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "17: \"\"\"Common utility functions with strings.\"\"\"",
          "18: from __future__ import annotations",
          "20: import string",
          "24: def get_random_string(length=8, choices=string.ascii_letters + string.digits):",
          "25:     \"\"\"Generate random string.\"\"\"",
          "29: TRUE_LIKE_VALUES = {\"on\", \"t\", \"true\", \"y\", \"yes\", \"1\"}",
          "",
          "[Removed Lines]",
          "21: from random import choice",
          "26:     return \"\".join(choice(choices) for _ in range(length))",
          "",
          "[Added Lines]",
          "20: import random",
          "26:     return \"\".join(random.choices(choices, k=length))",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/utils/docker_command_utils.py||dev/breeze/src/airflow_breeze/utils/docker_command_utils.py": [
          "File: dev/breeze/src/airflow_breeze/utils/docker_command_utils.py -> dev/breeze/src/airflow_breeze/utils/docker_command_utils.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "17: \"\"\"Various utils to prepare docker and docker compose commands.\"\"\"",
          "18: from __future__ import annotations",
          "20: import os",
          "21: import re",
          "22: import sys",
          "25: from subprocess import DEVNULL, CalledProcessError, CompletedProcess",
          "27: from airflow_breeze.params.build_ci_params import BuildCiParams",
          "",
          "[Removed Lines]",
          "23: from copy import deepcopy",
          "24: from random import randint",
          "",
          "[Added Lines]",
          "20: import copy",
          "22: import random",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "373:     value = \"true\" if raw_value is True else value",
          "374:     value = \"false\" if raw_value is False else value",
          "375:     if arg_name == \"upgrade_to_newer_dependencies\" and value == \"true\":",
          "377:     return value",
          "",
          "[Removed Lines]",
          "376:         value = f\"{randint(0, 2**32):x}\"",
          "",
          "[Added Lines]",
          "376:         value = f\"{random.randrange(2**32):x}\"",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "510: def build_cache(image_params: CommonBuildParams, output: Output | None) -> RunCommandResult:",
          "511:     build_command_result: CompletedProcess | CalledProcessError = CompletedProcess(args=[], returncode=0)",
          "512:     for platform in image_params.platforms:",
          "514:         # override the platform in the copied params to only be single platform per run",
          "515:         # as a workaround to https://github.com/docker/buildx/issues/1044",
          "516:         platform_image_params.platform = platform",
          "",
          "[Removed Lines]",
          "513:         platform_image_params = deepcopy(image_params)",
          "",
          "[Added Lines]",
          "513:         platform_image_params = copy.deepcopy(image_params)",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "722:         return",
          "723:     docker_syntax = get_docker_syntax_version()",
          "724:     get_console().print(f\"[info]Warming up the {docker_context} builder for syntax: {docker_syntax}\")",
          "726:     warm_up_image_param.image_tag = \"warmup\"",
          "727:     warm_up_image_param.push = False",
          "728:     build_command = prepare_base_build_command(image_params=warm_up_image_param)",
          "",
          "[Removed Lines]",
          "725:     warm_up_image_param = deepcopy(image_params)",
          "",
          "[Added Lines]",
          "725:     warm_up_image_param = copy.deepcopy(image_params)",
          "",
          "---------------"
        ],
        "dev/provider_packages/prepare_provider_packages.py||dev/provider_packages/prepare_provider_packages.py": [
          "File: dev/provider_packages/prepare_provider_packages.py -> dev/provider_packages/prepare_provider_packages.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "26: import json",
          "27: import logging",
          "28: import os",
          "29: import re",
          "30: import shutil",
          "31: import subprocess",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "29: import random",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "38: from enum import Enum",
          "39: from functools import lru_cache",
          "40: from pathlib import Path",
          "42: from shutil import copyfile",
          "43: from typing import Any, Generator, Iterable, NamedTuple",
          "",
          "[Removed Lines]",
          "41: from random import choice",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "1215:     given_answer = \"\"",
          "1216:     if answer and answer.lower() in [\"yes\", \"y\"]:",
          "1217:         # Simulate all possible non-terminal answers",
          "1219:             [",
          "1220:                 TypeOfChange.DOCUMENTATION,",
          "1221:                 TypeOfChange.BUGFIX,",
          "",
          "[Removed Lines]",
          "1218:         return choice(",
          "",
          "[Added Lines]",
          "1218:         return random.choice(",
          "",
          "---------------"
        ],
        "tests/models/test_cleartasks.py||tests/models/test_cleartasks.py": [
          "File: tests/models/test_cleartasks.py -> tests/models/test_cleartasks.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: from __future__ import annotations",
          "20: import datetime",
          "22: import pytest",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "21: import random",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "580:             assert tis[i].max_tries == 1",
          "582:         # test only_failed",
          "588:         session.commit()",
          "590:         DAG.clear_dags(dags, only_failed=True)",
          "598:             else:",
          "603:     def test_operator_clear(self, dag_maker):",
          "604:         with dag_maker(",
          "",
          "[Removed Lines]",
          "583:         from random import randint",
          "585:         failed_dag_idx = randint(0, len(tis) - 1)",
          "586:         tis[failed_dag_idx].state = State.FAILED",
          "587:         session.merge(tis[failed_dag_idx])",
          "592:         for i in range(num_of_dags):",
          "593:             tis[i].refresh_from_db()",
          "594:             if i != failed_dag_idx:",
          "595:                 assert tis[i].state == State.SUCCESS",
          "596:                 assert tis[i].try_number == 3",
          "597:                 assert tis[i].max_tries == 1",
          "599:                 assert tis[i].state == State.NONE",
          "600:                 assert tis[i].try_number == 3",
          "601:                 assert tis[i].max_tries == 2",
          "",
          "[Added Lines]",
          "584:         failed_dag = random.choice(tis)",
          "585:         failed_dag.state = State.FAILED",
          "586:         session.merge(failed_dag)",
          "591:         for ti in tis:",
          "592:             ti.refresh_from_db()",
          "593:             if ti is failed_dag:",
          "594:                 assert ti.state == State.NONE",
          "595:                 assert ti.try_number == 3",
          "596:                 assert ti.max_tries == 2",
          "598:                 assert ti.state == State.SUCCESS",
          "599:                 assert ti.try_number == 3",
          "600:                 assert ti.max_tries == 1",
          "",
          "---------------"
        ],
        "tests/providers/amazon/aws/hooks/test_batch_client.py||tests/providers/amazon/aws/hooks/test_batch_client.py": [
          "File: tests/providers/amazon/aws/hooks/test_batch_client.py -> tests/providers/amazon/aws/hooks/test_batch_client.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "426:         assert result >= minima",
          "427:         assert result <= width",
          "430:     @mock.patch(\"airflow.providers.amazon.aws.hooks.batch_client.sleep\")",
          "431:     def test_delay_defaults(self, mock_sleep, mock_uniform):",
          "432:         assert BatchClientHook.DEFAULT_DELAY_MIN == 1",
          "",
          "[Removed Lines]",
          "429:     @mock.patch(\"airflow.providers.amazon.aws.hooks.batch_client.uniform\")",
          "",
          "[Added Lines]",
          "429:     @mock.patch(\"airflow.providers.amazon.aws.hooks.batch_client.random.uniform\")",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "438:         )",
          "439:         mock_sleep.assert_called_once_with(0)",
          "442:     @mock.patch(\"airflow.providers.amazon.aws.hooks.batch_client.sleep\")",
          "443:     def test_delay_with_zero(self, mock_sleep, mock_uniform):",
          "444:         self.batch_client.delay(0)",
          "445:         mock_uniform.assert_called_once_with(0, 1)  # in add_jitter",
          "446:         mock_sleep.assert_called_once_with(mock_uniform.return_value)",
          "449:     @mock.patch(\"airflow.providers.amazon.aws.hooks.batch_client.sleep\")",
          "450:     def test_delay_with_int(self, mock_sleep, mock_uniform):",
          "451:         self.batch_client.delay(5)",
          "452:         mock_uniform.assert_called_once_with(4, 6)  # in add_jitter",
          "453:         mock_sleep.assert_called_once_with(mock_uniform.return_value)",
          "456:     @mock.patch(\"airflow.providers.amazon.aws.hooks.batch_client.sleep\")",
          "457:     def test_delay_with_float(self, mock_sleep, mock_uniform):",
          "458:         self.batch_client.delay(5.0)",
          "",
          "[Removed Lines]",
          "441:     @mock.patch(\"airflow.providers.amazon.aws.hooks.batch_client.uniform\")",
          "448:     @mock.patch(\"airflow.providers.amazon.aws.hooks.batch_client.uniform\")",
          "455:     @mock.patch(\"airflow.providers.amazon.aws.hooks.batch_client.uniform\")",
          "",
          "[Added Lines]",
          "441:     @mock.patch(\"airflow.providers.amazon.aws.hooks.batch_client.random.uniform\")",
          "448:     @mock.patch(\"airflow.providers.amazon.aws.hooks.batch_client.random.uniform\")",
          "455:     @mock.patch(\"airflow.providers.amazon.aws.hooks.batch_client.random.uniform\")",
          "",
          "---------------"
        ],
        "tests/providers/oracle/operators/test_oracle.py||tests/providers/oracle/operators/test_oracle.py": [
          "File: tests/providers/oracle/operators/test_oracle.py -> tests/providers/oracle/operators/test_oracle.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "16: # under the License.",
          "17: from __future__ import annotations",
          "19: import re",
          "21: from unittest import mock",
          "23: import oracledb",
          "",
          "[Removed Lines]",
          "20: from random import randrange",
          "",
          "[Added Lines]",
          "19: import random",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "90:         oracle_conn_id = \"oracle_default\"",
          "91:         parameters = {\"parameter\": \"value\"}",
          "92:         task_id = \"test_push\"",
          "94:         error = f\"ORA-{ora_exit_code}: This is a five-digit ORA error code\"",
          "95:         mock_callproc.side_effect = oracledb.DatabaseError(error)",
          "",
          "[Removed Lines]",
          "93:         ora_exit_code = f\"{randrange(10**5):05}\"",
          "",
          "[Added Lines]",
          "93:         ora_exit_code = f\"{random.randrange(10**5):05}\"",
          "",
          "---------------"
        ],
        "tests/providers/ssh/operators/test_ssh.py||tests/providers/ssh/operators/test_ssh.py": [
          "File: tests/providers/ssh/operators/test_ssh.py -> tests/providers/ssh/operators/test_ssh.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "17: # under the License.",
          "18: from __future__ import annotations",
          "21: from unittest import mock",
          "23: import pytest",
          "",
          "[Removed Lines]",
          "20: from random import randrange",
          "",
          "[Added Lines]",
          "20: import random",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "217:     def test_push_ssh_exit_to_xcom(self, request, dag_maker):",
          "218:         # Test pulls the value previously pushed to xcom and checks if it's the same",
          "219:         command = \"not_a_real_command\"",
          "221:         self.exec_ssh_client_command.return_value = (ssh_exit_code, b\"\", b\"ssh output\")",
          "223:         with dag_maker(dag_id=f\"dag_{request.node.name}\"):",
          "",
          "[Removed Lines]",
          "220:         ssh_exit_code = randrange(1, 100)",
          "",
          "[Added Lines]",
          "220:         ssh_exit_code = random.randrange(1, 100)",
          "",
          "---------------"
        ],
        "tests/system/providers/google/cloud/dataplex/resources/spark_example_pi.py||tests/system/providers/google/cloud/dataplex/resources/spark_example_pi.py": [
          "File: tests/system/providers/google/cloud/dataplex/resources/spark_example_pi.py -> tests/system/providers/google/cloud/dataplex/resources/spark_example_pi.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: from __future__ import annotations",
          "20: import sys",
          "21: from operator import add",
          "24: from pyspark.sql import SparkSession",
          "",
          "[Removed Lines]",
          "22: from random import random",
          "",
          "[Added Lines]",
          "20: import random",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "33:     n = 100000 * partitions",
          "35:     def f(_: int) -> float:",
          "38:         return 1 if x**2 + y**2 <= 1 else 0",
          "40:     count = spark.sparkContext.parallelize(range(1, n + 1), partitions).map(f).reduce(add)",
          "",
          "[Removed Lines]",
          "36:         x = random() * 2 - 1",
          "37:         y = random() * 2 - 1",
          "",
          "[Added Lines]",
          "36:         x = random.random() * 2 - 1",
          "37:         y = random.random() * 2 - 1",
          "",
          "---------------"
        ]
      }
    }
  ]
}