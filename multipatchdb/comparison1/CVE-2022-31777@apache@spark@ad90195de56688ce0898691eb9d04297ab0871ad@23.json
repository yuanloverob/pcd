{
  "cve_id": "CVE-2022-31777",
  "cve_desc": "A stored cross-site scripting (XSS) vulnerability in Apache Spark 3.2.1 and earlier, and 3.3.0, allows remote attackers to execute arbitrary JavaScript in the web browser of a user, by including a malicious payload into the logs which would be returned in logs rendered in the UI.",
  "repo": "apache/spark",
  "patch_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
  "patch_info": {
    "commit_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
    "repo": "apache/spark",
    "commit_url": "https://github.com/apache/spark/commit/ad90195de56688ce0898691eb9d04297ab0871ad",
    "files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ],
    "message": "[SPARK-39505][UI] Escape log content rendered in UI\n\n### What changes were proposed in this pull request?\n\nEscape log content rendered to the UI.\n\n### Why are the changes needed?\n\nLog content may contain reserved characters or other code in the log and be misinterpreted in the UI as HTML.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nExisting tests\n\nCloses #36902 from srowen/LogViewEscape.\n\nAuthored-by: Sean Owen <srowen@gmail.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
    "before_after_code_files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ]
  },
  "patch_diff": {
    "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js": [
      "File: core/src/main/resources/org/apache/spark/ui/static/log-view.js -> core/src/main/resources/org/apache/spark/ui/static/log-view.js",
      "--- Hunk 1 ---",
      "[Context before]",
      "85:       if (retStartByte == 0) {",
      "86:         disableMoreButton();",
      "87:       }",
      "90:       curLogLength = curLogLength + (startByte - retStartByte);",
      "91:       startByte = retStartByte;",
      "",
      "[Removed Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(cleanData);",
      "",
      "[Added Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(document.createTextNode(cleanData));",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "115:             var retLogLength = dataInfo[2];",
      "117:             var cleanData = data.substring(newlineIndex + 1);",
      "120:             curLogLength = curLogLength + (retEndByte - retStartByte);",
      "121:             endByte = retEndByte;",
      "",
      "[Removed Lines]",
      "118:             $(\"pre\", \".log-content\").append(cleanData);",
      "",
      "[Added Lines]",
      "118:             $(\"pre\", \".log-content\").append(document.createTextNode(cleanData));",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "4cd395ef2c2ea842c570b4187778c0a580596f3c",
      "candidate_info": {
        "commit_hash": "4cd395ef2c2ea842c570b4187778c0a580596f3c",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/4cd395ef2c2ea842c570b4187778c0a580596f3c",
        "files": [
          "R/pkg/R/DataFrame.R",
          "R/pkg/R/RDD.R",
          "R/pkg/R/sparkR.R",
          "assembly/pom.xml",
          "common/kvstore/pom.xml",
          "common/network-common/pom.xml",
          "common/network-shuffle/pom.xml",
          "common/network-yarn/pom.xml",
          "common/sketch/pom.xml",
          "common/tags/pom.xml",
          "common/unsafe/pom.xml",
          "core/pom.xml",
          "dev/checkstyle.xml",
          "examples/pom.xml",
          "external/avro/pom.xml",
          "external/docker-integration-tests/pom.xml",
          "external/kafka-0-10-assembly/pom.xml",
          "external/kafka-0-10-token-provider/pom.xml",
          "external/kafka-0-10/pom.xml",
          "external/kinesis-asl-assembly/pom.xml",
          "external/kinesis-asl/src/main/java/org/apache/spark/examples/streaming/JavaKinesisWordCountASL.java",
          "external/kinesis-asl/src/main/python/examples/streaming/kinesis_wordcount_asl.py",
          "external/kinesis-asl/src/main/scala/org/apache/spark/examples/streaming/KinesisWordCountASL.scala",
          "graphx/pom.xml",
          "launcher/pom.xml",
          "mllib-local/pom.xml",
          "mllib/pom.xml",
          "pom.xml",
          "repl/pom.xml",
          "resource-managers/kubernetes/docker/src/main/dockerfiles/spark/Dockerfile",
          "resource-managers/kubernetes/docker/src/main/dockerfiles/spark/Dockerfile.java17",
          "sql/catalyst/pom.xml",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala",
          "sql/core/pom.xml",
          "sql/hive-thriftserver/pom.xml",
          "sql/hive/pom.xml",
          "streaming/pom.xml",
          "streaming/src/main/scala/org/apache/spark/streaming/StreamingContext.scala",
          "streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaStreamingContext.scala",
          "tools/pom.xml"
        ],
        "message": "[SPARK-38778][INFRA][BUILD] Replace http with https for project url in pom\n\n### What changes were proposed in this pull request?\n\nchange <url>http://spark.apache.org/</url> to <url>https://spark.apache.org/</url> in the project URL of all pom files\n### Why are the changes needed?\n\nfix home page in maven central https://mvnrepository.com/artifact/org.apache.spark/spark-sql_2.13/3.2.1\n\n#### From\nLicense | Apache 2.0\n-- | --\nCategories |Hadoop Query Engines\nHomePage|http://spark.apache.org/\nDate | (Jan 26, 2022)\n\n#### to\n\nLicense | Apache 2.0\n-- | --\nCategories |Hadoop Query Engines\nHomePage|https://spark.apache.org/\nDate | (Jan 26, 2022)\n### Does this PR introduce _any_ user-facing change?\n\nno\n\n### How was this patch tested?\n\npass GHA\n\nCloses #36053 from yaooqinn/SPARK-38778.\n\nAuthored-by: Kent Yao <yao@apache.org>\nSigned-off-by: Yuming Wang <yumwang@ebay.com>\n(cherry picked from commit 65d347d145f0039d5246431431573ccd34724276)\nSigned-off-by: Yuming Wang <yumwang@ebay.com>",
        "before_after_code_files": [
          "R/pkg/R/DataFrame.R||R/pkg/R/DataFrame.R",
          "R/pkg/R/RDD.R||R/pkg/R/RDD.R",
          "R/pkg/R/sparkR.R||R/pkg/R/sparkR.R",
          "external/kinesis-asl/src/main/java/org/apache/spark/examples/streaming/JavaKinesisWordCountASL.java||external/kinesis-asl/src/main/java/org/apache/spark/examples/streaming/JavaKinesisWordCountASL.java",
          "external/kinesis-asl/src/main/python/examples/streaming/kinesis_wordcount_asl.py||external/kinesis-asl/src/main/python/examples/streaming/kinesis_wordcount_asl.py",
          "external/kinesis-asl/src/main/scala/org/apache/spark/examples/streaming/KinesisWordCountASL.scala||external/kinesis-asl/src/main/scala/org/apache/spark/examples/streaming/KinesisWordCountASL.scala",
          "resource-managers/kubernetes/docker/src/main/dockerfiles/spark/Dockerfile.java17||resource-managers/kubernetes/docker/src/main/dockerfiles/spark/Dockerfile.java17",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala",
          "streaming/src/main/scala/org/apache/spark/streaming/StreamingContext.scala||streaming/src/main/scala/org/apache/spark/streaming/StreamingContext.scala",
          "streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaStreamingContext.scala||streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaStreamingContext.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "R/pkg/R/DataFrame.R||R/pkg/R/DataFrame.R": [
          "File: R/pkg/R/DataFrame.R -> R/pkg/R/DataFrame.R",
          "--- Hunk 1 ---",
          "[Context before]",
          "608: #'",
          "609: #' Persist this SparkDataFrame with the specified storage level. For details of the",
          "610: #' supported storage levels, refer to",
          "612: #'",
          "613: #' @param x the SparkDataFrame to persist.",
          "614: #' @param newLevel storage level chosen for the persistence. See available options in",
          "",
          "[Removed Lines]",
          "611: #' \\url{http://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-persistence}.",
          "",
          "[Added Lines]",
          "611: #' \\url{https://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-persistence}.",
          "",
          "---------------"
        ],
        "R/pkg/R/RDD.R||R/pkg/R/RDD.R": [
          "File: R/pkg/R/RDD.R -> R/pkg/R/RDD.R",
          "--- Hunk 1 ---",
          "[Context before]",
          "227: #'",
          "228: #' Persist this RDD with the specified storage level. For details of the",
          "229: #' supported storage levels, refer to",
          "231: #'",
          "232: #' @param x The RDD to persist",
          "233: #' @param newLevel The new storage level to be assigned",
          "",
          "[Removed Lines]",
          "230: #'\\url{http://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-persistence}.",
          "",
          "[Added Lines]",
          "230: #'\\url{https://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-persistence}.",
          "",
          "---------------"
        ],
        "R/pkg/R/sparkR.R||R/pkg/R/sparkR.R": [
          "File: R/pkg/R/sparkR.R -> R/pkg/R/sparkR.R",
          "--- Hunk 1 ---",
          "[Context before]",
          "344: #' the warehouse, an accompanied metastore may also be automatically created in the current",
          "345: #' directory when a new SparkSession is initialized with \\code{enableHiveSupport} set to",
          "346: #' \\code{TRUE}, which is the default. For more details, refer to Hive configuration at",
          "348: #'",
          "349: #' For details on how to initialize and use SparkR, refer to SparkR programming guide at",
          "351: #'",
          "352: #' @param master the Spark master URL.",
          "353: #' @param appName application name to register with cluster manager.",
          "",
          "[Removed Lines]",
          "347: #' \\url{http://spark.apache.org/docs/latest/sql-programming-guide.html#hive-tables}.",
          "350: #' \\url{http://spark.apache.org/docs/latest/sparkr.html#starting-up-sparksession}.",
          "",
          "[Added Lines]",
          "347: #' \\url{https://spark.apache.org/docs/latest/sql-programming-guide.html#hive-tables}.",
          "350: #' \\url{https://spark.apache.org/docs/latest/sparkr.html#starting-up-sparksession}.",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "598: #",
          "599: # A few Spark Application and Runtime environment properties cannot take effect after driver",
          "600: # JVM has started, as documented in:",
          "602: # When starting SparkR without using spark-submit, for example, from Rstudio, add them to",
          "603: # spark-submit commandline if not already set in SPARKR_SUBMIT_ARGS so that they can be effective.",
          "604: getClientModeSparkSubmitOpts <- function(submitOps, sparkEnvirMap) {",
          "",
          "[Removed Lines]",
          "601: # http://spark.apache.org/docs/latest/configuration.html#application-properties",
          "",
          "[Added Lines]",
          "601: # https://spark.apache.org/docs/latest/configuration.html#application-properties",
          "",
          "---------------"
        ],
        "external/kinesis-asl/src/main/java/org/apache/spark/examples/streaming/JavaKinesisWordCountASL.java||external/kinesis-asl/src/main/java/org/apache/spark/examples/streaming/JavaKinesisWordCountASL.java": [
          "File: external/kinesis-asl/src/main/java/org/apache/spark/examples/streaming/JavaKinesisWordCountASL.java -> external/kinesis-asl/src/main/java/org/apache/spark/examples/streaming/JavaKinesisWordCountASL.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "91:           \"    <endpoint-url> is the endpoint of the Kinesis service\\n\" +",
          "92:           \"                   (e.g. https://kinesis.us-east-1.amazonaws.com)\\n\" +",
          "93:           \"Generate data for the Kinesis stream using the example KinesisWordProducerASL.\\n\" +",
          "95:           \"details.\\n\"",
          "96:       );",
          "97:       System.exit(1);",
          "",
          "[Removed Lines]",
          "94:           \"See http://spark.apache.org/docs/latest/streaming-kinesis-integration.html for more\\n\" +",
          "",
          "[Added Lines]",
          "94:           \"See https://spark.apache.org/docs/latest/streaming-kinesis-integration.html for more\\n\" +",
          "",
          "---------------"
        ],
        "external/kinesis-asl/src/main/python/examples/streaming/kinesis_wordcount_asl.py||external/kinesis-asl/src/main/python/examples/streaming/kinesis_wordcount_asl.py": [
          "File: external/kinesis-asl/src/main/python/examples/streaming/kinesis_wordcount_asl.py -> external/kinesis-asl/src/main/python/examples/streaming/kinesis_wordcount_asl.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "50:       Credential profiles file - default location (~/.aws/credentials) shared by all AWS SDKs",
          "51:       Instance profile credentials - delivered through the Amazon EC2 metadata service",
          "52:   For more information, see",
          "56:   the Kinesis Spark Streaming integration.",
          "57: \"\"\"",
          "58: import sys",
          "",
          "[Removed Lines]",
          "53:       http://docs.aws.amazon.com/AWSSdkDocsJava/latest/DeveloperGuide/credentials.html",
          "55:   See http://spark.apache.org/docs/latest/streaming-kinesis-integration.html for more details on",
          "",
          "[Added Lines]",
          "53:       https://docs.aws.amazon.com/AWSSdkDocsJava/latest/DeveloperGuide/credentials.html",
          "55:   See https://spark.apache.org/docs/latest/streaming-kinesis-integration.html for more details on",
          "",
          "---------------"
        ],
        "external/kinesis-asl/src/main/scala/org/apache/spark/examples/streaming/KinesisWordCountASL.scala||external/kinesis-asl/src/main/scala/org/apache/spark/examples/streaming/KinesisWordCountASL.scala": [
          "File: external/kinesis-asl/src/main/scala/org/apache/spark/examples/streaming/KinesisWordCountASL.scala -> external/kinesis-asl/src/main/scala/org/apache/spark/examples/streaming/KinesisWordCountASL.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "87:           |                   (e.g. https://kinesis.us-east-1.amazonaws.com)",
          "88:           |",
          "89:           |Generate input data for Kinesis stream using the example KinesisWordProducerASL.",
          "91:           |details.",
          "92:         \"\"\".stripMargin)",
          "93:       System.exit(1)",
          "",
          "[Removed Lines]",
          "90:           |See http://spark.apache.org/docs/latest/streaming-kinesis-integration.html for more",
          "",
          "[Added Lines]",
          "90:           |See https://spark.apache.org/docs/latest/streaming-kinesis-integration.html for more",
          "",
          "---------------"
        ],
        "resource-managers/kubernetes/docker/src/main/dockerfiles/spark/Dockerfile.java17||resource-managers/kubernetes/docker/src/main/dockerfiles/spark/Dockerfile.java17": [
          "File: resource-managers/kubernetes/docker/src/main/dockerfiles/spark/Dockerfile.java17 -> resource-managers/kubernetes/docker/src/main/dockerfiles/spark/Dockerfile.java17",
          "--- Hunk 1 ---",
          "[Context before]",
          "21: ARG spark_uid=185",
          "23: # Before building the docker image, first build and make a Spark distribution following",
          "25: # If this docker file is being used in the context of building your images from a Spark",
          "26: # distribution, the docker build command should be invoked from the top level directory",
          "27: # of the Spark distribution. E.g.:",
          "",
          "[Removed Lines]",
          "24: # the instructions in http://spark.apache.org/docs/latest/building-spark.html.",
          "",
          "[Added Lines]",
          "24: # the instructions in https://spark.apache.org/docs/latest/building-spark.html.",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "520:     new ClassNotFoundException(",
          "521:       s\"\"\"",
          "522:          |Failed to find data source: $provider. Please find packages at",
          "524:        \"\"\".stripMargin, error)",
          "525:   }",
          "",
          "[Removed Lines]",
          "523:          |http://spark.apache.org/third-party-projects.html",
          "",
          "[Added Lines]",
          "523:          |https://spark.apache.org/third-party-projects.html",
          "",
          "---------------"
        ],
        "streaming/src/main/scala/org/apache/spark/streaming/StreamingContext.scala||streaming/src/main/scala/org/apache/spark/streaming/StreamingContext.scala": [
          "File: streaming/src/main/scala/org/apache/spark/streaming/StreamingContext.scala -> streaming/src/main/scala/org/apache/spark/streaming/StreamingContext.scala"
        ],
        "streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaStreamingContext.scala||streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaStreamingContext.scala": [
          "File: streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaStreamingContext.scala -> streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaStreamingContext.scala"
        ]
      }
    },
    {
      "candidate_hash": "fef569507bcd23b3b515bcbf489e2a9151ac51be",
      "candidate_info": {
        "commit_hash": "fef569507bcd23b3b515bcbf489e2a9151ac51be",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/fef569507bcd23b3b515bcbf489e2a9151ac51be",
        "files": [
          "external/kafka-0-10-sql/src/test/scala/org/apache/spark/sql/kafka010/KafkaMicroBatchSourceSuite.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecutionSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/streaming/continuous/ContinuousSuite.scala"
        ],
        "message": "[SPARK-39346][SQL][3.3] Convert asserts/illegal state exception to internal errors on each phase\n\n### What changes were proposed in this pull request?\nIn the PR, I propose to catch asserts/illegal state exception on each phase of query execution: ANALYSIS, OPTIMIZATION, PLANNING, and convert them to a SparkException w/ the `INTERNAL_ERROR` error class.\n\nThis is a backport of https://github.com/apache/spark/pull/36704.\n\n### Why are the changes needed?\nTo improve user experience with Spark SQL and unify representation of user-facing errors.\n\n### Does this PR introduce _any_ user-facing change?\nNo. The changes might affect users in corner cases only.\n\n### How was this patch tested?\nBy running the affected test suites:\n```\n$ build/sbt \"test:testOnly *KafkaMicroBatchV1SourceSuite\"\n$ build/sbt \"test:testOnly *KafkaMicroBatchV2SourceSuite\"\n```\n\nAuthored-by: Max Gekk <max.gekkgmail.com>\nSigned-off-by: Max Gekk <max.gekkgmail.com>\n(cherry picked from commit 8894e785edae42a642351ad91e539324c39da8e4)\nSigned-off-by: Max Gekk <max.gekkgmail.com>\n\nCloses #36742 from MaxGekk/wrapby-INTERNAL_ERROR-every-phase-3.3.\n\nAuthored-by: Max Gekk <max.gekk@gmail.com>\nSigned-off-by: Max Gekk <max.gekk@gmail.com>",
        "before_after_code_files": [
          "external/kafka-0-10-sql/src/test/scala/org/apache/spark/sql/kafka010/KafkaMicroBatchSourceSuite.scala||external/kafka-0-10-sql/src/test/scala/org/apache/spark/sql/kafka010/KafkaMicroBatchSourceSuite.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala||sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecutionSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecutionSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/streaming/continuous/ContinuousSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/streaming/continuous/ContinuousSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "external/kafka-0-10-sql/src/test/scala/org/apache/spark/sql/kafka010/KafkaMicroBatchSourceSuite.scala||external/kafka-0-10-sql/src/test/scala/org/apache/spark/sql/kafka010/KafkaMicroBatchSourceSuite.scala": [
          "File: external/kafka-0-10-sql/src/test/scala/org/apache/spark/sql/kafka010/KafkaMicroBatchSourceSuite.scala -> external/kafka-0-10-sql/src/test/scala/org/apache/spark/sql/kafka010/KafkaMicroBatchSourceSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "34: import org.scalatest.concurrent.PatienceConfiguration.Timeout",
          "35: import org.scalatest.time.SpanSugar._",
          "37: import org.apache.spark.sql.{Dataset, ForeachWriter, Row, SparkSession}",
          "38: import org.apache.spark.sql.catalyst.util.CaseInsensitiveMap",
          "39: import org.apache.spark.sql.connector.read.streaming.SparkDataStream",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "37: import org.apache.spark.{SparkException, SparkThrowable}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "666:         testUtils.sendMessages(topic2, Array(\"6\"))",
          "667:       },",
          "668:       StartStream(),",
          "672:       })",
          "673:     )",
          "674:   }",
          "",
          "[Removed Lines]",
          "669:       ExpectFailure[IllegalStateException](e => {",
          "671:         assert(e.getMessage.contains(\"was changed from 2 to 1\"))",
          "",
          "[Added Lines]",
          "670:       ExpectFailure[SparkException](e => {",
          "671:         assert(e.asInstanceOf[SparkThrowable].getErrorClass === \"INTERNAL_ERROR\")",
          "673:         assert(e.getCause.getMessage.contains(\"was changed from 2 to 1\"))",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "765:       testStream(df)(",
          "766:         StartStream(checkpointLocation = metadataPath.getAbsolutePath),",
          "768:           Seq(",
          "769:             s\"maximum supported log version is v1, but encountered v99999\",",
          "770:             \"produced by a newer version of Spark and cannot be read by this version\"",
          "771:           ).foreach { message =>",
          "773:           }",
          "774:         }))",
          "775:     }",
          "",
          "[Removed Lines]",
          "767:         ExpectFailure[IllegalStateException](e => {",
          "772:             assert(e.toString.contains(message))",
          "",
          "[Added Lines]",
          "769:         ExpectFailure[SparkException](e => {",
          "770:           assert(e.asInstanceOf[SparkThrowable].getErrorClass === \"INTERNAL_ERROR\")",
          "775:             assert(e.getCause.toString.contains(message))",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala||sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala -> sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "28: import org.apache.commons.lang3.StringUtils",
          "31: import org.apache.spark.annotation.{DeveloperApi, Stable, Unstable}",
          "32: import org.apache.spark.api.java.JavaRDD",
          "33: import org.apache.spark.api.java.function._",
          "",
          "[Removed Lines]",
          "30: import org.apache.spark.{SparkException, SparkThrowable, TaskContext}",
          "",
          "[Added Lines]",
          "30: import org.apache.spark.TaskContext",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "3854:   private def withAction[U](name: String, qe: QueryExecution)(action: SparkPlan => U) = {",
          "3857:         qe.executedPlan.resetMetrics()",
          "3858:         action(qe.executedPlan)",
          "3859:       }",
          "3868:     }",
          "3869:   }",
          "",
          "[Removed Lines]",
          "3855:     try {",
          "3856:       SQLExecution.withNewExecutionId(qe, Some(name)) {",
          "3860:     } catch {",
          "3861:       case e: SparkThrowable => throw e",
          "3862:       case e @ (_: java.lang.IllegalStateException | _: java.lang.AssertionError) =>",
          "3863:         throw new SparkException(",
          "3864:           errorClass = \"INTERNAL_ERROR\",",
          "3865:           messageParameters = Array(s\"\"\"The \"$name\" action failed.\"\"\"),",
          "3866:           cause = e)",
          "3867:       case e: Throwable => throw e",
          "",
          "[Added Lines]",
          "3855:     SQLExecution.withNewExecutionId(qe, Some(name)) {",
          "3856:       QueryExecution.withInternalError(s\"\"\"The \"$name\" action failed.\"\"\") {",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "24: import org.apache.hadoop.fs.Path",
          "26: import org.apache.spark.internal.Logging",
          "27: import org.apache.spark.rdd.RDD",
          "28: import org.apache.spark.sql.{AnalysisException, Row, SparkSession}",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "26: import org.apache.spark.SparkException",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "180:   }",
          "182:   protected def executePhase[T](phase: String)(block: => T): T = sparkSession.withActive {",
          "184:   }",
          "186:   def simpleString: String = {",
          "",
          "[Removed Lines]",
          "183:     tracker.measurePhase(phase)(block)",
          "",
          "[Added Lines]",
          "184:     QueryExecution.withInternalError(s\"The Spark SQL phase $phase failed with an internal error.\") {",
          "185:       tracker.measurePhase(phase)(block)",
          "186:     }",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "484:     val preparationRules = preparations(session, Option(InsertAdaptiveSparkPlan(context)), true)",
          "485:     prepareForExecution(preparationRules, sparkPlan.clone())",
          "486:   }",
          "487: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "494:   private[sql] def toInternalError(msg: String, e: Throwable): Throwable = e match {",
          "495:     case e @ (_: java.lang.IllegalStateException | _: java.lang.NullPointerException |",
          "496:               _: java.lang.AssertionError) =>",
          "497:       new SparkException(",
          "498:         errorClass = \"INTERNAL_ERROR\",",
          "499:         messageParameters = Array(msg +",
          "500:           \" Please, fill a bug report in, and provide the full stack trace.\"),",
          "501:         cause = e)",
          "502:     case e: Throwable =>",
          "503:       e",
          "504:   }",
          "509:   private[sql] def withInternalError[T](msg: String)(block: => T): T = {",
          "510:     try {",
          "511:       block",
          "512:     } catch {",
          "513:       case e: Throwable => throw toInternalError(msg, e)",
          "514:     }",
          "515:   }",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "39: import org.apache.spark.sql.connector.catalog.{SupportsWrite, Table}",
          "40: import org.apache.spark.sql.connector.read.streaming.{Offset => OffsetV2, ReadLimit, SparkDataStream}",
          "41: import org.apache.spark.sql.connector.write.{LogicalWriteInfoImpl, SupportsTruncate, Write}",
          "42: import org.apache.spark.sql.execution.command.StreamingExplainCommand",
          "43: import org.apache.spark.sql.execution.datasources.v2.StreamWriterCommitProgress",
          "44: import org.apache.spark.sql.internal.SQLConf",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "42: import org.apache.spark.sql.execution.QueryExecution",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "321:         updateStatusMessage(\"Stopped\")",
          "323:         streamDeathCause = new StreamingQueryException(",
          "324:           toDebugString(includeLogicalPlan = isInitialized),",
          "325:           s\"Query $prettyIdString terminated with exception: ${e.getMessage}\",",
          "",
          "[Removed Lines]",
          "322:       case e: Throwable =>",
          "",
          "[Added Lines]",
          "323:       case t: Throwable =>",
          "324:         val e = QueryExecution.toInternalError(msg = s\"Execution of the stream $name failed.\", t)",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecutionSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecutionSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecutionSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecutionSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "22: import org.apache.commons.io.FileUtils",
          "23: import org.scalatest.BeforeAndAfter",
          "25: import org.apache.spark.sql.{DataFrame, Dataset, SparkSession}",
          "26: import org.apache.spark.sql.catalyst.plans.logical.Range",
          "27: import org.apache.spark.sql.connector.read.streaming",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "25: import org.apache.spark.{SparkException, SparkThrowable}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "93:     testStream(streamEvent) (",
          "94:       AddData(inputData, 1, 2, 3, 4, 5, 6),",
          "95:       StartStream(Trigger.Once, checkpointLocation = checkpointDir.getAbsolutePath),",
          "98:       }",
          "99:     )",
          "100:   }",
          "",
          "[Removed Lines]",
          "96:       ExpectFailure[IllegalStateException] { e =>",
          "97:         assert(e.getMessage.contains(\"batch 3 doesn't exist\"))",
          "",
          "[Added Lines]",
          "97:       ExpectFailure[SparkException] { e =>",
          "98:         assert(e.asInstanceOf[SparkThrowable].getErrorClass === \"INTERNAL_ERROR\")",
          "99:         assert(e.getCause.getMessage.contains(\"batch 3 doesn't exist\"))",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/streaming/continuous/ContinuousSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/streaming/continuous/ContinuousSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/streaming/continuous/ContinuousSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/streaming/continuous/ContinuousSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: import java.sql.Timestamp",
          "23: import org.apache.spark.scheduler.{SparkListener, SparkListenerTaskStart}",
          "24: import org.apache.spark.sql._",
          "25: import org.apache.spark.sql.execution.streaming._",
          "",
          "[Removed Lines]",
          "22: import org.apache.spark.{SparkContext, SparkException}",
          "",
          "[Added Lines]",
          "22: import org.apache.spark.{SparkContext, SparkException, SparkThrowable}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "441:       testStream(df)(",
          "442:         StartStream(Trigger.Continuous(1)),",
          "445:         }",
          "446:       )",
          "447:     }",
          "",
          "[Removed Lines]",
          "443:         ExpectFailure[IllegalStateException] { e =>",
          "444:           e.getMessage.contains(\"queue has exceeded its maximum\")",
          "",
          "[Added Lines]",
          "443:         ExpectFailure[SparkException] { e =>",
          "444:           assert(e.asInstanceOf[SparkThrowable].getErrorClass === \"INTERNAL_ERROR\")",
          "445:           e.getCause.getMessage.contains(\"queue has exceeded its maximum\")",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "0cdb081670b55d9181d8ffb125911333e8ab339b",
      "candidate_info": {
        "commit_hash": "0cdb081670b55d9181d8ffb125911333e8ab339b",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/0cdb081670b55d9181d8ffb125911333e8ab339b",
        "files": [
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFilters.scala",
          "sql/core/src/test/resources/test-data/tagged_int.parquet",
          "sql/core/src/test/resources/test-data/tagged_long.parquet",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFilterSuite.scala"
        ],
        "message": "[SPARK-40280][SQL] Add support for parquet push down for annotated int and long\n\n### What changes were proposed in this pull request?\nThis fixes SPARK-40280 by normalizing a parquet int/long that has optional metadata with it to look like the expected version that does not have the extra metadata.\n\n## Why are the changes needed?\nThis allows predicate push down in parquet to work when reading files that are complaint with the parquet specification, but different from what Spark writes.\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\nI added unit tests that cover this use case. I also did some manual testing on some queries to verify that less data is actually read after this change.\n\nCloses #37747 from revans2/normalize_int_long_parquet_push.\n\nAuthored-by: Robert (Bobby) Evans <bobby@apache.org>\nSigned-off-by: Thomas Graves <tgraves@apache.org>\n(cherry picked from commit 24b3baf0177fc1446bf59bb34987296aefd4b318)\nSigned-off-by: Thomas Graves <tgraves@apache.org>",
        "before_after_code_files": [
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFilters.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFilters.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFilterSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFilterSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFilters.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFilters.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFilters.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFilters.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "30: import org.apache.parquet.filter2.predicate.SparkFilterApi._",
          "31: import org.apache.parquet.io.api.Binary",
          "32: import org.apache.parquet.schema.{GroupType, LogicalTypeAnnotation, MessageType, PrimitiveComparator, PrimitiveType, Type}",
          "34: import org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName",
          "35: import org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName._",
          "36: import org.apache.parquet.schema.Type.Repetition",
          "",
          "[Removed Lines]",
          "33: import org.apache.parquet.schema.LogicalTypeAnnotation.{DecimalLogicalTypeAnnotation, TimeUnit}",
          "",
          "[Added Lines]",
          "33: import org.apache.parquet.schema.LogicalTypeAnnotation.{DecimalLogicalTypeAnnotation, IntLogicalTypeAnnotation, TimeUnit}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "61:   private val nameToParquetField : Map[String, ParquetPrimitiveField] = {",
          "64:     def getPrimitiveFields(",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "62:     def getNormalizedLogicalType(p: PrimitiveType): LogicalTypeAnnotation = {",
          "65:       (p.getPrimitiveTypeName, p.getLogicalTypeAnnotation) match {",
          "66:         case (INT32, intType: IntLogicalTypeAnnotation)",
          "67:           if intType.getBitWidth() == 32 && intType.isSigned() => null",
          "68:         case (INT64, intType: IntLogicalTypeAnnotation)",
          "69:           if intType.getBitWidth() == 64 && intType.isSigned() => null",
          "70:         case (_, otherType) => otherType",
          "71:       }",
          "72:     }",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "71:         case p: PrimitiveType if p.getRepetition != Repetition.REPEATED =>",
          "72:           Some(ParquetPrimitiveField(fieldNames = parentFieldNames :+ p.getName,",
          "74:               p.getPrimitiveTypeName, p.getTypeLength)))",
          "",
          "[Removed Lines]",
          "73:             fieldType = ParquetSchemaType(p.getLogicalTypeAnnotation,",
          "",
          "[Added Lines]",
          "85:             fieldType = ParquetSchemaType(getNormalizedLogicalType(p),",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFilterSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFilterSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFilterSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFilterSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "365:     }",
          "366:   }",
          "368:   test(\"filter pushdown - long\") {",
          "369:     val data = (1 to 4).map(i => Tuple1(Option(i.toLong)))",
          "370:     withNestedParquetDataFrame(data) { case (inputDF, colName, resultFun) =>",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "368:   test(\"SPARK-40280: filter pushdown - int with annotation\") {",
          "369:     implicit val df = readResourceParquetFile(\"test-data/tagged_int.parquet\")",
          "371:     val intAttr = df(\"_c0\").expr",
          "372:     assert(intAttr.dataType === IntegerType)",
          "374:     checkFilterPredicate(intAttr.isNull, classOf[Eq[_]], Seq.empty[Row])",
          "375:     checkFilterPredicate(intAttr.isNotNull, classOf[NotEq[_]],",
          "376:       (1 to 4).map(i => Row.apply(i)))",
          "378:     checkFilterPredicate(intAttr === 1, classOf[Eq[_]], 1)",
          "379:     checkFilterPredicate(intAttr <=> 1, classOf[Eq[_]], 1)",
          "380:     checkFilterPredicate(intAttr =!= 1, classOf[NotEq[_]],",
          "381:       (2 to 4).map(i => Row.apply(i)))",
          "383:     checkFilterPredicate(intAttr < 2, classOf[Lt[_]], 1)",
          "384:     checkFilterPredicate(intAttr > 3, classOf[Gt[_]], 4)",
          "385:     checkFilterPredicate(intAttr <= 1, classOf[LtEq[_]], 1)",
          "386:     checkFilterPredicate(intAttr >= 4, classOf[GtEq[_]], 4)",
          "388:     checkFilterPredicate(Literal(1) === intAttr, classOf[Eq[_]], 1)",
          "389:     checkFilterPredicate(Literal(1) <=> intAttr, classOf[Eq[_]], 1)",
          "390:     checkFilterPredicate(Literal(2) > intAttr, classOf[Lt[_]], 1)",
          "391:     checkFilterPredicate(Literal(3) < intAttr, classOf[Gt[_]], 4)",
          "392:     checkFilterPredicate(Literal(1) >= intAttr, classOf[LtEq[_]], 1)",
          "393:     checkFilterPredicate(Literal(4) <= intAttr, classOf[GtEq[_]], 4)",
          "395:     checkFilterPredicate(!(intAttr < 4), classOf[GtEq[_]], 4)",
          "396:     checkFilterPredicate(intAttr < 2 || intAttr > 3, classOf[Operators.Or],",
          "397:       Seq(Row(1), Row(4)))",
          "399:     Seq(3, 20).foreach { threshold =>",
          "400:       withSQLConf(SQLConf.PARQUET_FILTER_PUSHDOWN_INFILTERTHRESHOLD.key -> s\"$threshold\") {",
          "401:         checkFilterPredicate(",
          "402:           In(intAttr, Array(2, 3, 4, 5, 6, 7).map(Literal.apply)),",
          "403:           if (threshold == 3) classOf[FilterIn[_]] else classOf[Operators.Or],",
          "404:           Seq(Row(2), Row(3), Row(4)))",
          "405:       }",
          "406:     }",
          "407:   }",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "409:     }",
          "410:   }",
          "412:   test(\"filter pushdown - float\") {",
          "413:     val data = (1 to 4).map(i => Tuple1(Option(i.toFloat)))",
          "414:     withNestedParquetDataFrame(data) { case (inputDF, colName, resultFun) =>",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "453:   test(\"SPARK-40280: filter pushdown - long with annotation\") {",
          "454:     implicit val df = readResourceParquetFile(\"test-data/tagged_long.parquet\")",
          "456:     val longAttr = df(\"_c0\").expr",
          "457:     assert(longAttr.dataType === LongType)",
          "459:     checkFilterPredicate(longAttr.isNull, classOf[Eq[_]], Seq.empty[Row])",
          "460:     checkFilterPredicate(longAttr.isNotNull, classOf[NotEq[_]],",
          "461:       (1 to 4).map(i => Row.apply(i)))",
          "463:     checkFilterPredicate(longAttr === 1, classOf[Eq[_]], 1)",
          "464:     checkFilterPredicate(longAttr <=> 1, classOf[Eq[_]], 1)",
          "465:     checkFilterPredicate(longAttr =!= 1, classOf[NotEq[_]],",
          "466:       (2 to 4).map(i => Row.apply(i)))",
          "468:     checkFilterPredicate(longAttr < 2, classOf[Lt[_]], 1)",
          "469:     checkFilterPredicate(longAttr > 3, classOf[Gt[_]], 4)",
          "470:     checkFilterPredicate(longAttr <= 1, classOf[LtEq[_]], 1)",
          "471:     checkFilterPredicate(longAttr >= 4, classOf[GtEq[_]], 4)",
          "473:     checkFilterPredicate(Literal(1) === longAttr, classOf[Eq[_]], 1)",
          "474:     checkFilterPredicate(Literal(1) <=> longAttr, classOf[Eq[_]], 1)",
          "475:     checkFilterPredicate(Literal(2) > longAttr, classOf[Lt[_]], 1)",
          "476:     checkFilterPredicate(Literal(3) < longAttr, classOf[Gt[_]], 4)",
          "477:     checkFilterPredicate(Literal(1) >= longAttr, classOf[LtEq[_]], 1)",
          "478:     checkFilterPredicate(Literal(4) <= longAttr, classOf[GtEq[_]], 4)",
          "480:     checkFilterPredicate(!(longAttr < 4), classOf[GtEq[_]], 4)",
          "481:     checkFilterPredicate(longAttr < 2 || longAttr > 3, classOf[Operators.Or],",
          "482:       Seq(Row(1), Row(4)))",
          "484:     Seq(3, 20).foreach { threshold =>",
          "485:       withSQLConf(SQLConf.PARQUET_FILTER_PUSHDOWN_INFILTERTHRESHOLD.key -> s\"$threshold\") {",
          "486:         checkFilterPredicate(",
          "487:           In(longAttr, Array(2L, 3L, 4L, 5L, 6L, 7L).map(Literal.apply)),",
          "488:           if (threshold == 3) classOf[FilterIn[_]] else classOf[Operators.Or],",
          "489:           Seq(Row(2L), Row(3L), Row(4L)))",
          "490:       }",
          "491:     }",
          "492:   }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "77b131396b773748faceb20b5efa6c2d2f9a01ae",
      "candidate_info": {
        "commit_hash": "77b131396b773748faceb20b5efa6c2d2f9a01ae",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/77b131396b773748faceb20b5efa6c2d2f9a01ae",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/mathExpressions.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/MathExpressionsSuite.scala",
          "sql/core/src/test/resources/sql-tests/inputs/ceil-floor-with-scale-param.sql",
          "sql/core/src/test/resources/sql-tests/results/ceil-floor-with-scale-param.sql.out",
          "sql/core/src/test/resources/sql-tests/results/postgreSQL/numeric.sql.out",
          "sql/core/src/test/resources/tpcds-query-results/v1_4/q2.sql.out",
          "sql/core/src/test/scala/org/apache/spark/sql/MathFunctionsSuite.scala"
        ],
        "message": "[SPARK-39226][SQL] Fix the precision of the return type of round-like functions\n\n### What changes were proposed in this pull request?\n\nCurrently, the precision of the return type of round-like functions (round, bround, ceil, floor) with decimal inputs has a few problems:\n1. It does not reserve one more digit in the integral part, in the case of rounding. As a result, `CEIL(CAST(99 AS DECIMAL(2, 0)), -1)` fails.\n2. It should return more accurate precision, to count for the scale loose. For example, `CEIL(1.23456, 1)` does not need to keep the precision as 7 in the result.\n3. `round`/`bround` with negative scale fails if the input is decimal type. This is not a bug but a little weird.\n\nThis PR fixes these issues by correcting the formula of calculating the returned decimal type.\n\n### Why are the changes needed?\n\nFix bugs\n\n### Does this PR introduce _any_ user-facing change?\n\nYes, the new functions in 3.3:`ceil`/`floor` with scale parameter, can report a more accurate precision in the result type, and can run some certain queries which failed before. The old functions: `round` and `bround`, can support negative scale parameter with decimal inputs.\n\n### How was this patch tested?\n\nnew tests\n\nCloses #36598 from cloud-fan/follow.\n\nAuthored-by: Wenchen Fan <wenchen@databricks.com>\nSigned-off-by: Gengliang Wang <gengliang@apache.org>\n(cherry picked from commit ee0aecca05af9b0cb256fd81a78430958a09d19f)\nSigned-off-by: Gengliang Wang <gengliang@apache.org>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/mathExpressions.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/mathExpressions.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/MathExpressionsSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/MathExpressionsSuite.scala",
          "sql/core/src/test/resources/sql-tests/inputs/ceil-floor-with-scale-param.sql||sql/core/src/test/resources/sql-tests/inputs/ceil-floor-with-scale-param.sql",
          "sql/core/src/test/scala/org/apache/spark/sql/MathFunctionsSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/MathFunctionsSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/mathExpressions.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/mathExpressions.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/mathExpressions.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/mathExpressions.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "317: }",
          "319: case class RoundCeil(child: Expression, scale: Expression)",
          "323:   override def inputTypes: Seq[AbstractDataType] = Seq(DecimalType, IntegerType)",
          "335:   override def nodeName: String = \"ceil\"",
          "337:   override protected def withNewChildrenInternal(",
          "",
          "[Removed Lines]",
          "320:   extends RoundBase(child, scale, BigDecimal.RoundingMode.CEILING, \"ROUND_CEILING\")",
          "321:     with ImplicitCastInputTypes {",
          "325:   override lazy val dataType: DataType = child.dataType match {",
          "326:     case DecimalType.Fixed(p, s) =>",
          "327:       if (_scale < 0) {",
          "328:         DecimalType(math.max(p, 1 - _scale), 0)",
          "329:       } else {",
          "330:         DecimalType(p, math.min(s, _scale))",
          "331:       }",
          "332:     case t => t",
          "333:   }",
          "",
          "[Added Lines]",
          "320:   extends RoundBase(child, scale, BigDecimal.RoundingMode.CEILING, \"ROUND_CEILING\") {",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "563: }",
          "565: case class RoundFloor(child: Expression, scale: Expression)",
          "569:   override def inputTypes: Seq[AbstractDataType] = Seq(DecimalType, IntegerType)",
          "581:   override def nodeName: String = \"floor\"",
          "583:   override protected def withNewChildrenInternal(",
          "",
          "[Removed Lines]",
          "566:   extends RoundBase(child, scale, BigDecimal.RoundingMode.FLOOR, \"ROUND_FLOOR\")",
          "567:     with ImplicitCastInputTypes {",
          "571:   override lazy val dataType: DataType = child.dataType match {",
          "572:     case DecimalType.Fixed(p, s) =>",
          "573:       if (_scale < 0) {",
          "574:         DecimalType(math.max(p, 1 - _scale), 0)",
          "575:       } else {",
          "576:         DecimalType(p, math.min(s, _scale))",
          "577:       }",
          "578:     case t => t",
          "579:   }",
          "",
          "[Added Lines]",
          "555:   extends RoundBase(child, scale, BigDecimal.RoundingMode.FLOOR, \"ROUND_FLOOR\") {",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "1447:   override def foldable: Boolean = child.foldable",
          "1449:   override lazy val dataType: DataType = child.dataType match {",
          "1453:     case t => t",
          "1454:   }",
          "",
          "[Removed Lines]",
          "1452:     case DecimalType.Fixed(p, s) => DecimalType(p, if (_scale > s) s else _scale)",
          "",
          "[Added Lines]",
          "1428:     case DecimalType.Fixed(p, s) =>",
          "1431:       val integralLeastNumDigits = p - s + 1",
          "1432:       if (_scale < 0) {",
          "1435:         val newPrecision = math.max(integralLeastNumDigits, -_scale + 1)",
          "1437:         DecimalType(math.min(newPrecision, DecimalType.MAX_PRECISION), 0)",
          "1438:       } else {",
          "1439:         val newScale = math.min(s, _scale)",
          "1441:         DecimalType(math.min(integralLeastNumDigits + newScale, 38), newScale)",
          "1442:       }",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "1616:     Examples:",
          "1617:       > SELECT _FUNC_(2.5, 0);",
          "1618:        3",
          "1619:   \"\"\",",
          "1620:   since = \"1.5.0\",",
          "1621:   group = \"math_funcs\")",
          "1623: case class Round(child: Expression, scale: Expression)",
          "1626:   def this(child: Expression) = this(child, Literal(0))",
          "1627:   override protected def withNewChildrenInternal(newLeft: Expression, newRight: Expression): Round =",
          "1628:     copy(child = newLeft, scale = newRight)",
          "",
          "[Removed Lines]",
          "1624:   extends RoundBase(child, scale, BigDecimal.RoundingMode.HALF_UP, \"ROUND_HALF_UP\")",
          "1625:     with Serializable with ImplicitCastInputTypes {",
          "",
          "[Added Lines]",
          "1609:       > SELECT _FUNC_(25, -1);",
          "1610:        30",
          "1616:   extends RoundBase(child, scale, BigDecimal.RoundingMode.HALF_UP, \"ROUND_HALF_UP\") {",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "1640:     Examples:",
          "1641:       > SELECT _FUNC_(2.5, 0);",
          "1642:        2",
          "1643:   \"\"\",",
          "1644:   since = \"2.0.0\",",
          "1645:   group = \"math_funcs\")",
          "1647: case class BRound(child: Expression, scale: Expression)",
          "1650:   def this(child: Expression) = this(child, Literal(0))",
          "1651:   override protected def withNewChildrenInternal(",
          "1652:     newLeft: Expression, newRight: Expression): BRound = copy(child = newLeft, scale = newRight)",
          "",
          "[Removed Lines]",
          "1648:   extends RoundBase(child, scale, BigDecimal.RoundingMode.HALF_EVEN, \"ROUND_HALF_EVEN\")",
          "1649:     with Serializable with ImplicitCastInputTypes {",
          "",
          "[Added Lines]",
          "1634:       > SELECT _FUNC_(25, -1);",
          "1635:        20",
          "1641:   extends RoundBase(child, scale, BigDecimal.RoundingMode.HALF_EVEN, \"ROUND_HALF_EVEN\") {",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/MathExpressionsSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/MathExpressionsSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/MathExpressionsSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/MathExpressionsSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "806:     checkEvaluation(Round(-3.5, 0), -4.0)",
          "807:     checkEvaluation(Round(-0.35, 1), -0.4)",
          "808:     checkEvaluation(Round(-35, -1), -40)",
          "809:     checkEvaluation(BRound(2.5, 0), 2.0)",
          "810:     checkEvaluation(BRound(3.5, 0), 4.0)",
          "811:     checkEvaluation(BRound(-2.5, 0), -2.0)",
          "812:     checkEvaluation(BRound(-3.5, 0), -4.0)",
          "813:     checkEvaluation(BRound(-0.35, 1), -0.4)",
          "814:     checkEvaluation(BRound(-35, -1), -40)",
          "815:     checkEvaluation(checkDataTypeAndCast(RoundFloor(Literal(2.5), Literal(0))), Decimal(2))",
          "816:     checkEvaluation(checkDataTypeAndCast(RoundFloor(Literal(3.5), Literal(0))), Decimal(3))",
          "817:     checkEvaluation(checkDataTypeAndCast(RoundFloor(Literal(-2.5), Literal(0))), Decimal(-3L))",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "809:     checkEvaluation(Round(BigDecimal(\"45.00\"), -1), BigDecimal(50))",
          "816:     checkEvaluation(BRound(BigDecimal(\"45.00\"), -1), BigDecimal(40))",
          "",
          "---------------"
        ],
        "sql/core/src/test/resources/sql-tests/inputs/ceil-floor-with-scale-param.sql||sql/core/src/test/resources/sql-tests/inputs/ceil-floor-with-scale-param.sql": [
          "File: sql/core/src/test/resources/sql-tests/inputs/ceil-floor-with-scale-param.sql -> sql/core/src/test/resources/sql-tests/inputs/ceil-floor-with-scale-param.sql",
          "--- Hunk 1 ---",
          "[Context before]",
          "8: SELECT CEIL(-0.1, 0);",
          "9: SELECT CEIL(5, 0);",
          "10: SELECT CEIL(3.14115, -3);",
          "11: SELECT CEIL(2.5, null);",
          "12: SELECT CEIL(2.5, 'a');",
          "13: SELECT CEIL(2.5, 0, 0);",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "11: SELECT CEIL(9.9, 0);",
          "12: SELECT CEIL(CAST(99 AS DECIMAL(2, 0)), -1);",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "22: SELECT FLOOR(-0.1, 0);",
          "23: SELECT FLOOR(5, 0);",
          "24: SELECT FLOOR(3.14115, -3);",
          "25: SELECT FLOOR(2.5, null);",
          "26: SELECT FLOOR(2.5, 'a');",
          "27: SELECT FLOOR(2.5, 0, 0);",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "27: SELECT FLOOR(-9.9, 0);",
          "28: SELECT FLOOR(CAST(-99 AS DECIMAL(2, 0)), -1);",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/MathFunctionsSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/MathFunctionsSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/MathFunctionsSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/MathFunctionsSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "208:           types.StructType(Seq(types.StructField(\"a\", types.LongType))))",
          "209:     assert(",
          "210:       spark.range(1).select(ceil(col(\"id\"), lit(0)).alias(\"a\")).schema ==",
          "212:     checkAnswer(",
          "213:       sql(\"SELECT ceiling(0), ceiling(1), ceiling(1.5)\"),",
          "214:       Row(0L, 1L, 2L))",
          "",
          "[Removed Lines]",
          "211:           types.StructType(Seq(types.StructField(\"a\", types.DecimalType(20, 0)))))",
          "",
          "[Added Lines]",
          "211:           types.StructType(Seq(types.StructField(\"a\", types.DecimalType(21, 0)))))",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "263:           types.StructType(Seq(types.StructField(\"a\", types.LongType))))",
          "264:     assert(",
          "265:       spark.range(1).select(floor(col(\"id\"), lit(0)).alias(\"a\")).schema ==",
          "267:   }",
          "269:   test(\"factorial\") {",
          "",
          "[Removed Lines]",
          "266:           types.StructType(Seq(types.StructField(\"a\", types.DecimalType(20, 0)))))",
          "",
          "[Added Lines]",
          "266:           types.StructType(Seq(types.StructField(\"a\", types.DecimalType(21, 0)))))",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "1dea5746fe5af42b121f5500d0f6c0b1a7947b88",
      "candidate_info": {
        "commit_hash": "1dea5746fe5af42b121f5500d0f6c0b1a7947b88",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/1dea5746fe5af42b121f5500d0f6c0b1a7947b88",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/generators.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/GeneratorFunctionSuite.scala"
        ],
        "message": "[SPARK-39496][SQL] Handle null struct in `Inline.eval`\n\n### What changes were proposed in this pull request?\n\nChange `Inline.eval` to return a row of null values rather than a null row in the case of a null input struct.\n\n### Why are the changes needed?\n\nConsider the following query:\n```\nset spark.sql.codegen.wholeStage=false;\nselect inline(array(named_struct('a', 1, 'b', 2), null));\n```\nThis query fails with a `NullPointerException`:\n```\n22/06/16 15:10:06 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)\njava.lang.NullPointerException\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.GenerateExec.$anonfun$doExecute$11(GenerateExec.scala:122)\n```\n(In Spark 3.1.3, you don't need to set `spark.sql.codegen.wholeStage` to false to reproduce the error, since Spark 3.1.3 has no codegen path for `Inline`).\n\nThis query fails regardless of the setting of `spark.sql.codegen.wholeStage`:\n```\nval dfWide = (Seq((1))\n  .toDF(\"col0\")\n  .selectExpr(Seq.tabulate(99)(x => s\"$x as col${x + 1}\"): _*))\n\nval df = (dfWide\n  .selectExpr(\"*\", \"array(named_struct('a', 1, 'b', 2), null) as struct_array\"))\n\ndf.selectExpr(\"*\", \"inline(struct_array)\").collect\n```\nIt fails with\n```\n22/06/16 15:18:55 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)/ 1]\njava.lang.NullPointerException\n\tat org.apache.spark.sql.catalyst.expressions.JoinedRow.isNullAt(JoinedRow.scala:80)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.writeFields_0_8$(Unknown Source)\n```\nWhen `Inline.eval` returns a null row in the collection, GenerateExec gets a NullPointerException either when joining the null row with required child output, or projecting the null row.\n\nThis PR avoids producing the null row and produces a row of null values instead:\n```\nspark-sql> set spark.sql.codegen.wholeStage=false;\nspark.sql.codegen.wholeStage\tfalse\nTime taken: 3.095 seconds, Fetched 1 row(s)\nspark-sql> select inline(array(named_struct('a', 1, 'b', 2), null));\n1\t2\nNULL\tNULL\nTime taken: 1.214 seconds, Fetched 2 row(s)\nspark-sql>\n```\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nNew unit test.\n\nCloses #36903 from bersprockets/inline_eval_null_struct_issue.\n\nAuthored-by: Bruce Robbins <bersprockets@gmail.com>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>\n(cherry picked from commit c4d5390dd032d17a40ad50e38f0ed7bd9bbd4698)\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/generators.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/generators.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/GeneratorFunctionSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/GeneratorFunctionSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/generators.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/generators.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/generators.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/generators.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "453:   private lazy val numFields = elementSchema.fields.length",
          "455:   override def eval(input: InternalRow): TraversableOnce[InternalRow] = {",
          "456:     val inputArray = child.eval(input).asInstanceOf[ArrayData]",
          "457:     if (inputArray == null) {",
          "458:       Nil",
          "459:     } else {",
          "462:     }",
          "463:   }",
          "",
          "[Removed Lines]",
          "460:       for (i <- 0 until inputArray.numElements())",
          "461:         yield inputArray.getStruct(i, numFields)",
          "",
          "[Added Lines]",
          "455:   private lazy val generatorNullRow = new GenericInternalRow(elementSchema.length)",
          "462:       for (i <- 0 until inputArray.numElements()) yield {",
          "463:         val s = inputArray.getStruct(i, numFields)",
          "464:         if (s == null) generatorNullRow else s",
          "465:       }",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/GeneratorFunctionSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/GeneratorFunctionSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/GeneratorFunctionSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/GeneratorFunctionSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "23: import org.apache.spark.sql.catalyst.expressions.codegen.Block._",
          "24: import org.apache.spark.sql.catalyst.trees.LeafLike",
          "25: import org.apache.spark.sql.functions._",
          "26: import org.apache.spark.sql.test.SharedSparkSession",
          "27: import org.apache.spark.sql.types.{IntegerType, StructType}",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "26: import org.apache.spark.sql.internal.SQLConf",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "389:     }",
          "390:   }",
          "393:     val df = sql(",
          "394:       \"\"\"select * from values",
          "395:         |(",
          "",
          "[Removed Lines]",
          "392:   test(\"SPARK-39061: inline should handle null struct\") {",
          "",
          "[Added Lines]",
          "393:   def testNullStruct(): Unit = {",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "413:       sql(\"select a, inline(b) from t1\"),",
          "414:       Row(1, 0, 1) :: Row(1, null, null) :: Row(1, 2, 3) :: Row(1, null, null) :: Nil)",
          "415:   }",
          "416: }",
          "418: case class EmptyGenerator() extends Generator with LeafLike[Expression] {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "418:   test(\"SPARK-39061: inline should handle null struct\") {",
          "419:     testNullStruct",
          "420:   }",
          "422:   test(\"SPARK-39496: inline eval path should handle null struct\") {",
          "423:     withSQLConf(SQLConf.WHOLESTAGE_CODEGEN_ENABLED.key -> \"false\") {",
          "424:       testNullStruct",
          "425:     }",
          "426:   }",
          "",
          "---------------"
        ]
      }
    }
  ]
}