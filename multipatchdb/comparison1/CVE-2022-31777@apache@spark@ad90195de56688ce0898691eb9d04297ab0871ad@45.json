{
  "cve_id": "CVE-2022-31777",
  "cve_desc": "A stored cross-site scripting (XSS) vulnerability in Apache Spark 3.2.1 and earlier, and 3.3.0, allows remote attackers to execute arbitrary JavaScript in the web browser of a user, by including a malicious payload into the logs which would be returned in logs rendered in the UI.",
  "repo": "apache/spark",
  "patch_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
  "patch_info": {
    "commit_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
    "repo": "apache/spark",
    "commit_url": "https://github.com/apache/spark/commit/ad90195de56688ce0898691eb9d04297ab0871ad",
    "files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ],
    "message": "[SPARK-39505][UI] Escape log content rendered in UI\n\n### What changes were proposed in this pull request?\n\nEscape log content rendered to the UI.\n\n### Why are the changes needed?\n\nLog content may contain reserved characters or other code in the log and be misinterpreted in the UI as HTML.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nExisting tests\n\nCloses #36902 from srowen/LogViewEscape.\n\nAuthored-by: Sean Owen <srowen@gmail.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
    "before_after_code_files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ]
  },
  "patch_diff": {
    "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js": [
      "File: core/src/main/resources/org/apache/spark/ui/static/log-view.js -> core/src/main/resources/org/apache/spark/ui/static/log-view.js",
      "--- Hunk 1 ---",
      "[Context before]",
      "85:       if (retStartByte == 0) {",
      "86:         disableMoreButton();",
      "87:       }",
      "90:       curLogLength = curLogLength + (startByte - retStartByte);",
      "91:       startByte = retStartByte;",
      "",
      "[Removed Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(cleanData);",
      "",
      "[Added Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(document.createTextNode(cleanData));",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "115:             var retLogLength = dataInfo[2];",
      "117:             var cleanData = data.substring(newlineIndex + 1);",
      "120:             curLogLength = curLogLength + (retEndByte - retStartByte);",
      "121:             endByte = retEndByte;",
      "",
      "[Removed Lines]",
      "118:             $(\"pre\", \".log-content\").append(cleanData);",
      "",
      "[Added Lines]",
      "118:             $(\"pre\", \".log-content\").append(document.createTextNode(cleanData));",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "84ed6142ef5f4caa4ef94685a8fc5e0105231aea",
      "candidate_info": {
        "commit_hash": "84ed6142ef5f4caa4ef94685a8fc5e0105231aea",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/84ed6142ef5f4caa4ef94685a8fc5e0105231aea",
        "files": [
          "sql/core/src/test/scala/org/apache/spark/sql/TPCDSQueryTestSuite.scala"
        ],
        "message": "[SPARK-39856][SQL][TESTS][FOLLOW-UP] Increase the number of partitions in TPC-DS build to avoid out-of-memory\n\n### What changes were proposed in this pull request?\n\nThis PR increases the number of partitions further more (see also https://github.com/apache/spark/pull/37270)\n\n### Why are the changes needed?\n\nTo make the build pass.\n\nAt least, two builds (https://github.com/apache/spark/runs/7500542538?check_suite_focus=true and https://github.com/apache/spark/runs/7511748355?check_suite_focus=true) passed after https://github.com/apache/spark/pull/37273. I assume that the number of partitions helps, and this PR increases some more.\n\n### Does this PR introduce _any_ user-facing change?\nNo, test and dev-only.\n\n### How was this patch tested?\n\nIt's tested in https://github.com/LuciferYang/spark/runs/7497163716?check_suite_focus=true\n\nCloses #37286 from HyukjinKwon/SPARK-39856-follwup.\n\nAuthored-by: Hyukjin Kwon <gurwls223@apache.org>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>\n(cherry picked from commit 72b55ccf8327c00e173ab6130fdb428ad0d5aacc)\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>",
        "before_after_code_files": [
          "sql/core/src/test/scala/org/apache/spark/sql/TPCDSQueryTestSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/TPCDSQueryTestSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/core/src/test/scala/org/apache/spark/sql/TPCDSQueryTestSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/TPCDSQueryTestSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/TPCDSQueryTestSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/TPCDSQueryTestSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "64:   override protected def sparkConf: SparkConf = super.sparkConf",
          "67:   protected override def createSparkSession: TestSparkSession = {",
          "68:     new TestSparkSession(new SparkContext(\"local[1]\", this.getClass.getSimpleName, sparkConf))",
          "",
          "[Removed Lines]",
          "65:     .set(SQLConf.SHUFFLE_PARTITIONS.key, 16.toString)",
          "",
          "[Added Lines]",
          "65:     .set(SQLConf.SHUFFLE_PARTITIONS.key, 32.toString)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "cf13262bc2d7ee1bce8c08292725353b2beccadd",
      "candidate_info": {
        "commit_hash": "cf13262bc2d7ee1bce8c08292725353b2beccadd",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/cf13262bc2d7ee1bce8c08292725353b2beccadd",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/v2AlterTableCommands.scala"
        ],
        "message": "[SPARK-38939][SQL][FOLLOWUP] Replace named parameter with comment in ReplaceColumns\n\n### What changes were proposed in this pull request?\n\nThis PR aims to replace named parameter with comment in `ReplaceColumns`.\n\n### Why are the changes needed?\n\n#36252 changed signature of deleteColumn#**TableChange.java**, but this PR breaks sbt compilation in k8s integration test.\n```shell\n> build/sbt -Pkubernetes -Pkubernetes-integration-tests -Dtest.exclude.tags=r -Dspark.kubernetes.test.imageRepo=kubespark \"kubernetes-integration-tests/test\"\n[error] /Users/IdeaProjects/spark/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/v2AlterTableCommands.scala:147:45: not found: value ifExists\n[error]       TableChange.deleteColumn(Array(name), ifExists = false)\n[error]                                             ^\n[error] /Users/IdeaProjects/spark/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/v2AlterTableCommands.scala:159:19: value ++ is not a member of Array[Nothing]\n[error]     deleteChanges ++ addChanges\n[error]                   ^\n[error] two errors found\n[error] (catalyst / Compile / compileIncremental) Compilation failed\n```\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nPass the GA and k8s integration test.\n\nCloses #36487 from dcoliversun/SPARK-38939.\n\nAuthored-by: Qian.Sun <qian.sun2020@gmail.com>\nSigned-off-by: huaxingao <huaxin_gao@apple.com>\n(cherry picked from commit 16b5124d75dc974c37f2fd87c78d231f8a3bf772)\nSigned-off-by: huaxingao <huaxin_gao@apple.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/v2AlterTableCommands.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/v2AlterTableCommands.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/v2AlterTableCommands.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/v2AlterTableCommands.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/v2AlterTableCommands.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/v2AlterTableCommands.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "144:     require(table.resolved)",
          "145:     val deleteChanges = table.schema.fieldNames.map { name =>",
          "148:     }",
          "149:     val addChanges = columnsToAdd.map { col =>",
          "150:       assert(col.path.isEmpty)",
          "",
          "[Removed Lines]",
          "147:       TableChange.deleteColumn(Array(name), ifExists = false)",
          "",
          "[Added Lines]",
          "147:       TableChange.deleteColumn(Array(name), false /* ifExists */)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "972338ae771c99fc63acb5f75fdfa2f6d2c0ffab",
      "candidate_info": {
        "commit_hash": "972338ae771c99fc63acb5f75fdfa2f6d2c0ffab",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/972338ae771c99fc63acb5f75fdfa2f6d2c0ffab",
        "files": [
          "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/Config.scala",
          "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesConf.scala",
          "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStep.scala",
          "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/DriverServiceFeatureStep.scala",
          "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/submit/KubernetesClientUtils.scala",
          "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStepSuite.scala"
        ],
        "message": "[SPARK-39614][K8S] K8s pod name follows `DNS Subdomain Names` rule\n\nThis PR aims to fix a regression at Apache Spark 3.3.0 which doesn't allow long pod name prefix whose length is greater than 63.\n\nAlthough Pod's `hostname` follows [DNS Label Names](https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#dns-label-names), Pod name itself follows [DNS Subdomain Names](https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#dns-subdomain-names) whose maximum length is 253.\n\nYes, this fixes a regression.\n\nPass the CIs with the updated unit tests.\n\nCloses #36999 from dongjoon-hyun/SPARK-39614.\n\nAuthored-by: Dongjoon Hyun <dongjoon@apache.org>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>\n(cherry picked from commit c15508f0d6a49738db5edf7eb139cc1d438af9a9)\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
        "before_after_code_files": [
          "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/Config.scala||resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/Config.scala",
          "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesConf.scala||resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesConf.scala",
          "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStep.scala||resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStep.scala",
          "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/DriverServiceFeatureStep.scala||resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/DriverServiceFeatureStep.scala",
          "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/submit/KubernetesClientUtils.scala||resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/submit/KubernetesClientUtils.scala",
          "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStepSuite.scala||resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStepSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/Config.scala||resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/Config.scala": [
          "File: resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/Config.scala -> resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/Config.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "323:   private def isValidExecutorPodNamePrefix(prefix: String): Boolean = {",
          "325:     val reservedLen = Int.MaxValue.toString.length + 6",
          "327:     validLength && podConfValidator.matcher(prefix).matches()",
          "328:   }",
          "",
          "[Removed Lines]",
          "326:     val validLength = prefix.length + reservedLen <= KUBERNETES_DNSNAME_MAX_LENGTH",
          "",
          "[Added Lines]",
          "326:     val validLength = prefix.length + reservedLen <= KUBERNETES_DNS_SUBDOMAIN_NAME_MAX_LENGTH",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "331:     ConfigBuilder(\"spark.kubernetes.executor.podNamePrefix\")",
          "332:       .doc(\"Prefix to use in front of the executor pod names. It must conform the rules defined \" +",
          "333:         \"by the Kubernetes <a href=\\\"https://kubernetes.io/docs/concepts/overview/\" +",
          "335:         \"The prefix will be used to generate executor pod names in the form of \" +",
          "336:         \"<code>$podNamePrefix-exec-$id</code>, where the `id` is a positive int value, \" +",
          "338:       .version(\"2.3.0\")",
          "339:       .stringConf",
          "340:       .checkValue(isValidExecutorPodNamePrefix,",
          "341:         \"must conform https://kubernetes.io/docs/concepts/overview/working-with-objects\" +",
          "343:       .createOptional",
          "345:   val KUBERNETES_EXECUTOR_DISABLE_CONFIGMAP =",
          "",
          "[Removed Lines]",
          "334:         \"working-with-objects/names/#dns-label-names\\\">DNS Label Names</a>. \" +",
          "337:         \"so the length of the `podNamePrefix` needs to be <= 47(= 63 - 10 - 6).\")",
          "342:           \"/names/#dns-label-names and the value length <= 47\")",
          "",
          "[Added Lines]",
          "334:         \"working-with-objects/names/#dns-subdomain-names\\\">DNS Subdomain Names</a>. \" +",
          "337:         \"so the length of the `podNamePrefix` needs to be <= 237(= 253 - 10 - 6).\")",
          "342:           \"/names/#dns-subdomain-names and the value length <= 237\")",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "714:   val KUBERNETES_DRIVER_ENV_PREFIX = \"spark.kubernetes.driverEnv.\"",
          "717: }",
          "",
          "[Removed Lines]",
          "716:   val KUBERNETES_DNSNAME_MAX_LENGTH = 63",
          "",
          "[Added Lines]",
          "716:   val KUBERNETES_DNS_SUBDOMAIN_NAME_MAX_LENGTH = 253",
          "717:   val KUBERNETES_DNS_LABEL_NAME_MAX_LENGTH = 63",
          "",
          "---------------"
        ],
        "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesConf.scala||resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesConf.scala": [
          "File: resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesConf.scala -> resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesConf.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "270:         .replaceAll(\"[^a-z0-9\\\\-]\", \"-\")",
          "271:         .replaceAll(\"-+\", \"-\"),",
          "272:       \"\",",
          "274:     ).stripPrefix(\"-\").stripSuffix(\"-\")",
          "275:   }",
          "",
          "[Removed Lines]",
          "273:       KUBERNETES_DNSNAME_MAX_LENGTH",
          "",
          "[Added Lines]",
          "273:       KUBERNETES_DNS_LABEL_NAME_MAX_LENGTH",
          "",
          "---------------"
        ],
        "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStep.scala||resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStep.scala": [
          "File: resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStep.scala -> resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStep.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "106:     val keyToPaths = KubernetesClientUtils.buildKeyToPathObjects(confFilesMap)",
          "114:       .replaceAll(\"^[^\\\\w]+\", \"\")",
          "",
          "[Removed Lines]",
          "112:     val hostname = name.substring(Math.max(0, name.length - KUBERNETES_DNSNAME_MAX_LENGTH))",
          "",
          "[Added Lines]",
          "112:     val hostname = name.substring(Math.max(0, name.length - KUBERNETES_DNS_LABEL_NAME_MAX_LENGTH))",
          "",
          "---------------"
        ],
        "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/DriverServiceFeatureStep.scala||resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/DriverServiceFeatureStep.scala": [
          "File: resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/DriverServiceFeatureStep.scala -> resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/DriverServiceFeatureStep.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "21: import io.fabric8.kubernetes.api.model.{HasMetadata, ServiceBuilder}",
          "23: import org.apache.spark.deploy.k8s.{KubernetesDriverConf, KubernetesUtils, SparkPod}",
          "25: import org.apache.spark.deploy.k8s.Constants._",
          "26: import org.apache.spark.internal.{config, Logging}",
          "27: import org.apache.spark.util.{Clock, SystemClock}",
          "",
          "[Removed Lines]",
          "24: import org.apache.spark.deploy.k8s.Config.KUBERNETES_DNSNAME_MAX_LENGTH",
          "",
          "[Added Lines]",
          "24: import org.apache.spark.deploy.k8s.Config.KUBERNETES_DNS_LABEL_NAME_MAX_LENGTH",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "101:   val DRIVER_BIND_ADDRESS_KEY = config.DRIVER_BIND_ADDRESS.key",
          "102:   val DRIVER_HOST_KEY = config.DRIVER_HOST_ADDRESS.key",
          "103:   val DRIVER_SVC_POSTFIX = \"-driver-svc\"",
          "105: }",
          "",
          "[Removed Lines]",
          "104:   val MAX_SERVICE_NAME_LENGTH = KUBERNETES_DNSNAME_MAX_LENGTH",
          "",
          "[Added Lines]",
          "104:   val MAX_SERVICE_NAME_LENGTH = KUBERNETES_DNS_LABEL_NAME_MAX_LENGTH",
          "",
          "---------------"
        ],
        "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/submit/KubernetesClientUtils.scala||resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/submit/KubernetesClientUtils.scala": [
          "File: resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/submit/KubernetesClientUtils.scala -> resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/submit/KubernetesClientUtils.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "30: import org.apache.spark.SparkConf",
          "31: import org.apache.spark.deploy.k8s.{Config, Constants, KubernetesUtils}",
          "33: import org.apache.spark.deploy.k8s.Constants.ENV_SPARK_CONF_DIR",
          "34: import org.apache.spark.internal.Logging",
          "36: private[spark] object KubernetesClientUtils extends Logging {",
          "39:   def configMapName(prefix: String): String = {",
          "40:     val suffix = \"-conf-map\"",
          "42:   }",
          "44:   val configMapNameExecutor: String = configMapName(s\"spark-exec-${KubernetesUtils.uniqueID()}\")",
          "",
          "[Removed Lines]",
          "32: import org.apache.spark.deploy.k8s.Config.{KUBERNETES_DNSNAME_MAX_LENGTH, KUBERNETES_NAMESPACE}",
          "41:     s\"${prefix.take(KUBERNETES_DNSNAME_MAX_LENGTH - suffix.length)}$suffix\"",
          "",
          "[Added Lines]",
          "32: import org.apache.spark.deploy.k8s.Config.{KUBERNETES_DNS_SUBDOMAIN_NAME_MAX_LENGTH, KUBERNETES_NAMESPACE}",
          "41:     s\"${prefix.take(KUBERNETES_DNS_SUBDOMAIN_NAME_MAX_LENGTH - suffix.length)}$suffix\"",
          "",
          "---------------"
        ],
        "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStepSuite.scala||resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStepSuite.scala": [
          "File: resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStepSuite.scala -> resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStepSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "199:       val step = new BasicExecutorFeatureStep(newExecutorConf(), new SecurityManager(baseConf),",
          "200:         defaultProfile)",
          "201:       assert(step.configurePod(SparkPod.initialPod()).pod.getSpec.getHostname.length ===",
          "203:     }",
          "204:   }",
          "206:   test(\"SPARK-35460: invalid PodNamePrefixes\") {",
          "207:     withPodNamePrefix {",
          "209:         baseConf.set(KUBERNETES_EXECUTOR_POD_NAME_PREFIX, invalid)",
          "210:         val e = intercept[IllegalArgumentException](newExecutorConf())",
          "211:         assert(e.getMessage === s\"'$invalid' in spark.kubernetes.executor.podNamePrefix is\" +",
          "212:           s\" invalid. must conform https://kubernetes.io/docs/concepts/overview/\" +",
          "214:       }",
          "215:     }",
          "216:   }",
          "",
          "[Removed Lines]",
          "202:         KUBERNETES_DNSNAME_MAX_LENGTH)",
          "208:       Seq(\"_123\", \"spark_exec\", \"spark@\", \"a\" * 48).foreach { invalid =>",
          "213:           \"working-with-objects/names/#dns-label-names and the value length <= 47\")",
          "",
          "[Added Lines]",
          "202:         KUBERNETES_DNS_LABEL_NAME_MAX_LENGTH)",
          "208:       Seq(\"_123\", \"spark_exec\", \"spark@\", \"a\" * 238).foreach { invalid =>",
          "213:           \"working-with-objects/names/#dns-subdomain-names and the value length <= 237\")",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "224:       val step = new BasicExecutorFeatureStep(newExecutorConf(), new SecurityManager(baseConf),",
          "225:         defaultProfile)",
          "226:       val hostname = step.configurePod(SparkPod.initialPod()).pod.getSpec().getHostname()",
          "228:       assert(InternetDomainName.isValid(hostname))",
          "229:     }",
          "230:   }",
          "",
          "[Removed Lines]",
          "227:       assert(hostname.length <= KUBERNETES_DNSNAME_MAX_LENGTH)",
          "",
          "[Added Lines]",
          "227:       assert(hostname.length <= KUBERNETES_DNS_LABEL_NAME_MAX_LENGTH)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "27c03e5741af25b7afacac727865e23f60ce61fa",
      "candidate_info": {
        "commit_hash": "27c03e5741af25b7afacac727865e23f60ce61fa",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/27c03e5741af25b7afacac727865e23f60ce61fa",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala"
        ],
        "message": "[SPARK-39175][SQL] Provide runtime error query context for Cast when WSCG is off\n\n### What changes were proposed in this pull request?\n\nSimilar to https://github.com/apache/spark/pull/36525, this PR provides runtime error query context for the Cast expression when WSCG is off.\n\n### Why are the changes needed?\n\nEnhance the runtime error query context of Cast expression. After changes, it works when the whole stage codegen is not available.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nUT\n\nCloses #36535 from gengliangwang/fixCastContext.\n\nAuthored-by: Gengliang Wang <gengliang@apache.org>\nSigned-off-by: Gengliang Wang <gengliang@apache.org>\n(cherry picked from commit cdd33e83c3919c4475e2e1ef387acb604bea81b9)\nSigned-off-by: Gengliang Wang <gengliang@apache.org>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala||sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "277:   }",
          "278: }",
          "282:   def child: Expression",
          "",
          "[Removed Lines]",
          "280: abstract class CastBase extends UnaryExpression with TimeZoneAwareExpression with NullIntolerant {",
          "",
          "[Added Lines]",
          "280: abstract class CastBase extends UnaryExpression",
          "281:     with TimeZoneAwareExpression",
          "282:     with NullIntolerant",
          "283:     with SupportQueryContext {",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "308:   protected def ansiEnabled: Boolean",
          "312:   override lazy val resolved: Boolean =",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "313:   override def initQueryContext(): String = if (ansiEnabled) {",
          "314:     origin.context",
          "315:   } else {",
          "316:     \"\"",
          "317:   }",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "467:           false",
          "468:         } else {",
          "469:           if (ansiEnabled) {",
          "471:           } else {",
          "472:             null",
          "473:           }",
          "",
          "[Removed Lines]",
          "470:             throw QueryExecutionErrors.invalidInputSyntaxForBooleanError(s, origin.context)",
          "",
          "[Added Lines]",
          "479:             throw QueryExecutionErrors.invalidInputSyntaxForBooleanError(s, queryContext)",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "499:     case StringType =>",
          "500:       buildCast[UTF8String](_, utfs => {",
          "501:         if (ansiEnabled) {",
          "503:         } else {",
          "504:           DateTimeUtils.stringToTimestamp(utfs, zoneId).orNull",
          "505:         }",
          "",
          "[Removed Lines]",
          "502:           DateTimeUtils.stringToTimestampAnsi(utfs, zoneId, origin.context)",
          "",
          "[Added Lines]",
          "511:           DateTimeUtils.stringToTimestampAnsi(utfs, zoneId, queryContext)",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "525:     case DoubleType =>",
          "526:       if (ansiEnabled) {",
          "528:       } else {",
          "529:         buildCast[Double](_, d => doubleToTimestamp(d))",
          "530:       }",
          "532:     case FloatType =>",
          "533:       if (ansiEnabled) {",
          "535:       } else {",
          "536:         buildCast[Float](_, f => doubleToTimestamp(f.toDouble))",
          "537:       }",
          "",
          "[Removed Lines]",
          "527:         buildCast[Double](_, d => doubleToTimestampAnsi(d, origin.context))",
          "534:         buildCast[Float](_, f => doubleToTimestampAnsi(f.toDouble, origin.context))",
          "",
          "[Added Lines]",
          "536:         buildCast[Double](_, d => doubleToTimestampAnsi(d, queryContext))",
          "543:         buildCast[Float](_, f => doubleToTimestampAnsi(f.toDouble, queryContext))",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "541:     case StringType =>",
          "542:       buildCast[UTF8String](_, utfs => {",
          "543:         if (ansiEnabled) {",
          "545:         } else {",
          "546:           DateTimeUtils.stringToTimestampWithoutTimeZone(utfs).orNull",
          "547:         }",
          "",
          "[Removed Lines]",
          "544:           DateTimeUtils.stringToTimestampWithoutTimeZoneAnsi(utfs, origin.context)",
          "",
          "[Added Lines]",
          "553:           DateTimeUtils.stringToTimestampWithoutTimeZoneAnsi(utfs, queryContext)",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "574:   private[this] def castToDate(from: DataType): Any => Any = from match {",
          "575:     case StringType =>",
          "576:       if (ansiEnabled) {",
          "578:       } else {",
          "579:         buildCast[UTF8String](_, s => DateTimeUtils.stringToDate(s).orNull)",
          "580:       }",
          "",
          "[Removed Lines]",
          "577:         buildCast[UTF8String](_, s => DateTimeUtils.stringToDateAnsi(s, origin.context))",
          "",
          "[Added Lines]",
          "586:         buildCast[UTF8String](_, s => DateTimeUtils.stringToDateAnsi(s, queryContext))",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "632:   private[this] def castToLong(from: DataType): Any => Any = from match {",
          "633:     case StringType if ansiEnabled =>",
          "635:     case StringType =>",
          "636:       val result = new LongWrapper()",
          "637:       buildCast[UTF8String](_, s => if (s.toLong(result)) result.value else null)",
          "",
          "[Removed Lines]",
          "634:       buildCast[UTF8String](_, v => UTF8StringUtils.toLongExact(v, origin.context))",
          "",
          "[Added Lines]",
          "643:       buildCast[UTF8String](_, v => UTF8StringUtils.toLongExact(v, queryContext))",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "655:   private[this] def castToInt(from: DataType): Any => Any = from match {",
          "656:     case StringType if ansiEnabled =>",
          "658:     case StringType =>",
          "659:       val result = new IntWrapper()",
          "660:       buildCast[UTF8String](_, s => if (s.toInt(result)) result.value else null)",
          "",
          "[Removed Lines]",
          "657:       buildCast[UTF8String](_, v => UTF8StringUtils.toIntExact(v, origin.context))",
          "",
          "[Added Lines]",
          "666:       buildCast[UTF8String](_, v => UTF8StringUtils.toIntExact(v, queryContext))",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "687:   private[this] def castToShort(from: DataType): Any => Any = from match {",
          "688:     case StringType if ansiEnabled =>",
          "690:     case StringType =>",
          "691:       val result = new IntWrapper()",
          "692:       buildCast[UTF8String](_, s => if (s.toShort(result)) {",
          "",
          "[Removed Lines]",
          "689:       buildCast[UTF8String](_, v => UTF8StringUtils.toShortExact(v, origin.context))",
          "",
          "[Added Lines]",
          "698:       buildCast[UTF8String](_, v => UTF8StringUtils.toShortExact(v, queryContext))",
          "",
          "---------------",
          "--- Hunk 11 ---",
          "[Context before]",
          "734:   private[this] def castToByte(from: DataType): Any => Any = from match {",
          "735:     case StringType if ansiEnabled =>",
          "737:     case StringType =>",
          "738:       val result = new IntWrapper()",
          "739:       buildCast[UTF8String](_, s => if (s.toByte(result)) {",
          "",
          "[Removed Lines]",
          "736:       buildCast[UTF8String](_, v => UTF8StringUtils.toByteExact(v, origin.context))",
          "",
          "[Added Lines]",
          "745:       buildCast[UTF8String](_, v => UTF8StringUtils.toByteExact(v, queryContext))",
          "",
          "---------------",
          "--- Hunk 12 ---",
          "[Context before]",
          "793:         null",
          "794:       } else {",
          "795:         throw QueryExecutionErrors.cannotChangeDecimalPrecisionError(",
          "797:       }",
          "798:     }",
          "799:   }",
          "",
          "[Removed Lines]",
          "796:           value, decimalType.precision, decimalType.scale, origin.context)",
          "",
          "[Added Lines]",
          "805:           value, decimalType.precision, decimalType.scale, queryContext)",
          "",
          "---------------",
          "--- Hunk 13 ---",
          "[Context before]",
          "816:       })",
          "817:     case StringType if ansiEnabled =>",
          "818:       buildCast[UTF8String](_,",
          "820:     case BooleanType =>",
          "821:       buildCast[Boolean](_, b => toPrecision(if (b) Decimal.ONE else Decimal.ZERO, target))",
          "822:     case DateType =>",
          "",
          "[Removed Lines]",
          "819:         s => changePrecision(Decimal.fromStringANSI(s, target, origin.context), target))",
          "",
          "[Added Lines]",
          "828:         s => changePrecision(Decimal.fromStringANSI(s, target, queryContext), target))",
          "",
          "---------------",
          "--- Hunk 14 ---",
          "[Context before]",
          "846:             val d = Cast.processFloatingPointSpecialLiterals(doubleStr, false)",
          "847:             if(ansiEnabled && d == null) {",
          "848:               throw QueryExecutionErrors.invalidInputSyntaxForNumericError(",
          "850:             } else {",
          "851:               d",
          "852:             }",
          "",
          "[Removed Lines]",
          "849:                 DoubleType, s, origin.context)",
          "",
          "[Added Lines]",
          "858:                 DoubleType, s, queryContext)",
          "",
          "---------------",
          "--- Hunk 15 ---",
          "[Context before]",
          "872:             val f = Cast.processFloatingPointSpecialLiterals(floatStr, true)",
          "873:             if (ansiEnabled && f == null) {",
          "874:               throw QueryExecutionErrors.invalidInputSyntaxForNumericError(",
          "876:             } else {",
          "877:               f",
          "878:             }",
          "",
          "[Removed Lines]",
          "875:                 FloatType, s, origin.context)",
          "",
          "[Added Lines]",
          "884:                 FloatType, s, queryContext)",
          "",
          "---------------",
          "--- Hunk 16 ---",
          "[Context before]",
          "988:   }",
          "990:   def errorContextCode(codegenContext: CodegenContext): String = {",
          "992:   }",
          "994:   override def doGenCode(ctx: CodegenContext, ev: ExprCode): ExprCode = {",
          "",
          "[Removed Lines]",
          "991:     codegenContext.addReferenceObj(\"errCtx\", origin.context)",
          "",
          "[Added Lines]",
          "1000:     codegenContext.addReferenceObj(\"errCtx\", queryContext)",
          "",
          "---------------",
          "--- Hunk 17 ---",
          "[Context before]",
          "1298:         val intOpt = ctx.freshVariable(\"intOpt\", classOf[Option[Integer]])",
          "1299:         (c, evPrim, evNull) =>",
          "1300:           if (ansiEnabled) {",
          "1302:             code\"\"\"",
          "1303:               $evPrim = $dateTimeUtilsCls.stringToDateAnsi($c, $errorContext);",
          "1304:             \"\"\"",
          "",
          "[Removed Lines]",
          "1301:             val errorContext = ctx.addReferenceObj(\"errCtx\", origin.context)",
          "",
          "[Added Lines]",
          "1310:             val errorContext = ctx.addReferenceObj(\"errCtx\", queryContext)",
          "",
          "---------------",
          "--- Hunk 18 ---",
          "[Context before]",
          "1377:               }",
          "1378:           \"\"\"",
          "1379:       case StringType if ansiEnabled =>",
          "1381:         val toType = ctx.addReferenceObj(\"toType\", target)",
          "1382:         (c, evPrim, evNull) =>",
          "1383:           code\"\"\"",
          "",
          "[Removed Lines]",
          "1380:         val errorContext = ctx.addReferenceObj(\"errCtx\", origin.context)",
          "",
          "[Added Lines]",
          "1389:         val errorContext = ctx.addReferenceObj(\"errCtx\", queryContext)",
          "",
          "---------------",
          "--- Hunk 19 ---",
          "[Context before]",
          "1438:       val longOpt = ctx.freshVariable(\"longOpt\", classOf[Option[Long]])",
          "1439:       (c, evPrim, evNull) =>",
          "1440:         if (ansiEnabled) {",
          "1442:           code\"\"\"",
          "1443:             $evPrim = $dateTimeUtilsCls.stringToTimestampAnsi($c, $zid, $errorContext);",
          "1444:            \"\"\"",
          "",
          "[Removed Lines]",
          "1441:           val errorContext = ctx.addReferenceObj(\"errCtx\", origin.context)",
          "",
          "[Added Lines]",
          "1450:           val errorContext = ctx.addReferenceObj(\"errCtx\", queryContext)",
          "",
          "---------------",
          "--- Hunk 20 ---",
          "[Context before]",
          "1477:     case DoubleType =>",
          "1478:       (c, evPrim, evNull) =>",
          "1479:         if (ansiEnabled) {",
          "1481:           code\"$evPrim = $dateTimeUtilsCls.doubleToTimestampAnsi($c, $errorContext);\"",
          "1482:         } else {",
          "1483:           code\"\"\"",
          "",
          "[Removed Lines]",
          "1480:           val errorContext = ctx.addReferenceObj(\"errCtx\", origin.context)",
          "",
          "[Added Lines]",
          "1489:           val errorContext = ctx.addReferenceObj(\"errCtx\", queryContext)",
          "",
          "---------------",
          "--- Hunk 21 ---",
          "[Context before]",
          "1491:     case FloatType =>",
          "1492:       (c, evPrim, evNull) =>",
          "1493:         if (ansiEnabled) {",
          "1495:           code\"$evPrim = $dateTimeUtilsCls.doubleToTimestampAnsi((double)$c, $errorContext);\"",
          "1496:         } else {",
          "1497:           code\"\"\"",
          "",
          "[Removed Lines]",
          "1494:           val errorContext = ctx.addReferenceObj(\"errCtx\", origin.context)",
          "",
          "[Added Lines]",
          "1503:           val errorContext = ctx.addReferenceObj(\"errCtx\", queryContext)",
          "",
          "---------------",
          "--- Hunk 22 ---",
          "[Context before]",
          "1511:       val longOpt = ctx.freshVariable(\"longOpt\", classOf[Option[Long]])",
          "1512:       (c, evPrim, evNull) =>",
          "1513:         if (ansiEnabled) {",
          "1515:           code\"\"\"",
          "1516:             $evPrim = $dateTimeUtilsCls.stringToTimestampWithoutTimeZoneAnsi($c, $errorContext);",
          "1517:            \"\"\"",
          "",
          "[Removed Lines]",
          "1514:           val errorContext = ctx.addReferenceObj(\"errCtx\", origin.context)",
          "",
          "[Added Lines]",
          "1523:           val errorContext = ctx.addReferenceObj(\"errCtx\", queryContext)",
          "",
          "---------------",
          "--- Hunk 23 ---",
          "[Context before]",
          "1628:       val stringUtils = inline\"${StringUtils.getClass.getName.stripSuffix(\"$\")}\"",
          "1629:       (c, evPrim, evNull) =>",
          "1630:         val castFailureCode = if (ansiEnabled) {",
          "1632:           s\"throw QueryExecutionErrors.invalidInputSyntaxForBooleanError($c, $errorContext);\"",
          "1633:         } else {",
          "1634:           s\"$evNull = true;\"",
          "",
          "[Removed Lines]",
          "1631:           val errorContext = ctx.addReferenceObj(\"errCtx\", origin.context)",
          "",
          "[Added Lines]",
          "1640:           val errorContext = ctx.addReferenceObj(\"errCtx\", queryContext)",
          "",
          "---------------",
          "--- Hunk 24 ---",
          "[Context before]",
          "1763:   private[this] def castToByteCode(from: DataType, ctx: CodegenContext): CastFunction = from match {",
          "1764:     case StringType if ansiEnabled =>",
          "1765:       val stringUtils = UTF8StringUtils.getClass.getCanonicalName.stripSuffix(\"$\")",
          "1767:       (c, evPrim, evNull) => code\"$evPrim = $stringUtils.toByteExact($c, $errorContext);\"",
          "1768:     case StringType =>",
          "1769:       val wrapper = ctx.freshVariable(\"intWrapper\", classOf[UTF8String.IntWrapper])",
          "",
          "[Removed Lines]",
          "1766:       val errorContext = ctx.addReferenceObj(\"errCtx\", origin.context)",
          "",
          "[Added Lines]",
          "1775:       val errorContext = ctx.addReferenceObj(\"errCtx\", queryContext)",
          "",
          "---------------",
          "--- Hunk 25 ---",
          "[Context before]",
          "1800:       ctx: CodegenContext): CastFunction = from match {",
          "1801:     case StringType if ansiEnabled =>",
          "1802:       val stringUtils = UTF8StringUtils.getClass.getCanonicalName.stripSuffix(\"$\")",
          "1804:       (c, evPrim, evNull) => code\"$evPrim = $stringUtils.toShortExact($c, $errorContext);\"",
          "1805:     case StringType =>",
          "1806:       val wrapper = ctx.freshVariable(\"intWrapper\", classOf[UTF8String.IntWrapper])",
          "",
          "[Removed Lines]",
          "1803:       val errorContext = ctx.addReferenceObj(\"errCtx\", origin.context)",
          "",
          "[Added Lines]",
          "1812:       val errorContext = ctx.addReferenceObj(\"errCtx\", queryContext)",
          "",
          "---------------",
          "--- Hunk 26 ---",
          "[Context before]",
          "1835:   private[this] def castToIntCode(from: DataType, ctx: CodegenContext): CastFunction = from match {",
          "1836:     case StringType if ansiEnabled =>",
          "1837:       val stringUtils = UTF8StringUtils.getClass.getCanonicalName.stripSuffix(\"$\")",
          "1839:       (c, evPrim, evNull) => code\"$evPrim = $stringUtils.toIntExact($c, $errorContext);\"",
          "1840:     case StringType =>",
          "1841:       val wrapper = ctx.freshVariable(\"intWrapper\", classOf[UTF8String.IntWrapper])",
          "",
          "[Removed Lines]",
          "1838:       val errorContext = ctx.addReferenceObj(\"errCtx\", origin.context)",
          "",
          "[Added Lines]",
          "1847:       val errorContext = ctx.addReferenceObj(\"errCtx\", queryContext)",
          "",
          "---------------",
          "--- Hunk 27 ---",
          "[Context before]",
          "1870:   private[this] def castToLongCode(from: DataType, ctx: CodegenContext): CastFunction = from match {",
          "1871:     case StringType if ansiEnabled =>",
          "1872:       val stringUtils = UTF8StringUtils.getClass.getCanonicalName.stripSuffix(\"$\")",
          "1874:       (c, evPrim, evNull) => code\"$evPrim = $stringUtils.toLongExact($c, $errorContext);\"",
          "1875:     case StringType =>",
          "1876:       val wrapper = ctx.freshVariable(\"longWrapper\", classOf[UTF8String.LongWrapper])",
          "",
          "[Removed Lines]",
          "1873:       val errorContext = ctx.addReferenceObj(\"errCtx\", origin.context)",
          "",
          "[Added Lines]",
          "1882:       val errorContext = ctx.addReferenceObj(\"errCtx\", queryContext)",
          "",
          "---------------",
          "--- Hunk 28 ---",
          "[Context before]",
          "1907:         val floatStr = ctx.freshVariable(\"floatStr\", StringType)",
          "1908:         (c, evPrim, evNull) =>",
          "1909:           val handleNull = if (ansiEnabled) {",
          "1911:             s\"throw QueryExecutionErrors.invalidInputSyntaxForNumericError(\" +",
          "1912:               s\"org.apache.spark.sql.types.FloatType$$.MODULE$$,$c, $errorContext);\"",
          "1913:           } else {",
          "",
          "[Removed Lines]",
          "1910:             val errorContext = ctx.addReferenceObj(\"errCtx\", origin.context)",
          "",
          "[Added Lines]",
          "1919:             val errorContext = ctx.addReferenceObj(\"errCtx\", queryContext)",
          "",
          "---------------",
          "--- Hunk 29 ---",
          "[Context before]",
          "1945:         val doubleStr = ctx.freshVariable(\"doubleStr\", StringType)",
          "1946:         (c, evPrim, evNull) =>",
          "1947:           val handleNull = if (ansiEnabled) {",
          "1949:             s\"throw QueryExecutionErrors.invalidInputSyntaxForNumericError(\" +",
          "1950:               s\"org.apache.spark.sql.types.DoubleType$$.MODULE$$, $c, $errorContext);\"",
          "1951:           } else {",
          "",
          "[Removed Lines]",
          "1948:             val errorContext = ctx.addReferenceObj(\"errCtx\", origin.context)",
          "",
          "[Added Lines]",
          "1957:             val errorContext = ctx.addReferenceObj(\"errCtx\", queryContext)",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala||sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "4359:     }",
          "4360:   }",
          "4363:     withSQLConf(SQLConf.WHOLESTAGE_CODEGEN_ENABLED.key -> \"false\",",
          "4364:       SQLConf.ANSI_ENABLED.key -> \"true\") {",
          "4365:       withTable(\"t\") {",
          "",
          "[Removed Lines]",
          "4362:   test(\"SPARK-39166: Query context should be serialized to executors when WSCG is off\") {",
          "",
          "[Added Lines]",
          "4362:   test(\"SPARK-39166: Query context of binary arithmetic should be serialized to executors\" +",
          "4363:     \" when WSCG is off\") {",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "4379:     }",
          "4380:   }",
          "4382:   test(\"SPARK-38589: try_avg should return null if overflow happens before merging\") {",
          "4383:     val yearMonthDf = Seq(Int.MaxValue, Int.MaxValue, 2)",
          "4384:       .map(Period.ofMonths)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "4383:   test(\"SPARK-39175: Query context of Cast should be serialized to executors\" +",
          "4384:     \" when WSCG is off\") {",
          "4385:     withSQLConf(SQLConf.WHOLESTAGE_CODEGEN_ENABLED.key -> \"false\",",
          "4386:       SQLConf.ANSI_ENABLED.key -> \"true\") {",
          "4387:       withTable(\"t\") {",
          "4388:         sql(\"create table t(s string) using parquet\")",
          "4389:         sql(\"insert into t values('a')\")",
          "4390:         Seq(",
          "4391:           \"select cast(s as int) from t\",",
          "4392:           \"select cast(s as long) from t\",",
          "4393:           \"select cast(s as double) from t\",",
          "4394:           \"select cast(s as decimal(10, 2)) from t\",",
          "4395:           \"select cast(s as date) from t\",",
          "4396:           \"select cast(s as timestamp) from t\",",
          "4397:           \"select cast(s as boolean) from t\").foreach { query =>",
          "4398:           val msg = intercept[SparkException] {",
          "4399:             sql(query).collect()",
          "4400:           }.getMessage",
          "4401:           assert(msg.contains(query))",
          "4402:         }",
          "4403:       }",
          "4404:     }",
          "4405:   }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "37aa0793ae1b4018eb331c1ccd4de9bd5aef9905",
      "candidate_info": {
        "commit_hash": "37aa0793ae1b4018eb331c1ccd4de9bd5aef9905",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/37aa0793ae1b4018eb331c1ccd4de9bd5aef9905",
        "files": [
          "sql/core/src/test/resources/sql-tests/inputs/ansi/conditional-functions.sql",
          "sql/core/src/test/resources/sql-tests/results/ansi/conditional-functions.sql.out"
        ],
        "message": "[SPARK-39040][SQL][FOLLOWUP] Use a unique table name in conditional-functions.sql\n\n### What changes were proposed in this pull request?\n\nThis is a followup of https://github.com/apache/spark/pull/36376, to use a unique table name in the test. `t` is a quite common table name and may make test environment unstable.\n\n### Why are the changes needed?\n\nmake tests more stable\n\n### Does this PR introduce _any_ user-facing change?\n\nno\n\n### How was this patch tested?\n\nN/A\n\nCloses #36739 from cloud-fan/test.\n\nAuthored-by: Wenchen Fan <wenchen@databricks.com>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>\n(cherry picked from commit 4f672db5719549c522a24cffe7b4d0c1e0cb859b)\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>",
        "before_after_code_files": [
          "sql/core/src/test/resources/sql-tests/inputs/ansi/conditional-functions.sql||sql/core/src/test/resources/sql-tests/inputs/ansi/conditional-functions.sql"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/core/src/test/resources/sql-tests/inputs/ansi/conditional-functions.sql||sql/core/src/test/resources/sql-tests/inputs/ansi/conditional-functions.sql": [
          "File: sql/core/src/test/resources/sql-tests/inputs/ansi/conditional-functions.sql -> sql/core/src/test/resources/sql-tests/inputs/ansi/conditional-functions.sql",
          "--- Hunk 1 ---",
          "[Context before]",
          "1: -- Tests for conditional functions",
          "10: SELECT if(1 == 1, 1, 1/0);",
          "11: SELECT if(1 != 1, 1/0, 1);",
          "14: SELECT coalesce(1, 1/0);",
          "15: SELECT coalesce(null, 1, 1/0);",
          "18: SELECT case when 1 < 2 then 1 else 1/0 end;",
          "19: SELECT case when 1 > 2 then 1/0 else 1 end;",
          "",
          "[Removed Lines]",
          "3: CREATE TABLE t USING PARQUET AS SELECT c1, c2 FROM VALUES(1d, 0),(2d, 1),(null, 1),(CAST('NaN' AS DOUBLE), 0) AS t(c1, c2);",
          "5: SELECT nanvl(c2, c1/c2 + c1/c2) FROM t;",
          "6: SELECT nanvl(c2, 1/0) FROM t;",
          "7: SELECT nanvl(1-0, 1/0) FROM t;",
          "9: SELECT if(c2 >= 0, 1-0, 1/0) from t;",
          "13: SELECT coalesce(c2, 1/0) from t;",
          "17: SELECT case when c2 >= 0 then 1 else 1/0 end from t;",
          "21: DROP TABLE IF EXISTS t;",
          "",
          "[Added Lines]",
          "3: CREATE TABLE conditional_t USING PARQUET AS SELECT c1, c2 FROM VALUES(1d, 0),(2d, 1),(null, 1),(CAST('NaN' AS DOUBLE), 0) AS t(c1, c2);",
          "5: SELECT nanvl(c2, c1/c2 + c1/c2) FROM conditional_t;",
          "6: SELECT nanvl(c2, 1/0) FROM conditional_t;",
          "7: SELECT nanvl(1-0, 1/0) FROM conditional_t;",
          "9: SELECT if(c2 >= 0, 1-0, 1/0) from conditional_t;",
          "13: SELECT coalesce(c2, 1/0) from conditional_t;",
          "17: SELECT case when c2 >= 0 then 1 else 1/0 end from conditional_t;",
          "21: DROP TABLE conditional_t;",
          "",
          "---------------"
        ]
      }
    }
  ]
}