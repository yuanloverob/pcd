{
  "cve_id": "CVE-2023-39441",
  "cve_desc": "Apache Airflow SMTP Provider before 1.3.0, Apache Airflow IMAP Provider before 3.3.0, and\u00a0Apache Airflow before 2.7.0 are affected by the\u00a0Validation of OpenSSL Certificate vulnerability.\n\nThe default SSL context with SSL library did not check a server's X.509\u00a0certificate.\u00a0 Instead, the code accepted any certificate, which could\u00a0result in the disclosure of mail server credentials or mail contents\u00a0when the client connects to an attacker in a MITM position.\n\nUsers are strongly advised to upgrade to Apache Airflow version 2.7.0 or newer, Apache Airflow IMAP Provider version 3.3.0 or newer, and Apache Airflow SMTP Provider version 1.3.0 or newer to mitigate the risk associated with this vulnerability",
  "repo": "apache/airflow",
  "patch_hash": "dbacacbd4d476da757de148a4e747924c34fd7fe",
  "patch_info": {
    "commit_hash": "dbacacbd4d476da757de148a4e747924c34fd7fe",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/dbacacbd4d476da757de148a4e747924c34fd7fe",
    "files": [
      "airflow/providers/smtp/CHANGELOG.rst",
      "airflow/providers/smtp/hooks/smtp.py",
      "airflow/providers/smtp/provider.yaml",
      "docs/apache-airflow-providers-smtp/configurations-ref.rst",
      "docs/apache-airflow-providers-smtp/index.rst",
      "docs/apache-airflow/configurations-ref.rst",
      "tests/providers/smtp/hooks/test_smtp.py"
    ],
    "message": "Allows to choose SSL context for SMTP provider (#33075)\n\n* Allows to choose SSL context for SMTP provider\n\nThis change add two options to choose from when SSL SMTP connection\nis created:\n\n* default - for balance between compatibility and security\n* none - in case compatibility with existing infrastructure is\n\u00a0 preferred\n\nThe fallback is:\n\n* The Airflow \"email\", \"ssl_context\"\n* \"default\"\n\n* Update airflow/providers/smtp/CHANGELOG.rst\n\nCo-authored-by: Ephraim Anierobi <splendidzigy24@gmail.com>\n(cherry picked from commit e20325db38fdfdd9db423a345b13d18aab6fe578)",
    "before_after_code_files": [
      "airflow/providers/smtp/hooks/smtp.py||airflow/providers/smtp/hooks/smtp.py",
      "tests/providers/smtp/hooks/test_smtp.py||tests/providers/smtp/hooks/test_smtp.py"
    ]
  },
  "patch_diff": {
    "airflow/providers/smtp/hooks/smtp.py||airflow/providers/smtp/hooks/smtp.py": [
      "File: airflow/providers/smtp/hooks/smtp.py -> airflow/providers/smtp/hooks/smtp.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "26: import os",
      "27: import re",
      "28: import smtplib",
      "29: from email.mime.application import MIMEApplication",
      "30: from email.mime.multipart import MIMEMultipart",
      "31: from email.mime.text import MIMEText",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "29: import ssl",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "109:             smtp_kwargs[\"port\"] = self.port",
      "110:         smtp_kwargs[\"timeout\"] = self.timeout",
      "112:         return SMTP(**smtp_kwargs)",
      "114:     @classmethod",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "112:         if self.use_ssl:",
      "113:             from airflow.configuration import conf",
      "115:             ssl_context_string = conf.get(\"smtp_provider\", \"SSL_CONTEXT\", fallback=None)",
      "116:             if ssl_context_string is None:",
      "117:                 ssl_context_string = conf.get(\"email\", \"SSL_CONTEXT\", fallback=None)",
      "118:             if ssl_context_string is None:",
      "119:                 ssl_context_string = \"default\"",
      "120:             if ssl_context_string == \"default\":",
      "121:                 ssl_context = ssl.create_default_context()",
      "122:             elif ssl_context_string == \"none\":",
      "123:                 ssl_context = None",
      "124:             else:",
      "125:                 raise RuntimeError(",
      "126:                     f\"The email.ssl_context configuration variable must \"",
      "127:                     f\"be set to 'default' or 'none' and is '{ssl_context_string}'.\"",
      "128:                 )",
      "129:             smtp_kwargs[\"context\"] = ssl_context",
      "",
      "---------------"
    ],
    "tests/providers/smtp/hooks/test_smtp.py||tests/providers/smtp/hooks/test_smtp.py": [
      "File: tests/providers/smtp/hooks/test_smtp.py -> tests/providers/smtp/hooks/test_smtp.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "30: from airflow.providers.smtp.hooks.smtp import SmtpHook",
      "31: from airflow.utils import db",
      "32: from airflow.utils.session import create_session",
      "34: smtplib_string = \"airflow.providers.smtp.hooks.smtp.smtplib\"",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "33: from tests.test_utils.config import conf_vars",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "75:         )",
      "77:     @patch(smtplib_string)",
      "79:         mock_conn = _create_fake_smtp(mock_smtplib)",
      "81:         with SmtpHook():",
      "82:             pass",
      "85:         mock_conn.login.assert_called_once_with(\"smtp_user\", \"smtp_password\")",
      "86:         assert mock_conn.close.call_count == 1",
      "",
      "[Removed Lines]",
      "78:     def test_connect_and_disconnect(self, mock_smtplib):",
      "84:         mock_smtplib.SMTP_SSL.assert_called_once_with(host=\"smtp_server_address\", port=465, timeout=30)",
      "",
      "[Added Lines]",
      "79:     @patch(\"ssl.create_default_context\")",
      "80:     def test_connect_and_disconnect(self, create_default_context, mock_smtplib):",
      "85:         assert create_default_context.called",
      "86:         mock_smtplib.SMTP_SSL.assert_called_once_with(",
      "87:             host=\"smtp_server_address\", port=465, timeout=30, context=create_default_context.return_value",
      "88:         )",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "202:     @patch(\"smtplib.SMTP_SSL\")",
      "203:     @patch(\"smtplib.SMTP\")",
      "205:         mock_smtp_ssl.return_value = Mock()",
      "206:         with SmtpHook() as smtp_hook:",
      "207:             smtp_hook.send_email_smtp(to=\"to\", subject=\"subject\", html_content=\"content\", from_email=\"from\")",
      "208:         assert not mock_smtp.called",
      "211:     @patch(\"smtplib.SMTP_SSL\")",
      "212:     @patch(\"smtplib.SMTP\")",
      "",
      "[Removed Lines]",
      "204:     def test_send_mime_ssl(self, mock_smtp, mock_smtp_ssl):",
      "209:         mock_smtp_ssl.assert_called_once_with(host=\"smtp_server_address\", port=465, timeout=30)",
      "",
      "[Added Lines]",
      "208:     @patch(\"ssl.create_default_context\")",
      "209:     def test_send_mime_ssl(self, create_default_context, mock_smtp, mock_smtp_ssl):",
      "214:         assert create_default_context.called",
      "215:         mock_smtp_ssl.assert_called_once_with(",
      "216:             host=\"smtp_server_address\", port=465, timeout=30, context=create_default_context.return_value",
      "217:         )",
      "219:     @patch(\"smtplib.SMTP_SSL\")",
      "220:     @patch(\"smtplib.SMTP\")",
      "221:     @patch(\"ssl.create_default_context\")",
      "222:     def test_send_mime_ssl_none_email_context(self, create_default_context, mock_smtp, mock_smtp_ssl):",
      "223:         mock_smtp_ssl.return_value = Mock()",
      "224:         with conf_vars({(\"smtp\", \"smtp_ssl\"): \"True\", (\"email\", \"ssl_context\"): \"none\"}):",
      "225:             with SmtpHook() as smtp_hook:",
      "226:                 smtp_hook.send_email_smtp(",
      "227:                     to=\"to\", subject=\"subject\", html_content=\"content\", from_email=\"from\"",
      "228:                 )",
      "229:         assert not mock_smtp.called",
      "230:         assert not create_default_context.called",
      "231:         mock_smtp_ssl.assert_called_once_with(host=\"smtp_server_address\", port=465, timeout=30, context=None)",
      "233:     @patch(\"smtplib.SMTP_SSL\")",
      "234:     @patch(\"smtplib.SMTP\")",
      "235:     @patch(\"ssl.create_default_context\")",
      "236:     def test_send_mime_ssl_none_smtp_provider_context(self, create_default_context, mock_smtp, mock_smtp_ssl):",
      "237:         mock_smtp_ssl.return_value = Mock()",
      "238:         with conf_vars({(\"smtp\", \"smtp_ssl\"): \"True\", (\"smtp_provider\", \"ssl_context\"): \"none\"}):",
      "239:             with SmtpHook() as smtp_hook:",
      "240:                 smtp_hook.send_email_smtp(",
      "241:                     to=\"to\", subject=\"subject\", html_content=\"content\", from_email=\"from\"",
      "242:                 )",
      "243:         assert not mock_smtp.called",
      "244:         assert not create_default_context.called",
      "245:         mock_smtp_ssl.assert_called_once_with(host=\"smtp_server_address\", port=465, timeout=30, context=None)",
      "247:     @patch(\"smtplib.SMTP_SSL\")",
      "248:     @patch(\"smtplib.SMTP\")",
      "249:     @patch(\"ssl.create_default_context\")",
      "250:     def test_send_mime_ssl_none_smtp_provider_default_email_context(",
      "251:         self, create_default_context, mock_smtp, mock_smtp_ssl",
      "252:     ):",
      "253:         mock_smtp_ssl.return_value = Mock()",
      "254:         with conf_vars(",
      "255:             {",
      "256:                 (\"smtp\", \"smtp_ssl\"): \"True\",",
      "257:                 (\"email\", \"ssl_context\"): \"default\",",
      "258:                 (\"smtp_provider\", \"ssl_context\"): \"none\",",
      "259:             }",
      "260:         ):",
      "261:             with SmtpHook() as smtp_hook:",
      "262:                 smtp_hook.send_email_smtp(",
      "263:                     to=\"to\", subject=\"subject\", html_content=\"content\", from_email=\"from\"",
      "264:                 )",
      "265:         assert not mock_smtp.called",
      "266:         assert not create_default_context.called",
      "267:         mock_smtp_ssl.assert_called_once_with(host=\"smtp_server_address\", port=465, timeout=30, context=None)",
      "269:     @patch(\"smtplib.SMTP_SSL\")",
      "270:     @patch(\"smtplib.SMTP\")",
      "271:     @patch(\"ssl.create_default_context\")",
      "272:     def test_send_mime_ssl_default_smtp_provider_none_email_context(",
      "273:         self, create_default_context, mock_smtp, mock_smtp_ssl",
      "274:     ):",
      "275:         mock_smtp_ssl.return_value = Mock()",
      "276:         with conf_vars(",
      "277:             {",
      "278:                 (\"smtp\", \"smtp_ssl\"): \"True\",",
      "279:                 (\"email\", \"ssl_context\"): \"none\",",
      "280:                 (\"smtp_provider\", \"ssl_context\"): \"default\",",
      "281:             }",
      "282:         ):",
      "283:             with SmtpHook() as smtp_hook:",
      "284:                 smtp_hook.send_email_smtp(",
      "285:                     to=\"to\", subject=\"subject\", html_content=\"content\", from_email=\"from\"",
      "286:                 )",
      "287:         assert not mock_smtp.called",
      "288:         assert create_default_context.called",
      "289:         mock_smtp_ssl.assert_called_once_with(",
      "290:             host=\"smtp_server_address\", port=465, timeout=30, context=create_default_context.return_value",
      "291:         )",
      "",
      "---------------",
      "--- Hunk 4 ---",
      "[Context before]",
      "270:     @patch(\"airflow.models.connection.Connection\")",
      "271:     @patch(\"smtplib.SMTP_SSL\")",
      "273:         mock_smtp_ssl().sendmail.side_effect = smtplib.SMTPServerDisconnected()",
      "274:         custom_retry_limit = 10",
      "275:         custom_timeout = 60",
      "",
      "[Removed Lines]",
      "272:     def test_send_mime_custom_timeout_retrylimit(self, mock_smtp_ssl, connection_mock):",
      "",
      "[Added Lines]",
      "354:     @patch(\"ssl.create_default_context\")",
      "355:     def test_send_mime_custom_timeout_retrylimit(",
      "356:         self, create_default_context, mock_smtp_ssl, connection_mock",
      "357:     ):",
      "",
      "---------------",
      "--- Hunk 5 ---",
      "[Context before]",
      "287:             with pytest.raises(smtplib.SMTPServerDisconnected):",
      "288:                 smtp_hook.send_email_smtp(to=\"to\", subject=\"subject\", html_content=\"content\")",
      "289:         mock_smtp_ssl.assert_any_call(",
      "291:         )",
      "292:         assert mock_smtp_ssl().sendmail.call_count == 10",
      "",
      "[Removed Lines]",
      "290:             host=fake_conn.host, port=fake_conn.port, timeout=fake_conn.extra_dejson[\"timeout\"]",
      "",
      "[Added Lines]",
      "375:             host=fake_conn.host,",
      "376:             port=fake_conn.port,",
      "377:             timeout=fake_conn.extra_dejson[\"timeout\"],",
      "378:             context=create_default_context.return_value,",
      "380:         assert create_default_context.called",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "855d4684e9a966801da921d7a145a52a2a645014",
      "candidate_info": {
        "commit_hash": "855d4684e9a966801da921d7a145a52a2a645014",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/855d4684e9a966801da921d7a145a52a2a645014",
        "files": [
          "tests/jobs/test_backfill_job.py",
          "tests/providers/openlineage/extractors/test_bash_extractor.py",
          "tests/providers/openlineage/extractors/test_python_extractor.py",
          "tests/system/providers/apache/kafka/example_dag_hello_kafka.py",
          "tests/system/providers/ftp/example_ftp.py",
          "tests/system/providers/google/cloud/compute/example_compute.py",
          "tests/system/providers/google/cloud/compute/example_compute_igm.py",
          "tests/system/providers/google/cloud/compute/example_compute_ssh.py",
          "tests/system/providers/google/cloud/dataproc/example_dataproc_spark_deferrable.py",
          "tests/ti_deps/deps/test_runnable_exec_date_dep.py"
        ],
        "message": "Clean `schedule_interval` usages from example dags (#33131)\n\n* Clean `schedule_interval` usages from example dags\n\n* Fix Pytest parameter name to match test argument\n\n---------\n\nCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>\n(cherry picked from commit cdea9f176ca8f9882bd41c9c198b03421dbfc298)",
        "before_after_code_files": [
          "tests/jobs/test_backfill_job.py||tests/jobs/test_backfill_job.py",
          "tests/providers/openlineage/extractors/test_bash_extractor.py||tests/providers/openlineage/extractors/test_bash_extractor.py",
          "tests/providers/openlineage/extractors/test_python_extractor.py||tests/providers/openlineage/extractors/test_python_extractor.py",
          "tests/system/providers/apache/kafka/example_dag_hello_kafka.py||tests/system/providers/apache/kafka/example_dag_hello_kafka.py",
          "tests/system/providers/ftp/example_ftp.py||tests/system/providers/ftp/example_ftp.py",
          "tests/system/providers/google/cloud/compute/example_compute.py||tests/system/providers/google/cloud/compute/example_compute.py",
          "tests/system/providers/google/cloud/compute/example_compute_igm.py||tests/system/providers/google/cloud/compute/example_compute_igm.py",
          "tests/system/providers/google/cloud/compute/example_compute_ssh.py||tests/system/providers/google/cloud/compute/example_compute_ssh.py",
          "tests/system/providers/google/cloud/dataproc/example_dataproc_spark_deferrable.py||tests/system/providers/google/cloud/dataproc/example_dataproc_spark_deferrable.py",
          "tests/ti_deps/deps/test_runnable_exec_date_dep.py||tests/ti_deps/deps/test_runnable_exec_date_dep.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/33247"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "tests/jobs/test_backfill_job.py||tests/jobs/test_backfill_job.py": [
          "File: tests/jobs/test_backfill_job.py -> tests/jobs/test_backfill_job.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "2054:     def test_backfill_disable_retry(self, dag_maker, disable_retry, try_number, exception):",
          "2055:         with dag_maker(",
          "2056:             dag_id=\"test_disable_retry\",",
          "2058:             default_args={",
          "2059:                 \"retries\": 2,",
          "2060:                 \"retry_delay\": datetime.timedelta(seconds=3),",
          "",
          "[Removed Lines]",
          "2057:             schedule_interval=\"@daily\",",
          "",
          "[Added Lines]",
          "2057:             schedule=\"@daily\",",
          "",
          "---------------"
        ],
        "tests/providers/openlineage/extractors/test_bash_extractor.py||tests/providers/openlineage/extractors/test_bash_extractor.py": [
          "File: tests/providers/openlineage/extractors/test_bash_extractor.py -> tests/providers/openlineage/extractors/test_bash_extractor.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "33: with DAG(",
          "34:     dag_id=\"test_dummy_dag\",",
          "35:     description=\"Test dummy DAG\",",
          "37:     start_date=datetime(2020, 1, 8),",
          "38:     catchup=False,",
          "39:     max_active_runs=1,",
          "",
          "[Removed Lines]",
          "36:     schedule_interval=\"*/2 * * * *\",",
          "",
          "[Added Lines]",
          "36:     schedule=\"*/2 * * * *\",",
          "",
          "---------------"
        ],
        "tests/providers/openlineage/extractors/test_python_extractor.py||tests/providers/openlineage/extractors/test_python_extractor.py": [
          "File: tests/providers/openlineage/extractors/test_python_extractor.py -> tests/providers/openlineage/extractors/test_python_extractor.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "35: dag = DAG(",
          "36:     dag_id=\"test_dummy_dag\",",
          "37:     description=\"Test dummy DAG\",",
          "39:     start_date=datetime(2020, 1, 8),",
          "40:     catchup=False,",
          "41:     max_active_runs=1,",
          "",
          "[Removed Lines]",
          "38:     schedule_interval=\"*/2 * * * *\",",
          "",
          "[Added Lines]",
          "38:     schedule=\"*/2 * * * *\",",
          "",
          "---------------"
        ],
        "tests/system/providers/apache/kafka/example_dag_hello_kafka.py||tests/system/providers/apache/kafka/example_dag_hello_kafka.py": [
          "File: tests/system/providers/apache/kafka/example_dag_hello_kafka.py -> tests/system/providers/apache/kafka/example_dag_hello_kafka.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "151:     \"kafka-example\",",
          "152:     default_args=default_args,",
          "153:     description=\"Examples of Kafka Operators\",",
          "155:     start_date=datetime(2021, 1, 1),",
          "156:     catchup=False,",
          "157:     tags=[\"example\"],",
          "",
          "[Removed Lines]",
          "154:     schedule_interval=timedelta(days=1),",
          "",
          "[Added Lines]",
          "154:     schedule=timedelta(days=1),",
          "",
          "---------------"
        ],
        "tests/system/providers/ftp/example_ftp.py||tests/system/providers/ftp/example_ftp.py": [
          "File: tests/system/providers/ftp/example_ftp.py -> tests/system/providers/ftp/example_ftp.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "35: with DAG(",
          "36:     DAG_ID,",
          "38:     start_date=datetime(2021, 1, 1),",
          "39:     catchup=False,",
          "40:     tags=[\"example\", \"Ftp\", \"FtpFileTransmit\", \"Ftps\", \"FtpsFileTransmit\"],",
          "",
          "[Removed Lines]",
          "37:     schedule_interval=\"@once\",",
          "",
          "[Added Lines]",
          "37:     schedule=\"@once\",",
          "",
          "---------------"
        ],
        "tests/system/providers/google/cloud/compute/example_compute.py||tests/system/providers/google/cloud/compute/example_compute.py": [
          "File: tests/system/providers/google/cloud/compute/example_compute.py -> tests/system/providers/google/cloud/compute/example_compute.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "101: with models.DAG(",
          "102:     DAG_ID,",
          "104:     start_date=datetime(2021, 1, 1),",
          "105:     catchup=False,",
          "106:     tags=[\"example\"],",
          "",
          "[Removed Lines]",
          "103:     schedule_interval=\"@once\",",
          "",
          "[Added Lines]",
          "103:     schedule=\"@once\",",
          "",
          "---------------"
        ],
        "tests/system/providers/google/cloud/compute/example_compute_igm.py||tests/system/providers/google/cloud/compute/example_compute_igm.py": [
          "File: tests/system/providers/google/cloud/compute/example_compute_igm.py -> tests/system/providers/google/cloud/compute/example_compute_igm.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "111: with models.DAG(",
          "112:     DAG_ID,",
          "114:     start_date=datetime(2021, 1, 1),",
          "115:     catchup=False,",
          "116:     tags=[\"example\"],",
          "",
          "[Removed Lines]",
          "113:     schedule_interval=\"@once\",",
          "",
          "[Added Lines]",
          "113:     schedule=\"@once\",",
          "",
          "---------------"
        ],
        "tests/system/providers/google/cloud/compute/example_compute_ssh.py||tests/system/providers/google/cloud/compute/example_compute_ssh.py": [
          "File: tests/system/providers/google/cloud/compute/example_compute_ssh.py -> tests/system/providers/google/cloud/compute/example_compute_ssh.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "72: with models.DAG(",
          "73:     DAG_ID,",
          "75:     start_date=datetime(2021, 1, 1),",
          "76:     catchup=False,",
          "77:     tags=[\"example\", \"compute-ssh\"],",
          "",
          "[Removed Lines]",
          "74:     schedule_interval=\"@once\",",
          "",
          "[Added Lines]",
          "74:     schedule=\"@once\",",
          "",
          "---------------"
        ],
        "tests/system/providers/google/cloud/dataproc/example_dataproc_spark_deferrable.py||tests/system/providers/google/cloud/dataproc/example_dataproc_spark_deferrable.py": [
          "File: tests/system/providers/google/cloud/dataproc/example_dataproc_spark_deferrable.py -> tests/system/providers/google/cloud/dataproc/example_dataproc_spark_deferrable.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "69: with models.DAG(",
          "70:     DAG_ID,",
          "72:     start_date=datetime(2021, 1, 1),",
          "73:     catchup=False,",
          "74:     tags=[\"example\", \"dataproc\", \"spark\", \"deferrable\"],",
          "",
          "[Removed Lines]",
          "71:     schedule_interval=\"@once\",",
          "",
          "[Added Lines]",
          "71:     schedule=\"@once\",",
          "",
          "---------------"
        ],
        "tests/ti_deps/deps/test_runnable_exec_date_dep.py||tests/ti_deps/deps/test_runnable_exec_date_dep.py": [
          "File: tests/ti_deps/deps/test_runnable_exec_date_dep.py -> tests/ti_deps/deps/test_runnable_exec_date_dep.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "39: @time_machine.travel(\"2016-11-01\")",
          "40: @pytest.mark.parametrize(",
          "42:     [",
          "43:         (True, None, datetime(2016, 11, 3), True),",
          "44:         (True, \"@daily\", datetime(2016, 11, 3), False),",
          "",
          "[Removed Lines]",
          "41:     \"allow_trigger_in_future,schedule_interval,execution_date,is_met\",",
          "",
          "[Added Lines]",
          "41:     \"allow_trigger_in_future,schedule,execution_date,is_met\",",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "53:     session,",
          "54:     create_dummy_dag,",
          "55:     allow_trigger_in_future,",
          "57:     execution_date,",
          "58:     is_met,",
          "59: ):",
          "60:     \"\"\"",
          "62:     this dep should fail",
          "63:     \"\"\"",
          "64:     with patch.object(settings, \"ALLOW_FUTURE_EXEC_DATES\", allow_trigger_in_future):",
          "",
          "[Removed Lines]",
          "56:     schedule_interval,",
          "61:     If the dag's execution date is in the future but (allow_trigger_in_future=False or not schedule_interval)",
          "",
          "[Added Lines]",
          "56:     schedule,",
          "61:     If the dag's execution date is in the future but (allow_trigger_in_future=False or not schedule)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "66:             \"test_localtaskjob_heartbeat\",",
          "67:             start_date=datetime(2015, 1, 1),",
          "68:             end_date=datetime(2016, 11, 5),",
          "70:             with_dagrun_type=DagRunType.MANUAL,",
          "71:             session=session,",
          "72:         )",
          "",
          "[Removed Lines]",
          "69:             schedule=schedule_interval,",
          "",
          "[Added Lines]",
          "69:             schedule=schedule,",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "c80d91be62b643fe21efca8fdc5a94d724cf3f58",
      "candidate_info": {
        "commit_hash": "c80d91be62b643fe21efca8fdc5a94d724cf3f58",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/c80d91be62b643fe21efca8fdc5a94d724cf3f58",
        "files": [
          "tests/models/test_xcom_arg_map.py"
        ],
        "message": "Attempt to stabilise tests for xcom_arg_map (#33150)\n\nSimilarly to #33145 - this is an attempt to stabilise flaky tests\nfor the test_xcom_arg_map.\n\nEven if the mechanism is not entirely clear (provide_session should\nalso close the connection) seems like using pytest-fixture provided\nsession works better than relying on a new session created in run()\nmethods.\n\n(cherry picked from commit 3dd0c999f1159a2fefbf32d9f10208a274a79a62)",
        "before_after_code_files": [
          "tests/models/test_xcom_arg_map.py||tests/models/test_xcom_arg_map.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/33247"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "tests/models/test_xcom_arg_map.py||tests/models/test_xcom_arg_map.py": [
          "File: tests/models/test_xcom_arg_map.py -> tests/models/test_xcom_arg_map.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "84:     # Run \"push\".",
          "85:     decision = dr.task_instance_scheduling_decisions(session=session)",
          "86:     for ti in decision.schedulable_tis:",
          "89:     # Run \"pull\". This should automatically convert \"c\" to None.",
          "90:     decision = dr.task_instance_scheduling_decisions(session=session)",
          "91:     for ti in decision.schedulable_tis:",
          "93:     assert results == {\"a\", \"b\", None}",
          "",
          "[Removed Lines]",
          "87:         ti.run()",
          "92:         ti.run()",
          "",
          "[Added Lines]",
          "87:         ti.run(session=session)",
          "92:         ti.run(session=session)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "118:     # Run \"push\".",
          "119:     decision = dr.task_instance_scheduling_decisions(session=session)",
          "120:     for ti in decision.schedulable_tis:",
          "123:     # Prepare to run \"pull\"...",
          "124:     decision = dr.task_instance_scheduling_decisions(session=session)",
          "125:     tis = {(ti.task_id, ti.map_index): ti for ti in decision.schedulable_tis}",
          "127:     # The first two \"pull\" tis should also succeed.",
          "131:     # But the third one fails because the map() result cannot be used as kwargs.",
          "132:     with pytest.raises(ValueError) as ctx:",
          "134:     assert str(ctx.value) == \"expand_kwargs() expects a list[dict], not list[None]\"",
          "136:     assert [tis[(\"pull\", i)].state for i in range(3)] == [",
          "",
          "[Removed Lines]",
          "121:         ti.run()",
          "128:     tis[(\"pull\", 0)].run()",
          "129:     tis[(\"pull\", 1)].run()",
          "133:         tis[(\"pull\", 2)].run()",
          "",
          "[Added Lines]",
          "121:         ti.run(session=session)",
          "128:     tis[(\"pull\", 0)].run(session=session)",
          "129:     tis[(\"pull\", 1)].run(session=session)",
          "133:         tis[(\"pull\", 2)].run(session=session)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "163:     # The \"push\" task should not fail.",
          "164:     decision = dr.task_instance_scheduling_decisions(session=session)",
          "165:     for ti in decision.schedulable_tis:",
          "167:     assert [ti.state for ti in decision.schedulable_tis] == [TaskInstanceState.SUCCESS]",
          "169:     # Prepare to run \"pull\"...",
          "",
          "[Removed Lines]",
          "166:         ti.run()",
          "",
          "[Added Lines]",
          "166:         ti.run(session=session)",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "171:     tis = {(ti.task_id, ti.map_index): ti for ti in decision.schedulable_tis}",
          "173:     # The first two \"pull\" tis should also succeed.",
          "177:     # But the third one (for \"c\") will fail.",
          "178:     with pytest.raises(ValueError) as ctx:",
          "180:     assert str(ctx.value) == \"nope\"",
          "182:     assert [tis[(\"pull\", i)].state for i in range(3)] == [",
          "",
          "[Removed Lines]",
          "174:     tis[(\"pull\", 0)].run()",
          "175:     tis[(\"pull\", 1)].run()",
          "179:         tis[(\"pull\", 2)].run()",
          "",
          "[Added Lines]",
          "174:     tis[(\"pull\", 0)].run(session=session)",
          "175:     tis[(\"pull\", 1)].run(session=session)",
          "179:         tis[(\"pull\", 2)].run(session=session)",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "216:     # Run \"push\".",
          "217:     decision = dr.task_instance_scheduling_decisions(session=session)",
          "218:     for ti in decision.schedulable_tis:",
          "221:     # Run \"forward\". This should automatically skip \"c\".",
          "222:     decision = dr.task_instance_scheduling_decisions(session=session)",
          "223:     for ti in decision.schedulable_tis:",
          "226:     # Now \"collect\" should only get \"a\" and \"b\".",
          "227:     decision = dr.task_instance_scheduling_decisions(session=session)",
          "228:     for ti in decision.schedulable_tis:",
          "230:     assert result == [\"a\", \"b\"]",
          "",
          "[Removed Lines]",
          "219:         ti.run()",
          "224:         ti.run()",
          "229:         ti.run()",
          "",
          "[Added Lines]",
          "219:         ti.run(session=session)",
          "224:         ti.run(session=session)",
          "229:         ti.run(session=session)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "7463380047347e8fe98202d3b272a1b60e8db5b9",
      "candidate_info": {
        "commit_hash": "7463380047347e8fe98202d3b272a1b60e8db5b9",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/7463380047347e8fe98202d3b272a1b60e8db5b9",
        "files": [
          "airflow/www/views.py",
          "tests/www/views/test_views_tasks.py"
        ],
        "message": "Fix xcom view returning bytes as xcom value (#33202)\n\n(cherry picked from commit 36c2735ca48d4c2e2d239c210d3732fd8918fed2)",
        "before_after_code_files": [
          "airflow/www/views.py||airflow/www/views.py",
          "tests/www/views/test_views_tasks.py||tests/www/views/test_views_tasks.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/33247"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/www/views.py||airflow/www/views.py": [
          "File: airflow/www/views.py -> airflow/www/views.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1926:             flash(f\"Task [{dag_id}.{task_id}] doesn't seem to exist at the moment\", \"error\")",
          "1927:             return redirect(url_for(\"Airflow.index\"))",
          "1931:                 XCom.dag_id == dag_id,",
          "1932:                 XCom.task_id == task_id,",
          "1933:                 XCom.execution_date == dttm,",
          "1934:                 XCom.map_index == map_index,",
          "1935:             )",
          "1936:         )",
          "1939:         title = \"XCom\"",
          "1940:         return self.render_template(",
          "",
          "[Removed Lines]",
          "1929:         xcom_query = session.execute(",
          "1930:             select(XCom.key, XCom.value).where(",
          "1937:         attributes = [(k, v) for k, v in xcom_query if not k.startswith(\"_\")]",
          "",
          "[Added Lines]",
          "1929:         xcom_query = session.scalars(",
          "1930:             select(XCom).where(",
          "1937:         attributes = [(xcom.key, xcom.value) for xcom in xcom_query if not xcom.key.startswith(\"_\")]",
          "",
          "---------------"
        ],
        "tests/www/views/test_views_tasks.py||tests/www/views/test_views_tasks.py": [
          "File: tests/www/views/test_views_tasks.py -> tests/www/views/test_views_tasks.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "30: from airflow import settings",
          "31: from airflow.exceptions import AirflowException",
          "33: from airflow.models.dagcode import DagCode",
          "34: from airflow.operators.bash import BashOperator",
          "35: from airflow.providers.celery.executors.celery_executor import CeleryExecutor",
          "",
          "[Removed Lines]",
          "32: from airflow.models import DAG, DagBag, DagModel, TaskFail, TaskInstance, TaskReschedule",
          "",
          "[Added Lines]",
          "32: from airflow.models import DAG, DagBag, DagModel, TaskFail, TaskInstance, TaskReschedule, XCom",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "42: from airflow.www.views import TaskInstanceModelView",
          "43: from tests.test_utils.api_connexion_utils import create_user, delete_roles, delete_user",
          "44: from tests.test_utils.config import conf_vars",
          "46: from tests.test_utils.www import check_content_in_response, check_content_not_in_response, client_with_login",
          "48: DEFAULT_DATE = timezone.utcnow().replace(hour=0, minute=0, second=0, microsecond=0)",
          "",
          "[Removed Lines]",
          "45: from tests.test_utils.db import clear_db_runs",
          "",
          "[Added Lines]",
          "45: from tests.test_utils.db import clear_db_runs, clear_db_xcom",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "69:             start_date=timezone.utcnow(),",
          "70:             state=State.RUNNING,",
          "71:         )",
          "72:         app.dag_bag.get_dag(\"example_subdag_operator\").create_dagrun(",
          "73:             run_id=DEFAULT_DAGRUN,",
          "74:             run_type=DagRunType.SCHEDULED,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "72:         XCom.set(",
          "73:             key=\"return_value\",",
          "74:             value=\"{'x':1}\",",
          "75:             task_id=\"runme_0\",",
          "76:             dag_id=\"example_bash_operator\",",
          "77:             execution_date=DEFAULT_DATE,",
          "78:         )",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "103:         )",
          "104:     yield",
          "105:     clear_db_runs()",
          "108: @pytest.fixture(scope=\"module\")",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "113:     clear_db_xcom()",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "331:         check_content_in_response(content, resp)",
          "334: def test_rendered_task_view(admin_client):",
          "335:     url = f\"task?task_id=runme_0&dag_id=example_bash_operator&execution_date={DEFAULT_VAL}\"",
          "336:     resp = admin_client.get(url, follow_redirects=True)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "342: def test_xcom_return_value_is_not_bytes(admin_client):",
          "343:     url = f\"xcom?dag_id=example_bash_operator&task_id=runme_0&execution_date={DEFAULT_VAL}&map_index=-1\"",
          "344:     resp = admin_client.get(url, follow_redirects=True)",
          "345:     # check that {\"x\":1} is in the response",
          "346:     content = \"{&#39;x&#39;:1}\"",
          "347:     check_content_in_response(content, resp)",
          "348:     # check that b'{\"x\":1}' is not in the response",
          "349:     content = \"b&#39;&#34;{\\\\&#39;x\\\\&#39;:1}&#34;&#39;\"",
          "350:     check_content_not_in_response(content, resp)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "127445f577e34b7b0506a39c898bc335a32cd53f",
      "candidate_info": {
        "commit_hash": "127445f577e34b7b0506a39c898bc335a32cd53f",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/127445f577e34b7b0506a39c898bc335a32cd53f",
        "files": [
          ".pre-commit-config.yaml",
          "airflow/models/dag.py",
          "airflow/providers/qubole/hooks/qubole.py",
          "dev/breeze/src/airflow_breeze/commands/release_candidate_command.py",
          "scripts/ci/pre_commit/pre_commit_insert_extras.py",
          "scripts/ci/pre_commit/pre_commit_local_yml_mounts.py"
        ],
        "message": "Upgrade ruff to latest 0.0.282 version in pre-commits (#33152)\n\n(cherry picked from commit 6b892bf21f69a41caf670e2c498aea1c83086848)",
        "before_after_code_files": [
          "airflow/models/dag.py||airflow/models/dag.py",
          "airflow/providers/qubole/hooks/qubole.py||airflow/providers/qubole/hooks/qubole.py",
          "dev/breeze/src/airflow_breeze/commands/release_candidate_command.py||dev/breeze/src/airflow_breeze/commands/release_candidate_command.py",
          "scripts/ci/pre_commit/pre_commit_insert_extras.py||scripts/ci/pre_commit/pre_commit_insert_extras.py",
          "scripts/ci/pre_commit/pre_commit_local_yml_mounts.py||scripts/ci/pre_commit/pre_commit_local_yml_mounts.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/33247"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/models/dag.py||airflow/models/dag.py": [
          "File: airflow/models/dag.py -> airflow/models/dag.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "2219:             return 0",
          "2220:         if confirm_prompt:",
          "2221:             ti_list = \"\\n\".join(str(t) for t in tis)",
          "2225:             do_it = utils.helpers.ask_yesno(question)",
          "2227:         if do_it:",
          "",
          "[Removed Lines]",
          "2222:             question = (",
          "2223:                 \"You are about to delete these {count} tasks:\\n{ti_list}\\n\\nAre you sure? [y/n]\"",
          "2224:             ).format(count=count, ti_list=ti_list)",
          "",
          "[Added Lines]",
          "2222:             question = f\"You are about to delete these {count} tasks:\\n{ti_list}\\n\\nAre you sure? [y/n]\"",
          "",
          "---------------"
        ],
        "airflow/providers/qubole/hooks/qubole.py||airflow/providers/qubole/hooks/qubole.py": [
          "File: airflow/providers/qubole/hooks/qubole.py -> airflow/providers/qubole/hooks/qubole.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "185:             )",
          "187:         if self.cmd.status != \"done\":  # type: ignore[attr-defined]",
          "194:     def kill(self, ti):",
          "195:         \"\"\"",
          "",
          "[Removed Lines]",
          "188:             raise AirflowException(",
          "189:                 \"Command Id: {} failed with Status: {}\".format(",
          "190:                     self.cmd.id, self.cmd.status  # type: ignore[attr-defined]",
          "191:                 )",
          "192:             )",
          "",
          "[Added Lines]",
          "188:             raise AirflowException(f\"Command Id: {self.cmd.id} failed with Status: {self.cmd.status}\")",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/commands/release_candidate_command.py||dev/breeze/src/airflow_breeze/commands/release_candidate_command.py": [
          "File: dev/breeze/src/airflow_breeze/commands/release_candidate_command.py -> dev/breeze/src/airflow_breeze/commands/release_candidate_command.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "203:             \"environment. The package download link is available at: \"",
          "204:             \"https://test.pypi.org/project/apache-airflow/#files \"",
          "205:             \"Install it with the appropriate constraint file, adapt python version: \"",
          "207:         )",
          "",
          "[Removed Lines]",
          "206:             f\"pip install -i https://test.pypi.org/simple/ --extra-index-url https://pypi.org/simple/apache-airflow=={version} --constraint https://raw.githubusercontent.com/apache/airflow/constraints-{version}/constraints-3.8.txt\"  # noqa: 501",
          "",
          "[Added Lines]",
          "206:             f\"pip install -i https://test.pypi.org/simple/ --extra-index-url \"",
          "207:             f\"https://pypi.org/simple/apache-airflow=={version} --constraint \"",
          "208:             f\"https://raw.githubusercontent.com/apache/airflow/\"",
          "209:             f\"constraints-{version}/constraints-3.8.txt\"",
          "",
          "---------------"
        ],
        "scripts/ci/pre_commit/pre_commit_insert_extras.py||scripts/ci/pre_commit/pre_commit_insert_extras.py": [
          "File: scripts/ci/pre_commit/pre_commit_insert_extras.py -> scripts/ci/pre_commit/pre_commit_insert_extras.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "31: os.environ[\"_SKIP_PYTHON_VERSION_CHECK\"] = \"true\"",
          "36: sys.path.append(str(AIRFLOW_SOURCES_DIR))",
          "",
          "[Removed Lines]",
          "33: from common_precommit_utils import insert_documentation  # isort: skip",
          "34: from setup import EXTRAS_DEPENDENCIES  # isort:skip",
          "",
          "[Added Lines]",
          "33: from common_precommit_utils import insert_documentation  # isort: skip  # noqa: E402",
          "34: from setup import EXTRAS_DEPENDENCIES  # isort:skip  # noqa: E402",
          "",
          "---------------"
        ],
        "scripts/ci/pre_commit/pre_commit_local_yml_mounts.py||scripts/ci/pre_commit/pre_commit_local_yml_mounts.py": [
          "File: scripts/ci/pre_commit/pre_commit_local_yml_mounts.py -> scripts/ci/pre_commit/pre_commit_local_yml_mounts.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "23: sys.path.insert(0, str(Path(__file__).parent.resolve()))  # make sure common_precommit_utils is imported",
          "27: sys.path.insert(0, str(AIRFLOW_SOURCES_ROOT_PATH))  # make sure setup is imported from Airflow",
          "28: sys.path.insert(",
          "29:     0, str(AIRFLOW_SOURCES_ROOT_PATH / \"dev\" / \"breeze\" / \"src\")",
          "30: )  # make sure setup is imported from Airflow",
          "31: # flake8: noqa: F401",
          "36: sys.path.append(str(AIRFLOW_SOURCES_ROOT_PATH))",
          "",
          "[Removed Lines]",
          "25: from common_precommit_utils import AIRFLOW_SOURCES_ROOT_PATH  # isort: skip",
          "32: from airflow_breeze.utils.docker_command_utils import VOLUMES_FOR_SELECTED_MOUNTS  # isort: skip",
          "34: from common_precommit_utils import insert_documentation  # isort: skip",
          "",
          "[Added Lines]",
          "25: from common_precommit_utils import AIRFLOW_SOURCES_ROOT_PATH  # isort: skip  # noqa: E402",
          "32: from airflow_breeze.utils.docker_command_utils import VOLUMES_FOR_SELECTED_MOUNTS  # isort: skip  # noqa: E402",
          "34: from common_precommit_utils import insert_documentation  # isort: skip # noqa: E402",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "29dbd3265748feb49d94c12410392999618ab3f9",
      "candidate_info": {
        "commit_hash": "29dbd3265748feb49d94c12410392999618ab3f9",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/29dbd3265748feb49d94c12410392999618ab3f9",
        "files": [
          "airflow/utils/task_group.py",
          "tests/utils/test_task_group.py"
        ],
        "message": "Don't ignore setups when arrowing from group (#33097)\n\nThis enables us to have a group with just setups in it.\n\n(cherry picked from commit cd7e7bcb2310dea19f7ee946716a7c91ed610c68)",
        "before_after_code_files": [
          "airflow/utils/task_group.py||airflow/utils/task_group.py",
          "tests/utils/test_task_group.py||tests/utils/test_task_group.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/33247"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/utils/task_group.py||airflow/utils/task_group.py": [
          "File: airflow/utils/task_group.py -> airflow/utils/task_group.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "370:         tasks = list(self)",
          "371:         ids = {x.task_id for x in tasks}",
          "374:             for upstream_task in task.upstream_list:",
          "375:                 if upstream_task.task_id not in ids:",
          "376:                     continue",
          "379:                 else:",
          "380:                     yield upstream_task",
          "382:         for task in tasks:",
          "383:             if task.downstream_task_ids.isdisjoint(ids):",
          "385:                     yield task",
          "386:                 else:",
          "389:     def child_id(self, label):",
          "390:         \"\"\"Prefix label with group_id if prefix_group_id is True. Otherwise return the label as-is.\"\"\"",
          "",
          "[Removed Lines]",
          "373:         def recurse_for_first_non_setup_teardown(task):",
          "377:                 if upstream_task.is_setup or upstream_task.is_teardown:",
          "378:                     yield from recurse_for_first_non_setup_teardown(upstream_task)",
          "384:                 if not (task.is_teardown or task.is_setup):",
          "387:                     yield from recurse_for_first_non_setup_teardown(task)",
          "",
          "[Added Lines]",
          "373:         def recurse_for_first_non_teardown(task):",
          "376:                     # upstream task is not in task group",
          "377:                     continue",
          "378:                 elif upstream_task.is_teardown:",
          "379:                     yield from recurse_for_first_non_teardown(upstream_task)",
          "380:                 elif task.is_teardown and upstream_task.is_setup:",
          "381:                     # don't go through the teardown-to-setup path",
          "388:                 if not task.is_teardown:",
          "391:                     yield from recurse_for_first_non_teardown(task)",
          "",
          "---------------"
        ],
        "tests/utils/test_task_group.py||tests/utils/test_task_group.py": [
          "File: tests/utils/test_task_group.py -> tests/utils/test_task_group.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "22: import pendulum",
          "23: import pytest",
          "26: from airflow.exceptions import TaskAlreadyInTaskGroup",
          "27: from airflow.models.baseoperator import BaseOperator",
          "28: from airflow.models.dag import DAG",
          "",
          "[Removed Lines]",
          "25: from airflow.decorators import dag, task as task_decorator, task_group as task_group_decorator",
          "",
          "[Added Lines]",
          "25: from airflow.decorators import (",
          "26:     dag,",
          "27:     setup,",
          "28:     task as task_decorator,",
          "29:     task_group as task_group_decorator,",
          "30:     teardown,",
          "31: )",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1479:         tg1 >> w2",
          "1480:     assert t1.downstream_task_ids == set()",
          "1481:     assert w1.downstream_task_ids == {\"tg1.t1\", \"w2\"}",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1490: def test_task_group_arrow_with_setup_group():",
          "1491:     with DAG(dag_id=\"setup_group_teardown_group\", start_date=pendulum.now()):",
          "1492:         with TaskGroup(\"group_1\") as g1:",
          "1494:             @setup",
          "1495:             def setup_1():",
          "1496:                 ...",
          "1498:             @setup",
          "1499:             def setup_2():",
          "1500:                 ...",
          "1502:             s1 = setup_1()",
          "1503:             s2 = setup_2()",
          "1505:         with TaskGroup(\"group_2\") as g2:",
          "1507:             @teardown",
          "1508:             def teardown_1():",
          "1509:                 ...",
          "1511:             @teardown",
          "1512:             def teardown_2():",
          "1513:                 ...",
          "1515:             t1 = teardown_1()",
          "1516:             t2 = teardown_2()",
          "1518:         @task_decorator",
          "1519:         def work():",
          "1520:             ...",
          "1522:         w1 = work()",
          "1523:         g1 >> w1 >> g2",
          "1524:         t1.as_teardown(setups=s1)",
          "1525:         t2.as_teardown(setups=s2)",
          "1526:     assert set(s1.operator.downstream_task_ids) == {\"work\", \"group_2.teardown_1\"}",
          "1527:     assert set(s2.operator.downstream_task_ids) == {\"work\", \"group_2.teardown_2\"}",
          "1528:     assert set(w1.operator.downstream_task_ids) == {\"group_2.teardown_1\", \"group_2.teardown_2\"}",
          "1529:     assert set(t1.operator.downstream_task_ids) == set()",
          "1530:     assert set(t2.operator.downstream_task_ids) == set()",
          "1532:     def get_nodes(group):",
          "1533:         d = task_group_to_dict(group)",
          "1534:         new_d = {}",
          "1535:         new_d[\"id\"] = d[\"id\"]",
          "1536:         new_d[\"children\"] = [{\"id\": x[\"id\"]} for x in d[\"children\"]]",
          "1537:         return new_d",
          "1539:     assert get_nodes(g1) == {",
          "1540:         \"id\": \"group_1\",",
          "1541:         \"children\": [",
          "1542:             {\"id\": \"group_1.setup_1\"},",
          "1543:             {\"id\": \"group_1.setup_2\"},",
          "1544:             {\"id\": \"group_1.downstream_join_id\"},",
          "1545:         ],",
          "1546:     }",
          "1549: def test_task_group_arrow_with_setup_group_deeper_setup():",
          "1550:     \"\"\"",
          "1551:     When recursing upstream for a non-teardown leaf, we should ignore setups that",
          "1552:     are direct upstream of a teardown.",
          "1553:     \"\"\"",
          "1554:     with DAG(dag_id=\"setup_group_teardown_group_2\", start_date=pendulum.now()):",
          "1555:         with TaskGroup(\"group_1\") as g1:",
          "1557:             @setup",
          "1558:             def setup_1():",
          "1559:                 ...",
          "1561:             @setup",
          "1562:             def setup_2():",
          "1563:                 ...",
          "1565:             @teardown",
          "1566:             def teardown_0():",
          "1567:                 ...",
          "1569:             s1 = setup_1()",
          "1570:             s2 = setup_2()",
          "1571:             t0 = teardown_0()",
          "1572:             s2 >> t0",
          "1574:         with TaskGroup(\"group_2\") as g2:",
          "1576:             @teardown",
          "1577:             def teardown_1():",
          "1578:                 ...",
          "1580:             @teardown",
          "1581:             def teardown_2():",
          "1582:                 ...",
          "1584:             t1 = teardown_1()",
          "1585:             t2 = teardown_2()",
          "1587:         @task_decorator",
          "1588:         def work():",
          "1589:             ...",
          "1591:         w1 = work()",
          "1592:         g1 >> w1 >> g2",
          "1593:         t1.as_teardown(setups=s1)",
          "1594:         t2.as_teardown(setups=s2)",
          "1595:     assert set(s1.operator.downstream_task_ids) == {\"work\", \"group_2.teardown_1\"}",
          "1596:     assert set(s2.operator.downstream_task_ids) == {\"group_1.teardown_0\", \"group_2.teardown_2\"}",
          "1597:     assert set(w1.operator.downstream_task_ids) == {\"group_2.teardown_1\", \"group_2.teardown_2\"}",
          "1598:     assert set(t1.operator.downstream_task_ids) == set()",
          "1599:     assert set(t2.operator.downstream_task_ids) == set()",
          "",
          "---------------"
        ]
      }
    }
  ]
}