{
  "cve_id": "CVE-2022-31777",
  "cve_desc": "A stored cross-site scripting (XSS) vulnerability in Apache Spark 3.2.1 and earlier, and 3.3.0, allows remote attackers to execute arbitrary JavaScript in the web browser of a user, by including a malicious payload into the logs which would be returned in logs rendered in the UI.",
  "repo": "apache/spark",
  "patch_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
  "patch_info": {
    "commit_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
    "repo": "apache/spark",
    "commit_url": "https://github.com/apache/spark/commit/ad90195de56688ce0898691eb9d04297ab0871ad",
    "files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ],
    "message": "[SPARK-39505][UI] Escape log content rendered in UI\n\n### What changes were proposed in this pull request?\n\nEscape log content rendered to the UI.\n\n### Why are the changes needed?\n\nLog content may contain reserved characters or other code in the log and be misinterpreted in the UI as HTML.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nExisting tests\n\nCloses #36902 from srowen/LogViewEscape.\n\nAuthored-by: Sean Owen <srowen@gmail.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
    "before_after_code_files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ]
  },
  "patch_diff": {
    "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js": [
      "File: core/src/main/resources/org/apache/spark/ui/static/log-view.js -> core/src/main/resources/org/apache/spark/ui/static/log-view.js",
      "--- Hunk 1 ---",
      "[Context before]",
      "85:       if (retStartByte == 0) {",
      "86:         disableMoreButton();",
      "87:       }",
      "90:       curLogLength = curLogLength + (startByte - retStartByte);",
      "91:       startByte = retStartByte;",
      "",
      "[Removed Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(cleanData);",
      "",
      "[Added Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(document.createTextNode(cleanData));",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "115:             var retLogLength = dataInfo[2];",
      "117:             var cleanData = data.substring(newlineIndex + 1);",
      "120:             curLogLength = curLogLength + (retEndByte - retStartByte);",
      "121:             endByte = retEndByte;",
      "",
      "[Removed Lines]",
      "118:             $(\"pre\", \".log-content\").append(cleanData);",
      "",
      "[Added Lines]",
      "118:             $(\"pre\", \".log-content\").append(document.createTextNode(cleanData));",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "6ab8b384d03d9ba1a046327a4ba9b4c7406ad706",
      "candidate_info": {
        "commit_hash": "6ab8b384d03d9ba1a046327a4ba9b4c7406ad706",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/6ab8b384d03d9ba1a046327a4ba9b4c7406ad706",
        "files": [
          "mllib/src/test/scala/org/apache/spark/ml/recommendation/ALSSuite.scala"
        ],
        "message": "[SPARK-38776][MLLIB][TESTS][FOLLOWUP] Disable ANSI_ENABLED more for `Out of Range` failures\n\nThis is a follow-up of https://github.com/apache/spark/pull/36051.\nAfter fixing `Overflow` errors, `Out Of Range` failures are observed in the rest of test code in the same test case.\n\nTo make GitHub Action ANSI test CI pass.\n\nNo.\n\nAt this time, I used the following to simulate GitHub Action ANSI job.\n```\n$ SPARK_ANSI_SQL_MODE=true build/sbt \"mllib/testOnly *.ALSSuite\"\n...\n[info] All tests passed.\n[success] Total time: 80 s (01:20), completed Apr 3, 2022 1:05:50 PM\n```\n\nCloses #36054 from dongjoon-hyun/SPARK-38776-2.\n\nAuthored-by: Dongjoon Hyun <dongjoon@apache.org>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>\n(cherry picked from commit fbcab01ffb672dda98f6f472da44aed26b59b2a5)\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
        "before_after_code_files": [
          "mllib/src/test/scala/org/apache/spark/ml/recommendation/ALSSuite.scala||mllib/src/test/scala/org/apache/spark/ml/recommendation/ALSSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "mllib/src/test/scala/org/apache/spark/ml/recommendation/ALSSuite.scala||mllib/src/test/scala/org/apache/spark/ml/recommendation/ALSSuite.scala": [
          "File: mllib/src/test/scala/org/apache/spark/ml/recommendation/ALSSuite.scala -> mllib/src/test/scala/org/apache/spark/ml/recommendation/ALSSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "228:     }",
          "230:     val msg = \"either out of Integer range or contained a fractional part\"",
          "234:       }",
          "241:       }",
          "243:     }",
          "245:     withClue(\"Invalid Decimal: fractional part\") {",
          "",
          "[Removed Lines]",
          "231:     withClue(\"Invalid Long: out of range\") {",
          "232:       val e: SparkException = intercept[SparkException] {",
          "233:         df.select(checkedCast(lit(1231000000000L))).collect()",
          "235:       assert(e.getMessage.contains(msg))",
          "236:     }",
          "238:     withClue(\"Invalid Decimal: out of range\") {",
          "239:       val e: SparkException = intercept[SparkException] {",
          "240:         df.select(checkedCast(lit(1231000000000.0).cast(DecimalType(15, 2)))).collect()",
          "242:       assert(e.getMessage.contains(msg))",
          "",
          "[Added Lines]",
          "231:     withSQLConf(SQLConf.ANSI_ENABLED.key -> \"false\") {",
          "232:       withClue(\"Invalid Long: out of range\") {",
          "233:         val e: SparkException = intercept[SparkException] {",
          "234:           df.select(checkedCast(lit(1231000000000L))).collect()",
          "235:         }",
          "236:         assert(e.getMessage.contains(msg))",
          "239:       withClue(\"Invalid Decimal: out of range\") {",
          "240:         val e: SparkException = intercept[SparkException] {",
          "241:           df.select(checkedCast(lit(1231000000000.0).cast(DecimalType(15, 2)))).collect()",
          "242:         }",
          "243:         assert(e.getMessage.contains(msg))",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "249:       assert(e.getMessage.contains(msg))",
          "250:     }",
          "255:       }",
          "257:     }",
          "259:     withClue(\"Invalid Double: fractional part\") {",
          "",
          "[Removed Lines]",
          "252:     withClue(\"Invalid Double: out of range\") {",
          "253:       val e: SparkException = intercept[SparkException] {",
          "254:         df.select(checkedCast(lit(1231000000000.0))).collect()",
          "256:       assert(e.getMessage.contains(msg))",
          "",
          "[Added Lines]",
          "254:     withSQLConf(SQLConf.ANSI_ENABLED.key -> \"false\") {",
          "255:       withClue(\"Invalid Double: out of range\") {",
          "256:         val e: SparkException = intercept[SparkException] {",
          "257:           df.select(checkedCast(lit(1231000000000.0))).collect()",
          "258:         }",
          "259:         assert(e.getMessage.contains(msg))",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "000270a4ead61bb9d7333d05c55b02a2ec477a04",
      "candidate_info": {
        "commit_hash": "000270a4ead61bb9d7333d05c55b02a2ec477a04",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/000270a4ead61bb9d7333d05c55b02a2ec477a04",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/V2ExpressionUtils.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/V2ExpressionUtilsSuite.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/connector/catalog/InMemoryTable.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DistributionAndOrderingUtils.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2ScanPartitioning.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/connector/KeyGroupedPartitioningSuite.scala"
        ],
        "message": "[SPARK-39313][SQL] `toCatalystOrdering` should fail if V2Expression can not be translated\n\nAfter reading code changes in #35657, I guess the original intention of changing the return type of `V2ExpressionUtils.toCatalyst` from `Expression` to `Option[Expression]` is, for reading, spark can ignore unrecognized distribution and ordering, but for writing, it should always be strict.\n\nSpecifically, `V2ExpressionUtils.toCatalystOrdering` should fail if V2Expression can not be translated instead of returning empty Seq.\n\n`V2ExpressionUtils.toCatalystOrdering` is used by `DistributionAndOrderingUtils`, the current behavior will break the semantics of `RequiresDistributionAndOrdering#requiredOrdering` in some cases(see UT).\n\nNo.\n\nNew UT.\n\nCloses #36697 from pan3793/SPARK-39313.\n\nAuthored-by: Cheng Pan <chengpan@apache.org>\nSigned-off-by: Chao Sun <sunchao@apple.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/V2ExpressionUtils.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/V2ExpressionUtils.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/V2ExpressionUtilsSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/V2ExpressionUtilsSuite.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/connector/catalog/InMemoryTable.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/connector/catalog/InMemoryTable.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DistributionAndOrderingUtils.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DistributionAndOrderingUtils.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2ScanPartitioning.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2ScanPartitioning.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/connector/KeyGroupedPartitioningSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/connector/KeyGroupedPartitioningSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/V2ExpressionUtils.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/V2ExpressionUtils.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/V2ExpressionUtils.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/V2ExpressionUtils.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "27: import org.apache.spark.sql.connector.expressions.{BucketTransform, Expression => V2Expression, FieldReference, IdentityTransform, NamedReference, NamedTransform, NullOrdering => V2NullOrdering, SortDirection => V2SortDirection, SortOrder => V2SortOrder, SortValue, Transform}",
          "28: import org.apache.spark.sql.errors.QueryCompilationErrors",
          "29: import org.apache.spark.sql.types._",
          "",
          "[Removed Lines]",
          "30: import org.apache.spark.util.collection.Utils.sequenceToOption",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "56:   def toCatalystOrdering(ordering: Array[V2SortOrder], query: LogicalPlan): Seq[SortOrder] = {",
          "59:   }",
          "61:   def toCatalyst(",
          "62:       expr: V2Expression,",
          "63:       query: LogicalPlan,",
          "64:       funCatalogOpt: Option[FunctionCatalog] = None): Option[Expression] = {",
          "65:     expr match {",
          "66:       case t: Transform =>",
          "68:       case SortValue(child, direction, nullOrdering) =>",
          "70:           SortOrder(catalystChild, toCatalyst(direction), toCatalyst(nullOrdering), Seq.empty)",
          "71:         }",
          "72:       case ref: FieldReference =>",
          "",
          "[Removed Lines]",
          "57:     sequenceToOption(ordering.map(toCatalyst(_, query))).asInstanceOf[Option[Seq[SortOrder]]]",
          "58:       .getOrElse(Seq.empty)",
          "67:         toCatalystTransform(t, query, funCatalogOpt)",
          "69:         toCatalyst(child, query, funCatalogOpt).map { catalystChild =>",
          "",
          "[Added Lines]",
          "56:     ordering.map(toCatalyst(_, query).asInstanceOf[SortOrder])",
          "60:       expr: V2Expression,",
          "61:       query: LogicalPlan,",
          "62:       funCatalogOpt: Option[FunctionCatalog] = None): Expression =",
          "63:     toCatalystOpt(expr, query, funCatalogOpt)",
          "64:         .getOrElse(throw new AnalysisException(s\"$expr is not currently supported\"))",
          "66:   def toCatalystOpt(",
          "72:         toCatalystTransformOpt(t, query, funCatalogOpt)",
          "74:         toCatalystOpt(child, query, funCatalogOpt).map { catalystChild =>",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "76:     }",
          "77:   }",
          "80:       trans: Transform,",
          "81:       query: LogicalPlan,",
          "82:       funCatalogOpt: Option[FunctionCatalog] = None): Option[Expression] = trans match {",
          "",
          "[Removed Lines]",
          "79:   def toCatalystTransform(",
          "",
          "[Added Lines]",
          "84:   def toCatalystTransformOpt(",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "90:       val numBucketsRef = AttributeReference(\"numBuckets\", IntegerType, nullable = false)()",
          "91:       funCatalogOpt.flatMap { catalog =>",
          "93:           TransformExpression(bound, resolvedRefs, Some(numBuckets))",
          "94:         }",
          "95:       }",
          "",
          "[Removed Lines]",
          "92:         loadV2Function(catalog, \"bucket\", Seq(numBucketsRef) ++ resolvedRefs).map { bound =>",
          "",
          "[Added Lines]",
          "97:         loadV2FunctionOpt(catalog, \"bucket\", Seq(numBucketsRef) ++ resolvedRefs).map { bound =>",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "99:         resolveRef[NamedExpression](r, query)",
          "100:       }",
          "101:       funCatalogOpt.flatMap { catalog =>",
          "103:           TransformExpression(bound, resolvedRefs)",
          "104:         }",
          "105:       }",
          "",
          "[Removed Lines]",
          "102:         loadV2Function(catalog, name, resolvedRefs).map { bound =>",
          "",
          "[Added Lines]",
          "107:         loadV2FunctionOpt(catalog, name, resolvedRefs).map { bound =>",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "107:       throw new AnalysisException(s\"Transform $trans is not currently supported\")",
          "108:   }",
          "111:       catalog: FunctionCatalog,",
          "112:       name: String,",
          "113:       args: Seq[Expression]): Option[BoundFunction] = {",
          "",
          "[Removed Lines]",
          "110:   private def loadV2Function(",
          "",
          "[Added Lines]",
          "115:   private def loadV2FunctionOpt(",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/V2ExpressionUtilsSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/V2ExpressionUtilsSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/V2ExpressionUtilsSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/V2ExpressionUtilsSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "18: package org.apache.spark.sql.catalyst.expressions",
          "20: import org.apache.spark.SparkFunSuite",
          "21: import org.apache.spark.sql.AnalysisException",
          "22: import org.apache.spark.sql.catalyst.plans.logical.LocalRelation",
          "23: import org.apache.spark.sql.connector.expressions._",
          "24: import org.apache.spark.sql.types.StringType",
          "26: class V2ExpressionUtilsSuite extends SparkFunSuite {",
          "28:   test(\"SPARK-39313: toCatalystOrdering should fail if V2Expression can not be translated\") {",
          "29:     val supportedV2Sort = SortValue(",
          "30:       FieldReference(\"a\"), SortDirection.ASCENDING, NullOrdering.NULLS_FIRST)",
          "31:     val unsupportedV2Sort = supportedV2Sort.copy(",
          "32:       expression = ApplyTransform(\"v2Fun\", FieldReference(\"a\") :: Nil))",
          "33:     val exc = intercept[AnalysisException] {",
          "34:       V2ExpressionUtils.toCatalystOrdering(",
          "35:         Array(supportedV2Sort, unsupportedV2Sort),",
          "36:         LocalRelation.apply(AttributeReference(\"a\", StringType)()))",
          "37:     }",
          "38:     assert(exc.message.contains(\"v2Fun(a) ASC NULLS FIRST is not currently supported\"))",
          "39:   }",
          "40: }",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/connector/catalog/InMemoryTable.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/connector/catalog/InMemoryTable.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/connector/catalog/InMemoryTable.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/connector/catalog/InMemoryTable.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "29: import org.apache.spark.sql.catalyst.InternalRow",
          "30: import org.apache.spark.sql.catalyst.expressions.{GenericInternalRow, JoinedRow}",
          "31: import org.apache.spark.sql.catalyst.util.{CharVarcharUtils, DateTimeUtils}",
          "33: import org.apache.spark.sql.connector.expressions._",
          "34: import org.apache.spark.sql.connector.metric.{CustomMetric, CustomTaskMetric}",
          "35: import org.apache.spark.sql.connector.read._",
          "",
          "[Removed Lines]",
          "32: import org.apache.spark.sql.connector.distributions.{ClusteredDistribution, Distribution, Distributions}",
          "",
          "[Added Lines]",
          "32: import org.apache.spark.sql.connector.distributions.{Distribution, Distributions}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "291:     }",
          "293:     override def outputPartitioning(): Partitioning = {",
          "297:       }",
          "298:     }",
          "",
          "[Removed Lines]",
          "294:       InMemoryTable.this.distribution match {",
          "295:         case cd: ClusteredDistribution => new KeyGroupedPartitioning(cd.clustering(), data.size)",
          "296:         case _ => new UnknownPartitioning(data.size)",
          "",
          "[Added Lines]",
          "294:       if (InMemoryTable.this.partitioning.nonEmpty) {",
          "295:         new KeyGroupedPartitioning(",
          "296:           InMemoryTable.this.partitioning.map(_.asInstanceOf[Expression]),",
          "297:           data.size)",
          "298:       } else {",
          "299:         new UnknownPartitioning(data.size)",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DistributionAndOrderingUtils.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DistributionAndOrderingUtils.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DistributionAndOrderingUtils.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DistributionAndOrderingUtils.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "24: import org.apache.spark.sql.connector.write.{RequiresDistributionAndOrdering, Write}",
          "25: import org.apache.spark.sql.errors.QueryCompilationErrors",
          "26: import org.apache.spark.sql.internal.SQLConf",
          "29: object DistributionAndOrderingUtils {",
          "",
          "[Removed Lines]",
          "27: import org.apache.spark.util.collection.Utils.sequenceToOption",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "35:       val distribution = write.requiredDistribution match {",
          "36:         case d: OrderedDistribution => toCatalystOrdering(d.ordering(), query)",
          "40:         case _: UnspecifiedDistribution => Seq.empty[Expression]",
          "41:       }",
          "",
          "[Removed Lines]",
          "37:         case d: ClusteredDistribution =>",
          "38:           sequenceToOption(d.clustering.map(e => toCatalyst(e, query)))",
          "39:             .getOrElse(Seq.empty[Expression])",
          "",
          "[Added Lines]",
          "36:         case d: ClusteredDistribution => d.clustering.map(e => toCatalyst(e, query)).toSeq",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2ScanPartitioning.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2ScanPartitioning.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2ScanPartitioning.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2ScanPartitioning.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "33: object V2ScanPartitioning extends Rule[LogicalPlan] with SQLConfHelper {",
          "34:   override def apply(plan: LogicalPlan): LogicalPlan = plan transformDown {",
          "36:       val funCatalogOpt = relation.catalog.flatMap {",
          "37:         case c: FunctionCatalog => Some(c)",
          "38:         case _ => None",
          "",
          "[Removed Lines]",
          "35:     case d @ DataSourceV2ScanRelation(relation, scan: SupportsReportPartitioning, _, _) =>",
          "",
          "[Added Lines]",
          "35:     case d @ DataSourceV2ScanRelation(relation, scan: SupportsReportPartitioning, _, None) =>",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "41:       val catalystPartitioning = scan.outputPartitioning() match {",
          "42:         case kgp: KeyGroupedPartitioning => sequenceToOption(kgp.keys().map(",
          "44:         case _: UnknownPartitioning => None",
          "45:         case p => throw new IllegalArgumentException(\"Unsupported data source V2 partitioning \" +",
          "46:             \"type: \" + p.getClass.getSimpleName)",
          "",
          "[Removed Lines]",
          "43:           V2ExpressionUtils.toCatalyst(_, relation, funCatalogOpt)))",
          "",
          "[Added Lines]",
          "43:           V2ExpressionUtils.toCatalystOpt(_, relation, funCatalogOpt)))",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/connector/KeyGroupedPartitioningSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/connector/KeyGroupedPartitioningSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/connector/KeyGroupedPartitioningSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/connector/KeyGroupedPartitioningSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "21: import org.apache.spark.sql.{DataFrame, Row}",
          "22: import org.apache.spark.sql.catalyst.InternalRow",
          "24: import org.apache.spark.sql.catalyst.plans.physical",
          "25: import org.apache.spark.sql.connector.catalog.Identifier",
          "26: import org.apache.spark.sql.connector.catalog.InMemoryTableCatalog",
          "27: import org.apache.spark.sql.connector.catalog.functions._",
          "29: import org.apache.spark.sql.connector.distributions.Distributions",
          "30: import org.apache.spark.sql.connector.expressions._",
          "31: import org.apache.spark.sql.connector.expressions.Expressions._",
          "",
          "[Removed Lines]",
          "23: import org.apache.spark.sql.catalyst.expressions.{Ascending, SortOrder => catalystSortOrder, TransformExpression}",
          "28: import org.apache.spark.sql.connector.distributions.Distribution",
          "",
          "[Added Lines]",
          "23: import org.apache.spark.sql.catalyst.expressions.TransformExpression",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "83:     val partitions: Array[Transform] = Array(Expressions.years(\"ts\"))",
          "88:     sql(s\"INSERT INTO testcat.ns.$table VALUES \" +",
          "89:         s\"(0, 'aaa', CAST('2022-01-01' AS timestamp)), \" +",
          "90:         s\"(1, 'bbb', CAST('2021-01-01' AS timestamp)), \" +",
          "",
          "[Removed Lines]",
          "86:     createTable(table, schema, partitions,",
          "87:       Distributions.clustered(partitions.map(_.asInstanceOf[Expression])))",
          "",
          "[Added Lines]",
          "85:     createTable(table, schema, partitions)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "104:       physical.KeyGroupedPartitioning(catalystDistribution.clustering, partitionValues))",
          "105:   }",
          "125:   test(\"non-clustered distribution: no partition\") {",
          "126:     val partitions: Array[Transform] = Array(bucket(32, \"ts\"))",
          "130:     val df = sql(s\"SELECT * FROM testcat.ns.$table\")",
          "131:     val distribution = physical.ClusteredDistribution(",
          "",
          "[Removed Lines]",
          "107:   test(\"non-clustered distribution: fallback to super.partitioning\") {",
          "108:     val partitions: Array[Transform] = Array(years(\"ts\"))",
          "109:     val ordering: Array[SortOrder] = Array(sort(FieldReference(\"ts\"),",
          "110:       SortDirection.ASCENDING, NullOrdering.NULLS_FIRST))",
          "112:     createTable(table, schema, partitions, Distributions.ordered(ordering), ordering)",
          "113:     sql(s\"INSERT INTO testcat.ns.$table VALUES \" +",
          "114:         s\"(0, 'aaa', CAST('2022-01-01' AS timestamp)), \" +",
          "115:         s\"(1, 'bbb', CAST('2021-01-01' AS timestamp)), \" +",
          "116:         s\"(2, 'ccc', CAST('2020-01-01' AS timestamp))\")",
          "118:     val df = sql(s\"SELECT * FROM testcat.ns.$table\")",
          "119:     val catalystOrdering = Seq(catalystSortOrder(attr(\"ts\"), Ascending))",
          "120:     val catalystDistribution = physical.OrderedDistribution(catalystOrdering)",
          "122:     checkQueryPlan(df, catalystDistribution, physical.UnknownPartitioning(0))",
          "123:   }",
          "127:     createTable(table, schema, partitions,",
          "128:       Distributions.clustered(partitions.map(_.asInstanceOf[Expression])))",
          "",
          "[Added Lines]",
          "107:     createTable(table, schema, partitions)",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "137:   test(\"non-clustered distribution: single partition\") {",
          "138:     val partitions: Array[Transform] = Array(bucket(32, \"ts\"))",
          "141:     sql(s\"INSERT INTO testcat.ns.$table VALUES (0, 'aaa', CAST('2020-01-01' AS timestamp))\")",
          "143:     val df = sql(s\"SELECT * FROM testcat.ns.$table\")",
          "",
          "[Removed Lines]",
          "139:     createTable(table, schema, partitions,",
          "140:       Distributions.clustered(partitions.map(_.asInstanceOf[Expression])))",
          "",
          "[Added Lines]",
          "118:     createTable(table, schema, partitions)",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "152:     val nonFunctionCatalog = spark.sessionState.catalogManager.catalog(\"testcat2\")",
          "153:         .asInstanceOf[InMemoryTableCatalog]",
          "154:     val partitions: Array[Transform] = Array(bucket(32, \"ts\"))",
          "158:     sql(s\"INSERT INTO testcat2.ns.$table VALUES \" +",
          "159:         s\"(0, 'aaa', CAST('2022-01-01' AS timestamp)), \" +",
          "160:         s\"(1, 'bbb', CAST('2021-01-01' AS timestamp)), \" +",
          "",
          "[Removed Lines]",
          "155:     createTable(table, schema, partitions,",
          "156:       Distributions.clustered(partitions.map(_.asInstanceOf[Expression])),",
          "157:       catalog = nonFunctionCatalog)",
          "",
          "[Added Lines]",
          "133:     createTable(table, schema, partitions, catalog = nonFunctionCatalog)",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "174:     catalog.clearFunctions()",
          "176:     val partitions: Array[Transform] = Array(bucket(32, \"ts\"))",
          "179:     sql(s\"INSERT INTO testcat.ns.$table VALUES \" +",
          "180:         s\"(0, 'aaa', CAST('2022-01-01' AS timestamp)), \" +",
          "181:         s\"(1, 'bbb', CAST('2021-01-01' AS timestamp)), \" +",
          "",
          "[Removed Lines]",
          "177:     createTable(table, schema, partitions,",
          "178:       Distributions.clustered(partitions.map(_.asInstanceOf[Expression])))",
          "",
          "[Added Lines]",
          "153:     createTable(table, schema, partitions)",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "190:   test(\"non-clustered distribution: V2 bucketing disabled\") {",
          "191:     withSQLConf(SQLConf.V2_BUCKETING_ENABLED.key -> \"false\") {",
          "192:       val partitions: Array[Transform] = Array(bucket(32, \"ts\"))",
          "195:       sql(s\"INSERT INTO testcat.ns.$table VALUES \" +",
          "196:           s\"(0, 'aaa', CAST('2022-01-01' AS timestamp)), \" +",
          "197:           s\"(1, 'bbb', CAST('2021-01-01' AS timestamp)), \" +",
          "",
          "[Removed Lines]",
          "193:       createTable(table, schema, partitions,",
          "194:         Distributions.clustered(partitions.map(_.asInstanceOf[Expression])))",
          "",
          "[Added Lines]",
          "168:       createTable(table, schema, partitions)",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "239:       table: String,",
          "240:       schema: StructType,",
          "241:       partitions: Array[Transform],",
          "244:       catalog: InMemoryTableCatalog = catalog): Unit = {",
          "245:     catalog.createTable(Identifier.of(Array(\"ns\"), table),",
          "247:   }",
          "249:   private val customers: String = \"customers\"",
          "",
          "[Removed Lines]",
          "242:       distribution: Distribution = Distributions.unspecified(),",
          "243:       ordering: Array[expressions.SortOrder] = Array.empty,",
          "246:       schema, partitions, emptyProps, distribution, ordering, None)",
          "",
          "[Added Lines]",
          "218:       schema, partitions, emptyProps, Distributions.unspecified(), Array.empty, None)",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "260:   private def testWithCustomersAndOrders(",
          "261:       customers_partitions: Array[Transform],",
          "263:       orders_partitions: Array[Transform],",
          "265:       expectedNumOfShuffleExecs: Int): Unit = {",
          "267:     sql(s\"INSERT INTO testcat.ns.$customers VALUES \" +",
          "268:         s\"('aaa', 10, 1), ('bbb', 20, 2), ('ccc', 30, 3)\")",
          "271:     sql(s\"INSERT INTO testcat.ns.$orders VALUES \" +",
          "272:         s\"(100.0, 1), (200.0, 1), (150.0, 2), (250.0, 2), (350.0, 2), (400.50, 3)\")",
          "",
          "[Removed Lines]",
          "262:       customers_distribution: Distribution,",
          "264:       orders_distribution: Distribution,",
          "266:     createTable(customers, customers_schema, customers_partitions, customers_distribution)",
          "270:     createTable(orders, orders_schema, orders_partitions, orders_distribution)",
          "",
          "[Added Lines]",
          "236:     createTable(customers, customers_schema, customers_partitions)",
          "240:     createTable(orders, orders_schema, orders_partitions)",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "297:     val customers_partitions = Array(bucket(4, \"customer_id\"))",
          "298:     val orders_partitions = Array(bucket(4, \"customer_id\"))",
          "305:   }",
          "307:   test(\"partitioned join: number of buckets mismatch should trigger shuffle\") {",
          "",
          "[Removed Lines]",
          "300:     testWithCustomersAndOrders(customers_partitions,",
          "301:       Distributions.clustered(customers_partitions.toArray),",
          "302:       orders_partitions,",
          "303:       Distributions.clustered(orders_partitions.toArray),",
          "304:       0)",
          "",
          "[Added Lines]",
          "270:     testWithCustomersAndOrders(customers_partitions, orders_partitions, 0)",
          "",
          "---------------",
          "--- Hunk 11 ---",
          "[Context before]",
          "309:     val orders_partitions = Array(bucket(2, \"customer_id\"))",
          "317:   }",
          "319:   test(\"partitioned join: only one side reports partitioning\") {",
          "320:     val customers_partitions = Array(bucket(4, \"customer_id\"))",
          "321:     val orders_partitions = Array(bucket(2, \"customer_id\"))",
          "328:   }",
          "330:   private val items: String = \"items\"",
          "",
          "[Removed Lines]",
          "312:     testWithCustomersAndOrders(customers_partitions,",
          "313:       Distributions.clustered(customers_partitions.toArray),",
          "314:       orders_partitions,",
          "315:       Distributions.clustered(orders_partitions.toArray),",
          "316:       2)",
          "323:     testWithCustomersAndOrders(customers_partitions,",
          "324:       Distributions.clustered(customers_partitions.toArray),",
          "325:       orders_partitions,",
          "326:       Distributions.unspecified(),",
          "327:       2)",
          "",
          "[Added Lines]",
          "278:     testWithCustomersAndOrders(customers_partitions, orders_partitions, 2)",
          "285:     testWithCustomersAndOrders(customers_partitions, orders_partitions, 2)",
          "",
          "---------------",
          "--- Hunk 12 ---",
          "[Context before]",
          "343:   test(\"partitioned join: join with two partition keys and matching & sorted partitions\") {",
          "344:     val items_partitions = Array(bucket(8, \"id\"), days(\"arrive_time\"))",
          "347:     sql(s\"INSERT INTO testcat.ns.$items VALUES \" +",
          "348:         s\"(1, 'aa', 40.0, cast('2020-01-01' as timestamp)), \" +",
          "349:         s\"(1, 'aa', 41.0, cast('2020-01-15' as timestamp)), \" +",
          "",
          "[Removed Lines]",
          "345:     createTable(items, items_schema, items_partitions,",
          "346:       Distributions.clustered(items_partitions.toArray))",
          "",
          "[Added Lines]",
          "303:     createTable(items, items_schema, items_partitions)",
          "",
          "---------------",
          "--- Hunk 13 ---",
          "[Context before]",
          "352:         s\"(3, 'cc', 15.5, cast('2020-02-01' as timestamp))\")",
          "354:     val purchases_partitions = Array(bucket(8, \"item_id\"), days(\"time\"))",
          "357:     sql(s\"INSERT INTO testcat.ns.$purchases VALUES \" +",
          "358:         s\"(1, 42.0, cast('2020-01-01' as timestamp)), \" +",
          "359:         s\"(1, 44.0, cast('2020-01-15' as timestamp)), \" +",
          "",
          "[Removed Lines]",
          "355:     createTable(purchases, purchases_schema, purchases_partitions,",
          "356:       Distributions.clustered(purchases_partitions.toArray))",
          "",
          "[Added Lines]",
          "312:     createTable(purchases, purchases_schema, purchases_partitions)",
          "",
          "---------------",
          "--- Hunk 14 ---",
          "[Context before]",
          "376:   test(\"partitioned join: join with two partition keys and unsorted partitions\") {",
          "377:     val items_partitions = Array(bucket(8, \"id\"), days(\"arrive_time\"))",
          "380:     sql(s\"INSERT INTO testcat.ns.$items VALUES \" +",
          "381:         s\"(3, 'cc', 15.5, cast('2020-02-01' as timestamp)), \" +",
          "382:         s\"(1, 'aa', 40.0, cast('2020-01-01' as timestamp)), \" +",
          "",
          "[Removed Lines]",
          "378:     createTable(items, items_schema, items_partitions,",
          "379:       Distributions.clustered(items_partitions.toArray))",
          "",
          "[Added Lines]",
          "334:     createTable(items, items_schema, items_partitions)",
          "",
          "---------------",
          "--- Hunk 15 ---",
          "[Context before]",
          "385:         s\"(2, 'bb', 10.5, cast('2020-01-01' as timestamp))\")",
          "387:     val purchases_partitions = Array(bucket(8, \"item_id\"), days(\"time\"))",
          "390:     sql(s\"INSERT INTO testcat.ns.$purchases VALUES \" +",
          "391:         s\"(2, 11.0, cast('2020-01-01' as timestamp)), \" +",
          "392:         s\"(1, 42.0, cast('2020-01-01' as timestamp)), \" +",
          "",
          "[Removed Lines]",
          "388:     createTable(purchases, purchases_schema, purchases_partitions,",
          "389:       Distributions.clustered(purchases_partitions.toArray))",
          "",
          "[Added Lines]",
          "343:     createTable(purchases, purchases_schema, purchases_partitions)",
          "",
          "---------------",
          "--- Hunk 16 ---",
          "[Context before]",
          "409:   test(\"partitioned join: join with two partition keys and different # of partition keys\") {",
          "410:     val items_partitions = Array(bucket(8, \"id\"), days(\"arrive_time\"))",
          "414:     sql(s\"INSERT INTO testcat.ns.$items VALUES \" +",
          "415:         s\"(1, 'aa', 40.0, cast('2020-01-01' as timestamp)), \" +",
          "",
          "[Removed Lines]",
          "411:     createTable(items, items_schema, items_partitions,",
          "412:       Distributions.clustered(items_partitions.toArray))",
          "",
          "[Added Lines]",
          "365:     createTable(items, items_schema, items_partitions)",
          "",
          "---------------",
          "--- Hunk 17 ---",
          "[Context before]",
          "417:         s\"(3, 'cc', 15.5, cast('2020-02-01' as timestamp))\")",
          "419:     val purchases_partitions = Array(bucket(8, \"item_id\"), days(\"time\"))",
          "422:     sql(s\"INSERT INTO testcat.ns.$purchases VALUES \" +",
          "423:         s\"(1, 42.0, cast('2020-01-01' as timestamp)), \" +",
          "424:         s\"(2, 11.0, cast('2020-01-01' as timestamp))\")",
          "",
          "[Removed Lines]",
          "420:     createTable(purchases, purchases_schema, purchases_partitions,",
          "421:       Distributions.clustered(purchases_partitions.toArray))",
          "",
          "[Added Lines]",
          "373:     createTable(purchases, purchases_schema, purchases_partitions)",
          "",
          "---------------",
          "--- Hunk 18 ---",
          "[Context before]",
          "439:         SQLConf.DYNAMIC_PARTITION_PRUNING_REUSE_BROADCAST_ONLY.key -> \"false\",",
          "440:         SQLConf.DYNAMIC_PARTITION_PRUNING_FALLBACK_FILTER_RATIO.key -> \"10\") {",
          "441:       val items_partitions = Array(identity(\"id\"))",
          "444:       sql(s\"INSERT INTO testcat.ns.$items VALUES \" +",
          "445:           s\"(1, 'aa', 40.0, cast('2020-01-01' as timestamp)), \" +",
          "446:           s\"(1, 'aa', 41.0, cast('2020-01-15' as timestamp)), \" +",
          "",
          "[Removed Lines]",
          "442:       createTable(items, items_schema, items_partitions,",
          "443:         Distributions.clustered(items_partitions.toArray))",
          "",
          "[Added Lines]",
          "394:       createTable(items, items_schema, items_partitions)",
          "",
          "---------------",
          "--- Hunk 19 ---",
          "[Context before]",
          "449:           s\"(3, 'cc', 15.5, cast('2020-02-01' as timestamp))\")",
          "451:       val purchases_partitions = Array(identity(\"item_id\"))",
          "454:       sql(s\"INSERT INTO testcat.ns.$purchases VALUES \" +",
          "455:           s\"(1, 42.0, cast('2020-01-01' as timestamp)), \" +",
          "456:           s\"(1, 44.0, cast('2020-01-15' as timestamp)), \" +",
          "",
          "[Removed Lines]",
          "452:       createTable(purchases, purchases_schema, purchases_partitions,",
          "453:         Distributions.clustered(purchases_partitions.toArray))",
          "",
          "[Added Lines]",
          "403:       createTable(purchases, purchases_schema, purchases_partitions)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "0f2e3ecb9943aec91204c168b6402f3e5de53ca2",
      "candidate_info": {
        "commit_hash": "0f2e3ecb9943aec91204c168b6402f3e5de53ca2",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/0f2e3ecb9943aec91204c168b6402f3e5de53ca2",
        "files": [
          "docs/sql-migration-guide.md",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/DataFrameReader.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/csv/CSVSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/json/JsonSuite.scala"
        ],
        "message": "[SPARK-35912][SQL][FOLLOW-UP] Add a legacy configuration for respecting nullability in DataFrame.schema.csv/json(ds)\n\n### What changes were proposed in this pull request?\n\nThis PR is a followup of https://github.com/apache/spark/pull/33436, that adds a legacy configuration. It's found that it can break a valid usacase (https://github.com/apache/spark/pull/33436/files#r863271189):\n\n```scala\nimport org.apache.spark.sql.types._\nval ds = Seq(\"a,\", \"a,b\").toDS\nspark.read.schema(\n  StructType(\n    StructField(\"f1\", StringType, nullable = false) ::\n    StructField(\"f2\", StringType, nullable = false) :: Nil)\n  ).option(\"mode\", \"DROPMALFORMED\").csv(ds).show()\n```\n\n**Before:**\n\n```\n+---+---+\n| f1| f2|\n+---+---+\n|  a|  b|\n+---+---+\n```\n\n**After:**\n\n```\n+---+----+\n| f1|  f2|\n+---+----+\n|  a|null|\n|  a|   b|\n+---+----+\n```\n\nThis PR adds a configuration to restore **Before** behaviour.\n\n### Why are the changes needed?\n\nTo avoid breakage of valid usecases.\n\n### Does this PR introduce _any_ user-facing change?\n\nYes, it adds a new configuration `spark.sql.legacy.respectNullabilityInTextDatasetConversion` (`false` by default) to respect the nullability in `DataFrameReader.schema(schema).csv(dataset)` and `DataFrameReader.schema(schema).json(dataset)` when the user-specified schema is provided.\n\n### How was this patch tested?\n\nUnittests were added.\n\nCloses #36435 from HyukjinKwon/SPARK-35912.\n\nAuthored-by: Hyukjin Kwon <gurwls223@apache.org>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>\n(cherry picked from commit 6689b97ec76abe5bab27f02869f8f16b32530d1a)\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/DataFrameReader.scala||sql/core/src/main/scala/org/apache/spark/sql/DataFrameReader.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/csv/CSVSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/csv/CSVSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/json/JsonSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/json/JsonSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "2983:     .intConf",
          "2984:     .createOptional",
          "2986:   val REPL_EAGER_EVAL_ENABLED = buildConf(\"spark.sql.repl.eagerEval.enabled\")",
          "2987:     .doc(\"Enables eager evaluation or not. When true, the top K rows of Dataset will be \" +",
          "2988:       \"displayed if and only if the REPL supports the eager evaluation. Currently, the \" +",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "2986:   val LEGACY_RESPECT_NULLABILITY_IN_TEXT_DATASET_CONVERSION =",
          "2987:     buildConf(\"spark.sql.legacy.respectNullabilityInTextDatasetConversion\")",
          "2988:       .internal()",
          "2989:       .doc(\"When true, the nullability in the user-specified schema for \" +",
          "2990:         \"`DataFrameReader.schema(schema).json(jsonDataset)` and \" +",
          "2991:         \"`DataFrameReader.schema(schema).csv(csvDataset)` is respected. Otherwise, they are \" +",
          "2992:         \"turned to a nullable schema forcibly.\")",
          "2993:       .version(\"3.3.0\")",
          "2994:       .booleanConf",
          "2995:       .createWithDefault(false)",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/DataFrameReader.scala||sql/core/src/main/scala/org/apache/spark/sql/DataFrameReader.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/DataFrameReader.scala -> sql/core/src/main/scala/org/apache/spark/sql/DataFrameReader.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "38: import org.apache.spark.sql.execution.datasources.jdbc._",
          "39: import org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource",
          "40: import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils",
          "41: import org.apache.spark.sql.types.StructType",
          "42: import org.apache.spark.sql.util.CaseInsensitiveStringMap",
          "43: import org.apache.spark.unsafe.types.UTF8String",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "41: import org.apache.spark.sql.internal.SQLConf",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "406:       sparkSession.sessionState.conf.columnNameOfCorruptRecord)",
          "408:     userSpecifiedSchema.foreach(ExprUtils.checkJsonSchema(_).foreach(throw _))",
          "410:       TextInputJsonDataSource.inferFromDataset(jsonDataset, parsedOptions)",
          "411:     }",
          "",
          "[Removed Lines]",
          "409:     val schema = userSpecifiedSchema.map(_.asNullable).getOrElse {",
          "",
          "[Added Lines]",
          "410:     val schema = userSpecifiedSchema.map {",
          "411:       case s if !SQLConf.get.getConf(",
          "412:         SQLConf.LEGACY_RESPECT_NULLABILITY_IN_TEXT_DATASET_CONVERSION) => s.asNullable",
          "413:       case other => other",
          "414:     }.getOrElse {",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "478:         None",
          "479:       }",
          "482:       TextInputCSVDataSource.inferFromDataset(",
          "483:         sparkSession,",
          "484:         csvDataset,",
          "",
          "[Removed Lines]",
          "481:     val schema = userSpecifiedSchema.map(_.asNullable).getOrElse {",
          "",
          "[Added Lines]",
          "486:     val schema = userSpecifiedSchema.map {",
          "487:       case s if !SQLConf.get.getConf(",
          "488:         SQLConf.LEGACY_RESPECT_NULLABILITY_IN_TEXT_DATASET_CONVERSION) => s.asNullable",
          "489:       case other => other",
          "490:     }.getOrElse {",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/csv/CSVSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/csv/CSVSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/csv/CSVSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/csv/CSVSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "2693:       assert(df.schema == expected)",
          "2694:       checkAnswer(df, Row(1, null) :: Nil)",
          "2695:     }",
          "2696:   }",
          "2698:   test(\"SPARK-36536: use casting when datetime pattern is not set\") {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "2697:     withSQLConf(SQLConf.LEGACY_RESPECT_NULLABILITY_IN_TEXT_DATASET_CONVERSION.key -> \"true\") {",
          "2698:       checkAnswer(",
          "2699:         spark.read.schema(",
          "2700:           StructType(",
          "2701:             StructField(\"f1\", StringType, nullable = false) ::",
          "2702:             StructField(\"f2\", StringType, nullable = false) :: Nil)",
          "2703:         ).option(\"mode\", \"DROPMALFORMED\").csv(Seq(\"a,\", \"a,b\").toDS),",
          "2704:         Row(\"a\", \"b\"))",
          "2705:     }",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/json/JsonSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/json/JsonSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/json/JsonSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/json/JsonSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "3165:     Seq(missingFieldInput, nullValueInput).foreach { jsonString =>",
          "3166:       Seq(\"DROPMALFORMED\", \"FAILFAST\", \"PERMISSIVE\").foreach { mode =>",
          "3167:         val json = spark.createDataset(",
          "3169:         val df = spark.read",
          "3170:           .option(\"mode\", mode)",
          "3171:           .schema(schema)",
          "",
          "[Removed Lines]",
          "3168:           spark.sparkContext.parallelize(jsonString:: Nil))(Encoders.STRING)",
          "",
          "[Added Lines]",
          "3168:           spark.sparkContext.parallelize(jsonString :: Nil))(Encoders.STRING)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "3174:         checkAnswer(df, Row(1, null) :: Nil)",
          "3175:       }",
          "3176:     }",
          "3177:   }",
          "3179:   test(\"SPARK-36379: proceed parsing with root nulls in permissive mode\") {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "3178:     withSQLConf(SQLConf.LEGACY_RESPECT_NULLABILITY_IN_TEXT_DATASET_CONVERSION.key -> \"true\") {",
          "3179:       checkAnswer(",
          "3180:         spark.read.schema(",
          "3181:           StructType(",
          "3182:             StructField(\"f1\", LongType, nullable = false) ::",
          "3183:             StructField(\"f2\", LongType, nullable = false) :: Nil)",
          "3184:         ).option(\"mode\", \"DROPMALFORMED\").json(Seq(\"\"\"{\"f1\": 1}\"\"\").toDS),",
          "3187:         Row(1, 0))",
          "3188:     }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "2edd344392a5ddb44f97449b8ad3c6292eb334e3",
      "candidate_info": {
        "commit_hash": "2edd344392a5ddb44f97449b8ad3c6292eb334e3",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/2edd344392a5ddb44f97449b8ad3c6292eb334e3",
        "files": [
          "python/pyspark/pandas/numpy_compat.py"
        ],
        "message": "[SPARK-39611][PYTHON][PS] Fix wrong aliases in __array_ufunc__\n\n### What changes were proposed in this pull request?\nThis PR fix the wrong aliases in `__array_ufunc__`\n\n### Why are the changes needed?\nWhen running test with numpy 1.23.0 (current latest), hit a bug: `NotImplementedError: pandas-on-Spark objects currently do not support <ufunc 'divide'>.`\n\nIn `__array_ufunc__` we first call `maybe_dispatch_ufunc_to_dunder_op` to try dunder methods first, and then we try pyspark API. `maybe_dispatch_ufunc_to_dunder_op` is from pandas code.\n\npandas fix a bug https://github.com/pandas-dev/pandas/pull/44822#issuecomment-991166419 https://github.com/pandas-dev/pandas/pull/44822/commits/206b2496bc6f6aa025cb26cb42f52abeec227741 when upgrade to numpy 1.23.0, we need to also sync this.\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\n- Current CI passed\n- The exsiting UT `test_series_datetime` already cover this, I also test it in my local env with 1.23.0\n```shell\npip install \"numpy==1.23.0\"\npython/run-tests --testnames 'pyspark.pandas.tests.test_series_datetime SeriesDateTimeTest.test_arithmetic_op_exceptions'\n```\n\nCloses #37078 from Yikun/SPARK-39611.\n\nAuthored-by: Yikun Jiang <yikunkero@gmail.com>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>\n(cherry picked from commit fb48a14a67940b9270390b8ce74c19ae58e2880e)\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>",
        "before_after_code_files": [
          "python/pyspark/pandas/numpy_compat.py||python/pyspark/pandas/numpy_compat.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "python/pyspark/pandas/numpy_compat.py||python/pyspark/pandas/numpy_compat.py": [
          "File: python/pyspark/pandas/numpy_compat.py -> python/pyspark/pandas/numpy_compat.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "166:         \"true_divide\": \"truediv\",",
          "167:         \"power\": \"pow\",",
          "168:         \"remainder\": \"mod\",",
          "170:         \"equal\": \"eq\",",
          "171:         \"not_equal\": \"ne\",",
          "172:         \"less\": \"lt\",",
          "",
          "[Removed Lines]",
          "169:         \"divide\": \"div\",",
          "",
          "[Added Lines]",
          "169:         \"divide\": \"truediv\",",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "197c975a6765d693f5fafe7614bac6d832c5aabf",
      "candidate_info": {
        "commit_hash": "197c975a6765d693f5fafe7614bac6d832c5aabf",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/197c975a6765d693f5fafe7614bac6d832c5aabf",
        "files": [
          "core/src/main/resources/error/error-classes.json",
          "core/src/main/scala/org/apache/spark/ErrorInfo.scala"
        ],
        "message": "[SPARK-38972][SQL] Support <param> in error-class messages\n\nUse symbolic names for parameters in error messages which are substituted with %s before formatting the string.\n\nIncrease readability of error message docs (TBD)\n\nNo\n\nSQL Project.\n\nCloses #36289 from srielau/symbolic-error-arg-names.\n\nAuthored-by: Serge Rielau <serge.rielau@databricks.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\n(cherry picked from commit 43e610333fb78834a09cd82f3da32bad262564f3)\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>",
        "before_after_code_files": [
          "core/src/main/scala/org/apache/spark/ErrorInfo.scala||core/src/main/scala/org/apache/spark/ErrorInfo.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "core/src/main/scala/org/apache/spark/ErrorInfo.scala||core/src/main/scala/org/apache/spark/ErrorInfo.scala": [
          "File: core/src/main/scala/org/apache/spark/ErrorInfo.scala -> core/src/main/scala/org/apache/spark/ErrorInfo.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "58:   def getMessage(errorClass: String, messageParameters: Array[String]): String = {",
          "59:     val errorInfo = errorClassToInfoMap.getOrElse(errorClass,",
          "60:       throw new IllegalArgumentException(s\"Cannot find error class '$errorClass'\"))",
          "62:   }",
          "64:   def getSqlState(errorClass: String): String = {",
          "",
          "[Removed Lines]",
          "61:     String.format(errorInfo.messageFormat, messageParameters: _*)",
          "",
          "[Added Lines]",
          "61:     String.format(errorInfo.messageFormat.replaceAll(\"<[a-zA-Z0-9_-]+>\", \"%s\"),",
          "62:       messageParameters: _*)",
          "",
          "---------------"
        ]
      }
    }
  ]
}