{
  "cve_id": "CVE-2021-45229",
  "cve_desc": "It was discovered that the \"Trigger DAG with config\" screen was susceptible to XSS attacks via the `origin` query argument. This issue affects Apache Airflow versions 2.2.3 and below.",
  "repo": "apache/airflow",
  "patch_hash": "628aa1f99c865d97d0b1c7c76e630e43a7b8d319",
  "patch_info": {
    "commit_hash": "628aa1f99c865d97d0b1c7c76e630e43a7b8d319",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/628aa1f99c865d97d0b1c7c76e630e43a7b8d319",
    "files": [
      "airflow/www/templates/airflow/trigger.html",
      "tests/www/views/test_views_trigger_dag.py"
    ],
    "message": "Simplify trigger cancel button (#21591)\n\nCo-authored-by: Jed Cunningham <jedcunningham@apache.org>\n(cherry picked from commit 65297673a318660fba76797e50d0c06804dfcafc)",
    "before_after_code_files": [
      "airflow/www/templates/airflow/trigger.html||airflow/www/templates/airflow/trigger.html",
      "tests/www/views/test_views_trigger_dag.py||tests/www/views/test_views_trigger_dag.py"
    ]
  },
  "patch_diff": {
    "airflow/www/templates/airflow/trigger.html||airflow/www/templates/airflow/trigger.html": [
      "File: airflow/www/templates/airflow/trigger.html -> airflow/www/templates/airflow/trigger.html",
      "--- Hunk 1 ---",
      "[Context before]",
      "63:       </label>",
      "64:     </div>",
      "65:     <button type=\"submit\" class=\"btn btn-primary\">Trigger</button>",
      "67:   </form>",
      "68: {% endblock %}",
      "",
      "[Removed Lines]",
      "66:     <button type=\"button\" class=\"btn\" onclick=\"location.href = '{{ origin }}'; return false\">Cancel</button>",
      "",
      "[Added Lines]",
      "66:     <a class=\"btn\" href=\"{{ origin }}\">Cancel</a>",
      "",
      "---------------"
    ],
    "tests/www/views/test_views_trigger_dag.py||tests/www/views/test_views_trigger_dag.py": [
      "File: tests/www/views/test_views_trigger_dag.py -> tests/www/views/test_views_trigger_dag.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "133:         (\"javascript:alert(1)\", \"/home\"),",
      "134:         (\"http://google.com\", \"/home\"),",
      "135:         (\"36539'%3balert(1)%2f%2f166\", \"/home\"),",
      "136:         (",
      "137:             \"%2Ftree%3Fdag_id%3Dexample_bash_operator';alert(33)//\",",
      "138:             \"/home\",",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "136:         (",
      "137:             '\"><script>alert(99)</script><a href=\"',",
      "138:             \"&#34;&gt;&lt;script&gt;alert(99)&lt;/script&gt;&lt;a href=&#34;\",",
      "139:         ),",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "145:     test_dag_id = \"example_bash_operator\"",
      "147:     resp = admin_client.get(f'trigger?dag_id={test_dag_id}&origin={test_origin}')",
      "156: @pytest.mark.parametrize(",
      "",
      "[Removed Lines]",
      "148:     check_content_in_response(",
      "149:         '<button type=\"button\" class=\"btn\" onclick=\"location.href = \\'{}\\'; return false\">'.format(",
      "150:             expected_origin",
      "151:         ),",
      "152:         resp,",
      "153:     )",
      "",
      "[Added Lines]",
      "152:     check_content_in_response(f'<a class=\"btn\" href=\"{expected_origin}\">Cancel</a>', resp)",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "2ad02ef545769912eb17e5047020be5a9ca94f91",
      "candidate_info": {
        "commit_hash": "2ad02ef545769912eb17e5047020be5a9ca94f91",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/2ad02ef545769912eb17e5047020be5a9ca94f91",
        "files": [
          "airflow/providers/google/cloud/example_dags/example_dataproc_metastore.py",
          "airflow/providers/google/cloud/hooks/dataproc_metastore.py",
          "airflow/providers/google/cloud/operators/dataproc_metastore.py",
          "airflow/providers/google/provider.yaml",
          "docs/apache-airflow-providers-google/operators/cloud/dataproc_metastore.rst",
          "setup.py",
          "tests/providers/google/cloud/hooks/test_dataproc_metastore.py",
          "tests/providers/google/cloud/operators/test_dataproc_metastore.py",
          "tests/providers/google/cloud/operators/test_dataproc_metastore_system.py"
        ],
        "message": "Add dataproc metastore operators (#18945)\n\n(cherry picked from commit 26ad55beb00f5a0915ba4bec541e3d67044834e9)",
        "before_after_code_files": [
          "airflow/providers/google/cloud/example_dags/example_dataproc_metastore.py||airflow/providers/google/cloud/example_dags/example_dataproc_metastore.py",
          "airflow/providers/google/cloud/hooks/dataproc_metastore.py||airflow/providers/google/cloud/hooks/dataproc_metastore.py",
          "airflow/providers/google/cloud/operators/dataproc_metastore.py||airflow/providers/google/cloud/operators/dataproc_metastore.py",
          "setup.py||setup.py",
          "tests/providers/google/cloud/hooks/test_dataproc_metastore.py||tests/providers/google/cloud/hooks/test_dataproc_metastore.py",
          "tests/providers/google/cloud/operators/test_dataproc_metastore.py||tests/providers/google/cloud/operators/test_dataproc_metastore.py",
          "tests/providers/google/cloud/operators/test_dataproc_metastore_system.py||tests/providers/google/cloud/operators/test_dataproc_metastore_system.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/providers/google/cloud/example_dags/example_dataproc_metastore.py||airflow/providers/google/cloud/example_dags/example_dataproc_metastore.py": [
          "File: airflow/providers/google/cloud/example_dags/example_dataproc_metastore.py -> airflow/providers/google/cloud/example_dags/example_dataproc_metastore.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: #",
          "2: # Licensed to the Apache Software Foundation (ASF) under one",
          "3: # or more contributor license agreements.  See the NOTICE file",
          "4: # distributed with this work for additional information",
          "5: # regarding copyright ownership.  The ASF licenses this file",
          "6: # to you under the Apache License, Version 2.0 (the",
          "7: # \"License\"); you may not use this file except in compliance",
          "8: # with the License.  You may obtain a copy of the License at",
          "9: #",
          "10: #   http://www.apache.org/licenses/LICENSE-2.0",
          "11: #",
          "12: # Unless required by applicable law or agreed to in writing,",
          "13: # software distributed under the License is distributed on an",
          "14: # \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
          "15: # KIND, either express or implied.  See the License for the",
          "16: # specific language governing permissions and limitations",
          "17: # under the License.",
          "18: \"\"\"",
          "19: Example Airflow DAG that show how to use various Dataproc Metastore",
          "20: operators to manage a service.",
          "21: \"\"\"",
          "23: import datetime",
          "24: import os",
          "26: from airflow import models",
          "27: from airflow.models.baseoperator import chain",
          "28: from airflow.providers.google.cloud.operators.dataproc_metastore import (",
          "29:     DataprocMetastoreCreateBackupOperator,",
          "30:     DataprocMetastoreCreateMetadataImportOperator,",
          "31:     DataprocMetastoreCreateServiceOperator,",
          "32:     DataprocMetastoreDeleteBackupOperator,",
          "33:     DataprocMetastoreDeleteServiceOperator,",
          "34:     DataprocMetastoreExportMetadataOperator,",
          "35:     DataprocMetastoreGetServiceOperator,",
          "36:     DataprocMetastoreListBackupsOperator,",
          "37:     DataprocMetastoreRestoreServiceOperator,",
          "38:     DataprocMetastoreUpdateServiceOperator,",
          "39: )",
          "41: PROJECT_ID = os.environ.get(\"GCP_PROJECT_ID\", \"<PROJECT_ID>\")",
          "42: SERVICE_ID = os.environ.get(\"GCP_DATAPROC_METASTORE_SERVICE_ID\", \"dataproc-metastore-system-tests-service-1\")",
          "43: BACKUP_ID = os.environ.get(\"GCP_DATAPROC_METASTORE_BACKUP_ID\", \"dataproc-metastore-system-tests-backup-1\")",
          "44: REGION = os.environ.get(\"GCP_REGION\", \"<REGION>\")",
          "45: BUCKET = os.environ.get(\"GCP_DATAPROC_METASTORE_BUCKET\", \"INVALID BUCKET NAME\")",
          "46: METADATA_IMPORT_FILE = os.environ.get(\"GCS_METADATA_IMPORT_FILE\", None)",
          "47: GCS_URI = os.environ.get(\"GCS_URI\", f\"gs://{BUCKET}/data/hive.sql\")",
          "48: METADATA_IMPORT_ID = \"dataproc-metastore-system-tests-metadata-import-1\"",
          "49: TIMEOUT = 1200",
          "50: DB_TYPE = \"MYSQL\"",
          "51: DESTINATION_GCS_FOLDER = f\"gs://{BUCKET}/>\"",
          "53: # Service definition",
          "54: # Docs: https://cloud.google.com/dataproc-metastore/docs/reference/rest/v1/projects.locations.services#Service",
          "55: # [START how_to_cloud_dataproc_metastore_create_service]",
          "56: SERVICE = {",
          "57:     \"name\": \"test-service\",",
          "58: }",
          "59: # [END how_to_cloud_dataproc_metastore_create_service]",
          "61: # Update service",
          "62: # [START how_to_cloud_dataproc_metastore_update_service]",
          "63: SERVICE_TO_UPDATE = {",
          "64:     \"labels\": {",
          "65:         \"mylocalmachine\": \"mylocalmachine\",",
          "66:         \"systemtest\": \"systemtest\",",
          "67:     }",
          "68: }",
          "69: UPDATE_MASK = {\"paths\": [\"labels\"]}",
          "70: # [END how_to_cloud_dataproc_metastore_update_service]",
          "72: # Backup definition",
          "73: # [START how_to_cloud_dataproc_metastore_create_backup]",
          "74: BACKUP = {",
          "75:     \"name\": \"test-backup\",",
          "76: }",
          "77: # [END how_to_cloud_dataproc_metastore_create_backup]",
          "79: # Metadata import definition",
          "80: # [START how_to_cloud_dataproc_metastore_create_metadata_import]",
          "81: METADATA_IMPORT = {",
          "82:     \"name\": \"test-metadata-import\",",
          "83:     \"database_dump\": {",
          "84:         \"gcs_uri\": GCS_URI,",
          "85:         \"database_type\": DB_TYPE,",
          "86:     },",
          "87: }",
          "88: # [END how_to_cloud_dataproc_metastore_create_metadata_import]",
          "91: with models.DAG(",
          "92:     \"example_gcp_dataproc_metastore\", start_date=datetime.datetime(2021, 1, 1), schedule_interval=\"@once\"",
          "93: ) as dag:",
          "94:     # [START how_to_cloud_dataproc_metastore_create_service_operator]",
          "95:     create_service = DataprocMetastoreCreateServiceOperator(",
          "96:         task_id=\"create_service\",",
          "97:         region=REGION,",
          "98:         project_id=PROJECT_ID,",
          "99:         service=SERVICE,",
          "100:         service_id=SERVICE_ID,",
          "101:         timeout=TIMEOUT,",
          "102:     )",
          "103:     # [END how_to_cloud_dataproc_metastore_create_service_operator]",
          "105:     # [START how_to_cloud_dataproc_metastore_get_service_operator]",
          "106:     get_service_details = DataprocMetastoreGetServiceOperator(",
          "107:         task_id=\"get_service\",",
          "108:         region=REGION,",
          "109:         project_id=PROJECT_ID,",
          "110:         service_id=SERVICE_ID,",
          "111:     )",
          "112:     # [END how_to_cloud_dataproc_metastore_get_service_operator]",
          "114:     # [START how_to_cloud_dataproc_metastore_update_service_operator]",
          "115:     update_service = DataprocMetastoreUpdateServiceOperator(",
          "116:         task_id=\"update_service\",",
          "117:         project_id=PROJECT_ID,",
          "118:         service_id=SERVICE_ID,",
          "119:         region=REGION,",
          "120:         service=SERVICE_TO_UPDATE,",
          "121:         update_mask=UPDATE_MASK,",
          "122:         timeout=TIMEOUT,",
          "123:     )",
          "124:     # [END how_to_cloud_dataproc_metastore_update_service_operator]",
          "126:     # [START how_to_cloud_dataproc_metastore_create_metadata_import_operator]",
          "127:     import_metadata = DataprocMetastoreCreateMetadataImportOperator(",
          "128:         task_id=\"create_metadata_import\",",
          "129:         project_id=PROJECT_ID,",
          "130:         region=REGION,",
          "131:         service_id=SERVICE_ID,",
          "132:         metadata_import=METADATA_IMPORT,",
          "133:         metadata_import_id=METADATA_IMPORT_ID,",
          "134:         timeout=TIMEOUT,",
          "135:     )",
          "136:     # [END how_to_cloud_dataproc_metastore_create_metadata_import_operator]",
          "138:     # [START how_to_cloud_dataproc_metastore_export_metadata_operator]",
          "139:     export_metadata = DataprocMetastoreExportMetadataOperator(",
          "140:         task_id=\"export_metadata\",",
          "141:         destination_gcs_folder=DESTINATION_GCS_FOLDER,",
          "142:         project_id=PROJECT_ID,",
          "143:         region=REGION,",
          "144:         service_id=SERVICE_ID,",
          "145:         timeout=TIMEOUT,",
          "146:     )",
          "147:     # [END how_to_cloud_dataproc_metastore_export_metadata_operator]",
          "149:     # [START how_to_cloud_dataproc_metastore_create_backup_operator]",
          "150:     backup_service = DataprocMetastoreCreateBackupOperator(",
          "151:         task_id=\"create_backup\",",
          "152:         project_id=PROJECT_ID,",
          "153:         region=REGION,",
          "154:         service_id=SERVICE_ID,",
          "155:         backup=BACKUP,",
          "156:         backup_id=BACKUP_ID,",
          "157:         timeout=TIMEOUT,",
          "158:     )",
          "159:     # [END how_to_cloud_dataproc_metastore_create_backup_operator]",
          "161:     # [START how_to_cloud_dataproc_metastore_list_backups_operator]",
          "162:     list_backups = DataprocMetastoreListBackupsOperator(",
          "163:         task_id=\"list_backups\",",
          "164:         project_id=PROJECT_ID,",
          "165:         region=REGION,",
          "166:         service_id=SERVICE_ID,",
          "167:     )",
          "168:     # [END how_to_cloud_dataproc_metastore_list_backups_operator]",
          "170:     # [START how_to_cloud_dataproc_metastore_delete_backup_operator]",
          "171:     delete_backup = DataprocMetastoreDeleteBackupOperator(",
          "172:         task_id=\"delete_backup\",",
          "173:         project_id=PROJECT_ID,",
          "174:         region=REGION,",
          "175:         service_id=SERVICE_ID,",
          "176:         backup_id=BACKUP_ID,",
          "177:         timeout=TIMEOUT,",
          "178:     )",
          "179:     # [END how_to_cloud_dataproc_metastore_delete_backup_operator]",
          "181:     # [START how_to_cloud_dataproc_metastore_restore_service_operator]",
          "182:     restore_service = DataprocMetastoreRestoreServiceOperator(",
          "183:         task_id=\"restore_metastore\",",
          "184:         region=REGION,",
          "185:         project_id=PROJECT_ID,",
          "186:         service_id=SERVICE_ID,",
          "187:         backup_id=BACKUP_ID,",
          "188:         backup_region=REGION,",
          "189:         backup_project_id=PROJECT_ID,",
          "190:         backup_service_id=SERVICE_ID,",
          "191:         timeout=TIMEOUT,",
          "192:     )",
          "193:     # [END how_to_cloud_dataproc_metastore_restore_service_operator]",
          "195:     # [START how_to_cloud_dataproc_metastore_delete_service_operator]",
          "196:     delete_service = DataprocMetastoreDeleteServiceOperator(",
          "197:         task_id=\"delete_service\",",
          "198:         region=REGION,",
          "199:         project_id=PROJECT_ID,",
          "200:         service_id=SERVICE_ID,",
          "201:         timeout=TIMEOUT,",
          "202:     )",
          "203:     # [END how_to_cloud_dataproc_metastore_delete_service_operator]",
          "205:     chain(",
          "206:         create_service,",
          "207:         update_service,",
          "208:         get_service_details,",
          "209:         backup_service,",
          "210:         list_backups,",
          "211:         restore_service,",
          "212:         delete_backup,",
          "213:         export_metadata,",
          "214:         import_metadata,",
          "215:         delete_service,",
          "216:     )",
          "",
          "---------------"
        ],
        "airflow/providers/google/cloud/hooks/dataproc_metastore.py||airflow/providers/google/cloud/hooks/dataproc_metastore.py": [
          "File: airflow/providers/google/cloud/hooks/dataproc_metastore.py -> airflow/providers/google/cloud/hooks/dataproc_metastore.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: #",
          "2: # Licensed to the Apache Software Foundation (ASF) under one",
          "3: # or more contributor license agreements.  See the NOTICE file",
          "4: # distributed with this work for additional information",
          "5: # regarding copyright ownership.  The ASF licenses this file",
          "6: # to you under the Apache License, Version 2.0 (the",
          "7: # \"License\"); you may not use this file except in compliance",
          "8: # with the License.  You may obtain a copy of the License at",
          "9: #",
          "10: #   http://www.apache.org/licenses/LICENSE-2.0",
          "11: #",
          "12: # Unless required by applicable law or agreed to in writing,",
          "13: # software distributed under the License is distributed on an",
          "14: # \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
          "15: # KIND, either express or implied.  See the License for the",
          "16: # specific language governing permissions and limitations",
          "17: # under the License.",
          "18: #",
          "19: \"\"\"This module contains a Google Cloud Dataproc Metastore hook.\"\"\"",
          "21: from typing import Dict, Optional, Sequence, Tuple, Union",
          "23: from google.api_core.operation import Operation",
          "24: from google.api_core.retry import Retry",
          "25: from google.cloud.metastore_v1 import DataprocMetastoreClient",
          "26: from google.cloud.metastore_v1.types import Backup, MetadataImport, Service",
          "27: from google.cloud.metastore_v1.types.metastore import DatabaseDumpSpec, Restore",
          "28: from google.protobuf.field_mask_pb2 import FieldMask",
          "30: from airflow.exceptions import AirflowException",
          "31: from airflow.providers.google.common.hooks.base_google import GoogleBaseHook",
          "34: class DataprocMetastoreHook(GoogleBaseHook):",
          "35:     \"\"\"Hook for Google Cloud Dataproc Metastore APIs.\"\"\"",
          "37:     def get_dataproc_metastore_client(self) -> DataprocMetastoreClient:",
          "38:         \"\"\"Returns DataprocMetastoreClient.\"\"\"",
          "39:         client_options = {'api_endpoint': 'metastore.googleapis.com:443'}",
          "41:         return DataprocMetastoreClient(",
          "42:             credentials=self._get_credentials(), client_info=self.client_info, client_options=client_options",
          "43:         )",
          "45:     def wait_for_operation(self, timeout: float, operation: Operation):",
          "46:         \"\"\"Waits for long-lasting operation to complete.\"\"\"",
          "47:         try:",
          "48:             return operation.result(timeout=timeout)",
          "49:         except Exception:",
          "50:             error = operation.exception(timeout=timeout)",
          "51:             raise AirflowException(error)",
          "53:     @GoogleBaseHook.fallback_to_default_project_id",
          "54:     def create_backup(",
          "55:         self,",
          "56:         project_id: str,",
          "57:         region: str,",
          "58:         service_id: str,",
          "59:         backup: Backup,",
          "60:         backup_id: str,",
          "61:         request_id: Optional[str] = None,",
          "62:         retry: Optional[Retry] = None,",
          "63:         timeout: Optional[float] = None,",
          "64:         metadata: Optional[Sequence[Tuple[str, str]]] = None,",
          "65:     ):",
          "66:         \"\"\"",
          "67:         Creates a new backup in a given project and location.",
          "69:         :param project_id: Required. The ID of the Google Cloud project that the service belongs to.",
          "70:         :type project_id: str",
          "71:         :param region: Required. The ID of the Google Cloud region that the service belongs to.",
          "72:         :type region: str",
          "73:         :param service_id:  Required. The ID of the metastore service, which is used as the final component of",
          "74:             the metastore service's name. This value must be between 2 and 63 characters long inclusive, begin",
          "75:             with a letter, end with a letter or number, and consist of alphanumeric ASCII characters or",
          "76:             hyphens.",
          "78:             This corresponds to the ``service_id`` field on the ``request`` instance; if ``request`` is",
          "79:             provided, this should not be set.",
          "80:         :type service_id: str",
          "81:         :param backup:  Required. The backup to create. The ``name`` field is ignored. The ID of the created",
          "82:             backup must be provided in the request's ``backup_id`` field.",
          "84:             This corresponds to the ``backup`` field on the ``request`` instance; if ``request`` is provided,",
          "85:             this should not be set.",
          "86:         :type backup: google.cloud.metastore_v1.types.Backup",
          "87:         :param backup_id:  Required. The ID of the backup, which is used as the final component of the",
          "88:             backup's name. This value must be between 1 and 64 characters long, begin with a letter, end with",
          "89:             a letter or number, and consist of alphanumeric ASCII characters or hyphens.",
          "91:             This corresponds to the ``backup_id`` field on the ``request`` instance; if ``request`` is",
          "92:             provided, this should not be set.",
          "93:         :type backup_id: str",
          "94:         :param request_id: Optional. A unique id used to identify the request.",
          "95:         :type request_id: str",
          "96:         :param retry: Designation of what errors, if any, should be retried.",
          "97:         :type retry: google.api_core.retry.Retry",
          "98:         :param timeout: The timeout for this request.",
          "99:         :type timeout: float",
          "100:         :param metadata: Strings which should be sent along with the request as metadata.",
          "101:         :type metadata: Sequence[Tuple[str, str]]",
          "102:         \"\"\"",
          "103:         parent = f'projects/{project_id}/locations/{region}/services/{service_id}'",
          "105:         client = self.get_dataproc_metastore_client()",
          "106:         result = client.create_backup(",
          "107:             request={",
          "108:                 'parent': parent,",
          "109:                 'backup': backup,",
          "110:                 'backup_id': backup_id,",
          "111:                 'request_id': request_id,",
          "112:             },",
          "113:             retry=retry,",
          "114:             timeout=timeout,",
          "115:             metadata=metadata,",
          "116:         )",
          "117:         return result",
          "119:     @GoogleBaseHook.fallback_to_default_project_id",
          "120:     def create_metadata_import(",
          "121:         self,",
          "122:         project_id: str,",
          "123:         region: str,",
          "124:         service_id: str,",
          "125:         metadata_import: MetadataImport,",
          "126:         metadata_import_id: str,",
          "127:         request_id: Optional[str] = None,",
          "128:         retry: Optional[Retry] = None,",
          "129:         timeout: Optional[float] = None,",
          "130:         metadata: Optional[Sequence[Tuple[str, str]]] = None,",
          "131:     ):",
          "132:         \"\"\"",
          "133:         Creates a new MetadataImport in a given project and location.",
          "135:         :param project_id: Required. The ID of the Google Cloud project that the service belongs to.",
          "136:         :type project_id: str",
          "137:         :param region: Required. The ID of the Google Cloud region that the service belongs to.",
          "138:         :type region: str",
          "139:         :param service_id:  Required. The ID of the metastore service, which is used as the final component of",
          "140:             the metastore service's name. This value must be between 2 and 63 characters long inclusive, begin",
          "141:             with a letter, end with a letter or number, and consist of alphanumeric ASCII characters or",
          "142:             hyphens.",
          "144:             This corresponds to the ``service_id`` field on the ``request`` instance; if ``request`` is",
          "145:             provided, this should not be set.",
          "146:         :type service_id: str",
          "147:         :param metadata_import:  Required. The metadata import to create. The ``name`` field is ignored. The",
          "148:             ID of the created metadata import must be provided in the request's ``metadata_import_id`` field.",
          "150:             This corresponds to the ``metadata_import`` field on the ``request`` instance; if ``request`` is",
          "151:             provided, this should not be set.",
          "152:         :type metadata_import: google.cloud.metastore_v1.types.MetadataImport",
          "153:         :param metadata_import_id:  Required. The ID of the metadata import, which is used as the final",
          "154:             component of the metadata import's name. This value must be between 1 and 64 characters long,",
          "155:             begin with a letter, end with a letter or number, and consist of alphanumeric ASCII characters or",
          "156:             hyphens.",
          "158:             This corresponds to the ``metadata_import_id`` field on the ``request`` instance; if ``request``",
          "159:             is provided, this should not be set.",
          "160:         :type metadata_import_id: str",
          "161:         :param request_id: Optional. A unique id used to identify the request.",
          "162:         :type request_id: str",
          "163:         :param retry: Designation of what errors, if any, should be retried.",
          "164:         :type retry: google.api_core.retry.Retry",
          "165:         :param timeout: The timeout for this request.",
          "166:         :type timeout: float",
          "167:         :param metadata: Strings which should be sent along with the request as metadata.",
          "168:         :type metadata: Sequence[Tuple[str, str]]",
          "169:         \"\"\"",
          "170:         parent = f'projects/{project_id}/locations/{region}/services/{service_id}'",
          "172:         client = self.get_dataproc_metastore_client()",
          "173:         result = client.create_metadata_import(",
          "174:             request={",
          "175:                 'parent': parent,",
          "176:                 'metadata_import': metadata_import,",
          "177:                 'metadata_import_id': metadata_import_id,",
          "178:                 'request_id': request_id,",
          "179:             },",
          "180:             retry=retry,",
          "181:             timeout=timeout,",
          "182:             metadata=metadata,",
          "183:         )",
          "184:         return result",
          "186:     @GoogleBaseHook.fallback_to_default_project_id",
          "187:     def create_service(",
          "188:         self,",
          "189:         region: str,",
          "190:         project_id: str,",
          "191:         service: Union[Dict, Service],",
          "192:         service_id: str,",
          "193:         request_id: Optional[str] = None,",
          "194:         retry: Optional[Retry] = None,",
          "195:         timeout: Optional[float] = None,",
          "196:         metadata: Optional[Sequence[Tuple[str, str]]] = (),",
          "197:     ):",
          "198:         \"\"\"",
          "199:         Creates a metastore service in a project and location.",
          "201:         :param region: Required. The ID of the Google Cloud region that the service belongs to.",
          "202:         :type region: str",
          "203:         :param project_id: Required. The ID of the Google Cloud project that the service belongs to.",
          "204:         :type project_id: str",
          "205:         :param service:  Required. The Metastore service to create. The ``name`` field is ignored. The ID of",
          "206:             the created metastore service must be provided in the request's ``service_id`` field.",
          "208:             This corresponds to the ``service`` field on the ``request`` instance; if ``request`` is provided,",
          "209:             this should not be set.",
          "210:         :type service: google.cloud.metastore_v1.types.Service",
          "211:         :param service_id:  Required. The ID of the metastore service, which is used as the final component of",
          "212:             the metastore service's name. This value must be between 2 and 63 characters long inclusive, begin",
          "213:             with a letter, end with a letter or number, and consist of alphanumeric ASCII characters or",
          "214:             hyphens.",
          "216:             This corresponds to the ``service_id`` field on the ``request`` instance; if ``request`` is",
          "217:             provided, this should not be set.",
          "218:         :type service_id: str",
          "219:         :param request_id: Optional. A unique id used to identify the request.",
          "220:         :type request_id: str",
          "221:         :param retry: Designation of what errors, if any, should be retried.",
          "222:         :type retry: google.api_core.retry.Retry",
          "223:         :param timeout: The timeout for this request.",
          "224:         :type timeout: float",
          "225:         :param metadata: Strings which should be sent along with the request as metadata.",
          "226:         :type metadata: Sequence[Tuple[str, str]]",
          "227:         \"\"\"",
          "228:         parent = f'projects/{project_id}/locations/{region}'",
          "230:         client = self.get_dataproc_metastore_client()",
          "231:         result = client.create_service(",
          "232:             request={",
          "233:                 'parent': parent,",
          "234:                 'service_id': service_id,",
          "235:                 'service': service if service else {},",
          "236:                 'request_id': request_id,",
          "237:             },",
          "238:             retry=retry,",
          "239:             timeout=timeout,",
          "240:             metadata=metadata,",
          "241:         )",
          "242:         return result",
          "244:     @GoogleBaseHook.fallback_to_default_project_id",
          "245:     def delete_backup(",
          "246:         self,",
          "247:         project_id: str,",
          "248:         region: str,",
          "249:         service_id: str,",
          "250:         backup_id: str,",
          "251:         request_id: Optional[str] = None,",
          "252:         retry: Optional[Retry] = None,",
          "253:         timeout: Optional[float] = None,",
          "254:         metadata: Optional[Sequence[Tuple[str, str]]] = None,",
          "255:     ):",
          "256:         \"\"\"",
          "257:         Deletes a single backup.",
          "259:         :param project_id: Required. The ID of the Google Cloud project that the service belongs to.",
          "260:         :type project_id: str",
          "261:         :param region: Required. The ID of the Google Cloud region that the service belongs to.",
          "262:         :type region: str",
          "263:         :param service_id:  Required. The ID of the metastore service, which is used as the final component of",
          "264:             the metastore service's name. This value must be between 2 and 63 characters long inclusive, begin",
          "265:             with a letter, end with a letter or number, and consist of alphanumeric ASCII characters or",
          "266:             hyphens.",
          "268:             This corresponds to the ``service_id`` field on the ``request`` instance; if ``request`` is",
          "269:             provided, this should not be set.",
          "270:         :type service_id: str",
          "271:         :param backup_id:  Required. The ID of the backup, which is used as the final component of the",
          "272:             backup's name. This value must be between 1 and 64 characters long, begin with a letter, end with",
          "273:             a letter or number, and consist of alphanumeric ASCII characters or hyphens.",
          "275:             This corresponds to the ``backup_id`` field on the ``request`` instance; if ``request`` is",
          "276:             provided, this should not be set.",
          "277:         :type backup_id: str",
          "278:         :param request_id: Optional. A unique id used to identify the request.",
          "279:         :type request_id: str",
          "280:         :param retry: Designation of what errors, if any, should be retried.",
          "281:         :type retry: google.api_core.retry.Retry",
          "282:         :param timeout: The timeout for this request.",
          "283:         :type timeout: float",
          "284:         :param metadata: Strings which should be sent along with the request as metadata.",
          "285:         :type metadata: Sequence[Tuple[str, str]]",
          "286:         \"\"\"",
          "287:         name = f'projects/{project_id}/locations/{region}/services/{service_id}/backups/{backup_id}'",
          "289:         client = self.get_dataproc_metastore_client()",
          "290:         result = client.delete_backup(",
          "291:             request={",
          "292:                 'name': name,",
          "293:                 'request_id': request_id,",
          "294:             },",
          "295:             retry=retry,",
          "296:             timeout=timeout,",
          "297:             metadata=metadata,",
          "298:         )",
          "299:         return result",
          "301:     @GoogleBaseHook.fallback_to_default_project_id",
          "302:     def delete_service(",
          "303:         self,",
          "304:         project_id: str,",
          "305:         region: str,",
          "306:         service_id: str,",
          "307:         request_id: Optional[str] = None,",
          "308:         retry: Optional[Retry] = None,",
          "309:         timeout: Optional[float] = None,",
          "310:         metadata: Optional[Sequence[Tuple[str, str]]] = None,",
          "311:     ):",
          "312:         \"\"\"",
          "313:         Deletes a single service.",
          "315:         :param project_id: Required. The ID of the Google Cloud project that the service belongs to.",
          "316:         :type project_id: str",
          "317:         :param region: Required. The ID of the Google Cloud region that the service belongs to.",
          "318:         :type region: str",
          "319:         :param service_id:  Required. The ID of the metastore service, which is used as the final component of",
          "320:             the metastore service's name. This value must be between 2 and 63 characters long inclusive, begin",
          "321:             with a letter, end with a letter or number, and consist of alphanumeric ASCII characters or",
          "322:             hyphens.",
          "324:             This corresponds to the ``service_id`` field on the ``request`` instance; if ``request`` is",
          "325:             provided, this should not be set.",
          "326:         :type service_id: str",
          "327:         :param request_id: Optional. A unique id used to identify the request.",
          "328:         :type request_id: str",
          "329:         :param retry: Designation of what errors, if any, should be retried.",
          "330:         :type retry: google.api_core.retry.Retry",
          "331:         :param timeout: The timeout for this request.",
          "332:         :type timeout: float",
          "333:         :param metadata: Strings which should be sent along with the request as metadata.",
          "334:         :type metadata: Sequence[Tuple[str, str]]",
          "335:         \"\"\"",
          "336:         name = f'projects/{project_id}/locations/{region}/services/{service_id}'",
          "338:         client = self.get_dataproc_metastore_client()",
          "339:         result = client.delete_service(",
          "340:             request={",
          "341:                 'name': name,",
          "342:                 'request_id': request_id,",
          "343:             },",
          "344:             retry=retry,",
          "345:             timeout=timeout,",
          "346:             metadata=metadata,",
          "347:         )",
          "348:         return result",
          "350:     @GoogleBaseHook.fallback_to_default_project_id",
          "351:     def export_metadata(",
          "352:         self,",
          "353:         destination_gcs_folder: str,",
          "354:         project_id: str,",
          "355:         region: str,",
          "356:         service_id: str,",
          "357:         request_id: Optional[str] = None,",
          "358:         database_dump_type: Optional[DatabaseDumpSpec] = None,",
          "359:         retry: Optional[Retry] = None,",
          "360:         timeout: Optional[float] = None,",
          "361:         metadata: Optional[Sequence[Tuple[str, str]]] = None,",
          "362:     ):",
          "363:         \"\"\"",
          "364:         Exports metadata from a service.",
          "366:         :param destination_gcs_folder: A Cloud Storage URI of a folder, in the format",
          "367:             ``gs://<bucket_name>/<path_inside_bucket>``. A sub-folder",
          "368:             ``<export_folder>`` containing exported files will be",
          "369:             created below it.",
          "370:         :type destination_gcs_folder: str",
          "371:         :param project_id: Required. The ID of the Google Cloud project that the service belongs to.",
          "372:         :type project_id: str",
          "373:         :param region: Required. The ID of the Google Cloud region that the service belongs to.",
          "374:         :type region: str",
          "375:         :param service_id:  Required. The ID of the metastore service, which is used as the final component of",
          "376:             the metastore service's name. This value must be between 2 and 63 characters long inclusive, begin",
          "377:             with a letter, end with a letter or number, and consist of alphanumeric ASCII characters or",
          "378:             hyphens.",
          "380:             This corresponds to the ``service_id`` field on the ``request`` instance; if ``request`` is",
          "381:             provided, this should not be set.",
          "382:         :type service_id: str",
          "383:         :param request_id: Optional. A unique id used to identify the request.",
          "384:         :type request_id: str",
          "385:         :param database_dump_type: Optional. The type of the database dump. If unspecified,",
          "386:             defaults to ``MYSQL``.",
          "387:         :type database_dump_type: google.cloud.metastore_v1.types.DatabaseDumpSpec.Type",
          "388:         :param retry: Designation of what errors, if any, should be retried.",
          "389:         :type retry: google.api_core.retry.Retry",
          "390:         :param timeout: The timeout for this request.",
          "391:         :type timeout: float",
          "392:         :param metadata: Strings which should be sent along with the request as metadata.",
          "393:         :type metadata: Sequence[Tuple[str, str]]",
          "394:         \"\"\"",
          "395:         service = f'projects/{project_id}/locations/{region}/services/{service_id}'",
          "397:         client = self.get_dataproc_metastore_client()",
          "398:         result = client.export_metadata(",
          "399:             request={",
          "400:                 'destination_gcs_folder': destination_gcs_folder,",
          "401:                 'service': service,",
          "402:                 'request_id': request_id,",
          "403:                 'database_dump_type': database_dump_type,",
          "404:             },",
          "405:             retry=retry,",
          "406:             timeout=timeout,",
          "407:             metadata=metadata,",
          "408:         )",
          "409:         return result",
          "411:     @GoogleBaseHook.fallback_to_default_project_id",
          "412:     def get_service(",
          "413:         self,",
          "414:         project_id: str,",
          "415:         region: str,",
          "416:         service_id: str,",
          "417:         retry: Optional[Retry] = None,",
          "418:         timeout: Optional[float] = None,",
          "419:         metadata: Optional[Sequence[Tuple[str, str]]] = None,",
          "420:     ):",
          "421:         \"\"\"",
          "422:         Gets the details of a single service.",
          "424:         :param project_id: Required. The ID of the Google Cloud project that the service belongs to.",
          "425:         :type project_id: str",
          "426:         :param region: Required. The ID of the Google Cloud region that the service belongs to.",
          "427:         :type region: str",
          "428:         :param service_id:  Required. The ID of the metastore service, which is used as the final component of",
          "429:             the metastore service's name. This value must be between 2 and 63 characters long inclusive, begin",
          "430:             with a letter, end with a letter or number, and consist of alphanumeric ASCII characters or",
          "431:             hyphens.",
          "433:             This corresponds to the ``service_id`` field on the ``request`` instance; if ``request`` is",
          "434:             provided, this should not be set.",
          "435:         :type service_id: str",
          "436:         :param retry: Designation of what errors, if any, should be retried.",
          "437:         :type retry: google.api_core.retry.Retry",
          "438:         :param timeout: The timeout for this request.",
          "439:         :type timeout: float",
          "440:         :param metadata: Strings which should be sent along with the request as metadata.",
          "441:         :type metadata: Sequence[Tuple[str, str]]",
          "442:         \"\"\"",
          "443:         name = f'projects/{project_id}/locations/{region}/services/{service_id}'",
          "445:         client = self.get_dataproc_metastore_client()",
          "446:         result = client.get_service(",
          "447:             request={",
          "448:                 'name': name,",
          "449:             },",
          "450:             retry=retry,",
          "451:             timeout=timeout,",
          "452:             metadata=metadata,",
          "453:         )",
          "454:         return result",
          "456:     @GoogleBaseHook.fallback_to_default_project_id",
          "457:     def list_backups(",
          "458:         self,",
          "459:         project_id: str,",
          "460:         region: str,",
          "461:         service_id: str,",
          "462:         page_size: Optional[int] = None,",
          "463:         page_token: Optional[str] = None,",
          "464:         filter: Optional[str] = None,",
          "465:         order_by: Optional[str] = None,",
          "466:         retry: Optional[Retry] = None,",
          "467:         timeout: Optional[float] = None,",
          "468:         metadata: Optional[Sequence[Tuple[str, str]]] = None,",
          "469:     ):",
          "470:         \"\"\"",
          "471:         Lists backups in a service.",
          "473:         :param project_id: Required. The ID of the Google Cloud project that the service belongs to.",
          "474:         :type project_id: str",
          "475:         :param region: Required. The ID of the Google Cloud region that the service belongs to.",
          "476:         :type region: str",
          "477:         :param service_id:  Required. The ID of the metastore service, which is used as the final component of",
          "478:             the metastore service's name. This value must be between 2 and 63 characters long inclusive, begin",
          "479:             with a letter, end with a letter or number, and consist of alphanumeric ASCII characters or",
          "480:             hyphens.",
          "482:             This corresponds to the ``service_id`` field on the ``request`` instance; if ``request`` is",
          "483:             provided, this should not be set.",
          "484:         :type service_id: str",
          "485:         :param page_size: Optional. The maximum number of backups to",
          "486:             return. The response may contain less than the",
          "487:             maximum number. If unspecified, no more than 500",
          "488:             backups are returned. The maximum value is 1000;",
          "489:             values above 1000 are changed to 1000.",
          "490:         :type page_size: int",
          "491:         :param page_token: Optional. A page token, received from a previous",
          "492:             [DataprocMetastore.ListBackups][google.cloud.metastore.v1.DataprocMetastore.ListBackups]",
          "493:             call. Provide this token to retrieve the subsequent page.",
          "494:             To retrieve the first page, supply an empty page token.",
          "495:             When paginating, other parameters provided to",
          "496:             [DataprocMetastore.ListBackups][google.cloud.metastore.v1.DataprocMetastore.ListBackups]",
          "497:             must match the call that provided the page token.",
          "498:         :type page_token: str",
          "499:         :param filter: Optional. The filter to apply to list",
          "500:             results.",
          "501:         :type filter: str",
          "502:         :param order_by: Optional. Specify the ordering of results as described in",
          "503:             `Sorting",
          "504:             Order <https://cloud.google.com/apis/design/design_patterns#sorting_order>`__.",
          "505:             If not specified, the results will be sorted in the default",
          "506:             order.",
          "507:         :type order_by: str",
          "508:         :param retry: Designation of what errors, if any, should be retried.",
          "509:         :type retry: google.api_core.retry.Retry",
          "510:         :param timeout: The timeout for this request.",
          "511:         :type timeout: float",
          "512:         :param metadata: Strings which should be sent along with the request as metadata.",
          "513:         :type metadata: Sequence[Tuple[str, str]]",
          "514:         \"\"\"",
          "515:         parent = f'projects/{project_id}/locations/{region}/services/{service_id}/backups'",
          "517:         client = self.get_dataproc_metastore_client()",
          "518:         result = client.list_backups(",
          "519:             request={",
          "520:                 'parent': parent,",
          "521:                 'page_size': page_size,",
          "522:                 'page_token': page_token,",
          "523:                 'filter': filter,",
          "524:                 'order_by': order_by,",
          "525:             },",
          "526:             retry=retry,",
          "527:             timeout=timeout,",
          "528:             metadata=metadata,",
          "529:         )",
          "530:         return result",
          "532:     @GoogleBaseHook.fallback_to_default_project_id",
          "533:     def restore_service(",
          "534:         self,",
          "535:         project_id: str,",
          "536:         region: str,",
          "537:         service_id: str,",
          "538:         backup_project_id: str,",
          "539:         backup_region: str,",
          "540:         backup_service_id: str,",
          "541:         backup_id: str,",
          "542:         restore_type: Optional[Restore] = None,",
          "543:         request_id: Optional[str] = None,",
          "544:         retry: Optional[Retry] = None,",
          "545:         timeout: Optional[float] = None,",
          "546:         metadata: Optional[Sequence[Tuple[str, str]]] = None,",
          "547:     ):",
          "548:         \"\"\"",
          "549:         Restores a service from a backup.",
          "551:         :param project_id: Required. The ID of the Google Cloud project that the service belongs to.",
          "552:         :type project_id: str",
          "553:         :param region: Required. The ID of the Google Cloud region that the service belongs to.",
          "554:         :type region: str",
          "555:         :param service_id:  Required. The ID of the metastore service, which is used as the final component of",
          "556:             the metastore service's name. This value must be between 2 and 63 characters long inclusive, begin",
          "557:             with a letter, end with a letter or number, and consist of alphanumeric ASCII characters or",
          "558:             hyphens.",
          "560:             This corresponds to the ``service_id`` field on the ``request`` instance; if ``request`` is",
          "561:             provided, this should not be set.",
          "562:         :type service_id: str",
          "563:         :param backup_project_id: Required. The ID of the Google Cloud project that the metastore service",
          "564:             backup to restore from.",
          "565:         :type backup_project_id: str",
          "566:         :param backup_region: Required. The ID of the Google Cloud region that the metastore",
          "567:             service backup to restore from.",
          "568:         :type backup_region: str",
          "569:         :param backup_service_id:  Required. The ID of the metastore service backup to restore from,",
          "570:             which is used as the final component of the metastore service's name. This value must be",
          "571:             between 2 and 63 characters long inclusive, begin with a letter, end with a letter or number,",
          "572:             and consist of alphanumeric ASCII characters or hyphens.",
          "573:         :type backup_service_id: str",
          "574:         :param backup_id:  Required. The ID of the metastore service backup to restore from",
          "575:         :type backup_id: str",
          "576:         :param restore_type: Optional. The type of restore. If unspecified, defaults to",
          "577:             ``METADATA_ONLY``",
          "578:         :type restore_type: google.cloud.metastore_v1.types.Restore.RestoreType",
          "579:         :param request_id: Optional. A unique id used to identify the request.",
          "580:         :type request_id: str",
          "581:         :param retry: Designation of what errors, if any, should be retried.",
          "582:         :type retry: google.api_core.retry.Retry",
          "583:         :param timeout: The timeout for this request.",
          "584:         :type timeout: float",
          "585:         :param metadata: Strings which should be sent along with the request as metadata.",
          "586:         :type metadata: Sequence[Tuple[str, str]]",
          "587:         \"\"\"",
          "588:         service = f'projects/{project_id}/locations/{region}/services/{service_id}'",
          "589:         backup = (",
          "590:             f'projects/{backup_project_id}/locations/{backup_region}/services/'",
          "591:             f'{backup_service_id}/backups/{backup_id}'",
          "592:         )",
          "594:         client = self.get_dataproc_metastore_client()",
          "595:         result = client.restore_service(",
          "596:             request={",
          "597:                 'service': service,",
          "598:                 'backup': backup,",
          "599:                 'restore_type': restore_type,",
          "600:                 'request_id': request_id,",
          "601:             },",
          "602:             retry=retry,",
          "603:             timeout=timeout,",
          "604:             metadata=metadata,",
          "605:         )",
          "606:         return result",
          "608:     @GoogleBaseHook.fallback_to_default_project_id",
          "609:     def update_service(",
          "610:         self,",
          "611:         project_id: str,",
          "612:         region: str,",
          "613:         service_id: str,",
          "614:         service: Union[Dict, Service],",
          "615:         update_mask: FieldMask,",
          "616:         request_id: Optional[str] = None,",
          "617:         retry: Optional[Retry] = None,",
          "618:         timeout: Optional[float] = None,",
          "619:         metadata: Optional[Sequence[Tuple[str, str]]] = None,",
          "620:     ):",
          "621:         \"\"\"",
          "622:         Updates the parameters of a single service.",
          "624:         :param project_id: Required. The ID of the Google Cloud project that the service belongs to.",
          "625:         :type project_id: str",
          "626:         :param region: Required. The ID of the Google Cloud region that the service belongs to.",
          "627:         :type region: str",
          "628:         :param service_id:  Required. The ID of the metastore service, which is used as the final component of",
          "629:             the metastore service's name. This value must be between 2 and 63 characters long inclusive, begin",
          "630:             with a letter, end with a letter or number, and consist of alphanumeric ASCII characters or",
          "631:             hyphens.",
          "633:             This corresponds to the ``service_id`` field on the ``request`` instance; if ``request`` is",
          "634:             provided, this should not be set.",
          "635:         :type service_id: str",
          "636:         :param service:  Required. The metastore service to update. The server only merges fields in the",
          "637:             service if they are specified in ``update_mask``.",
          "639:             The metastore service's ``name`` field is used to identify the metastore service to be updated.",
          "641:             This corresponds to the ``service`` field on the ``request`` instance; if ``request`` is provided,",
          "642:             this should not be set.",
          "643:         :type service: Union[Dict, google.cloud.metastore_v1.types.Service]",
          "644:         :param update_mask:  Required. A field mask used to specify the fields to be overwritten in the",
          "645:             metastore service resource by the update. Fields specified in the ``update_mask`` are relative to",
          "646:             the resource (not to the full request). A field is overwritten if it is in the mask.",
          "648:             This corresponds to the ``update_mask`` field on the ``request`` instance; if ``request`` is",
          "649:             provided, this should not be set.",
          "650:         :type update_mask: google.protobuf.field_mask_pb2.FieldMask",
          "651:         :param request_id: Optional. A unique id used to identify the request.",
          "652:         :type request_id: str",
          "653:         :param retry: Designation of what errors, if any, should be retried.",
          "654:         :type retry: google.api_core.retry.Retry",
          "655:         :param timeout: The timeout for this request.",
          "656:         :type timeout: float",
          "657:         :param metadata: Strings which should be sent along with the request as metadata.",
          "658:         :type metadata: Sequence[Tuple[str, str]]",
          "659:         \"\"\"",
          "660:         client = self.get_dataproc_metastore_client()",
          "662:         service_name = f'projects/{project_id}/locations/{region}/services/{service_id}'",
          "664:         service[\"name\"] = service_name",
          "666:         result = client.update_service(",
          "667:             request={",
          "668:                 'service': service,",
          "669:                 'update_mask': update_mask,",
          "670:                 'request_id': request_id,",
          "671:             },",
          "672:             retry=retry,",
          "673:             timeout=timeout,",
          "674:             metadata=metadata,",
          "675:         )",
          "676:         return result",
          "",
          "---------------"
        ],
        "airflow/providers/google/cloud/operators/dataproc_metastore.py||airflow/providers/google/cloud/operators/dataproc_metastore.py": [
          "File: airflow/providers/google/cloud/operators/dataproc_metastore.py -> airflow/providers/google/cloud/operators/dataproc_metastore.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: #",
          "2: # Licensed to the Apache Software Foundation (ASF) under one",
          "3: # or more contributor license agreements.  See the NOTICE file",
          "4: # distributed with this work for additional information",
          "5: # regarding copyright ownership.  The ASF licenses this file",
          "6: # to you under the Apache License, Version 2.0 (the",
          "7: # \"License\"); you may not use this file except in compliance",
          "8: # with the License.  You may obtain a copy of the License at",
          "9: #",
          "10: #   http://www.apache.org/licenses/LICENSE-2.0",
          "11: #",
          "12: # Unless required by applicable law or agreed to in writing,",
          "13: # software distributed under the License is distributed on an",
          "14: # \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
          "15: # KIND, either express or implied.  See the License for the",
          "16: # specific language governing permissions and limitations",
          "17: # under the License.",
          "18: #",
          "19: \"\"\"This module contains Google Dataproc Metastore operators.\"\"\"",
          "21: from time import sleep",
          "22: from typing import Dict, Optional, Sequence, Tuple, Union",
          "24: from google.api_core.retry import Retry, exponential_sleep_generator",
          "25: from google.cloud.metastore_v1 import MetadataExport, MetadataManagementActivity",
          "26: from google.cloud.metastore_v1.types import Backup, MetadataImport, Service",
          "27: from google.cloud.metastore_v1.types.metastore import DatabaseDumpSpec, Restore",
          "28: from google.protobuf.field_mask_pb2 import FieldMask",
          "29: from googleapiclient.errors import HttpError",
          "31: from airflow import AirflowException",
          "32: from airflow.models import BaseOperator",
          "33: from airflow.providers.google.cloud.hooks.dataproc_metastore import DataprocMetastoreHook",
          "36: class DataprocMetastoreCreateBackupOperator(BaseOperator):",
          "37:     \"\"\"",
          "38:     Creates a new backup in a given project and location.",
          "40:     :param project_id: Required. The ID of the Google Cloud project that the service belongs to.",
          "41:     :type project_id: str",
          "42:     :param region: Required. The ID of the Google Cloud region that the service belongs to.",
          "43:     :type region: str",
          "44:     :param service_id:  Required. The ID of the metastore service, which is used as the final component of",
          "45:         the metastore service's name. This value must be between 2 and 63 characters long inclusive, begin",
          "46:         with a letter, end with a letter or number, and consist of alphanumeric ASCII characters or",
          "47:         hyphens.",
          "49:         This corresponds to the ``service_id`` field on the ``request`` instance; if ``request`` is",
          "50:         provided, this should not be set.",
          "51:     :type service_id: str",
          "52:     :param backup:  Required. The backup to create. The ``name`` field is ignored. The ID of the created",
          "53:         backup must be provided in the request's ``backup_id`` field.",
          "55:         This corresponds to the ``backup`` field on the ``request`` instance; if ``request`` is provided, this",
          "56:         should not be set.",
          "57:     :type backup: google.cloud.metastore_v1.types.Backup",
          "58:     :param backup_id:  Required. The ID of the backup, which is used as the final component of the backup's",
          "59:         name. This value must be between 1 and 64 characters long, begin with a letter, end with a letter or",
          "60:         number, and consist of alphanumeric ASCII characters or hyphens.",
          "62:         This corresponds to the ``backup_id`` field on the ``request`` instance; if ``request`` is provided,",
          "63:         this should not be set.",
          "64:     :type backup_id: str",
          "65:     :param request_id: Optional. A unique id used to identify the request.",
          "66:     :type request_id: str",
          "67:     :param retry: Optional. Designation of what errors, if any, should be retried.",
          "68:     :type retry: google.api_core.retry.Retry",
          "69:     :param timeout: Optional. The timeout for this request.",
          "70:     :type timeout: float",
          "71:     :param metadata: Optional. Strings which should be sent along with the request as metadata.",
          "72:     :type metadata: Sequence[Tuple[str, str]]",
          "73:     :param gcp_conn_id: The connection ID to use connecting to Google Cloud.",
          "74:     :type gcp_conn_id: str",
          "75:     :param impersonation_chain: Optional service account to impersonate using short-term",
          "76:         credentials, or chained list of accounts required to get the access_token",
          "77:         of the last account in the list, which will be impersonated in the request.",
          "78:         If set as a string, the account must grant the originating account",
          "79:         the Service Account Token Creator IAM role.",
          "80:         If set as a sequence, the identities from the list must grant",
          "81:         Service Account Token Creator IAM role to the directly preceding identity, with first",
          "82:         account from the list granting this role to the originating account (templated).",
          "83:     :type impersonation_chain: Union[str, Sequence[str]]",
          "84:     \"\"\"",
          "86:     template_fields = (",
          "87:         'project_id',",
          "88:         'backup',",
          "89:         'impersonation_chain',",
          "90:     )",
          "91:     template_fields_renderers = {'backup': 'json'}",
          "93:     def __init__(",
          "94:         self,",
          "96:         project_id: str,",
          "97:         region: str,",
          "98:         service_id: str,",
          "99:         backup: Union[Dict, Backup],",
          "100:         backup_id: str,",
          "101:         request_id: Optional[str] = None,",
          "102:         retry: Optional[Retry] = None,",
          "103:         timeout: Optional[float] = None,",
          "104:         metadata: Optional[Sequence[Tuple[str, str]]] = (),",
          "105:         gcp_conn_id: str = \"google_cloud_default\",",
          "106:         impersonation_chain: Optional[Union[str, Sequence[str]]] = None,",
          "108:     ) -> None:",
          "109:         super().__init__(**kwargs)",
          "110:         self.project_id = project_id",
          "111:         self.region = region",
          "112:         self.service_id = service_id",
          "113:         self.backup = backup",
          "114:         self.backup_id = backup_id",
          "115:         self.request_id = request_id",
          "116:         self.retry = retry",
          "117:         self.timeout = timeout",
          "118:         self.metadata = metadata",
          "119:         self.gcp_conn_id = gcp_conn_id",
          "120:         self.impersonation_chain = impersonation_chain",
          "122:     def execute(self, context: dict) -> dict:",
          "123:         hook = DataprocMetastoreHook(",
          "124:             gcp_conn_id=self.gcp_conn_id, impersonation_chain=self.impersonation_chain",
          "125:         )",
          "126:         self.log.info(\"Creating Dataproc Metastore backup: %s\", self.backup_id)",
          "128:         try:",
          "129:             operation = hook.create_backup(",
          "130:                 project_id=self.project_id,",
          "131:                 region=self.region,",
          "132:                 service_id=self.service_id,",
          "133:                 backup=self.backup,",
          "134:                 backup_id=self.backup_id,",
          "135:                 request_id=self.request_id,",
          "136:                 retry=self.retry,",
          "137:                 timeout=self.timeout,",
          "138:                 metadata=self.metadata,",
          "139:             )",
          "140:             backup = hook.wait_for_operation(self.timeout, operation)",
          "141:             self.log.info(\"Backup %s created successfully\", self.backup_id)",
          "142:         except HttpError as err:",
          "143:             if err.resp.status not in (409, '409'):",
          "144:                 raise",
          "145:             self.log.info(\"Backup %s already exists\", self.backup_id)",
          "146:             backup = hook.get_backup(",
          "147:                 project_id=self.project_id,",
          "148:                 region=self.region,",
          "149:                 service_id=self.service_id,",
          "150:                 backup_id=self.backup_id,",
          "151:                 retry=self.retry,",
          "152:                 timeout=self.timeout,",
          "153:                 metadata=self.metadata,",
          "154:             )",
          "155:         return Backup.to_dict(backup)",
          "158: class DataprocMetastoreCreateMetadataImportOperator(BaseOperator):",
          "159:     \"\"\"",
          "160:     Creates a new MetadataImport in a given project and location.",
          "162:     :param project_id: Required. The ID of the Google Cloud project that the service belongs to.",
          "163:     :type project_id: str",
          "164:     :param region: Required. The ID of the Google Cloud region that the service belongs to.",
          "165:     :type region: str",
          "166:     :param service_id:  Required. The ID of the metastore service, which is used as the final component of",
          "167:         the metastore service's name. This value must be between 2 and 63 characters long inclusive, begin",
          "168:         with a letter, end with a letter or number, and consist of alphanumeric ASCII characters or",
          "169:         hyphens.",
          "171:         This corresponds to the ``service_id`` field on the ``request`` instance; if ``request`` is",
          "172:         provided, this should not be set.",
          "173:     :type service_id: str",
          "174:     :param metadata_import:  Required. The metadata import to create. The ``name`` field is ignored. The ID of",
          "175:         the created metadata import must be provided in the request's ``metadata_import_id`` field.",
          "177:         This corresponds to the ``metadata_import`` field on the ``request`` instance; if ``request`` is",
          "178:         provided, this should not be set.",
          "179:     :type metadata_import: google.cloud.metastore_v1.types.MetadataImport",
          "180:     :param metadata_import_id:  Required. The ID of the metadata import, which is used as the final component",
          "181:         of the metadata import's name. This value must be between 1 and 64 characters long, begin with a",
          "182:         letter, end with a letter or number, and consist of alphanumeric ASCII characters or hyphens.",
          "184:         This corresponds to the ``metadata_import_id`` field on the ``request`` instance; if ``request`` is",
          "185:         provided, this should not be set.",
          "186:     :type metadata_import_id: str",
          "187:     :param request_id: Optional. A unique id used to identify the request.",
          "188:     :type request_id: str",
          "189:     :param retry: Optional. Designation of what errors, if any, should be retried.",
          "190:     :type retry: google.api_core.retry.Retry",
          "191:     :param timeout: Optional. The timeout for this request.",
          "192:     :type timeout: float",
          "193:     :param metadata: Optional. Strings which should be sent along with the request as metadata.",
          "194:     :type metadata: Sequence[Tuple[str, str]]",
          "195:     :param gcp_conn_id: The connection ID to use connecting to Google Cloud.",
          "196:     :type gcp_conn_id: str",
          "197:     :param impersonation_chain: Optional service account to impersonate using short-term",
          "198:         credentials, or chained list of accounts required to get the access_token",
          "199:         of the last account in the list, which will be impersonated in the request.",
          "200:         If set as a string, the account must grant the originating account",
          "201:         the Service Account Token Creator IAM role.",
          "202:         If set as a sequence, the identities from the list must grant",
          "203:         Service Account Token Creator IAM role to the directly preceding identity, with first",
          "204:         account from the list granting this role to the originating account (templated).",
          "205:     :type impersonation_chain: Union[str, Sequence[str]]",
          "206:     \"\"\"",
          "208:     template_fields = (",
          "209:         'project_id',",
          "210:         'metadata_import',",
          "211:         'impersonation_chain',",
          "212:     )",
          "213:     template_fields_renderers = {'metadata_import': 'json'}",
          "215:     def __init__(",
          "216:         self,",
          "218:         project_id: str,",
          "219:         region: str,",
          "220:         service_id: str,",
          "221:         metadata_import: MetadataImport,",
          "222:         metadata_import_id: str,",
          "223:         request_id: Optional[str] = None,",
          "224:         retry: Optional[Retry] = None,",
          "225:         timeout: Optional[float] = None,",
          "226:         metadata: Optional[Sequence[Tuple[str, str]]] = (),",
          "227:         gcp_conn_id: str = \"google_cloud_default\",",
          "228:         impersonation_chain: Optional[Union[str, Sequence[str]]] = None,",
          "230:     ) -> None:",
          "231:         super().__init__(**kwargs)",
          "232:         self.project_id = project_id",
          "233:         self.region = region",
          "234:         self.service_id = service_id",
          "235:         self.metadata_import = metadata_import",
          "236:         self.metadata_import_id = metadata_import_id",
          "237:         self.request_id = request_id",
          "238:         self.retry = retry",
          "239:         self.timeout = timeout",
          "240:         self.metadata = metadata",
          "241:         self.gcp_conn_id = gcp_conn_id",
          "242:         self.impersonation_chain = impersonation_chain",
          "244:     def execute(self, context: dict):",
          "245:         hook = DataprocMetastoreHook(",
          "246:             gcp_conn_id=self.gcp_conn_id, impersonation_chain=self.impersonation_chain",
          "247:         )",
          "248:         self.log.info(\"Creating Dataproc Metastore metadata import: %s\", self.metadata_import_id)",
          "249:         operation = hook.create_metadata_import(",
          "250:             project_id=self.project_id,",
          "251:             region=self.region,",
          "252:             service_id=self.service_id,",
          "253:             metadata_import=self.metadata_import,",
          "254:             metadata_import_id=self.metadata_import_id,",
          "255:             request_id=self.request_id,",
          "256:             retry=self.retry,",
          "257:             timeout=self.timeout,",
          "258:             metadata=self.metadata,",
          "259:         )",
          "260:         metadata_import = hook.wait_for_operation(self.timeout, operation)",
          "261:         self.log.info(\"Metadata import %s created successfully\", self.metadata_import_id)",
          "262:         return MetadataImport.to_dict(metadata_import)",
          "265: class DataprocMetastoreCreateServiceOperator(BaseOperator):",
          "266:     \"\"\"",
          "267:     Creates a metastore service in a project and location.",
          "269:     :param region: Required. The ID of the Google Cloud region that the service belongs to.",
          "270:     :type region: str",
          "271:     :param project_id: Required. The ID of the Google Cloud project that the service belongs to.",
          "272:     :type project_id: str",
          "273:     :param service:  Required. The Metastore service to create. The ``name`` field is ignored. The ID of",
          "274:         the created metastore service must be provided in the request's ``service_id`` field.",
          "276:         This corresponds to the ``service`` field on the ``request`` instance; if ``request`` is provided,",
          "277:         this should not be set.",
          "278:     :type service: google.cloud.metastore_v1.types.Service",
          "279:     :param service_id:  Required. The ID of the metastore service, which is used as the final component of",
          "280:         the metastore service's name. This value must be between 2 and 63 characters long inclusive, begin",
          "281:         with a letter, end with a letter or number, and consist of alphanumeric ASCII characters or",
          "282:         hyphens.",
          "284:         This corresponds to the ``service_id`` field on the ``request`` instance; if ``request`` is",
          "285:         provided, this should not be set.",
          "286:     :type service_id: str",
          "287:     :param request_id: Optional. A unique id used to identify the request.",
          "288:     :type request_id: str",
          "289:     :param retry: Designation of what errors, if any, should be retried.",
          "290:     :type retry: google.api_core.retry.Retry",
          "291:     :param timeout: The timeout for this request.",
          "292:     :type timeout: float",
          "293:     :param metadata: Strings which should be sent along with the request as metadata.",
          "294:     :type metadata: Sequence[Tuple[str, str]]",
          "295:     :param gcp_conn_id: The connection ID to use connecting to Google Cloud.",
          "296:     :type gcp_conn_id: str",
          "297:     :param impersonation_chain: Optional service account to impersonate using short-term",
          "298:         credentials, or chained list of accounts required to get the access_token",
          "299:         of the last account in the list, which will be impersonated in the request.",
          "300:         If set as a string, the account must grant the originating account",
          "301:         the Service Account Token Creator IAM role.",
          "302:         If set as a sequence, the identities from the list must grant",
          "303:         Service Account Token Creator IAM role to the directly preceding identity, with first",
          "304:         account from the list granting this role to the originating account (templated).",
          "305:     :type impersonation_chain: Union[str, Sequence[str]]",
          "306:     \"\"\"",
          "308:     template_fields = (",
          "309:         'project_id',",
          "310:         'service',",
          "311:         'impersonation_chain',",
          "312:     )",
          "313:     template_fields_renderers = {'service': 'json'}",
          "315:     def __init__(",
          "316:         self,",
          "318:         region: str,",
          "319:         project_id: str,",
          "320:         service: Optional[Union[Dict, Service]] = None,",
          "321:         service_id: str,",
          "322:         request_id: Optional[str] = None,",
          "323:         retry: Optional[Retry] = None,",
          "324:         timeout: Optional[float] = None,",
          "325:         metadata: Optional[Sequence[Tuple[str, str]]] = (),",
          "326:         gcp_conn_id: str = \"google_cloud_default\",",
          "327:         impersonation_chain: Optional[Union[str, Sequence[str]]] = None,",
          "329:     ) -> None:",
          "330:         super().__init__(**kwargs)",
          "331:         self.region = region",
          "332:         self.project_id = project_id",
          "333:         self.service = service",
          "334:         self.service_id = service_id",
          "335:         self.request_id = request_id",
          "336:         self.retry = retry",
          "337:         self.timeout = timeout",
          "338:         self.metadata = metadata",
          "339:         self.gcp_conn_id = gcp_conn_id",
          "340:         self.impersonation_chain = impersonation_chain",
          "342:     def execute(self, context) -> dict:",
          "343:         hook = DataprocMetastoreHook(",
          "344:             gcp_conn_id=self.gcp_conn_id, impersonation_chain=self.impersonation_chain",
          "345:         )",
          "346:         self.log.info(\"Creating Dataproc Metastore service: %s\", self.project_id)",
          "347:         try:",
          "348:             operation = hook.create_service(",
          "349:                 region=self.region,",
          "350:                 project_id=self.project_id,",
          "351:                 service=self.service,",
          "352:                 service_id=self.service_id,",
          "353:                 request_id=self.request_id,",
          "354:                 retry=self.retry,",
          "355:                 timeout=self.timeout,",
          "356:                 metadata=self.metadata,",
          "357:             )",
          "358:             service = hook.wait_for_operation(self.timeout, operation)",
          "359:             self.log.info(\"Service %s created successfully\", self.service_id)",
          "360:         except HttpError as err:",
          "361:             if err.resp.status not in (409, '409'):",
          "362:                 raise",
          "363:             self.log.info(\"Instance %s already exists\", self.service_id)",
          "364:             service = hook.get_service(",
          "365:                 region=self.region,",
          "366:                 project_id=self.project_id,",
          "367:                 service_id=self.service_id,",
          "368:                 retry=self.retry,",
          "369:                 timeout=self.timeout,",
          "370:                 metadata=self.metadata,",
          "371:             )",
          "372:         return Service.to_dict(service)",
          "375: class DataprocMetastoreDeleteBackupOperator(BaseOperator):",
          "376:     \"\"\"",
          "377:     Deletes a single backup.",
          "379:     :param project_id: Required. The ID of the Google Cloud project that the backup belongs to.",
          "380:     :type project_id: str",
          "381:     :param region: Required. The ID of the Google Cloud region that the backup belongs to.",
          "382:     :type region: str",
          "383:     :param service_id: Required. The ID of the metastore service, which is used as the final component of",
          "384:         the metastore service's name. This value must be between 2 and 63 characters long inclusive, begin",
          "385:         with a letter, end with a letter or number, and consist of alphanumeric ASCII characters or",
          "386:         hyphens.",
          "388:         This corresponds to the ``service_id`` field on the ``request`` instance; if ``request`` is",
          "389:         provided, this should not be set.",
          "390:     :type service_id: str",
          "391:     :param backup_id:  Required. The ID of the backup, which is used as the final component of the backup's",
          "392:         name. This value must be between 1 and 64 characters long, begin with a letter, end with a letter or",
          "393:         number, and consist of alphanumeric ASCII characters or hyphens.",
          "395:         This corresponds to the ``backup_id`` field on the ``request`` instance; if ``request`` is provided,",
          "396:         this should not be set.",
          "397:     :type backup_id: str",
          "398:     :param request_id: Optional. A unique id used to identify the request.",
          "399:     :type request_id: str",
          "400:     :param retry: Optional. Designation of what errors, if any, should be retried.",
          "401:     :type retry: google.api_core.retry.Retry",
          "402:     :param timeout: Optional. The timeout for this request.",
          "403:     :type timeout: float",
          "404:     :param metadata: Optional. Strings which should be sent along with the request as metadata.",
          "405:     :type metadata: Sequence[Tuple[str, str]]",
          "406:     :param gcp_conn_id: The connection ID to use connecting to Google Cloud.",
          "407:     :type gcp_conn_id: str",
          "408:     :param impersonation_chain: Optional service account to impersonate using short-term",
          "409:         credentials, or chained list of accounts required to get the access_token",
          "410:         of the last account in the list, which will be impersonated in the request.",
          "411:         If set as a string, the account must grant the originating account",
          "412:         the Service Account Token Creator IAM role.",
          "413:         If set as a sequence, the identities from the list must grant",
          "414:         Service Account Token Creator IAM role to the directly preceding identity, with first",
          "415:         account from the list granting this role to the originating account (templated).",
          "416:     :type impersonation_chain: Union[str, Sequence[str]]",
          "417:     \"\"\"",
          "419:     template_fields = (",
          "420:         'project_id',",
          "421:         'impersonation_chain',",
          "422:     )",
          "424:     def __init__(",
          "425:         self,",
          "427:         project_id: str,",
          "428:         region: str,",
          "429:         service_id: str,",
          "430:         backup_id: str,",
          "431:         request_id: Optional[str] = None,",
          "432:         retry: Optional[Retry] = None,",
          "433:         timeout: Optional[float] = None,",
          "434:         metadata: Optional[Sequence[Tuple[str, str]]] = (),",
          "435:         gcp_conn_id: str = \"google_cloud_default\",",
          "436:         impersonation_chain: Optional[Union[str, Sequence[str]]] = None,",
          "438:     ) -> None:",
          "439:         super().__init__(**kwargs)",
          "440:         self.project_id = project_id",
          "441:         self.region = region",
          "442:         self.service_id = service_id",
          "443:         self.backup_id = backup_id",
          "444:         self.request_id = request_id",
          "445:         self.retry = retry",
          "446:         self.timeout = timeout",
          "447:         self.metadata = metadata",
          "448:         self.gcp_conn_id = gcp_conn_id",
          "449:         self.impersonation_chain = impersonation_chain",
          "451:     def execute(self, context: dict) -> None:",
          "452:         hook = DataprocMetastoreHook(",
          "453:             gcp_conn_id=self.gcp_conn_id, impersonation_chain=self.impersonation_chain",
          "454:         )",
          "455:         self.log.info(\"Deleting Dataproc Metastore backup: %s\", self.backup_id)",
          "456:         operation = hook.delete_backup(",
          "457:             project_id=self.project_id,",
          "458:             region=self.region,",
          "459:             service_id=self.service_id,",
          "460:             backup_id=self.backup_id,",
          "461:             request_id=self.request_id,",
          "462:             retry=self.retry,",
          "463:             timeout=self.timeout,",
          "464:             metadata=self.metadata,",
          "465:         )",
          "466:         hook.wait_for_operation(self.timeout, operation)",
          "467:         self.log.info(\"Backup %s deleted successfully\", self.project_id)",
          "470: class DataprocMetastoreDeleteServiceOperator(BaseOperator):",
          "471:     \"\"\"",
          "472:     Deletes a single service.",
          "474:     :param request:  The request object. Request message for",
          "475:         [DataprocMetastore.DeleteService][google.cloud.metastore.v1.DataprocMetastore.DeleteService].",
          "476:     :type request: google.cloud.metastore_v1.types.DeleteServiceRequest",
          "477:     :param project_id: Required. The ID of the Google Cloud project that the service belongs to.",
          "478:     :type project_id: str",
          "479:     :param retry: Designation of what errors, if any, should be retried.",
          "480:     :type retry: google.api_core.retry.Retry",
          "481:     :param timeout: The timeout for this request.",
          "482:     :type timeout: float",
          "483:     :param metadata: Strings which should be sent along with the request as metadata.",
          "484:     :type metadata: Sequence[Tuple[str, str]]",
          "485:     :param gcp_conn_id:",
          "486:     :type gcp_conn_id: str",
          "487:     \"\"\"",
          "489:     template_fields = (",
          "490:         'project_id',",
          "491:         'impersonation_chain',",
          "492:     )",
          "494:     def __init__(",
          "495:         self,",
          "497:         region: str,",
          "498:         project_id: str,",
          "499:         service_id: str,",
          "500:         retry: Optional[Retry] = None,",
          "501:         timeout: Optional[float] = None,",
          "502:         metadata: Optional[Sequence[Tuple[str, str]]] = (),",
          "503:         gcp_conn_id: str = \"google_cloud_default\",",
          "504:         impersonation_chain: Optional[Union[str, Sequence[str]]] = None,",
          "506:     ) -> None:",
          "507:         super().__init__(**kwargs)",
          "508:         self.region = region",
          "509:         self.project_id = project_id",
          "510:         self.service_id = service_id",
          "511:         self.retry = retry",
          "512:         self.timeout = timeout",
          "513:         self.metadata = metadata",
          "514:         self.gcp_conn_id = gcp_conn_id",
          "515:         self.impersonation_chain = impersonation_chain",
          "517:     def execute(self, context) -> dict:",
          "518:         hook = DataprocMetastoreHook(",
          "519:             gcp_conn_id=self.gcp_conn_id, impersonation_chain=self.impersonation_chain",
          "520:         )",
          "521:         self.log.info(\"Deleting Dataproc Metastore service: %s\", self.project_id)",
          "522:         operation = hook.delete_service(",
          "523:             region=self.region,",
          "524:             project_id=self.project_id,",
          "525:             service_id=self.service_id,",
          "526:             retry=self.retry,",
          "527:             timeout=self.timeout,",
          "528:             metadata=self.metadata,",
          "529:         )",
          "530:         hook.wait_for_operation(self.timeout, operation)",
          "531:         self.log.info(\"Service %s deleted successfully\", self.project_id)",
          "534: class DataprocMetastoreExportMetadataOperator(BaseOperator):",
          "535:     \"\"\"",
          "536:     Exports metadata from a service.",
          "538:     :param destination_gcs_folder: A Cloud Storage URI of a folder, in the format",
          "539:         ``gs://<bucket_name>/<path_inside_bucket>``. A sub-folder",
          "540:         ``<export_folder>`` containing exported files will be",
          "541:         created below it.",
          "542:     :type destination_gcs_folder: str",
          "543:     :param project_id: Required. The ID of the Google Cloud project that the service belongs to.",
          "544:     :type project_id: str",
          "545:     :param region: Required. The ID of the Google Cloud region that the service belongs to.",
          "546:     :type region: str",
          "547:     :param service_id:  Required. The ID of the metastore service, which is used as the final component of",
          "548:         the metastore service's name. This value must be between 2 and 63 characters long inclusive, begin",
          "549:         with a letter, end with a letter or number, and consist of alphanumeric ASCII characters or",
          "550:         hyphens.",
          "551:         This corresponds to the ``service_id`` field on the ``request`` instance; if ``request`` is",
          "552:         provided, this should not be set.",
          "553:     :type service_id: str",
          "554:     :param request_id: Optional. A unique id used to identify the request.",
          "555:     :type request_id: str",
          "556:     :param retry: Optional. Designation of what errors, if any, should be retried.",
          "557:     :type retry: google.api_core.retry.Retry",
          "558:     :param timeout: Optional. The timeout for this request.",
          "559:     :type timeout: float",
          "560:     :param metadata: Optional. Strings which should be sent along with the request as metadata.",
          "561:     :type metadata: Sequence[Tuple[str, str]]",
          "562:     :param gcp_conn_id: The connection ID to use connecting to Google Cloud.",
          "563:     :type gcp_conn_id: str",
          "564:     :param impersonation_chain: Optional service account to impersonate using short-term",
          "565:         credentials, or chained list of accounts required to get the access_token",
          "566:         of the last account in the list, which will be impersonated in the request.",
          "567:         If set as a string, the account must grant the originating account",
          "568:         the Service Account Token Creator IAM role.",
          "569:         If set as a sequence, the identities from the list must grant",
          "570:         Service Account Token Creator IAM role to the directly preceding identity, with first",
          "571:         account from the list granting this role to the originating account (templated).",
          "572:     :type impersonation_chain: Union[str, Sequence[str]]",
          "573:     \"\"\"",
          "575:     template_fields = (",
          "576:         'project_id',",
          "577:         'impersonation_chain',",
          "578:     )",
          "580:     def __init__(",
          "581:         self,",
          "583:         destination_gcs_folder: str,",
          "584:         project_id: str,",
          "585:         region: str,",
          "586:         service_id: str,",
          "587:         request_id: Optional[str] = None,",
          "588:         database_dump_type: Optional[DatabaseDumpSpec] = None,",
          "589:         retry: Optional[Retry] = None,",
          "590:         timeout: Optional[float] = None,",
          "591:         metadata: Optional[Sequence[Tuple[str, str]]] = (),",
          "592:         gcp_conn_id: str = \"google_cloud_default\",",
          "593:         impersonation_chain: Optional[Union[str, Sequence[str]]] = None,",
          "595:     ) -> None:",
          "596:         super().__init__(**kwargs)",
          "597:         self.destination_gcs_folder = destination_gcs_folder",
          "598:         self.project_id = project_id",
          "599:         self.region = region",
          "600:         self.service_id = service_id",
          "601:         self.request_id = request_id",
          "602:         self.database_dump_type = database_dump_type",
          "603:         self.retry = retry",
          "604:         self.timeout = timeout",
          "605:         self.metadata = metadata",
          "606:         self.gcp_conn_id = gcp_conn_id",
          "607:         self.impersonation_chain = impersonation_chain",
          "609:     def execute(self, context: Dict):",
          "610:         hook = DataprocMetastoreHook(",
          "611:             gcp_conn_id=self.gcp_conn_id, impersonation_chain=self.impersonation_chain",
          "612:         )",
          "613:         self.log.info(\"Exporting metadata from Dataproc Metastore service: %s\", self.service_id)",
          "614:         hook.export_metadata(",
          "615:             destination_gcs_folder=self.destination_gcs_folder,",
          "616:             project_id=self.project_id,",
          "617:             region=self.region,",
          "618:             service_id=self.service_id,",
          "619:             request_id=self.request_id,",
          "620:             database_dump_type=self.database_dump_type,",
          "621:             retry=self.retry,",
          "622:             timeout=self.timeout,",
          "623:             metadata=self.metadata,",
          "624:         )",
          "625:         metadata_export = self._wait_for_export_metadata(hook)",
          "626:         self.log.info(\"Metadata from service %s exported successfully\", self.service_id)",
          "627:         return MetadataExport.to_dict(metadata_export)",
          "629:     def _wait_for_export_metadata(self, hook: DataprocMetastoreHook):",
          "630:         \"\"\"",
          "631:         Workaround to check that export was created successfully.",
          "632:         We discovered a issue to parse result to MetadataExport inside the SDK",
          "633:         \"\"\"",
          "634:         for time_to_wait in exponential_sleep_generator(initial=10, maximum=120):",
          "635:             sleep(time_to_wait)",
          "636:             service = hook.get_service(",
          "637:                 region=self.region,",
          "638:                 project_id=self.project_id,",
          "639:                 service_id=self.service_id,",
          "640:                 retry=self.retry,",
          "641:                 timeout=self.timeout,",
          "642:                 metadata=self.metadata,",
          "643:             )",
          "644:             activities: MetadataManagementActivity = service.metadata_management_activity",
          "645:             metadata_export: MetadataExport = activities.metadata_exports[0]",
          "646:             if metadata_export.state == MetadataExport.State.SUCCEEDED:",
          "647:                 return metadata_export",
          "648:             if metadata_export.state == MetadataExport.State.FAILED:",
          "649:                 raise AirflowException(",
          "650:                     f\"Exporting metadata from Dataproc Metastore {metadata_export.name} FAILED\"",
          "651:                 )",
          "654: class DataprocMetastoreGetServiceOperator(BaseOperator):",
          "655:     \"\"\"",
          "656:     Gets the details of a single service.",
          "658:     :param region: Required. The ID of the Google Cloud region that the service belongs to.",
          "659:     :type region: str",
          "660:     :param project_id: Required. The ID of the Google Cloud project that the service belongs to.",
          "661:     :type project_id: str",
          "662:     :param service_id:  Required. The ID of the metastore service, which is used as the final component of",
          "663:         the metastore service's name. This value must be between 2 and 63 characters long inclusive, begin",
          "664:         with a letter, end with a letter or number, and consist of alphanumeric ASCII characters or",
          "665:         hyphens.",
          "667:         This corresponds to the ``service_id`` field on the ``request`` instance; if ``request`` is",
          "668:         provided, this should not be set.",
          "669:     :type service_id: str",
          "670:     :param retry: Designation of what errors, if any, should be retried.",
          "671:     :type retry: google.api_core.retry.Retry",
          "672:     :param timeout: The timeout for this request.",
          "673:     :type timeout: float",
          "674:     :param metadata: Strings which should be sent along with the request as metadata.",
          "675:     :type metadata: Sequence[Tuple[str, str]]",
          "676:     :param gcp_conn_id: The connection ID to use connecting to Google Cloud.",
          "677:     :type gcp_conn_id: str",
          "678:     :param impersonation_chain: Optional service account to impersonate using short-term",
          "679:         credentials, or chained list of accounts required to get the access_token",
          "680:         of the last account in the list, which will be impersonated in the request.",
          "681:         If set as a string, the account must grant the originating account",
          "682:         the Service Account Token Creator IAM role.",
          "683:         If set as a sequence, the identities from the list must grant",
          "684:         Service Account Token Creator IAM role to the directly preceding identity, with first",
          "685:         account from the list granting this role to the originating account (templated).",
          "686:     :type impersonation_chain: Union[str, Sequence[str]]",
          "687:     \"\"\"",
          "689:     template_fields = (",
          "690:         'project_id',",
          "691:         'impersonation_chain',",
          "692:     )",
          "694:     def __init__(",
          "695:         self,",
          "697:         region: str,",
          "698:         project_id: str,",
          "699:         service_id: str,",
          "700:         retry: Optional[Retry] = None,",
          "701:         timeout: Optional[float] = None,",
          "702:         metadata: Optional[Sequence[Tuple[str, str]]] = (),",
          "703:         gcp_conn_id: str = \"google_cloud_default\",",
          "704:         impersonation_chain: Optional[Union[str, Sequence[str]]] = None,",
          "706:     ) -> None:",
          "707:         super().__init__(**kwargs)",
          "708:         self.region = region",
          "709:         self.project_id = project_id",
          "710:         self.service_id = service_id",
          "711:         self.retry = retry",
          "712:         self.timeout = timeout",
          "713:         self.metadata = metadata",
          "714:         self.gcp_conn_id = gcp_conn_id",
          "715:         self.impersonation_chain = impersonation_chain",
          "717:     def execute(self, context) -> dict:",
          "718:         hook = DataprocMetastoreHook(",
          "719:             gcp_conn_id=self.gcp_conn_id, impersonation_chain=self.impersonation_chain",
          "720:         )",
          "721:         self.log.info(\"Gets the details of a single Dataproc Metastore service: %s\", self.project_id)",
          "722:         result = hook.get_service(",
          "723:             region=self.region,",
          "724:             project_id=self.project_id,",
          "725:             service_id=self.service_id,",
          "726:             retry=self.retry,",
          "727:             timeout=self.timeout,",
          "728:             metadata=self.metadata,",
          "729:         )",
          "730:         return Service.to_dict(result)",
          "733: class DataprocMetastoreListBackupsOperator(BaseOperator):",
          "734:     \"\"\"",
          "735:     Lists backups in a service.",
          "737:     :param project_id: Required. The ID of the Google Cloud project that the backup belongs to.",
          "738:     :type project_id: str",
          "739:     :param region: Required. The ID of the Google Cloud region that the backup belongs to.",
          "740:     :type region: str",
          "741:     :param service_id: Required. The ID of the metastore service, which is used as the final component of",
          "742:         the metastore service's name. This value must be between 2 and 63 characters long inclusive, begin",
          "743:         with a letter, end with a letter or number, and consist of alphanumeric ASCII characters or",
          "744:         hyphens.",
          "746:         This corresponds to the ``service_id`` field on the ``request`` instance; if ``request`` is",
          "747:         provided, this should not be set.",
          "748:     :type service_id: str",
          "749:     :param retry: Optional. Designation of what errors, if any, should be retried.",
          "750:     :type retry: google.api_core.retry.Retry",
          "751:     :param timeout: Optional. The timeout for this request.",
          "752:     :type timeout: float",
          "753:     :param metadata: Optional. Strings which should be sent along with the request as metadata.",
          "754:     :type metadata: Sequence[Tuple[str, str]]",
          "755:     :param gcp_conn_id: The connection ID to use connecting to Google Cloud.",
          "756:     :type gcp_conn_id: str",
          "757:     :param impersonation_chain: Optional service account to impersonate using short-term",
          "758:         credentials, or chained list of accounts required to get the access_token",
          "759:         of the last account in the list, which will be impersonated in the request.",
          "760:         If set as a string, the account must grant the originating account",
          "761:         the Service Account Token Creator IAM role.",
          "762:         If set as a sequence, the identities from the list must grant",
          "763:         Service Account Token Creator IAM role to the directly preceding identity, with first",
          "764:         account from the list granting this role to the originating account (templated).",
          "765:     :type impersonation_chain: Union[str, Sequence[str]]",
          "766:     \"\"\"",
          "768:     template_fields = (",
          "769:         'project_id',",
          "770:         'impersonation_chain',",
          "771:     )",
          "773:     def __init__(",
          "774:         self,",
          "776:         project_id: str,",
          "777:         region: str,",
          "778:         service_id: str,",
          "779:         page_size: Optional[int] = None,",
          "780:         page_token: Optional[str] = None,",
          "781:         filter: Optional[str] = None,",
          "782:         order_by: Optional[str] = None,",
          "783:         retry: Optional[Retry] = None,",
          "784:         timeout: Optional[float] = None,",
          "785:         metadata: Optional[Sequence[Tuple[str, str]]] = (),",
          "786:         gcp_conn_id: str = \"google_cloud_default\",",
          "787:         impersonation_chain: Optional[Union[str, Sequence[str]]] = None,",
          "789:     ) -> None:",
          "790:         super().__init__(**kwargs)",
          "791:         self.project_id = project_id",
          "792:         self.region = region",
          "793:         self.service_id = service_id",
          "794:         self.page_size = page_size",
          "795:         self.page_token = page_token",
          "796:         self.filter = filter",
          "797:         self.order_by = order_by",
          "798:         self.retry = retry",
          "799:         self.timeout = timeout",
          "800:         self.metadata = metadata",
          "801:         self.gcp_conn_id = gcp_conn_id",
          "802:         self.impersonation_chain = impersonation_chain",
          "804:     def execute(self, context: dict) -> dict:",
          "805:         hook = DataprocMetastoreHook(",
          "806:             gcp_conn_id=self.gcp_conn_id, impersonation_chain=self.impersonation_chain",
          "807:         )",
          "808:         self.log.info(\"Listing Dataproc Metastore backups: %s\", self.service_id)",
          "809:         backups = hook.list_backups(",
          "810:             project_id=self.project_id,",
          "811:             region=self.region,",
          "812:             service_id=self.service_id,",
          "813:             page_size=self.page_size,",
          "814:             page_token=self.page_token,",
          "815:             filter=self.filter,",
          "816:             order_by=self.order_by,",
          "817:             retry=self.retry,",
          "818:             timeout=self.timeout,",
          "819:             metadata=self.metadata,",
          "820:         )",
          "821:         return [Backup.to_dict(backup) for backup in backups]",
          "824: class DataprocMetastoreRestoreServiceOperator(BaseOperator):",
          "825:     \"\"\"",
          "826:     Restores a service from a backup.",
          "828:     :param project_id: Required. The ID of the Google Cloud project that the service belongs to.",
          "829:     :type project_id: str",
          "830:     :param region: Required. The ID of the Google Cloud region that the service belongs to.",
          "831:     :type region: str",
          "832:     :param service_id: Required. The ID of the metastore service, which is used as the final component of",
          "833:         the metastore service's name. This value must be between 2 and 63 characters long inclusive, begin",
          "834:         with a letter, end with a letter or number, and consist of alphanumeric ASCII characters or",
          "835:         hyphens.",
          "837:         This corresponds to the ``service_id`` field on the ``request`` instance; if ``request`` is",
          "838:         provided, this should not be set.",
          "839:     :type service_id: str",
          "840:     :param backup_project_id: Required. The ID of the Google Cloud project that the metastore",
          "841:         service backup to restore from.",
          "842:     :type backup_project_id: str",
          "843:     :param backup_region: Required. The ID of the Google Cloud region that the metastore",
          "844:         service backup to restore from.",
          "845:     :type backup_region: str",
          "846:     :param backup_service_id:  Required. The ID of the metastore service backup to restore from, which is",
          "847:         used as the final component of the metastore service's name. This value must be between 2 and 63",
          "848:         characters long inclusive, begin with a letter, end with a letter or number, and consist",
          "849:         of alphanumeric ASCII characters or hyphens.",
          "850:     :type backup_service_id: str",
          "851:     :param backup_id:  Required. The ID of the metastore service backup to restore from",
          "852:     :type backup_id: str",
          "853:     :param restore_type: Optional. The type of restore. If unspecified, defaults to",
          "854:         ``METADATA_ONLY``",
          "855:     :type restore_type: google.cloud.metastore_v1.types.Restore.RestoreType",
          "856:     :param request_id: Optional. A unique id used to identify the request.",
          "857:     :type request_id: str",
          "858:     :param retry: Optional. Designation of what errors, if any, should be retried.",
          "859:     :type retry: google.api_core.retry.Retry",
          "860:     :param timeout: Optional. The timeout for this request.",
          "861:     :type timeout: float",
          "862:     :param metadata: Optional. Strings which should be sent along with the request as metadata.",
          "863:     :type metadata: Sequence[Tuple[str, str]]",
          "864:     :param gcp_conn_id: The connection ID to use connecting to Google Cloud.",
          "865:     :type gcp_conn_id: str",
          "866:     :param impersonation_chain: Optional service account to impersonate using short-term",
          "867:         credentials, or chained list of accounts required to get the access_token",
          "868:         of the last account in the list, which will be impersonated in the request.",
          "869:         If set as a string, the account must grant the originating account",
          "870:         the Service Account Token Creator IAM role.",
          "871:         If set as a sequence, the identities from the list must grant",
          "872:         Service Account Token Creator IAM role to the directly preceding identity, with first",
          "873:         account from the list granting this role to the originating account (templated).",
          "874:     :type impersonation_chain: Union[str, Sequence[str]]",
          "875:     \"\"\"",
          "877:     template_fields = (",
          "878:         'project_id',",
          "879:         'impersonation_chain',",
          "880:     )",
          "882:     def __init__(",
          "883:         self,",
          "885:         project_id: str,",
          "886:         region: str,",
          "887:         service_id: str,",
          "888:         backup_project_id: str,",
          "889:         backup_region: str,",
          "890:         backup_service_id: str,",
          "891:         backup_id: str,",
          "892:         restore_type: Optional[Restore] = None,",
          "893:         request_id: Optional[str] = None,",
          "894:         retry: Optional[Retry] = None,",
          "895:         timeout: Optional[float] = None,",
          "896:         metadata: Optional[Sequence[Tuple[str, str]]] = (),",
          "897:         gcp_conn_id: str = \"google_cloud_default\",",
          "898:         impersonation_chain: Optional[Union[str, Sequence[str]]] = None,",
          "900:     ) -> None:",
          "901:         super().__init__(**kwargs)",
          "902:         self.project_id = project_id",
          "903:         self.region = region",
          "904:         self.service_id = service_id",
          "905:         self.backup_project_id = backup_project_id",
          "906:         self.backup_region = backup_region",
          "907:         self.backup_service_id = backup_service_id",
          "908:         self.backup_id = backup_id",
          "909:         self.restore_type = restore_type",
          "910:         self.request_id = request_id",
          "911:         self.retry = retry",
          "912:         self.timeout = timeout",
          "913:         self.metadata = metadata",
          "914:         self.gcp_conn_id = gcp_conn_id",
          "915:         self.impersonation_chain = impersonation_chain",
          "917:     def execute(self, context) -> dict:",
          "918:         hook = DataprocMetastoreHook(",
          "919:             gcp_conn_id=self.gcp_conn_id, impersonation_chain=self.impersonation_chain",
          "920:         )",
          "921:         self.log.info(",
          "922:             \"Restoring Dataproc Metastore service: %s from backup: %s\", self.service_id, self.backup_id",
          "923:         )",
          "924:         hook.restore_service(",
          "925:             project_id=self.project_id,",
          "926:             region=self.region,",
          "927:             service_id=self.service_id,",
          "928:             backup_project_id=self.backup_project_id,",
          "929:             backup_region=self.backup_region,",
          "930:             backup_service_id=self.backup_service_id,",
          "931:             backup_id=self.backup_id,",
          "932:             restore_type=self.restore_type,",
          "933:             request_id=self.request_id,",
          "934:             retry=self.retry,",
          "935:             timeout=self.timeout,",
          "936:             metadata=self.metadata,",
          "937:         )",
          "938:         self._wait_for_restore_service(hook)",
          "939:         self.log.info(\"Service %s restored from backup %s\", self.service_id, self.backup_id)",
          "941:     def _wait_for_restore_service(self, hook: DataprocMetastoreHook):",
          "942:         \"\"\"",
          "943:         Workaround to check that restore service was finished successfully.",
          "944:         We discovered an issue to parse result to Restore inside the SDK",
          "945:         \"\"\"",
          "946:         for time_to_wait in exponential_sleep_generator(initial=10, maximum=120):",
          "947:             sleep(time_to_wait)",
          "948:             service = hook.get_service(",
          "949:                 region=self.region,",
          "950:                 project_id=self.project_id,",
          "951:                 service_id=self.service_id,",
          "952:                 retry=self.retry,",
          "953:                 timeout=self.timeout,",
          "954:                 metadata=self.metadata,",
          "955:             )",
          "956:             activities: MetadataManagementActivity = service.metadata_management_activity",
          "957:             restore_service: Restore = activities.restores[0]",
          "958:             if restore_service.state == Restore.State.SUCCEEDED:",
          "959:                 return restore_service",
          "960:             if restore_service.state == Restore.State.FAILED:",
          "961:                 raise AirflowException(\"Restoring service FAILED\")",
          "964: class DataprocMetastoreUpdateServiceOperator(BaseOperator):",
          "965:     \"\"\"",
          "966:     Updates the parameters of a single service.",
          "968:     :param project_id: Required. The ID of the Google Cloud project that the service belongs to.",
          "969:     :type project_id: str",
          "970:     :param region: Required. The ID of the Google Cloud region that the service belongs to.",
          "971:     :type region: str",
          "972:     :param service_id:  Required. The ID of the metastore service, which is used as the final component of",
          "973:         the metastore service's name. This value must be between 2 and 63 characters long inclusive, begin",
          "974:         with a letter, end with a letter or number, and consist of alphanumeric ASCII characters or",
          "975:         hyphens.",
          "977:         This corresponds to the ``service_id`` field on the ``request`` instance; if ``request`` is",
          "978:         provided, this should not be set.",
          "979:     :type service_id: str",
          "980:     :param service:  Required. The metastore service to update. The server only merges fields in the service",
          "981:         if they are specified in ``update_mask``.",
          "983:         The metastore service's ``name`` field is used to identify the metastore service to be updated.",
          "985:         This corresponds to the ``service`` field on the ``request`` instance; if ``request`` is provided,",
          "986:         this should not be set.",
          "987:     :type service: Union[Dict, google.cloud.metastore_v1.types.Service]",
          "988:     :param update_mask:  Required. A field mask used to specify the fields to be overwritten in the metastore",
          "989:         service resource by the update. Fields specified in the ``update_mask`` are relative to the resource",
          "990:         (not to the full request). A field is overwritten if it is in the mask.",
          "992:         This corresponds to the ``update_mask`` field on the ``request`` instance; if ``request`` is provided,",
          "993:         this should not be set.",
          "994:     :type update_mask: google.protobuf.field_mask_pb2.FieldMask",
          "995:     :param request_id: Optional. A unique id used to identify the request.",
          "996:     :type request_id: str",
          "997:     :param retry: Optional. Designation of what errors, if any, should be retried.",
          "998:     :type retry: google.api_core.retry.Retry",
          "999:     :param timeout: Optional. The timeout for this request.",
          "1000:     :type timeout: float",
          "1001:     :param metadata: Optional. Strings which should be sent along with the request as metadata.",
          "1002:     :type metadata: Sequence[Tuple[str, str]]",
          "1003:     :param gcp_conn_id: The connection ID to use connecting to Google Cloud.",
          "1004:     :type gcp_conn_id: str",
          "1005:     :param impersonation_chain: Optional service account to impersonate using short-term",
          "1006:         credentials, or chained list of accounts required to get the access_token",
          "1007:         of the last account in the list, which will be impersonated in the request.",
          "1008:         If set as a string, the account must grant the originating account",
          "1009:         the Service Account Token Creator IAM role.",
          "1010:         If set as a sequence, the identities from the list must grant",
          "1011:         Service Account Token Creator IAM role to the directly preceding identity, with first",
          "1012:         account from the list granting this role to the originating account (templated).",
          "1013:     :type impersonation_chain: Union[str, Sequence[str]]",
          "1014:     \"\"\"",
          "1016:     template_fields = (",
          "1017:         'project_id',",
          "1018:         'impersonation_chain',",
          "1019:     )",
          "1021:     def __init__(",
          "1022:         self,",
          "1024:         project_id: str,",
          "1025:         region: str,",
          "1026:         service_id: str,",
          "1027:         service: Union[Dict, Service],",
          "1028:         update_mask: Union[Dict, FieldMask],",
          "1029:         request_id: Optional[str] = None,",
          "1030:         retry: Optional[Retry] = None,",
          "1031:         timeout: Optional[float] = None,",
          "1032:         metadata: Optional[Sequence[Tuple[str, str]]] = (),",
          "1033:         gcp_conn_id: str = \"google_cloud_default\",",
          "1034:         impersonation_chain: Optional[Union[str, Sequence[str]]] = None,",
          "1036:     ) -> None:",
          "1037:         super().__init__(**kwargs)",
          "1038:         self.project_id = project_id",
          "1039:         self.region = region",
          "1040:         self.service_id = service_id",
          "1041:         self.service = service",
          "1042:         self.update_mask = update_mask",
          "1043:         self.request_id = request_id",
          "1044:         self.retry = retry",
          "1045:         self.timeout = timeout",
          "1046:         self.metadata = metadata",
          "1047:         self.gcp_conn_id = gcp_conn_id",
          "1048:         self.impersonation_chain = impersonation_chain",
          "1050:     def execute(self, context: Dict):",
          "1051:         hook = DataprocMetastoreHook(",
          "1052:             gcp_conn_id=self.gcp_conn_id, impersonation_chain=self.impersonation_chain",
          "1053:         )",
          "1054:         self.log.info(\"Updating Dataproc Metastore service: %s\", self.service.get(\"name\"))",
          "1056:         operation = hook.update_service(",
          "1057:             project_id=self.project_id,",
          "1058:             region=self.region,",
          "1059:             service_id=self.service_id,",
          "1060:             service=self.service,",
          "1061:             update_mask=self.update_mask,",
          "1062:             request_id=self.request_id,",
          "1063:             retry=self.retry,",
          "1064:             timeout=self.timeout,",
          "1065:             metadata=self.metadata,",
          "1066:         )",
          "1067:         hook.wait_for_operation(self.timeout, operation)",
          "1068:         self.log.info(\"Service %s updated successfully\", self.service.get(\"name\"))",
          "",
          "---------------"
        ],
        "setup.py||setup.py": [
          "File: setup.py -> setup.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "304:     'google-cloud-container>=0.1.1,<2.0.0',",
          "305:     'google-cloud-datacatalog>=3.0.0,<4.0.0',",
          "306:     'google-cloud-dataproc>=2.2.0,<4.0.0',",
          "307:     'google-cloud-dlp>=0.11.0,<2.0.0',",
          "308:     'google-cloud-kms>=2.0.0,<3.0.0',",
          "309:     'google-cloud-language>=1.1.1,<2.0.0',",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "307:     'google-cloud-dataproc-metastore>=1.2.0,<2.0.0',",
          "",
          "---------------"
        ],
        "tests/providers/google/cloud/hooks/test_dataproc_metastore.py||tests/providers/google/cloud/hooks/test_dataproc_metastore.py": [
          "File: tests/providers/google/cloud/hooks/test_dataproc_metastore.py -> tests/providers/google/cloud/hooks/test_dataproc_metastore.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: #",
          "2: # Licensed to the Apache Software Foundation (ASF) under one",
          "3: # or more contributor license agreements.  See the NOTICE file",
          "4: # distributed with this work for additional information",
          "5: # regarding copyright ownership.  The ASF licenses this file",
          "6: # to you under the Apache License, Version 2.0 (the",
          "7: # \"License\"); you may not use this file except in compliance",
          "8: # with the License.  You may obtain a copy of the License at",
          "9: #",
          "10: #   http://www.apache.org/licenses/LICENSE-2.0",
          "11: #",
          "12: # Unless required by applicable law or agreed to in writing,",
          "13: # software distributed under the License is distributed on an",
          "14: # \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
          "15: # KIND, either express or implied.  See the License for the",
          "16: # specific language governing permissions and limitations",
          "17: # under the License.",
          "18: #",
          "20: from unittest import TestCase, mock",
          "22: from airflow.providers.google.cloud.hooks.dataproc_metastore import DataprocMetastoreHook",
          "23: from tests.providers.google.cloud.utils.base_gcp_mock import (",
          "24:     mock_base_gcp_hook_default_project_id,",
          "25:     mock_base_gcp_hook_no_default_project_id,",
          "26: )",
          "28: TEST_GCP_CONN_ID: str = \"test-gcp-conn-id\"",
          "29: TEST_REGION: str = \"test-region\"",
          "30: TEST_PROJECT_ID: str = \"test-project-id\"",
          "31: TEST_BACKUP: str = \"test-backup\"",
          "32: TEST_BACKUP_ID: str = \"test-backup-id\"",
          "33: TEST_METADATA_IMPORT: dict = {",
          "34:     \"name\": \"test-metadata-import\",",
          "35:     \"database_dump\": {",
          "36:         \"gcs_uri\": \"gs://bucket_name/path_inside_bucket\",",
          "37:         \"database_type\": \"MYSQL\",",
          "38:     },",
          "39: }",
          "40: TEST_METADATA_IMPORT_ID: str = \"test-metadata-import-id\"",
          "41: TEST_SERVICE: dict = {\"name\": \"test-service\"}",
          "42: TEST_SERVICE_ID: str = \"test-service-id\"",
          "43: TEST_SERVICE_TO_UPDATE = {",
          "44:     \"labels\": {",
          "45:         \"first_key\": \"first_value\",",
          "46:         \"second_key\": \"second_value\",",
          "47:     }",
          "48: }",
          "49: TEST_UPDATE_MASK: dict = {\"paths\": [\"labels\"]}",
          "50: TEST_PARENT: str = \"projects/{}/locations/{}\"",
          "51: TEST_PARENT_SERVICES: str = \"projects/{}/locations/{}/services/{}\"",
          "52: TEST_PARENT_BACKUPS: str = \"projects/{}/locations/{}/services/{}/backups\"",
          "53: TEST_NAME_BACKUPS: str = \"projects/{}/locations/{}/services/{}/backups/{}\"",
          "54: TEST_DESTINATION_GCS_FOLDER: str = \"gs://bucket_name/path_inside_bucket\"",
          "56: BASE_STRING = \"airflow.providers.google.common.hooks.base_google.{}\"",
          "57: DATAPROC_METASTORE_STRING = \"airflow.providers.google.cloud.hooks.dataproc_metastore.{}\"",
          "60: class TestDataprocMetastoreWithDefaultProjectIdHook(TestCase):",
          "61:     def setUp(self):",
          "62:         with mock.patch(",
          "63:             BASE_STRING.format(\"GoogleBaseHook.__init__\"), new=mock_base_gcp_hook_default_project_id",
          "64:         ):",
          "65:             self.hook = DataprocMetastoreHook(gcp_conn_id=TEST_GCP_CONN_ID)",
          "67:     @mock.patch(DATAPROC_METASTORE_STRING.format(\"DataprocMetastoreHook.get_dataproc_metastore_client\"))",
          "68:     def test_create_backup(self, mock_client) -> None:",
          "69:         self.hook.create_backup(",
          "70:             project_id=TEST_PROJECT_ID,",
          "71:             region=TEST_REGION,",
          "72:             service_id=TEST_SERVICE_ID,",
          "73:             backup=TEST_BACKUP,",
          "74:             backup_id=TEST_BACKUP_ID,",
          "75:         )",
          "76:         mock_client.assert_called_once()",
          "77:         mock_client.return_value.create_backup.assert_called_once_with(",
          "78:             request=dict(",
          "79:                 parent=TEST_PARENT_SERVICES.format(TEST_PROJECT_ID, TEST_REGION, TEST_SERVICE_ID),",
          "80:                 backup=TEST_BACKUP,",
          "81:                 backup_id=TEST_BACKUP_ID,",
          "82:                 request_id=None,",
          "83:             ),",
          "84:             metadata=None,",
          "85:             retry=None,",
          "86:             timeout=None,",
          "87:         )",
          "89:     @mock.patch(DATAPROC_METASTORE_STRING.format(\"DataprocMetastoreHook.get_dataproc_metastore_client\"))",
          "90:     def test_create_metadata_import(self, mock_client) -> None:",
          "91:         self.hook.create_metadata_import(",
          "92:             project_id=TEST_PROJECT_ID,",
          "93:             region=TEST_REGION,",
          "94:             service_id=TEST_SERVICE_ID,",
          "95:             metadata_import=TEST_METADATA_IMPORT,",
          "96:             metadata_import_id=TEST_METADATA_IMPORT_ID,",
          "97:         )",
          "98:         mock_client.assert_called_once()",
          "99:         mock_client.return_value.create_metadata_import.assert_called_once_with(",
          "100:             request=dict(",
          "101:                 parent=TEST_PARENT_SERVICES.format(TEST_PROJECT_ID, TEST_REGION, TEST_SERVICE_ID),",
          "102:                 metadata_import=TEST_METADATA_IMPORT,",
          "103:                 metadata_import_id=TEST_METADATA_IMPORT_ID,",
          "104:                 request_id=None,",
          "105:             ),",
          "106:             metadata=None,",
          "107:             retry=None,",
          "108:             timeout=None,",
          "109:         )",
          "111:     @mock.patch(DATAPROC_METASTORE_STRING.format(\"DataprocMetastoreHook.get_dataproc_metastore_client\"))",
          "112:     def test_create_service(self, mock_client) -> None:",
          "113:         self.hook.create_service(",
          "114:             region=TEST_REGION,",
          "115:             project_id=TEST_PROJECT_ID,",
          "116:             service=TEST_SERVICE,",
          "117:             service_id=TEST_SERVICE_ID,",
          "118:         )",
          "119:         mock_client.assert_called_once()",
          "120:         mock_client.return_value.create_service.assert_called_once_with(",
          "121:             request=dict(",
          "122:                 parent=TEST_PARENT.format(TEST_PROJECT_ID, TEST_REGION),",
          "123:                 service_id=TEST_SERVICE_ID,",
          "124:                 service=TEST_SERVICE,",
          "125:                 request_id=None,",
          "126:             ),",
          "127:             metadata=(),",
          "128:             retry=None,",
          "129:             timeout=None,",
          "130:         )",
          "132:     @mock.patch(DATAPROC_METASTORE_STRING.format(\"DataprocMetastoreHook.get_dataproc_metastore_client\"))",
          "133:     def test_delete_backup(self, mock_client) -> None:",
          "134:         self.hook.delete_backup(",
          "135:             project_id=TEST_PROJECT_ID,",
          "136:             region=TEST_REGION,",
          "137:             service_id=TEST_SERVICE_ID,",
          "138:             backup_id=TEST_BACKUP_ID,",
          "139:         )",
          "140:         mock_client.assert_called_once()",
          "141:         mock_client.return_value.delete_backup.assert_called_once_with(",
          "142:             request=dict(",
          "143:                 name=TEST_NAME_BACKUPS.format(TEST_PROJECT_ID, TEST_REGION, TEST_SERVICE_ID, TEST_BACKUP_ID),",
          "144:                 request_id=None,",
          "145:             ),",
          "146:             metadata=None,",
          "147:             retry=None,",
          "148:             timeout=None,",
          "149:         )",
          "151:     @mock.patch(DATAPROC_METASTORE_STRING.format(\"DataprocMetastoreHook.get_dataproc_metastore_client\"))",
          "152:     def test_delete_service(self, mock_client) -> None:",
          "153:         self.hook.delete_service(",
          "154:             project_id=TEST_PROJECT_ID,",
          "155:             region=TEST_REGION,",
          "156:             service_id=TEST_SERVICE_ID,",
          "157:         )",
          "158:         mock_client.assert_called_once()",
          "159:         mock_client.return_value.delete_service.assert_called_once_with(",
          "160:             request=dict(",
          "161:                 name=TEST_PARENT_SERVICES.format(TEST_PROJECT_ID, TEST_REGION, TEST_SERVICE_ID),",
          "162:                 request_id=None,",
          "163:             ),",
          "164:             retry=None,",
          "165:             timeout=None,",
          "166:             metadata=None,",
          "167:         )",
          "169:     @mock.patch(DATAPROC_METASTORE_STRING.format(\"DataprocMetastoreHook.get_dataproc_metastore_client\"))",
          "170:     def test_export_metadata(self, mock_client) -> None:",
          "171:         self.hook.export_metadata(",
          "172:             destination_gcs_folder=TEST_DESTINATION_GCS_FOLDER,",
          "173:             project_id=TEST_PROJECT_ID,",
          "174:             region=TEST_REGION,",
          "175:             service_id=TEST_SERVICE_ID,",
          "176:         )",
          "177:         mock_client.assert_called_once()",
          "178:         mock_client.return_value.export_metadata.assert_called_once_with(",
          "179:             request=dict(",
          "180:                 destination_gcs_folder=TEST_DESTINATION_GCS_FOLDER,",
          "181:                 service=TEST_PARENT_SERVICES.format(TEST_PROJECT_ID, TEST_REGION, TEST_SERVICE_ID),",
          "182:                 request_id=None,",
          "183:                 database_dump_type=None,",
          "184:             ),",
          "185:             retry=None,",
          "186:             timeout=None,",
          "187:             metadata=None,",
          "188:         )",
          "190:     @mock.patch(DATAPROC_METASTORE_STRING.format(\"DataprocMetastoreHook.get_dataproc_metastore_client\"))",
          "191:     def test_get_service(self, mock_client) -> None:",
          "192:         self.hook.get_service(",
          "193:             project_id=TEST_PROJECT_ID,",
          "194:             region=TEST_REGION,",
          "195:             service_id=TEST_SERVICE_ID,",
          "196:         )",
          "197:         mock_client.assert_called_once()",
          "198:         mock_client.return_value.get_service.assert_called_once_with(",
          "199:             request=dict(",
          "200:                 name=TEST_PARENT_SERVICES.format(TEST_PROJECT_ID, TEST_REGION, TEST_SERVICE_ID),",
          "201:             ),",
          "202:             metadata=None,",
          "203:             retry=None,",
          "204:             timeout=None,",
          "205:         )",
          "207:     @mock.patch(DATAPROC_METASTORE_STRING.format(\"DataprocMetastoreHook.get_dataproc_metastore_client\"))",
          "208:     def test_list_backups(self, mock_client) -> None:",
          "209:         self.hook.list_backups(",
          "210:             project_id=TEST_PROJECT_ID,",
          "211:             region=TEST_REGION,",
          "212:             service_id=TEST_SERVICE_ID,",
          "213:         )",
          "214:         mock_client.assert_called_once()",
          "215:         mock_client.return_value.list_backups.assert_called_once_with(",
          "216:             request=dict(",
          "217:                 parent=TEST_PARENT_BACKUPS.format(TEST_PROJECT_ID, TEST_REGION, TEST_SERVICE_ID),",
          "218:                 page_size=None,",
          "219:                 page_token=None,",
          "220:                 filter=None,",
          "221:                 order_by=None,",
          "222:             ),",
          "223:             metadata=None,",
          "224:             retry=None,",
          "225:             timeout=None,",
          "226:         )",
          "228:     @mock.patch(DATAPROC_METASTORE_STRING.format(\"DataprocMetastoreHook.get_dataproc_metastore_client\"))",
          "229:     def test_restore_service(self, mock_client) -> None:",
          "230:         self.hook.restore_service(",
          "231:             project_id=TEST_PROJECT_ID,",
          "232:             region=TEST_REGION,",
          "233:             service_id=TEST_SERVICE_ID,",
          "234:             backup_project_id=TEST_PROJECT_ID,",
          "235:             backup_region=TEST_REGION,",
          "236:             backup_service_id=TEST_SERVICE_ID,",
          "237:             backup_id=TEST_BACKUP_ID,",
          "238:         )",
          "239:         mock_client.assert_called_once()",
          "240:         mock_client.return_value.restore_service.assert_called_once_with(",
          "241:             request=dict(",
          "242:                 service=TEST_PARENT_SERVICES.format(TEST_PROJECT_ID, TEST_REGION, TEST_SERVICE_ID),",
          "243:                 backup=TEST_NAME_BACKUPS.format(",
          "244:                     TEST_PROJECT_ID, TEST_REGION, TEST_SERVICE_ID, TEST_BACKUP_ID",
          "245:                 ),",
          "246:                 restore_type=None,",
          "247:                 request_id=None,",
          "248:             ),",
          "249:             metadata=None,",
          "250:             retry=None,",
          "251:             timeout=None,",
          "252:         )",
          "254:     @mock.patch(DATAPROC_METASTORE_STRING.format(\"DataprocMetastoreHook.get_dataproc_metastore_client\"))",
          "255:     def test_update_service(self, mock_client) -> None:",
          "256:         self.hook.update_service(",
          "257:             project_id=TEST_PROJECT_ID,",
          "258:             region=TEST_REGION,",
          "259:             service_id=TEST_SERVICE_ID,",
          "260:             service=TEST_SERVICE_TO_UPDATE,",
          "261:             update_mask=TEST_UPDATE_MASK,",
          "262:         )",
          "263:         mock_client.assert_called_once()",
          "264:         mock_client.return_value.update_service.assert_called_once_with(",
          "265:             request=dict(",
          "266:                 service=TEST_SERVICE_TO_UPDATE,",
          "267:                 update_mask=TEST_UPDATE_MASK,",
          "268:                 request_id=None,",
          "269:             ),",
          "270:             retry=None,",
          "271:             timeout=None,",
          "272:             metadata=None,",
          "273:         )",
          "276: class TestDataprocMetastoreWithoutDefaultProjectIdHook(TestCase):",
          "277:     def setUp(self):",
          "278:         with mock.patch(",
          "279:             BASE_STRING.format(\"GoogleBaseHook.__init__\"), new=mock_base_gcp_hook_no_default_project_id",
          "280:         ):",
          "281:             self.hook = DataprocMetastoreHook(gcp_conn_id=TEST_GCP_CONN_ID)",
          "283:     @mock.patch(DATAPROC_METASTORE_STRING.format(\"DataprocMetastoreHook.get_dataproc_metastore_client\"))",
          "284:     def test_create_backup(self, mock_client) -> None:",
          "285:         self.hook.create_backup(",
          "286:             project_id=TEST_PROJECT_ID,",
          "287:             region=TEST_REGION,",
          "288:             service_id=TEST_SERVICE_ID,",
          "289:             backup=TEST_BACKUP,",
          "290:             backup_id=TEST_BACKUP_ID,",
          "291:         )",
          "292:         mock_client.assert_called_once()",
          "293:         mock_client.return_value.create_backup.assert_called_once_with(",
          "294:             request=dict(",
          "295:                 parent=TEST_PARENT_SERVICES.format(TEST_PROJECT_ID, TEST_REGION, TEST_SERVICE_ID),",
          "296:                 backup=TEST_BACKUP,",
          "297:                 backup_id=TEST_BACKUP_ID,",
          "298:                 request_id=None,",
          "299:             ),",
          "300:             metadata=None,",
          "301:             retry=None,",
          "302:             timeout=None,",
          "303:         )",
          "305:     @mock.patch(DATAPROC_METASTORE_STRING.format(\"DataprocMetastoreHook.get_dataproc_metastore_client\"))",
          "306:     def test_create_metadata_import(self, mock_client) -> None:",
          "307:         self.hook.create_metadata_import(",
          "308:             project_id=TEST_PROJECT_ID,",
          "309:             region=TEST_REGION,",
          "310:             service_id=TEST_SERVICE_ID,",
          "311:             metadata_import=TEST_METADATA_IMPORT,",
          "312:             metadata_import_id=TEST_METADATA_IMPORT_ID,",
          "313:         )",
          "314:         mock_client.assert_called_once()",
          "315:         mock_client.return_value.create_metadata_import.assert_called_once_with(",
          "316:             request=dict(",
          "317:                 parent=TEST_PARENT_SERVICES.format(TEST_PROJECT_ID, TEST_REGION, TEST_SERVICE_ID),",
          "318:                 metadata_import=TEST_METADATA_IMPORT,",
          "319:                 metadata_import_id=TEST_METADATA_IMPORT_ID,",
          "320:                 request_id=None,",
          "321:             ),",
          "322:             metadata=None,",
          "323:             retry=None,",
          "324:             timeout=None,",
          "325:         )",
          "327:     @mock.patch(DATAPROC_METASTORE_STRING.format(\"DataprocMetastoreHook.get_dataproc_metastore_client\"))",
          "328:     def test_create_service(self, mock_client) -> None:",
          "329:         self.hook.create_service(",
          "330:             region=TEST_REGION,",
          "331:             project_id=TEST_PROJECT_ID,",
          "332:             service=TEST_SERVICE,",
          "333:             service_id=TEST_SERVICE_ID,",
          "334:         )",
          "335:         mock_client.assert_called_once()",
          "336:         mock_client.return_value.create_service.assert_called_once_with(",
          "337:             request=dict(",
          "338:                 parent=TEST_PARENT.format(TEST_PROJECT_ID, TEST_REGION),",
          "339:                 service_id=TEST_SERVICE_ID,",
          "340:                 service=TEST_SERVICE,",
          "341:                 request_id=None,",
          "342:             ),",
          "343:             metadata=(),",
          "344:             retry=None,",
          "345:             timeout=None,",
          "346:         )",
          "348:     @mock.patch(DATAPROC_METASTORE_STRING.format(\"DataprocMetastoreHook.get_dataproc_metastore_client\"))",
          "349:     def test_delete_backup(self, mock_client) -> None:",
          "350:         self.hook.delete_backup(",
          "351:             project_id=TEST_PROJECT_ID,",
          "352:             region=TEST_REGION,",
          "353:             service_id=TEST_SERVICE_ID,",
          "354:             backup_id=TEST_BACKUP_ID,",
          "355:         )",
          "356:         mock_client.assert_called_once()",
          "357:         mock_client.return_value.delete_backup.assert_called_once_with(",
          "358:             request=dict(",
          "359:                 name=TEST_NAME_BACKUPS.format(TEST_PROJECT_ID, TEST_REGION, TEST_SERVICE_ID, TEST_BACKUP_ID),",
          "360:                 request_id=None,",
          "361:             ),",
          "362:             metadata=None,",
          "363:             retry=None,",
          "364:             timeout=None,",
          "365:         )",
          "367:     @mock.patch(DATAPROC_METASTORE_STRING.format(\"DataprocMetastoreHook.get_dataproc_metastore_client\"))",
          "368:     def test_delete_service(self, mock_client) -> None:",
          "369:         self.hook.delete_service(",
          "370:             project_id=TEST_PROJECT_ID,",
          "371:             region=TEST_REGION,",
          "372:             service_id=TEST_SERVICE_ID,",
          "373:         )",
          "374:         mock_client.assert_called_once()",
          "375:         mock_client.return_value.delete_service.assert_called_once_with(",
          "376:             request=dict(",
          "377:                 name=TEST_PARENT_SERVICES.format(TEST_PROJECT_ID, TEST_REGION, TEST_SERVICE_ID),",
          "378:                 request_id=None,",
          "379:             ),",
          "380:             retry=None,",
          "381:             timeout=None,",
          "382:             metadata=None,",
          "383:         )",
          "385:     @mock.patch(DATAPROC_METASTORE_STRING.format(\"DataprocMetastoreHook.get_dataproc_metastore_client\"))",
          "386:     def test_export_metadata(self, mock_client) -> None:",
          "387:         self.hook.export_metadata(",
          "388:             destination_gcs_folder=TEST_DESTINATION_GCS_FOLDER,",
          "389:             project_id=TEST_PROJECT_ID,",
          "390:             region=TEST_REGION,",
          "391:             service_id=TEST_SERVICE_ID,",
          "392:         )",
          "393:         mock_client.assert_called_once()",
          "394:         mock_client.return_value.export_metadata.assert_called_once_with(",
          "395:             request=dict(",
          "396:                 destination_gcs_folder=TEST_DESTINATION_GCS_FOLDER,",
          "397:                 service=TEST_PARENT_SERVICES.format(TEST_PROJECT_ID, TEST_REGION, TEST_SERVICE_ID),",
          "398:                 request_id=None,",
          "399:                 database_dump_type=None,",
          "400:             ),",
          "401:             retry=None,",
          "402:             timeout=None,",
          "403:             metadata=None,",
          "404:         )",
          "406:     @mock.patch(DATAPROC_METASTORE_STRING.format(\"DataprocMetastoreHook.get_dataproc_metastore_client\"))",
          "407:     def test_get_service(self, mock_client) -> None:",
          "408:         self.hook.get_service(",
          "409:             project_id=TEST_PROJECT_ID,",
          "410:             region=TEST_REGION,",
          "411:             service_id=TEST_SERVICE_ID,",
          "412:         )",
          "413:         mock_client.assert_called_once()",
          "414:         mock_client.return_value.get_service.assert_called_once_with(",
          "415:             request=dict(",
          "416:                 name=TEST_PARENT_SERVICES.format(TEST_PROJECT_ID, TEST_REGION, TEST_SERVICE_ID),",
          "417:             ),",
          "418:             metadata=None,",
          "419:             retry=None,",
          "420:             timeout=None,",
          "421:         )",
          "423:     @mock.patch(DATAPROC_METASTORE_STRING.format(\"DataprocMetastoreHook.get_dataproc_metastore_client\"))",
          "424:     def test_list_backups(self, mock_client) -> None:",
          "425:         self.hook.list_backups(",
          "426:             project_id=TEST_PROJECT_ID,",
          "427:             region=TEST_REGION,",
          "428:             service_id=TEST_SERVICE_ID,",
          "429:         )",
          "430:         mock_client.assert_called_once()",
          "431:         mock_client.return_value.list_backups.assert_called_once_with(",
          "432:             request=dict(",
          "433:                 parent=TEST_PARENT_BACKUPS.format(TEST_PROJECT_ID, TEST_REGION, TEST_SERVICE_ID),",
          "434:                 page_size=None,",
          "435:                 page_token=None,",
          "436:                 filter=None,",
          "437:                 order_by=None,",
          "438:             ),",
          "439:             metadata=None,",
          "440:             retry=None,",
          "441:             timeout=None,",
          "442:         )",
          "444:     @mock.patch(DATAPROC_METASTORE_STRING.format(\"DataprocMetastoreHook.get_dataproc_metastore_client\"))",
          "445:     def test_restore_service(self, mock_client) -> None:",
          "446:         self.hook.restore_service(",
          "447:             project_id=TEST_PROJECT_ID,",
          "448:             region=TEST_REGION,",
          "449:             service_id=TEST_SERVICE_ID,",
          "450:             backup_project_id=TEST_PROJECT_ID,",
          "451:             backup_region=TEST_REGION,",
          "452:             backup_service_id=TEST_SERVICE_ID,",
          "453:             backup_id=TEST_BACKUP_ID,",
          "454:         )",
          "455:         mock_client.assert_called_once()",
          "456:         mock_client.return_value.restore_service.assert_called_once_with(",
          "457:             request=dict(",
          "458:                 service=TEST_PARENT_SERVICES.format(TEST_PROJECT_ID, TEST_REGION, TEST_SERVICE_ID),",
          "459:                 backup=TEST_NAME_BACKUPS.format(",
          "460:                     TEST_PROJECT_ID, TEST_REGION, TEST_SERVICE_ID, TEST_BACKUP_ID",
          "461:                 ),",
          "462:                 restore_type=None,",
          "463:                 request_id=None,",
          "464:             ),",
          "465:             metadata=None,",
          "466:             retry=None,",
          "467:             timeout=None,",
          "468:         )",
          "470:     @mock.patch(DATAPROC_METASTORE_STRING.format(\"DataprocMetastoreHook.get_dataproc_metastore_client\"))",
          "471:     def test_update_service(self, mock_client) -> None:",
          "472:         self.hook.update_service(",
          "473:             project_id=TEST_PROJECT_ID,",
          "474:             region=TEST_REGION,",
          "475:             service_id=TEST_SERVICE_ID,",
          "476:             service=TEST_SERVICE_TO_UPDATE,",
          "477:             update_mask=TEST_UPDATE_MASK,",
          "478:         )",
          "479:         mock_client.assert_called_once()",
          "480:         mock_client.return_value.update_service.assert_called_once_with(",
          "481:             request=dict(",
          "482:                 service=TEST_SERVICE_TO_UPDATE,",
          "483:                 update_mask=TEST_UPDATE_MASK,",
          "484:                 request_id=None,",
          "485:             ),",
          "486:             retry=None,",
          "487:             timeout=None,",
          "488:             metadata=None,",
          "489:         )",
          "",
          "---------------"
        ],
        "tests/providers/google/cloud/operators/test_dataproc_metastore.py||tests/providers/google/cloud/operators/test_dataproc_metastore.py": [
          "File: tests/providers/google/cloud/operators/test_dataproc_metastore.py -> tests/providers/google/cloud/operators/test_dataproc_metastore.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: # Licensed to the Apache Software Foundation (ASF) under one",
          "2: # or more contributor license agreements.  See the NOTICE file",
          "3: # distributed with this work for additional information",
          "4: # regarding copyright ownership.  The ASF licenses this file",
          "5: # to you under the Apache License, Version 2.0 (the",
          "6: # \"License\"); you may not use this file except in compliance",
          "7: # with the License.  You may obtain a copy of the License at",
          "8: #",
          "9: #   http://www.apache.org/licenses/LICENSE-2.0",
          "10: #",
          "11: # Unless required by applicable law or agreed to in writing,",
          "12: # software distributed under the License is distributed on an",
          "13: # \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
          "14: # KIND, either express or implied.  See the License for the",
          "15: # specific language governing permissions and limitations",
          "16: # under the License.",
          "18: from unittest import TestCase, mock",
          "20: from google.api_core.retry import Retry",
          "22: from airflow.providers.google.cloud.operators.dataproc_metastore import (",
          "23:     DataprocMetastoreCreateBackupOperator,",
          "24:     DataprocMetastoreCreateMetadataImportOperator,",
          "25:     DataprocMetastoreCreateServiceOperator,",
          "26:     DataprocMetastoreDeleteBackupOperator,",
          "27:     DataprocMetastoreDeleteServiceOperator,",
          "28:     DataprocMetastoreExportMetadataOperator,",
          "29:     DataprocMetastoreGetServiceOperator,",
          "30:     DataprocMetastoreListBackupsOperator,",
          "31:     DataprocMetastoreRestoreServiceOperator,",
          "32:     DataprocMetastoreUpdateServiceOperator,",
          "33: )",
          "35: TASK_ID: str = \"task_id\"",
          "36: GCP_LOCATION: str = \"test-location\"",
          "37: GCP_PROJECT_ID: str = \"test-project-id\"",
          "39: GCP_CONN_ID: str = \"test-gcp-conn-id\"",
          "40: IMPERSONATION_CHAIN = [\"ACCOUNT_1\", \"ACCOUNT_2\", \"ACCOUNT_3\"]",
          "42: TEST_SERVICE: dict = {\"name\": \"test-service\"}",
          "43: TEST_SERVICE_ID: str = \"test-service-id\"",
          "45: TEST_TIMEOUT = 120",
          "46: TEST_RETRY = mock.MagicMock(Retry)",
          "47: TEST_METADATA = [(\"key\", \"value\")]",
          "48: TEST_REQUEST_ID = \"request_id_uuid\"",
          "50: TEST_BACKUP: dict = {\"name\": \"test-backup\"}",
          "51: TEST_BACKUP_ID: str = \"test-backup-id\"",
          "52: TEST_METADATA_IMPORT: dict = {",
          "53:     \"name\": \"test-metadata-import\",",
          "54:     \"database_dump\": {",
          "55:         \"gcs_uri\": \"gs://bucket_name/path_inside_bucket\",",
          "56:         \"database_type\": \"MYSQL\",",
          "57:     },",
          "58: }",
          "59: TEST_METADATA_IMPORT_ID: str = \"test-metadata-import-id\"",
          "60: TEST_SERVICE_TO_UPDATE = {",
          "61:     \"labels\": {",
          "62:         \"first_key\": \"first_value\",",
          "63:         \"second_key\": \"second_value\",",
          "64:     }",
          "65: }",
          "66: TEST_UPDATE_MASK: dict = {\"paths\": [\"labels\"]}",
          "67: TEST_DESTINATION_GCS_FOLDER: str = \"gs://bucket_name/path_inside_bucket\"",
          "70: class TestDataprocMetastoreCreateBackupOperator(TestCase):",
          "71:     @mock.patch(\"airflow.providers.google.cloud.operators.dataproc_metastore.DataprocMetastoreHook\")",
          "72:     @mock.patch(\"airflow.providers.google.cloud.operators.dataproc_metastore.Backup\")",
          "73:     def test_assert_valid_hook_call(self, mock_backup, mock_hook) -> None:",
          "74:         task = DataprocMetastoreCreateBackupOperator(",
          "75:             task_id=TASK_ID,",
          "76:             project_id=GCP_PROJECT_ID,",
          "77:             region=GCP_LOCATION,",
          "78:             backup=TEST_BACKUP,",
          "79:             backup_id=TEST_BACKUP_ID,",
          "80:             service_id=TEST_SERVICE_ID,",
          "81:             retry=TEST_RETRY,",
          "82:             timeout=TEST_TIMEOUT,",
          "83:             metadata=TEST_METADATA,",
          "84:             gcp_conn_id=GCP_CONN_ID,",
          "85:             impersonation_chain=IMPERSONATION_CHAIN,",
          "86:         )",
          "87:         mock_hook.return_value.wait_for_operation.return_value = None",
          "88:         mock_backup.return_value.to_dict.return_value = None",
          "89:         task.execute(context=mock.MagicMock())",
          "90:         mock_hook.assert_called_once_with(gcp_conn_id=GCP_CONN_ID, impersonation_chain=IMPERSONATION_CHAIN)",
          "91:         mock_hook.return_value.create_backup.assert_called_once_with(",
          "92:             project_id=GCP_PROJECT_ID,",
          "93:             region=GCP_LOCATION,",
          "94:             backup=TEST_BACKUP,",
          "95:             backup_id=TEST_BACKUP_ID,",
          "96:             service_id=TEST_SERVICE_ID,",
          "97:             request_id=None,",
          "98:             retry=TEST_RETRY,",
          "99:             timeout=TEST_TIMEOUT,",
          "100:             metadata=TEST_METADATA,",
          "101:         )",
          "104: class TestDataprocMetastoreCreateMetadataImportOperator(TestCase):",
          "105:     @mock.patch(\"airflow.providers.google.cloud.operators.dataproc_metastore.DataprocMetastoreHook\")",
          "106:     @mock.patch(\"airflow.providers.google.cloud.operators.dataproc_metastore.MetadataImport\")",
          "107:     def test_assert_valid_hook_call(self, mock_metadata_import, mock_hook) -> None:",
          "108:         task = DataprocMetastoreCreateMetadataImportOperator(",
          "109:             task_id=TASK_ID,",
          "110:             project_id=GCP_PROJECT_ID,",
          "111:             region=GCP_LOCATION,",
          "112:             service_id=TEST_SERVICE_ID,",
          "113:             metadata_import=TEST_METADATA_IMPORT,",
          "114:             metadata_import_id=TEST_METADATA_IMPORT_ID,",
          "115:             retry=TEST_RETRY,",
          "116:             timeout=TEST_TIMEOUT,",
          "117:             metadata=TEST_METADATA,",
          "118:             gcp_conn_id=GCP_CONN_ID,",
          "119:             impersonation_chain=IMPERSONATION_CHAIN,",
          "120:         )",
          "121:         mock_hook.return_value.wait_for_operation.return_value = None",
          "122:         mock_metadata_import.return_value.to_dict.return_value = None",
          "123:         task.execute(context=mock.MagicMock())",
          "124:         mock_hook.assert_called_once_with(gcp_conn_id=GCP_CONN_ID, impersonation_chain=IMPERSONATION_CHAIN)",
          "125:         mock_hook.return_value.create_metadata_import.assert_called_once_with(",
          "126:             project_id=GCP_PROJECT_ID,",
          "127:             region=GCP_LOCATION,",
          "128:             service_id=TEST_SERVICE_ID,",
          "129:             metadata_import=TEST_METADATA_IMPORT,",
          "130:             metadata_import_id=TEST_METADATA_IMPORT_ID,",
          "131:             request_id=None,",
          "132:             retry=TEST_RETRY,",
          "133:             timeout=TEST_TIMEOUT,",
          "134:             metadata=TEST_METADATA,",
          "135:         )",
          "138: class TestDataprocMetastoreCreateServiceOperator(TestCase):",
          "139:     @mock.patch(\"airflow.providers.google.cloud.operators.dataproc_metastore.DataprocMetastoreHook\")",
          "140:     @mock.patch(\"airflow.providers.google.cloud.operators.dataproc_metastore.Service\")",
          "141:     def test_execute(self, mock_service, mock_hook) -> None:",
          "142:         task = DataprocMetastoreCreateServiceOperator(",
          "143:             task_id=TASK_ID,",
          "144:             region=GCP_LOCATION,",
          "145:             project_id=GCP_PROJECT_ID,",
          "146:             service=TEST_SERVICE,",
          "147:             service_id=TEST_SERVICE_ID,",
          "148:             request_id=TEST_REQUEST_ID,",
          "149:             retry=TEST_RETRY,",
          "150:             timeout=TEST_TIMEOUT,",
          "151:             metadata=TEST_METADATA,",
          "152:             gcp_conn_id=GCP_CONN_ID,",
          "153:             impersonation_chain=IMPERSONATION_CHAIN,",
          "154:         )",
          "155:         mock_hook.return_value.wait_for_operation.return_value = None",
          "156:         mock_service.return_value.to_dict.return_value = None",
          "157:         task.execute(context=mock.MagicMock())",
          "158:         mock_hook.assert_called_once_with(gcp_conn_id=GCP_CONN_ID, impersonation_chain=IMPERSONATION_CHAIN)",
          "159:         mock_hook.return_value.create_service.assert_called_once_with(",
          "160:             region=GCP_LOCATION,",
          "161:             project_id=GCP_PROJECT_ID,",
          "162:             service=TEST_SERVICE,",
          "163:             service_id=TEST_SERVICE_ID,",
          "164:             request_id=TEST_REQUEST_ID,",
          "165:             retry=TEST_RETRY,",
          "166:             timeout=TEST_TIMEOUT,",
          "167:             metadata=TEST_METADATA,",
          "168:         )",
          "171: class TestDataprocMetastoreDeleteBackupOperator(TestCase):",
          "172:     @mock.patch(\"airflow.providers.google.cloud.operators.dataproc_metastore.DataprocMetastoreHook\")",
          "173:     def test_assert_valid_hook_call(self, mock_hook) -> None:",
          "174:         task = DataprocMetastoreDeleteBackupOperator(",
          "175:             task_id=TASK_ID,",
          "176:             project_id=GCP_PROJECT_ID,",
          "177:             region=GCP_LOCATION,",
          "178:             retry=TEST_RETRY,",
          "179:             service_id=TEST_SERVICE_ID,",
          "180:             backup_id=TEST_BACKUP_ID,",
          "181:             timeout=TEST_TIMEOUT,",
          "182:             metadata=TEST_METADATA,",
          "183:             gcp_conn_id=GCP_CONN_ID,",
          "184:             impersonation_chain=IMPERSONATION_CHAIN,",
          "185:         )",
          "186:         mock_hook.return_value.wait_for_operation.return_value = None",
          "187:         task.execute(context=mock.MagicMock())",
          "188:         mock_hook.assert_called_once_with(gcp_conn_id=GCP_CONN_ID, impersonation_chain=IMPERSONATION_CHAIN)",
          "189:         mock_hook.return_value.delete_backup.assert_called_once_with(",
          "190:             project_id=GCP_PROJECT_ID,",
          "191:             region=GCP_LOCATION,",
          "192:             service_id=TEST_SERVICE_ID,",
          "193:             backup_id=TEST_BACKUP_ID,",
          "194:             request_id=None,",
          "195:             retry=TEST_RETRY,",
          "196:             timeout=TEST_TIMEOUT,",
          "197:             metadata=TEST_METADATA,",
          "198:         )",
          "201: class TestDataprocMetastoreDeleteServiceOperator(TestCase):",
          "202:     @mock.patch(\"airflow.providers.google.cloud.operators.dataproc_metastore.DataprocMetastoreHook\")",
          "203:     def test_execute(self, mock_hook) -> None:",
          "204:         task = DataprocMetastoreDeleteServiceOperator(",
          "205:             task_id=TASK_ID,",
          "206:             region=GCP_LOCATION,",
          "207:             project_id=GCP_PROJECT_ID,",
          "208:             service_id=TEST_SERVICE_ID,",
          "209:             retry=TEST_RETRY,",
          "210:             timeout=TEST_TIMEOUT,",
          "211:             metadata=TEST_METADATA,",
          "212:             gcp_conn_id=GCP_CONN_ID,",
          "213:             impersonation_chain=IMPERSONATION_CHAIN,",
          "214:         )",
          "215:         mock_hook.return_value.wait_for_operation.return_value = None",
          "216:         task.execute(context=mock.MagicMock())",
          "217:         mock_hook.assert_called_once_with(gcp_conn_id=GCP_CONN_ID, impersonation_chain=IMPERSONATION_CHAIN)",
          "218:         mock_hook.return_value.delete_service.assert_called_once_with(",
          "219:             region=GCP_LOCATION,",
          "220:             project_id=GCP_PROJECT_ID,",
          "221:             service_id=TEST_SERVICE_ID,",
          "222:             retry=TEST_RETRY,",
          "223:             timeout=TEST_TIMEOUT,",
          "224:             metadata=TEST_METADATA,",
          "225:         )",
          "228: class TestDataprocMetastoreExportMetadataOperator(TestCase):",
          "229:     @mock.patch(\"airflow.providers.google.cloud.operators.dataproc_metastore.DataprocMetastoreHook\")",
          "230:     @mock.patch(\"airflow.providers.google.cloud.operators.dataproc_metastore.MetadataExport\")",
          "231:     @mock.patch(",
          "232:         \"airflow.providers.google.cloud.operators.dataproc_metastore\"",
          "233:         \".DataprocMetastoreExportMetadataOperator._wait_for_export_metadata\"",
          "234:     )",
          "235:     def test_assert_valid_hook_call(self, mock_wait, mock_export_metadata, mock_hook) -> None:",
          "236:         task = DataprocMetastoreExportMetadataOperator(",
          "237:             task_id=TASK_ID,",
          "238:             service_id=TEST_SERVICE_ID,",
          "239:             destination_gcs_folder=TEST_DESTINATION_GCS_FOLDER,",
          "240:             project_id=GCP_PROJECT_ID,",
          "241:             region=GCP_LOCATION,",
          "242:             retry=TEST_RETRY,",
          "243:             timeout=TEST_TIMEOUT,",
          "244:             metadata=TEST_METADATA,",
          "245:             gcp_conn_id=GCP_CONN_ID,",
          "246:             impersonation_chain=IMPERSONATION_CHAIN,",
          "247:         )",
          "248:         mock_wait.return_value = None",
          "249:         mock_export_metadata.return_value.to_dict.return_value = None",
          "250:         task.execute(context=mock.MagicMock())",
          "251:         mock_hook.assert_called_once_with(gcp_conn_id=GCP_CONN_ID, impersonation_chain=IMPERSONATION_CHAIN)",
          "252:         mock_hook.return_value.export_metadata.assert_called_once_with(",
          "253:             database_dump_type=None,",
          "254:             destination_gcs_folder=TEST_DESTINATION_GCS_FOLDER,",
          "255:             project_id=GCP_PROJECT_ID,",
          "256:             region=GCP_LOCATION,",
          "257:             service_id=TEST_SERVICE_ID,",
          "258:             request_id=None,",
          "259:             retry=TEST_RETRY,",
          "260:             timeout=TEST_TIMEOUT,",
          "261:             metadata=TEST_METADATA,",
          "262:         )",
          "265: class TestDataprocMetastoreGetServiceOperator(TestCase):",
          "266:     @mock.patch(\"airflow.providers.google.cloud.operators.dataproc_metastore.DataprocMetastoreHook\")",
          "267:     @mock.patch(\"airflow.providers.google.cloud.operators.dataproc_metastore.Service\")",
          "268:     def test_execute(self, mock_service, mock_hook) -> None:",
          "269:         task = DataprocMetastoreGetServiceOperator(",
          "270:             task_id=TASK_ID,",
          "271:             region=GCP_LOCATION,",
          "272:             project_id=GCP_PROJECT_ID,",
          "273:             service_id=TEST_SERVICE_ID,",
          "274:             retry=TEST_RETRY,",
          "275:             timeout=TEST_TIMEOUT,",
          "276:             metadata=TEST_METADATA,",
          "277:             gcp_conn_id=GCP_CONN_ID,",
          "278:             impersonation_chain=IMPERSONATION_CHAIN,",
          "279:         )",
          "280:         mock_hook.return_value.wait_for_operation.return_value = None",
          "281:         mock_service.return_value.to_dict.return_value = None",
          "282:         task.execute(context=mock.MagicMock())",
          "283:         mock_hook.assert_called_once_with(gcp_conn_id=GCP_CONN_ID, impersonation_chain=IMPERSONATION_CHAIN)",
          "284:         mock_hook.return_value.get_service.assert_called_once_with(",
          "285:             region=GCP_LOCATION,",
          "286:             project_id=GCP_PROJECT_ID,",
          "287:             service_id=TEST_SERVICE_ID,",
          "288:             retry=TEST_RETRY,",
          "289:             timeout=TEST_TIMEOUT,",
          "290:             metadata=TEST_METADATA,",
          "291:         )",
          "294: class TestDataprocMetastoreListBackupsOperator(TestCase):",
          "295:     @mock.patch(\"airflow.providers.google.cloud.operators.dataproc_metastore.DataprocMetastoreHook\")",
          "296:     @mock.patch(\"airflow.providers.google.cloud.operators.dataproc_metastore.Backup\")",
          "297:     def test_assert_valid_hook_call(self, mock_backup, mock_hook) -> None:",
          "298:         task = DataprocMetastoreListBackupsOperator(",
          "299:             task_id=TASK_ID,",
          "300:             project_id=GCP_PROJECT_ID,",
          "301:             region=GCP_LOCATION,",
          "302:             service_id=TEST_SERVICE_ID,",
          "303:             retry=TEST_RETRY,",
          "304:             timeout=TEST_TIMEOUT,",
          "305:             metadata=TEST_METADATA,",
          "306:             gcp_conn_id=GCP_CONN_ID,",
          "307:             impersonation_chain=IMPERSONATION_CHAIN,",
          "308:         )",
          "309:         mock_hook.return_value.wait_for_operation.return_value = None",
          "310:         mock_backup.return_value.to_dict.return_value = None",
          "311:         task.execute(context=mock.MagicMock())",
          "312:         mock_hook.assert_called_once_with(gcp_conn_id=GCP_CONN_ID, impersonation_chain=IMPERSONATION_CHAIN)",
          "313:         mock_hook.return_value.list_backups.assert_called_once_with(",
          "314:             project_id=GCP_PROJECT_ID,",
          "315:             region=GCP_LOCATION,",
          "316:             service_id=TEST_SERVICE_ID,",
          "317:             retry=TEST_RETRY,",
          "318:             timeout=TEST_TIMEOUT,",
          "319:             metadata=TEST_METADATA,",
          "320:             filter=None,",
          "321:             order_by=None,",
          "322:             page_size=None,",
          "323:             page_token=None,",
          "324:         )",
          "327: class TestDataprocMetastoreRestoreServiceOperator(TestCase):",
          "328:     @mock.patch(\"airflow.providers.google.cloud.operators.dataproc_metastore.DataprocMetastoreHook\")",
          "329:     @mock.patch(",
          "330:         \"airflow.providers.google.cloud.operators.dataproc_metastore\"",
          "331:         \".DataprocMetastoreRestoreServiceOperator._wait_for_restore_service\"",
          "332:     )",
          "333:     def test_assert_valid_hook_call(self, mock_wait, mock_hook) -> None:",
          "334:         task = DataprocMetastoreRestoreServiceOperator(",
          "335:             task_id=TASK_ID,",
          "336:             region=GCP_LOCATION,",
          "337:             project_id=GCP_PROJECT_ID,",
          "338:             service_id=TEST_SERVICE_ID,",
          "339:             backup_id=TEST_BACKUP_ID,",
          "340:             backup_region=GCP_LOCATION,",
          "341:             backup_project_id=GCP_PROJECT_ID,",
          "342:             backup_service_id=TEST_SERVICE_ID,",
          "343:             retry=TEST_RETRY,",
          "344:             timeout=TEST_TIMEOUT,",
          "345:             metadata=TEST_METADATA,",
          "346:             gcp_conn_id=GCP_CONN_ID,",
          "347:             impersonation_chain=IMPERSONATION_CHAIN,",
          "348:         )",
          "349:         mock_wait.return_value = None",
          "350:         task.execute(context=mock.MagicMock())",
          "351:         mock_hook.assert_called_once_with(gcp_conn_id=GCP_CONN_ID, impersonation_chain=IMPERSONATION_CHAIN)",
          "352:         mock_hook.return_value.restore_service.assert_called_once_with(",
          "353:             region=GCP_LOCATION,",
          "354:             project_id=GCP_PROJECT_ID,",
          "355:             service_id=TEST_SERVICE_ID,",
          "356:             backup_id=TEST_BACKUP_ID,",
          "357:             backup_region=GCP_LOCATION,",
          "358:             backup_project_id=GCP_PROJECT_ID,",
          "359:             backup_service_id=TEST_SERVICE_ID,",
          "360:             restore_type=None,",
          "361:             request_id=None,",
          "362:             retry=TEST_RETRY,",
          "363:             timeout=TEST_TIMEOUT,",
          "364:             metadata=TEST_METADATA,",
          "365:         )",
          "368: class TestDataprocMetastoreUpdateServiceOperator(TestCase):",
          "369:     @mock.patch(\"airflow.providers.google.cloud.operators.dataproc_metastore.DataprocMetastoreHook\")",
          "370:     def test_assert_valid_hook_call(self, mock_hook) -> None:",
          "371:         task = DataprocMetastoreUpdateServiceOperator(",
          "372:             task_id=TASK_ID,",
          "373:             region=GCP_LOCATION,",
          "374:             project_id=GCP_PROJECT_ID,",
          "375:             service_id=TEST_SERVICE_ID,",
          "376:             service=TEST_SERVICE_TO_UPDATE,",
          "377:             update_mask=TEST_UPDATE_MASK,",
          "378:             retry=TEST_RETRY,",
          "379:             timeout=TEST_TIMEOUT,",
          "380:             metadata=TEST_METADATA,",
          "381:             gcp_conn_id=GCP_CONN_ID,",
          "382:             impersonation_chain=IMPERSONATION_CHAIN,",
          "383:         )",
          "384:         task.execute(context=mock.MagicMock())",
          "385:         mock_hook.assert_called_once_with(gcp_conn_id=GCP_CONN_ID, impersonation_chain=IMPERSONATION_CHAIN)",
          "386:         mock_hook.return_value.update_service.assert_called_once_with(",
          "387:             region=GCP_LOCATION,",
          "388:             project_id=GCP_PROJECT_ID,",
          "389:             service_id=TEST_SERVICE_ID,",
          "390:             service=TEST_SERVICE_TO_UPDATE,",
          "391:             update_mask=TEST_UPDATE_MASK,",
          "392:             request_id=None,",
          "393:             retry=TEST_RETRY,",
          "394:             timeout=TEST_TIMEOUT,",
          "395:             metadata=TEST_METADATA,",
          "396:         )",
          "",
          "---------------"
        ],
        "tests/providers/google/cloud/operators/test_dataproc_metastore_system.py||tests/providers/google/cloud/operators/test_dataproc_metastore_system.py": [
          "File: tests/providers/google/cloud/operators/test_dataproc_metastore_system.py -> tests/providers/google/cloud/operators/test_dataproc_metastore_system.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: #",
          "2: # Licensed to the Apache Software Foundation (ASF) under one",
          "3: # or more contributor license agreements.  See the NOTICE file",
          "4: # distributed with this work for additional information",
          "5: # regarding copyright ownership.  The ASF licenses this file",
          "6: # to you under the Apache License, Version 2.0 (the",
          "7: # \"License\"); you may not use this file except in compliance",
          "8: # with the License.  You may obtain a copy of the License at",
          "9: #",
          "10: #   http://www.apache.org/licenses/LICENSE-2.0",
          "11: #",
          "12: # Unless required by applicable law or agreed to in writing,",
          "13: # software distributed under the License is distributed on an",
          "14: # \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
          "15: # KIND, either express or implied.  See the License for the",
          "16: # specific language governing permissions and limitations",
          "17: # under the License.",
          "18: import pytest",
          "20: from airflow.providers.google.cloud.example_dags.example_dataproc_metastore import BUCKET",
          "21: from tests.providers.google.cloud.utils.gcp_authenticator import GCP_DATAPROC_KEY",
          "22: from tests.test_utils.gcp_system_helpers import CLOUD_DAG_FOLDER, GoogleSystemTest, provide_gcp_context",
          "25: @pytest.mark.backend(\"mysql\", \"postgres\")",
          "26: @pytest.mark.credential_file(GCP_DATAPROC_KEY)",
          "27: class DataprocMetastoreExampleDagsTest(GoogleSystemTest):",
          "28:     @provide_gcp_context(GCP_DATAPROC_KEY)",
          "29:     def setUp(self):",
          "30:         super().setUp()",
          "31:         self.create_gcs_bucket(BUCKET)",
          "33:     @provide_gcp_context(GCP_DATAPROC_KEY)",
          "34:     def tearDown(self):",
          "35:         self.delete_gcs_bucket(BUCKET)",
          "36:         super().tearDown()",
          "38:     @provide_gcp_context(GCP_DATAPROC_KEY)",
          "39:     def test_run_example_dag(self):",
          "40:         self.run_dag(dag_id=\"example_gcp_dataproc_metastore\", dag_folder=CLOUD_DAG_FOLDER)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "4c60fb2fd2a25ddf2a7097c8254d326f3b6078ed",
      "candidate_info": {
        "commit_hash": "4c60fb2fd2a25ddf2a7097c8254d326f3b6078ed",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/4c60fb2fd2a25ddf2a7097c8254d326f3b6078ed",
        "files": [
          "airflow/providers/yandex/example_dags/example_yandexcloud_dataproc.py",
          "airflow/providers/yandex/hooks/yandex.py",
          "airflow/providers/yandex/operators/yandexcloud_dataproc.py",
          "docs/spelling_wordlist.txt",
          "setup.py",
          "tests/providers/yandex/operators/test_yandexcloud_dataproc.py"
        ],
        "message": "YandexCloud provider: Support new Yandex SDK features: log_group_id, user-agent, maven packages (#20103)\n\n(cherry picked from commit 41c49c7ff6dfa1d6805e5f74c0a36dd549e159ea)",
        "before_after_code_files": [
          "airflow/providers/yandex/example_dags/example_yandexcloud_dataproc.py||airflow/providers/yandex/example_dags/example_yandexcloud_dataproc.py",
          "airflow/providers/yandex/hooks/yandex.py||airflow/providers/yandex/hooks/yandex.py",
          "airflow/providers/yandex/operators/yandexcloud_dataproc.py||airflow/providers/yandex/operators/yandexcloud_dataproc.py",
          "setup.py||setup.py",
          "tests/providers/yandex/operators/test_yandexcloud_dataproc.py||tests/providers/yandex/operators/test_yandexcloud_dataproc.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/providers/yandex/example_dags/example_yandexcloud_dataproc.py||airflow/providers/yandex/example_dags/example_yandexcloud_dataproc.py": [
          "File: airflow/providers/yandex/example_dags/example_yandexcloud_dataproc.py -> airflow/providers/yandex/example_dags/example_yandexcloud_dataproc.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "14: # KIND, either express or implied.  See the License for the",
          "15: # specific language governing permissions and limitations",
          "16: # under the License.",
          "18: from datetime import datetime",
          "20: from airflow import DAG",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "17: import uuid",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "81:             '-input',",
          "82:             's3a://data-proc-public/jobs/sources/data/cities500.txt.bz2',",
          "83:             '-output',",
          "85:         ],",
          "86:         properties={",
          "87:             'yarn.app.mapreduce.am.resource.mb': '2048',",
          "",
          "[Removed Lines]",
          "84:             f's3a://{S3_BUCKET_NAME_FOR_JOB_LOGS}/dataproc/job/results',",
          "",
          "[Added Lines]",
          "84:             f's3a://{S3_BUCKET_NAME_FOR_JOB_LOGS}/dataproc/job/results/{uuid.uuid4()}',",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "113:         properties={",
          "114:             'spark.submit.deployMode': 'cluster',",
          "115:         },",
          "116:     )",
          "118:     create_pyspark_job = DataprocCreatePysparkJobOperator(",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "116:         packages=['org.slf4j:slf4j-simple:1.7.30'],",
          "117:         repositories=['https://repo1.maven.org/maven2'],",
          "118:         exclude_packages=['com.amazonaws:amazon-kinesis-client'],",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "129:         ],",
          "130:         args=[",
          "131:             's3a://data-proc-public/jobs/sources/data/cities500.txt.bz2',",
          "133:         ],",
          "134:         jar_file_uris=[",
          "135:             's3a://data-proc-public/jobs/sources/java/dataproc-examples-1.0.jar',",
          "",
          "[Removed Lines]",
          "132:             f's3a://{S3_BUCKET_NAME_FOR_JOB_LOGS}/jobs/results/${{JOB_ID}}',",
          "",
          "[Added Lines]",
          "135:             f's3a://{S3_BUCKET_NAME_FOR_JOB_LOGS}/dataproc/job/results/${{JOB_ID}}',",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "139:         properties={",
          "140:             'spark.submit.deployMode': 'cluster',",
          "141:         },",
          "142:     )",
          "144:     delete_cluster = DataprocDeleteClusterOperator(",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "145:         packages=['org.slf4j:slf4j-simple:1.7.30'],",
          "146:         repositories=['https://repo1.maven.org/maven2'],",
          "147:         exclude_packages=['com.amazonaws:amazon-kinesis-client'],",
          "",
          "---------------"
        ],
        "airflow/providers/yandex/hooks/yandex.py||airflow/providers/yandex/hooks/yandex.py": [
          "File: airflow/providers/yandex/hooks/yandex.py -> airflow/providers/yandex/hooks/yandex.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "80:             ),",
          "81:         }",
          "83:     @staticmethod",
          "84:     def get_ui_field_behaviour() -> Dict:",
          "85:         \"\"\"Returns custom field behaviour\"\"\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "83:     @classmethod",
          "84:     def provider_user_agent(cls) -> Optional[str]:",
          "85:         \"\"\"Construct User-Agent from Airflow core & provider package versions\"\"\"",
          "86:         import airflow",
          "87:         from airflow.providers_manager import ProvidersManager",
          "89:         try:",
          "90:             manager = ProvidersManager()",
          "91:             provider_name = manager.hooks[cls.conn_type].package_name",
          "92:             provider = manager.providers[provider_name]",
          "93:             return f'apache-airflow/{airflow.__version__} {provider_name}/{provider.version}'",
          "94:         except KeyError:",
          "95:             warnings.warn(f\"Hook '{cls.hook_name}' info is not initialized in airflow.ProviderManager\")",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "107:         self.connection = self.get_connection(self.connection_id)",
          "108:         self.extras = self.connection.extra_dejson",
          "109:         credentials = self._get_credentials()",
          "111:         self.default_folder_id = default_folder_id or self._get_field('folder_id', False)",
          "112:         self.default_public_ssh_key = default_public_ssh_key or self._get_field('public_ssh_key', False)",
          "113:         self.client = self.sdk.client",
          "",
          "[Removed Lines]",
          "110:         self.sdk = yandexcloud.SDK(**credentials)",
          "",
          "[Added Lines]",
          "124:         self.sdk = yandexcloud.SDK(user_agent=self.provider_user_agent(), **credentials)",
          "",
          "---------------"
        ],
        "airflow/providers/yandex/operators/yandexcloud_dataproc.py||airflow/providers/yandex/operators/yandexcloud_dataproc.py": [
          "File: airflow/providers/yandex/operators/yandexcloud_dataproc.py -> airflow/providers/yandex/operators/yandexcloud_dataproc.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "93:     :param computenode_decommission_timeout: Timeout to gracefully decommission nodes during downscaling.",
          "94:                                              In seconds.",
          "95:     :type computenode_decommission_timeout: int",
          "96:     \"\"\"",
          "98:     def __init__(",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "96:     :param log_group_id: Id of log group to write logs. By default logs will be sent to default log group.",
          "97:                     To disable cloud log sending set cluster property dataproc:disable_cloud_logging = true",
          "98:     :type log_group_id: str",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "127:         computenode_cpu_utilization_target: Optional[int] = None,",
          "128:         computenode_decommission_timeout: Optional[int] = None,",
          "129:         connection_id: Optional[str] = None,",
          "131:     ) -> None:",
          "132:         super().__init__(**kwargs)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "133:         log_group_id: Optional[str] = None,",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "159:         self.computenode_preemptible = computenode_preemptible",
          "160:         self.computenode_cpu_utilization_target = computenode_cpu_utilization_target",
          "161:         self.computenode_decommission_timeout = computenode_decommission_timeout",
          "163:         self.hook: Optional[DataprocHook] = None",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "166:         self.log_group_id = log_group_id",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "195:             computenode_preemptible=self.computenode_preemptible,",
          "196:             computenode_cpu_utilization_target=self.computenode_cpu_utilization_target,",
          "197:             computenode_decommission_timeout=self.computenode_decommission_timeout,",
          "198:         )",
          "199:         context['task_instance'].xcom_push(key='cluster_id', value=operation_result.response.id)",
          "200:         context['task_instance'].xcom_push(key='yandexcloud_connection_id', value=self.yandex_conn_id)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "203:             log_group_id=self.log_group_id,",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "399:     :type cluster_id: Optional[str]",
          "400:     :param connection_id: ID of the Yandex.Cloud Airflow connection.",
          "401:     :type connection_id: Optional[str]",
          "402:     \"\"\"",
          "404:     template_fields = ['cluster_id']",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "408:     :param packages: List of maven coordinates of jars to include on the driver and executor classpaths.",
          "409:     :type packages: Optional[Iterable[str]]",
          "410:     :param repositories: List of additional remote repositories to search for the maven coordinates",
          "411:                         given with --packages.",
          "412:     :type repositories: Optional[Iterable[str]]",
          "413:     :param exclude_packages: List of groupId:artifactId, to exclude while resolving the dependencies",
          "414:                         provided in --packages to avoid dependency conflicts.",
          "415:     :type exclude_packages: Optional[Iterable[str]]",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "416:         name: str = 'Spark job',",
          "417:         cluster_id: Optional[str] = None,",
          "418:         connection_id: Optional[str] = None,",
          "420:     ) -> None:",
          "421:         super().__init__(**kwargs)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "433:         packages: Optional[Iterable[str]] = None,",
          "434:         repositories: Optional[Iterable[str]] = None,",
          "435:         exclude_packages: Optional[Iterable[str]] = None,",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "429:         self.name = name",
          "430:         self.cluster_id = cluster_id",
          "431:         self.connection_id = connection_id",
          "432:         self.hook: Optional[DataprocHook] = None",
          "434:     def execute(self, context) -> None:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "449:         self.packages = packages",
          "450:         self.repositories = repositories",
          "451:         self.exclude_packages = exclude_packages",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "447:             file_uris=self.file_uris,",
          "448:             args=self.args,",
          "449:             properties=self.properties,",
          "450:             name=self.name,",
          "451:             cluster_id=cluster_id,",
          "452:         )",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "470:             packages=self.packages,",
          "471:             repositories=self.repositories,",
          "472:             exclude_packages=self.exclude_packages,",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "476:     :type cluster_id: Optional[str]",
          "477:     :param connection_id: ID of the Yandex.Cloud Airflow connection.",
          "478:     :type connection_id: Optional[str]",
          "479:     \"\"\"",
          "481:     template_fields = ['cluster_id']",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "502:     :param packages: List of maven coordinates of jars to include on the driver and executor classpaths.",
          "503:     :type packages: Optional[Iterable[str]]",
          "504:     :param repositories: List of additional remote repositories to search for the maven coordinates",
          "505:                          given with --packages.",
          "506:     :type repositories: Optional[Iterable[str]]",
          "507:     :param exclude_packages: List of groupId:artifactId, to exclude while resolving the dependencies",
          "508:                          provided in --packages to avoid dependency conflicts.",
          "509:     :type exclude_packages: Optional[Iterable[str]]",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "493:         name: str = 'Pyspark job',",
          "494:         cluster_id: Optional[str] = None,",
          "495:         connection_id: Optional[str] = None,",
          "497:     ) -> None:",
          "498:         super().__init__(**kwargs)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "527:         packages: Optional[Iterable[str]] = None,",
          "528:         repositories: Optional[Iterable[str]] = None,",
          "529:         exclude_packages: Optional[Iterable[str]] = None,",
          "",
          "---------------",
          "--- Hunk 11 ---",
          "[Context before]",
          "506:         self.name = name",
          "507:         self.cluster_id = cluster_id",
          "508:         self.connection_id = connection_id",
          "509:         self.hook: Optional[DataprocHook] = None",
          "511:     def execute(self, context) -> None:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "543:         self.packages = packages",
          "544:         self.repositories = repositories",
          "545:         self.exclude_packages = exclude_packages",
          "",
          "---------------",
          "--- Hunk 12 ---",
          "[Context before]",
          "524:             file_uris=self.file_uris,",
          "525:             args=self.args,",
          "526:             properties=self.properties,",
          "527:             name=self.name,",
          "528:             cluster_id=cluster_id,",
          "529:         )",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "564:             packages=self.packages,",
          "565:             repositories=self.repositories,",
          "566:             exclude_packages=self.exclude_packages,",
          "",
          "---------------"
        ],
        "setup.py||setup.py": [
          "File: setup.py -> setup.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "505:     'pywinrm~=0.4',",
          "506: ]",
          "507: yandex = [",
          "509: ]",
          "510: zendesk = [",
          "511:     'zdesk',",
          "",
          "[Removed Lines]",
          "508:     'yandexcloud>=0.97.0',",
          "",
          "[Added Lines]",
          "508:     'yandexcloud>=0.122.0',",
          "",
          "---------------"
        ],
        "tests/providers/yandex/operators/test_yandexcloud_dataproc.py||tests/providers/yandex/operators/test_yandexcloud_dataproc.py": [
          "File: tests/providers/yandex/operators/test_yandexcloud_dataproc.py -> tests/providers/yandex/operators/test_yandexcloud_dataproc.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "60:     'cFDe6faKCxH6iDRteo4D8L8BxwzN42uZSB0nfmjkIxFTcEU3mFSXEbWByg78aoddMrAAjatyrhH1pON6P0='",
          "61: ]",
          "64: class DataprocClusterCreateOperatorTest(TestCase):",
          "65:     def setUp(self):",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "63: # https://cloud.yandex.com/en-ru/docs/logging/concepts/log-group",
          "64: LOG_GROUP_ID = 'my_log_group_id'",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "87:             connection_id=CONNECTION_ID,",
          "88:             s3_bucket=S3_BUCKET_NAME_FOR_LOGS,",
          "89:             cluster_image_version=CLUSTER_IMAGE_VERSION,",
          "90:         )",
          "91:         context = {'task_instance': MagicMock()}",
          "92:         operator.execute(context)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "93:             log_group_id=LOG_GROUP_ID,",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "122:             ],",
          "123:             subnet_id='my_subnet_id',",
          "124:             zone='ru-central1-c',",
          "125:         )",
          "126:         context['task_instance'].xcom_push.assert_has_calls(",
          "127:             [",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "129:             log_group_id=LOG_GROUP_ID,",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "300:             main_jar_file_uri='s3a://data-proc-public/jobs/sources/java/dataproc-examples-1.0.jar',",
          "301:             name='Spark job',",
          "302:             properties={'spark.submit.deployMode': 'cluster'},",
          "303:         )",
          "305:     @patch('airflow.providers.yandex.hooks.yandex.YandexCloudBaseHook._get_credentials')",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "308:             packages=None,",
          "309:             repositories=None,",
          "310:             exclude_packages=None,",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "359:             name='Pyspark job',",
          "360:             properties={'spark.submit.deployMode': 'cluster'},",
          "361:             python_file_uris=['s3a://some-in-bucket/jobs/sources/pyspark-001/geonames.py'],",
          "362:         )",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "370:             packages=None,",
          "371:             repositories=None,",
          "372:             exclude_packages=None,",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "95e71b731feccc6c75b3ae5494e8bd8aac3d9ba6",
      "candidate_info": {
        "commit_hash": "95e71b731feccc6c75b3ae5494e8bd8aac3d9ba6",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/95e71b731feccc6c75b3ae5494e8bd8aac3d9ba6",
        "files": [
          "Dockerfile",
          "Dockerfile.ci"
        ],
        "message": "Remove PyJWT upper bound from Dockerfile (#20503)\n\n(cherry picked from commit 528baf4fd301ff0f97edbf59cb42888a71c6b95c)",
        "before_after_code_files": [
          "Dockerfile.ci||Dockerfile.ci"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "Dockerfile.ci||Dockerfile.ci": [
          "File: Dockerfile.ci -> Dockerfile.ci",
          "--- Hunk 1 ---",
          "[Context before]",
          "272: # force them on the main Airflow package. Those limitations are:",
          "273: # * certifi<2021.0.0: required by snowflake provider",
          "274: # * lazy-object-proxy<1.5.0: required by astroid",
          "276: # * dill<0.3.3 required by apache-beam",
          "277: # * google-ads<14.0.1 required to prevent updating google-python-api>=2.0.0",
          "279: ARG UPGRADE_TO_NEWER_DEPENDENCIES=\"false\"",
          "280: ENV EAGER_UPGRADE_ADDITIONAL_REQUIREMENTS=${EAGER_UPGRADE_ADDITIONAL_REQUIREMENTS} \\",
          "281:     UPGRADE_TO_NEWER_DEPENDENCIES=${UPGRADE_TO_NEWER_DEPENDENCIES}",
          "",
          "[Removed Lines]",
          "275: # * pyjwt<2.0.0: flask-jwt-extended requires it",
          "278: ARG EAGER_UPGRADE_ADDITIONAL_REQUIREMENTS=\"lazy-object-proxy<1.5.0 pyjwt<2.0.0 dill<0.3.3 certifi<2021.0.0 google-ads<14.0.1\"",
          "",
          "[Added Lines]",
          "277: ARG EAGER_UPGRADE_ADDITIONAL_REQUIREMENTS=\"lazy-object-proxy<1.5.0 dill<0.3.3 certifi<2021.0.0 google-ads<14.0.1\"",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "36920077f790f4202238f1bf27608714f0794f25",
      "candidate_info": {
        "commit_hash": "36920077f790f4202238f1bf27608714f0794f25",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/36920077f790f4202238f1bf27608714f0794f25",
        "files": [
          "airflow/cli/commands/standalone_command.py"
        ],
        "message": "20496 fix port standalone mode (#20505)\n\n(cherry picked from commit f743e46c5a4fdd0b76fea2d07729b744644fc416)",
        "before_after_code_files": [
          "airflow/cli/commands/standalone_command.py||airflow/cli/commands/standalone_command.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/cli/commands/standalone_command.py||airflow/cli/commands/standalone_command.py": [
          "File: airflow/cli/commands/standalone_command.py -> airflow/cli/commands/standalone_command.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "81:             command=[\"triggerer\"],",
          "82:             env=env,",
          "83:         )",
          "84:         # Run subcommand threads",
          "85:         for command in self.subcommands.values():",
          "86:             command.start()",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "85:         self.web_server_port = conf.getint('webserver', 'WEB_SERVER_PORT', fallback=8080)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "206:         Detects when all Airflow components are ready to serve.",
          "207:         For now, it's simply time-based.",
          "208:         \"\"\"",
          "211:     def port_open(self, port):",
          "212:         \"\"\"",
          "",
          "[Removed Lines]",
          "209:         return self.port_open(8080) and self.job_running(SchedulerJob) and self.job_running(TriggererJob)",
          "",
          "[Added Lines]",
          "211:         return (",
          "212:             self.port_open(self.web_server_port)",
          "213:             and self.job_running(SchedulerJob)",
          "214:             and self.job_running(TriggererJob)",
          "215:         )",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "60e2b65a8abce533faa5648dfb4925c24b13e520",
      "candidate_info": {
        "commit_hash": "60e2b65a8abce533faa5648dfb4925c24b13e520",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/60e2b65a8abce533faa5648dfb4925c24b13e520",
        "files": [
          "setup.py"
        ],
        "message": "Add pandas requirements for providers that use pandas (#18997)\n\nAs we removed pandas as core airflow requirement, the providers\nthat need it should get pandas explicitlyly as installation\nrequirements.\n\nFixes: #18901\n(cherry picked from commit de98976581294e080967e2aa52043176dffb644f)",
        "before_after_code_files": [
          "setup.py||setup.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "setup.py||setup.py": [
          "File: setup.py -> setup.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "176:         file.write(text)",
          "179: # 'Start dependencies group' and 'Start dependencies group' are mark for ./scripts/ci/check_order_setup.py",
          "180: # If you change this mark you should also change ./scripts/ci/check_order_setup.py",
          "181: # Start dependencies group",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "179: pandas_requirement = 'pandas>=0.17.1, <2.0'",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "186:     'boto3>=1.15.0,<1.19.0',",
          "187:     'watchtower~=1.0.6',",
          "188:     'jsonpath_ng>=1.5.3',",
          "189: ]",
          "190: apache_beam = [",
          "191:     'apache-beam>=2.20.0',",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "191:     'redshift_connector~=2.0.888',",
          "192:     'sqlalchemy_redshift~=0.8.6',",
          "193:     pandas_requirement,",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "269:     'elasticsearch-dbapi',",
          "270:     'elasticsearch-dsl>=5.0.0',",
          "271: ]",
          "275: facebook = [",
          "276:     'facebook-business>=6.0.2',",
          "277: ]",
          "",
          "[Removed Lines]",
          "272: exasol = [",
          "273:     'pyexasol>=0.5.1,<1.0.0',",
          "274: ]",
          "",
          "[Added Lines]",
          "277: exasol = ['pyexasol>=0.5.1,<1.0.0', pandas_requirement]",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "328:     # pandas-gbq 0.15.0 release broke google provider's bigquery import",
          "329:     # _check_google_client_version (airflow/providers/google/cloud/hooks/bigquery.py:49)",
          "330:     'pandas-gbq<0.15.0',",
          "331: ]",
          "332: grpc = [",
          "333:     'google-auth>=1.0.0, <3.0.0',",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "334:     pandas_requirement,",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "344:     'hmsclient>=0.1.0',",
          "345:     'pyhive[hive]>=0.6.0;python_version<\"3.9\"',",
          "346:     'thrift>=0.9.2',",
          "347: ]",
          "348: http = [",
          "349:     # The 2.26.0 release of requests got rid of the chardet LGPL mandatory dependency, allowing us to",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "351:     pandas_requirement,",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "353: http_provider = [",
          "354:     'apache-airflow-providers-http',",
          "355: ]",
          "357: jdbc = [",
          "358:     'jaydebeapi>=1.1.1',",
          "359: ]",
          "",
          "[Removed Lines]",
          "356: influxdb = ['pandas>=0.17.1, <2.0', 'influxdb-client>=1.19.0']",
          "",
          "[Added Lines]",
          "361: influxdb = [",
          "362:     'influxdb-client>=1.19.0',",
          "363:     pandas_requirement,",
          "364: ]",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "400:     'pdpyras>=4.1.2,<5',",
          "401: ]",
          "402: pandas = [",
          "404: ]",
          "405: papermill = [",
          "406:     'papermill[all]>=1.2.1',",
          "",
          "[Removed Lines]",
          "403:     'pandas>=0.17.1, <2.0',",
          "",
          "[Added Lines]",
          "411:     pandas_requirement,",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "421: postgres = [",
          "422:     'psycopg2-binary>=2.7.4',",
          "423: ]",
          "425: psrp = [",
          "426:     'pypsrp~=0.5',",
          "427: ]",
          "",
          "[Removed Lines]",
          "424: presto = ['presto-python-client>=0.7.0,<0.8']",
          "",
          "[Added Lines]",
          "432: presto = [",
          "433:     'presto-python-client>=0.7.0,<0.8',",
          "434:     pandas_requirement,",
          "435: ]",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "434: redis = [",
          "435:     'redis~=3.2',",
          "436: ]",
          "441: samba = [",
          "442:     'smbprotocol>=1.5.0',",
          "443: ]",
          "",
          "[Removed Lines]",
          "437: salesforce = [",
          "438:     'simple-salesforce>=1.0.0',",
          "439:     'tableauserverclient',",
          "440: ]",
          "",
          "[Added Lines]",
          "448: salesforce = ['simple-salesforce>=1.0.0', 'tableauserverclient', pandas_requirement]",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "479: telegram = [",
          "480:     'python-telegram-bot~=13.0',",
          "481: ]",
          "483: vertica = [",
          "484:     'vertica-python>=0.5.1',",
          "485: ]",
          "",
          "[Removed Lines]",
          "482: trino = ['trino>=0.301.0']",
          "",
          "[Added Lines]",
          "490: trino = [",
          "491:     'trino>=0.301.0',",
          "492:     pandas_requirement,",
          "493: ]",
          "",
          "---------------"
        ]
      }
    }
  ]
}