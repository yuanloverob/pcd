{
  "cve_id": "CVE-2022-31777",
  "cve_desc": "A stored cross-site scripting (XSS) vulnerability in Apache Spark 3.2.1 and earlier, and 3.3.0, allows remote attackers to execute arbitrary JavaScript in the web browser of a user, by including a malicious payload into the logs which would be returned in logs rendered in the UI.",
  "repo": "apache/spark",
  "patch_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
  "patch_info": {
    "commit_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
    "repo": "apache/spark",
    "commit_url": "https://github.com/apache/spark/commit/ad90195de56688ce0898691eb9d04297ab0871ad",
    "files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ],
    "message": "[SPARK-39505][UI] Escape log content rendered in UI\n\n### What changes were proposed in this pull request?\n\nEscape log content rendered to the UI.\n\n### Why are the changes needed?\n\nLog content may contain reserved characters or other code in the log and be misinterpreted in the UI as HTML.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nExisting tests\n\nCloses #36902 from srowen/LogViewEscape.\n\nAuthored-by: Sean Owen <srowen@gmail.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
    "before_after_code_files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ]
  },
  "patch_diff": {
    "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js": [
      "File: core/src/main/resources/org/apache/spark/ui/static/log-view.js -> core/src/main/resources/org/apache/spark/ui/static/log-view.js",
      "--- Hunk 1 ---",
      "[Context before]",
      "85:       if (retStartByte == 0) {",
      "86:         disableMoreButton();",
      "87:       }",
      "90:       curLogLength = curLogLength + (startByte - retStartByte);",
      "91:       startByte = retStartByte;",
      "",
      "[Removed Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(cleanData);",
      "",
      "[Added Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(document.createTextNode(cleanData));",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "115:             var retLogLength = dataInfo[2];",
      "117:             var cleanData = data.substring(newlineIndex + 1);",
      "120:             curLogLength = curLogLength + (retEndByte - retStartByte);",
      "121:             endByte = retEndByte;",
      "",
      "[Removed Lines]",
      "118:             $(\"pre\", \".log-content\").append(cleanData);",
      "",
      "[Added Lines]",
      "118:             $(\"pre\", \".log-content\").append(document.createTextNode(cleanData));",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "24d588c8587f84f7e8c1c9f665d55eb14869b707",
      "candidate_info": {
        "commit_hash": "24d588c8587f84f7e8c1c9f665d55eb14869b707",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/24d588c8587f84f7e8c1c9f665d55eb14869b707",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/connector/SupportsPushDownCatalystFilters.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/FileScanBuilder.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/PushDownUtils.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/parquet/ParquetScanBuilder.scala"
        ],
        "message": "[SPARK-38950][SQL] Return Array of Predicate for SupportsPushDownCatalystFilters.pushedFilters\n\n### What changes were proposed in this pull request?\nin `SupportsPushDownCatalystFilters`, change\n```\ndef pushedFilters: Array[Filter]\n```\nto\n\n```\ndef pushedFilters: Array[Predicate]\n```\n\n### Why are the changes needed?\nuse v2Filter in DS V2\n\n### Does this PR introduce _any_ user-facing change?\nyes\n\n### How was this patch tested?\nexisting tests\n\nCloses #36264 from huaxingao/V2Filter.\n\nAuthored-by: huaxingao <huaxin_gao@apple.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\n(cherry picked from commit 7221d754075656ce41edacb0fccc1cf89a62fc77)\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/connector/SupportsPushDownCatalystFilters.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/internal/connector/SupportsPushDownCatalystFilters.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/FileScanBuilder.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/FileScanBuilder.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/PushDownUtils.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/PushDownUtils.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/parquet/ParquetScanBuilder.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/parquet/ParquetScanBuilder.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/connector/SupportsPushDownCatalystFilters.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/internal/connector/SupportsPushDownCatalystFilters.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/internal/connector/SupportsPushDownCatalystFilters.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/internal/connector/SupportsPushDownCatalystFilters.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "17: package org.apache.spark.sql.internal.connector",
          "19: import org.apache.spark.sql.catalyst.expressions.Expression",
          "",
          "[Removed Lines]",
          "20: import org.apache.spark.sql.sources.Filter",
          "",
          "[Added Lines]",
          "20: import org.apache.spark.sql.connector.expressions.filter.Predicate",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "41: }",
          "",
          "[Removed Lines]",
          "40:   def pushedFilters: Array[Filter]",
          "",
          "[Added Lines]",
          "40:   def pushedFilters: Array[Predicate]",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/FileScanBuilder.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/FileScanBuilder.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/FileScanBuilder.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/FileScanBuilder.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "21: import org.apache.spark.sql.{sources, SparkSession}",
          "22: import org.apache.spark.sql.catalyst.expressions.Expression",
          "23: import org.apache.spark.sql.connector.read.{ScanBuilder, SupportsPushDownRequiredColumns}",
          "24: import org.apache.spark.sql.execution.datasources.{DataSourceStrategy, DataSourceUtils, PartitioningAwareFileIndex, PartitioningUtils}",
          "25: import org.apache.spark.sql.internal.connector.SupportsPushDownCatalystFilters",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "23: import org.apache.spark.sql.connector.expressions.filter.Predicate",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "84:     dataFilters",
          "85:   }",
          "",
          "[Removed Lines]",
          "87:   override def pushedFilters: Array[Filter] = pushedDataFilters",
          "",
          "[Added Lines]",
          "88:   override def pushedFilters: Array[Predicate] = pushedDataFilters.map(_.toV2)",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/PushDownUtils.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/PushDownUtils.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/PushDownUtils.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/PushDownUtils.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "99:       case f: FileScanBuilder =>",
          "100:         val postScanFilters = f.pushFilters(filters)",
          "102:       case _ => (Left(Nil), filters)",
          "103:     }",
          "104:   }",
          "",
          "[Removed Lines]",
          "101:         (Left(f.pushedFilters), postScanFilters)",
          "",
          "[Added Lines]",
          "101:         (Right(f.pushedFilters), postScanFilters)",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/parquet/ParquetScanBuilder.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/parquet/ParquetScanBuilder.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/parquet/ParquetScanBuilder.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/parquet/ParquetScanBuilder.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "22: import org.apache.spark.sql.SparkSession",
          "23: import org.apache.spark.sql.catalyst.util.RebaseDateTime.RebaseSpec",
          "24: import org.apache.spark.sql.connector.expressions.aggregate.Aggregation",
          "25: import org.apache.spark.sql.connector.read.{Scan, SupportsPushDownAggregates}",
          "26: import org.apache.spark.sql.execution.datasources.{AggregatePushDownUtils, PartitioningAwareFileIndex}",
          "27: import org.apache.spark.sql.execution.datasources.parquet.{ParquetFilters, SparkToParquetSchemaConverter}",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "25: import org.apache.spark.sql.connector.expressions.filter.Predicate",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "89:   override def pushAggregation(aggregation: Aggregation): Boolean = {",
          "90:     if (!sparkSession.sessionState.conf.parquetAggregatePushDown) {",
          "",
          "[Removed Lines]",
          "87:   override def pushedFilters(): Array[Filter] = pushedParquetFilters",
          "",
          "[Added Lines]",
          "88:   override def pushedFilters: Array[Predicate] = pushedParquetFilters.map(_.toV2)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "1741aabb5b27709ecd0043f4f671dadf3fa6dee5",
      "candidate_info": {
        "commit_hash": "1741aabb5b27709ecd0043f4f671dadf3fa6dee5",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/1741aabb5b27709ecd0043f4f671dadf3fa6dee5",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/unresolved.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Canonicalize.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/DynamicPruning.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/PythonUDF.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/ScalaUDF.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/interfaces.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/arithmetic.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/bitwiseExpressions.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/complexTypeExtractors.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/decimalExpressions.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/higherOrderFunctions.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/namedExpressions.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/predicates.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/subquery.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CanonicalizeSuite.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/subquery.scala"
        ],
        "message": "[SPARK-40362][SQL][3.3] Fix BinaryComparison canonicalization\n\n### What changes were proposed in this pull request?\nChange canonicalization to a one pass process and move logic from `Canonicalize.reorderCommutativeOperators` to the respective commutative operators' `canonicalize`.\n\n### Why are the changes needed?\nhttps://github.com/apache/spark/pull/34883 improved expression canonicalization performance but introduced regression when a commutative operator is under a `BinaryComparison`. This is because children reorder by their hashcode can't happen in `preCanonicalized` phase when children are not yet \"final\".\n\n### Does this PR introduce _any_ user-facing change?\nNo.\n\n### How was this patch tested?\nAdded new UT.\n\nCloses #37866 from peter-toth/SPARK-40362-fix-binarycomparison-canonicalization-3.3.\n\nLead-authored-by: Peter Toth <ptoth@cloudera.com>\nCo-authored-by: Peter Toth <peter.toth@gmail.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/unresolved.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/unresolved.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Canonicalize.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Canonicalize.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/DynamicPruning.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/DynamicPruning.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/PythonUDF.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/PythonUDF.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/ScalaUDF.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/ScalaUDF.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/interfaces.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/interfaces.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/arithmetic.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/arithmetic.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/bitwiseExpressions.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/bitwiseExpressions.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/complexTypeExtractors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/complexTypeExtractors.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/decimalExpressions.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/decimalExpressions.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/higherOrderFunctions.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/higherOrderFunctions.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/namedExpressions.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/namedExpressions.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/predicates.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/predicates.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/subquery.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/subquery.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CanonicalizeSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CanonicalizeSuite.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/subquery.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/subquery.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/unresolved.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/unresolved.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/unresolved.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/unresolved.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "642: case class TempResolvedColumn(child: Expression, nameParts: Seq[String]) extends UnaryExpression",
          "643:   with Unevaluable {",
          "645:   override def dataType: DataType = child.dataType",
          "646:   override protected def withNewChildInternal(newChild: Expression): Expression =",
          "647:     copy(child = newChild)",
          "",
          "[Removed Lines]",
          "644:   override lazy val preCanonicalized = child.preCanonicalized",
          "",
          "[Added Lines]",
          "644:   override lazy val canonicalized = child.canonicalized",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Canonicalize.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Canonicalize.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Canonicalize.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Canonicalize.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "322:   override lazy val resolved: Boolean =",
          "323:     childrenResolved && checkInputDataTypes().isSuccess && (!needsTimeZone || timeZoneId.isDefined)",
          "327:     if (timeZoneId.isDefined && !needsTimeZone) {",
          "328:       basic.withTimeZone(null)",
          "329:     } else {",
          "",
          "[Removed Lines]",
          "325:   override lazy val preCanonicalized: Expression = {",
          "326:     val basic = withNewChildren(Seq(child.preCanonicalized)).asInstanceOf[CastBase]",
          "",
          "[Added Lines]",
          "325:   override lazy val canonicalized: Expression = {",
          "326:     val basic = withNewChildren(Seq(child.canonicalized)).asInstanceOf[CastBase]",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/DynamicPruning.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/DynamicPruning.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/DynamicPruning.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/DynamicPruning.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "75:   override def toString: String = s\"dynamicpruning#${exprId.id} $conditionString\"",
          "78:     copy(",
          "80:       buildQuery = buildQuery.canonicalized,",
          "82:       exprId = ExprId(0))",
          "83:   }",
          "",
          "[Removed Lines]",
          "77:   override lazy val preCanonicalized: DynamicPruning = {",
          "79:       pruningKey = pruningKey.preCanonicalized,",
          "81:       buildKeys = buildKeys.map(_.preCanonicalized),",
          "",
          "[Added Lines]",
          "77:   override lazy val canonicalized: DynamicPruning = {",
          "79:       pruningKey = pruningKey.canonicalized,",
          "81:       buildKeys = buildKeys.map(_.canonicalized),",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "223:   def childrenResolved: Boolean = children.forall(_.resolved)",
          "",
          "[Removed Lines]",
          "248:   lazy val preCanonicalized: Expression = {",
          "249:     val canonicalizedChildren = children.map(_.preCanonicalized)",
          "250:     withNewChildren(canonicalizedChildren)",
          "251:   }",
          "261:   lazy val canonicalized: Expression = Canonicalize.reorderCommutativeOperators(preCanonicalized)",
          "",
          "[Added Lines]",
          "243:   lazy val canonicalized: Expression = {",
          "244:     val canonicalizedChildren = children.map(_.canonicalized)",
          "245:     withNewChildren(canonicalizedChildren)",
          "246:   }",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "367:   final override def eval(input: InternalRow = null): Any =",
          "368:     throw QueryExecutionErrors.cannotEvaluateExpressionError(this)",
          "",
          "[Removed Lines]",
          "365:   override lazy val preCanonicalized: Expression = replacement.preCanonicalized",
          "",
          "[Added Lines]",
          "350:   override lazy val canonicalized: Expression = replacement.canonicalized",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "1156: trait UserDefinedExpression {",
          "1157:   def name: String",
          "1158: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1145: trait CommutativeExpression extends Expression {",
          "1147:   private def gatherCommutative(",
          "1148:       e: Expression,",
          "1149:       f: PartialFunction[CommutativeExpression, Seq[Expression]]): Seq[Expression] = e match {",
          "1150:     case c: CommutativeExpression if f.isDefinedAt(c) => f(c).flatMap(gatherCommutative(_, f))",
          "1151:     case other => other.canonicalized :: Nil",
          "1152:   }",
          "1158:   protected def orderCommutative(",
          "1159:       f: PartialFunction[CommutativeExpression, Seq[Expression]]): Seq[Expression] =",
          "1160:     gatherCommutative(this, f).sortBy(_.hashCode())",
          "1161: }",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/PythonUDF.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/PythonUDF.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/PythonUDF.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/PythonUDF.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "72:   override def nullable: Boolean = true",
          "77:     this.copy(resultId = ExprId(-1)).withNewChildren(canonicalizedChildren)",
          "78:   }",
          "",
          "[Removed Lines]",
          "74:   override lazy val preCanonicalized: Expression = {",
          "75:     val canonicalizedChildren = children.map(_.preCanonicalized)",
          "",
          "[Added Lines]",
          "74:   override lazy val canonicalized: Expression = {",
          "75:     val canonicalizedChildren = children.map(_.canonicalized)",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/ScalaUDF.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/ScalaUDF.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/ScalaUDF.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/ScalaUDF.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "65:   override def name: String = udfName.getOrElse(\"UDF\")",
          "71:   }",
          "",
          "[Removed Lines]",
          "67:   override lazy val preCanonicalized: Expression = {",
          "70:     copy(children = children.map(_.preCanonicalized), inputEncoders = Nil, outputEncoder = None)",
          "",
          "[Added Lines]",
          "67:   override lazy val canonicalized: Expression = {",
          "70:     copy(children = children.map(_.canonicalized), inputEncoders = Nil, outputEncoder = None)",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/interfaces.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/interfaces.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/interfaces.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/interfaces.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "123:   def filterAttributes: AttributeSet = filter.map(_.references).getOrElse(AttributeSet.empty)",
          "127:     val normalizedAggFunc = mode match {",
          "",
          "[Removed Lines]",
          "126:   override lazy val preCanonicalized: Expression = {",
          "",
          "[Added Lines]",
          "126:   override lazy val canonicalized: Expression = {",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "134:     }",
          "136:     AggregateExpression(",
          "138:       mode,",
          "139:       isDistinct,",
          "141:       ExprId(0))",
          "142:   }",
          "",
          "[Removed Lines]",
          "137:       normalizedAggFunc.preCanonicalized.asInstanceOf[AggregateFunction],",
          "140:       filter.map(_.preCanonicalized),",
          "",
          "[Added Lines]",
          "137:       normalizedAggFunc.canonicalized.asInstanceOf[AggregateFunction],",
          "140:       filter.map(_.canonicalized),",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/arithmetic.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/arithmetic.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/arithmetic.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/arithmetic.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "313: case class Add(",
          "314:     left: Expression,",
          "315:     right: Expression,",
          "318:   def this(left: Expression, right: Expression) = this(left, right, SQLConf.get.ansiEnabled)",
          "",
          "[Removed Lines]",
          "316:     failOnError: Boolean = SQLConf.get.ansiEnabled) extends BinaryArithmetic {",
          "",
          "[Added Lines]",
          "316:     failOnError: Boolean = SQLConf.get.ansiEnabled) extends BinaryArithmetic",
          "317:   with CommutativeExpression {",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "350:   override protected def withNewChildrenInternal(newLeft: Expression, newRight: Expression): Add =",
          "351:     copy(left = newLeft, right = newRight)",
          "352: }",
          "354: @ExpressionDescription(",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "354:   override lazy val canonicalized: Expression = {",
          "356:     orderCommutative({ case Add(l, r, _) => Seq(l, r) }).reduce(Add(_, _, failOnError))",
          "357:   }",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "413: case class Multiply(",
          "414:     left: Expression,",
          "415:     right: Expression,",
          "418:   def this(left: Expression, right: Expression) = this(left, right, SQLConf.get.ansiEnabled)",
          "",
          "[Removed Lines]",
          "416:     failOnError: Boolean = SQLConf.get.ansiEnabled) extends BinaryArithmetic {",
          "",
          "[Added Lines]",
          "422:     failOnError: Boolean = SQLConf.get.ansiEnabled) extends BinaryArithmetic",
          "423:   with CommutativeExpression {",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "437:   override protected def withNewChildrenInternal(",
          "438:     newLeft: Expression, newRight: Expression): Multiply = copy(left = newLeft, right = newRight)",
          "439: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "447:   override lazy val canonicalized: Expression = {",
          "449:     orderCommutative({ case Multiply(l, r, _) => Seq(l, r) }).reduce(Multiply(_, _, failOnError))",
          "450:   }",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "905:   \"\"\",",
          "906:   since = \"1.5.0\",",
          "907:   group = \"math_funcs\")",
          "910:   override def nullable: Boolean = children.forall(_.nullable)",
          "911:   override def foldable: Boolean = children.forall(_.foldable)",
          "",
          "[Removed Lines]",
          "908: case class Least(children: Seq[Expression]) extends ComplexTypeMergingExpression {",
          "",
          "[Added Lines]",
          "920: case class Least(children: Seq[Expression]) extends ComplexTypeMergingExpression",
          "921:   with CommutativeExpression {",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "969:   override protected def withNewChildrenInternal(newChildren: IndexedSeq[Expression]): Least =",
          "970:     copy(children = newChildren)",
          "971: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "985:   override lazy val canonicalized: Expression = {",
          "986:     Least(orderCommutative({ case Least(children) => children }))",
          "987:   }",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "983:   \"\"\",",
          "984:   since = \"1.5.0\",",
          "985:   group = \"math_funcs\")",
          "988:   override def nullable: Boolean = children.forall(_.nullable)",
          "989:   override def foldable: Boolean = children.forall(_.foldable)",
          "",
          "[Removed Lines]",
          "986: case class Greatest(children: Seq[Expression]) extends ComplexTypeMergingExpression {",
          "",
          "[Added Lines]",
          "1003: case class Greatest(children: Seq[Expression]) extends ComplexTypeMergingExpression",
          "1004:   with CommutativeExpression {",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "1047:   override protected def withNewChildrenInternal(newChildren: IndexedSeq[Expression]): Greatest =",
          "1048:     copy(children = newChildren)",
          "1049: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1068:   override lazy val canonicalized: Expression = {",
          "1069:     Greatest(orderCommutative({ case Greatest(children) => children }))",
          "1070:   }",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/bitwiseExpressions.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/bitwiseExpressions.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/bitwiseExpressions.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/bitwiseExpressions.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "36:   \"\"\",",
          "37:   since = \"1.4.0\",",
          "38:   group = \"bitwise_funcs\")",
          "41:   protected override val failOnError: Boolean = false",
          "",
          "[Removed Lines]",
          "39: case class BitwiseAnd(left: Expression, right: Expression) extends BinaryArithmetic {",
          "",
          "[Added Lines]",
          "39: case class BitwiseAnd(left: Expression, right: Expression) extends BinaryArithmetic",
          "40:   with CommutativeExpression {",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "60:   override protected def withNewChildrenInternal(",
          "61:     newLeft: Expression, newRight: Expression): BitwiseAnd = copy(left = newLeft, right = newRight)",
          "62: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "64:   override lazy val canonicalized: Expression = {",
          "65:     orderCommutative({ case BitwiseAnd(l, r) => Seq(l, r) }).reduce(BitwiseAnd)",
          "66:   }",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "75:   \"\"\",",
          "76:   since = \"1.4.0\",",
          "77:   group = \"bitwise_funcs\")",
          "80:   protected override val failOnError: Boolean = false",
          "",
          "[Removed Lines]",
          "78: case class BitwiseOr(left: Expression, right: Expression) extends BinaryArithmetic {",
          "",
          "[Added Lines]",
          "83: case class BitwiseOr(left: Expression, right: Expression) extends BinaryArithmetic",
          "84:   with CommutativeExpression {",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "99:   override protected def withNewChildrenInternal(",
          "100:     newLeft: Expression, newRight: Expression): BitwiseOr = copy(left = newLeft, right = newRight)",
          "101: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "108:   override lazy val canonicalized: Expression = {",
          "109:     orderCommutative({ case BitwiseOr(l, r) => Seq(l, r) }).reduce(BitwiseOr)",
          "110:   }",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "114:   \"\"\",",
          "115:   since = \"1.4.0\",",
          "116:   group = \"bitwise_funcs\")",
          "119:   protected override val failOnError: Boolean = false",
          "",
          "[Removed Lines]",
          "117: case class BitwiseXor(left: Expression, right: Expression) extends BinaryArithmetic {",
          "",
          "[Added Lines]",
          "127: case class BitwiseXor(left: Expression, right: Expression) extends BinaryArithmetic",
          "128:   with CommutativeExpression {",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "138:   override protected def withNewChildrenInternal(",
          "139:     newLeft: Expression, newRight: Expression): BitwiseXor = copy(left = newLeft, right = newRight)",
          "140: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "152:   override lazy val canonicalized: Expression = {",
          "153:     orderCommutative({ case BitwiseXor(l, r) => Seq(l, r) }).reduce(BitwiseXor)",
          "154:   }",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/complexTypeExtractors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/complexTypeExtractors.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/complexTypeExtractors.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/complexTypeExtractors.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "107:   lazy val childSchema = child.dataType.asInstanceOf[StructType]",
          "111:   }",
          "113:   override def dataType: DataType = childSchema(ordinal).dataType",
          "",
          "[Removed Lines]",
          "109:   override lazy val preCanonicalized: Expression = {",
          "110:     copy(child = child.preCanonicalized, name = None)",
          "",
          "[Added Lines]",
          "109:   override lazy val canonicalized: Expression = {",
          "110:     copy(child = child.canonicalized, name = None)",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/decimalExpressions.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/decimalExpressions.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/decimalExpressions.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/decimalExpressions.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "114:     child.genCode(ctx)",
          "115:   override def prettyName: String = \"promote_precision\"",
          "116:   override def sql: String = child.sql",
          "119:   override protected def withNewChildInternal(newChild: Expression): Expression =",
          "120:     copy(child = newChild)",
          "",
          "[Removed Lines]",
          "117:   override lazy val preCanonicalized: Expression = child.preCanonicalized",
          "",
          "[Added Lines]",
          "117:   override lazy val canonicalized: Expression = child.canonicalized",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/higherOrderFunctions.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/higherOrderFunctions.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/higherOrderFunctions.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/higherOrderFunctions.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "208:       }",
          "209:   }",
          "212:     var currExprId = -1",
          "213:     val argumentMap = functions.flatMap(_.collect {",
          "214:       case l: NamedLambdaVariable =>",
          "",
          "[Removed Lines]",
          "211:   override lazy val preCanonicalized: Expression = {",
          "",
          "[Added Lines]",
          "211:   override lazy val canonicalized: Expression = {",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "221:         val newExprId = argumentMap(l.exprId)",
          "222:         NamedLambdaVariable(\"none\", l.dataType, l.nullable, exprId = ExprId(newExprId), null)",
          "223:     }",
          "225:     withNewChildren(canonicalizedChildren)",
          "226:   }",
          "227: }",
          "",
          "[Removed Lines]",
          "224:     val canonicalizedChildren = cleaned.children.map(_.preCanonicalized)",
          "",
          "[Added Lines]",
          "224:     val canonicalizedChildren = cleaned.children.map(_.canonicalized)",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/namedExpressions.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/namedExpressions.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/namedExpressions.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/namedExpressions.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "296:     h",
          "297:   }",
          "300:     AttributeReference(\"none\", dataType)(exprId)",
          "301:   }",
          "",
          "[Removed Lines]",
          "299:   override lazy val preCanonicalized: Expression = {",
          "",
          "[Added Lines]",
          "299:   override lazy val canonicalized: Expression = {",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/predicates.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/predicates.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/predicates.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/predicates.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "329:   final override val nodePatterns: Seq[TreePattern] = Seq(NOT)",
          "333:       case Not(GreaterThan(l, r)) => LessThanOrEqual(l, r)",
          "334:       case Not(LessThan(l, r)) => GreaterThanOrEqual(l, r)",
          "335:       case Not(GreaterThanOrEqual(l, r)) => LessThan(l, r)",
          "",
          "[Removed Lines]",
          "331:   override lazy val preCanonicalized: Expression = {",
          "332:     withNewChildren(Seq(child.preCanonicalized)) match {",
          "",
          "[Added Lines]",
          "331:   override lazy val canonicalized: Expression = {",
          "332:     withNewChildren(Seq(child.canonicalized)) match {",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "467:   final override val nodePatterns: Seq[TreePattern] = Seq(IN)",
          "471:     if (list.size > 1) {",
          "472:       basic.copy(list = basic.list.sortBy(_.hashCode()))",
          "473:     } else {",
          "",
          "[Removed Lines]",
          "469:   override lazy val preCanonicalized: Expression = {",
          "470:     val basic = withNewChildren(children.map(_.preCanonicalized)).asInstanceOf[In]",
          "",
          "[Added Lines]",
          "469:   override lazy val canonicalized: Expression = {",
          "470:     val basic = withNewChildren(children.map(_.canonicalized)).asInstanceOf[In]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "736:   \"\"\",",
          "737:   since = \"1.0.0\",",
          "738:   group = \"predicate_funcs\")",
          "741:   override def inputType: AbstractDataType = BooleanType",
          "",
          "[Removed Lines]",
          "739: case class And(left: Expression, right: Expression) extends BinaryOperator with Predicate {",
          "",
          "[Added Lines]",
          "739: case class And(left: Expression, right: Expression) extends BinaryOperator with Predicate",
          "740:   with CommutativeExpression {",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "808:   override protected def withNewChildrenInternal(newLeft: Expression, newRight: Expression): And =",
          "809:     copy(left = newLeft, right = newRight)",
          "810: }",
          "812: @ExpressionDescription(",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "812:   override lazy val canonicalized: Expression = {",
          "813:     orderCommutative({ case And(l, r) => Seq(l, r) }).reduce(And)",
          "814:   }",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "824:   \"\"\",",
          "825:   since = \"1.0.0\",",
          "826:   group = \"predicate_funcs\")",
          "829:   override def inputType: AbstractDataType = BooleanType",
          "",
          "[Removed Lines]",
          "827: case class Or(left: Expression, right: Expression) extends BinaryOperator with Predicate {",
          "",
          "[Added Lines]",
          "832: case class Or(left: Expression, right: Expression) extends BinaryOperator with Predicate",
          "833:   with CommutativeExpression {",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "897:   override protected def withNewChildrenInternal(newLeft: Expression, newRight: Expression): Or =",
          "898:     copy(left = newLeft, right = newRight)",
          "899: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "906:   override lazy val canonicalized: Expression = {",
          "907:     orderCommutative({ case Or(l, r) => Seq(l, r) }).reduce(Or)",
          "908:   }",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "908:   final override val nodePatterns: Seq[TreePattern] = Seq(BINARY_COMPARISON)",
          "912:       case EqualTo(l, r) if l.hashCode() > r.hashCode() => EqualTo(r, l)",
          "913:       case EqualNullSafe(l, r) if l.hashCode() > r.hashCode() => EqualNullSafe(r, l)",
          "",
          "[Removed Lines]",
          "910:   override lazy val preCanonicalized: Expression = {",
          "911:     withNewChildren(children.map(_.preCanonicalized)) match {",
          "",
          "[Added Lines]",
          "920:   override lazy val canonicalized: Expression = {",
          "921:     withNewChildren(children.map(_.canonicalized)) match {",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/subquery.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/subquery.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/subquery.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/subquery.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "263:   override def nullable: Boolean = true",
          "264:   override def withNewPlan(plan: LogicalPlan): ScalarSubquery = copy(plan = plan)",
          "265:   override def toString: String = s\"scalar-subquery#${exprId.id} $conditionString\"",
          "267:     ScalarSubquery(",
          "268:       plan.canonicalized,",
          "270:       ExprId(0),",
          "272:   }",
          "274:   override protected def withNewChildrenInternal(",
          "",
          "[Removed Lines]",
          "266:   override lazy val preCanonicalized: Expression = {",
          "269:       outerAttrs.map(_.preCanonicalized),",
          "271:       joinCond.map(_.preCanonicalized))",
          "",
          "[Added Lines]",
          "266:   override lazy val canonicalized: Expression = {",
          "269:       outerAttrs.map(_.canonicalized),",
          "271:       joinCond.map(_.canonicalized))",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "305:   override def nullable: Boolean = true",
          "306:   override def withNewPlan(plan: LogicalPlan): LateralSubquery = copy(plan = plan)",
          "307:   override def toString: String = s\"lateral-subquery#${exprId.id} $conditionString\"",
          "309:     LateralSubquery(",
          "310:       plan.canonicalized,",
          "312:       ExprId(0),",
          "314:   }",
          "316:   override protected def withNewChildrenInternal(",
          "",
          "[Removed Lines]",
          "308:   override lazy val preCanonicalized: Expression = {",
          "311:       outerAttrs.map(_.preCanonicalized),",
          "313:       joinCond.map(_.preCanonicalized))",
          "",
          "[Added Lines]",
          "308:   override lazy val canonicalized: Expression = {",
          "311:       outerAttrs.map(_.canonicalized),",
          "313:       joinCond.map(_.canonicalized))",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "350:   override def nullable: Boolean = false",
          "351:   override def withNewPlan(plan: LogicalPlan): ListQuery = copy(plan = plan)",
          "352:   override def toString: String = s\"list#${exprId.id} $conditionString\"",
          "354:     ListQuery(",
          "355:       plan.canonicalized,",
          "357:       ExprId(0),",
          "360:   }",
          "362:   override protected def withNewChildrenInternal(newChildren: IndexedSeq[Expression]): ListQuery =",
          "",
          "[Removed Lines]",
          "353:   override lazy val preCanonicalized: Expression = {",
          "356:       outerAttrs.map(_.preCanonicalized),",
          "358:       childOutputs.map(_.preCanonicalized.asInstanceOf[Attribute]),",
          "359:       joinCond.map(_.preCanonicalized))",
          "",
          "[Added Lines]",
          "353:   override lazy val canonicalized: Expression = {",
          "356:       outerAttrs.map(_.canonicalized),",
          "358:       childOutputs.map(_.canonicalized.asInstanceOf[Attribute]),",
          "359:       joinCond.map(_.canonicalized))",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "402:   override def nullable: Boolean = false",
          "403:   override def withNewPlan(plan: LogicalPlan): Exists = copy(plan = plan)",
          "404:   override def toString: String = s\"exists#${exprId.id} $conditionString\"",
          "406:     Exists(",
          "407:       plan.canonicalized,",
          "409:       ExprId(0),",
          "411:   }",
          "413:   override protected def withNewChildrenInternal(newChildren: IndexedSeq[Expression]): Exists =",
          "",
          "[Removed Lines]",
          "405:   override lazy val preCanonicalized: Expression = {",
          "408:       outerAttrs.map(_.preCanonicalized),",
          "410:       joinCond.map(_.preCanonicalized))",
          "",
          "[Added Lines]",
          "405:   override lazy val canonicalized: Expression = {",
          "408:       outerAttrs.map(_.canonicalized),",
          "410:       joinCond.map(_.canonicalized))",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CanonicalizeSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CanonicalizeSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CanonicalizeSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CanonicalizeSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "191:     assert(cast.canonicalized.resolved)",
          "192:   }",
          "193: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "194:   test(\"SPARK-40362: Commutative operator under BinaryComparison\") {",
          "195:     Seq(EqualTo, EqualNullSafe, GreaterThan, LessThan, GreaterThanOrEqual, LessThanOrEqual)",
          "196:       .foreach { bc =>",
          "197:         assert(bc(Add($\"a\", $\"b\"), Literal(10)).semanticEquals(bc(Add($\"b\", $\"a\"), Literal(10))))",
          "198:       }",
          "199:   }",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/execution/subquery.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/subquery.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/subquery.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/subquery.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "68:   override def toString: String = plan.simpleString(SQLConf.get.maxToStringFields)",
          "69:   override def withNewPlan(query: BaseSubqueryExec): ScalarSubquery = copy(plan = query)",
          "72:     ScalarSubquery(plan.canonicalized.asInstanceOf[BaseSubqueryExec], ExprId(0))",
          "73:   }",
          "",
          "[Removed Lines]",
          "71:   override lazy val preCanonicalized: Expression = {",
          "",
          "[Added Lines]",
          "71:   override lazy val canonicalized: Expression = {",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "158:     inSet.doGenCode(ctx, ev)",
          "159:   }",
          "162:     copy(",
          "164:       plan = plan.canonicalized.asInstanceOf[BaseSubqueryExec],",
          "165:       exprId = ExprId(0),",
          "166:       resultBroadcast = null,",
          "",
          "[Removed Lines]",
          "161:   override lazy val preCanonicalized: InSubqueryExec = {",
          "163:       child = child.preCanonicalized,",
          "",
          "[Added Lines]",
          "161:   override lazy val canonicalized: InSubqueryExec = {",
          "163:       child = child.canonicalized,",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "21e5ec529e1df114e94f81ac8d4abfbd943f1a05",
      "candidate_info": {
        "commit_hash": "21e5ec529e1df114e94f81ac8d4abfbd943f1a05",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/21e5ec529e1df114e94f81ac8d4abfbd943f1a05",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/InjectRuntimeFilter.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/InjectRuntimeFilterSuite.scala"
        ],
        "message": "[SPARK-32268][SQL][FOLLOWUP] Add ColumnPruning in injectBloomFilter\n\n### What changes were proposed in this pull request?\nAdd `ColumnPruning` in `InjectRuntimeFilter.injectBloomFilter` to optimize the BoomFilter creation query.\n\n### Why are the changes needed?\nIt seems BloomFilter subqueries injected by `InjectRuntimeFilter` will read as many columns as filterCreationSidePlan. This does not match \"Only scan the required columns\" as the design said. We can check this by a simple case in `InjectRuntimeFilterSuite`:\n```scala\nwithSQLConf(SQLConf.RUNTIME_BLOOM_FILTER_ENABLED.key -> \"true\",\n  SQLConf.RUNTIME_BLOOM_FILTER_APPLICATION_SIDE_SCAN_SIZE_THRESHOLD.key -> \"3000\",\n  SQLConf.AUTO_BROADCASTJOIN_THRESHOLD.key -> \"2000\") {\n  val query = \"select * from bf1 join bf2 on bf1.c1 = bf2.c2 where bf2.a2 = 62\"\n  sql(query).explain()\n}\n```\nThe reason is subqueries have not been optimized by `ColumnPruning`, and this pr will fix it.\n\n### Does this PR introduce _any_ user-facing change?\nNo, not released\n\n### How was this patch tested?\nImprove the test by adding `columnPruningTakesEffect` to check the optimizedPlan of bloom filter join.\n\nCloses #36047 from Flyangz/SPARK-32268-FOllOWUP.\n\nAuthored-by: Yang Liu <yintai@xiaohongshu.com>\nSigned-off-by: Yuming Wang <yumwang@ebay.com>\n(cherry picked from commit c98725a2b9574ba3c9a10567af740db7467df59d)\nSigned-off-by: Yuming Wang <yumwang@ebay.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/InjectRuntimeFilter.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/InjectRuntimeFilter.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/InjectRuntimeFilterSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/InjectRuntimeFilterSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/InjectRuntimeFilter.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/InjectRuntimeFilter.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/InjectRuntimeFilter.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/InjectRuntimeFilter.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "85:       }",
          "86:     val aggExp = AggregateExpression(bloomFilterAgg, Complete, isDistinct = false, None)",
          "87:     val alias = Alias(aggExp, \"bloomFilter\")()",
          "89:     val bloomFilterSubquery = ScalarSubquery(aggregate, Nil)",
          "90:     val filter = BloomFilterMightContain(bloomFilterSubquery,",
          "91:       new XxHash64(Seq(filterApplicationSideExp)))",
          "",
          "[Removed Lines]",
          "88:     val aggregate = ConstantFolding(Aggregate(Nil, Seq(alias), filterCreationSidePlan))",
          "",
          "[Added Lines]",
          "88:     val aggregate =",
          "89:       ConstantFolding(ColumnPruning(Aggregate(Nil, Seq(alias), filterCreationSidePlan)))",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/InjectRuntimeFilterSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/InjectRuntimeFilterSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/InjectRuntimeFilterSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/InjectRuntimeFilterSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "255:         planEnabled = sql(query).queryExecution.optimizedPlan",
          "256:         checkAnswer(sql(query), expectedAnswer)",
          "257:         if (shouldReplace) {",
          "258:           assert(getNumBloomFilters(planEnabled) > getNumBloomFilters(planDisabled))",
          "259:         } else {",
          "260:           assert(getNumBloomFilters(planEnabled) == getNumBloomFilters(planDisabled))",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "258:           assert(!columnPruningTakesEffect(planEnabled))",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "288:     numMightContains",
          "289:   }",
          "291:   def assertRewroteSemiJoin(query: String): Unit = {",
          "292:     checkWithAndWithoutFeatureEnabled(query, testSemiJoin = true, shouldReplace = true)",
          "293:   }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "292:   def columnPruningTakesEffect(plan: LogicalPlan): Boolean = {",
          "293:     def takesEffect(plan: LogicalPlan): Boolean = {",
          "294:       val result = org.apache.spark.sql.catalyst.optimizer.ColumnPruning.apply(plan)",
          "295:       !result.fastEquals(plan)",
          "296:     }",
          "298:     plan.collectFirst {",
          "299:       case Filter(condition, _) if condition.collectFirst {",
          "300:         case subquery: org.apache.spark.sql.catalyst.expressions.ScalarSubquery",
          "301:           if takesEffect(subquery.plan) => true",
          "302:       }.nonEmpty => true",
          "303:     }.nonEmpty",
          "304:   }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "4bbaf3777e9cd90151ec526a05dd67aab22da403",
      "candidate_info": {
        "commit_hash": "4bbaf3777e9cd90151ec526a05dd67aab22da403",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/4bbaf3777e9cd90151ec526a05dd67aab22da403",
        "files": [
          "core/src/main/scala/org/apache/spark/storage/BlockInfoManager.scala",
          "core/src/test/scala/org/apache/spark/storage/BlockInfoManagerSuite.scala"
        ],
        "message": "[SPARK-38675][CORE] Fix race during unlock in BlockInfoManager\n\n### What changes were proposed in this pull request?\nThis PR fixes a race in the `BlockInfoManager` between `unlock` and `releaseAllLocksForTask`, resulting in a negative reader count for a block (which trips an assert). This happens when the following events take place:\n\n1. [THREAD 1] calls `releaseAllLocksForTask`. This starts by collecting all the blocks to be unlocked for this task.\n2. [THREAD 2] calls `unlock` for a read lock for the same task (this means the block is also in the list collected in step 1). It then proceeds to unlock the block by decrementing the reader count.\n3. [THREAD 1] now starts to release the collected locks, it does this by decrementing the readers counts for blocks by the number of acquired read locks. The problem is that step 2 made the lock counts for blocks incorrect, and we decrement by one (or a few) too many. This triggers a negative reader count assert.\n\nWe fix this by adding a check to `unlock` that makes sure we are not in the process of unlocking. We do this by checking if there is a multiset associated with the task that contains the read locks.\n\n### Why are the changes needed?\nIt is a bug. Not fixing this can cause negative reader counts for blocks, and this causes task failures.\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\nAdded a regression test in BlockInfoManager suite.\n\nCloses #35991 from hvanhovell/SPARK-38675.\n\nAuthored-by: Herman van Hovell <herman@databricks.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\n(cherry picked from commit 078b505d2f0a0a4958dec7da816a7d672820b637)\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>",
        "before_after_code_files": [
          "core/src/main/scala/org/apache/spark/storage/BlockInfoManager.scala||core/src/main/scala/org/apache/spark/storage/BlockInfoManager.scala",
          "core/src/test/scala/org/apache/spark/storage/BlockInfoManagerSuite.scala||core/src/test/scala/org/apache/spark/storage/BlockInfoManagerSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "core/src/main/scala/org/apache/spark/storage/BlockInfoManager.scala||core/src/main/scala/org/apache/spark/storage/BlockInfoManager.scala": [
          "File: core/src/main/scala/org/apache/spark/storage/BlockInfoManager.scala -> core/src/main/scala/org/apache/spark/storage/BlockInfoManager.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "360:         info.writerTask = BlockInfo.NO_WRITER",
          "361:         writeLocksByTask.get(taskAttemptId).remove(blockId)",
          "362:       } else {",
          "365:         val countsForTask = readLocksByTask.get(taskAttemptId)",
          "369:       }",
          "370:       condition.signalAll()",
          "371:     }",
          "",
          "[Removed Lines]",
          "363:         assert(info.readerCount > 0, s\"Block $blockId is not locked for reading\")",
          "364:         info.readerCount -= 1",
          "366:         val newPinCountForTask: Int = countsForTask.remove(blockId, 1) - 1",
          "367:         assert(newPinCountForTask >= 0,",
          "368:           s\"Task $taskAttemptId release lock on block $blockId more times than it acquired it\")",
          "",
          "[Added Lines]",
          "367:         if (countsForTask != null) {",
          "368:           assert(info.readerCount > 0, s\"Block $blockId is not locked for reading\")",
          "369:           info.readerCount -= 1",
          "370:           val newPinCountForTask: Int = countsForTask.remove(blockId, 1) - 1",
          "371:           assert(newPinCountForTask >= 0,",
          "372:             s\"Task $taskAttemptId release lock on block $blockId more times than it acquired it\")",
          "373:         }",
          "",
          "---------------"
        ],
        "core/src/test/scala/org/apache/spark/storage/BlockInfoManagerSuite.scala||core/src/test/scala/org/apache/spark/storage/BlockInfoManagerSuite.scala": [
          "File: core/src/test/scala/org/apache/spark/storage/BlockInfoManagerSuite.scala -> core/src/test/scala/org/apache/spark/storage/BlockInfoManagerSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "360:     blockInfoManager.releaseAllLocksForTask(0)",
          "361:     assert(blockInfoManager.getNumberOfMapEntries === initialNumMapEntries - 1)",
          "362:   }",
          "363: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "364:   test(\"SPARK-38675 - concurrent unlock and releaseAllLocksForTask calls should not fail\") {",
          "366:     val blockId = TestBlockId(\"block\")",
          "367:     assert(blockInfoManager.lockNewBlockForWriting(blockId, newBlockInfo()))",
          "368:     blockInfoManager.unlock(blockId)",
          "372:     (0 to 10).foreach { task =>",
          "373:       withTaskId(task) {",
          "374:         blockInfoManager.registerTask(task)",
          "377:         (0 to 50).foreach { _ =>",
          "378:           assert(blockInfoManager.lockForReading(blockId).isDefined)",
          "379:         }",
          "382:         val futures = (0 to 50).map { _ =>",
          "383:           Future(blockInfoManager.unlock(blockId, Option(0L)))",
          "384:         }",
          "387:         blockInfoManager.releaseAllLocksForTask(task)",
          "390:         futures.foreach(ThreadUtils.awaitReady(_, 100.millis))",
          "391:       }",
          "392:     }",
          "393:   }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "4a43b4d7ddea96873095ddedae517268cbbe1663",
      "candidate_info": {
        "commit_hash": "4a43b4d7ddea96873095ddedae517268cbbe1663",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/4a43b4d7ddea96873095ddedae517268cbbe1663",
        "files": [
          "core/src/test/scala/org/apache/spark/FileSuite.scala"
        ],
        "message": "[SPARK-36681][CORE][TESTS][FOLLOW-UP] Handle LinkageError when Snappy native library is not available in low Hadoop versions\n\n### What changes were proposed in this pull request?\n\nThis is a follow-up to https://github.com/apache/spark/pull/36136 to fix `LinkageError` handling in `FileSuite` to avoid test suite abort when Snappy native library is not available in low Hadoop versions:\n```\n23:16:22 FileSuite:\n23:16:22 org.apache.spark.FileSuite *** ABORTED ***\n23:16:22   java.lang.RuntimeException: Unable to load a Suite class that was discovered in the runpath: org.apache.spark.FileSuite\n23:16:22   at org.scalatest.tools.DiscoverySuite$.getSuiteInstance(DiscoverySuite.scala:81)\n23:16:22   at org.scalatest.tools.DiscoverySuite.$anonfun$nestedSuites$1(DiscoverySuite.scala:38)\n23:16:22   at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n23:16:22   at scala.collection.Iterator.foreach(Iterator.scala:941)\n23:16:22   at scala.collection.Iterator.foreach$(Iterator.scala:941)\n23:16:22   at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n23:16:22   at scala.collection.IterableLike.foreach(IterableLike.scala:74)\n23:16:22   at scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n23:16:22   at scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n23:16:22   at scala.collection.TraversableLike.map(TraversableLike.scala:238)\n23:16:22   ...\n23:16:22   Cause: java.lang.UnsatisfiedLinkError: org.apache.hadoop.util.NativeCodeLoader.buildSupportsSnappy()Z\n23:16:22   at org.apache.hadoop.util.NativeCodeLoader.buildSupportsSnappy(Native Method)\n23:16:22   at org.apache.hadoop.io.compress.SnappyCodec.checkNativeCodeLoaded(SnappyCodec.java:63)\n23:16:22   at org.apache.hadoop.io.compress.SnappyCodec.getCompressorType(SnappyCodec.java:136)\n23:16:22   at org.apache.spark.FileSuite.$anonfun$new$12(FileSuite.scala:145)\n23:16:22   at scala.util.Try$.apply(Try.scala:213)\n23:16:22   at org.apache.spark.FileSuite.<init>(FileSuite.scala:141)\n23:16:22   at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n23:16:22   at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n23:16:22   at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n23:16:22   at java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n```\nScala's `Try` can handle only `NonFatal` throwables.\n\n### Why are the changes needed?\nTo make the tests robust.\n\n### Does this PR introduce _any_ user-facing change?\nNope, this is test-only.\n\n### How was this patch tested?\nManual test.\n\nCloses #36687 from peter-toth/SPARK-36681-handle-linkageerror.\n\nAuthored-by: Peter Toth <ptoth@cloudera.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>\n(cherry picked from commit dbde77856d2e51ff502a7fc1dba7f10316c2211b)\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
        "before_after_code_files": [
          "core/src/test/scala/org/apache/spark/FileSuite.scala||core/src/test/scala/org/apache/spark/FileSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "core/src/test/scala/org/apache/spark/FileSuite.scala||core/src/test/scala/org/apache/spark/FileSuite.scala": [
          "File: core/src/test/scala/org/apache/spark/FileSuite.scala -> core/src/test/scala/org/apache/spark/FileSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "23: import java.util.zip.GZIPOutputStream",
          "25: import scala.io.Source",
          "27: import com.google.common.io.Files",
          "28: import org.apache.hadoop.conf.Configuration",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "26: import scala.util.control.NonFatal",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "139:   private val codecs = Seq((new DefaultCodec(), \"default\"), (new BZip2Codec(), \"bzip2\")) ++ {",
          "144:       new SnappyCodec().getCompressorType",
          "147:   } ++ {",
          "148:     if (VersionUtils.isHadoop3) Seq((new Lz4Codec(), \"lz4\")) else Seq.empty",
          "149:   }",
          "",
          "[Removed Lines]",
          "140:     scala.util.Try {",
          "145:       (new SnappyCodec(), \"snappy\")",
          "146:     }.toOption",
          "",
          "[Added Lines]",
          "141:     try {",
          "146:       Some(new SnappyCodec(), \"snappy\")",
          "147:     } catch {",
          "148:       case _: LinkageError => None",
          "149:       case NonFatal(_) => None",
          "150:     }",
          "",
          "---------------"
        ]
      }
    }
  ]
}