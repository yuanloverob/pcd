{
  "cve_id": "CVE-2022-31777",
  "cve_desc": "A stored cross-site scripting (XSS) vulnerability in Apache Spark 3.2.1 and earlier, and 3.3.0, allows remote attackers to execute arbitrary JavaScript in the web browser of a user, by including a malicious payload into the logs which would be returned in logs rendered in the UI.",
  "repo": "apache/spark",
  "patch_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
  "patch_info": {
    "commit_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
    "repo": "apache/spark",
    "commit_url": "https://github.com/apache/spark/commit/ad90195de56688ce0898691eb9d04297ab0871ad",
    "files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ],
    "message": "[SPARK-39505][UI] Escape log content rendered in UI\n\n### What changes were proposed in this pull request?\n\nEscape log content rendered to the UI.\n\n### Why are the changes needed?\n\nLog content may contain reserved characters or other code in the log and be misinterpreted in the UI as HTML.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nExisting tests\n\nCloses #36902 from srowen/LogViewEscape.\n\nAuthored-by: Sean Owen <srowen@gmail.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
    "before_after_code_files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ]
  },
  "patch_diff": {
    "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js": [
      "File: core/src/main/resources/org/apache/spark/ui/static/log-view.js -> core/src/main/resources/org/apache/spark/ui/static/log-view.js",
      "--- Hunk 1 ---",
      "[Context before]",
      "85:       if (retStartByte == 0) {",
      "86:         disableMoreButton();",
      "87:       }",
      "90:       curLogLength = curLogLength + (startByte - retStartByte);",
      "91:       startByte = retStartByte;",
      "",
      "[Removed Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(cleanData);",
      "",
      "[Added Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(document.createTextNode(cleanData));",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "115:             var retLogLength = dataInfo[2];",
      "117:             var cleanData = data.substring(newlineIndex + 1);",
      "120:             curLogLength = curLogLength + (retEndByte - retStartByte);",
      "121:             endByte = retEndByte;",
      "",
      "[Removed Lines]",
      "118:             $(\"pre\", \".log-content\").append(cleanData);",
      "",
      "[Added Lines]",
      "118:             $(\"pre\", \".log-content\").append(document.createTextNode(cleanData));",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "784cb0f68af84ac85e66b67439c8c7289f1c8b6e",
      "candidate_info": {
        "commit_hash": "784cb0f68af84ac85e66b67439c8c7289f1c8b6e",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/784cb0f68af84ac85e66b67439c8c7289f1c8b6e",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/grouping.scala",
          "sql/core/src/test/resources/sql-tests/inputs/grouping_set.sql",
          "sql/core/src/test/resources/sql-tests/results/grouping_set.sql.out"
        ],
        "message": "[SPARK-40218][SQL] GROUPING SETS should preserve the grouping columns\n\n### What changes were proposed in this pull request?\n\nThis PR fixes a bug caused by https://github.com/apache/spark/pull/32022 . Although we deprecate `GROUP BY ... GROUPING SETS ...`, it should still work if it worked before.\n\nhttps://github.com/apache/spark/pull/32022 made a mistake that it didn't preserve the order of user-specified group by columns. Usually it's not a problem, as `GROUP BY a, b` is no different from `GROUP BY b, a`. However, the `grouping_id(...)` function requires the input to be exactly the same with the group by columns. This PR fixes the problem by preserve the order of user-specified group by columns.\n\n### Why are the changes needed?\n\nbug fix\n\n### Does this PR introduce _any_ user-facing change?\n\nYes, now a query that worked before 3.2 can work again.\n\n### How was this patch tested?\n\nnew test\n\nCloses #37655 from cloud-fan/grouping.\n\nAuthored-by: Wenchen Fan <wenchen@databricks.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\n(cherry picked from commit 1ed592ef28abdb14aa1d8c8a129d6ac3084ffb0c)\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/grouping.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/grouping.scala",
          "sql/core/src/test/resources/sql-tests/inputs/grouping_set.sql||sql/core/src/test/resources/sql-tests/inputs/grouping_set.sql"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/grouping.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/grouping.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/grouping.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/grouping.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "151:   override def selectedGroupByExprs: Seq[Seq[Expression]] = groupingSets",
          "155:   override protected def withNewChildrenInternal(",
          "156:       newChildren: IndexedSeq[Expression]): GroupingSets =",
          "158: }",
          "160: object GroupingSets {",
          "",
          "[Removed Lines]",
          "154:   override def children: Seq[Expression] = flatGroupingSets ++ userGivenGroupByExprs",
          "157:     super.legacyWithNewChildren(newChildren).asInstanceOf[GroupingSets]",
          "",
          "[Added Lines]",
          "157:   override def children: Seq[Expression] = userGivenGroupByExprs ++ flatGroupingSets",
          "160:     copy(",
          "161:       userGivenGroupByExprs = newChildren.take(userGivenGroupByExprs.length),",
          "162:       flatGroupingSets = newChildren.drop(userGivenGroupByExprs.length))",
          "",
          "---------------"
        ],
        "sql/core/src/test/resources/sql-tests/inputs/grouping_set.sql||sql/core/src/test/resources/sql-tests/inputs/grouping_set.sql": [
          "File: sql/core/src/test/resources/sql-tests/inputs/grouping_set.sql -> sql/core/src/test/resources/sql-tests/inputs/grouping_set.sql",
          "--- Hunk 1 ---",
          "[Context before]",
          "57: SELECT grouping__id, k1, k2, avg(v) FROM (VALUES (1,1,1),(2,2,2)) AS t(k1,k2,v) GROUP BY GROUPING SETS ((k1),(k1,k2),(k2,k1));",
          "59: SELECT grouping(k1), k1, k2, avg(v) FROM (VALUES (1,1,1),(2,2,2)) AS t(k1,k2,v) GROUP BY GROUPING SETS ((k1),(k1,k2),(k2,k1));",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "61: -- grouping_id function",
          "62: SELECT grouping_id(k1, k2), avg(v) from (VALUES (1,1,1),(2,2,2)) AS t(k1,k2,v) GROUP BY k1, k2 GROUPING SETS ((k2, k1), k1);",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "ee8cafbd0ff36116a212ac99fdf65b24c486cae8",
      "candidate_info": {
        "commit_hash": "ee8cafbd0ff36116a212ac99fdf65b24c486cae8",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/ee8cafbd0ff36116a212ac99fdf65b24c486cae8",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/UnsafeRowUtils.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/UnsafeRowUtilsSuite.scala"
        ],
        "message": "[SPARK-39839][SQL] Handle special case of null variable-length Decimal with non-zero offsetAndSize in UnsafeRow structural integrity check\n\n### What changes were proposed in this pull request?\n\nUpdate the `UnsafeRow` structural integrity check in `UnsafeRowUtils.validateStructuralIntegrity` to handle a special case with null variable-length DecimalType value.\n\n### Why are the changes needed?\n\nThe check should follow the format that `UnsafeRowWriter` produces. In general, `UnsafeRowWriter` clears out a field with zero when the field is set to be null, c.f. `UnsafeRowWriter.setNullAt(ordinal)` and `UnsafeRow.setNullAt(ordinal)`.\n\nBut there's a special case for `DecimalType` values: this is the only type that is both:\n- can be fixed-length or variable-length, depending on the precision, and\n- is mutable in `UnsafeRow`.\n\nTo support a variable-length `DecimalType` to be mutable in `UnsafeRow`, the `UnsafeRowWriter` always leaves a 16-byte space in the variable-length section of the `UnsafeRow` (tail end of the row), regardless of whether the `Decimal` value being written is null or not. In the fixed-length part of the field, it would be an \"OffsetAndSize\", and the `offset` part always points to the start offset of the variable-length part of the field, while the `size` part will either be `0` for the null value, or `1` to at most `16` for non-null values.\nWhen `setNullAt(ordinal)` is called instead of passing a null value to `write(int, Decimal, int, int)`, however, the `offset` part gets zero'd out and this field stops being mutable. There's a comment on `UnsafeRow.setDecimal` that mentions to keep this field able to support updates, `setNullAt(ordinal)` cannot be called, but there's no code enforcement of that.\n\nSo we need to recognize that in the structural integrity check and allow variable-length `DecimalType` to have non-zero field even for null.\n\nNote that for non-null values, the existing check does conform to the format from `UnsafeRowWriter`. It's only null value of variable-length `DecimalType` that'd trigger a bug, which can affect Structured Streaming's checkpoint file read where this check is applied.\n\n### Does this PR introduce _any_ user-facing change?\n\nYes, previously the `UnsafeRow` structural integrity validation will return false positive for correct data, when there's a null value in a variable-length `DecimalType` field. The fix will no longer return false positive.\nBecause the Structured Streaming checkpoint file validation uses this check, previously a good checkpoint file may be rejected by the check, and the only workaround is to disable the check; with the fix, the correct checkpoint file will be allowed to load.\n\n### How was this patch tested?\n\nAdded new test case in `UnsafeRowUtilsSuite`\n\nCloses #37252 from rednaxelafx/fix-unsaferow-validation.\n\nAuthored-by: Kris Mok <kris.mok@databricks.com>\nSigned-off-by: Jungtaek Lim <kabhwan.opensource@gmail.com>\n(cherry picked from commit c608ae2fc6a3a50f2e67f2a3dad8d4e4be1aaf9f)\nSigned-off-by: Jungtaek Lim <kabhwan.opensource@gmail.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/UnsafeRowUtils.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/UnsafeRowUtils.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/UnsafeRowUtilsSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/UnsafeRowUtilsSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/UnsafeRowUtils.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/UnsafeRowUtils.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/UnsafeRowUtils.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/UnsafeRowUtils.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "52:     var varLenFieldsSizeInBytes = 0",
          "53:     expectedSchema.fields.zipWithIndex.foreach {",
          "54:       case (field, index) if !UnsafeRow.isFixedLength(field.dataType) && !row.isNullAt(index) =>",
          "58:         if (size < 0 ||",
          "59:             offset < bitSetWidthInBytes + 8 * row.numFields || offset + size > rowSizeInBytes) {",
          "60:           return false",
          "",
          "[Removed Lines]",
          "55:         val offsetAndSize = row.getLong(index)",
          "56:         val offset = (offsetAndSize >> 32).toInt",
          "57:         val size = offsetAndSize.toInt",
          "",
          "[Added Lines]",
          "62:         val (offset, size) = getOffsetAndSize(row, index)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "74:             if ((row.getLong(index) >> 32) != 0L) return false",
          "75:           case _ =>",
          "76:         }",
          "79:       case _ =>",
          "80:     }",
          "81:     if (bitSetWidthInBytes + 8 * row.numFields + varLenFieldsSizeInBytes > rowSizeInBytes) {",
          "",
          "[Removed Lines]",
          "77:       case (_, index) if row.isNullAt(index) =>",
          "78:         if (row.getLong(index) != 0L) return false",
          "",
          "[Added Lines]",
          "82:       case (field, index) if row.isNullAt(index) =>",
          "83:         field.dataType match {",
          "84:           case dt: DecimalType if !UnsafeRow.isFixedLength(dt) =>",
          "94:             val (offset, size) = getOffsetAndSize(row, index)",
          "95:             if (size != 0 || offset != 0 &&",
          "96:                 (offset < bitSetWidthInBytes + 8 * row.numFields || offset > rowSizeInBytes)) {",
          "97:               return false",
          "98:             }",
          "99:           case _ =>",
          "100:             if (row.getLong(index) != 0L) return false",
          "101:         }",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "83:     }",
          "84:     true",
          "85:   }",
          "86: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "110:   def getOffsetAndSize(row: UnsafeRow, index: Int): (Int, Int) = {",
          "111:     val offsetAndSize = row.getLong(index)",
          "112:     val offset = (offsetAndSize >> 32).toInt",
          "113:     val size = offsetAndSize.toInt",
          "114:     (offset, size)",
          "115:   }",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/UnsafeRowUtilsSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/UnsafeRowUtilsSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/UnsafeRowUtilsSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/UnsafeRowUtilsSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: package org.apache.spark.sql.catalyst.util",
          "20: import org.apache.spark.SparkFunSuite",
          "21: import org.apache.spark.sql.catalyst.expressions.{SpecificInternalRow, UnsafeProjection, UnsafeRow}",
          "24: class UnsafeRowUtilsSuite extends SparkFunSuite {",
          "",
          "[Removed Lines]",
          "22: import org.apache.spark.sql.types.{IntegerType, StringType, StructField, StructType}",
          "",
          "[Added Lines]",
          "20: import java.math.{BigDecimal => JavaBigDecimal}",
          "24: import org.apache.spark.sql.types.{Decimal, DecimalType, IntegerType, StringType, StructField, StructType}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "52:         StructField(\"value2\", IntegerType, false)))",
          "53:     assert(!UnsafeRowUtils.validateStructuralIntegrity(testRow, invalidSchema))",
          "54:   }",
          "55: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "58:   test(\"Handle special case for null variable-length Decimal\") {",
          "59:     val schema = StructType(StructField(\"d\", DecimalType(19, 0), nullable = true) :: Nil)",
          "60:     val unsafeRowProjection = UnsafeProjection.create(schema)",
          "61:     val row = unsafeRowProjection(new SpecificInternalRow(schema))",
          "64:     assert(row.isNullAt(0) && UnsafeRowUtils.getOffsetAndSize(row, 0) == (16, 0))",
          "65:     assert(UnsafeRowUtils.validateStructuralIntegrity(row, schema))",
          "68:     val bigDecimalVal = Decimal(new JavaBigDecimal(\"12345678901234567890\")) // precision=20, scale=0",
          "69:     row.setDecimal(0, bigDecimalVal, 19) // should overflow and become null",
          "70:     assert(row.isNullAt(0) && UnsafeRowUtils.getOffsetAndSize(row, 0) == (16, 0))",
          "71:     assert(UnsafeRowUtils.validateStructuralIntegrity(row, schema))",
          "74:     val bigDecimalVal2 = Decimal(new JavaBigDecimal(\"1234567890123456789\")) // precision=19, scale=0",
          "75:     row.setDecimal(0, bigDecimalVal2, 19) // should succeed",
          "76:     assert(!row.isNullAt(0) && UnsafeRowUtils.getOffsetAndSize(row, 0) == (16, 8))",
          "77:     assert(UnsafeRowUtils.validateStructuralIntegrity(row, schema))",
          "80:     row.setNullAt(0)",
          "81:     assert(row.isNullAt(0) && UnsafeRowUtils.getOffsetAndSize(row, 0) == (0, 0))",
          "82:     assert(UnsafeRowUtils.validateStructuralIntegrity(row, schema))",
          "83:   }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "c10160b4163be00b8009cb462b1e33704b0ff3d6",
      "candidate_info": {
        "commit_hash": "c10160b4163be00b8009cb462b1e33704b0ff3d6",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/c10160b4163be00b8009cb462b1e33704b0ff3d6",
        "files": [
          "core/src/main/scala/org/apache/spark/storage/BlockManagerMasterEndpoint.scala",
          "core/src/test/scala/org/apache/spark/ExternalShuffleServiceSuite.scala",
          "core/src/test/scala/org/apache/spark/storage/BlockManagerInfoSuite.scala"
        ],
        "message": "[SPARK-38640][CORE] Fix NPE with memory-only cache blocks and RDD fetching\n\n### What changes were proposed in this pull request?\n\nFixes a bug where if `spark.shuffle.service.fetch.rdd.enabled=true`, memory-only cached blocks will fail to unpersist.\n\n### Why are the changes needed?\n\nIn https://github.com/apache/spark/pull/33020, when all RDD blocks are removed from `externalShuffleServiceBlockStatus`, the underlying Map is nulled to reduce memory. When persisting blocks we check if it's using disk before adding it to `externalShuffleServiceBlockStatus`, but when removing them there is no check, so a memory-only cache block will keep `externalShuffleServiceBlockStatus` null, and when unpersisting it throw an NPE because it tries to remove from the null Map. This adds checks to the removal as well to only remove if the block is on disk, and therefore should have been added to `externalShuffleServiceBlockStatus` in the first place.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nNew and updated UT\n\nCloses #35959 from Kimahriman/fetch-rdd-memory-only-unpersist.\n\nAuthored-by: Adam Binford <adamq43@gmail.com>\nSigned-off-by: Sean Owen <srowen@gmail.com>\n(cherry picked from commit e0939f0f7c3d3bd4baa89e720038dbd3c7363a72)\nSigned-off-by: Sean Owen <srowen@gmail.com>",
        "before_after_code_files": [
          "core/src/main/scala/org/apache/spark/storage/BlockManagerMasterEndpoint.scala||core/src/main/scala/org/apache/spark/storage/BlockManagerMasterEndpoint.scala",
          "core/src/test/scala/org/apache/spark/ExternalShuffleServiceSuite.scala||core/src/test/scala/org/apache/spark/ExternalShuffleServiceSuite.scala",
          "core/src/test/scala/org/apache/spark/storage/BlockManagerInfoSuite.scala||core/src/test/scala/org/apache/spark/storage/BlockManagerInfoSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "core/src/main/scala/org/apache/spark/storage/BlockManagerMasterEndpoint.scala||core/src/main/scala/org/apache/spark/storage/BlockManagerMasterEndpoint.scala": [
          "File: core/src/main/scala/org/apache/spark/storage/BlockManagerMasterEndpoint.scala -> core/src/main/scala/org/apache/spark/storage/BlockManagerMasterEndpoint.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "838:   }",
          "840:   def remove(blockId: BlockId): Unit = {",
          "844:     }",
          "845:   }",
          "",
          "[Removed Lines]",
          "841:     blocks.remove(blockId)",
          "842:     if (blocks.isEmpty) {",
          "843:       blocks = null",
          "",
          "[Added Lines]",
          "841:     if (blocks != null) {",
          "842:       blocks.remove(blockId)",
          "843:       if (blocks.isEmpty) {",
          "844:         blocks = null",
          "845:       }",
          "",
          "---------------"
        ],
        "core/src/test/scala/org/apache/spark/ExternalShuffleServiceSuite.scala||core/src/test/scala/org/apache/spark/ExternalShuffleServiceSuite.scala": [
          "File: core/src/test/scala/org/apache/spark/ExternalShuffleServiceSuite.scala -> core/src/test/scala/org/apache/spark/ExternalShuffleServiceSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "255:       }",
          "256:     }",
          "257:   }",
          "258: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "259:   test(\"SPARK-38640: memory only blocks can unpersist using shuffle service cache fetching\") {",
          "260:     for (enabled <- Seq(true, false)) {",
          "261:       val confWithRddFetch =",
          "262:         conf.clone.set(config.SHUFFLE_SERVICE_FETCH_RDD_ENABLED, enabled)",
          "263:       sc = new SparkContext(\"local-cluster[1,1,1024]\", \"test\", confWithRddFetch)",
          "264:       sc.env.blockManager.externalShuffleServiceEnabled should equal(true)",
          "265:       sc.env.blockManager.blockStoreClient.getClass should equal(classOf[ExternalBlockStoreClient])",
          "266:       try {",
          "267:         val rdd = sc.parallelize(0 until 100, 2)",
          "268:           .map { i => (i, 1) }",
          "269:           .persist(StorageLevel.MEMORY_ONLY)",
          "271:         rdd.count()",
          "272:         rdd.unpersist(true)",
          "273:         assert(sc.persistentRdds.isEmpty)",
          "274:       } finally {",
          "275:         rpcHandler.applicationRemoved(sc.conf.getAppId, true)",
          "276:         sc.stop()",
          "277:       }",
          "278:     }",
          "279:   }",
          "",
          "---------------"
        ],
        "core/src/test/scala/org/apache/spark/storage/BlockManagerInfoSuite.scala||core/src/test/scala/org/apache/spark/storage/BlockManagerInfoSuite.scala": [
          "File: core/src/test/scala/org/apache/spark/storage/BlockManagerInfoSuite.scala -> core/src/test/scala/org/apache/spark/storage/BlockManagerInfoSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "63:     if (svcEnabled) {",
          "64:       assert(getEssBlockStatus(bmInfo, rddId).isEmpty)",
          "65:     }",
          "66:   }",
          "68:   testWithShuffleServiceOnOff(\"RDD block with MEMORY_AND_DISK\") { (svcEnabled, bmInfo) =>",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "66:     bmInfo.updateBlockInfo(rddId, StorageLevel.NONE, memSize = 0, diskSize = 0)",
          "67:     assert(bmInfo.remainingMem === 30000)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "db121c34dbe22e809c42ebd72618c1597932ac29",
      "candidate_info": {
        "commit_hash": "db121c34dbe22e809c42ebd72618c1597932ac29",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/db121c34dbe22e809c42ebd72618c1597932ac29",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala"
        ],
        "message": "[SPARK-39184][SQL][FOLLOWUP] Make interpreted and codegen paths for date/timestamp sequences the same\n\n### What changes were proposed in this pull request?\n\nChange how the length of the new result array is calculated in `InternalSequenceBase.eval` to match how the same is calculated in the generated code.\n\n### Why are the changes needed?\n\nThis change brings the interpreted mode code in line with the generated code. Although I am not aware of any case where the current interpreted mode code fails, the generated code is more correct (it handles the case where the result array must grow more than once, whereas the current interpreted mode code does not).\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nExisting unit tests.\n\nCloses #37542 from bersprockets/date_sequence_array_size_issue_follow_up.\n\nAuthored-by: Bruce Robbins <bersprockets@gmail.com>\nSigned-off-by: Max Gekk <max.gekk@gmail.com>\n(cherry picked from commit d718867a16754c62cb8c30a750485f4856481efc)\nSigned-off-by: Max Gekk <max.gekk@gmail.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "3072:           if (i == arr.length) {",
          "3074:           }",
          "3075:           arr(i) = fromLong(result)",
          "3076:           i += 1",
          "",
          "[Removed Lines]",
          "3073:             arr = arr.padTo(estimatedArrayLength + 1, fromLong(0L))",
          "",
          "[Added Lines]",
          "3073:             arr = arr.padTo(i + 1, fromLong(0L))",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "94d3d6b5fcea7866ba0a88f7ae5699f96857a7de",
      "candidate_info": {
        "commit_hash": "94d3d6b5fcea7866ba0a88f7ae5699f96857a7de",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/94d3d6b5fcea7866ba0a88f7ae5699f96857a7de",
        "files": [
          "sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/ParquetColumnVector.java",
          "sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedRleValuesReader.java"
        ],
        "message": "[SPARK-38891][SQL] Skipping allocating vector for repetition & definition levels when possible\n\n### What changes were proposed in this pull request?\n\nThis PR adds two optimization on the vectorized Parquet reader for complex types:\n- avoid allocating vectors for repetition and definition levels whenever can be applied\n- avoid reading definition levels whenever can be applied\n\n### Why are the changes needed?\n\nAt the moment, Spark will allocate vectors for repetition and definition levels, and also read definition levels even if it's not necessary, for instance, when reading primitive types. This may add extra memory footprint especially when reading wide tables. Therefore, we should avoid them if possible.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nExisting tests.\n\nCloses #36202 from sunchao/SPARK-38891.\n\nAuthored-by: Chao Sun <sunchao@apple.com>\nSigned-off-by: Chao Sun <sunchao@apple.com>",
        "before_after_code_files": [
          "sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/ParquetColumnVector.java||sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/ParquetColumnVector.java",
          "sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedRleValuesReader.java||sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedRleValuesReader.java"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/ParquetColumnVector.java||sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/ParquetColumnVector.java": [
          "File: sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/ParquetColumnVector.java -> sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/ParquetColumnVector.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "61:       int capacity,",
          "62:       MemoryMode memoryMode,",
          "63:       Set<ParquetColumn> missingColumns) {",
          "65:     DataType sparkType = column.sparkType();",
          "66:     if (!sparkType.sameType(vector.dataType())) {",
          "67:       throw new IllegalArgumentException(\"Spark type: \" + sparkType +",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "64:     this(column, vector, capacity, memoryMode, missingColumns, true);",
          "65:   }",
          "67:   ParquetColumnVector(",
          "68:       ParquetColumn column,",
          "69:       WritableColumnVector vector,",
          "70:       int capacity,",
          "71:       MemoryMode memoryMode,",
          "72:       Set<ParquetColumn> missingColumns,",
          "73:       boolean isTopLevel) {",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "79:     }",
          "81:     if (isPrimitive) {",
          "86:     } else {",
          "87:       Preconditions.checkArgument(column.children().size() == vector.getNumChildren());",
          "88:       for (int i = 0; i < column.children().size(); i++) {",
          "89:         ParquetColumnVector childCv = new ParquetColumnVector(column.children().apply(i),",
          "91:         children.add(childCv);",
          "95:         if (!childCv.vector.isAllNull()) {",
          "96:           this.repetitionLevels = childCv.repetitionLevels;",
          "97:           this.definitionLevels = childCv.definitionLevels;",
          "98:         }",
          "",
          "[Removed Lines]",
          "84:       repetitionLevels = allocateLevelsVector(capacity, memoryMode);",
          "85:       definitionLevels = allocateLevelsVector(capacity, memoryMode);",
          "90:           vector.getChild(i), capacity, memoryMode, missingColumns);",
          "",
          "[Added Lines]",
          "91:       if (column.repetitionLevel() > 0) {",
          "92:         repetitionLevels = allocateLevelsVector(capacity, memoryMode);",
          "93:       }",
          "95:       if (!isTopLevel) {",
          "96:         definitionLevels = allocateLevelsVector(capacity, memoryMode);",
          "97:       }",
          "100:       boolean allChildrenAreMissing = true;",
          "104:           vector.getChild(i), capacity, memoryMode, missingColumns, false);",
          "111:           allChildrenAreMissing = false;",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "104:         vector.setAllNull();",
          "105:       }",
          "106:     }",
          "",
          "[Removed Lines]",
          "103:       if (repetitionLevels == null) {",
          "",
          "[Added Lines]",
          "119:       if (allChildrenAreMissing) {",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "163:     if (vector.isAllNull()) return;",
          "165:     vector.reset();",
          "168:     for (ParquetColumnVector child : children) {",
          "169:       child.reset();",
          "170:     }",
          "",
          "[Removed Lines]",
          "166:     repetitionLevels.reset();",
          "167:     definitionLevels.reset();",
          "",
          "[Added Lines]",
          "182:     if (repetitionLevels != null) {",
          "183:       repetitionLevels.reset();",
          "184:     }",
          "185:     if (definitionLevels != null) {",
          "186:       definitionLevels.reset();",
          "187:     }",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "289:     vector.reserve(definitionLevels.getElementsAppended());",
          "291:     int rowId = 0;",
          "293:     for (int i = 0; i < definitionLevels.getElementsAppended(); i++) {",
          "",
          "[Removed Lines]",
          "292:     boolean hasRepetitionLevels = repetitionLevels.getElementsAppended() > 0;",
          "",
          "[Added Lines]",
          "312:     boolean hasRepetitionLevels =",
          "313:       repetitionLevels != null && repetitionLevels.getElementsAppended() > 0;",
          "",
          "---------------"
        ],
        "sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedRleValuesReader.java||sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedRleValuesReader.java": [
          "File: sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedRleValuesReader.java -> sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedRleValuesReader.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "172:       WritableColumnVector defLevels,",
          "173:       VectorizedValuesReader valueReader,",
          "174:       ParquetVectorUpdater updater) {",
          "176:   }",
          "",
          "[Removed Lines]",
          "175:     readBatchInternal(state, values, values, defLevels, valueReader, updater);",
          "",
          "[Added Lines]",
          "175:     if (defLevels == null) {",
          "176:       readBatchInternal(state, values, values, valueReader, updater);",
          "177:     } else {",
          "178:       readBatchInternalWithDefLevels(state, values, values, defLevels, valueReader, updater);",
          "179:     }",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "185:       WritableColumnVector nulls,",
          "186:       WritableColumnVector defLevels,",
          "187:       VectorizedValuesReader valueReader) {",
          "190:   }",
          "192:   private void readBatchInternal(",
          "193:       ParquetReadState state,",
          "194:       WritableColumnVector values,",
          "195:       WritableColumnVector nulls,",
          "",
          "[Removed Lines]",
          "188:     readBatchInternal(state, values, nulls, defLevels, valueReader,",
          "189:       new ParquetVectorUpdaterFactory.IntegerUpdater());",
          "",
          "[Added Lines]",
          "192:     if (defLevels == null) {",
          "193:       readBatchInternal(state, values, nulls, valueReader,",
          "194:         new ParquetVectorUpdaterFactory.IntegerUpdater());",
          "195:     } else {",
          "196:       readBatchInternalWithDefLevels(state, values, nulls, defLevels, valueReader,",
          "197:         new ParquetVectorUpdaterFactory.IntegerUpdater());",
          "198:     }",
          "202:       ParquetReadState state,",
          "203:       WritableColumnVector values,",
          "204:       WritableColumnVector nulls,",
          "205:       VectorizedValuesReader valueReader,",
          "206:       ParquetVectorUpdater updater) {",
          "208:     long rowId = state.rowId;",
          "209:     int leftInBatch = state.rowsToReadInBatch;",
          "210:     int leftInPage = state.valuesToReadInPage;",
          "212:     while (leftInBatch > 0 && leftInPage > 0) {",
          "213:       if (currentCount == 0 && !readNextGroup()) break;",
          "214:       int n = Math.min(leftInBatch, Math.min(leftInPage, this.currentCount));",
          "216:       long rangeStart = state.currentRangeStart();",
          "217:       long rangeEnd = state.currentRangeEnd();",
          "219:       if (rowId + n < rangeStart) {",
          "220:         skipValues(n, state, valueReader, updater);",
          "221:         rowId += n;",
          "222:         leftInPage -= n;",
          "223:       } else if (rowId > rangeEnd) {",
          "224:         state.nextRange();",
          "225:       } else {",
          "227:         long start = Math.max(rangeStart, rowId);",
          "228:         long end = Math.min(rangeEnd, rowId + n - 1);",
          "231:         int toSkip = (int) (start - rowId);",
          "232:         if (toSkip > 0) {",
          "233:           skipValues(toSkip, state, valueReader, updater);",
          "234:           rowId += toSkip;",
          "235:           leftInPage -= toSkip;",
          "236:         }",
          "239:         n = (int) (end - start + 1);",
          "241:         switch (mode) {",
          "242:           case RLE:",
          "243:             if (currentValue == state.maxDefinitionLevel) {",
          "244:               updater.readValues(n, state.valueOffset, values, valueReader);",
          "245:             } else {",
          "246:               nulls.putNulls(state.valueOffset, n);",
          "247:             }",
          "248:             state.valueOffset += n;",
          "249:             break;",
          "250:           case PACKED:",
          "251:             for (int i = 0; i < n; ++i) {",
          "252:               int currentValue = currentBuffer[currentBufferIdx++];",
          "253:               if (currentValue == state.maxDefinitionLevel) {",
          "254:                 updater.readValue(state.valueOffset++, values, valueReader);",
          "255:               } else {",
          "256:                 nulls.putNull(state.valueOffset++);",
          "257:               }",
          "258:             }",
          "259:             break;",
          "260:         }",
          "261:         state.levelOffset += n;",
          "262:         leftInBatch -= n;",
          "263:         rowId += n;",
          "264:         leftInPage -= n;",
          "265:         currentCount -= n;",
          "266:       }",
          "267:     }",
          "269:     state.rowsToReadInBatch = leftInBatch;",
          "270:     state.valuesToReadInPage = leftInPage;",
          "271:     state.rowId = rowId;",
          "272:   }",
          "274:   private void readBatchInternalWithDefLevels(",
          "",
          "---------------"
        ]
      }
    }
  ]
}