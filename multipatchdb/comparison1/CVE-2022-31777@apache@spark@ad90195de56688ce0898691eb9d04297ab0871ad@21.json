{
  "cve_id": "CVE-2022-31777",
  "cve_desc": "A stored cross-site scripting (XSS) vulnerability in Apache Spark 3.2.1 and earlier, and 3.3.0, allows remote attackers to execute arbitrary JavaScript in the web browser of a user, by including a malicious payload into the logs which would be returned in logs rendered in the UI.",
  "repo": "apache/spark",
  "patch_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
  "patch_info": {
    "commit_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
    "repo": "apache/spark",
    "commit_url": "https://github.com/apache/spark/commit/ad90195de56688ce0898691eb9d04297ab0871ad",
    "files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ],
    "message": "[SPARK-39505][UI] Escape log content rendered in UI\n\n### What changes were proposed in this pull request?\n\nEscape log content rendered to the UI.\n\n### Why are the changes needed?\n\nLog content may contain reserved characters or other code in the log and be misinterpreted in the UI as HTML.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nExisting tests\n\nCloses #36902 from srowen/LogViewEscape.\n\nAuthored-by: Sean Owen <srowen@gmail.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
    "before_after_code_files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ]
  },
  "patch_diff": {
    "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js": [
      "File: core/src/main/resources/org/apache/spark/ui/static/log-view.js -> core/src/main/resources/org/apache/spark/ui/static/log-view.js",
      "--- Hunk 1 ---",
      "[Context before]",
      "85:       if (retStartByte == 0) {",
      "86:         disableMoreButton();",
      "87:       }",
      "90:       curLogLength = curLogLength + (startByte - retStartByte);",
      "91:       startByte = retStartByte;",
      "",
      "[Removed Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(cleanData);",
      "",
      "[Added Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(document.createTextNode(cleanData));",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "115:             var retLogLength = dataInfo[2];",
      "117:             var cleanData = data.substring(newlineIndex + 1);",
      "120:             curLogLength = curLogLength + (retEndByte - retStartByte);",
      "121:             endByte = retEndByte;",
      "",
      "[Removed Lines]",
      "118:             $(\"pre\", \".log-content\").append(cleanData);",
      "",
      "[Added Lines]",
      "118:             $(\"pre\", \".log-content\").append(document.createTextNode(cleanData));",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "b852645f69b3b7a0a2140a732c4c03b302f8795a",
      "candidate_info": {
        "commit_hash": "b852645f69b3b7a0a2140a732c4c03b302f8795a",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/b852645f69b3b7a0a2140a732c4c03b302f8795a",
        "files": [
          "python/pyspark/mllib/fpm.py",
          "python/pyspark/mllib/fpm.pyi"
        ],
        "message": "[SPARK-37423][PYTHON] Inline type hints for fpm.py in python/pyspark/mllib\n\n### What changes were proposed in this pull request?\nInline type hints for fpm.py, test.py in python/pyspark/mllib/\n\n### Why are the changes needed?\nWe can take advantage of static type checking within the functions by inlining the type hints.\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\nExisting tests\n\nCloses #35067 from dchvn/fpm_2.\n\nAuthored-by: dch nguyen <dchvn.dgd@gmail.com>\nSigned-off-by: zero323 <mszymkiewicz@gmail.com>\n(cherry picked from commit 93f646dd00ba8b3370bb904ba91862c407c62cc2)\nSigned-off-by: zero323 <mszymkiewicz@gmail.com>",
        "before_after_code_files": [
          "python/pyspark/mllib/fpm.py||python/pyspark/mllib/fpm.py",
          "python/pyspark/mllib/fpm.pyi||python/pyspark/mllib/fpm.pyi"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "python/pyspark/mllib/fpm.py||python/pyspark/mllib/fpm.py": [
          "File: python/pyspark/mllib/fpm.py -> python/pyspark/mllib/fpm.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: import sys",
          "23: from pyspark.mllib.common import JavaModelWrapper, callMLlibFunc",
          "24: from pyspark.mllib.util import JavaSaveable, JavaLoader, inherit_doc",
          "26: __all__ = [\"FPGrowth\", \"FPGrowthModel\", \"PrefixSpan\", \"PrefixSpanModel\"]",
          "29: @inherit_doc",
          "31:     \"\"\"",
          "32:     A FP-Growth model for mining frequent itemsets",
          "33:     using the Parallel FP-Growth algorithm.",
          "",
          "[Removed Lines]",
          "20: from collections import namedtuple",
          "22: from pyspark import since",
          "30: class FPGrowthModel(JavaModelWrapper, JavaSaveable, JavaLoader):",
          "",
          "[Added Lines]",
          "20: from typing import Any, Generic, List, NamedTuple, TypeVar",
          "22: from pyspark import since, SparkContext",
          "25: from pyspark.rdd import RDD",
          "29: T = TypeVar(\"T\")",
          "33: class FPGrowthModel(JavaModelWrapper, JavaSaveable, JavaLoader[\"FPGrowthModel\"]):",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "49:     \"\"\"",
          "51:     @since(\"1.4.0\")",
          "53:         \"\"\"",
          "54:         Returns the frequent itemsets of this model.",
          "55:         \"\"\"",
          "",
          "[Removed Lines]",
          "52:     def freqItemsets(self):",
          "",
          "[Added Lines]",
          "55:     def freqItemsets(self) -> RDD[\"FPGrowth.FreqItemset\"]:",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "58:     @classmethod",
          "59:     @since(\"2.0.0\")",
          "61:         \"\"\"",
          "62:         Load a model from the given path.",
          "63:         \"\"\"",
          "64:         model = cls._load_java(sc, path)",
          "65:         wrapper = sc._jvm.org.apache.spark.mllib.api.python.FPGrowthModelWrapper(model)",
          "66:         return FPGrowthModel(wrapper)",
          "",
          "[Removed Lines]",
          "60:     def load(cls, sc, path):",
          "",
          "[Added Lines]",
          "63:     def load(cls, sc: SparkContext, path: str) -> \"FPGrowthModel\":",
          "68:         assert sc._jvm is not None",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "74:     \"\"\"",
          "76:     @classmethod",
          "78:         \"\"\"",
          "79:         Computes an FP-Growth model that contains frequent itemsets.",
          "",
          "[Removed Lines]",
          "77:     def train(cls, data, minSupport=0.3, numPartitions=-1):",
          "",
          "[Added Lines]",
          "81:     def train(",
          "82:         cls, data: RDD[List[T]], minSupport: float = 0.3, numPartitions: int = -1",
          "83:     ) -> \"FPGrowthModel\":",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "95:         model = callMLlibFunc(\"trainFPGrowthModel\", data, float(minSupport), int(numPartitions))",
          "96:         return FPGrowthModel(model)",
          "99:         \"\"\"",
          "100:         Represents an (items, freq) tuple.",
          "102:         .. versionadded:: 1.4.0",
          "103:         \"\"\"",
          "106: @inherit_doc",
          "108:     \"\"\"",
          "109:     Model fitted by PrefixSpan",
          "",
          "[Removed Lines]",
          "98:     class FreqItemset(namedtuple(\"FreqItemset\", [\"items\", \"freq\"])):",
          "107: class PrefixSpanModel(JavaModelWrapper):",
          "",
          "[Added Lines]",
          "104:     class FreqItemset(NamedTuple):",
          "111:         items: List[Any]",
          "112:         freq: int",
          "116: class PrefixSpanModel(JavaModelWrapper, Generic[T]):",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "124:     \"\"\"",
          "126:     @since(\"1.6.0\")",
          "128:         \"\"\"Gets frequent sequences\"\"\"",
          "129:         return self.call(\"getFreqSequences\").map(lambda x: PrefixSpan.FreqSequence(x[0], x[1]))",
          "",
          "[Removed Lines]",
          "127:     def freqSequences(self):",
          "",
          "[Added Lines]",
          "136:     def freqSequences(self) -> RDD[\"PrefixSpan.FreqSequence\"]:",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "144:     \"\"\"",
          "146:     @classmethod",
          "148:         \"\"\"",
          "149:         Finds the complete set of frequent sequential patterns in the",
          "150:         input sequences of itemsets.",
          "",
          "[Removed Lines]",
          "147:     def train(cls, data, minSupport=0.1, maxPatternLength=10, maxLocalProjDBSize=32000000):",
          "",
          "[Added Lines]",
          "156:     def train(",
          "157:         cls,",
          "158:         data: RDD[List[List[T]]],",
          "159:         minSupport: float = 0.1,",
          "160:         maxPatternLength: int = 10,",
          "161:         maxLocalProjDBSize: int = 32000000,",
          "162:     ) -> PrefixSpanModel[T]:",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "177:         )",
          "178:         return PrefixSpanModel(model)",
          "181:         \"\"\"",
          "182:         Represents a (sequence, freq) tuple.",
          "184:         .. versionadded:: 1.6.0",
          "185:         \"\"\"",
          "189:     import doctest",
          "190:     from pyspark.sql import SparkSession",
          "191:     import pyspark.mllib.fpm",
          "",
          "[Removed Lines]",
          "180:     class FreqSequence(namedtuple(\"FreqSequence\", [\"sequence\", \"freq\"])):",
          "188: def _test():",
          "",
          "[Added Lines]",
          "195:     class FreqSequence(NamedTuple):",
          "202:         sequence: List[List[Any]]",
          "203:         freq: int",
          "206: def _test() -> None:",
          "",
          "---------------"
        ],
        "python/pyspark/mllib/fpm.pyi||python/pyspark/mllib/fpm.pyi": [
          "File: python/pyspark/mllib/fpm.pyi -> python/pyspark/mllib/fpm.pyi",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "b2960b25b7f506c5543e7137a46d409cba44ff4c",
      "candidate_info": {
        "commit_hash": "b2960b25b7f506c5543e7137a46d409cba44ff4c",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/b2960b25b7f506c5543e7137a46d409cba44ff4c",
        "files": [
          "docs/sql-ref-ansi-compliance.md",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/FunctionRegistry.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/Average.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/Sum.scala",
          "sql/core/src/test/resources/sql-functions/sql-expression-schema.md",
          "sql/core/src/test/resources/sql-tests/inputs/try_aggregates.sql",
          "sql/core/src/test/resources/sql-tests/results/ansi/try_aggregates.sql.out",
          "sql/core/src/test/resources/sql-tests/results/try_aggregates.sql.out",
          "sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala"
        ],
        "message": "[SPARK-38589][SQL] New SQL function: try_avg\n\n### What changes were proposed in this pull request?\n\nAdd a new SQL function: try_avg. It is identical to the function `avg`, except that it returns NULL result instead of throwing an exception on decimal/interval value overflow.\nNote it is also different from `avg` when ANSI mode is off on interval overflows\n| Function         | avg                                | try_avg      |\n|------------------|------------------------------------|-------------|\n| year-month interval overflow | Error                       | Return NULL |\n| day-time interval overflow | Error | Return NULL |\n\n### Why are the changes needed?\n\n* Users can manage to finish queries without interruptions in ANSI mode.\n* Users can get NULLs instead of runtime errors if interval overflow occurs when ANSI mode is off. For example\n```\n> SELECT avg(col) FROM VALUES (interval '2147483647 months'),(interval '1 months') AS tab(col)\njava.lang.ArithmeticException: integer overflow.\n\n> SELECT try_avg(col) FROM VALUES (interval '2147483647 months'),(interval '1 months') AS tab(col)\nNULL\n```\n\n### Does this PR introduce _any_ user-facing change?\n\nYes, adding a new SQL function: try_avg. It is identical to the function `avg`, except that it returns NULL result instead of throwing an exception on decimal/interval value overflow.\n\n### How was this patch tested?\n\nUT\n\nCloses #35896 from gengliangwang/tryAvg.\n\nLead-authored-by: Gengliang Wang <gengliang@apache.org>\nCo-authored-by: Gengliang Wang <ltnwgl@gmail.com>\nSigned-off-by: Gengliang Wang <gengliang@apache.org>\n(cherry picked from commit a7f0adb2dd8449af6f9e9b5a25f11b5dcf5868f1)\nSigned-off-by: Gengliang Wang <gengliang@apache.org>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/FunctionRegistry.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/FunctionRegistry.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/Average.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/Average.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/Sum.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/Sum.scala",
          "sql/core/src/test/resources/sql-tests/inputs/try_aggregates.sql||sql/core/src/test/resources/sql-tests/inputs/try_aggregates.sql",
          "sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala||sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/FunctionRegistry.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/FunctionRegistry.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/FunctionRegistry.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/FunctionRegistry.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "474:     expression[TrySubtract](\"try_subtract\"),",
          "475:     expression[TryMultiply](\"try_multiply\"),",
          "476:     expression[TryElementAt](\"try_element_at\"),",
          "477:     expression[TrySum](\"try_sum\"),",
          "478:     expression[TryToBinary](\"try_to_binary\"),",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "477:     expression[TryAverage](\"try_avg\"),",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/Average.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/Average.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/Average.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/Average.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "26: import org.apache.spark.sql.internal.SQLConf",
          "27: import org.apache.spark.sql.types._",
          "43:   extends DeclarativeAggregate",
          "44:   with ImplicitCastInputTypes",
          "45:   with UnaryLike[Expression] {",
          "49:   override def prettyName: String = getTagValue(FunctionRegistry.FUNC_ALIAS).getOrElse(\"avg\")",
          "",
          "[Removed Lines]",
          "29: @ExpressionDescription(",
          "30:   usage = \"_FUNC_(expr) - Returns the mean calculated from values of a group.\",",
          "31:   examples = \"\"\"",
          "32:     Examples:",
          "33:       > SELECT _FUNC_(col) FROM VALUES (1), (2), (3) AS tab(col);",
          "34:        2.0",
          "35:       > SELECT _FUNC_(col) FROM VALUES (1), (2), (NULL) AS tab(col);",
          "36:        1.5",
          "37:   \"\"\",",
          "38:   group = \"agg_funcs\",",
          "39:   since = \"1.0.0\")",
          "40: case class Average(",
          "41:     child: Expression,",
          "42:     failOnError: Boolean = SQLConf.get.ansiEnabled)",
          "47:   def this(child: Expression) = this(child, failOnError = SQLConf.get.ansiEnabled)",
          "",
          "[Added Lines]",
          "29: abstract class AverageBase",
          "35:   def useAnsiAdd: Boolean",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "62:   final override val nodePatterns: Seq[TreePattern] = Seq(AVERAGE)",
          "65:     case DecimalType.Fixed(p, s) =>",
          "66:       DecimalType.bounded(p + 4, s + 4)",
          "67:     case _: YearMonthIntervalType => YearMonthIntervalType()",
          "",
          "[Removed Lines]",
          "64:   private lazy val resultType = child.dataType match {",
          "",
          "[Added Lines]",
          "52:   protected lazy val resultType = child.dataType match {",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "87:   )",
          "92:   )",
          "97:     case _: DecimalType =>",
          "98:       DecimalPrecision.decimalAndDecimal()(",
          "99:         Divide(",
          "101:           count.cast(DecimalType.LongDecimal), failOnError = false)).cast(resultType)",
          "102:     case _: YearMonthIntervalType =>",
          "103:       If(EqualTo(count, Literal(0L)),",
          "",
          "[Removed Lines]",
          "89:   override lazy val mergeExpressions = Seq(",
          "96:   override lazy val evaluateExpression = child.dataType match {",
          "100:           CheckOverflowInSum(sum, sumDataType.asInstanceOf[DecimalType], !failOnError),",
          "",
          "[Added Lines]",
          "77:   protected def getMergeExpressions = Seq(",
          "84:   protected def getEvaluateExpression = child.dataType match {",
          "88:           CheckOverflowInSum(sum, sumDataType.asInstanceOf[DecimalType], !useAnsiAdd),",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "109:       Divide(sum.cast(resultType), count.cast(resultType), failOnError = false)",
          "110:   }",
          "114:     Add(",
          "115:       sum,",
          "118:   )",
          "120:   override protected def withNewChildInternal(newChild: Expression): Average =",
          "121:     copy(child = newChild)",
          "125: }",
          "",
          "[Removed Lines]",
          "112:   override lazy val updateExpressions: Seq[Expression] = Seq(",
          "116:       coalesce(child.cast(sumDataType), Literal.default(sumDataType))),",
          "124:   override def flatArguments: Iterator[Any] = Iterator(child)",
          "",
          "[Added Lines]",
          "100:   protected def getUpdateExpressions: Seq[Expression] = Seq(",
          "104:       coalesce(child.cast(sumDataType), Literal.default(sumDataType)),",
          "105:       failOnError = useAnsiAdd),",
          "110:   override def flatArguments: Iterator[Any] = Iterator(child)",
          "111: }",
          "113: @ExpressionDescription(",
          "114:   usage = \"_FUNC_(expr) - Returns the mean calculated from values of a group.\",",
          "115:   examples = \"\"\"",
          "116:     Examples:",
          "117:       > SELECT _FUNC_(col) FROM VALUES (1), (2), (3) AS tab(col);",
          "118:        2.0",
          "119:       > SELECT _FUNC_(col) FROM VALUES (1), (2), (NULL) AS tab(col);",
          "120:        1.5",
          "121:   \"\"\",",
          "122:   group = \"agg_funcs\",",
          "123:   since = \"1.0.0\")",
          "124: case class Average(",
          "125:     child: Expression,",
          "126:     useAnsiAdd: Boolean = SQLConf.get.ansiEnabled) extends AverageBase {",
          "127:   def this(child: Expression) = this(child, useAnsiAdd = SQLConf.get.ansiEnabled)",
          "132:   override lazy val updateExpressions: Seq[Expression] = getUpdateExpressions",
          "134:   override lazy val mergeExpressions: Seq[Expression] = getMergeExpressions",
          "136:   override lazy val evaluateExpression: Expression = getEvaluateExpression",
          "137: }",
          "140: @ExpressionDescription(",
          "141:   usage = \"_FUNC_(expr) - Returns the mean calculated from values of a group and the result is null on overflow.\",",
          "142:   examples = \"\"\"",
          "143:     Examples:",
          "144:       > SELECT _FUNC_(col) FROM VALUES (1), (2), (3) AS tab(col);",
          "145:        2.0",
          "146:       > SELECT _FUNC_(col) FROM VALUES (1), (2), (NULL) AS tab(col);",
          "147:        1.5",
          "148:       > SELECT _FUNC_(col) FROM VALUES (interval '2147483647 months'), (interval '1 months') AS tab(col);",
          "149:        NULL",
          "150:   \"\"\",",
          "151:   group = \"agg_funcs\",",
          "152:   since = \"3.3.0\")",
          "154: case class TryAverage(child: Expression) extends AverageBase {",
          "155:   override def useAnsiAdd: Boolean = resultType match {",
          "159:     case _: DoubleType | _: DecimalType => false",
          "160:     case _ => true",
          "161:   }",
          "163:   private def addTryEvalIfNeeded(expression: Expression): Expression = {",
          "164:     if (useAnsiAdd) {",
          "165:       TryEval(expression)",
          "166:     } else {",
          "167:       expression",
          "168:     }",
          "169:   }",
          "171:   override lazy val updateExpressions: Seq[Expression] = {",
          "172:     val expressions = getUpdateExpressions",
          "173:     addTryEvalIfNeeded(expressions.head) +: expressions.tail",
          "174:   }",
          "176:   override lazy val mergeExpressions: Seq[Expression] = {",
          "177:     val expressions = getMergeExpressions",
          "178:     if (useAnsiAdd) {",
          "179:       val bufferOverflow = sum.left.isNull && count.left > 0L",
          "180:       val inputOverflow = sum.right.isNull && count.right > 0L",
          "181:       Seq(",
          "182:         If(",
          "183:           bufferOverflow || inputOverflow,",
          "184:           Literal.create(null, resultType),",
          "187:           TryEval(Add(KnownNotNull(sum.left), KnownNotNull(sum.right), useAnsiAdd))),",
          "188:           expressions(1))",
          "189:     } else {",
          "190:       expressions",
          "191:     }",
          "192:   }",
          "194:   override lazy val evaluateExpression: Expression = {",
          "195:     addTryEvalIfNeeded(getEvaluateExpression)",
          "196:   }",
          "198:   override protected def withNewChildInternal(newChild: Expression): Expression =",
          "199:     copy(child = newChild)",
          "201:   override def prettyName: String = \"try_avg\"",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/Sum.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/Sum.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/Sum.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/Sum.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "30:   with ImplicitCastInputTypes",
          "31:   with UnaryLike[Expression] {",
          "35:   protected def shouldTrackIsEmpty: Boolean",
          "",
          "[Removed Lines]",
          "33:   def failOnError: Boolean",
          "",
          "[Added Lines]",
          "34:   def useAnsiAdd: Boolean",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "82:     val sumExpr = if (child.nullable) {",
          "83:       If(child.isNull, sum,",
          "85:     } else {",
          "87:     }",
          "89:     val isEmptyExpr = if (child.nullable) {",
          "",
          "[Removed Lines]",
          "84:         Add(sum, KnownNotNull(child).cast(resultType), failOnError = failOnError))",
          "86:       Add(sum, child.cast(resultType), failOnError = failOnError)",
          "",
          "[Added Lines]",
          "85:         Add(sum, KnownNotNull(child).cast(resultType), failOnError = useAnsiAdd))",
          "87:       Add(sum, child.cast(resultType), failOnError = useAnsiAdd)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "100:     if (child.nullable) {",
          "102:         sum))",
          "103:     } else {",
          "105:     }",
          "106:   }",
          "",
          "[Removed Lines]",
          "101:       Seq(coalesce(Add(coalesce(sum, zero), child.cast(resultType), failOnError = failOnError),",
          "104:       Seq(Add(coalesce(sum, zero), child.cast(resultType), failOnError = failOnError))",
          "",
          "[Added Lines]",
          "102:       Seq(coalesce(Add(coalesce(sum, zero), child.cast(resultType), failOnError = useAnsiAdd),",
          "105:       Seq(Add(coalesce(sum, zero), child.cast(resultType), failOnError = useAnsiAdd))",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "131:       isEmpty.left && isEmpty.right)",
          "132:   } else {",
          "133:     Seq(coalesce(",
          "135:       sum.left))",
          "136:   }",
          "",
          "[Removed Lines]",
          "130:         Add(KnownNotNull(sum.left), KnownNotNull(sum.right), failOnError)),",
          "134:       Add(coalesce(sum.left, zero), sum.right, failOnError = failOnError),",
          "",
          "[Added Lines]",
          "131:         Add(KnownNotNull(sum.left), KnownNotNull(sum.right), useAnsiAdd)),",
          "135:       Add(coalesce(sum.left, zero), sum.right, failOnError = useAnsiAdd),",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "145:   protected def getEvaluateExpression: Expression = resultType match {",
          "146:     case d: DecimalType =>",
          "147:       If(isEmpty, Literal.create(null, resultType),",
          "149:     case _ if shouldTrackIsEmpty =>",
          "150:       If(isEmpty, Literal.create(null, resultType), sum)",
          "151:     case _ => sum",
          "152:   }",
          "155:   override def flatArguments: Iterator[Any] = Iterator(child)",
          "156: }",
          "",
          "[Removed Lines]",
          "148:         CheckOverflowInSum(sum, d, !failOnError))",
          "",
          "[Added Lines]",
          "149:         CheckOverflowInSum(sum, d, !useAnsiAdd))",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "170:   since = \"1.0.0\")",
          "171: case class Sum(",
          "172:     child: Expression,",
          "174:   extends SumBase(child) {",
          "177:   override def shouldTrackIsEmpty: Boolean = resultType match {",
          "178:     case _: DecimalType => true",
          "",
          "[Removed Lines]",
          "173:     failOnError: Boolean = SQLConf.get.ansiEnabled)",
          "175:   def this(child: Expression) = this(child, failOnError = SQLConf.get.ansiEnabled)",
          "",
          "[Added Lines]",
          "174:     useAnsiAdd: Boolean = SQLConf.get.ansiEnabled)",
          "176:   def this(child: Expression) = this(child, useAnsiAdd = SQLConf.get.ansiEnabled)",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "208: case class TrySum(child: Expression) extends SumBase(child) {",
          "214:     case _: DoubleType | _: DecimalType => false",
          "215:     case _ => true",
          "216:   }",
          "",
          "[Removed Lines]",
          "210:   override def failOnError: Boolean = dataType match {",
          "",
          "[Added Lines]",
          "211:   override def useAnsiAdd: Boolean = dataType match {",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "224:   }",
          "226:   override lazy val updateExpressions: Seq[Expression] =",
          "228:       val expressions = getUpdateExpressions",
          "",
          "[Removed Lines]",
          "227:     if (failOnError) {",
          "",
          "[Added Lines]",
          "228:     if (useAnsiAdd) {",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "234:     }",
          "236:   override lazy val mergeExpressions: Seq[Expression] =",
          "238:       getMergeExpressions.map(TryEval)",
          "239:     } else {",
          "240:       getMergeExpressions",
          "241:     }",
          "243:   override lazy val evaluateExpression: Expression =",
          "245:       TryEval(getEvaluateExpression)",
          "246:     } else {",
          "247:       getEvaluateExpression",
          "",
          "[Removed Lines]",
          "237:     if (failOnError) {",
          "244:     if (failOnError) {",
          "",
          "[Added Lines]",
          "238:     if (useAnsiAdd) {",
          "245:     if (useAnsiAdd) {",
          "",
          "---------------"
        ],
        "sql/core/src/test/resources/sql-tests/inputs/try_aggregates.sql||sql/core/src/test/resources/sql-tests/inputs/try_aggregates.sql": [
          "File: sql/core/src/test/resources/sql-tests/inputs/try_aggregates.sql -> sql/core/src/test/resources/sql-tests/inputs/try_aggregates.sql",
          "--- Hunk 1 ---",
          "[Context before]",
          "11: SELECT try_sum(col) FROM VALUES (interval '2147483647 months'), (interval '1 months') AS tab(col);",
          "12: SELECT try_sum(col) FROM VALUES (interval '1 seconds'), (interval '1 seconds') AS tab(col);",
          "13: SELECT try_sum(col) FROM VALUES (interval '106751991 DAYS'), (interval '1 DAYS') AS tab(col);",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "15: -- try_avg",
          "16: SELECT try_avg(col) FROM VALUES (5), (10), (15) AS tab(col);",
          "17: SELECT try_avg(col) FROM VALUES (5.0), (10.0), (15.0) AS tab(col);",
          "18: SELECT try_avg(col) FROM VALUES (NULL), (10), (15) AS tab(col);",
          "19: SELECT try_avg(col) FROM VALUES (NULL), (NULL) AS tab(col);",
          "20: SELECT try_avg(col) FROM VALUES (9223372036854775807L), (1L) AS tab(col);",
          "21: -- test overflow in Decimal(38, 0)",
          "22: SELECT try_avg(col) FROM VALUES (98765432109876543210987654321098765432BD), (98765432109876543210987654321098765432BD) AS tab(col);",
          "24: SELECT try_avg(col) FROM VALUES (interval '1 months'), (interval '1 months') AS tab(col);",
          "25: SELECT try_avg(col) FROM VALUES (interval '2147483647 months'), (interval '1 months') AS tab(col);",
          "26: SELECT try_avg(col) FROM VALUES (interval '1 seconds'), (interval '1 seconds') AS tab(col);",
          "27: SELECT try_avg(col) FROM VALUES (interval '106751991 DAYS'), (interval '1 DAYS') AS tab(col);",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala||sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "4341:       checkAnswer(df.repartitionByRange(2, col(\"v\")).selectExpr(\"try_sum(v)\"), Row(null))",
          "4342:     }",
          "4343:   }",
          "4344: }",
          "4346: case class Foo(bar: Option[String])",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "4345:   test(\"SPARK-38589: try_avg should return null if overflow happens before merging\") {",
          "4346:     val yearMonthDf = Seq(Int.MaxValue, Int.MaxValue, 2)",
          "4347:       .map(Period.ofMonths)",
          "4348:       .toDF(\"v\")",
          "4349:     val dayTimeDf = Seq(106751991L, 106751991L, 2L)",
          "4350:       .map(Duration.ofDays)",
          "4351:       .toDF(\"v\")",
          "4352:     Seq(yearMonthDf, dayTimeDf).foreach { df =>",
          "4353:       checkAnswer(df.repartitionByRange(2, col(\"v\")).selectExpr(\"try_avg(v)\"), Row(null))",
          "4354:     }",
          "4355:   }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "c7e2604b098d153b98825db6e049e3e1a515a148",
      "candidate_info": {
        "commit_hash": "c7e2604b098d153b98825db6e049e3e1a515a148",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/c7e2604b098d153b98825db6e049e3e1a515a148",
        "files": [
          "sql/core/src/test/scala/org/apache/spark/sql/TPCDSQueryTestSuite.scala"
        ],
        "message": "[SPARK-39856][SQL][TESTS] Increase the number of partitions in TPC-DS build to avoid out-of-memory\n\n### What changes were proposed in this pull request?\n\nThis PR proposes to avoid out-of-memory in TPC-DS build at GitHub Actions CI by:\n\n- Increasing the number of partitions being used in shuffle.\n- Truncating precisions after 10th in floats.\n    The number of partitions was previously set to 1 because of different results in precisions that generally we can just ignore.\n- Sort the results regardless of join type since Apache Spark does not guarantee the order of results\n\n### Why are the changes needed?\n\nOne of the reasons for the large memory usage seems to be single partition that's being used in the shuffle.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo, test-only.\n\n### How was this patch tested?\n\nGitHub Actions in this CI will test it out.\n\nCloses #37270 from HyukjinKwon/deflake-tpcds.\n\nAuthored-by: Hyukjin Kwon <gurwls223@apache.org>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>\n(cherry picked from commit 7358253755762f9bfe6cedc1a50ec14616cfeace)\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>",
        "before_after_code_files": [
          "sql/core/src/test/scala/org/apache/spark/sql/TPCDSQueryTestSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/TPCDSQueryTestSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/core/src/test/scala/org/apache/spark/sql/TPCDSQueryTestSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/TPCDSQueryTestSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/TPCDSQueryTestSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/TPCDSQueryTestSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "64:   override protected def sparkConf: SparkConf = super.sparkConf",
          "67:   protected override def createSparkSession: TestSparkSession = {",
          "68:     new TestSparkSession(new SparkContext(\"local[1]\", this.getClass.getSimpleName, sparkConf))",
          "",
          "[Removed Lines]",
          "65:     .set(SQLConf.SHUFFLE_PARTITIONS.key, \"1\")",
          "",
          "[Added Lines]",
          "65:     .set(SQLConf.SHUFFLE_PARTITIONS.key, 4.toString)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "105:       query: String,",
          "106:       goldenFile: File,",
          "107:       conf: Map[String, String]): Unit = {",
          "109:     withSQLConf(conf.toSeq: _*) {",
          "110:       try {",
          "111:         val (schema, output) = handleExceptions(getNormalizedResult(spark, query))",
          "",
          "[Removed Lines]",
          "108:     val shouldSortResults = sortMergeJoinConf != conf  // Sort for other joins",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "143:         assertResult(expectedSchema, s\"Schema did not match\\n$queryString\") {",
          "144:           schema",
          "145:         }",
          "157:         }",
          "158:       } catch {",
          "159:         case e: Throwable =>",
          "",
          "[Removed Lines]",
          "146:         if (shouldSortResults) {",
          "147:           val expectSorted = expectedOutput.split(\"\\n\").sorted.map(_.trim)",
          "148:             .mkString(\"\\n\").replaceAll(\"\\\\s+$\", \"\")",
          "149:           val outputSorted = output.sorted.map(_.trim).mkString(\"\\n\").replaceAll(\"\\\\s+$\", \"\")",
          "150:           assertResult(expectSorted, s\"Result did not match\\n$queryString\") {",
          "151:             outputSorted",
          "152:           }",
          "153:         } else {",
          "154:           assertResult(expectedOutput, s\"Result did not match\\n$queryString\") {",
          "155:             outputString",
          "156:           }",
          "",
          "[Added Lines]",
          "146:         val expectSorted = expectedOutput.split(\"\\n\").sorted.map(_.trim)",
          "147:           .mkString(\"\\n\").replaceAll(\"\\\\s+$\", \"\")",
          "148:           .replaceAll(\"\"\"([0-9]+.[0-9]{10})([0-9]*)\"\"\", \"$1\")",
          "149:         val outputSorted = output.sorted.map(_.trim).mkString(\"\\n\")",
          "150:           .replaceAll(\"\\\\s+$\", \"\")",
          "151:           .replaceAll(\"\"\"([0-9]+.[0-9]{10})([0-9]*)\"\"\", \"$1\")",
          "152:         assertResult(expectSorted, s\"Result did not match\\n$queryString\") {",
          "153:           outputSorted",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "23fa70e9b2b5c896a12f95173dd581d9044b85a7",
      "candidate_info": {
        "commit_hash": "23fa70e9b2b5c896a12f95173dd581d9044b85a7",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/23fa70e9b2b5c896a12f95173dd581d9044b85a7",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/AliasHelper.scala"
        ],
        "message": "[SPARK-28090][SQL] Improve `replaceAliasButKeepName` performance\n\n### What changes were proposed in this pull request?\n\nSPARK-28090 ticket description contains an example query with multiple nested struct creation and field extraction. The following is is an example of the query when the sample code range is set to only 3:\n```\nProject [struct(num1, numerics#23.num1, num10, numerics#23.num10, num11, numerics#23.num11, num12, numerics#23.num12, num13, numerics#23.num13, num14, numerics#23.num14, num15, numerics#23.num15, num2, numerics#23.num2, num3, numerics#23.num3, num4, numerics#23.num4, num5, numerics#23.num5, num6, numerics#23.num6, num7, numerics#23.num7, num8, numerics#23.num8, num9, numerics#23.num9, out_num1, numerics#23.out_num1, out_num2, -numerics#23.num2) AS numerics#42]\n+- Project [struct(num1, numerics#5.num1, num10, numerics#5.num10, num11, numerics#5.num11, num12, numerics#5.num12, num13, numerics#5.num13, num14, numerics#5.num14, num15, numerics#5.num15, num2, numerics#5.num2, num3, numerics#5.num3, num4, numerics#5.num4, num5, numerics#5.num5, num6, numerics#5.num6, num7, numerics#5.num7, num8, numerics#5.num8, num9, numerics#5.num9, out_num1, -numerics#5.num1) AS numerics#23]\n   +- LogicalRDD [numerics#5], false\n```\nIf the level of nesting reaches 7 the query plan generation becomes extremely slow on Spark 2.4. That is because both\n- `CollapseProject` rule is slow and\n- some of the expression optimization rules running on the huge, not yet simplified expression tree of the single, collapsed `Project` node are slow.\n\nOn Spark 3.3, after SPARK-36718, `CollapseProject` doesn't collapse such plans so the above issues don't occur,\nbut `PhysicalOperation` extractor has an issue that it also builds up that huge expression tree and then traverses and modifies it in `AliasHelper.replaceAliasButKeepName()`. With a small change in that function we can avoid such costly operations.\n\n### Why are the changes needed?\nThe suggested change reduced the plan generation time of the example query from minutes (range = 7) or hours (range = 8+) to seconds.\n\n### Does this PR introduce _any_ user-facing change?\nThe example query can be executed.\n\n### How was this patch tested?\nExisting UTs + manual test of the example query in the ticket description.\n\nCloses #35382 from peter-toth/SPARK-28090-improve-replacealiasbutkeepname.\n\nAuthored-by: Peter Toth <peter.toth@gmail.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/AliasHelper.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/AliasHelper.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/AliasHelper.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/AliasHelper.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/AliasHelper.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/AliasHelper.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "70:   protected def replaceAliasButKeepName(",
          "71:      expr: NamedExpression,",
          "72:      aliasMap: AttributeMap[Alias]): NamedExpression = {",
          "76:       case a: Attribute => aliasMap.get(a).map(_.withName(a.name)).getOrElse(a)",
          "78:   }",
          "80:   protected def trimAliases(e: Expression): Expression = {",
          "",
          "[Removed Lines]",
          "75:     trimNonTopLevelAliases(expr.transformUp {",
          "77:     }).asInstanceOf[NamedExpression]",
          "",
          "[Added Lines]",
          "73:     expr match {",
          "77:       case o =>",
          "80:         o.mapChildren(_.transformUp {",
          "81:           case a: Attribute => aliasMap.get(a).map(_.child).getOrElse(a)",
          "82:         }).asInstanceOf[NamedExpression]",
          "83:     }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "bdf76b629ce4a85affb534dc596357dc49a8e894",
      "candidate_info": {
        "commit_hash": "bdf76b629ce4a85affb534dc596357dc49a8e894",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/bdf76b629ce4a85affb534dc596357dc49a8e894",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/DistinctKeyVisitor.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/plans/logical/DistinctKeyVisitorSuite.scala"
        ],
        "message": "[SPARK-36194][SQL][FOLLOWUP] Propagate distinct keys more precisely\n\n### What changes were proposed in this pull request?\n\nThis PR is a followup of https://github.com/apache/spark/pull/35779 , to propagate distinct keys more precisely in 2 cases:\n1. For `LIMIT 1`, each output attribute is a distinct key, not the entire tuple.\n2. For aggregate, we can still propagate distinct keys from child.\n\n### Why are the changes needed?\n\nmake the optimization cover more cases\n\n### Does this PR introduce _any_ user-facing change?\n\nno\n\n### How was this patch tested?\n\nnew tests\n\nCloses #36100 from cloud-fan/followup.\n\nAuthored-by: Wenchen Fan <wenchen@databricks.com>\nSigned-off-by: Yuming Wang <yumwang@ebay.com>\n(cherry picked from commit fbe82fb8ffaa0243c4085627e6e9a2813dc93e57)\nSigned-off-by: Yuming Wang <yumwang@ebay.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/DistinctKeyVisitor.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/DistinctKeyVisitor.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/plans/logical/DistinctKeyVisitorSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/plans/logical/DistinctKeyVisitorSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/DistinctKeyVisitor.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/DistinctKeyVisitor.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/DistinctKeyVisitor.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/DistinctKeyVisitor.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "50:     }.filter(_.nonEmpty)",
          "51:   }",
          "53:   override def default(p: LogicalPlan): Set[ExpressionSet] = Set.empty[ExpressionSet]",
          "55:   override def visitAggregate(p: Aggregate): Set[ExpressionSet] = {",
          "56:     val groupingExps = ExpressionSet(p.groupingExpressions) // handle group by a, a",
          "58:   }",
          "60:   override def visitDistinct(p: Distinct): Set[ExpressionSet] = Set(ExpressionSet(p.output))",
          "",
          "[Removed Lines]",
          "57:     projectDistinctKeys(Set(groupingExps), p.aggregateExpressions)",
          "",
          "[Added Lines]",
          "59:   private def addDistinctKey(",
          "60:       keys: Set[ExpressionSet],",
          "61:       newExpressionSet: ExpressionSet): Set[ExpressionSet] = {",
          "62:     if (keys.exists(_.subsetOf(newExpressionSet))) {",
          "63:       keys",
          "64:     } else {",
          "65:       keys.filterNot(s => newExpressionSet.subsetOf(s)) + newExpressionSet",
          "66:     }",
          "67:   }",
          "73:     projectDistinctKeys(addDistinctKey(p.child.distinctKeys, groupingExps), p.aggregateExpressions)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "71:   override def visitGlobalLimit(p: GlobalLimit): Set[ExpressionSet] = {",
          "72:     p.maxRows match {",
          "74:       case _ => p.child.distinctKeys",
          "75:     }",
          "76:   }",
          "",
          "[Removed Lines]",
          "73:       case Some(value) if value <= 1 => Set(ExpressionSet(p.output))",
          "",
          "[Added Lines]",
          "89:       case Some(value) if value <= 1 => p.output.map(attr => ExpressionSet(Seq(attr))).toSet",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/plans/logical/DistinctKeyVisitorSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/plans/logical/DistinctKeyVisitorSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/plans/logical/DistinctKeyVisitorSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/plans/logical/DistinctKeyVisitorSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "66:       Set(ExpressionSet(Seq(a)), ExpressionSet(Seq(d.toAttribute))))",
          "67:     checkDistinctAttributes(t1.groupBy(f.child, 'b)(f, 'b, sum('c)),",
          "68:       Set(ExpressionSet(Seq(f.toAttribute, b))))",
          "69:   }",
          "71:   test(\"Distinct's distinct attributes\") {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "71:     checkDistinctAttributes(t1.limit(1).groupBy($\"a\", $\"b\")($\"a\", $\"b\"),",
          "72:       Set(ExpressionSet(Seq(a)), ExpressionSet(Seq(b))))",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "86:   test(\"Limit's distinct attributes\") {",
          "87:     checkDistinctAttributes(Distinct(t1).limit(10), Set(ExpressionSet(Seq(a, b, c))))",
          "88:     checkDistinctAttributes(LocalLimit(10, Distinct(t1)), Set(ExpressionSet(Seq(a, b, c))))",
          "90:   }",
          "92:   test(\"Intersect's distinct attributes\") {",
          "",
          "[Removed Lines]",
          "89:     checkDistinctAttributes(t1.limit(1), Set(ExpressionSet(Seq(a, b, c))))",
          "",
          "[Added Lines]",
          "93:     checkDistinctAttributes(t1.limit(1),",
          "94:       Set(ExpressionSet(Seq(a)), ExpressionSet(Seq(b)), ExpressionSet(Seq(c))))",
          "",
          "---------------"
        ]
      }
    }
  ]
}