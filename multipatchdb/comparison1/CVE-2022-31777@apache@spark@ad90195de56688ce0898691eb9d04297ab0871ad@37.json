{
  "cve_id": "CVE-2022-31777",
  "cve_desc": "A stored cross-site scripting (XSS) vulnerability in Apache Spark 3.2.1 and earlier, and 3.3.0, allows remote attackers to execute arbitrary JavaScript in the web browser of a user, by including a malicious payload into the logs which would be returned in logs rendered in the UI.",
  "repo": "apache/spark",
  "patch_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
  "patch_info": {
    "commit_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
    "repo": "apache/spark",
    "commit_url": "https://github.com/apache/spark/commit/ad90195de56688ce0898691eb9d04297ab0871ad",
    "files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ],
    "message": "[SPARK-39505][UI] Escape log content rendered in UI\n\n### What changes were proposed in this pull request?\n\nEscape log content rendered to the UI.\n\n### Why are the changes needed?\n\nLog content may contain reserved characters or other code in the log and be misinterpreted in the UI as HTML.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nExisting tests\n\nCloses #36902 from srowen/LogViewEscape.\n\nAuthored-by: Sean Owen <srowen@gmail.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
    "before_after_code_files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ]
  },
  "patch_diff": {
    "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js": [
      "File: core/src/main/resources/org/apache/spark/ui/static/log-view.js -> core/src/main/resources/org/apache/spark/ui/static/log-view.js",
      "--- Hunk 1 ---",
      "[Context before]",
      "85:       if (retStartByte == 0) {",
      "86:         disableMoreButton();",
      "87:       }",
      "90:       curLogLength = curLogLength + (startByte - retStartByte);",
      "91:       startByte = retStartByte;",
      "",
      "[Removed Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(cleanData);",
      "",
      "[Added Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(document.createTextNode(cleanData));",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "115:             var retLogLength = dataInfo[2];",
      "117:             var cleanData = data.substring(newlineIndex + 1);",
      "120:             curLogLength = curLogLength + (retEndByte - retStartByte);",
      "121:             endByte = retEndByte;",
      "",
      "[Removed Lines]",
      "118:             $(\"pre\", \".log-content\").append(cleanData);",
      "",
      "[Added Lines]",
      "118:             $(\"pre\", \".log-content\").append(document.createTextNode(cleanData));",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "9db11628ff7c8ebf2fc9bd8c7cd7c36003d8436f",
      "candidate_info": {
        "commit_hash": "9db11628ff7c8ebf2fc9bd8c7cd7c36003d8436f",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/9db11628ff7c8ebf2fc9bd8c7cd7c36003d8436f",
        "files": [
          "docs/sql-migration-guide.md",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/stringExpressions.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/StringExpressionsSuite.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/functions.scala"
        ],
        "message": "[SPARK-37047][SQL][FOLLOWUP] Add legacy flag for the breaking change of lpad and rpad for binary type\n\n### What changes were proposed in this pull request?\nAdd a legacy flag `spark.sql.legacy.lpadRpadForBinaryType.enabled` for the breaking change introduced in https://github.com/apache/spark/pull/34154.\n\nThe flag is enabled by default. When it is disabled, restore the pre-change behavior that there is no special handling on `BINARY` input types.\n\n### Why are the changes needed?\nThe original commit is a breaking change, and breaking changes should be encouraged to add a flag to turn it off for smooth migration between versions.\n\n### Does this PR introduce _any_ user-facing change?\nWith the default value of the conf, there is no user-facing difference.\nIf users turn this conf off, they can restore the pre-change behavior.\n\n### How was this patch tested?\nThrough unit tests.\n\nCloses #36103 from anchovYu/flags-lpad-rpad-binary.\n\nAuthored-by: Xinyi Yu <xinyi.yu@databricks.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\n(cherry picked from commit e2683c2f3c6e758ef852355533793c707fd5e061)\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/stringExpressions.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/stringExpressions.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/StringExpressionsSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/StringExpressionsSuite.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/functions.scala||sql/core/src/main/scala/org/apache/spark/sql/functions.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/stringExpressions.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/stringExpressions.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/stringExpressions.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/stringExpressions.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "1462: trait PadExpressionBuilderBase extends ExpressionBuilder {",
          "1463:   override def build(funcName: String, expressions: Seq[Expression]): Expression = {",
          "1464:     val numArgs = expressions.length",
          "1465:     if (numArgs == 2) {",
          "1467:         BinaryPad(funcName, expressions(0), expressions(1), Literal(Array[Byte](0)))",
          "1468:       } else {",
          "1469:         createStringPad(expressions(0), expressions(1), Literal(\" \"))",
          "1470:       }",
          "1471:     } else if (numArgs == 3) {",
          "1473:         BinaryPad(funcName, expressions(0), expressions(1), expressions(2))",
          "1474:       } else {",
          "1475:         createStringPad(expressions(0), expressions(1), expressions(2))",
          "",
          "[Removed Lines]",
          "1466:       if (expressions(0).dataType == BinaryType) {",
          "1472:       if (expressions(0).dataType == BinaryType && expressions(2).dataType == BinaryType) {",
          "",
          "[Added Lines]",
          "1464:     val behaviorChangeEnabled = !SQLConf.get.getConf(SQLConf.LEGACY_LPAD_RPAD_BINARY_TYPE_AS_STRING)",
          "1467:       if (expressions(0).dataType == BinaryType && behaviorChangeEnabled) {",
          "1473:       if (expressions(0).dataType == BinaryType && expressions(2).dataType == BinaryType",
          "1474:         && behaviorChangeEnabled) {",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "3712:       .booleanConf",
          "3713:       .createWithDefault(true)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "3715:   val LEGACY_LPAD_RPAD_BINARY_TYPE_AS_STRING =",
          "3716:     buildConf(\"spark.sql.legacy.lpadRpadAlwaysReturnString\")",
          "3717:       .internal()",
          "3718:       .doc(\"When set to false, when the first argument and the optional padding pattern is a \" +",
          "3719:         \"byte sequence, the result is a BINARY value. The default padding pattern in this case \" +",
          "3720:         \"is the zero byte. \" +",
          "3721:         \"When set to true, it restores the legacy behavior of always returning string types \" +",
          "3722:         \"even for binary inputs.\")",
          "3723:       .version(\"3.3.0\")",
          "3724:       .booleanConf",
          "3725:       .createWithDefault(false)",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/StringExpressionsSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/StringExpressionsSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/StringExpressionsSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/StringExpressionsSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "773:     checkEvaluation(StringRPad(Literal(\"hi\"), Literal(1)), \"h\")",
          "774:   }",
          "776:   test(\"REPEAT\") {",
          "777:     val s1 = 'a.string.at(0)",
          "778:     val s2 = 'b.int.at(1)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "776:   test(\"PadExpressionBuilderBase\") {",
          "778:     Seq(true, false).foreach { confVal =>",
          "779:       SQLConf.get.setConf(SQLConf.LEGACY_LPAD_RPAD_BINARY_TYPE_AS_STRING, confVal)",
          "781:       val lpadExp1 = LPadExpressionBuilder.build(\"lpad\", Seq(Literal(\"hi\"), Literal(5)))",
          "782:       val lpadExp2 = LPadExpressionBuilder.build(\"lpad\", Seq(Literal(Array[Byte]()), Literal(5)))",
          "783:       val lpadExp3 = LPadExpressionBuilder.build(\"lpad\",",
          "784:         Seq(Literal(\"hi\"), Literal(5), Literal(\"somepadding\")))",
          "785:       val lpadExp4 = LPadExpressionBuilder.build(\"lpad\",",
          "786:         Seq(Literal(Array[Byte](1, 2)), Literal(5), Literal(\"somepadding\")))",
          "787:       val lpadExp5 = LPadExpressionBuilder.build(\"lpad\",",
          "788:         Seq(Literal(Array[Byte](1, 2)), Literal(5), Literal(Array[Byte](1))))",
          "790:       val rpadExp1 = RPadExpressionBuilder.build(\"rpad\", Seq(Literal(\"hi\"), Literal(5)))",
          "791:       val rpadExp2 = RPadExpressionBuilder.build(\"rpad\", Seq(Literal(Array[Byte]()), Literal(5)))",
          "792:       val rpadExp3 = RPadExpressionBuilder.build(\"rpad\",",
          "793:         Seq(Literal(\"hi\"), Literal(5), Literal(\"somepadding\")))",
          "794:       val rpadExp4 = RPadExpressionBuilder.build(\"rpad\",",
          "795:         Seq(Literal(Array[Byte](1, 2)), Literal(5), Literal(\"somepadding\")))",
          "796:       val rpadExp5 = RPadExpressionBuilder.build(\"rpad\",",
          "797:         Seq(Literal(Array[Byte](1, 2)), Literal(5), Literal(Array[Byte](1))))",
          "799:       assert(lpadExp1 == StringLPad(Literal(\"hi\"), Literal(5), Literal(\" \")))",
          "800:       assert(lpadExp3 == StringLPad(Literal(\"hi\"), Literal(5), Literal(\"somepadding\")))",
          "801:       assert(lpadExp4 == StringLPad(Literal(Array[Byte](1, 2)), Literal(5), Literal(\"somepadding\")))",
          "803:       assert(rpadExp1 == StringRPad(Literal(\"hi\"), Literal(5), Literal(\" \")))",
          "804:       assert(rpadExp3 == StringRPad(Literal(\"hi\"), Literal(5), Literal(\"somepadding\")))",
          "805:       assert(rpadExp4 == StringRPad(Literal(Array[Byte](1, 2)), Literal(5), Literal(\"somepadding\")))",
          "807:       if (!SQLConf.get.getConf(SQLConf.LEGACY_LPAD_RPAD_BINARY_TYPE_AS_STRING)) {",
          "808:         assert(lpadExp2 ==",
          "809:           BinaryPad(\"lpad\", Literal(Array[Byte]()), Literal(5), Literal(Array[Byte](0))))",
          "810:         assert(lpadExp5 ==",
          "811:           BinaryPad(\"lpad\", Literal(Array[Byte](1, 2)), Literal(5), Literal(Array[Byte](1))))",
          "813:         assert(rpadExp2 ==",
          "814:           BinaryPad(\"rpad\", Literal(Array[Byte]()), Literal(5), Literal(Array[Byte](0))))",
          "815:         assert(rpadExp5 ==",
          "816:           BinaryPad(\"rpad\", Literal(Array[Byte](1, 2)), Literal(5), Literal(Array[Byte](1))))",
          "817:       } else {",
          "818:         assert(lpadExp2 ==",
          "819:           StringLPad(Literal(Array[Byte]()), Literal(5), Literal(\" \")))",
          "820:         assert(lpadExp5 ==",
          "821:           StringLPad(Literal(Array[Byte](1, 2)), Literal(5), Literal(Array[Byte](1))))",
          "823:         assert(rpadExp2 ==",
          "824:           StringRPad(Literal(Array[Byte]()), Literal(5), Literal(\" \")))",
          "825:         assert(rpadExp5 ==",
          "826:           StringRPad(Literal(Array[Byte](1, 2)), Literal(5), Literal(Array[Byte](1))))",
          "827:       }",
          "828:     }",
          "829:   }",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/functions.scala||sql/core/src/main/scala/org/apache/spark/sql/functions.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/functions.scala -> sql/core/src/main/scala/org/apache/spark/sql/functions.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "2778:   def lpad(str: Column, len: Int, pad: Array[Byte]): Column = withExpr {",
          "2780:   }",
          "",
          "[Removed Lines]",
          "2779:     BinaryPad(\"lpad\", str.expr, lit(len).expr, lit(pad).expr)",
          "",
          "[Added Lines]",
          "2779:     UnresolvedFunction(\"lpad\", Seq(str.expr, lit(len).expr, lit(pad).expr), isDistinct = false)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "2867:   def rpad(str: Column, len: Int, pad: Array[Byte]): Column = withExpr {",
          "2869:   }",
          "",
          "[Removed Lines]",
          "2868:     BinaryPad(\"rpad\", str.expr, lit(len).expr, lit(pad).expr)",
          "",
          "[Added Lines]",
          "2868:     UnresolvedFunction(\"rpad\", Seq(str.expr, lit(len).expr, lit(pad).expr), isDistinct = false)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "74043ddd0d60111717a290902014b02e9e9972da",
      "candidate_info": {
        "commit_hash": "74043ddd0d60111717a290902014b02e9e9972da",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/74043ddd0d60111717a290902014b02e9e9972da",
        "files": [
          "sql/core/src/test/scala/org/apache/spark/sql/streaming/StreamingJoinSuite.scala"
        ],
        "message": "[SPARK-38941][TESTS][SQL][3.3] Skip RocksDB-based test case in StreamingJoinSuite on Apple Silicon\n\n### What changes were proposed in this pull request?\nThis PR aims to skip RocksDB-based test case in `StreamingJoinSuite` on Apple Silicon.\n\n### Why are the changes needed?\nCurrently, it is broken on Apple Silicon.\n\n**BEFORE**\n```\n$ build/sbt \"sql/testOnly org.apache.spark.sql.streaming.Streaming*JoinSuite\"\n...\n[info] Run completed in 2 minutes, 47 seconds.\n[info] Total number of tests run: 43\n[info] Suites: completed 4, aborted 0\n[info] Tests: succeeded 42, failed 1, canceled 0, ignored 0, pending 0\n[info] *** 1 TEST FAILED ***\n[error] Failed tests:\n[error] \torg.apache.spark.sql.streaming.StreamingOuterJoinSuite\n[error] (sql / Test / testOnly) sbt.TestsFailedException: Tests unsuccessful\n```\n\n**AFTER**\n```\n$ build/sbt \"sql/testOnly org.apache.spark.sql.streaming.Streaming*JoinSuite\"\n...\n[info] Run completed in 2 minutes, 52 seconds.\n[info] Total number of tests run: 42\n[info] Suites: completed 4, aborted 0\n[info] Tests: succeeded 42, failed 0, canceled 1, ignored 0, pending 0\n[info] All tests passed.\n```\n\n### Does this PR introduce _any_ user-facing change?\nNo.\n\n### How was this patch tested?\nManually test on Apple Silicon.\n\nCloses #36254 from williamhyun/SPARK-38941.\n\nAuthored-by: William Hyun <william@apache.org>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
        "before_after_code_files": [
          "sql/core/src/test/scala/org/apache/spark/sql/streaming/StreamingJoinSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/streaming/StreamingJoinSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/core/src/test/scala/org/apache/spark/sql/streaming/StreamingJoinSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/streaming/StreamingJoinSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/streaming/StreamingJoinSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/streaming/StreamingJoinSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "1357:   test(\"SPARK-38684: outer join works correctly even if processing input rows and \" +",
          "1358:     \"evicting state rows for same grouping key happens in the same micro-batch\") {",
          "1361:     withSQLConf(",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1359:     assume(!Utils.isMacOnAppleSilicon)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "4177626e634cbb0ee446fe042a4a6201b9d8531e",
      "candidate_info": {
        "commit_hash": "4177626e634cbb0ee446fe042a4a6201b9d8531e",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/4177626e634cbb0ee446fe042a4a6201b9d8531e",
        "files": [
          "sql/core/src/test/scala/org/apache/spark/sql/JsonFunctionsSuite.scala"
        ],
        "message": "[SPARK-35320][SQL][FOLLOWUP] Remove duplicated test\n\n### What changes were proposed in this pull request?\n\nFollow-up for https://github.com/apache/spark/pull/33525 to remove duplicated test.\n\n### Why are the changes needed?\n\nWe don't need to do the same test twice.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nThis patch remove the duplicated test, so the existing test should pass.\n\nCloses #36436 from itholic/SPARK-35320.\n\nAuthored-by: itholic <haejoon.lee@databricks.com>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>\n(cherry picked from commit a1ac5c57c7b79fb70656638d284b77dfc4261d35)\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>",
        "before_after_code_files": [
          "sql/core/src/test/scala/org/apache/spark/sql/JsonFunctionsSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/JsonFunctionsSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/core/src/test/scala/org/apache/spark/sql/JsonFunctionsSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/JsonFunctionsSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/JsonFunctionsSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/JsonFunctionsSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "391:   test(\"SPARK-24027: from_json of a map with unsupported key type\") {",
          "392:     val schema = MapType(StructType(StructField(\"f\", IntegerType) :: Nil), StringType)",
          "393:     val startMsg = \"cannot resolve 'entries' due to data type mismatch:\"",
          "395:       Seq(\"\"\"{{\"f\": 1}: \"a\"}\"\"\").toDS().select(from_json($\"value\", schema))",
          "396:     }.getMessage",
          "402:   }",
          "404:   test(\"SPARK-24709: infers schemas of json strings and pass them to from_json\") {",
          "",
          "[Removed Lines]",
          "394:     val exception1 = intercept[AnalysisException] {",
          "397:     assert(exception1.contains(startMsg))",
          "398:     val exception2 = intercept[AnalysisException] {",
          "399:       Seq(\"\"\"{{\"f\": 1}: \"a\"}\"\"\").toDS().select(from_json($\"value\", schema))",
          "400:     }.getMessage",
          "401:     assert(exception2.contains(startMsg))",
          "",
          "[Added Lines]",
          "394:     val exception = intercept[AnalysisException] {",
          "397:     assert(exception.contains(startMsg))",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "26f0d501595d4bddc7420e5c0505ccb1a9a991c4",
      "candidate_info": {
        "commit_hash": "26f0d501595d4bddc7420e5c0505ccb1a9a991c4",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/26f0d501595d4bddc7420e5c0505ccb1a9a991c4",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala"
        ],
        "message": "[SPARK-39981][SQL] Throw the exception QueryExecutionErrors.castingCauseOverflowErrorInTableInsert in Cast\n\nThis PR is a followup of https://github.com/apache/spark/pull/37283. It missed `throw` keyword in the interpreted path.\n\nTo throw an exception as intended instead of returning an exception itself.\n\nYes, it will throw an exception as expected in the interpreted path.\n\nHaven't tested because it's too much straightforward.\n\nCloses #37414 from HyukjinKwon/SPARK-39981.\n\nAuthored-by: Hyukjin Kwon <gurwls223@apache.org>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>\n(cherry picked from commit e6b9c6166a08ad4dca2550bbbb151fa575b730a8)\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "2365:     child.eval(input)",
          "2366:   } catch {",
          "2367:     case e: SparkArithmeticException =>",
          "2369:         child.child.dataType,",
          "2370:         child.dataType,",
          "2371:         columnName)",
          "",
          "[Removed Lines]",
          "2368:       QueryExecutionErrors.castingCauseOverflowErrorInTableInsert(",
          "",
          "[Added Lines]",
          "2368:       throw QueryExecutionErrors.castingCauseOverflowErrorInTableInsert(",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "a33d697e1e7c2854d77cc2302015ef54bf0c32ab",
      "candidate_info": {
        "commit_hash": "a33d697e1e7c2854d77cc2302015ef54bf0c32ab",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/a33d697e1e7c2854d77cc2302015ef54bf0c32ab",
        "files": [
          "python/pyspark/testing/pandasutils.py"
        ],
        "message": "[SPARK-39273][PS][TESTS] Make PandasOnSparkTestCase inherit ReusedSQLTestCase\n\n### What changes were proposed in this pull request?\n\nThis PR proposes to make `PandasOnSparkTestCase` inherit `ReusedSQLTestCase`.\n\n### Why are the changes needed?\n\nWe don't need this:\n\n```python\n    classmethod\n    def tearDownClass(cls):\n        # We don't stop Spark session to reuse across all tests.\n        # The Spark session will be started and stopped at PyTest session level.\n        # Please see pyspark/pandas/conftest.py.\n        pass\n```\n\nanymore in Apache Spark. This has existed to speed up the tests when the codes are in Koalas repository where the tests run sequentially in single process.\n\nIn Apache Spark, we run in multiple processes, and we don't need this anymore.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo, test-only.\n\n### How was this patch tested?\n\nExisting CI should test it out.\n\nCloses #36652 from HyukjinKwon/SPARK-39273.\n\nAuthored-by: Hyukjin Kwon <gurwls223@apache.org>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>\n(cherry picked from commit a6dd6076d708713d11585bf7f3401d522ea48822)\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>",
        "before_after_code_files": [
          "python/pyspark/testing/pandasutils.py||python/pyspark/testing/pandasutils.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "python/pyspark/testing/pandasutils.py||python/pyspark/testing/pandasutils.py": [
          "File: python/pyspark/testing/pandasutils.py -> python/pyspark/testing/pandasutils.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: import functools",
          "19: import shutil",
          "20: import tempfile",
          "22: import warnings",
          "23: from contextlib import contextmanager",
          "24: from distutils.version import LooseVersion",
          "",
          "[Removed Lines]",
          "21: import unittest",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "32: from pyspark.pandas.frame import DataFrame",
          "33: from pyspark.pandas.indexes import Index",
          "34: from pyspark.pandas.series import Series",
          "39: tabulate_requirement_message = None",
          "40: try:",
          "",
          "[Removed Lines]",
          "35: from pyspark.pandas.utils import default_session, SPARK_CONF_ARROW_ENABLED",
          "36: from pyspark.testing.sqlutils import SQLTestUtils",
          "",
          "[Added Lines]",
          "34: from pyspark.pandas.utils import SPARK_CONF_ARROW_ENABLED",
          "35: from pyspark.testing.sqlutils import ReusedSQLTestCase",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "61: have_plotly = plotly_requirement_message is None",
          "65:     @classmethod",
          "66:     def setUpClass(cls):",
          "68:         cls.spark.conf.set(SPARK_CONF_ARROW_ENABLED, True)",
          "77:     def assertPandasEqual(self, left, right, check_exact=True):",
          "78:         if isinstance(left, pd.DataFrame) and isinstance(right, pd.DataFrame):",
          "79:             try:",
          "",
          "[Removed Lines]",
          "64: class PandasOnSparkTestCase(unittest.TestCase, SQLTestUtils):",
          "67:         cls.spark = default_session()",
          "70:     @classmethod",
          "71:     def tearDownClass(cls):",
          "72:         # We don't stop Spark session to reuse across all tests.",
          "73:         # The Spark session will be started and stopped at PyTest session level.",
          "74:         # Please see pyspark/pandas/conftest.py.",
          "75:         pass",
          "",
          "[Added Lines]",
          "62: class PandasOnSparkTestCase(ReusedSQLTestCase):",
          "65:         super(PandasOnSparkTestCase, cls).setUpClass()",
          "",
          "---------------"
        ]
      }
    }
  ]
}