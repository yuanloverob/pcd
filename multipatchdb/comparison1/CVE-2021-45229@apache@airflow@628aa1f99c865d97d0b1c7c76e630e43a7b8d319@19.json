{
  "cve_id": "CVE-2021-45229",
  "cve_desc": "It was discovered that the \"Trigger DAG with config\" screen was susceptible to XSS attacks via the `origin` query argument. This issue affects Apache Airflow versions 2.2.3 and below.",
  "repo": "apache/airflow",
  "patch_hash": "628aa1f99c865d97d0b1c7c76e630e43a7b8d319",
  "patch_info": {
    "commit_hash": "628aa1f99c865d97d0b1c7c76e630e43a7b8d319",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/628aa1f99c865d97d0b1c7c76e630e43a7b8d319",
    "files": [
      "airflow/www/templates/airflow/trigger.html",
      "tests/www/views/test_views_trigger_dag.py"
    ],
    "message": "Simplify trigger cancel button (#21591)\n\nCo-authored-by: Jed Cunningham <jedcunningham@apache.org>\n(cherry picked from commit 65297673a318660fba76797e50d0c06804dfcafc)",
    "before_after_code_files": [
      "airflow/www/templates/airflow/trigger.html||airflow/www/templates/airflow/trigger.html",
      "tests/www/views/test_views_trigger_dag.py||tests/www/views/test_views_trigger_dag.py"
    ]
  },
  "patch_diff": {
    "airflow/www/templates/airflow/trigger.html||airflow/www/templates/airflow/trigger.html": [
      "File: airflow/www/templates/airflow/trigger.html -> airflow/www/templates/airflow/trigger.html",
      "--- Hunk 1 ---",
      "[Context before]",
      "63:       </label>",
      "64:     </div>",
      "65:     <button type=\"submit\" class=\"btn btn-primary\">Trigger</button>",
      "67:   </form>",
      "68: {% endblock %}",
      "",
      "[Removed Lines]",
      "66:     <button type=\"button\" class=\"btn\" onclick=\"location.href = '{{ origin }}'; return false\">Cancel</button>",
      "",
      "[Added Lines]",
      "66:     <a class=\"btn\" href=\"{{ origin }}\">Cancel</a>",
      "",
      "---------------"
    ],
    "tests/www/views/test_views_trigger_dag.py||tests/www/views/test_views_trigger_dag.py": [
      "File: tests/www/views/test_views_trigger_dag.py -> tests/www/views/test_views_trigger_dag.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "133:         (\"javascript:alert(1)\", \"/home\"),",
      "134:         (\"http://google.com\", \"/home\"),",
      "135:         (\"36539'%3balert(1)%2f%2f166\", \"/home\"),",
      "136:         (",
      "137:             \"%2Ftree%3Fdag_id%3Dexample_bash_operator';alert(33)//\",",
      "138:             \"/home\",",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "136:         (",
      "137:             '\"><script>alert(99)</script><a href=\"',",
      "138:             \"&#34;&gt;&lt;script&gt;alert(99)&lt;/script&gt;&lt;a href=&#34;\",",
      "139:         ),",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "145:     test_dag_id = \"example_bash_operator\"",
      "147:     resp = admin_client.get(f'trigger?dag_id={test_dag_id}&origin={test_origin}')",
      "156: @pytest.mark.parametrize(",
      "",
      "[Removed Lines]",
      "148:     check_content_in_response(",
      "149:         '<button type=\"button\" class=\"btn\" onclick=\"location.href = \\'{}\\'; return false\">'.format(",
      "150:             expected_origin",
      "151:         ),",
      "152:         resp,",
      "153:     )",
      "",
      "[Added Lines]",
      "152:     check_content_in_response(f'<a class=\"btn\" href=\"{expected_origin}\">Cancel</a>', resp)",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "b19dfdbc946254cd570b14e032ae9b63042d9339",
      "candidate_info": {
        "commit_hash": "b19dfdbc946254cd570b14e032ae9b63042d9339",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/b19dfdbc946254cd570b14e032ae9b63042d9339",
        "files": [
          "airflow/cli/commands/standalone_command.py",
          "docs/apache-airflow/start/local.rst"
        ],
        "message": "fix(standalone): Remove hardcoded Webserver port (#20429)\n\nPort 8080 is the default port for webserver (https://airflow.apache.org/docs/apache-airflow/stable/cli-and-env-variables-ref.html?highlight=webserver#webserver). By setting it here again explicitly, we forbid users to override it using AIRFLOW__WEBSERVER__WEB_SERVER_PORT. Removing it IMO is not a breaking change, since it will still default to 8080.\n\n(cherry picked from commit 9d36b1fdac16d8db8907d4b792fdbe13a6e80f7e)",
        "before_after_code_files": [
          "airflow/cli/commands/standalone_command.py||airflow/cli/commands/standalone_command.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/cli/commands/standalone_command.py||airflow/cli/commands/standalone_command.py": [
          "File: airflow/cli/commands/standalone_command.py -> airflow/cli/commands/standalone_command.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "72:         self.subcommands[\"webserver\"] = SubCommand(",
          "73:             self,",
          "74:             name=\"webserver\",",
          "76:             env=env,",
          "77:         )",
          "78:         self.subcommands[\"triggerer\"] = SubCommand(",
          "",
          "[Removed Lines]",
          "75:             command=[\"webserver\", \"--port\", \"8080\"],",
          "",
          "[Added Lines]",
          "75:             command=[\"webserver\"],",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "d761affd4f7fd3812ddbc53399f6f4fbe1fbb82b",
      "candidate_info": {
        "commit_hash": "d761affd4f7fd3812ddbc53399f6f4fbe1fbb82b",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/d761affd4f7fd3812ddbc53399f6f4fbe1fbb82b",
        "files": [
          "setup.py"
        ],
        "message": "Update minimum sphinx versions after upgrading sphinx-autoapi (#20170)\n\n* Allow point releases of AutoAPI 1.8 (I used with 1.8.4 in all my testing)\n* Require at least Sphinx v4\n\n  A few things got deprecated in Sphinx 4, and as this dep is only for\n  us building docs we can pick and choose what we like without impacting\n  users, so lets stay up-to-date.\n\n(cherry picked from commit 214b62d88de90d107ea8fe6640cf150ead5784f6)",
        "before_after_code_files": [
          "setup.py||setup.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "setup.py||setup.py": [
          "File: setup.py -> setup.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "250: ]",
          "251: doc = [",
          "252:     'click>=7.1,<9',",
          "255:     'sphinx-airflow-theme',",
          "256:     'sphinx-argparse>=0.1.13',",
          "258:     'sphinx-copybutton',",
          "259:     'sphinx-jinja~=1.1',",
          "260:     'sphinx-rtd-theme>=0.1.6',",
          "",
          "[Removed Lines]",
          "253:     # Sphinx is limited to < 3.5.0 because of https://github.com/sphinx-doc/sphinx/issues/8880",
          "254:     'sphinx>=3.5.0, <5.0.0',",
          "257:     'sphinx-autoapi==1.8.0',",
          "",
          "[Added Lines]",
          "253:     'sphinx>=4.0.0, <5.0.0',",
          "256:     'sphinx-autoapi~=1.8.0',",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "daebc586d0aaaddaea4658734c9292dece150c6a",
      "candidate_info": {
        "commit_hash": "daebc586d0aaaddaea4658734c9292dece150c6a",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/daebc586d0aaaddaea4658734c9292dece150c6a",
        "files": [
          "airflow/www/views.py"
        ],
        "message": "Fix session usage in ``/rendered-k8s`` view (#21006)\n\nWe can't commit the session too early because later functions need that\nsession to fetch related objects.\n\nFix #20534.\n\n(cherry picked from commit a665f48b606065977e0d3952bc74635ce11726d1)",
        "before_after_code_files": [
          "airflow/www/views.py||airflow/www/views.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/www/views.py||airflow/www/views.py": [
          "File: airflow/www/views.py -> airflow/www/views.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "84: from pygments.formatters import HtmlFormatter",
          "85: from sqlalchemy import Date, and_, desc, func, inspect, union_all",
          "86: from sqlalchemy.exc import IntegrityError",
          "88: from wtforms import SelectField, validators",
          "89: from wtforms.validators import InputRequired",
          "",
          "[Removed Lines]",
          "87: from sqlalchemy.orm import joinedload",
          "",
          "[Added Lines]",
          "87: from sqlalchemy.orm import Session, joinedload",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "116: from airflow.utils.helpers import alchemy_to_dict",
          "117: from airflow.utils.log import secrets_masker",
          "118: from airflow.utils.log.log_reader import TaskLogReader",
          "120: from airflow.utils.state import State",
          "121: from airflow.utils.strings import to_boolean",
          "122: from airflow.version import version",
          "",
          "[Removed Lines]",
          "119: from airflow.utils.session import create_session, provide_session",
          "",
          "[Added Lines]",
          "119: from airflow.utils.session import NEW_SESSION, create_session, provide_session",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "1124:         ]",
          "1125:     )",
          "1126:     @action_logging",
          "1128:         \"\"\"Get rendered k8s yaml.\"\"\"",
          "1129:         if not settings.IS_K8S_OR_K8SCELERY_EXECUTOR:",
          "1130:             abort(404)",
          "",
          "[Removed Lines]",
          "1127:     def rendered_k8s(self):",
          "",
          "[Added Lines]",
          "1127:     @provide_session",
          "1128:     def rendered_k8s(self, session: Session = NEW_SESSION):",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "1135:         form = DateTimeForm(data={'execution_date': dttm})",
          "1136:         root = request.args.get('root', '')",
          "1137:         logging.info(\"Retrieving rendered templates.\")",
          "1139:         task = dag.get_task(task_id)",
          "1143:         pod_spec = None",
          "1144:         try:",
          "1146:         except AirflowException as e:",
          "1147:             msg = \"Error rendering Kubernetes POD Spec: \" + escape(e)",
          "1148:             if e.__cause__:",
          "",
          "[Removed Lines]",
          "1138:         dag = current_app.dag_bag.get_dag(dag_id)",
          "1140:         dag_run = dag.get_dagrun(execution_date=dttm)",
          "1141:         ti = dag_run.get_task_instance(task_id=task.task_id)",
          "1145:             pod_spec = ti.get_rendered_k8s_spec()",
          "",
          "[Added Lines]",
          "1140:         dag: DAG = current_app.dag_bag.get_dag(dag_id)",
          "1142:         dag_run = dag.get_dagrun(execution_date=dttm, session=session)",
          "1143:         ti = dag_run.get_task_instance(task_id=task.task_id, session=session)",
          "1147:             pod_spec = ti.get_rendered_k8s_spec(session=session)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "95eaef37f621e8abaa30bb145866f1863471fdd6",
      "candidate_info": {
        "commit_hash": "95eaef37f621e8abaa30bb145866f1863471fdd6",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/95eaef37f621e8abaa30bb145866f1863471fdd6",
        "files": [
          "airflow/operators/trigger_dagrun.py",
          "tests/operators/test_trigger_dagrun.py"
        ],
        "message": "Fix TriggerDagRunOperator extra link (#19410)\n\nThe extra link provided by the operator was previously using the\nexecution date of the triggering dag, not the triggered dag. Store the\nexecution date of the triggered dag in xcom so that it can be read back\nlater within the webserver when the link is being created.\n\n(cherry picked from commit 820e836c4a2e45239279d4d71e1db9434022fec5)",
        "before_after_code_files": [
          "airflow/operators/trigger_dagrun.py||airflow/operators/trigger_dagrun.py",
          "tests/operators/test_trigger_dagrun.py||tests/operators/test_trigger_dagrun.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/operators/trigger_dagrun.py||airflow/operators/trigger_dagrun.py": [
          "File: airflow/operators/trigger_dagrun.py -> airflow/operators/trigger_dagrun.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "24: from airflow.api.common.trigger_dag import trigger_dag",
          "25: from airflow.exceptions import AirflowException, DagNotFound, DagRunAlreadyExists",
          "26: from airflow.models import BaseOperator, BaseOperatorLink, DagBag, DagModel, DagRun",
          "27: from airflow.utils import timezone",
          "28: from airflow.utils.helpers import build_airflow_url_with_query",
          "29: from airflow.utils.state import State",
          "30: from airflow.utils.types import DagRunType",
          "33: class TriggerDagRunLink(BaseOperatorLink):",
          "34:     \"\"\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "27: from airflow.models.xcom import XCom",
          "33: XCOM_EXECUTION_DATE_ISO = \"trigger_execution_date_iso\"",
          "34: XCOM_RUN_ID = \"trigger_run_id\"",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "39:     name = 'Triggered DAG'",
          "41:     def get_link(self, operator, dttm):",
          "43:         return build_airflow_url_with_query(query)",
          "",
          "[Removed Lines]",
          "42:         query = {\"dag_id\": operator.trigger_dag_id, \"execution_date\": dttm.isoformat()}",
          "",
          "[Added Lines]",
          "46:         # Fetch the correct execution date for the triggerED dag which is",
          "47:         # stored in xcom during execution of the triggerING task.",
          "48:         trigger_execution_date_iso = XCom.get_one(",
          "49:             execution_date=dttm, key=XCOM_EXECUTION_DATE_ISO, task_id=operator.task_id, dag_id=operator.dag_id",
          "50:         )",
          "52:         query = {\"dag_id\": operator.trigger_dag_id, \"base_date\": trigger_execution_date_iso}",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "157:             else:",
          "158:                 raise e",
          "160:         if self.wait_for_completion:",
          "161:             # wait for dag to complete",
          "162:             while True:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "171:         # Store the execution date from the dag run (either created or found above) to",
          "172:         # be used when creating the extra link on the webserver.",
          "173:         ti = context['task_instance']",
          "174:         ti.xcom_push(key=XCOM_EXECUTION_DATE_ISO, value=dag_run.execution_date.isoformat())",
          "175:         ti.xcom_push(key=XCOM_RUN_ID, value=dag_run.run_id)",
          "",
          "---------------"
        ],
        "tests/operators/test_trigger_dagrun.py||tests/operators/test_trigger_dagrun.py": [
          "File: tests/operators/test_trigger_dagrun.py -> tests/operators/test_trigger_dagrun.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "19: import pathlib",
          "20: import tempfile",
          "21: from datetime import datetime",
          "24: import pytest",
          "",
          "[Removed Lines]",
          "22: from unittest import TestCase",
          "",
          "[Added Lines]",
          "22: from unittest import TestCase, mock",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "77:         pathlib.Path(self._tmpfile).unlink()",
          "79:     def test_trigger_dagrun(self):",
          "80:         \"\"\"Test TriggerDagRunOperator.\"\"\"",
          "81:         task = TriggerDagRunOperator(task_id=\"test_task\", trigger_dag_id=TRIGGERED_DAG_ID, dag=self.dag)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "79:     @mock.patch('airflow.operators.trigger_dagrun.build_airflow_url_with_query')",
          "80:     def assert_extra_link(self, triggering_exec_date, triggered_dag_run, triggering_task, mock_build_url):",
          "81:         \"\"\"",
          "82:         Asserts whether the correct extra links url will be created.",
          "84:         Specifically it tests whether the correct dag id and date are passed to",
          "85:         the method which constructs the final url.",
          "86:         Note: We can't run that method to generate the url itself because the Flask app context",
          "87:         isn't available within the test logic, so it is mocked here.",
          "88:         \"\"\"",
          "89:         triggering_task.get_extra_links(triggering_exec_date, 'Triggered DAG')",
          "90:         assert mock_build_url.called",
          "91:         args, _ = mock_build_url.call_args",
          "92:         expected_args = {",
          "93:             'dag_id': triggered_dag_run.dag_id,",
          "94:             'base_date': triggered_dag_run.execution_date.isoformat(),",
          "95:         }",
          "96:         assert expected_args in args",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "84:         with create_session() as session:",
          "85:             dagruns = session.query(DagRun).filter(DagRun.dag_id == TRIGGERED_DAG_ID).all()",
          "86:             assert len(dagruns) == 1",
          "89:     def test_trigger_dagrun_custom_run_id(self):",
          "90:         task = TriggerDagRunOperator(",
          "",
          "[Removed Lines]",
          "87:             assert dagruns[0].external_trigger",
          "",
          "[Added Lines]",
          "106:             triggered_dag_run = dagruns[0]",
          "107:             assert triggered_dag_run.external_trigger",
          "108:             self.assert_extra_link(DEFAULT_DATE, triggered_dag_run, task)",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "114:         with create_session() as session:",
          "115:             dagruns = session.query(DagRun).filter(DagRun.dag_id == TRIGGERED_DAG_ID).all()",
          "116:             assert len(dagruns) == 1",
          "120:     def test_trigger_dagrun_twice(self):",
          "121:         \"\"\"Test TriggerDagRunOperator with custom execution_date.\"\"\"",
          "",
          "[Removed Lines]",
          "117:             assert dagruns[0].external_trigger",
          "118:             assert dagruns[0].execution_date == utc_now",
          "",
          "[Added Lines]",
          "138:             triggered_dag_run = dagruns[0]",
          "139:             assert triggered_dag_run.external_trigger",
          "140:             assert triggered_dag_run.execution_date == utc_now",
          "141:             self.assert_extra_link(DEFAULT_DATE, triggered_dag_run, task)",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "140:             )",
          "141:             session.add(dag_run)",
          "142:             session.commit()",
          "145:             dagruns = session.query(DagRun).filter(DagRun.dag_id == TRIGGERED_DAG_ID).all()",
          "146:             assert len(dagruns) == 1",
          "150:     def test_trigger_dagrun_with_templated_execution_date(self):",
          "151:         \"\"\"Test TriggerDagRunOperator with templated execution_date.\"\"\"",
          "",
          "[Removed Lines]",
          "143:             task.execute(None)",
          "147:             assert dagruns[0].external_trigger",
          "148:             assert dagruns[0].execution_date == utc_now",
          "",
          "[Added Lines]",
          "166:             task.run(start_date=DEFAULT_DATE, end_date=DEFAULT_DATE, ignore_ti_state=True)",
          "170:             triggered_dag_run = dagruns[0]",
          "171:             assert triggered_dag_run.external_trigger",
          "172:             assert triggered_dag_run.execution_date == utc_now",
          "173:             self.assert_extra_link(DEFAULT_DATE, triggered_dag_run, task)",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "160:         with create_session() as session:",
          "161:             dagruns = session.query(DagRun).filter(DagRun.dag_id == TRIGGERED_DAG_ID).all()",
          "162:             assert len(dagruns) == 1",
          "166:     def test_trigger_dagrun_operator_conf(self):",
          "167:         \"\"\"Test passing conf to the triggered DagRun.\"\"\"",
          "",
          "[Removed Lines]",
          "163:             assert dagruns[0].external_trigger",
          "164:             assert dagruns[0].execution_date == DEFAULT_DATE",
          "",
          "[Added Lines]",
          "188:             triggered_dag_run = dagruns[0]",
          "189:             assert triggered_dag_run.external_trigger",
          "190:             assert triggered_dag_run.execution_date == DEFAULT_DATE",
          "191:             self.assert_extra_link(DEFAULT_DATE, triggered_dag_run, task)",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "288:                 .all()",
          "289:             )",
          "290:             assert len(dagruns) == 2",
          "293:     def test_trigger_dagrun_triggering_itself_with_execution_date(self):",
          "294:         \"\"\"Test TriggerDagRunOperator that triggers itself with execution date,",
          "",
          "[Removed Lines]",
          "291:             assert dagruns[1].state == State.QUEUED",
          "",
          "[Added Lines]",
          "318:             triggered_dag_run = dagruns[1]",
          "319:             assert triggered_dag_run.state == State.QUEUED",
          "320:             self.assert_extra_link(execution_date, triggered_dag_run, task)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "933716b258841251b47c9c3c6d0152b19971b182",
      "candidate_info": {
        "commit_hash": "933716b258841251b47c9c3c6d0152b19971b182",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/933716b258841251b47c9c3c6d0152b19971b182",
        "files": [
          "setup.py"
        ],
        "message": "Fix: pin pymongo < 4.0.0 (#20511)\n\n(cherry picked from commit f85880e989d7751cfa3ae2d4665d7cc0cb3cc945)",
        "before_after_code_files": [
          "setup.py||setup.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "setup.py||setup.py": [
          "File: setup.py -> setup.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "389: leveldb = ['plyvel']",
          "390: mongo = [",
          "391:     'dnspython>=1.13.0,<3.0.0',",
          "393: ]",
          "394: mssql = [",
          "395:     'pymssql~=2.1,>=2.1.5',",
          "",
          "[Removed Lines]",
          "392:     'pymongo>=3.6.0',",
          "",
          "[Added Lines]",
          "392:     # pymongo 4.0.0 removes connection option `ssl_cert_reqs` which is used in providers-mongo/2.2.0",
          "393:     'pymongo>=3.6.0,<4.0.0',",
          "",
          "---------------"
        ]
      }
    }
  ]
}