{
  "cve_id": "CVE-2022-31777",
  "cve_desc": "A stored cross-site scripting (XSS) vulnerability in Apache Spark 3.2.1 and earlier, and 3.3.0, allows remote attackers to execute arbitrary JavaScript in the web browser of a user, by including a malicious payload into the logs which would be returned in logs rendered in the UI.",
  "repo": "apache/spark",
  "patch_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
  "patch_info": {
    "commit_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
    "repo": "apache/spark",
    "commit_url": "https://github.com/apache/spark/commit/ad90195de56688ce0898691eb9d04297ab0871ad",
    "files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ],
    "message": "[SPARK-39505][UI] Escape log content rendered in UI\n\n### What changes were proposed in this pull request?\n\nEscape log content rendered to the UI.\n\n### Why are the changes needed?\n\nLog content may contain reserved characters or other code in the log and be misinterpreted in the UI as HTML.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nExisting tests\n\nCloses #36902 from srowen/LogViewEscape.\n\nAuthored-by: Sean Owen <srowen@gmail.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
    "before_after_code_files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ]
  },
  "patch_diff": {
    "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js": [
      "File: core/src/main/resources/org/apache/spark/ui/static/log-view.js -> core/src/main/resources/org/apache/spark/ui/static/log-view.js",
      "--- Hunk 1 ---",
      "[Context before]",
      "85:       if (retStartByte == 0) {",
      "86:         disableMoreButton();",
      "87:       }",
      "90:       curLogLength = curLogLength + (startByte - retStartByte);",
      "91:       startByte = retStartByte;",
      "",
      "[Removed Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(cleanData);",
      "",
      "[Added Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(document.createTextNode(cleanData));",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "115:             var retLogLength = dataInfo[2];",
      "117:             var cleanData = data.substring(newlineIndex + 1);",
      "120:             curLogLength = curLogLength + (retEndByte - retStartByte);",
      "121:             endByte = retEndByte;",
      "",
      "[Removed Lines]",
      "118:             $(\"pre\", \".log-content\").append(cleanData);",
      "",
      "[Added Lines]",
      "118:             $(\"pre\", \".log-content\").append(document.createTextNode(cleanData));",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "1695b30e1b2120b52a84f3df2784d6435874867c",
      "candidate_info": {
        "commit_hash": "1695b30e1b2120b52a84f3df2784d6435874867c",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/1695b30e1b2120b52a84f3df2784d6435874867c",
        "files": [
          "python/docs/source/user_guide/sql/arrow_pandas.rst",
          "python/pyspark/pandas/generic.py",
          "python/pyspark/pandas/indexes/base.py",
          "python/pyspark/sql/streaming.py"
        ],
        "message": "[MINOR][DOCS] Remove PySpark doc build warnings\n\nThis PR fixes a various documentation build warnings in PySpark documentation\n\nTo render the docs better.\n\nYes, it changes the documentation to be prettier. Pretty minor though.\n\nI manually tested it by building the PySpark documentation\n\nCloses #36057 from HyukjinKwon/remove-pyspark-build-warnings.\n\nAuthored-by: Hyukjin Kwon <gurwls223@apache.org>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>\n(cherry picked from commit a252c109b32bd3bbb269d6790f0c35e0a4ae705f)\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>",
        "before_after_code_files": [
          "python/pyspark/pandas/generic.py||python/pyspark/pandas/generic.py",
          "python/pyspark/pandas/indexes/base.py||python/pyspark/pandas/indexes/base.py",
          "python/pyspark/sql/streaming.py||python/pyspark/sql/streaming.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "python/pyspark/pandas/generic.py||python/pyspark/pandas/generic.py": [
          "File: python/pyspark/pandas/generic.py -> python/pyspark/pandas/generic.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "952:             This parameter only works when `path` is specified.",
          "954:         Returns",
          "956:         str or None",
          "958:         Examples",
          "",
          "[Removed Lines]",
          "955:         --------",
          "",
          "[Added Lines]",
          "955:         -------",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "2317:         the object does not have exactly 1 element, or that element is not boolean",
          "2319:         Returns",
          "2321:         bool",
          "2323:         Examples",
          "",
          "[Removed Lines]",
          "2320:         --------",
          "",
          "[Added Lines]",
          "2320:         -------",
          "",
          "---------------"
        ],
        "python/pyspark/pandas/indexes/base.py||python/pyspark/pandas/indexes/base.py": [
          "File: python/pyspark/pandas/indexes/base.py -> python/pyspark/pandas/indexes/base.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1312:         \"\"\"",
          "1313:         Return Index if a valid level is given.",
          "1316:         --------",
          "1317:         >>> psidx = ps.Index(['a', 'b', 'c'], name='ks')",
          "1318:         >>> psidx.get_level_values(0)",
          "",
          "[Removed Lines]",
          "1315:         Examples:",
          "",
          "[Added Lines]",
          "1315:         Examples",
          "",
          "---------------"
        ],
        "python/pyspark/sql/streaming.py||python/pyspark/sql/streaming.py": [
          "File: python/pyspark/sql/streaming.py -> python/pyspark/sql/streaming.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "823:             string, for the name of the table.",
          "825:         Returns",
          "827:         :class:`DataFrame`",
          "829:         Notes",
          "",
          "[Removed Lines]",
          "826:         --------",
          "",
          "[Added Lines]",
          "826:         -------",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "82581684c64dcd388d757ebde8ec0b9eb2a30816",
      "candidate_info": {
        "commit_hash": "82581684c64dcd388d757ebde8ec0b9eb2a30816",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/82581684c64dcd388d757ebde8ec0b9eb2a30816",
        "files": [
          "core/src/main/scala/org/apache/spark/network/netty/NettyBlockRpcServer.scala",
          "core/src/test/scala/org/apache/spark/network/netty/NettyBlockRpcServerSuite.scala"
        ],
        "message": "[SPARK-38830][CORE] Warn on corrupted block messages\n\n### What changes were proposed in this pull request?\n\nThis PR aims to warn when `NettyBlockRpcServer` received a corrupted block RPC message or under attack.\n\n- `IllegalArgumentException`: When the type is unknown/invalid when decoding. This fails at Spark layer.\n- `NegativeArraySizeException`: When the size read is negative. This fails at Spark layer during buffer creation.\n- `IndexOutOfBoundsException`: When the data field isn't matched with the size. This fails at Netty later.\n\n### Why are the changes needed?\n\nWhen the RPC messages are corrupted or the servers are under attack, Spark shows `IndexOutOfBoundsException` due to the failure from `Decoder`. Instead of `Exception`, we had better ignore the message with a directional warning message.\n```\njava.lang.IndexOutOfBoundsException:\n    readerIndex(5) + length(602416) exceeds writerIndex(172):\nUnpooledUnsafeDirectByteBuf(ridx: 5, widx: 172, cap: 172/172)\n    at io.netty.buffer.AbstractByteBuf.checkReadableBytes0(AbstractByteBuf.java:1477)\n    at io.netty.buffer.AbstractByteBuf.checkReadableBytes(AbstractByteBuf.java:1463)\n    at io.netty.buffer.UnpooledDirectByteBuf.readBytes(UnpooledDirectByteBuf.java:316)\n    at io.netty.buffer.AbstractByteBuf.readBytes(AbstractByteBuf.java:904)\n    at org.apache.spark.network.protocol.Encoders$Strings.decode(Encoders.java:45)\n    at org.apache.spark.network.shuffle.protocol.UploadBlock.decode(UploadBlock.java:112)\n    at org.apache.spark.network.shuffle.protocol.BlockTransferMessage$Decoder.fromByteBuffer(BlockTransferMessage.java:71)\n    at org.apache.spark.network.netty.NettyBlockRpcServer.receive(NettyBlockRpcServer.scala:53)\n    at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:161)\n    at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:109)\n    at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)\n    at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)\n\n```\n\n### Does this PR introduce _any_ user-facing change?\n\nYes, but this clarify the log messages from exceptions, `IndexOutOfBoundsException`.\n\n### How was this patch tested?\n\nPass the CIs with newly added test suite.\n\nCloses #36116 from dongjoon-hyun/SPARK-38830.\n\nAuthored-by: Dongjoon Hyun <dongjoon@apache.org>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>\n(cherry picked from commit 8ac06474f8cfa8e5619f817aaeea29a77ec8a2a4)\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
        "before_after_code_files": [
          "core/src/main/scala/org/apache/spark/network/netty/NettyBlockRpcServer.scala||core/src/main/scala/org/apache/spark/network/netty/NettyBlockRpcServer.scala",
          "core/src/test/scala/org/apache/spark/network/netty/NettyBlockRpcServerSuite.scala||core/src/test/scala/org/apache/spark/network/netty/NettyBlockRpcServerSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "core/src/main/scala/org/apache/spark/network/netty/NettyBlockRpcServer.scala||core/src/main/scala/org/apache/spark/network/netty/NettyBlockRpcServer.scala": [
          "File: core/src/main/scala/org/apache/spark/network/netty/NettyBlockRpcServer.scala -> core/src/main/scala/org/apache/spark/network/netty/NettyBlockRpcServer.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "50:       client: TransportClient,",
          "51:       rpcMessage: ByteBuffer,",
          "52:       responseContext: RpcResponseCallback): Unit = {",
          "54:     logTrace(s\"Received request: $message\")",
          "56:     message match {",
          "",
          "[Removed Lines]",
          "53:     val message = BlockTransferMessage.Decoder.fromByteBuffer(rpcMessage)",
          "",
          "[Added Lines]",
          "53:     val message = try {",
          "54:       BlockTransferMessage.Decoder.fromByteBuffer(rpcMessage)",
          "55:     } catch {",
          "56:       case e: IllegalArgumentException if e.getMessage.startsWith(\"Unknown message type\") =>",
          "57:         logWarning(s\"This could be a corrupted RPC message (capacity: ${rpcMessage.capacity()}) \" +",
          "58:           s\"from ${client.getSocketAddress}. Please use `spark.authenticate.*` configurations \" +",
          "59:           \"in case of security incidents.\")",
          "60:         throw e",
          "62:       case _: IndexOutOfBoundsException | _: NegativeArraySizeException =>",
          "65:         logWarning(s\"Ignored a corrupted RPC message (capacity: ${rpcMessage.capacity()}) \" +",
          "66:           s\"from ${client.getSocketAddress}. Please use `spark.authenticate.*` configurations \" +",
          "67:           \"in case of security incidents.\")",
          "68:         return",
          "69:     }",
          "",
          "---------------"
        ],
        "core/src/test/scala/org/apache/spark/network/netty/NettyBlockRpcServerSuite.scala||core/src/test/scala/org/apache/spark/network/netty/NettyBlockRpcServerSuite.scala": [
          "File: core/src/test/scala/org/apache/spark/network/netty/NettyBlockRpcServerSuite.scala -> core/src/test/scala/org/apache/spark/network/netty/NettyBlockRpcServerSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "18: package org.apache.spark.network.netty",
          "20: import java.nio.ByteBuffer",
          "22: import org.mockito.Mockito.mock",
          "24: import org.apache.spark.{SparkConf, SparkFunSuite}",
          "25: import org.apache.spark.network.client.TransportClient",
          "26: import org.apache.spark.serializer.JavaSerializer",
          "28: class NettyBlockRpcServerSuite extends SparkFunSuite {",
          "30:   test(\"SPARK-38830: Rethrow IllegalArgumentException due to `Unknown message type`\") {",
          "31:     val serializer = new JavaSerializer(new SparkConf)",
          "32:     val server = new NettyBlockRpcServer(\"enhanced-rpc-server\", serializer, null)",
          "33:     val bytes = Array[Byte](100.toByte)",
          "34:     val message = ByteBuffer.wrap(bytes)",
          "35:     val client = mock(classOf[TransportClient])",
          "36:     val m = intercept[IllegalArgumentException] {",
          "37:       server.receive(client, message)",
          "38:     }.getMessage",
          "39:     assert(m.startsWith(\"Unknown message type: 100\"))",
          "40:   }",
          "42:   test(\"SPARK-38830: Warn and ignore NegativeArraySizeException due to the corruption\") {",
          "43:     val serializer = new JavaSerializer(new SparkConf)",
          "44:     val server = new NettyBlockRpcServer(\"enhanced-rpc-server\", serializer, null)",
          "45:     val bytes = Array[Byte](0.toByte, 0xFF.toByte, 0xFF.toByte, 0xFF.toByte, 0xFF.toByte)",
          "46:     val message = ByteBuffer.wrap(bytes)",
          "47:     val client = mock(classOf[TransportClient])",
          "48:     server.receive(client, message)",
          "49:   }",
          "51:   test(\"SPARK-38830: Warn and ignore IndexOutOfBoundsException due to the corruption\") {",
          "52:     val serializer = new JavaSerializer(new SparkConf)",
          "53:     val server = new NettyBlockRpcServer(\"enhanced-rpc-server\", serializer, null)",
          "54:     val bytes = Array[Byte](1.toByte)",
          "55:     val message = ByteBuffer.wrap(bytes)",
          "56:     val client = mock(classOf[TransportClient])",
          "57:     server.receive(client, message)",
          "58:   }",
          "59: }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "01cdfb4b7858e85a89162435ee176dc64b63b700",
      "candidate_info": {
        "commit_hash": "01cdfb4b7858e85a89162435ee176dc64b63b700",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/01cdfb4b7858e85a89162435ee176dc64b63b700",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/FunctionRegistry.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/TryEval.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/stringExpressions.scala",
          "sql/core/src/test/resources/sql-functions/sql-expression-schema.md",
          "sql/core/src/test/resources/sql-tests/inputs/string-functions.sql",
          "sql/core/src/test/resources/sql-tests/inputs/try-string-functions.sql",
          "sql/core/src/test/resources/sql-tests/results/ansi/string-functions.sql.out",
          "sql/core/src/test/resources/sql-tests/results/string-functions.sql.out",
          "sql/core/src/test/resources/sql-tests/results/try-string-functions.sql.out"
        ],
        "message": "[SPARK-38590][SQL] New SQL function: try_to_binary\n\n### What changes were proposed in this pull request?\n\nAdd a new SQL function: `try_to_binary`. It is identical to the function `to_binary`, except that it returns NULL results instead of throwing an exception on encoding errors.\nThere is a similar function in Snowflake: https://docs.snowflake.com/en/sql-reference/functions/try_to_binary.html\n\n### Why are the changes needed?\n\nUsers can manage to finish queries without interruptions by encoding errors.\n\n### Does this PR introduce _any_ user-facing change?\n\nYes, adding a new SQL function: `try_to_binary`. It is identical to the function `to_binary`, except that it returns NULL results instead of throwing an exception on encoding errors.\n\n### How was this patch tested?\n\nUT\n\nCloses #35897 from gengliangwang/try_to_binary.\n\nAuthored-by: Gengliang Wang <gengliang@apache.org>\nSigned-off-by: Max Gekk <max.gekk@gmail.com>\n(cherry picked from commit becda3339381b3975ed567c156260eda036d7a1b)\nSigned-off-by: Max Gekk <max.gekk@gmail.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/FunctionRegistry.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/FunctionRegistry.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/TryEval.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/TryEval.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/stringExpressions.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/stringExpressions.scala",
          "sql/core/src/test/resources/sql-tests/inputs/string-functions.sql||sql/core/src/test/resources/sql-tests/inputs/string-functions.sql",
          "sql/core/src/test/resources/sql-tests/inputs/try-string-functions.sql||sql/core/src/test/resources/sql-tests/inputs/try-string-functions.sql"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/FunctionRegistry.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/FunctionRegistry.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/FunctionRegistry.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/FunctionRegistry.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "454:     expression[TryMultiply](\"try_multiply\"),",
          "455:     expression[TryElementAt](\"try_element_at\"),",
          "456:     expression[TrySum](\"try_sum\"),",
          "459:     expression[HyperLogLogPlusPlus](\"approx_count_distinct\"),",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "457:     expression[TryToBinary](\"try_to_binary\"),",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/TryEval.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/TryEval.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/TryEval.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/TryEval.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "181:   override protected def withNewChildInternal(newChild: Expression): Expression =",
          "182:     this.copy(replacement = newChild)",
          "183: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "186: @ExpressionDescription(",
          "187:   usage = \"_FUNC_(str[, fmt]) - This is a special version of `to_binary` that performs the same operation, but returns a NULL value instead of raising an error if the conversion cannot be performed.\",",
          "188:   examples = \"\"\"",
          "189:     Examples:",
          "190:       > SELECT _FUNC_('abc', 'utf-8');",
          "191:        abc",
          "192:       > select _FUNC_('a!', 'base64');",
          "193:        NULL",
          "194:       > select _FUNC_('abc', 'invalidFormat');",
          "195:        NULL",
          "196:   \"\"\",",
          "197:   since = \"3.3.0\",",
          "198:   group = \"string_funcs\")",
          "200: case class TryToBinary(",
          "201:     expr: Expression,",
          "202:     format: Option[Expression],",
          "203:     replacement: Expression) extends RuntimeReplaceable",
          "204:   with InheritAnalysisRules {",
          "205:   def this(expr: Expression) =",
          "206:     this(expr, None, TryEval(ToBinary(expr, None, nullOnInvalidFormat = true)))",
          "208:   def this(expr: Expression, formatExpression: Expression) =",
          "209:     this(expr, Some(formatExpression),",
          "210:       TryEval(ToBinary(expr, Some(formatExpression), nullOnInvalidFormat = true)))",
          "212:   override def prettyName: String = \"try_to_binary\"",
          "214:   override def parameters: Seq[Expression] = expr +: format.toSeq",
          "216:   override protected def withNewChildInternal(newChild: Expression): Expression =",
          "217:     this.copy(replacement = newChild)",
          "218: }",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/stringExpressions.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/stringExpressions.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/stringExpressions.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/stringExpressions.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "2638:   since = \"3.3.0\",",
          "2639:   group = \"string_funcs\")",
          "2642:   with ImplicitCastInputTypes {",
          "2644:   override lazy val replacement: Expression = format.map { f =>",
          "",
          "[Removed Lines]",
          "2641: case class ToBinary(expr: Expression, format: Option[Expression]) extends RuntimeReplaceable",
          "",
          "[Added Lines]",
          "2641: case class ToBinary(",
          "2642:     expr: Expression,",
          "2643:     format: Option[Expression],",
          "2644:     nullOnInvalidFormat: Boolean = false) extends RuntimeReplaceable",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "2651:         case \"hex\" => Unhex(expr)",
          "2652:         case \"utf-8\" => Encode(expr, Literal(\"UTF-8\"))",
          "2653:         case \"base64\" => UnBase64(expr)",
          "2654:         case other => throw QueryCompilationErrors.invalidStringLiteralParameter(",
          "2655:           \"to_binary\", \"format\", other,",
          "2656:           Some(\"The value has to be a case-insensitive string literal of \" +",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "2657:         case _ if nullOnInvalidFormat => Literal(null, BinaryType)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "2659:     }",
          "2660:   }.getOrElse(Unhex(expr))",
          "2664:   def this(expr: Expression, format: Expression) = this(expr, Some({",
          "2673:   override def prettyName: String = \"to_binary\"",
          "",
          "[Removed Lines]",
          "2662:   def this(expr: Expression) = this(expr, None)",
          "2666:     if (format.foldable && (format.dataType == StringType || format.dataType == NullType)) {",
          "2667:       format",
          "2668:     } else {",
          "2669:       throw QueryCompilationErrors.requireLiteralParameter(\"to_binary\", \"format\", \"string\")",
          "2670:     }",
          "2671:   }))",
          "",
          "[Added Lines]",
          "2666:   def this(expr: Expression) = this(expr, None, false)",
          "2670:       if (format.foldable && (format.dataType == StringType || format.dataType == NullType)) {",
          "2671:         format",
          "2672:       } else {",
          "2673:         throw QueryCompilationErrors.requireLiteralParameter(\"to_binary\", \"format\", \"string\")",
          "2674:       }",
          "2675:     }),",
          "2676:     false",
          "2677:     )",
          "",
          "---------------"
        ],
        "sql/core/src/test/resources/sql-tests/inputs/string-functions.sql||sql/core/src/test/resources/sql-tests/inputs/string-functions.sql": [
          "File: sql/core/src/test/resources/sql-tests/inputs/string-functions.sql -> sql/core/src/test/resources/sql-tests/inputs/string-functions.sql",
          "--- Hunk 1 ---",
          "[Context before]",
          "184: -- 'format' parameter must be string type or void type.",
          "185: select to_binary(null, cast(null as int));",
          "186: select to_binary('abc', 1);",
          "188: select to_binary('abc', 'invalidFormat');",
          "",
          "[Removed Lines]",
          "187: -- invalid inputs.",
          "",
          "[Added Lines]",
          "187: -- invalid format",
          "189: -- invalid string input",
          "190: select to_binary('a!', 'base64');",
          "",
          "---------------"
        ],
        "sql/core/src/test/resources/sql-tests/inputs/try-string-functions.sql||sql/core/src/test/resources/sql-tests/inputs/try-string-functions.sql": [
          "File: sql/core/src/test/resources/sql-tests/inputs/try-string-functions.sql -> sql/core/src/test/resources/sql-tests/inputs/try-string-functions.sql",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: -- try_to_binary",
          "2: select try_to_binary('abc');",
          "3: select try_to_binary('abc', 'utf-8');",
          "4: select try_to_binary('abc', 'base64');",
          "5: select try_to_binary('abc', 'hex');",
          "6: -- 'format' parameter can be any foldable string value, not just literal.",
          "7: select try_to_binary('abc', concat('utf', '-8'));",
          "8: -- 'format' parameter is case insensitive.",
          "9: select try_to_binary('abc', 'Hex');",
          "10: -- null inputs lead to null result.",
          "11: select try_to_binary('abc', null);",
          "12: select try_to_binary(null, 'utf-8');",
          "13: select try_to_binary(null, null);",
          "14: select try_to_binary(null, cast(null as string));",
          "15: -- 'format' parameter must be string type or void type.",
          "16: select try_to_binary(null, cast(null as int));",
          "17: select try_to_binary('abc', 1);",
          "18: -- invalid format",
          "19: select try_to_binary('abc', 'invalidFormat');",
          "20: -- invalid string input",
          "21: select try_to_binary('a!', 'base64');",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "c9710c50290be58ab5a044afe76c73c81f84b0a7",
      "candidate_info": {
        "commit_hash": "c9710c50290be58ab5a044afe76c73c81f84b0a7",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/c9710c50290be58ab5a044afe76c73c81f84b0a7",
        "files": [
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CollectionExpressionsSuite.scala"
        ],
        "message": "[SPARK-40152][SQL][TESTS][FOLLOW-UP][3.3] Capture a different error message in 3.3\n\n### What changes were proposed in this pull request?\n\nThis PR fixes the error message in branch-3.3. Different error message is thrown at the test added in https://github.com/apache/spark/commit/4b0c3bab1ab082565a051990bf45774f15962deb.\n\n### Why are the changes needed?\n\n`branch-3.3` is broken because of the different error message being thrown (https://github.com/apache/spark/runs/8065373173?check_suite_focus=true).\n\n```\n[info] - elementAt *** FAILED *** (996 milliseconds)\n[info]   (non-codegen mode) Expected error message is `The index 0 is invalid`, but `SQL array indices start at 1` found (ExpressionEvalHelper.scala:176)\n[info]   org.scalatest.exceptions.TestFailedException:\n[info]   at org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:472)\n[info]   at org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:471)\n[info]   at org.scalatest.funsuite.AnyFunSuite.newAssertionFailedException(AnyFunSuite.scala:1563)\n[info]   at org.scalatest.Assertions.fail(Assertions.scala:933)\n[info]   at org.scalatest.Assertions.fail$(Assertions.scala:929)\n[info]   at org.scalatest.funsuite.AnyFunSuite.fail(AnyFunSuite.scala:1563)\n[info]   at org.apache.spark.sql.catalyst.expressions.ExpressionEvalHelper.$anonfun$checkExceptionInExpression$1(ExpressionEvalHelper.scala:176)\n[info]   at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n[info]   at org.scalatest.Assertions.withClue(Assertions.scala:1065)\n[info]   at org.scalatest.Assertions.withClue$(Assertions.scala:1052)\n[info]   at org.scalatest.funsuite.AnyFunSuite.withClue(AnyFunSuite.scala:1563)\n[info]   at org.apache.spark.sql.catalyst.expressions.ExpressionEvalHelper.checkException$1(ExpressionEvalHelper.scala:163)\n[info]   at org.apache.spark.sql.catalyst.expressions.ExpressionEvalHelper.checkExceptionInExpression(ExpressionEvalHelper.scala:183)\n[info]   at org.apache.spark.sql.catalyst.expressions.ExpressionEvalHelper.checkExceptionInExpression$(ExpressionEvalHelper.scala:156)\n[info]   at org.apache.spark.sql.catalyst.expressions.CollectionExpressionsSuite.checkExceptionInExpression(CollectionExpressionsSuite.scala:39)\n[info]   at org.apache.spark.sql.catalyst.expressions.ExpressionEvalHelper.checkExceptionInExpression(ExpressionEvalHelper.scala:153)\n[info]   at org.apache.spark.sql.catalyst.expressions.ExpressionEvalHelper.checkExceptionInExpression$(ExpressionEvalHelper.scala:150)\n[info]   at org.apache.spark.sql.catalyst.expressions.CollectionExpressionsSuite.checkExceptionInExpression(CollectionExpressionsSuite.scala:39)\n[info]   at org.apache.spark.sql.catalyst.expressions.CollectionExpressionsSuite.$anonfun$new$365(CollectionExpressionsSuite.scala:1555)\n[info]   at org.apache.spark.sql.catalyst.plans.SQLHelper.withSQLConf(SQLHelper.scala:54)\n[info]   at org.apache.spark.sql.catalyst.plans.SQLHelper.withSQLConf$(SQLHelper.scala:38)\n```\n\n### Does this PR introduce _any_ user-facing change?\n\nNo, test-only.\n\n### How was this patch tested?\n\nCI in this PR should test it out.\n\nCloses #37708 from HyukjinKwon/SPARK-40152-3.3.\n\nAuthored-by: Hyukjin Kwon <gurwls223@apache.org>\nSigned-off-by: Yuming Wang <yumwang@ebay.com>",
        "before_after_code_files": [
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CollectionExpressionsSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CollectionExpressionsSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CollectionExpressionsSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CollectionExpressionsSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CollectionExpressionsSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CollectionExpressionsSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "1553:         Literal.create(null, StringType)), Literal(1), outOfBoundValue), null)",
          "1555:       checkExceptionInExpression[Exception](",
          "1557:     }",
          "1558:   }",
          "",
          "[Removed Lines]",
          "1556:         ElementAt(str, Literal(0), outOfBoundValue), \"The index 0 is invalid\")",
          "",
          "[Added Lines]",
          "1556:         ElementAt(str, Literal(0), outOfBoundValue), \"SQL array indices start at 1\")",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "386c75693b5b9dd5e3b2147d49f0284badaa7d6d",
      "candidate_info": {
        "commit_hash": "386c75693b5b9dd5e3b2147d49f0284badaa7d6d",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/386c75693b5b9dd5e3b2147d49f0284badaa7d6d",
        "files": [
          "python/pyspark/pandas/generic.py",
          "python/pyspark/pandas/tests/test_stats.py"
        ],
        "message": "[SPARK-39186][PYTHON] Make pandas-on-Spark's skew consistent with pandas\n\n### What changes were proposed in this pull request?\n\nthe logics of computing skewness are different between spark sql and pandas:\n\nspark sql:   [`sqrt(n) * m3 / sqrt(m2 * m2 * m2))`](https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/CentralMomentAgg.scala#L304)\n\npandas: [`(count * (count - 1) ** 0.5 / (count - 2)) * (m3 / m2**1.5)`](https://github.com/pandas-dev/pandas/blob/main/pandas/core/nanops.py#L1221)\n\n### Why are the changes needed?\n\nto make skew consistent with pandas\n\n### Does this PR introduce _any_ user-facing change?\nyes, the logic to compute skew was changed\n\n### How was this patch tested?\nadded UT\n\nCloses #36549 from zhengruifeng/adjust_pandas_skew.\n\nAuthored-by: Ruifeng Zheng <ruifengz@apache.org>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>\n(cherry picked from commit 7e4519c9a8ba35958ef6d408be3ca4e97917c965)\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>",
        "before_after_code_files": [
          "python/pyspark/pandas/generic.py||python/pyspark/pandas/generic.py",
          "python/pyspark/pandas/tests/test_stats.py||python/pyspark/pandas/tests/test_stats.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "python/pyspark/pandas/generic.py||python/pyspark/pandas/generic.py": [
          "File: python/pyspark/pandas/generic.py -> python/pyspark/pandas/generic.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1468:                         spark_type_to_pandas_dtype(spark_type), spark_type.simpleString()",
          "1469:                     )",
          "1470:                 )",
          "1473:         return self._reduce_for_stat_function(",
          "1474:             skew, name=\"skew\", axis=axis, numeric_only=numeric_only",
          "",
          "[Removed Lines]",
          "1471:             return F.skewness(spark_column)",
          "",
          "[Added Lines]",
          "1472:             count_scol = F.count(F.when(~spark_column.isNull(), 1).otherwise(None))",
          "1473:             # refer to the Pandas implementation 'nanskew'",
          "1474:             # https://github.com/pandas-dev/pandas/blob/main/pandas/core/nanops.py#L1152",
          "1475:             return F.when(",
          "1476:                 count_scol > 2,",
          "1477:                 F.skewness(spark_column)",
          "1480:             ).otherwise(None)",
          "",
          "---------------"
        ],
        "python/pyspark/pandas/tests/test_stats.py||python/pyspark/pandas/tests/test_stats.py": [
          "File: python/pyspark/pandas/tests/test_stats.py -> python/pyspark/pandas/tests/test_stats.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "181:             self.assert_eq(psdf.sum(axis=1), pdf.sum(axis=1))",
          "182:             self.assert_eq(psdf.product(axis=1), pdf.product(axis=1))",
          "183:             self.assert_eq(psdf.kurtosis(axis=1), pdf.kurtosis(axis=1))",
          "184:             self.assert_eq(psdf.skew(axis=1), pdf.skew(axis=1))",
          "185:             self.assert_eq(psdf.mean(axis=1), pdf.mean(axis=1))",
          "186:             self.assert_eq(psdf.sem(axis=1), pdf.sem(axis=1))",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "184:             self.assert_eq(psdf.skew(axis=0), pdf.skew(axis=0), almost=True)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "218:             self.assert_eq(",
          "219:                 psdf.kurtosis(axis=1, numeric_only=True), pdf.kurtosis(axis=1, numeric_only=True)",
          "220:             )",
          "221:             self.assert_eq(",
          "222:                 psdf.skew(axis=1, numeric_only=True), pdf.skew(axis=1, numeric_only=True)",
          "223:             )",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "222:             self.assert_eq(",
          "223:                 psdf.skew(axis=0, numeric_only=True),",
          "224:                 pdf.skew(axis=0, numeric_only=True),",
          "225:                 almost=True,",
          "226:             )",
          "",
          "---------------"
        ]
      }
    }
  ]
}