{
  "cve_id": "CVE-2022-31777",
  "cve_desc": "A stored cross-site scripting (XSS) vulnerability in Apache Spark 3.2.1 and earlier, and 3.3.0, allows remote attackers to execute arbitrary JavaScript in the web browser of a user, by including a malicious payload into the logs which would be returned in logs rendered in the UI.",
  "repo": "apache/spark",
  "patch_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
  "patch_info": {
    "commit_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
    "repo": "apache/spark",
    "commit_url": "https://github.com/apache/spark/commit/ad90195de56688ce0898691eb9d04297ab0871ad",
    "files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ],
    "message": "[SPARK-39505][UI] Escape log content rendered in UI\n\n### What changes were proposed in this pull request?\n\nEscape log content rendered to the UI.\n\n### Why are the changes needed?\n\nLog content may contain reserved characters or other code in the log and be misinterpreted in the UI as HTML.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nExisting tests\n\nCloses #36902 from srowen/LogViewEscape.\n\nAuthored-by: Sean Owen <srowen@gmail.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
    "before_after_code_files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ]
  },
  "patch_diff": {
    "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js": [
      "File: core/src/main/resources/org/apache/spark/ui/static/log-view.js -> core/src/main/resources/org/apache/spark/ui/static/log-view.js",
      "--- Hunk 1 ---",
      "[Context before]",
      "85:       if (retStartByte == 0) {",
      "86:         disableMoreButton();",
      "87:       }",
      "90:       curLogLength = curLogLength + (startByte - retStartByte);",
      "91:       startByte = retStartByte;",
      "",
      "[Removed Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(cleanData);",
      "",
      "[Added Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(document.createTextNode(cleanData));",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "115:             var retLogLength = dataInfo[2];",
      "117:             var cleanData = data.substring(newlineIndex + 1);",
      "120:             curLogLength = curLogLength + (retEndByte - retStartByte);",
      "121:             endByte = retEndByte;",
      "",
      "[Removed Lines]",
      "118:             $(\"pre\", \".log-content\").append(cleanData);",
      "",
      "[Added Lines]",
      "118:             $(\"pre\", \".log-content\").append(document.createTextNode(cleanData));",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "87f957dea86fe1b8c5979e499b5400866b235e43",
      "candidate_info": {
        "commit_hash": "87f957dea86fe1b8c5979e499b5400866b235e43",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/87f957dea86fe1b8c5979e499b5400866b235e43",
        "files": [
          "python/pyspark/ml/tests/test_persistence.py",
          "python/pyspark/ml/wrapper.py"
        ],
        "message": "[SPARK-35542][ML] Fix: Bucketizer created for multiple columns with parameters splitsArray,  inputCols and outputCols can not be loaded after saving it\n\nSigned-off-by: Weichen Xu <weichen.xudatabricks.com>\n\n### What changes were proposed in this pull request?\nFix: Bucketizer created for multiple columns with parameters splitsArray,  inputCols and outputCols can not be loaded after saving it\n\n### Why are the changes needed?\nBugfix.\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\nUnit test\n\nCloses #37568 from WeichenXu123/SPARK-35542.\n\nAuthored-by: Weichen Xu <weichen.xu@databricks.com>\nSigned-off-by: Weichen Xu <weichen.xu@databricks.com>\n(cherry picked from commit 876ce6a5df118095de51c3c4789d6db6da95eb23)\nSigned-off-by: Weichen Xu <weichen.xu@databricks.com>",
        "before_after_code_files": [
          "python/pyspark/ml/tests/test_persistence.py||python/pyspark/ml/tests/test_persistence.py",
          "python/pyspark/ml/wrapper.py||python/pyspark/ml/wrapper.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "python/pyspark/ml/tests/test_persistence.py||python/pyspark/ml/tests/test_persistence.py": [
          "File: python/pyspark/ml/tests/test_persistence.py -> python/pyspark/ml/tests/test_persistence.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "32:     OneVsRestModel,",
          "33: )",
          "34: from pyspark.ml.clustering import KMeans",
          "36: from pyspark.ml.linalg import Vectors",
          "37: from pyspark.ml.param import Params",
          "38: from pyspark.ml.pipeline import Pipeline, PipelineModel",
          "",
          "[Removed Lines]",
          "35: from pyspark.ml.feature import Binarizer, HashingTF, PCA",
          "",
          "[Added Lines]",
          "35: from pyspark.ml.feature import Binarizer, Bucketizer, HashingTF, PCA",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "518:         )",
          "519:         reader.getAndSetParams(lr, loadedMetadata)",
          "522: if __name__ == \"__main__\":",
          "523:     from pyspark.ml.tests.test_persistence import *  # noqa: F401",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "521:     # Test for SPARK-35542 fix.",
          "522:     def test_save_and_load_on_nested_list_params(self):",
          "523:         temp_path = tempfile.mkdtemp()",
          "524:         splitsArray = [",
          "525:             [-float(\"inf\"), 0.5, 1.4, float(\"inf\")],",
          "526:             [-float(\"inf\"), 0.1, 1.2, float(\"inf\")],",
          "527:         ]",
          "528:         bucketizer = Bucketizer(",
          "529:             splitsArray=splitsArray, inputCols=[\"values\", \"values\"], outputCols=[\"b1\", \"b2\"]",
          "530:         )",
          "531:         savePath = temp_path + \"/bk\"",
          "532:         bucketizer.write().overwrite().save(savePath)",
          "533:         loadedBucketizer = Bucketizer.load(savePath)",
          "534:         assert loadedBucketizer.getSplitsArray() == splitsArray",
          "",
          "---------------"
        ],
        "python/pyspark/ml/wrapper.py||python/pyspark/ml/wrapper.py": [
          "File: python/pyspark/ml/wrapper.py -> python/pyspark/ml/wrapper.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "220:                 java_param = self._java_obj.getParam(param.name)",
          "221:                 # SPARK-14931: Only check set params back to avoid default params mismatch.",
          "222:                 if self._java_obj.isSet(java_param):",
          "224:                     self._set(**{param.name: value})",
          "225:                 # SPARK-10931: Temporary fix for params that have a default in Java",
          "226:                 if self._java_obj.hasDefault(java_param) and not self.isDefined(param):",
          "",
          "[Removed Lines]",
          "223:                     value = _java2py(sc, self._java_obj.getOrDefault(java_param))",
          "",
          "[Added Lines]",
          "223:                     java_value = self._java_obj.getOrDefault(java_param)",
          "224:                     if param.typeConverter.__name__.startswith(\"toList\"):",
          "225:                         value = [_java2py(sc, x) for x in list(java_value)]",
          "226:                     else:",
          "227:                         value = _java2py(sc, java_value)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "61d22b6f313c20de1b65a595e88b6f5bd9595299",
      "candidate_info": {
        "commit_hash": "61d22b6f313c20de1b65a595e88b6f5bd9595299",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/61d22b6f313c20de1b65a595e88b6f5bd9595299",
        "files": [
          "core/src/main/scala/org/apache/spark/errors/SparkCoreErrors.scala",
          "core/src/main/scala/org/apache/spark/storage/BlockSavedOnDecommissionedBlockManagerException.scala",
          "launcher/src/main/java/org/apache/spark/launcher/AbstractLauncher.java",
          "launcher/src/main/java/org/apache/spark/launcher/InProcessLauncher.java",
          "launcher/src/main/java/org/apache/spark/launcher/JavaModuleOptions.java"
        ],
        "message": "[SPARK-39371][DOCS][CORE] Review and fix issues in Scala/Java API docs of Core module\n\nCompare the 3.3.0 API doc with the latest release version 3.2.1. Fix the following issues:\n\n* Add missing Since annotation for new APIs\n* Remove the leaking class/object in API doc\n\nImprove API docs\n\nNo\n\nExisting UT\n\nCloses #36757 from xuanyuanking/doc.\n\nAuthored-by: Yuanjian Li <yuanjian.li@databricks.com>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>\n(cherry picked from commit 1fbb1d46feb992c3441f2a4f2c5d5179da465d4b)\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>",
        "before_after_code_files": [
          "core/src/main/scala/org/apache/spark/errors/SparkCoreErrors.scala||core/src/main/scala/org/apache/spark/errors/SparkCoreErrors.scala",
          "core/src/main/scala/org/apache/spark/storage/BlockSavedOnDecommissionedBlockManagerException.scala||core/src/main/scala/org/apache/spark/storage/BlockSavedOnDecommissionedBlockManagerException.scala",
          "launcher/src/main/java/org/apache/spark/launcher/AbstractLauncher.java||launcher/src/main/java/org/apache/spark/launcher/AbstractLauncher.java",
          "launcher/src/main/java/org/apache/spark/launcher/InProcessLauncher.java||launcher/src/main/java/org/apache/spark/launcher/InProcessLauncher.java",
          "launcher/src/main/java/org/apache/spark/launcher/JavaModuleOptions.java||launcher/src/main/java/org/apache/spark/launcher/JavaModuleOptions.java"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "core/src/main/scala/org/apache/spark/errors/SparkCoreErrors.scala||core/src/main/scala/org/apache/spark/errors/SparkCoreErrors.scala": [
          "File: core/src/main/scala/org/apache/spark/errors/SparkCoreErrors.scala -> core/src/main/scala/org/apache/spark/errors/SparkCoreErrors.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "34:   def unexpectedPy4JServerError(other: Object): Throwable = {",
          "35:     new RuntimeException(s\"Unexpected Py4J server ${other.getClass}\")",
          "36:   }",
          "",
          "[Removed Lines]",
          "33: object SparkCoreErrors {",
          "",
          "[Added Lines]",
          "33: private[spark] object SparkCoreErrors {",
          "",
          "---------------"
        ],
        "core/src/main/scala/org/apache/spark/storage/BlockSavedOnDecommissionedBlockManagerException.scala||core/src/main/scala/org/apache/spark/storage/BlockSavedOnDecommissionedBlockManagerException.scala": [
          "File: core/src/main/scala/org/apache/spark/storage/BlockSavedOnDecommissionedBlockManagerException.scala -> core/src/main/scala/org/apache/spark/storage/BlockSavedOnDecommissionedBlockManagerException.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: package org.apache.spark.storage",
          "21:   extends Exception(s\"Block $blockId cannot be saved on decommissioned executor\")",
          "",
          "[Removed Lines]",
          "20: class BlockSavedOnDecommissionedBlockManagerException(blockId: BlockId)",
          "",
          "[Added Lines]",
          "20: private[spark] class BlockSavedOnDecommissionedBlockManagerException(blockId: BlockId)",
          "",
          "---------------"
        ],
        "launcher/src/main/java/org/apache/spark/launcher/AbstractLauncher.java||launcher/src/main/java/org/apache/spark/launcher/AbstractLauncher.java": [
          "File: launcher/src/main/java/org/apache/spark/launcher/AbstractLauncher.java -> launcher/src/main/java/org/apache/spark/launcher/AbstractLauncher.java"
        ],
        "launcher/src/main/java/org/apache/spark/launcher/InProcessLauncher.java||launcher/src/main/java/org/apache/spark/launcher/InProcessLauncher.java": [
          "File: launcher/src/main/java/org/apache/spark/launcher/InProcessLauncher.java -> launcher/src/main/java/org/apache/spark/launcher/InProcessLauncher.java"
        ],
        "launcher/src/main/java/org/apache/spark/launcher/JavaModuleOptions.java||launcher/src/main/java/org/apache/spark/launcher/JavaModuleOptions.java": [
          "File: launcher/src/main/java/org/apache/spark/launcher/JavaModuleOptions.java -> launcher/src/main/java/org/apache/spark/launcher/JavaModuleOptions.java"
        ]
      }
    },
    {
      "candidate_hash": "bb1a523a399b8e29b7543e615c0c884d4bf76215",
      "candidate_info": {
        "commit_hash": "bb1a523a399b8e29b7543e615c0c884d4bf76215",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/bb1a523a399b8e29b7543e615c0c884d4bf76215",
        "files": [
          "core/src/main/resources/error/error-classes.json",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryErrorsBase.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryParsingErrors.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/DDLParserSuite.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/PlanParserSuite.scala",
          "sql/core/src/test/resources/sql-tests/results/window.sql.out",
          "sql/core/src/test/scala/org/apache/spark/sql/SQLInsertTestSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryCompilationErrorsDSv2Suite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionErrorsSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryParsingErrorsSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/command/DDLParserSuite.scala"
        ],
        "message": "[SPARK-38913][SQL][3.3] Output identifiers in error messages in SQL style\n\n### What changes were proposed in this pull request?\nIn the PR, I propose to use backticks to wrap SQL identifiers in error messages. I added new util functions `toSQLId()` to the trait `QueryErrorsBase`, and applied it in `Query.*Errors` (also modified tests in `Query.*ErrorsSuite`). For example:\n\nBefore:\n```sql\nInvalid SQL syntax: The definition of window win is repetitive.\n```\n\nAfter:\n```\nInvalid SQL syntax: The definition of window `win` is repetitive.\n```\n\n### Why are the changes needed?\nTo improve user experience with Spark SQL. The changes highlight SQL identifiers in error massages and make them more visible for users.\n\n### Does this PR introduce _any_ user-facing change?\nNo since error classes haven't been released yet.\n\n### How was this patch tested?\nBy running the affected test suites:\n```\n$ build/sbt \"test:testOnly *QueryParsingErrorsSuite\"\n$ build/sbt \"test:testOnly *QueryCompilationErrorsSuite\"\n$ build/sbt \"test:testOnly *QueryCompilationErrorsDSv2Suite\"\n$ build/sbt \"test:testOnly *QueryExecutionErrorsSuite\"\n$ build/sbt \"testOnly *PlanParserSuite\"\n$ build/sbt \"testOnly *DDLParserSuite\"\n$ build/sbt -Phive-2.3 \"testOnly *HiveSQLInsertTestSuite\"\n$ build/sbt \"sql/testOnly org.apache.spark.sql.SQLQueryTestSuite -- -z window.sql\"\n$ build/sbt \"testOnly *DSV2SQLInsertTestSuite\"\n```\n\nAuthored-by: Max Gekk <max.gekkgmail.com>\nSigned-off-by: Max Gekk <max.gekkgmail.com>\n(cherry picked from commit 2ff6914e6bac053231825c083fd508726a11a349)\n\nCloses #36288 from MaxGekk/error-class-toSQLId-3.3.\n\nAuthored-by: Max Gekk <max.gekk@gmail.com>\nSigned-off-by: Max Gekk <max.gekk@gmail.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryErrorsBase.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryErrorsBase.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryParsingErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryParsingErrors.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/DDLParserSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/DDLParserSuite.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/PlanParserSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/PlanParserSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/SQLInsertTestSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/SQLInsertTestSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryCompilationErrorsDSv2Suite.scala||sql/core/src/test/scala/org/apache/spark/sql/errors/QueryCompilationErrorsDSv2Suite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionErrorsSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionErrorsSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryParsingErrorsSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/errors/QueryParsingErrorsSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/command/DDLParserSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/command/DDLParserSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "95:     new AnalysisException(",
          "96:       errorClass = \"UNSUPPORTED_FEATURE\",",
          "97:       messageParameters = Array(",
          "99:         s\"by ${toSQLStmt(\"INSERT INTO\")}.\"))",
          "100:   }",
          "102:   def nonPartitionColError(partitionName: String): Throwable = {",
          "103:     new AnalysisException(",
          "104:       errorClass = \"NON_PARTITION_COLUMN\",",
          "106:   }",
          "108:   def missingStaticPartitionColumn(staticName: String): Throwable = {",
          "",
          "[Removed Lines]",
          "98:         s\"${toSQLStmt(\"IF NOT EXISTS\")} for the table '$tableName' \" +",
          "105:       messageParameters = Array(partitionName))",
          "",
          "[Added Lines]",
          "98:         s\"${toSQLStmt(\"IF NOT EXISTS\")} for the table ${toSQLId(tableName)} \" +",
          "105:       messageParameters = Array(toSQLId(partitionName)))",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryErrorsBase.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryErrorsBase.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryErrorsBase.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryErrorsBase.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: import java.util.Locale",
          "22: import org.apache.spark.sql.catalyst.expressions.Literal",
          "23: import org.apache.spark.sql.types.{DataType, DoubleType, FloatType}",
          "25: trait QueryErrorsBase {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "23: import org.apache.spark.sql.catalyst.util.quoteIdentifier",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "52:     \"\\\"\" + text.toUpperCase(Locale.ROOT) + \"\\\"\"",
          "53:   }",
          "55:   def toSQLType(t: DataType): String = {",
          "56:     t.sql",
          "57:   }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "56:   def toSQLId(parts: Seq[String]): String = {",
          "57:     parts.map(quoteIdentifier).mkString(\".\")",
          "58:   }",
          "60:   def toSQLId(parts: String): String = {",
          "61:     toSQLId(parts.split(\"\\\\.\"))",
          "62:   }",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "1936:       messageParameters = Array(s\"${toSQLStmt(\"pivot\")} not after a ${toSQLStmt(\"group by\")}.\"))",
          "1937:   }",
          "1939:   def invalidAesKeyLengthError(actualLength: Int): RuntimeException = {",
          "1940:     new SparkRuntimeException(",
          "1941:       errorClass = \"INVALID_PARAMETER_VALUE\",",
          "1942:       messageParameters = Array(",
          "1943:         \"key\",",
          "1945:         s\"expects a binary value with 16, 24 or 32 bytes, but got ${actualLength.toString} bytes.\"))",
          "1946:   }",
          "",
          "[Removed Lines]",
          "1944:         \"the aes_encrypt/aes_decrypt function\",",
          "",
          "[Added Lines]",
          "1939:   private val aesFuncName = toSQLId(\"aes_encrypt\") + \"/\" + toSQLId(\"aes_decrypt\")",
          "1946:         s\"the $aesFuncName function\",",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1949:     new SparkRuntimeException(",
          "1950:       errorClass = \"UNSUPPORTED_FEATURE\",",
          "1951:       messageParameters = Array(",
          "1953:   }",
          "1955:   def aesCryptoError(detailMessage: String): RuntimeException = {",
          "",
          "[Removed Lines]",
          "1952:         s\"AES-$mode with the padding $padding by the aes_encrypt/aes_decrypt function.\"))",
          "",
          "[Added Lines]",
          "1954:         s\"AES-$mode with the padding $padding by the $aesFuncName function.\"))",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "1957:       errorClass = \"INVALID_PARAMETER_VALUE\",",
          "1958:       messageParameters = Array(",
          "1959:         \"expr, key\",",
          "1961:         s\"Detail message: $detailMessage\"))",
          "1962:   }",
          "",
          "[Removed Lines]",
          "1960:         \"the aes_encrypt/aes_decrypt function\",",
          "",
          "[Added Lines]",
          "1962:         s\"the $aesFuncName function\",",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryParsingErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryParsingErrors.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryParsingErrors.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryParsingErrors.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "141:   def repetitiveWindowDefinitionError(name: String, ctx: WindowClauseContext): Throwable = {",
          "142:     new ParseException(\"INVALID_SQL_SYNTAX\",",
          "144:   }",
          "146:   def invalidWindowReferenceError(name: String, ctx: WindowClauseContext): Throwable = {",
          "147:     new ParseException(\"INVALID_SQL_SYNTAX\",",
          "149:   }",
          "151:   def cannotResolveWindowReferenceError(name: String, ctx: WindowClauseContext): Throwable = {",
          "152:     new ParseException(\"INVALID_SQL_SYNTAX\",",
          "154:   }",
          "156:   def naturalCrossJoinUnsupportedError(ctx: RelationContext): Throwable = {",
          "",
          "[Removed Lines]",
          "143:       Array(s\"The definition of window '$name' is repetitive.\"), ctx)",
          "148:       Array(s\"Window reference '$name' is not a window specification.\"), ctx)",
          "153:       Array(s\"Cannot resolve window reference '$name'.\"), ctx)",
          "",
          "[Added Lines]",
          "143:       Array(s\"The definition of window ${toSQLId(name)} is repetitive.\"), ctx)",
          "148:       Array(s\"Window reference ${toSQLId(name)} is not a window specification.\"), ctx)",
          "153:       Array(s\"Cannot resolve window reference ${toSQLId(name)}.\"), ctx)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "249:   }",
          "251:   def tooManyArgumentsForTransformError(name: String, ctx: ApplyTransformContext): Throwable = {",
          "253:   }",
          "255:   def invalidBucketsNumberError(describe: String, ctx: ApplyTransformContext): Throwable = {",
          "",
          "[Removed Lines]",
          "252:     new ParseException(\"INVALID_SQL_SYNTAX\", Array(s\"Too many arguments for transform $name\"), ctx)",
          "",
          "[Added Lines]",
          "252:     new ParseException(",
          "253:       errorClass = \"INVALID_SQL_SYNTAX\",",
          "254:       messageParameters = Array(s\"Too many arguments for transform ${toSQLId(name)}\"),",
          "255:       ctx)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "327:     new ParseException(",
          "328:       errorClass = \"INVALID_SQL_SYNTAX\",",
          "329:       messageParameters = Array(",
          "331:         s\"It must be a ${toSQLType(StringType)} literal.\"),",
          "332:       ctx)",
          "333:   }",
          "",
          "[Removed Lines]",
          "330:         s\"Invalid pattern in ${toSQLStmt(\"SHOW FUNCTIONS\")}: $pattern. \" +",
          "",
          "[Added Lines]",
          "333:         s\"Invalid pattern in ${toSQLStmt(\"SHOW FUNCTIONS\")}: ${toSQLId(pattern)}. \" +",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "352:   def duplicateKeysError(key: String, ctx: ParserRuleContext): Throwable = {",
          "355:   }",
          "357:   def unexpectedFomatForSetConfigurationError(ctx: ParserRuleContext): Throwable = {",
          "",
          "[Removed Lines]",
          "354:     new ParseException(errorClass = \"DUPLICATE_KEY\", messageParameters = Array(key), ctx)",
          "",
          "[Added Lines]",
          "357:     new ParseException(errorClass = \"DUPLICATE_KEY\", messageParameters = Array(toSQLId(key)), ctx)",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/DDLParserSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/DDLParserSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/DDLParserSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/DDLParserSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "2050:       ShowFunctions(UnresolvedNamespace(Seq(\"db\")), true, true, Some(\"funct*\")))",
          "2051:     intercept(\"SHOW other FUNCTIONS\", \"\\\"SHOW\\\" other \\\"FUNCTIONS\\\" not supported\")",
          "2052:     intercept(\"SHOW FUNCTIONS IN db f1\",",
          "2054:     intercept(\"SHOW FUNCTIONS IN db LIKE f1\",",
          "2058:     comparePlans(",
          "",
          "[Removed Lines]",
          "2053:       \"Invalid pattern in \\\"SHOW FUNCTIONS\\\": f1\")",
          "2055:       \"Invalid pattern in \\\"SHOW FUNCTIONS\\\": f1\")",
          "",
          "[Added Lines]",
          "2053:       \"Invalid pattern in \\\"SHOW FUNCTIONS\\\": `f1`\")",
          "2055:       \"Invalid pattern in \\\"SHOW FUNCTIONS\\\": `f1`\")",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/PlanParserSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/PlanParserSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/PlanParserSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/PlanParserSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "430:          |       w2 as w1,",
          "431:          |       w3 as w1\"\"\".stripMargin,",
          "432:       WithWindowDefinition(ws1, plan))",
          "445:   }",
          "447:   test(\"lateral view\") {",
          "",
          "[Removed Lines]",
          "435:     intercept(s\"$sql window w2 as w1\", \"Cannot resolve window reference 'w1'\")",
          "438:     intercept(",
          "439:       s\"\"\"$sql",
          "440:          |window w1 as (partition by a, b order by c rows between 1 preceding and 1 following),",
          "441:          |       w2 as w1,",
          "442:          |       w3 as w2\"\"\".stripMargin,",
          "443:       \"Window reference 'w2' is not a window specification\"",
          "444:     )",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/SQLInsertTestSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/SQLInsertTestSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/SQLInsertTestSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/SQLInsertTestSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "264:       val e = intercept[AnalysisException] {",
          "265:         sql(\"INSERT OVERWRITE t PARTITION (c='2', C='3') VALUES (1)\")",
          "266:       }",
          "268:     }",
          "",
          "[Removed Lines]",
          "267:       assert(e.getMessage.contains(\"Found duplicate keys 'c'\"))",
          "",
          "[Added Lines]",
          "267:       assert(e.getMessage.contains(\"Found duplicate keys `c`\"))",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryCompilationErrorsDSv2Suite.scala||sql/core/src/test/scala/org/apache/spark/sql/errors/QueryCompilationErrorsDSv2Suite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/errors/QueryCompilationErrorsDSv2Suite.scala -> sql/core/src/test/scala/org/apache/spark/sql/errors/QueryCompilationErrorsDSv2Suite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "44:         checkAnswer(spark.table(tbl), spark.emptyDataFrame)",
          "45:         assert(e.getMessage === \"The feature is not supported: \" +",
          "47:         assert(e.getErrorClass === \"UNSUPPORTED_FEATURE\")",
          "48:         assert(e.getSqlState === \"0A000\")",
          "49:       }",
          "",
          "[Removed Lines]",
          "46:           s\"\"\"\"IF NOT EXISTS\" for the table '$tbl' by \"INSERT INTO\".\"\"\")",
          "",
          "[Added Lines]",
          "46:           s\"\"\"\"IF NOT EXISTS\" for the table `testcat`.`ns1`.`ns2`.`tbl` by \"INSERT INTO\".\"\"\")",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionErrorsSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionErrorsSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionErrorsSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionErrorsSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "59:       assert(e.getErrorClass === \"INVALID_PARAMETER_VALUE\")",
          "60:       assert(e.getSqlState === \"22023\")",
          "61:       assert(e.getMessage.matches(",
          "64:     }",
          "",
          "[Removed Lines]",
          "62:         \"The value of parameter\\\\(s\\\\) 'key' in the aes_encrypt/aes_decrypt function is invalid: \" +",
          "63:         \"expects a binary value with 16, 24 or 32 bytes, but got \\\\d+ bytes.\"))",
          "",
          "[Added Lines]",
          "62:         \"The value of parameter\\\\(s\\\\) 'key' in the `aes_encrypt`/`aes_decrypt` function \" +",
          "63:         \"is invalid: expects a binary value with 16, 24 or 32 bytes, but got \\\\d+ bytes.\"))",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "93:       assert(e.getErrorClass === \"INVALID_PARAMETER_VALUE\")",
          "94:       assert(e.getSqlState === \"22023\")",
          "95:       assert(e.getMessage ===",
          "97:         \"is invalid: Detail message: \" +",
          "98:         \"Given final block not properly padded. \" +",
          "99:         \"Such issues can arise if a bad key is used during decryption.\")",
          "",
          "[Removed Lines]",
          "96:         \"The value of parameter(s) 'expr, key' in the aes_encrypt/aes_decrypt function \" +",
          "",
          "[Added Lines]",
          "96:         \"The value of parameter(s) 'expr, key' in the `aes_encrypt`/`aes_decrypt` function \" +",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "111:       assert(e.getErrorClass === \"UNSUPPORTED_FEATURE\")",
          "112:       assert(e.getSqlState === \"0A000\")",
          "113:       assert(e.getMessage.matches(\"\"\"The feature is not supported: AES-\\w+ with the padding \\w+\"\"\" +",
          "115:     }",
          "",
          "[Removed Lines]",
          "114:         \" by the aes_encrypt/aes_decrypt function.\"))",
          "",
          "[Added Lines]",
          "114:         \" by the `aes_encrypt`/`aes_decrypt` function.\"))",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryParsingErrorsSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/errors/QueryParsingErrorsSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/errors/QueryParsingErrorsSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/errors/QueryParsingErrorsSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "130:       sqlState = \"42000\",",
          "131:       message =",
          "132:         \"\"\"",
          "134:           |",
          "135:           |== SQL ==",
          "136:           |SELECT min(a) OVER win FROM t1 WINDOW win AS win, win AS win2",
          "",
          "[Removed Lines]",
          "133:           |Invalid SQL syntax: The definition of window 'win' is repetitive.(line 1, pos 31)",
          "",
          "[Added Lines]",
          "133:           |Invalid SQL syntax: The definition of window `win` is repetitive.(line 1, pos 31)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "145:       sqlState = \"42000\",",
          "146:       message =",
          "147:         \"\"\"",
          "149:           |",
          "150:           |== SQL ==",
          "151:           |SELECT min(a) OVER win FROM t1 WINDOW win AS win",
          "",
          "[Removed Lines]",
          "148:           |Invalid SQL syntax: Window reference 'win' is not a window specification.(line 1, pos 31)",
          "",
          "[Added Lines]",
          "148:           |Invalid SQL syntax: Window reference `win` is not a window specification.(line 1, pos 31)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "160:       sqlState = \"42000\",",
          "161:       message =",
          "162:         \"\"\"",
          "164:           |",
          "165:           |== SQL ==",
          "166:           |SELECT min(a) OVER win FROM t1 WINDOW win AS win2",
          "",
          "[Removed Lines]",
          "163:           |Invalid SQL syntax: Cannot resolve window reference 'win2'.(line 1, pos 31)",
          "",
          "[Added Lines]",
          "163:           |Invalid SQL syntax: Cannot resolve window reference `win2`.(line 1, pos 31)",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "206:       sqlState = \"42000\",",
          "207:       message =",
          "208:         \"\"\"",
          "210:           |",
          "211:           |== SQL ==",
          "212:           |CREATE TABLE table(col int) PARTITIONED BY (years(col,col))",
          "",
          "[Removed Lines]",
          "209:           |Invalid SQL syntax: Too many arguments for transform years(line 1, pos 44)",
          "",
          "[Added Lines]",
          "209:           |Invalid SQL syntax: Too many arguments for transform `years`(line 1, pos 44)",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/execution/command/DDLParserSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/command/DDLParserSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/execution/command/DDLParserSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/execution/command/DDLParserSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "200:     assert(parsed.isInstanceOf[Project])",
          "201:   }",
          "218:   test(\"unsupported operations\") {",
          "219:     intercept[ParseException] {",
          "220:       parser.parsePlan(",
          "",
          "[Removed Lines]",
          "203:   test(\"duplicate keys in table properties\") {",
          "204:     val e = intercept[ParseException] {",
          "205:       parser.parsePlan(\"ALTER TABLE dbx.tab1 SET TBLPROPERTIES ('key1' = '1', 'key1' = '2')\")",
          "206:     }.getMessage",
          "207:     assert(e.contains(\"Found duplicate keys 'key1'\"))",
          "208:   }",
          "210:   test(\"duplicate columns in partition specs\") {",
          "211:     val e = intercept[ParseException] {",
          "212:       parser.parsePlan(",
          "213:         \"ALTER TABLE dbx.tab1 PARTITION (a='1', a='2') RENAME TO PARTITION (a='100', a='200')\")",
          "214:     }.getMessage",
          "215:     assert(e.contains(\"Found duplicate keys 'a'\"))",
          "216:   }",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "c6584af37a45bd782e92b10e130caba42877c64c",
      "candidate_info": {
        "commit_hash": "c6584af37a45bd782e92b10e130caba42877c64c",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/c6584af37a45bd782e92b10e130caba42877c64c",
        "files": [
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2ScanRelationPushDown.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCV2Suite.scala"
        ],
        "message": "[SPARK-39135][SQL] DS V2 aggregate partial push-down should supports group by without aggregate functions\n\n### What changes were proposed in this pull request?\nCurrently, the SQL show below not supported by DS V2 aggregate partial push-down.\n`select key from tab group by key`\n\n### Why are the changes needed?\nMake DS V2 aggregate partial push-down supports group by without aggregate functions.\n\n### Does this PR introduce _any_ user-facing change?\n'No'.\nNew feature.\n\n### How was this patch tested?\nNew tests\n\nCloses #36492 from beliefer/SPARK-39135.\n\nAuthored-by: Jiaan Geng <beliefer@163.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\n(cherry picked from commit decd393e23406d82b47aa75c4d24db04c7d1efd6)\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>",
        "before_after_code_files": [
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2ScanRelationPushDown.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2ScanRelationPushDown.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCV2Suite.scala||sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCV2Suite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2ScanRelationPushDown.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2ScanRelationPushDown.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2ScanRelationPushDown.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2ScanRelationPushDown.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "294:   private def supportPartialAggPushDown(agg: Aggregation): Boolean = {",
          "298:       case sum: Sum => !sum.isDistinct",
          "299:       case count: Count => !count.isDistinct",
          "300:       case avg: Avg => !avg.isDistinct",
          "",
          "[Removed Lines]",
          "297:     agg.aggregateExpressions().exists {",
          "",
          "[Added Lines]",
          "297:     agg.aggregateExpressions().isEmpty || agg.aggregateExpressions().exists {",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCV2Suite.scala||sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCV2Suite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCV2Suite.scala -> sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCV2Suite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "671:     checkAnswer(df, Seq(Row(5)))",
          "672:   }",
          "674:   test(\"scan with aggregate push-down: COUNT(col)\") {",
          "675:     val df = sql(\"select COUNT(DEPT) FROM h2.test.employee\")",
          "676:     checkAggregateRemoved(df)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "674:   test(\"scan with aggregate push-down: GROUP BY without aggregate functions\") {",
          "675:     val df = sql(\"select name FROM h2.test.employee GROUP BY name\")",
          "676:     checkAggregateRemoved(df)",
          "677:     checkPushedInfo(df,",
          "678:       \"PushedAggregates: [], PushedFilters: [], PushedGroupByExpressions: [NAME],\")",
          "679:     checkAnswer(df, Seq(Row(\"alex\"), Row(\"amy\"), Row(\"cathy\"), Row(\"david\"), Row(\"jen\")))",
          "681:     val df2 = spark.read",
          "682:       .option(\"partitionColumn\", \"dept\")",
          "683:       .option(\"lowerBound\", \"0\")",
          "684:       .option(\"upperBound\", \"2\")",
          "685:       .option(\"numPartitions\", \"2\")",
          "686:       .table(\"h2.test.employee\")",
          "687:       .groupBy($\"name\")",
          "688:       .agg(Map.empty[String, String])",
          "689:     checkAggregateRemoved(df2, false)",
          "690:     checkPushedInfo(df2,",
          "691:       \"PushedAggregates: [], PushedFilters: [], PushedGroupByExpressions: [NAME],\")",
          "692:     checkAnswer(df2, Seq(Row(\"alex\"), Row(\"amy\"), Row(\"cathy\"), Row(\"david\"), Row(\"jen\")))",
          "694:     val df3 = sql(\"SELECT CASE WHEN SALARY > 8000 AND SALARY < 10000 THEN SALARY ELSE 0 END as\" +",
          "695:       \" key FROM h2.test.employee GROUP BY key\")",
          "696:     checkAggregateRemoved(df3)",
          "697:     checkPushedInfo(df3,",
          "698:       \"\"\"",
          "699:         |PushedAggregates: [],",
          "700:         |PushedFilters: [],",
          "701:         |PushedGroupByExpressions:",
          "702:         |[CASE WHEN (SALARY > 8000.00) AND (SALARY < 10000.00) THEN SALARY ELSE 0.00 END],",
          "703:         |\"\"\".stripMargin.replaceAll(\"\\n\", \" \"))",
          "704:     checkAnswer(df3, Seq(Row(0), Row(9000)))",
          "706:     val df4 = spark.read",
          "707:       .option(\"partitionColumn\", \"dept\")",
          "708:       .option(\"lowerBound\", \"0\")",
          "709:       .option(\"upperBound\", \"2\")",
          "710:       .option(\"numPartitions\", \"2\")",
          "711:       .table(\"h2.test.employee\")",
          "712:       .groupBy(when(($\"SALARY\" > 8000).and($\"SALARY\" < 10000), $\"SALARY\").otherwise(0).as(\"key\"))",
          "713:       .agg(Map.empty[String, String])",
          "714:     checkAggregateRemoved(df4, false)",
          "715:     checkPushedInfo(df4,",
          "716:       \"\"\"",
          "717:         |PushedAggregates: [],",
          "718:         |PushedFilters: [],",
          "719:         |PushedGroupByExpressions:",
          "720:         |[CASE WHEN (SALARY > 8000.00) AND (SALARY < 10000.00) THEN SALARY ELSE 0.00 END],",
          "721:         |\"\"\".stripMargin.replaceAll(\"\\n\", \" \"))",
          "722:     checkAnswer(df4, Seq(Row(0), Row(9000)))",
          "723:   }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "d3f7f42b2780416b2cf5cb50e522909bb68e8c56",
      "candidate_info": {
        "commit_hash": "d3f7f42b2780416b2cf5cb50e522909bb68e8c56",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/d3f7f42b2780416b2cf5cb50e522909bb68e8c56",
        "files": [
          "core/src/main/scala/org/apache/spark/MapOutputTracker.scala",
          "core/src/test/scala/org/apache/spark/MapOutputTrackerSuite.scala"
        ],
        "message": "[SPARK-39553][CORE] Multi-thread unregister shuffle shouldn't throw NPE when using Scala 2.13\n\n### What changes were proposed in this pull request?\nThis pr add a `shuffleStatus != null` condition to `o.a.s.MapOutputTrackerMaster#unregisterShuffle` method to avoid throwing NPE when using Scala 2.13.\n\n### Why are the changes needed?\nEnsure that no NPE is thrown when `o.a.s.MapOutputTrackerMaster#unregisterShuffle` is called by multiple threads, this pr is only for Scala 2.13.\n\n`o.a.s.MapOutputTrackerMaster#unregisterShuffle` method will be called concurrently by the following two paths:\n\n- BlockManagerStorageEndpoint:\n\nhttps://github.com/apache/spark/blob/6f1046afa40096f477b29beecca5ca6286dfa7f3/core/src/main/scala/org/apache/spark/storage/BlockManagerStorageEndpoint.scala#L56-L62\n\n- ContextCleaner:\n\nhttps://github.com/apache/spark/blob/6f1046afa40096f477b29beecca5ca6286dfa7f3/core/src/main/scala/org/apache/spark/ContextCleaner.scala#L234-L241\n\nWhen test with Scala 2.13, for example `sql/core` module,  there are many log as follows\uff0calthough these did not cause UTs failure:\n\n```\n17:44:09.957 WARN org.apache.spark.storage.BlockManagerMaster: Failed to remove shuffle 87 - null\njava.lang.NullPointerException\n\tat org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1(MapOutputTracker.scala:882)\n\tat org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1$adapted(MapOutputTracker.scala:881)\n\tat scala.Option.foreach(Option.scala:437)\n\tat org.apache.spark.MapOutputTrackerMaster.unregisterShuffle(MapOutputTracker.scala:881)\n\tat org.apache.spark.storage.BlockManagerStorageEndpoint$$anonfun$receiveAndReply$1.$anonfun$applyOrElse$3(BlockManagerStorageEndpoint.scala:59)\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.scala:17)\n\tat org.apache.spark.storage.BlockManagerStorageEndpoint.$anonfun$doAsync$1(BlockManagerStorageEndpoint.scala:89)\n\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:678)\n\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n17:44:09.958 ERROR org.apache.spark.ContextCleaner: Error cleaning shuffle 94\njava.lang.NullPointerException\n\tat org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1(MapOutputTracker.scala:882)\n\tat org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1$adapted(MapOutputTracker.scala:881)\n\tat scala.Option.foreach(Option.scala:437)\n\tat org.apache.spark.MapOutputTrackerMaster.unregisterShuffle(MapOutputTracker.scala:881)\n\tat org.apache.spark.ContextCleaner.doCleanupShuffle(ContextCleaner.scala:241)\n\tat org.apache.spark.ContextCleaner.$anonfun$keepCleaning$3(ContextCleaner.scala:202)\n\tat org.apache.spark.ContextCleaner.$anonfun$keepCleaning$3$adapted(ContextCleaner.scala:195)\n\tat scala.Option.foreach(Option.scala:437)\n\tat org.apache.spark.ContextCleaner.$anonfun$keepCleaning$1(ContextCleaner.scala:195)\n\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1432)\n\tat org.apache.spark.ContextCleaner.org$apache$spark$ContextCleaner$$keepCleaning(ContextCleaner.scala:189)\n\tat org.apache.spark.ContextCleaner$$anon$1.run(ContextCleaner.scala:79)\n```\n\nI think this is a bug of Scala 2.13.8 and already submit an issue to https://github.com/scala/bug/issues/12613, this PR is only for protection, we should remove this protection after Scala 2.13(maybe https://github.com/scala/scala/pull/9957) fixes this issue.\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\n\n- Pass GA\n- Add new test `SPARK-39553: Multi-thread unregister shuffle shouldn't throw NPE` to `MapOutputTrackerSuite`, we can test manually as follows:\n\n```\ndev/change-scala-version.sh 2.13\nmvn clean install -DskipTests -pl core -am -Pscala-2.13\nmvn clean test -pl core -Pscala-2.13 -Dtest=none -DwildcardSuites=org.apache.spark.MapOutputTrackerSuite\n```\n\n**Before**\n\n```\n- SPARK-39553: Multi-thread unregister shuffle shouldn't throw NPE *** FAILED ***\n  3 did not equal 0 (MapOutputTrackerSuite.scala:971)\nRun completed in 17 seconds, 505 milliseconds.\nTotal number of tests run: 25\nSuites: completed 2, aborted 0\nTests: succeeded 24, failed 1, canceled 0, ignored 1, pending 0\n*** 1 TEST FAILED ***\n```\n\n**After**\n\n```\n- SPARK-39553: Multi-thread unregister shuffle shouldn't throw NPE\nRun completed in 17 seconds, 996 milliseconds.\nTotal number of tests run: 25\nSuites: completed 2, aborted 0\nTests: succeeded 25, failed 0, canceled 0, ignored 1, pending 0\nAll tests passed.\n\n```\n\nCloses #37024 from LuciferYang/SPARK-39553.\n\nAuthored-by: yangjie01 <yangjie01@baidu.com>\nSigned-off-by: Sean Owen <srowen@gmail.com>\n(cherry picked from commit 29258964cae45cea43617ade971fb4ea9fe2902a)\nSigned-off-by: Sean Owen <srowen@gmail.com>",
        "before_after_code_files": [
          "core/src/main/scala/org/apache/spark/MapOutputTracker.scala||core/src/main/scala/org/apache/spark/MapOutputTracker.scala",
          "core/src/test/scala/org/apache/spark/MapOutputTrackerSuite.scala||core/src/test/scala/org/apache/spark/MapOutputTrackerSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "core/src/main/scala/org/apache/spark/MapOutputTracker.scala||core/src/main/scala/org/apache/spark/MapOutputTracker.scala": [
          "File: core/src/main/scala/org/apache/spark/MapOutputTracker.scala -> core/src/main/scala/org/apache/spark/MapOutputTracker.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "880:   def unregisterShuffle(shuffleId: Int): Unit = {",
          "881:     shuffleStatuses.remove(shuffleId).foreach { shuffleStatus =>",
          "884:     }",
          "885:   }",
          "",
          "[Removed Lines]",
          "882:       shuffleStatus.invalidateSerializedMapOutputStatusCache()",
          "883:       shuffleStatus.invalidateSerializedMergeOutputStatusCache()",
          "",
          "[Added Lines]",
          "884:       if (shuffleStatus != null) {",
          "885:         shuffleStatus.invalidateSerializedMapOutputStatusCache()",
          "886:         shuffleStatus.invalidateSerializedMergeOutputStatusCache()",
          "887:       }",
          "",
          "---------------"
        ],
        "core/src/test/scala/org/apache/spark/MapOutputTrackerSuite.scala||core/src/test/scala/org/apache/spark/MapOutputTrackerSuite.scala": [
          "File: core/src/test/scala/org/apache/spark/MapOutputTrackerSuite.scala -> core/src/test/scala/org/apache/spark/MapOutputTrackerSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: package org.apache.spark",
          "20: import scala.collection.mutable.ArrayBuffer",
          "22: import org.mockito.ArgumentMatchers.any",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "20: import java.util.concurrent.atomic.LongAdder",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "938:       assert(worker.shufflePushMergerLocations.isEmpty)",
          "939:     }",
          "940:   }",
          "941: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "944:   test(\"SPARK-39553: Multi-thread unregister shuffle shouldn't throw NPE\") {",
          "945:     val rpcEnv = createRpcEnv(\"test\")",
          "946:     val tracker = newTrackerMaster()",
          "947:     tracker.trackerEndpoint = rpcEnv.setupEndpoint(MapOutputTracker.ENDPOINT_NAME,",
          "948:       new MapOutputTrackerMasterEndpoint(rpcEnv, tracker, conf))",
          "949:     val shuffleIdRange = 0 until 100",
          "950:     shuffleIdRange.foreach { shuffleId =>",
          "951:       tracker.registerShuffle(shuffleId, 2, MergeStatus.SHUFFLE_PUSH_DUMMY_NUM_REDUCES)",
          "952:     }",
          "953:     val npeCounter = new LongAdder()",
          "955:     val threads = new Array[Thread](5)",
          "956:     threads.indices.foreach { i =>",
          "957:       threads(i) = new Thread() {",
          "958:         override def run(): Unit = {",
          "959:           shuffleIdRange.foreach { shuffleId =>",
          "960:             try {",
          "961:               tracker.unregisterShuffle(shuffleId)",
          "962:             } catch {",
          "963:               case _: NullPointerException => npeCounter.increment()",
          "964:             }",
          "965:           }",
          "966:         }",
          "967:       }",
          "968:     }",
          "969:     threads.foreach(_.start())",
          "970:     threads.foreach(_.join())",
          "971:     tracker.stop()",
          "972:     rpcEnv.shutdown()",
          "973:     assert(npeCounter.intValue() == 0)",
          "974:   }",
          "",
          "---------------"
        ]
      }
    }
  ]
}