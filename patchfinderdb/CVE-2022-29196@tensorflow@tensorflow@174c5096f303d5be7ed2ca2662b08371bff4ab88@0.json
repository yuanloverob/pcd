{
  "cve_id": "CVE-2022-29196",
  "cve_desc": "TensorFlow is an open source platform for machine learning. Prior to versions 2.9.0, 2.8.1, 2.7.2, and 2.6.4, the implementation of `tf.raw_ops.Conv3DBackpropFilterV2` does not fully validate the input arguments. This results in a `CHECK`-failure which can be used to trigger a denial of service attack. The code does not validate that the `filter_sizes` argument is a vector. Versions 2.9.0, 2.8.1, 2.7.2, and 2.6.4 contain a patch for this issue.",
  "repo": "tensorflow/tensorflow",
  "patch_hash": "174c5096f303d5be7ed2ca2662b08371bff4ab88",
  "patch_info": {
    "commit_hash": "174c5096f303d5be7ed2ca2662b08371bff4ab88",
    "repo": "tensorflow/tensorflow",
    "commit_url": "https://github.com/tensorflow/tensorflow/commit/174c5096f303d5be7ed2ca2662b08371bff4ab88",
    "files": [
      "tensorflow/core/kernels/conv_grad_ops_3d.cc",
      "tensorflow/python/kernel_tests/nn_ops/conv3d_backprop_filter_v2_grad_test.py"
    ],
    "message": "Fix failed check in Conv3DBackpropFilterV2.\n\nPassing in a rank-0 `filter_size` causes a check fail and crash,\ncoming from a `filter_size.vec<>()` call.  Here we check the size\nfirst.\n\nPiperOrigin-RevId: 445517122",
    "before_after_code_files": [
      "tensorflow/core/kernels/conv_grad_ops_3d.cc||tensorflow/core/kernels/conv_grad_ops_3d.cc",
      "tensorflow/python/kernel_tests/nn_ops/conv3d_backprop_filter_v2_grad_test.py||tensorflow/python/kernel_tests/nn_ops/conv3d_backprop_filter_v2_grad_test.py"
    ]
  },
  "patch_diff": {
    "tensorflow/core/kernels/conv_grad_ops_3d.cc||tensorflow/core/kernels/conv_grad_ops_3d.cc": [
      "File: tensorflow/core/kernels/conv_grad_ops_3d.cc -> tensorflow/core/kernels/conv_grad_ops_3d.cc",
      "--- Hunk 1 ---",
      "[Context before]",
      "741:     TensorShape filter_shape;",
      "742:     if (takes_shape_) {",
      "743:       const Tensor& filter_sizes = context->input(1);",
      "744:       OP_REQUIRES_OK(context, TensorShapeUtils::MakeShape(",
      "745:                                   filter_sizes.vec<int32>(), &filter_shape));",
      "746:     } else {",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "744:       OP_REQUIRES(context, TensorShapeUtils::IsVector(filter_sizes.shape()),",
      "745:                   errors::InvalidArgument(",
      "746:                       \"filter_sizes shape must be rank 1 but is rank \",",
      "747:                       filter_sizes.shape().dims()));",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "875:     TensorShape filter_shape;",
      "876:     if (takes_shape_) {",
      "877:       const Tensor& filter_sizes = context->input(1);",
      "878:       OP_REQUIRES_OK(context, TensorShapeUtils::MakeShape(",
      "879:                                   filter_sizes.vec<int32>(), &filter_shape));",
      "880:     } else {",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "882:       OP_REQUIRES(context, TensorShapeUtils::IsVector(filter_sizes.shape()),",
      "883:                   errors::InvalidArgument(",
      "884:                       \"filter_sizes shape must be rank 1 but is rank \",",
      "885:                       filter_sizes.shape().dims()));",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "1638:     TensorShape filter_shape;",
      "1639:     if (takes_shape_) {",
      "1640:       const Tensor& filter_sizes = context->input(1);",
      "1641:       OP_REQUIRES_OK(context, tensor::MakeShape(filter_sizes, &filter_shape));",
      "1642:     } else {",
      "1643:       filter_shape = context->input(1).shape();",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "1649:       OP_REQUIRES(context, TensorShapeUtils::IsVector(filter_sizes.shape()),",
      "1650:                   errors::InvalidArgument(",
      "1651:                       \"filter_sizes shape must be rank 1 but is rank \",",
      "1652:                       filter_sizes.shape().dims()));",
      "",
      "---------------"
    ],
    "tensorflow/python/kernel_tests/nn_ops/conv3d_backprop_filter_v2_grad_test.py||tensorflow/python/kernel_tests/nn_ops/conv3d_backprop_filter_v2_grad_test.py": [
      "File: tensorflow/python/kernel_tests/nn_ops/conv3d_backprop_filter_v2_grad_test.py -> tensorflow/python/kernel_tests/nn_ops/conv3d_backprop_filter_v2_grad_test.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "19: from tensorflow.python.framework import constant_op",
      "20: from tensorflow.python.framework import dtypes",
      "21: from tensorflow.python.framework import test_util",
      "22: from tensorflow.python.ops import array_ops",
      "23: from tensorflow.python.ops import gradient_checker",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "21: from tensorflow.python.framework import errors",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "58:           err_tolerance = 1e-3",
      "59:           self.assertLess(err, err_tolerance)",
      "62: if __name__ == \"__main__\":",
      "63:   test.main()",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "62:   def testBadFilterShape(self):",
      "63:     strides = [1, 1, 1, 1, 1]",
      "64:     padding = \"VALID\"",
      "65:     tin = constant_op.constant(",
      "66:         .5053710941, shape=[2, 2, 2, 2, 1], dtype=dtypes.float32)",
      "67:     filter_sizes = constant_op.constant(0, shape=[], dtype=dtypes.int32)",
      "68:     out_backprop = constant_op.constant(",
      "69:         .5053710941, shape=[2, 2, 2, 2, 1], dtype=dtypes.float32)",
      "71:     with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
      "72:                                 \"must be rank 1\"):",
      "73:       nn_ops.conv3d_backprop_filter_v2(",
      "74:           input=tin,",
      "75:           filter_sizes=filter_sizes,",
      "76:           out_backprop=out_backprop,",
      "77:           strides=strides,",
      "78:           padding=padding)",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "3e8539cbca22e04e6e8f6ca30460d526fee4b5aa",
      "candidate_info": {
        "commit_hash": "3e8539cbca22e04e6e8f6ca30460d526fee4b5aa",
        "repo": "tensorflow/tensorflow",
        "commit_url": "https://github.com/tensorflow/tensorflow/commit/3e8539cbca22e04e6e8f6ca30460d526fee4b5aa",
        "files": [
          "tensorflow/core/kernels/BUILD",
          "tensorflow/core/kernels/conv_2d_gpu_bfloat16.cu.cc",
          "tensorflow/core/kernels/conv_grad_filter_ops.cc",
          "tensorflow/core/kernels/conv_grad_input_ops.cc",
          "tensorflow/core/kernels/conv_grad_ops_3d.cc",
          "tensorflow/core/kernels/conv_ops.cc",
          "tensorflow/core/kernels/conv_ops_3d.cc",
          "tensorflow/core/kernels/conv_ops_gpu.cc",
          "tensorflow/core/kernels/depthwise_conv_grad_op.cc",
          "tensorflow/core/kernels/depthwise_conv_op.cc",
          "tensorflow/core/kernels/depthwise_conv_op.h",
          "tensorflow/core/kernels/depthwise_conv_op_gpu.h",
          "tensorflow/core/kernels/depthwise_conv_op_gpu_bfloat16.cu.cc",
          "tensorflow/core/util/gpu_device_functions.h",
          "tensorflow/python/kernel_tests/nn_ops/conv_ops_test.py",
          "tensorflow/python/kernel_tests/nn_ops/depthwise_conv_op_base.py"
        ],
        "message": "Rollback of PR #57674 rollback BF16 Conv Ops - breaks internal tests\n\nPiperOrigin-RevId: 486951879",
        "before_after_code_files": [
          "tensorflow/core/kernels/conv_2d_gpu_bfloat16.cu.cc||tensorflow/core/kernels/conv_2d_gpu_bfloat16.cu.cc",
          "tensorflow/core/kernels/conv_grad_filter_ops.cc||tensorflow/core/kernels/conv_grad_filter_ops.cc",
          "tensorflow/core/kernels/conv_grad_input_ops.cc||tensorflow/core/kernels/conv_grad_input_ops.cc",
          "tensorflow/core/kernels/conv_grad_ops_3d.cc||tensorflow/core/kernels/conv_grad_ops_3d.cc",
          "tensorflow/core/kernels/conv_ops.cc||tensorflow/core/kernels/conv_ops.cc",
          "tensorflow/core/kernels/conv_ops_3d.cc||tensorflow/core/kernels/conv_ops_3d.cc",
          "tensorflow/core/kernels/conv_ops_gpu.cc||tensorflow/core/kernels/conv_ops_gpu.cc",
          "tensorflow/core/kernels/depthwise_conv_grad_op.cc||tensorflow/core/kernels/depthwise_conv_grad_op.cc",
          "tensorflow/core/kernels/depthwise_conv_op.cc||tensorflow/core/kernels/depthwise_conv_op.cc",
          "tensorflow/core/kernels/depthwise_conv_op.h||tensorflow/core/kernels/depthwise_conv_op.h",
          "tensorflow/core/kernels/depthwise_conv_op_gpu.h||tensorflow/core/kernels/depthwise_conv_op_gpu.h",
          "tensorflow/core/kernels/depthwise_conv_op_gpu_bfloat16.cu.cc||tensorflow/core/kernels/depthwise_conv_op_gpu_bfloat16.cu.cc",
          "tensorflow/core/util/gpu_device_functions.h||tensorflow/core/util/gpu_device_functions.h",
          "tensorflow/python/kernel_tests/nn_ops/conv_ops_test.py||tensorflow/python/kernel_tests/nn_ops/conv_ops_test.py",
          "tensorflow/python/kernel_tests/nn_ops/depthwise_conv_op_base.py||tensorflow/python/kernel_tests/nn_ops/depthwise_conv_op_base.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "tensorflow/core/kernels/conv_grad_ops_3d.cc||tensorflow/core/kernels/conv_grad_ops_3d.cc"
          ],
          "candidate": [
            "tensorflow/core/kernels/conv_grad_ops_3d.cc||tensorflow/core/kernels/conv_grad_ops_3d.cc"
          ]
        }
      },
      "candidate_diff": {
        "tensorflow/core/kernels/conv_2d_gpu_bfloat16.cu.cc||tensorflow/core/kernels/conv_2d_gpu_bfloat16.cu.cc": [
          "File: tensorflow/core/kernels/conv_2d_gpu_bfloat16.cu.cc -> tensorflow/core/kernels/conv_2d_gpu_bfloat16.cu.cc",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "tensorflow/core/kernels/conv_grad_filter_ops.cc||tensorflow/core/kernels/conv_grad_filter_ops.cc": [
          "File: tensorflow/core/kernels/conv_grad_filter_ops.cc -> tensorflow/core/kernels/conv_grad_filter_ops.cc",
          "--- Hunk 1 ---",
          "[Context before]",
          "49: #endif",
          "51: #if GOOGLE_CUDA || TENSORFLOW_USE_ROCM",
          "53: #include \"tensorflow/core/kernels/conv_ops_gpu.h\"",
          "54: #include \"tensorflow/core/platform/stream_executor.h\"",
          "55: #include \"tensorflow/core/protobuf/autotuning.pb.h\"",
          "",
          "[Removed Lines]",
          "52: #include \"tensorflow/core/kernels/cast_op.h\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "924:       filter_backprop->tensor<T, 4>());",
          "925: }",
          "979: namespace functor {",
          "980: #define DECLARE_GPU_SPEC(T)                                             \\",
          "",
          "[Removed Lines]",
          "927: template <>",
          "928: void LaunchConv2DBackpropFilterOp<Eigen::GpuDevice, Eigen::bfloat16>::",
          "929: operator()(OpKernelContext* ctx, bool use_cudnn, bool cudnn_use_autotune,",
          "930:            const Tensor& out_backprop, const Tensor& input, int row_dilation,",
          "931:            int col_dilation, int row_stride, int col_stride,",
          "932:            const Padding& padding,",
          "933:            const std::vector<int64_t>& explicit_paddings,",
          "934:            Tensor* filter_backprop, TensorFormat data_format) {",
          "937:   auto* stream = ctx->op_device_context()->stream();",
          "938:   const bool cast_to_float = !stream->GetCudaComputeCapability().IsAtLeast(",
          "939:       se::CudaComputeCapability::AMPERE);",
          "940:   Tensor casted_input = input;",
          "941:   Tensor casted_out_backprop = out_backprop;",
          "942:   Tensor casted_filter_backprop = *filter_backprop;",
          "944:   if (cast_to_float) {",
          "945:     const GPUDevice& device = ctx->eigen_device<GPUDevice>();",
          "946:     functor::CastFunctor<GPUDevice, float, Eigen::bfloat16> cast;",
          "947:     OP_REQUIRES_OK(ctx,",
          "948:                    ctx->allocate_temp(DT_FLOAT, input.shape(), &casted_input));",
          "949:     cast(device, casted_input.template flat<float>(),",
          "950:          input.template flat<Eigen::bfloat16>());",
          "952:     OP_REQUIRES_OK(ctx, ctx->allocate_temp(DT_FLOAT, out_backprop.shape(),",
          "953:                                            &casted_out_backprop));",
          "954:     cast(device, casted_out_backprop.template flat<float>(),",
          "955:          out_backprop.template flat<Eigen::bfloat16>());",
          "957:     OP_REQUIRES_OK(ctx, ctx->allocate_temp(DT_FLOAT, filter_backprop->shape(),",
          "958:                                            &casted_filter_backprop));",
          "960:     LaunchConv2DBackpropFilterOp<Eigen::GpuDevice, float>()(",
          "961:         ctx, use_cudnn, cudnn_use_autotune, casted_out_backprop, casted_input,",
          "962:         row_dilation, col_dilation, row_stride, col_stride, padding,",
          "963:         explicit_paddings, &casted_filter_backprop, data_format);",
          "965:     functor::CastFunctor<GPUDevice, Eigen::bfloat16, float> cast_back;",
          "966:     const Tensor& casted_filter_backprop_const = casted_filter_backprop;",
          "967:     cast_back(device, filter_backprop->template flat<Eigen::bfloat16>(),",
          "968:               casted_filter_backprop_const.template flat<float>());",
          "969:     return;",
          "970:   }",
          "972:   LaunchConv2DBackpropFilterOp<Eigen::GpuDevice, Eigen::bfloat16>()(",
          "973:       ctx, use_cudnn, cudnn_use_autotune, casted_out_backprop, casted_input,",
          "974:       row_dilation, col_dilation, row_stride, col_stride, padding,",
          "975:       explicit_paddings, &casted_filter_backprop, data_format);",
          "976: }",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "996: DECLARE_GPU_SPEC(float);",
          "997: DECLARE_GPU_SPEC(Eigen::half);",
          "999: DECLARE_GPU_SPEC(double);",
          "1000: #undef DECLARE_GPU_SPEC",
          "1001: }  // namespace functor",
          "",
          "[Removed Lines]",
          "998: DECLARE_GPU_SPEC(Eigen::bfloat16);",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "1015:                             .TypeConstraint<Eigen::half>(\"T\")",
          "1016:                             .HostMemory(\"filter_sizes\"),",
          "1017:                         Conv2DBackpropFilterOp<GPUDevice, Eigen::half>);",
          "",
          "[Removed Lines]",
          "1018: REGISTER_KERNEL_BUILDER(Name(\"Conv2DBackpropFilter\")",
          "1019:                             .Device(DEVICE_GPU)",
          "1020:                             .TypeConstraint<Eigen::bfloat16>(\"T\")",
          "1021:                             .HostMemory(\"filter_sizes\"),",
          "1022:                         Conv2DBackpropFilterOp<GPUDevice, Eigen::bfloat16>);",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "tensorflow/core/kernels/conv_grad_input_ops.cc||tensorflow/core/kernels/conv_grad_input_ops.cc": [
          "File: tensorflow/core/kernels/conv_grad_input_ops.cc -> tensorflow/core/kernels/conv_grad_input_ops.cc",
          "--- Hunk 1 ---",
          "[Context before]",
          "22: #include \"tensorflow/core/profiler/lib/scoped_annotation.h\"",
          "24: #if GOOGLE_CUDA || TENSORFLOW_USE_ROCM",
          "26: #include \"tensorflow/core/protobuf/autotuning.pb.h\"",
          "27: #include \"tensorflow/core/util/autotune_maps/conv_parameters.h\"",
          "28: #endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM",
          "",
          "[Removed Lines]",
          "25: #include \"tensorflow/core/kernels/cast_op.h\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "433:   }",
          "434: }",
          "487: namespace functor {",
          "488: #define DECLARE_GPU_SPEC(T)                                             \\",
          "",
          "[Removed Lines]",
          "436: template <>",
          "437: void LaunchConv2DBackpropInputOp<GPUDevice, Eigen::bfloat16>::operator()(",
          "438:     OpKernelContext* ctx, bool use_cudnn, bool cudnn_use_autotune,",
          "439:     const Tensor& out_backprop, const Tensor& filter, int row_dilation,",
          "440:     int col_dilation, int row_stride, int col_stride, const Padding& padding,",
          "441:     const std::vector<int64_t>& explicit_paddings, Tensor* in_backprop,",
          "442:     TensorFormat data_format) {",
          "445:   auto* stream = ctx->op_device_context()->stream();",
          "446:   const bool cast_to_float = !stream->GetCudaComputeCapability().IsAtLeast(",
          "447:       se::CudaComputeCapability::AMPERE);",
          "448:   Tensor casted_out_backprop = out_backprop;",
          "449:   Tensor casted_filter = filter;",
          "450:   Tensor casted_in_backprop = *in_backprop;",
          "452:   if (cast_to_float) {",
          "453:     const GPUDevice& device = ctx->eigen_device<GPUDevice>();",
          "454:     functor::CastFunctor<GPUDevice, float, Eigen::bfloat16> cast;",
          "455:     OP_REQUIRES_OK(ctx, ctx->allocate_temp(DT_FLOAT, out_backprop.shape(),",
          "456:                                            &casted_out_backprop));",
          "457:     cast(device, casted_out_backprop.template flat<float>(),",
          "458:          out_backprop.template flat<Eigen::bfloat16>());",
          "460:     OP_REQUIRES_OK(",
          "461:         ctx, ctx->allocate_temp(DT_FLOAT, filter.shape(), &casted_filter));",
          "462:     cast(device, casted_filter.template flat<float>(),",
          "463:          filter.template flat<Eigen::bfloat16>());",
          "465:     OP_REQUIRES_OK(ctx, ctx->allocate_temp(DT_FLOAT, in_backprop->shape(),",
          "466:                                            &casted_in_backprop));",
          "468:     LaunchConv2DBackpropInputOp<Eigen::GpuDevice, float>()(",
          "469:         ctx, use_cudnn, cudnn_use_autotune, casted_out_backprop, casted_filter,",
          "470:         row_dilation, col_dilation, row_stride, col_stride, padding,",
          "471:         explicit_paddings, &casted_in_backprop, data_format);",
          "473:     functor::CastFunctor<GPUDevice, Eigen::bfloat16, float> cast_back;",
          "474:     const Tensor& casted_in_backprop_const = casted_in_backprop;",
          "475:     cast_back(device, in_backprop->template flat<Eigen::bfloat16>(),",
          "476:               casted_in_backprop_const.template flat<float>());",
          "477:     return;",
          "478:   }",
          "480:   LaunchConv2DBackpropInputOp<Eigen::GpuDevice, Eigen::bfloat16>()(",
          "481:       ctx, use_cudnn, cudnn_use_autotune, casted_out_backprop, casted_filter,",
          "482:       row_dilation, col_dilation, row_stride, col_stride, padding,",
          "483:       explicit_paddings, &casted_in_backprop, data_format);",
          "484: }",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "504: DECLARE_GPU_SPEC(float);",
          "505: DECLARE_GPU_SPEC(Eigen::half);",
          "507: DECLARE_GPU_SPEC(double);",
          "508: #undef DECLARE_GPU_SPEC",
          "",
          "[Removed Lines]",
          "506: DECLARE_GPU_SPEC(Eigen::bfloat16);",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "545:                             .TypeConstraint<Eigen::half>(\"T\")",
          "546:                             .HostMemory(\"input_sizes\"),",
          "547:                         Conv2DBackpropInputOp<GPUDevice, Eigen::half>);",
          "553: REGISTER_KERNEL_BUILDER(Name(\"Conv2DBackpropInput\")",
          "554:                             .Device(DEVICE_GPU)",
          "555:                             .TypeConstraint<int32>(\"T\")",
          "",
          "[Removed Lines]",
          "548: REGISTER_KERNEL_BUILDER(Name(\"Conv2DBackpropInput\")",
          "549:                             .Device(DEVICE_GPU)",
          "550:                             .TypeConstraint<Eigen::bfloat16>(\"T\")",
          "551:                             .HostMemory(\"input_sizes\"),",
          "552:                         Conv2DBackpropInputOp<GPUDevice, Eigen::bfloat16>);",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "tensorflow/core/kernels/conv_grad_ops_3d.cc||tensorflow/core/kernels/conv_grad_ops_3d.cc": [
          "File: tensorflow/core/kernels/conv_grad_ops_3d.cc -> tensorflow/core/kernels/conv_grad_ops_3d.cc",
          "--- Hunk 1 ---",
          "[Context before]",
          "44: #endif",
          "46: #if GOOGLE_CUDA || TENSORFLOW_USE_ROCM",
          "48: #include \"tensorflow/core/platform/stream_executor.h\"",
          "49: using stream_executor::dnn::DimIndex;",
          "50: #include \"tensorflow/core/protobuf/autotuning.pb.h\"",
          "",
          "[Removed Lines]",
          "47: #include \"tensorflow/core/kernels/cast_op.h\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1234:       const T& padding_value);",
          "1236: DECLARE_GPU_SPEC(Eigen::half);",
          "1238: DECLARE_GPU_SPEC(float);",
          "1239: DECLARE_GPU_SPEC(double);",
          "1240: #undef DECLARE_GPU_SPEC",
          "",
          "[Removed Lines]",
          "1237: DECLARE_GPU_SPEC(Eigen::bfloat16);",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "1251:     AutotuneConv3dBwdData;",
          "1253: template <typename T>",
          "1260:     const TensorShape& filter_shape = filter.shape();",
          "1261:     const TensorShape& out_backprop_shape = out_backprop.shape();",
          "1264:     ConvBackpropDimensions dims;",
          "1265:     OP_REQUIRES_OK(context, ConvBackpropComputeDimensionsV2(",
          "1266:                                 \"Conv3DBackpropInputOp\", /*num_spatial_dims=*/3,",
          "1267:                                 input_shape, filter_shape, out_backprop_shape,",
          "1271:     auto* stream = context->op_device_context()->stream();",
          "1272:     OP_REQUIRES(context, stream, errors::Internal(\"No GPU stream available.\"));",
          "",
          "[Removed Lines]",
          "1254: struct LaunchConvBackpropInputOp {",
          "1255:   static void launch(OpKernelContext* context, bool cudnn_use_autotune,",
          "1256:                      const Tensor& out_backprop, const Tensor& filter,",
          "1257:                      const std::vector<int32>& dilation,",
          "1258:                      const std::vector<int32>& stride, const Padding& padding,",
          "1259:                      Tensor* in_backprop, TensorFormat data_format) {",
          "1262:     const TensorShape& input_shape = in_backprop->shape();",
          "1268:                                 dilation, stride, padding,",
          "",
          "[Added Lines]",
          "1252: class Conv3DBackpropInputOp<GPUDevice, T> : public OpKernel {",
          "1253:  public:",
          "1254:   explicit Conv3DBackpropInputOp(OpKernelConstruction* context)",
          "1255:       : OpKernel(context),",
          "1256:         data_format_(FORMAT_NHWC),",
          "1257:         takes_shape_(type_string().find(\"V2\") != std::string::npos) {",
          "1259:     if (takes_shape_) {",
          "1260:       string data_format;",
          "1261:       OP_REQUIRES_OK(context, context->GetAttr(\"data_format\", &data_format));",
          "1262:       OP_REQUIRES(context, FormatFromString(data_format, &data_format_),",
          "1263:                   errors::InvalidArgument(\"Invalid data format\"));",
          "1264:     }",
          "1265:     OP_REQUIRES_OK(context, context->GetAttr(\"dilations\", &dilation_));",
          "1266:     OP_REQUIRES(context, dilation_.size() == 5,",
          "1267:                 errors::InvalidArgument(\"Dilation rates field must \"",
          "1268:                                         \"specify 5 dimensions\"));",
          "1269:     OP_REQUIRES(context,",
          "1270:                 (GetTensorDim(dilation_, data_format_, 'C') == 1 &&",
          "1271:                  GetTensorDim(dilation_, data_format_, 'N') == 1),",
          "1272:                 errors::InvalidArgument(",
          "1273:                     \"Current implementation does not yet support \"",
          "1274:                     \"dilation rates in the batch and depth dimensions.\"));",
          "1275:     OP_REQUIRES(",
          "1276:         context,",
          "1277:         (GetTensorDim(dilation_, data_format_, '0') > 0 &&",
          "1278:          GetTensorDim(dilation_, data_format_, '1') > 0 &&",
          "1279:          GetTensorDim(dilation_, data_format_, '2') > 0),",
          "1280:         errors::InvalidArgument(\"Dilated rates should be larger than 0.\"));",
          "1281:     OP_REQUIRES_OK(context, context->GetAttr(\"strides\", &stride_));",
          "1282:     OP_REQUIRES(context, stride_.size() == 5,",
          "1283:                 errors::InvalidArgument(\"Sliding window strides field must \"",
          "1284:                                         \"specify 5 dimensions\"));",
          "1285:     OP_REQUIRES(",
          "1286:         context,",
          "1287:         (GetTensorDim(stride_, data_format_, 'C') == 1 &&",
          "1288:          GetTensorDim(stride_, data_format_, 'N') == 1),",
          "1289:         errors::InvalidArgument(\"Current implementation does not yet support \"",
          "1290:                                 \"strides in the batch and depth dimensions.\"));",
          "1291:     OP_REQUIRES(",
          "1292:         context,",
          "1293:         (GetTensorDim(stride_, data_format_, '0') > 0 &&",
          "1294:          GetTensorDim(stride_, data_format_, '1') > 0 &&",
          "1295:          GetTensorDim(stride_, data_format_, '2') > 0),",
          "1296:         errors::InvalidArgument(\"Spatial strides should be larger than 0.\"));",
          "1297:     OP_REQUIRES_OK(context, context->GetAttr(\"padding\", &padding_));",
          "1298:     cudnn_use_autotune_ = CudnnUseAutotune();",
          "1299:   }",
          "1300:   void Compute(OpKernelContext* context) override {",
          "1301:     const Tensor& filter = context->input(1);",
          "1304:     const Tensor& out_backprop = context->input(2);",
          "1307:     TensorShape input_shape;",
          "1308:     if (takes_shape_) {",
          "1309:       const Tensor& input_sizes = context->input(0);",
          "1310:       OP_REQUIRES_OK(context, tensor::MakeShape(input_sizes, &input_shape));",
          "1311:     } else {",
          "1312:       input_shape = context->input(0).shape();",
          "1313:     }",
          "1319:                                 dilation_, stride_, padding_,",
          "1322:     Tensor* in_backprop;",
          "1323:     OP_REQUIRES_OK(context,",
          "1324:                    context->allocate_output(0, input_shape, &in_backprop));",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "1276:         dims.filter_size(1) == 1 && dims.filter_size(2) == 1 &&",
          "1277:         dims.dilation(0) == 1 && dims.dilation(1) == 1 &&",
          "1278:         dims.dilation(2) == 1 && dims.stride(0) == 1 && dims.stride(1) == 1 &&",
          "1280:       const uint64 m = dims.batch_size * dims.input_size(0) *",
          "1281:                        dims.input_size(1) * dims.input_size(2);",
          "1282:       const uint64 k = dims.out_depth;",
          "",
          "[Removed Lines]",
          "1279:         dims.stride(2) == 1 && data_format == FORMAT_NHWC) {",
          "",
          "[Added Lines]",
          "1334:         dims.stride(2) == 1 && data_format_ == FORMAT_NHWC) {",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "1301:                dims.filter_size(0) == dims.input_size(0) &&",
          "1302:                dims.filter_size(1) == dims.input_size(1) &&",
          "1303:                dims.filter_size(2) == dims.input_size(2) &&",
          "1305:       const uint64 m = dims.batch_size;",
          "1306:       const uint64 k = dims.out_depth;",
          "1307:       const uint64 n = dims.input_size(0) * dims.input_size(1) *",
          "",
          "[Removed Lines]",
          "1304:                padding == Padding::VALID && data_format == FORMAT_NHWC) {",
          "",
          "[Added Lines]",
          "1359:                padding_ == Padding::VALID && data_format_ == FORMAT_NHWC) {",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "1324:       return;",
          "1325:     }",
          "1330:     const bool planes_odd = (padding_planes % 2 != 0);",
          "1331:     const bool rows_odd = (padding_rows % 2 != 0);",
          "1332:     const bool cols_odd = (padding_cols % 2 != 0);",
          "",
          "[Removed Lines]",
          "1327:     int padding_planes = dims.SpatialPadding(padding, 0);",
          "1328:     int padding_rows = dims.SpatialPadding(padding, 1);",
          "1329:     int padding_cols = dims.SpatialPadding(padding, 2);",
          "",
          "[Added Lines]",
          "1382:     int padding_planes = dims.SpatialPadding(padding_, 0);",
          "1383:     int padding_rows = dims.SpatialPadding(padding_, 1);",
          "1384:     int padding_cols = dims.SpatialPadding(padding_, 2);",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "1354:     const bool compute_in_nhwc = ComputeInNhwcEnabled(",
          "1355:         DataTypeToEnum<T>::value, stream, /*use_4d_tensor=*/false);",
          "1357:     const TensorFormat compute_data_format =",
          "1361:     VLOG(3) << \"Compute Conv3DBackpropInput with cuDNN:\"",
          "1363:             << \" compute_data_format=\" << ToString(compute_data_format);",
          "1365:     constexpr auto kComputeInNHWC =",
          "",
          "[Removed Lines]",
          "1358:         (compute_in_nhwc && data_format == FORMAT_NHWC) ? FORMAT_NHWC",
          "1359:                                                         : FORMAT_NCHW;",
          "1362:             << \" data_format=\" << ToString(data_format)",
          "",
          "[Added Lines]",
          "1412:         (compute_in_nhwc && data_format_ == FORMAT_NHWC) ? FORMAT_NHWC",
          "1413:                                                          : FORMAT_NCHW;",
          "1416:             << \" data_format=\" << ToString(data_format_)",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "1433:     Tensor transformed_out_backprop;",
          "1435:       TensorShape nchw_shape = {dims.batch_size, dims.out_depth,",
          "1436:                                 dims.output_size(0), dims.output_size(1),",
          "1437:                                 dims.output_size(2)};",
          "",
          "[Removed Lines]",
          "1434:     if (data_format == FORMAT_NHWC && compute_data_format == FORMAT_NCHW) {",
          "",
          "[Added Lines]",
          "1488:     if (data_format_ == FORMAT_NHWC && compute_data_format == FORMAT_NCHW) {",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "1497:     using se::dnn::ProfileResult;",
          "1499:     auto entry_or = AutotuneUnfusedConv(",
          "1501:         conv_parameters, context, se::dnn::ConvolutionKind::BACKWARD_DATA,",
          "1502:         input_desc, in_backprop_ptr, filter_desc, filter_ptr, conv_desc,",
          "1503:         output_desc, out_backprop_ptr, ConvolveBackwardDataScratchSize);",
          "",
          "[Removed Lines]",
          "1500:         cudnn_use_autotune, AutotuneConv3dBwdData::GetInstance(),",
          "",
          "[Added Lines]",
          "1554:         cudnn_use_autotune_, AutotuneConv3dBwdData::GetInstance(),",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "1539:       pre_transformed_in_backprop = in_backprop_remove_padding;",
          "1540:     }",
          "1543:       auto toConstTensor = [](const Tensor& x) -> const Tensor { return x; };",
          "1544:       functor::NCHWToNHWC<GPUDevice, T, 5>()(",
          "1545:           context->eigen_device<GPUDevice>(),",
          "",
          "[Removed Lines]",
          "1542:     if (data_format == FORMAT_NHWC && compute_data_format == FORMAT_NCHW) {",
          "",
          "[Added Lines]",
          "1596:     if (data_format_ == FORMAT_NHWC && compute_data_format == FORMAT_NCHW) {",
          "",
          "---------------",
          "--- Hunk 11 ---",
          "[Context before]",
          "1550:     }",
          "1551:   }",
          "1601: };",
          "1603: template <typename T>",
          "1605:  public:",
          "1607:       : OpKernel(context),",
          "1608:         data_format_(FORMAT_NHWC),",
          "1609:         takes_shape_(type_string().find(\"V2\") != std::string::npos) {",
          "",
          "[Removed Lines]",
          "1552: };",
          "1554: template <>",
          "1555: struct LaunchConvBackpropInputOp<Eigen::bfloat16> {",
          "1556:   static void launch(OpKernelContext* ctx, bool cudnn_use_autotune,",
          "1557:                      const Tensor& out_backprop, const Tensor& filter,",
          "1558:                      const std::vector<int32>& dilation,",
          "1559:                      const std::vector<int32>& strides, const Padding& padding,",
          "1560:                      Tensor* in_backprop, TensorFormat data_format) {",
          "1563:     auto* stream = ctx->op_device_context()->stream();",
          "1564:     const bool cast_to_float = !stream->GetCudaComputeCapability().IsAtLeast(",
          "1565:         se::CudaComputeCapability::AMPERE);",
          "1566:     Tensor casted_out_backprop = out_backprop;",
          "1567:     Tensor casted_filter = filter;",
          "1568:     Tensor casted_in_backprop = *in_backprop;",
          "1570:     if (cast_to_float) {",
          "1571:       const GPUDevice& device = ctx->eigen_device<GPUDevice>();",
          "1572:       functor::CastFunctor<GPUDevice, float, Eigen::bfloat16> cast;",
          "1573:       OP_REQUIRES_OK(ctx, ctx->allocate_temp(DT_FLOAT, out_backprop.shape(),",
          "1574:                                              &casted_out_backprop));",
          "1575:       cast(device, casted_out_backprop.template flat<float>(),",
          "1576:            out_backprop.template flat<Eigen::bfloat16>());",
          "1578:       OP_REQUIRES_OK(",
          "1579:           ctx, ctx->allocate_temp(DT_FLOAT, filter.shape(), &casted_filter));",
          "1580:       cast(device, casted_filter.template flat<float>(),",
          "1581:            filter.template flat<Eigen::bfloat16>());",
          "1583:       OP_REQUIRES_OK(ctx, ctx->allocate_temp(DT_FLOAT, in_backprop->shape(),",
          "1584:                                              &casted_in_backprop));",
          "1586:       LaunchConvBackpropInputOp<float>::launch(",
          "1587:           ctx, cudnn_use_autotune, casted_out_backprop, casted_filter, dilation,",
          "1588:           strides, padding, &casted_in_backprop, data_format);",
          "1590:       functor::CastFunctor<GPUDevice, Eigen::bfloat16, float> cast_back;",
          "1591:       const Tensor& casted_in_backprop_const = casted_in_backprop;",
          "1592:       cast_back(device, in_backprop->template flat<Eigen::bfloat16>(),",
          "1593:                 casted_in_backprop_const.template flat<float>());",
          "1594:       return;",
          "1595:     }",
          "1597:     LaunchConvBackpropInputOp<Eigen::bfloat16>::launch(",
          "1598:         ctx, cudnn_use_autotune, casted_out_backprop, casted_filter, dilation,",
          "1599:         strides, padding, &casted_in_backprop, data_format);",
          "1600:   }",
          "1604: class Conv3DBackpropInputOp<GPUDevice, T> : public OpKernel {",
          "1606:   explicit Conv3DBackpropInputOp(OpKernelConstruction* context)",
          "",
          "[Added Lines]",
          "1607:  private:",
          "1608:   std::vector<int32> dilation_;",
          "1609:   std::vector<int32> stride_;",
          "1610:   Padding padding_;",
          "1611:   TensorFormat data_format_;",
          "1612:   bool takes_shape_;",
          "1613:   bool cudnn_use_autotune_;",
          "1614: };",
          "1617: struct Conv3dBackwardFilterAutotuneGroup {",
          "1618:   static string name() { return \"Conv3dBwdFilter\"; }",
          "1621: typedef AutotuneSingleton<Conv3dBackwardFilterAutotuneGroup, ConvParameters,",
          "1622:                           AutotuneEntry<se::dnn::ConvOp>>",
          "1623:     AutotuneConv3dBwdFilter;",
          "1626: class Conv3DBackpropFilterOp<GPUDevice, T> : public OpKernel {",
          "1628:   explicit Conv3DBackpropFilterOp(OpKernelConstruction* context)",
          "",
          "---------------",
          "--- Hunk 12 ---",
          "[Context before]",
          "1649:     OP_REQUIRES_OK(context, context->GetAttr(\"padding\", &padding_));",
          "1650:     cudnn_use_autotune_ = CudnnUseAutotune();",
          "1651:   }",
          "1652:   void Compute(OpKernelContext* context) override {",
          "1655:     const Tensor& out_backprop = context->input(2);",
          "1658:     if (takes_shape_) {",
          "1661:     } else {",
          "1663:     }",
          "1703:     ConvBackpropDimensions dims;",
          "1704:     OP_REQUIRES_OK(",
          "1705:         context,",
          "1706:         ConvBackpropComputeDimensionsV2(",
          "1707:             \"Conv3DBackpropFilterOp\", /*num_spatial_dims=*/3, input_shape,",
          "1711:     auto* stream = context->op_device_context()->stream();",
          "1712:     OP_REQUIRES(context, stream, errors::Internal(\"No GPU stream available.\"));",
          "1723:     bool is_grouped_convolution = filter_shape.dim_size(3) != dims.in_depth;",
          "1724:     if (!is_grouped_convolution && dims.filter_size(1) == 1 &&",
          "1725:         dims.filter_size(2) == 1 && dims.filter_size(0) == 1 &&",
          "1726:         dims.dilation(2) == 1 && dims.dilation(1) == 1 &&",
          "1727:         dims.dilation(0) == 1 && dims.stride(2) == 1 && dims.stride(1) == 1 &&",
          "1729:       const uint64 m = dims.in_depth;",
          "1730:       const uint64 k = dims.batch_size * dims.input_size(1) *",
          "1731:                        dims.input_size(2) * dims.input_size(0);",
          "",
          "[Removed Lines]",
          "1653:     const Tensor& filter = context->input(1);",
          "1657:     TensorShape input_shape;",
          "1659:       const Tensor& input_sizes = context->input(0);",
          "1660:       OP_REQUIRES_OK(context, tensor::MakeShape(input_sizes, &input_shape));",
          "1662:       input_shape = context->input(0).shape();",
          "1665:     Tensor* in_backprop;",
          "1666:     OP_REQUIRES_OK(context,",
          "1667:                    context->allocate_output(0, input_shape, &in_backprop));",
          "1669:     LaunchConvBackpropInputOp<T>::launch(",
          "1670:         context, cudnn_use_autotune_, out_backprop, filter, dilation_, stride_,",
          "1671:         padding_, in_backprop, data_format_);",
          "1672:   }",
          "1674:  private:",
          "1675:   std::vector<int32> dilation_;",
          "1676:   std::vector<int32> stride_;",
          "1677:   Padding padding_;",
          "1678:   TensorFormat data_format_;",
          "1679:   bool takes_shape_;",
          "1680:   bool cudnn_use_autotune_;",
          "1681: };",
          "1684: struct Conv3dBackwardFilterAutotuneGroup {",
          "1685:   static string name() { return \"Conv3dBwdFilter\"; }",
          "1686: };",
          "1688: typedef AutotuneSingleton<Conv3dBackwardFilterAutotuneGroup, ConvParameters,",
          "1689:                           AutotuneEntry<se::dnn::ConvOp>>",
          "1690:     AutotuneConv3dBwdFilter;",
          "1692: template <typename T>",
          "1693: struct LaunchConvBackpropFilterOp {",
          "1694:   static void launch(OpKernelContext* context, bool cudnn_use_autotune,",
          "1695:                      const Tensor& input, const Tensor& out_backprop,",
          "1696:                      const std::vector<int32>& dilation,",
          "1697:                      const std::vector<int32>& stride, const Padding& padding,",
          "1698:                      Tensor* filter_backprop, TensorFormat data_format) {",
          "1699:     const TensorShape& input_shape = input.shape();",
          "1700:     const TensorShape& out_backprop_shape = out_backprop.shape();",
          "1701:     const TensorShape& filter_shape = filter_backprop->shape();",
          "1708:             filter_shape, out_backprop_shape, dilation, stride, padding,",
          "1714:     if (DataTypeToEnum<T>::value == DT_BFLOAT16 &&",
          "1715:         !stream->GetCudaComputeCapability().IsAtLeast(",
          "1716:             se::CudaComputeCapability::AMPERE)) {",
          "1717:       context->SetStatus(errors::Unimplemented(",
          "1718:           \"Conv3DBackpropFilter for GPU with bfloat16 is only supported \"",
          "1719:           \"with cuDNN on Ampere GPUs or later.\"));",
          "1720:       return;",
          "1721:     }",
          "1728:         dims.stride(0) == 1 && data_format == FORMAT_NHWC) {",
          "",
          "[Added Lines]",
          "1676:     const Tensor& input = context->input(0);",
          "1677:     const TensorShape& input_shape = input.shape();",
          "1680:     const TensorShape& out_backprop_shape = out_backprop.shape();",
          "1682:     TensorShape filter_shape;",
          "1684:       const Tensor& filter_sizes = context->input(1);",
          "1685:       OP_REQUIRES(context, TensorShapeUtils::IsVector(filter_sizes.shape()),",
          "1686:                   errors::InvalidArgument(",
          "1687:                       \"filter_sizes shape must be rank 1 but is rank \",",
          "1688:                       filter_sizes.shape().dims()));",
          "1689:       OP_REQUIRES_OK(context, tensor::MakeShape(filter_sizes, &filter_shape));",
          "1691:       filter_shape = context->input(1).shape();",
          "1699:             filter_shape, out_backprop_shape, dilation_, stride_, padding_,",
          "1702:     Tensor* filter_backprop;",
          "1703:     OP_REQUIRES_OK(context,",
          "1704:                    context->allocate_output(0, filter_shape, &filter_backprop));",
          "1714:         dims.stride(0) == 1 && data_format_ == FORMAT_NHWC) {",
          "",
          "---------------",
          "--- Hunk 13 ---",
          "[Context before]",
          "1759:                dims.filter_size(0) == dims.input_size(0) &&",
          "1760:                dims.filter_size(1) == dims.input_size(1) &&",
          "1761:                dims.filter_size(2) == dims.input_size(2) &&",
          "1763:       const uint64 m = dims.input_size(0) * dims.input_size(1) *",
          "1764:                        dims.input_size(2) * dims.in_depth;",
          "1765:       const uint64 k = dims.batch_size;",
          "",
          "[Removed Lines]",
          "1762:                padding == Padding::VALID && data_format == FORMAT_NHWC) {",
          "",
          "[Added Lines]",
          "1748:                padding_ == Padding::VALID && data_format_ == FORMAT_NHWC) {",
          "",
          "---------------",
          "--- Hunk 14 ---",
          "[Context before]",
          "1780:       return;",
          "1781:     }",
          "1786:     const bool planes_odd = (padding_planes % 2 != 0);",
          "1787:     const bool rows_odd = (padding_rows % 2 != 0);",
          "1788:     const bool cols_odd = (padding_cols % 2 != 0);",
          "",
          "[Removed Lines]",
          "1783:     int padding_planes = dims.SpatialPadding(padding, 0);",
          "1784:     int padding_rows = dims.SpatialPadding(padding, 1);",
          "1785:     int padding_cols = dims.SpatialPadding(padding, 2);",
          "",
          "[Added Lines]",
          "1769:     int padding_planes = dims.SpatialPadding(padding_, 0);",
          "1770:     int padding_rows = dims.SpatialPadding(padding_, 1);",
          "1771:     int padding_cols = dims.SpatialPadding(padding_, 2);",
          "",
          "---------------",
          "--- Hunk 15 ---",
          "[Context before]",
          "1792:       OP_REQUIRES_OK(context,",
          "1793:                      context->allocate_temp(",
          "1794:                          DataTypeToEnum<T>::value,",
          "1796:                                          {{dims.input_size(0) + planes_odd,",
          "1797:                                            dims.input_size(1) + rows_odd,",
          "1798:                                            dims.input_size(2) + cols_odd}},",
          "",
          "[Removed Lines]",
          "1795:                          ShapeFromFormat(data_format, dims.batch_size,",
          "",
          "[Added Lines]",
          "1781:                          ShapeFromFormat(data_format_, dims.batch_size,",
          "",
          "---------------",
          "--- Hunk 16 ---",
          "[Context before]",
          "1802:           context->template eigen_device<GPUDevice>(),",
          "1803:           To32Bit(input.tensor<T, 5>()), {{0, 0, 0}},",
          "1804:           {{planes_odd, rows_odd, cols_odd}},",
          "1806:     } else {",
          "1807:       compatible_input = input;",
          "1808:     }",
          "",
          "[Removed Lines]",
          "1805:           To32Bit(compatible_input.tensor<T, 5>()), data_format, T{});",
          "",
          "[Added Lines]",
          "1791:           To32Bit(compatible_input.tensor<T, 5>()), data_format_, T{});",
          "",
          "---------------",
          "--- Hunk 17 ---",
          "[Context before]",
          "1819:     const bool compute_in_nhwc = false;",
          "1820: #endif",
          "1821:     const TensorFormat compute_data_format =",
          "1825:     VLOG(3) << \"Compute Conv3DBackpropFilter with cuDNN:\"",
          "1827:             << \" compute_data_format=\" << ToString(compute_data_format);",
          "1829:     constexpr auto kComputeInNHWC =",
          "",
          "[Removed Lines]",
          "1822:         (compute_in_nhwc && data_format == FORMAT_NHWC) ? FORMAT_NHWC",
          "1823:                                                         : FORMAT_NCHW;",
          "1826:             << \" data_format=\" << ToString(data_format)",
          "",
          "[Added Lines]",
          "1808:         (compute_in_nhwc && data_format_ == FORMAT_NHWC) ? FORMAT_NHWC",
          "1809:                                                          : FORMAT_NCHW;",
          "1812:             << \" data_format=\" << ToString(data_format_)",
          "",
          "---------------",
          "--- Hunk 18 ---",
          "[Context before]",
          "1842:     se::dnn::BatchDescriptor input_desc(3);",
          "1843:     input_desc.set_count(dims.batch_size)",
          "1844:         .set_spatial_dim(DimIndex::X,",
          "1846:         .set_spatial_dim(DimIndex::Y,",
          "1848:         .set_spatial_dim(DimIndex::Z,",
          "1850:         .set_feature_map_count(dims.in_depth)",
          "1851:         .set_layout(compute_data_layout);",
          "1852:     se::dnn::BatchDescriptor output_desc(3);",
          "",
          "[Removed Lines]",
          "1845:                          GetTensorDim(compatible_input, data_format, '2'))",
          "1847:                          GetTensorDim(compatible_input, data_format, '1'))",
          "1849:                          GetTensorDim(compatible_input, data_format, '0'))",
          "",
          "[Added Lines]",
          "1831:                          GetTensorDim(compatible_input, data_format_, '2'))",
          "1833:                          GetTensorDim(compatible_input, data_format_, '1'))",
          "1835:                          GetTensorDim(compatible_input, data_format_, '0'))",
          "",
          "---------------",
          "--- Hunk 19 ---",
          "[Context before]",
          "1891:                                           &pre_transformed_filter_backprop));",
          "1893:     Tensor transformed_out_backprop;",
          "1895:       VLOG(4) << \"Convert the `out_backprop` tensor from NDHWC to NCDHW.\";",
          "1896:       TensorShape nchw_shape = {dims.batch_size, dims.out_depth,",
          "1897:                                 dims.output_size(0), dims.output_size(1),",
          "",
          "[Removed Lines]",
          "1894:     if (data_format == FORMAT_NHWC && compute_data_format == FORMAT_NCHW) {",
          "",
          "[Added Lines]",
          "1880:     if (data_format_ == FORMAT_NHWC && compute_data_format == FORMAT_NCHW) {",
          "",
          "---------------",
          "--- Hunk 20 ---",
          "[Context before]",
          "1910:       transformed_out_backprop = out_backprop;",
          "1911:     }",
          "1912:     Tensor transformed_input;",
          "1914:       VLOG(4) << \"Convert the `input` tensor from NDHWC to NCDHW.\";",
          "1915:       TensorShape nchw_shape = {",
          "1916:           dims.batch_size, dims.in_depth, compatible_input.dim_size(1),",
          "",
          "[Removed Lines]",
          "1913:     if (data_format == FORMAT_NHWC && compute_data_format == FORMAT_NCHW) {",
          "",
          "[Added Lines]",
          "1899:     if (data_format_ == FORMAT_NHWC && compute_data_format == FORMAT_NCHW) {",
          "",
          "---------------",
          "--- Hunk 21 ---",
          "[Context before]",
          "1964:     using se::dnn::ProfileResult;",
          "1966:     auto entry_or = AutotuneUnfusedConv(",
          "1968:         conv_parameters, context, se::dnn::ConvolutionKind::BACKWARD_FILTER,",
          "1969:         input_desc, input_ptr, filter_desc, filter_backprop_ptr, conv_desc,",
          "1970:         output_desc, out_backprop_ptr, ConvolveBackwardFilterScratchSize);",
          "",
          "[Removed Lines]",
          "1967:         cudnn_use_autotune, AutotuneConv3dBwdFilter::GetInstance(),",
          "",
          "[Added Lines]",
          "1953:         cudnn_use_autotune_, AutotuneConv3dBwdFilter::GetInstance(),",
          "",
          "---------------",
          "--- Hunk 22 ---",
          "[Context before]",
          "1989:         toConstTensor(pre_transformed_filter_backprop).template tensor<T, 5>(),",
          "1990:         filter_backprop->tensor<T, 5>());",
          "1991:   }",
          "2118:  private:",
          "2119:   std::vector<int32> dilation_;",
          "",
          "[Removed Lines]",
          "1992: };",
          "1994: template <>",
          "1995: struct LaunchConvBackpropFilterOp<Eigen::bfloat16> {",
          "1996:   static void launch(OpKernelContext* ctx, bool cudnn_use_autotune,",
          "1997:                      const Tensor& input, const Tensor& out_backprop,",
          "1998:                      const std::vector<int32>& dilation,",
          "1999:                      const std::vector<int32>& stride, const Padding& padding,",
          "2000:                      Tensor* filter_backprop, TensorFormat data_format) {",
          "2003:     auto* stream = ctx->op_device_context()->stream();",
          "2004:     const bool cast_to_float = !stream->GetCudaComputeCapability().IsAtLeast(",
          "2005:         se::CudaComputeCapability::AMPERE);",
          "2006:     Tensor casted_input = input;",
          "2007:     Tensor casted_out_backprop = out_backprop;",
          "2008:     Tensor casted_filter_backprop = *filter_backprop;",
          "2010:     if (cast_to_float) {",
          "2011:       const GPUDevice& device = ctx->eigen_device<GPUDevice>();",
          "2012:       functor::CastFunctor<GPUDevice, float, Eigen::bfloat16> cast;",
          "2013:       OP_REQUIRES_OK(",
          "2014:           ctx, ctx->allocate_temp(DT_FLOAT, input.shape(), &casted_input));",
          "2015:       cast(device, casted_input.template flat<float>(),",
          "2016:            input.template flat<Eigen::bfloat16>());",
          "2018:       OP_REQUIRES_OK(ctx, ctx->allocate_temp(DT_FLOAT, out_backprop.shape(),",
          "2019:                                              &casted_out_backprop));",
          "2020:       cast(device, casted_out_backprop.template flat<float>(),",
          "2021:            out_backprop.template flat<Eigen::bfloat16>());",
          "2023:       OP_REQUIRES_OK(ctx, ctx->allocate_temp(DT_FLOAT, filter_backprop->shape(),",
          "2024:                                              &casted_filter_backprop));",
          "2026:       LaunchConvBackpropFilterOp<float>::launch(",
          "2027:           ctx, cudnn_use_autotune, casted_input, casted_out_backprop, dilation,",
          "2028:           stride, padding, &casted_filter_backprop, data_format);",
          "2030:       functor::CastFunctor<GPUDevice, Eigen::bfloat16, float> cast_back;",
          "2031:       const Tensor& casted_filter_backprop_const = casted_filter_backprop;",
          "2032:       cast_back(device, filter_backprop->template flat<Eigen::bfloat16>(),",
          "2033:                 casted_filter_backprop_const.template flat<float>());",
          "2034:       return;",
          "2035:     }",
          "2037:     LaunchConvBackpropFilterOp<Eigen::bfloat16>::launch(",
          "2038:         ctx, cudnn_use_autotune, casted_input, casted_out_backprop, dilation,",
          "2039:         stride, padding, &casted_filter_backprop, data_format);",
          "2040:   }",
          "2041: };",
          "2043: template <typename T>",
          "2044: class Conv3DBackpropFilterOp<GPUDevice, T> : public OpKernel {",
          "2045:  public:",
          "2046:   explicit Conv3DBackpropFilterOp(OpKernelConstruction* context)",
          "2047:       : OpKernel(context),",
          "2048:         data_format_(FORMAT_NHWC),",
          "2049:         takes_shape_(type_string().find(\"V2\") != std::string::npos) {",
          "2051:     if (takes_shape_) {",
          "2052:       string data_format;",
          "2053:       OP_REQUIRES_OK(context, context->GetAttr(\"data_format\", &data_format));",
          "2054:       OP_REQUIRES(context, FormatFromString(data_format, &data_format_),",
          "2055:                   errors::InvalidArgument(\"Invalid data format\"));",
          "2056:     }",
          "2057:     OP_REQUIRES_OK(context, context->GetAttr(\"dilations\", &dilation_));",
          "2058:     OP_REQUIRES(context, dilation_.size() == 5,",
          "2059:                 errors::InvalidArgument(\"Dilation rates field must \"",
          "2060:                                         \"specify 5 dimensions\"));",
          "2061:     OP_REQUIRES(context,",
          "2062:                 (GetTensorDim(dilation_, data_format_, 'C') == 1 &&",
          "2063:                  GetTensorDim(dilation_, data_format_, 'N') == 1),",
          "2064:                 errors::InvalidArgument(",
          "2065:                     \"Current implementation does not yet support \"",
          "2066:                     \"dilation rates in the batch and depth dimensions.\"));",
          "2067:     OP_REQUIRES(",
          "2068:         context,",
          "2069:         (GetTensorDim(dilation_, data_format_, '0') > 0 &&",
          "2070:          GetTensorDim(dilation_, data_format_, '1') > 0 &&",
          "2071:          GetTensorDim(dilation_, data_format_, '2') > 0),",
          "2072:         errors::InvalidArgument(\"Dilated rates should be larger than 0.\"));",
          "2073:     OP_REQUIRES_OK(context, context->GetAttr(\"strides\", &stride_));",
          "2074:     OP_REQUIRES(context, stride_.size() == 5,",
          "2075:                 errors::InvalidArgument(\"Sliding window strides field must \"",
          "2076:                                         \"specify 5 dimensions\"));",
          "2077:     OP_REQUIRES(",
          "2078:         context,",
          "2079:         (GetTensorDim(stride_, data_format_, 'C') == 1 &&",
          "2080:          GetTensorDim(stride_, data_format_, 'N') == 1),",
          "2081:         errors::InvalidArgument(\"Current implementation does not yet support \"",
          "2082:                                 \"strides in the batch and depth dimensions.\"));",
          "2083:     OP_REQUIRES(",
          "2084:         context,",
          "2085:         (GetTensorDim(stride_, data_format_, '0') > 0 &&",
          "2086:          GetTensorDim(stride_, data_format_, '1') > 0 &&",
          "2087:          GetTensorDim(stride_, data_format_, '2') > 0),",
          "2088:         errors::InvalidArgument(\"Spatial strides should be larger than 0.\"));",
          "2089:     OP_REQUIRES_OK(context, context->GetAttr(\"padding\", &padding_));",
          "2090:     cudnn_use_autotune_ = CudnnUseAutotune();",
          "2091:   }",
          "2093:   void Compute(OpKernelContext* context) override {",
          "2094:     const Tensor& input = context->input(0);",
          "2095:     const Tensor& out_backprop = context->input(2);",
          "2097:     TensorShape filter_shape;",
          "2098:     if (takes_shape_) {",
          "2099:       const Tensor& filter_sizes = context->input(1);",
          "2100:       OP_REQUIRES(context, TensorShapeUtils::IsVector(filter_sizes.shape()),",
          "2101:                   errors::InvalidArgument(",
          "2102:                       \"filter_sizes shape must be rank 1 but is rank \",",
          "2103:                       filter_sizes.shape().dims()));",
          "2104:       OP_REQUIRES_OK(context, tensor::MakeShape(filter_sizes, &filter_shape));",
          "2105:     } else {",
          "2106:       filter_shape = context->input(1).shape();",
          "2107:     }",
          "2109:     Tensor* filter_backprop;",
          "2110:     OP_REQUIRES_OK(context,",
          "2111:                    context->allocate_output(0, filter_shape, &filter_backprop));",
          "2113:     LaunchConvBackpropFilterOp<T>::launch(",
          "2114:         context, cudnn_use_autotune_, input, out_backprop, dilation_, stride_,",
          "2115:         padding_, filter_backprop, data_format_);",
          "2116:   }",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 23 ---",
          "[Context before]",
          "2142:                               .HostMemory(\"filter_sizes\"),                    \\",
          "2143:                           Conv3DBackpropFilterOp<GPUDevice, T>);",
          "2144: TF_CALL_half(REGISTER_GPU_KERNEL);",
          "2146: TF_CALL_float(REGISTER_GPU_KERNEL);",
          "2147: TF_CALL_double(REGISTER_GPU_KERNEL);",
          "2148: #undef REGISTER_GPU_KERNEL",
          "",
          "[Removed Lines]",
          "2145: TF_CALL_bfloat16(REGISTER_GPU_KERNEL);",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "tensorflow/core/kernels/conv_ops.cc||tensorflow/core/kernels/conv_ops.cc": [
          "File: tensorflow/core/kernels/conv_ops.cc -> tensorflow/core/kernels/conv_ops.cc",
          "--- Hunk 1 ---",
          "[Context before]",
          "58: #include \"tensorflow/core/util/use_cudnn.h\"",
          "60: #if GOOGLE_CUDA || TENSORFLOW_USE_ROCM",
          "62: #include \"tensorflow/core/kernels/conv_ops_gpu.h\"",
          "63: #include \"tensorflow/core/platform/stream_executor.h\"",
          "64: #include \"tensorflow/core/protobuf/autotuning.pb.h\"",
          "",
          "[Removed Lines]",
          "61: #include \"tensorflow/core/kernels/cast_op.h\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1078:   }",
          "1079: }",
          "1132: namespace functor {",
          "1133: #define DECLARE_GPU_SPEC(T)                                                 \\",
          "",
          "[Removed Lines]",
          "1081: template <>",
          "1082: void LaunchConv2DOp<GPUDevice, Eigen::bfloat16>::operator()(",
          "1083:     OpKernelContext* ctx, bool use_cudnn, bool cudnn_use_autotune,",
          "1084:     const Tensor& input_param, const Tensor& filter, int row_dilation,",
          "1085:     int col_dilation, int row_stride, int col_stride, const Padding& padding,",
          "1086:     const std::vector<int64_t>& explicit_paddings, Tensor* output,",
          "1087:     TensorFormat data_format) {",
          "1090:   auto* stream = ctx->op_device_context()->stream();",
          "1091:   const bool cast_to_float = !stream->GetCudaComputeCapability().IsAtLeast(",
          "1092:       se::CudaComputeCapability::AMPERE);",
          "1093:   Tensor casted_input = input_param;",
          "1094:   Tensor casted_filter = filter;",
          "1095:   Tensor casted_out = *output;",
          "1097:   if (cast_to_float) {",
          "1098:     const GPUDevice& device = ctx->eigen_device<GPUDevice>();",
          "1099:     functor::CastFunctor<GPUDevice, float, Eigen::bfloat16> cast;",
          "1100:     OP_REQUIRES_OK(",
          "1101:         ctx, ctx->allocate_temp(DT_FLOAT, input_param.shape(), &casted_input));",
          "1102:     cast(device, casted_input.template flat<float>(),",
          "1103:          input_param.template flat<Eigen::bfloat16>());",
          "1105:     OP_REQUIRES_OK(",
          "1106:         ctx, ctx->allocate_temp(DT_FLOAT, filter.shape(), &casted_filter));",
          "1107:     cast(device, casted_filter.template flat<float>(),",
          "1108:          filter.template flat<Eigen::bfloat16>());",
          "1110:     OP_REQUIRES_OK(ctx,",
          "1111:                    ctx->allocate_temp(DT_FLOAT, output->shape(), &casted_out));",
          "1113:     LaunchConv2DOp<GPUDevice, float>()(",
          "1114:         ctx, use_cudnn, cudnn_use_autotune, casted_input, casted_filter,",
          "1115:         row_dilation, col_dilation, row_stride, col_stride, padding,",
          "1116:         explicit_paddings, &casted_out, data_format);",
          "1118:     functor::CastFunctor<GPUDevice, Eigen::bfloat16, float> cast_back;",
          "1119:     const Tensor& casted_out_const = casted_out;",
          "1120:     cast_back(device, output->template flat<Eigen::bfloat16>(),",
          "1121:               casted_out_const.template flat<float>());",
          "1122:     return;",
          "1123:   }",
          "1125:   LaunchConv2DOp<GPUDevice, Eigen::bfloat16>()(",
          "1126:       ctx, use_cudnn, cudnn_use_autotune, casted_input, casted_filter,",
          "1127:       row_dilation, col_dilation, row_stride, col_stride, padding,",
          "1128:       explicit_paddings, &casted_out, data_format);",
          "1129: }",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "1174: DECLARE_GPU_SPEC(float);",
          "1175: DECLARE_GPU_SPEC(Eigen::half);",
          "1177: DECLARE_GPU_SPEC(double);",
          "1178: DECLARE_GPU_SPEC(int32);",
          "1179: #undef DECLARE_GPU_SPEC",
          "",
          "[Removed Lines]",
          "1176: DECLARE_GPU_SPEC(Eigen::bfloat16);",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "1184: REGISTER_KERNEL_BUILDER(",
          "1185:     Name(\"Conv2D\").Device(DEVICE_GPU).TypeConstraint<Eigen::half>(\"T\"),",
          "1186:     Conv2DOp<GPUDevice, Eigen::half>);",
          "1190: REGISTER_KERNEL_BUILDER(",
          "1191:     Name(\"Conv2D\").Device(DEVICE_GPU).TypeConstraint<float>(\"T\"),",
          "1192:     Conv2DOp<GPUDevice, float>);",
          "",
          "[Removed Lines]",
          "1187: REGISTER_KERNEL_BUILDER(",
          "1188:     Name(\"Conv2D\").Device(DEVICE_GPU).TypeConstraint<Eigen::bfloat16>(\"T\"),",
          "1189:     Conv2DOp<GPUDevice, Eigen::bfloat16>);",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "tensorflow/core/kernels/conv_ops_3d.cc||tensorflow/core/kernels/conv_ops_3d.cc": [
          "File: tensorflow/core/kernels/conv_ops_3d.cc -> tensorflow/core/kernels/conv_ops_3d.cc",
          "--- Hunk 1 ---",
          "[Context before]",
          "36: #include \"tensorflow/core/util/use_cudnn.h\"",
          "38: #if GOOGLE_CUDA || TENSORFLOW_USE_ROCM",
          "40: #include \"tensorflow/core/platform/stream_executor.h\"",
          "41: #include \"tensorflow/core/protobuf/autotuning.pb.h\"",
          "42: #include \"tensorflow/core/util/autotune_maps/conv_parameters.h\"",
          "",
          "[Removed Lines]",
          "39: #include \"tensorflow/core/kernels/cast_op.h\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "529:   }",
          "530: };",
          "",
          "[Removed Lines]",
          "532: template <>",
          "533: struct LaunchConvOp<GPUDevice, Eigen::bfloat16> {",
          "534:   static void launch(OpKernelContext* ctx, bool cudnn_use_autotune,",
          "535:                      const Tensor& input_param, const Tensor& filter,",
          "536:                      const std::array<int64, 3>& dilations,",
          "537:                      const std::array<int64, 3>& strides, const Padding padding,",
          "538:                      TensorFormat data_format, Tensor* output) {",
          "541:     auto* stream = ctx->op_device_context()->stream();",
          "542:     const bool cast_to_float = !stream->GetCudaComputeCapability().IsAtLeast(",
          "543:         se::CudaComputeCapability::AMPERE);",
          "544:     Tensor casted_input = input_param;",
          "545:     Tensor casted_filter = filter;",
          "546:     Tensor casted_out = *output;",
          "548:     if (cast_to_float) {",
          "549:       const GPUDevice& device = ctx->eigen_device<GPUDevice>();",
          "550:       functor::CastFunctor<GPUDevice, float, Eigen::bfloat16> cast;",
          "551:       OP_REQUIRES_OK(ctx, ctx->allocate_temp(DT_FLOAT, input_param.shape(),",
          "552:                                              &casted_input));",
          "553:       cast(device, casted_input.template flat<float>(),",
          "554:            input_param.template flat<Eigen::bfloat16>());",
          "556:       OP_REQUIRES_OK(",
          "557:           ctx, ctx->allocate_temp(DT_FLOAT, filter.shape(), &casted_filter));",
          "558:       cast(device, casted_filter.template flat<float>(),",
          "559:            filter.template flat<Eigen::bfloat16>());",
          "561:       OP_REQUIRES_OK(",
          "562:           ctx, ctx->allocate_temp(DT_FLOAT, output->shape(), &casted_out));",
          "564:       LaunchConvOp<GPUDevice, float>::launch(",
          "565:           ctx, cudnn_use_autotune, casted_input, casted_filter, dilations,",
          "566:           strides, padding, data_format, &casted_out);",
          "568:       functor::CastFunctor<GPUDevice, Eigen::bfloat16, float> cast_back;",
          "569:       const Tensor& casted_out_const = casted_out;",
          "570:       cast_back(device, output->template flat<Eigen::bfloat16>(),",
          "571:                 casted_out_const.template flat<float>());",
          "572:       return;",
          "573:     }",
          "575:     LaunchConvOp<GPUDevice, Eigen::bfloat16>::launch(",
          "576:         ctx, cudnn_use_autotune, casted_input, casted_filter, dilations,",
          "577:         strides, padding, data_format, &casted_out);",
          "578:   }",
          "579: };",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "610:       typename TTypes<T, 5>::Tensor out);",
          "612: DECLARE_GPU_SPEC(Eigen::half);",
          "614: DECLARE_GPU_SPEC(float);",
          "615: DECLARE_GPU_SPEC(double);",
          "616: #undef DECLARE_GPU_SPEC",
          "",
          "[Removed Lines]",
          "613: DECLARE_GPU_SPEC(Eigen::bfloat16);",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "621: REGISTER_KERNEL_BUILDER(",
          "622:     Name(\"Conv3D\").Device(DEVICE_GPU).TypeConstraint<Eigen::half>(\"T\"),",
          "623:     Conv3DOp<GPUDevice, Eigen::half>);",
          "627: REGISTER_KERNEL_BUILDER(",
          "628:     Name(\"Conv3D\").Device(DEVICE_GPU).TypeConstraint<float>(\"T\"),",
          "629:     Conv3DOp<GPUDevice, float>);",
          "",
          "[Removed Lines]",
          "624: REGISTER_KERNEL_BUILDER(",
          "625:     Name(\"Conv3D\").Device(DEVICE_GPU).TypeConstraint<Eigen::bfloat16>(\"T\"),",
          "626:     Conv3DOp<GPUDevice, Eigen::bfloat16>);",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "tensorflow/core/kernels/conv_ops_gpu.cc||tensorflow/core/kernels/conv_ops_gpu.cc": [
          "File: tensorflow/core/kernels/conv_ops_gpu.cc -> tensorflow/core/kernels/conv_ops_gpu.cc",
          "--- Hunk 1 ---",
          "[Context before]",
          "46:   bool use_nhwc_fp16 =",
          "47:       data_type == DT_HALF && stream->GetCudaComputeCapability().IsAtLeast(",
          "48:                                   se::CudaComputeCapability::VOLTA);",
          "52:   if (use_4d_tensor) {",
          "54:   }",
          "57: #else",
          "59:   return false;",
          "",
          "[Removed Lines]",
          "49:   bool use_nhwc_bf16 =",
          "50:       data_type == DT_BFLOAT16 && stream->GetCudaComputeCapability().IsAtLeast(",
          "51:                                       se::CudaComputeCapability::AMPERE);",
          "53:     return use_nhwc_fp16 || use_nhwc_tf32 || use_nhwc_bf16;",
          "55:   return CUDNN_VERSION >= 8000 &&",
          "56:          (use_nhwc_fp16 || use_nhwc_tf32 || use_nhwc_bf16);",
          "",
          "[Added Lines]",
          "50:     return use_nhwc_fp16 || use_nhwc_tf32;",
          "52:   return CUDNN_VERSION >= 8000 && (use_nhwc_fp16 || use_nhwc_tf32);",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "401:   return autotune_entry;",
          "402: }",
          "426: }  // namespace tensorflow",
          "",
          "[Removed Lines]",
          "404: #define DECLARE_GPU_SPEC(T)                                                 \\",
          "405:   template StatusOr<AutotuneEntry<se::dnn::ConvOp>> AutotuneUnfusedConv<T>( \\",
          "406:       bool cudnn_use_autotune,                                              \\",
          "407:       AutotuneMap<ConvParameters, AutotuneEntry<se::dnn::ConvOp>>*          \\",
          "408:           autotune_map,                                                     \\",
          "409:       const ConvParameters& conv_parameters, OpKernelContext* ctx,          \\",
          "410:       se::dnn::ConvolutionKind kind,                                        \\",
          "411:       const se::dnn::BatchDescriptor& input_desc,                           \\",
          "412:       se::DeviceMemory<T> input_ptr,                                        \\",
          "413:       const se::dnn::FilterDescriptor& filter_desc,                         \\",
          "414:       se::DeviceMemory<T> filter_ptr,                                       \\",
          "415:       const se::dnn::ConvolutionDescriptor& conv_desc,                      \\",
          "416:       const se::dnn::BatchDescriptor& output_desc,                          \\",
          "417:       se::DeviceMemory<T> output_ptr, int64_t scratch_size_limit);",
          "419: DECLARE_GPU_SPEC(double);",
          "420: DECLARE_GPU_SPEC(float);",
          "421: DECLARE_GPU_SPEC(Eigen::half);",
          "422: DECLARE_GPU_SPEC(Eigen::bfloat16);",
          "424: #undef DECLARE_GPU_SPEC",
          "",
          "[Added Lines]",
          "400: template StatusOr<AutotuneEntry<se::dnn::ConvOp>> AutotuneUnfusedConv<double>(",
          "401:     bool cudnn_use_autotune,",
          "402:     AutotuneMap<ConvParameters, AutotuneEntry<se::dnn::ConvOp>>* autotune_map,",
          "403:     const ConvParameters& conv_parameters, OpKernelContext* ctx,",
          "404:     se::dnn::ConvolutionKind kind, const se::dnn::BatchDescriptor& input_desc,",
          "405:     se::DeviceMemory<double> input_ptr,",
          "406:     const se::dnn::FilterDescriptor& filter_desc,",
          "407:     se::DeviceMemory<double> filter_ptr,",
          "408:     const se::dnn::ConvolutionDescriptor& conv_desc,",
          "409:     const se::dnn::BatchDescriptor& output_desc,",
          "410:     se::DeviceMemory<double> output_ptr, int64_t scratch_size_limit);",
          "412: template StatusOr<AutotuneEntry<se::dnn::ConvOp>> AutotuneUnfusedConv<float>(",
          "413:     bool cudnn_use_autotune,",
          "414:     AutotuneMap<ConvParameters, AutotuneEntry<se::dnn::ConvOp>>* autotune_map,",
          "415:     const ConvParameters& conv_parameters, OpKernelContext* ctx,",
          "416:     se::dnn::ConvolutionKind kind, const se::dnn::BatchDescriptor& input_desc,",
          "417:     se::DeviceMemory<float> input_ptr,",
          "418:     const se::dnn::FilterDescriptor& filter_desc,",
          "419:     se::DeviceMemory<float> filter_ptr,",
          "420:     const se::dnn::ConvolutionDescriptor& conv_desc,",
          "421:     const se::dnn::BatchDescriptor& output_desc,",
          "422:     se::DeviceMemory<float> output_ptr, int64_t scratch_size_limit);",
          "424: template StatusOr<AutotuneEntry<se::dnn::ConvOp>>",
          "425: AutotuneUnfusedConv<Eigen::half>(",
          "426:     bool cudnn_use_autotune,",
          "427:     AutotuneMap<ConvParameters, AutotuneEntry<se::dnn::ConvOp>>* autotune_map,",
          "428:     const ConvParameters& conv_parameters, OpKernelContext* ctx,",
          "429:     se::dnn::ConvolutionKind kind, const se::dnn::BatchDescriptor& input_desc,",
          "430:     se::DeviceMemory<Eigen::half> input_ptr,",
          "431:     const se::dnn::FilterDescriptor& filter_desc,",
          "432:     se::DeviceMemory<Eigen::half> filter_ptr,",
          "433:     const se::dnn::ConvolutionDescriptor& conv_desc,",
          "434:     const se::dnn::BatchDescriptor& output_desc,",
          "435:     se::DeviceMemory<Eigen::half> output_ptr, int64_t scratch_size_limit);",
          "",
          "---------------"
        ],
        "tensorflow/core/kernels/depthwise_conv_grad_op.cc||tensorflow/core/kernels/depthwise_conv_grad_op.cc": [
          "File: tensorflow/core/kernels/depthwise_conv_grad_op.cc -> tensorflow/core/kernels/depthwise_conv_grad_op.cc",
          "--- Hunk 1 ---",
          "[Context before]",
          "4: you may not use this file except in compliance with the License.",
          "5: You may obtain a copy of the License at",
          "9: Unless required by applicable law or agreed to in writing, software",
          "10: distributed under the License is distributed on an \"AS IS\" BASIS,",
          "",
          "[Removed Lines]",
          "7:    http://www.apache.org/licenses/LICENSE-2.0",
          "",
          "[Added Lines]",
          "7:     http://www.apache.org/licenses/LICENSE-2.0",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "39: #include \"tensorflow/core/util/use_cudnn.h\"",
          "40: #include \"tensorflow/core/util/work_sharder.h\"",
          "42: #if GOOGLE_CUDA",
          "43: #include \"third_party/gpus/cudnn/cudnn.h\"",
          "44: #endif",
          "46: namespace tensorflow {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "42: #if GOOGLE_CUDA || TENSORFLOW_USE_ROCM",
          "48: #include \"tensorflow/core/platform/stream_executor.h\"",
          "49: #endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "532: #if GOOGLE_CUDA || TENSORFLOW_USE_ROCM",
          "536: extern template struct LaunchConv2DBackpropInputOp<GPUDevice, Eigen::half>;",
          "537: extern template struct LaunchConv2DBackpropInputOp<GPUDevice, float>;",
          "538: extern template struct LaunchConv2DBackpropInputOp<GPUDevice, double>;",
          "543: extern template struct LaunchDepthwiseConvBackpropInputOp<GPUDevice,",
          "544:                                                           Eigen::half>;",
          "545: extern template struct LaunchDepthwiseConvBackpropInputOp<GPUDevice, float>;",
          "",
          "[Removed Lines]",
          "535: extern template struct LaunchConv2DBackpropInputOp<GPUDevice, Eigen::bfloat16>;",
          "541: extern template struct LaunchDepthwiseConvBackpropInputOp<GPUDevice,",
          "542:                                                           Eigen::bfloat16>;",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "594:     use_cudnn_grouped_conv_ =",
          "596:         ((data_format_ == FORMAT_NCHW && stride_ == 1 && stride_w == 1) ||",
          "597:          (data_format_ == FORMAT_NHWC && stride_ == stride_w &&",
          "598:           (stride_ == 1 || stride_ == 2)));",
          "",
          "[Removed Lines]",
          "595:         (dtype_ == DT_HALF || dtype_ == DT_BFLOAT16) &&",
          "",
          "[Added Lines]",
          "597:         dtype_ == DT_HALF &&",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "642:     bool use_cudnn =",
          "643:         std::is_same<Device, GPUDevice>::value &&",
          "649:     VLOG(2) << \"DepthwiseConv2dNativeBackpropInput: \"",
          "650:             << \" Input: [\" << batch << \", \" << input_rows << \", \" << input_cols",
          "",
          "[Removed Lines]",
          "644:         (in_depth == 1 ||",
          "645:          (use_cudnn_grouped_conv_ && UseCudnnWith16BitFloat(context, dtype_) &&",
          "646:           ShouldCudnnGroupedConvolutionBeUsed(filter_rows, filter_cols,",
          "647:                                               in_depth, out_depth)));",
          "",
          "[Added Lines]",
          "646:         (in_depth == 1 || (use_cudnn_grouped_conv_ &&",
          "647:                            ShouldCudnnGroupedConvolutionBeUsed(",
          "648:                                filter_rows, filter_cols, in_depth, out_depth)));",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "730:                               .HostMemory(\"input_sizes\"),            \\",
          "731:                           DepthwiseConv2dNativeBackpropInputOp<GPUDevice, T>)",
          "734: TF_CALL_half(REGISTER_GPU_KERNEL);",
          "735: TF_CALL_float(REGISTER_GPU_KERNEL);",
          "736: TF_CALL_double(REGISTER_GPU_KERNEL);",
          "",
          "[Removed Lines]",
          "733: TF_CALL_bfloat16(REGISTER_GPU_KERNEL);",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "755:                               .Label(\"cudnn_grouped_convolution\"),   \\",
          "756:                           DepthwiseConv2dGroupedConvBackpropInputOp<T>)",
          "759: TF_CALL_half(REGISTER_GROUPED_CONV_KERNEL);",
          "760: TF_CALL_float(REGISTER_GROUPED_CONV_KERNEL);",
          "761: TF_CALL_double(REGISTER_GROUPED_CONV_KERNEL);",
          "",
          "[Removed Lines]",
          "758: TF_CALL_bfloat16(REGISTER_GROUPED_CONV_KERNEL);",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "1041: #if GOOGLE_CUDA || TENSORFLOW_USE_ROCM",
          "1045: extern template struct LaunchConv2DBackpropFilterOp<GPUDevice, Eigen::half>;",
          "1046: extern template struct LaunchConv2DBackpropFilterOp<GPUDevice, float>;",
          "1047: extern template struct LaunchConv2DBackpropFilterOp<GPUDevice, double>;",
          "1052: extern template struct LaunchDepthwiseConvBackpropFilterOp<GPUDevice,",
          "1053:                                                            Eigen::half>;",
          "1054: extern template struct LaunchDepthwiseConvBackpropFilterOp<GPUDevice, float>;",
          "",
          "[Removed Lines]",
          "1044: extern template struct LaunchConv2DBackpropFilterOp<GPUDevice, Eigen::bfloat16>;",
          "1050: extern template struct LaunchDepthwiseConvBackpropFilterOp<GPUDevice,",
          "1051:                                                            Eigen::bfloat16>;",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "1104:     } else {",
          "1105:       LOG(ERROR) << \"Only bfloat16, half, float, and double are supported.\";",
          "1106:     }",
          "1114: #else",
          "1115:     use_cudnn_grouped_conv_ = false;",
          "1116: #endif",
          "",
          "[Removed Lines]",
          "1107: #if CUDNN_VERSION >= 8000",
          "1108:     use_cudnn_grouped_conv_ = dtype_ == DT_HALF || dtype_ == DT_BFLOAT16;",
          "1109: #elif CUDNN_VERSION >= 7603",
          "1113:     use_cudnn_grouped_conv_ = dtype_ == DT_HALF;",
          "",
          "[Added Lines]",
          "1103: #if CUDNN_VERSION >= 7603",
          "1112:     use_cudnn_grouped_conv_ = OpDeterminismRequired() || dtype_ == DT_HALF;",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "1166:     VLOG(2) << \"DepthwiseConv2dNativeBackpropFilter: \"",
          "1167:             << \" Input: [\" << batch << \", \" << input_rows << \", \" << input_cols",
          "",
          "[Removed Lines]",
          "1155:     bool determinism_required = false;",
          "1156: #if CUDNN_VERSION >= 7603",
          "1157:     determinism_required = OpDeterminismRequired();",
          "1158: #endif  // CUDNN_VERSION >= 7603",
          "1159:     bool use_cudnn =",
          "1160:         std::is_same<Device, GPUDevice>::value &&",
          "1161:         (in_depth == 1 || determinism_required ||",
          "1162:          (use_cudnn_grouped_conv_ && UseCudnnWith16BitFloat(context, dtype_) &&",
          "1163:           ShouldCudnnGroupedConvolutionBeUsed(filter_rows, filter_cols,",
          "1164:                                               in_depth, out_depth)));",
          "",
          "[Added Lines]",
          "1149:     bool use_cudnn = std::is_same<Device, GPUDevice>::value &&",
          "1150:                      (in_depth == 1 ||",
          "1151:                       (use_cudnn_grouped_conv_ &&",
          "1152:                        (ShouldCudnnGroupedConvolutionBeUsed(",
          "1153:                             filter_rows, filter_cols, in_depth, out_depth) ||",
          "1154:                         OpDeterminismRequired())));",
          "",
          "---------------",
          "--- Hunk 11 ---",
          "[Context before]",
          "1199:       return;",
          "1200:     }",
          "1207:                                    std::is_same<Device, GPUDevice>::value;",
          "1208:     using U = typename std::conditional<cast_to_float, float, T>::type;",
          "1209:     Tensor casted_out_backprop = out_backprop;",
          "",
          "[Removed Lines]",
          "1205:     constexpr bool cast_to_float = (std::is_same<T, Eigen::half>::value ||",
          "1206:                                     std::is_same<T, Eigen::bfloat16>::value) &&",
          "",
          "[Added Lines]",
          "1195:     constexpr bool cast_to_float = std::is_same<T, Eigen::half>::value &&",
          "",
          "---------------",
          "--- Hunk 12 ---",
          "[Context before]",
          "1211:     Tensor casted_filter_backprop = *filter_backprop;",
          "1212:     const Device& device = context->template eigen_device<Device>();",
          "1213:     if (cast_to_float) {",
          "1215:       OP_REQUIRES_OK(context,",
          "1216:                      context->allocate_temp(DT_FLOAT, out_backprop.shape(),",
          "1217:                                             &casted_out_backprop));",
          "1218:       cast(device, casted_out_backprop.template flat<float>(),",
          "1220:       OP_REQUIRES_OK(context, context->allocate_temp(DT_FLOAT, input.shape(),",
          "1221:                                                      &casted_input));",
          "1222:       cast(device, casted_input.template flat<float>(),",
          "1224:       OP_REQUIRES_OK(context,",
          "1225:                      context->allocate_temp(DT_FLOAT, filter_backprop->shape(),",
          "1226:                                             &casted_filter_backprop));",
          "",
          "[Removed Lines]",
          "1214:       functor::CastFunctor<Device, float, T> cast;",
          "1219:            out_backprop.template flat<T>());",
          "1223:            input.template flat<T>());",
          "",
          "[Added Lines]",
          "1203:       functor::CastFunctor<Device, float, Eigen::half> cast;",
          "1208:            out_backprop.template flat<Eigen::half>());",
          "1212:            input.template flat<Eigen::half>());",
          "",
          "---------------",
          "--- Hunk 13 ---",
          "[Context before]",
          "1234:         data_format_);",
          "1236:     if (cast_to_float) {",
          "1238:       const Tensor& casted_filter_backprop_const = casted_filter_backprop;",
          "1240:            casted_filter_backprop_const.template flat<float>());",
          "1241:     }",
          "1242:   }",
          "",
          "[Removed Lines]",
          "1237:       functor::CastFunctor<Device, T, float> cast;",
          "1239:       cast(device, filter_backprop->template flat<T>(),",
          "",
          "[Added Lines]",
          "1226:       functor::CastFunctor<Device, Eigen::half, float> cast;",
          "1228:       cast(device, filter_backprop->template flat<Eigen::half>(),",
          "",
          "---------------",
          "--- Hunk 14 ---",
          "[Context before]",
          "1281:                               .HostMemory(\"filter_sizes\"),            \\",
          "1282:                           DepthwiseConv2dNativeBackpropFilterOp<GPUDevice, T>)",
          "1285: TF_CALL_half(REGISTER_GPU_KERNEL);",
          "1286: TF_CALL_float(REGISTER_GPU_KERNEL);",
          "1287: TF_CALL_double(REGISTER_GPU_KERNEL);",
          "",
          "[Removed Lines]",
          "1284: TF_CALL_bfloat16(REGISTER_GPU_KERNEL);",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 15 ---",
          "[Context before]",
          "1306:                               .Label(\"cudnn_grouped_convolution\"),    \\",
          "1307:                           DepthwiseConv2dGroupedConvBackpropFilterOp<T>)",
          "1310: TF_CALL_half(REGISTER_GROUPED_CONV_KERNEL);",
          "1311: TF_CALL_float(REGISTER_GROUPED_CONV_KERNEL);",
          "1312: TF_CALL_double(REGISTER_GROUPED_CONV_KERNEL);",
          "",
          "[Removed Lines]",
          "1309: TF_CALL_bfloat16(REGISTER_GROUPED_CONV_KERNEL);",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "tensorflow/core/kernels/depthwise_conv_op.cc||tensorflow/core/kernels/depthwise_conv_op.cc": [
          "File: tensorflow/core/kernels/depthwise_conv_op.cc -> tensorflow/core/kernels/depthwise_conv_op.cc",
          "--- Hunk 1 ---",
          "[Context before]",
          "41: #include \"tensorflow/core/util/use_cudnn.h\"",
          "42: #include \"tensorflow/core/util/work_sharder.h\"",
          "44: #if GOOGLE_CUDA",
          "45: #include \"third_party/gpus/cudnn/cudnn.h\"",
          "46: #endif",
          "48: namespace tensorflow {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "44: #if GOOGLE_CUDA || TENSORFLOW_USE_ROCM",
          "50: #include \"tensorflow/core/platform/stream_executor.h\"",
          "51: #endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "57: typedef Eigen::ThreadPoolDevice CPUDevice;",
          "58: typedef Eigen::GpuDevice GPUDevice;",
          "",
          "[Removed Lines]",
          "60: bool UseCudnnWith16BitFloat(OpKernelContext* ctx, DataType dtype) {",
          "61: #if GOOGLE_CUDA",
          "62:   if (dtype == DT_HALF) {",
          "63:     return true;",
          "64:   } else if (dtype == DT_BFLOAT16) {",
          "65:     auto* stream = ctx->op_device_context()->stream();",
          "66:     if (!stream) return false;",
          "67:     return stream->GetCudaComputeCapability().IsAtLeast(",
          "68:         se::CudaComputeCapability::AMPERE);",
          "69:   }",
          "70: #endif",
          "71:   return false;",
          "72: }",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "267: #if GOOGLE_CUDA || TENSORFLOW_USE_ROCM",
          "271: extern template struct LaunchConv2DOp<GPUDevice, Eigen::half>;",
          "272: extern template struct LaunchConv2DOp<GPUDevice, float>;",
          "273: extern template struct LaunchConv2DOp<GPUDevice, double>;",
          "277: extern template struct LaunchDepthwiseConvOp<GPUDevice, Eigen::half>;",
          "278: extern template struct LaunchDepthwiseConvOp<GPUDevice, float>;",
          "279: extern template struct LaunchDepthwiseConvOp<GPUDevice, double>;",
          "",
          "[Removed Lines]",
          "270: extern template struct LaunchConv2DOp<GPUDevice, Eigen::bfloat16>;",
          "276: extern template struct LaunchDepthwiseConvOp<GPUDevice, Eigen::bfloat16>;",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "342:     use_cudnn_grouped_conv_ =",
          "344:         (data_format_ == FORMAT_NCHW ||",
          "345:          (data_format_ == FORMAT_NHWC && stride_ == stride_w &&",
          "346:           (stride_ == 1 || stride_ == 2)));",
          "",
          "[Removed Lines]",
          "343:         (dtype_ == DT_HALF || dtype_ == DT_BFLOAT16) &&",
          "",
          "[Added Lines]",
          "332:         dtype_ == DT_HALF &&",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "440:     bool use_cudnn =",
          "441:         std::is_same<Device, GPUDevice>::value &&",
          "447:     VLOG(2) << \"DepthwiseConv2dNative: \"",
          "448:             << \" Input: [\" << batch << \", \" << input_rows << \", \" << input_cols",
          "",
          "[Removed Lines]",
          "442:         (in_depth == 1 ||",
          "443:          (use_cudnn_grouped_conv_ && UseCudnnWith16BitFloat(context, dtype_) &&",
          "444:           ShouldCudnnGroupedConvolutionBeUsed(filter_rows, filter_cols,",
          "445:                                               in_depth, out_depth)));",
          "",
          "[Added Lines]",
          "431:         (in_depth == 1 || (use_cudnn_grouped_conv_ &&",
          "432:                            ShouldCudnnGroupedConvolutionBeUsed(",
          "433:                                filter_rows, filter_cols, in_depth, out_depth)));",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "539:       Name(\"DepthwiseConv2dNative\").Device(DEVICE_GPU).TypeConstraint<T>(\"T\"), \\",
          "540:       DepthwiseConv2dNativeOp<GPUDevice, T>)",
          "543: TF_CALL_half(REGISTER_GPU_KERNEL);",
          "544: TF_CALL_float(REGISTER_GPU_KERNEL);",
          "545: TF_CALL_double(REGISTER_GPU_KERNEL);",
          "",
          "[Removed Lines]",
          "542: TF_CALL_bfloat16(REGISTER_GPU_KERNEL);",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "562:                               .Label(\"cudnn_grouped_convolution\"), \\",
          "563:                           DepthwiseConv2dGroupedConvOp<T>)",
          "566: TF_CALL_half(REGISTER_GROUPED_CONV_KERNEL);",
          "567: TF_CALL_float(REGISTER_GROUPED_CONV_KERNEL);",
          "568: TF_CALL_double(REGISTER_GROUPED_CONV_KERNEL);",
          "",
          "[Removed Lines]",
          "565: TF_CALL_bfloat16(REGISTER_GROUPED_CONV_KERNEL);",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "tensorflow/core/kernels/depthwise_conv_op.h||tensorflow/core/kernels/depthwise_conv_op.h": [
          "File: tensorflow/core/kernels/depthwise_conv_op.h -> tensorflow/core/kernels/depthwise_conv_op.h",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: #include \"tensorflow/core/framework/types.h\"",
          "21: #include \"tensorflow/core/util/tensor_format.h\"",
          "27: namespace tensorflow {",
          "29: struct DepthwiseArgs {",
          "",
          "[Removed Lines]",
          "23: #if GOOGLE_CUDA || TENSORFLOW_USE_ROCM",
          "24: #include \"tensorflow/core/platform/stream_executor.h\"",
          "25: #endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "84:                   TensorFormat data_format);",
          "85: };",
          "89: #if GOOGLE_CUDA || TENSORFLOW_USE_ROCM",
          "90: template <typename T>",
          "91: struct LaunchDepthwiseConvOp<Eigen::GpuDevice, T> {",
          "",
          "[Removed Lines]",
          "87: bool UseCudnnWith16BitFloat(OpKernelContext* ctx, DataType dtype);",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "tensorflow/core/kernels/depthwise_conv_op_gpu.h||tensorflow/core/kernels/depthwise_conv_op_gpu.h": [
          "File: tensorflow/core/kernels/depthwise_conv_op_gpu.h -> tensorflow/core/kernels/depthwise_conv_op_gpu.h",
          "--- Hunk 1 ---",
          "[Context before]",
          "47: struct PseudoHalfType<Eigen::half> {",
          "48:   using Type = float;",
          "49: };",
          "54: }  // namespace detail",
          "56: using Eigen::GpuDevice;",
          "",
          "[Removed Lines]",
          "50: template <>",
          "51: struct PseudoHalfType<Eigen::bfloat16> {",
          "52:   using Type = float;",
          "53: };",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "tensorflow/core/kernels/depthwise_conv_op_gpu_bfloat16.cu.cc||tensorflow/core/kernels/depthwise_conv_op_gpu_bfloat16.cu.cc": [
          "File: tensorflow/core/kernels/depthwise_conv_op_gpu_bfloat16.cu.cc -> tensorflow/core/kernels/depthwise_conv_op_gpu_bfloat16.cu.cc",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "tensorflow/core/util/gpu_device_functions.h||tensorflow/core/util/gpu_device_functions.h": [
          "File: tensorflow/core/util/gpu_device_functions.h -> tensorflow/core/util/gpu_device_functions.h",
          "--- Hunk 1 ---",
          "[Context before]",
          "663:   }",
          "664: }",
          "678: template <typename F>",
          "679: __device__ long long GpuAtomicCasHelper(long long* ptr, F accumulate) {",
          "680:   return static_cast<long long>(",
          "",
          "[Removed Lines]",
          "666: template <typename F>",
          "667: __device__ Eigen::bfloat16 GpuAtomicCasHelper(Eigen::bfloat16* ptr,",
          "668:                                               F accumulate) {",
          "669:   Eigen::half ret = detail::GpuAtomicCasHelper(",
          "670:       reinterpret_cast<Eigen::half*>(ptr), [accumulate](Eigen::half a) {",
          "671:         Eigen::bfloat16 acc =",
          "672:             accumulate(Eigen::numext::bit_cast<Eigen::bfloat16>(a));",
          "673:         return Eigen::numext::bit_cast<Eigen::half>(acc);",
          "674:       });",
          "675:   return Eigen::numext::bit_cast<Eigen::bfloat16>(ret);",
          "676: }",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "737:       ptr, [value](Eigen::half a) { return a + value; });",
          "738: }",
          "746: #if (__CUDA_ARCH__ < 600) || TENSORFLOW_USE_ROCM",
          "747: __device__ inline double GpuAtomicAdd(double* ptr, double value) {",
          "748:   return detail::GpuAtomicCasHelper(ptr,",
          "",
          "[Removed Lines]",
          "740: __device__ inline Eigen::bfloat16 GpuAtomicAdd(Eigen::bfloat16* ptr,",
          "741:                                                Eigen::bfloat16 value) {",
          "742:   return detail::GpuAtomicCasHelper(",
          "743:       ptr, [value](Eigen::bfloat16 a) { return a + value; });",
          "744: }",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "tensorflow/python/kernel_tests/nn_ops/conv_ops_test.py||tensorflow/python/kernel_tests/nn_ops/conv_ops_test.py": [
          "File: tensorflow/python/kernel_tests/nn_ops/conv_ops_test.py -> tensorflow/python/kernel_tests/nn_ops/conv_ops_test.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "172:         out.append(dtypes.float16)",
          "173:       if not test.is_built_with_rocm():",
          "174:         out.append(dtypes.float64)",
          "176:       return out",
          "178:     return [dtypes.float32, dtypes.float64, dtypes.float16, dtypes.bfloat16]",
          "",
          "[Removed Lines]",
          "175:         out.append(dtypes.bfloat16)",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "tensorflow/python/kernel_tests/nn_ops/depthwise_conv_op_base.py||tensorflow/python/kernel_tests/nn_ops/depthwise_conv_op_base.py": [
          "File: tensorflow/python/kernel_tests/nn_ops/depthwise_conv_op_base.py -> tensorflow/python/kernel_tests/nn_ops/depthwise_conv_op_base.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "416:       tf_logging.info(",
          "417:           \"Testing DepthwiseConv2DCudnn, %dth config: %r * %r, stride: %d, \"",
          "418:           \"padding: %s\", index, input_size, filter_size, stride, padding)",
          "431:   @test_util.run_v1_only(\"b/120545219\")",
          "432:   def testDepthwiseConv2D(self):",
          "",
          "[Removed Lines]",
          "419:       data_types = [dtypes.float16, dtypes.bfloat16]",
          "420:       for data_type in data_types:",
          "421:         self._VerifyValues(",
          "422:             input_size,",
          "423:             filter_size,",
          "424:             stride,",
          "425:             padding,",
          "426:             data_type,",
          "427:             use_gpu=True,",
          "428:             data_format=\"NCHW\",",
          "429:             dilations=dilations)",
          "",
          "[Added Lines]",
          "419:       data_type = dtypes.float16",
          "420:       self._VerifyValues(",
          "421:           input_size,",
          "422:           filter_size,",
          "423:           stride,",
          "424:           padding,",
          "425:           data_type,",
          "426:           use_gpu=True,",
          "427:           data_format=\"NCHW\",",
          "428:           dilations=dilations)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "510:           \"Testing DepthwiseConv2D, %dth config: %r * %r, stride: %d, padding: \"",
          "511:           \"%s\", index, input_size, filter_size, stride, padding)",
          "512:       # double datatype is currently not supported for convolution ops",
          "517:       data_formats = [\"NHWC\", \"NCHW\"] if test.is_gpu_available() else [\"NHWC\"]",
          "519:         for data_format in data_formats:",
          "520:           self._VerifyValues(",
          "521:               input_size,",
          "",
          "[Removed Lines]",
          "513:       # on the ROCm platform and its support for bfloat16 is unknown.",
          "514:       data_types = [dtypes.float16, dtypes.float32]",
          "515:       if not test.is_built_with_rocm():",
          "516:         data_types.extend([dtypes.float64, dtypes.bfloat16])",
          "518:       for data_type in data_types:",
          "",
          "[Added Lines]",
          "512:       # on the ROCm platform",
          "513:       optional_float64 = [] if test.is_built_with_rocm() else [dtypes.float64]",
          "515:       for data_type in [dtypes.float16, dtypes.float32] + optional_float64:",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "665:           dtypes.float16: 4e-0,",
          "666:           dtypes.float32: 8e-4,",
          "667:           dtypes.float64: 1e-12,",
          "669:       }[data_type]",
          "671:       input_tensor = constant_op.constant(",
          "",
          "[Removed Lines]",
          "668:           dtypes.bfloat16: 1e-0,",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "747:           \"Testing DepthwiseConv2DInputGradCudnn, %dth config: %r * %r, \"",
          "748:           \"stride: %d, padding: %s\", index, input_size, filter_size, stride,",
          "749:           padding)",
          "764:   @test_util.run_v1_only(\"b/120545219\")",
          "765:   def testDepthwiseConv2DInputGrad(self):",
          "",
          "[Removed Lines]",
          "750:       data_types = [dtypes.float16, dtypes.bfloat16]",
          "751:       for data_type in data_types:",
          "752:         self._ConstructAndTestGradient(",
          "753:             input_size,",
          "754:             filter_size,",
          "755:             output_size,",
          "756:             stride,",
          "757:             padding,",
          "758:             data_type,",
          "759:             test_input=True,",
          "760:             use_gpu=True,",
          "761:             data_format=\"NCHW\",",
          "762:             dilations=dilations)",
          "",
          "[Added Lines]",
          "746:       data_type = dtypes.float16",
          "747:       self._ConstructAndTestGradient(",
          "748:           input_size,",
          "749:           filter_size,",
          "750:           output_size,",
          "751:           stride,",
          "752:           padding,",
          "753:           data_type,",
          "754:           test_input=True,",
          "755:           use_gpu=True,",
          "756:           data_format=\"NCHW\",",
          "757:           dilations=dilations)",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "830:           \"stride: %d, padding: %s\", index, input_size, filter_size, stride,",
          "831:           padding)",
          "832:       # double datatype is currently not supported for convolution ops",
          "837:       data_formats = [\"NHWC\", \"NCHW\"] if test.is_gpu_available() else [\"NHWC\"]",
          "839:         for data_format in data_formats:",
          "840:           self._ConstructAndTestGradient(",
          "841:               input_size,",
          "",
          "[Removed Lines]",
          "833:       # on the ROCm platform and its support for bfloat16 is unknown.",
          "834:       data_types = [dtypes.float16, dtypes.float32]",
          "835:       if not test.is_built_with_rocm():",
          "836:         data_types.extend([dtypes.float64, dtypes.bfloat16])",
          "838:       for data_type in data_types:",
          "",
          "[Added Lines]",
          "828:       # on the ROCm platform",
          "829:       optional_float64 = [] if test.is_built_with_rocm() else [dtypes.float64]",
          "831:       for data_type in [dtypes.float16, dtypes.float32] + optional_float64:",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "860:           \"Testing DepthwiseConv2DFilterGradCudnn, %dth config: %r * %r, \"",
          "861:           \"stride: %d, padding: %s\", index, input_size, filter_size, stride,",
          "862:           padding)",
          "888:   @test_util.run_v1_only(\"b/120545219\")",
          "889:   def testDepthwiseConv2DFilterGrad(self):",
          "",
          "[Removed Lines]",
          "863:       data_types = [dtypes.float16, dtypes.bfloat16]",
          "864:       for data_type in data_types:",
          "865:         self._ConstructAndTestGradient(",
          "866:             input_size,",
          "867:             filter_size,",
          "868:             output_size,",
          "869:             stride,",
          "870:             padding,",
          "871:             data_type,",
          "872:             test_input=False,",
          "873:             use_gpu=True,",
          "874:             data_format=\"NCHW\",",
          "875:             dilations=dilations)",
          "876:         self._ConstructAndTestGradient(",
          "877:             input_size,",
          "878:             filter_size,",
          "879:             output_size,",
          "880:             stride,",
          "881:             padding,",
          "882:             data_type,",
          "883:             test_input=False,",
          "884:             use_gpu=True,",
          "885:             data_format=\"NHWC\",",
          "886:             dilations=dilations)",
          "",
          "[Added Lines]",
          "856:       data_type = dtypes.float16",
          "857:       self._ConstructAndTestGradient(",
          "858:           input_size,",
          "859:           filter_size,",
          "860:           output_size,",
          "861:           stride,",
          "862:           padding,",
          "863:           data_type,",
          "864:           test_input=False,",
          "865:           use_gpu=True,",
          "866:           data_format=\"NCHW\",",
          "867:           dilations=dilations)",
          "868:       self._ConstructAndTestGradient(",
          "869:           input_size,",
          "870:           filter_size,",
          "871:           output_size,",
          "872:           stride,",
          "873:           padding,",
          "874:           data_type,",
          "875:           test_input=False,",
          "876:           use_gpu=True,",
          "877:           data_format=\"NHWC\",",
          "878:           dilations=dilations)",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "943:           \"stride: %d, padding: %s\", index, input_size, filter_size, stride,",
          "944:           padding)",
          "945:       # double datatype is currently not supported for convolution ops",
          "950:       data_formats = [\"NHWC\", \"NCHW\"] if test.is_gpu_available() else [\"NHWC\"]",
          "952:         for data_format in data_formats:",
          "953:           self._ConstructAndTestGradient(",
          "954:               input_size,",
          "",
          "[Removed Lines]",
          "946:       # on the ROCm platform and its support for bfloat16 is unknown.",
          "947:       data_types = [dtypes.float16, dtypes.float32]",
          "948:       if not test.is_built_with_rocm():",
          "949:         data_types.extend([dtypes.float64, dtypes.bfloat16])",
          "951:       for data_type in data_types:",
          "",
          "[Added Lines]",
          "938:       # on the ROCm platform",
          "939:       optional_float64 = [] if test.is_built_with_rocm() else [dtypes.float64]",
          "941:       for data_type in [dtypes.float16, dtypes.float32] + optional_float64:",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "965:   def _CompareBackpropInput(self, input_sizes, filter_sizes, output_sizes,",
          "966:                             stride, padding, dtype):",
          "969:     if isinstance(padding, list):",
          "970:       padding = [(0, 0)] + padding + [(0, 0)]",
          "973:       with self.cached_session(use_gpu=use_gpu):",
          "974:         t0 = constant_op.constant(input_sizes, shape=[len(input_sizes)])",
          "977:         backprop = nn_ops.depthwise_conv2d_native_backprop_input(",
          "978:             t0, t1, t2, strides=[1, stride, stride, 1], padding=padding)",
          "979:         ret = self.evaluate(backprop)",
          "980:         self.assertShapeEqual(ret, backprop)",
          "981:         return ret",
          "988:   @test_util.run_gpu_only",
          "989:   def testDepthwiseConv2DInputGradCompare(self):",
          "",
          "[Removed Lines]",
          "967:     x1 = np.random.rand(*filter_sizes)",
          "968:     x2 = np.random.rand(*output_sizes)",
          "972:     def _GetVal(use_gpu, dtype):",
          "975:         t1 = constant_op.constant(x1, shape=filter_sizes, dtype=dtype)",
          "976:         t2 = constant_op.constant(x2, shape=output_sizes, dtype=dtype)",
          "983:     rtol, atol = (1e-1, 1e-1) if dtype == \"bfloat16\" else (1e-4, 1e-4)",
          "984:     gpu_value = _GetVal(use_gpu=True, dtype=dtype)",
          "985:     cpu_value = _GetVal(use_gpu=False, dtype=dtype)",
          "986:     self.assertAllClose(cpu_value, gpu_value, rtol=rtol, atol=atol)",
          "",
          "[Added Lines]",
          "957:     x1 = np.random.rand(*filter_sizes).astype(dtype)",
          "958:     x2 = np.random.rand(*output_sizes).astype(dtype)",
          "962:     def _GetVal(use_gpu):",
          "965:         t1 = constant_op.constant(x1, shape=filter_sizes)",
          "966:         t2 = constant_op.constant(x2, shape=output_sizes)",
          "973:     gpu_value = _GetVal(use_gpu=True)",
          "974:     cpu_value = _GetVal(use_gpu=False)",
          "975:     self.assertAllClose(cpu_value, gpu_value, rtol=1e-4, atol=1e-4)",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "997:           padding)",
          "998:       self._CompareBackpropInput(input_size, filter_size, output_size, stride,",
          "999:                                  padding, \"float32\")",
          "1002:       if test.is_built_with_rocm():",
          "1003:         continue",
          "1004:       self._CompareBackpropInput(input_size, filter_size, output_size, stride,",
          "1005:                                  padding, \"float64\")",
          "1009:   @test_util.run_gpu_only",
          "1010:   def testDepthwiseConv2DInputGradExplicitCompare(self):",
          "",
          "[Removed Lines]",
          "1000:       # Convolutions on the ROCm platform don't support double dtype. And its",
          "1001:       # support for bf16 is unknown. So, we skip these tests.",
          "1006:       self._CompareBackpropInput(input_size, filter_size, output_size, stride,",
          "1007:                                  padding, \"bfloat16\")",
          "",
          "[Added Lines]",
          "989:       # double datatype is currently not supported for convolution ops",
          "990:       # on the ROCm platform",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "1018:           padding)",
          "1019:       self._CompareBackpropInput(input_size, filter_size, output_size, stride,",
          "1020:                                  padding, \"float32\")",
          "1023:       if test.is_built_with_rocm():",
          "1024:         continue",
          "1025:       self._CompareBackpropInput(input_size, filter_size, output_size, stride,",
          "1026:                                  padding, \"float64\")",
          "1030:   def _CompareBackpropFilter(self, input_sizes, filter_sizes, output_sizes,",
          "1031:                              stride, padding, dtype):",
          "1034:     padding_nhwc = padding",
          "1035:     padding_nchw = padding",
          "1036:     if isinstance(padding, list):",
          "1037:       padding_nhwc = [(0, 0)] + padding + [(0, 0)]",
          "1038:       padding_nchw = [(0, 0)] + [(0, 0)] + padding",
          "1041:       with self.cached_session(use_gpu=use_gpu):",
          "1043:         t1 = constant_op.constant(filter_sizes, shape=[len(filter_sizes)])",
          "1045:         strides = [1, stride, stride, 1]",
          "1046:         padding = padding_nhwc",
          "1047:         if data_format == \"NCHW\":",
          "",
          "[Removed Lines]",
          "1021:       # Convolutions on the ROCm platform don't support double dtype. And its",
          "1022:       # support for bf16 is unknown. So, we skip these tests.",
          "1027:       self._CompareBackpropInput(input_size, filter_size, output_size, stride,",
          "1028:                                  padding, \"bfloat16\")",
          "1032:     x0 = np.random.rand(*input_sizes)",
          "1033:     x2 = np.random.rand(*output_sizes)",
          "1040:     def _GetVal(use_gpu, dtype, data_format=\"NHWC\"):",
          "1042:         t0 = constant_op.constant(x0, shape=input_sizes, dtype=dtype)",
          "1044:         t2 = constant_op.constant(x2, shape=output_sizes, dtype=dtype)",
          "",
          "[Added Lines]",
          "1008:       # double datatype is currently not supported for convolution ops",
          "1009:       # on the ROCm platform",
          "1017:     x0 = np.random.rand(*input_sizes).astype(dtype)",
          "1018:     x2 = np.random.rand(*output_sizes).astype(dtype)",
          "1025:     def _GetVal(use_gpu, data_format=\"NHWC\"):",
          "1027:         t0 = constant_op.constant(x0, shape=input_sizes)",
          "1029:         t2 = constant_op.constant(x2, shape=output_sizes)",
          "",
          "---------------",
          "--- Hunk 11 ---",
          "[Context before]",
          "1060:         self.assertShapeEqual(ret, backprop)",
          "1061:         return ret",
          "1064:     for data_format in [\"NHWC\", \"NCHW\"]:",
          "1069:   @test_util.run_gpu_only",
          "1070:   def testDepthwiseConv2DFilterGradCompare(self):",
          "",
          "[Removed Lines]",
          "1063:     cpu_value = _GetVal(use_gpu=False, dtype=dtype)",
          "1065:       gpu_value = _GetVal(use_gpu=True, dtype=dtype, data_format=data_format)",
          "1066:       self.assertAllCloseAccordingToType(",
          "1067:           cpu_value, gpu_value, rtol=1e-4, atol=1e-4, bfloat16_rtol=1e-0)",
          "",
          "[Added Lines]",
          "1048:     cpu_value = _GetVal(use_gpu=False)",
          "1050:       gpu_value = _GetVal(use_gpu=True, data_format=data_format)",
          "1051:       self.assertAllClose(cpu_value, gpu_value, rtol=1e-4, atol=1e-4)",
          "",
          "---------------",
          "--- Hunk 12 ---",
          "[Context before]",
          "1078:           padding)",
          "1079:       self._CompareBackpropFilter(input_size, filter_size, output_size, stride,",
          "1080:                                   padding, \"float32\")",
          "1083:       if test.is_built_with_rocm():",
          "1084:         continue",
          "1085:       self._CompareBackpropFilter(input_size, filter_size, output_size, stride,",
          "1086:                                   padding, \"float64\")",
          "1091:   @test_util.run_gpu_only",
          "1092:   def testDepthwiseConv2DFilterGradExplicitCompare(self):",
          "1093:     for index, (input_size, filter_size, output_size, stride, padding,",
          "",
          "[Removed Lines]",
          "1081:       # Convolutions on the ROCm platform don't support double dtype. And its",
          "1082:       # support for bf16 is unknown. So, we skip these tests.",
          "1088:       self._CompareBackpropFilter(input_size, filter_size, output_size, stride,",
          "1089:                                   padding, \"bfloat16\")",
          "",
          "[Added Lines]",
          "1065:       # double datatype is currently not supported for convolution ops",
          "1066:       # on the ROCm platform",
          "",
          "---------------",
          "--- Hunk 13 ---",
          "[Context before]",
          "1100:           padding)",
          "1101:       self._CompareBackpropFilter(input_size, filter_size, output_size, stride,",
          "1102:                                   padding, \"float32\")",
          "1105:       if test.is_built_with_rocm():",
          "1106:         continue",
          "1107:       self._CompareBackpropFilter(input_size, filter_size, output_size, stride,",
          "1108:                                   padding, \"float64\")",
          "",
          "[Removed Lines]",
          "1103:       # Convolutions on the ROCm platform don't support double dtype. And its",
          "1104:       # support for bf16 is unknown. So, we skip these tests.",
          "1110:       self._CompareBackpropFilter(input_size, filter_size, output_size, stride,",
          "1111:                                   padding, \"bfloat16\")",
          "1113:   def _CompareForward(self, input_sizes, filter_sizes, output_sizes, stride,",
          "1114:                       padding, dtype):",
          "1115:     x1 = np.random.rand(*input_sizes)",
          "1116:     x2 = np.random.rand(*filter_sizes)",
          "1117:     if isinstance(padding, list):",
          "1118:       padding = [(0, 0)] + padding + [(0, 0)]",
          "1120:     def _GetVal(use_gpu, dtype):",
          "1121:       with self.cached_session(use_gpu=use_gpu):",
          "1122:         t1 = constant_op.constant(x1, shape=input_sizes, dtype=dtype)",
          "1123:         t2 = constant_op.constant(x2, shape=filter_sizes, dtype=dtype)",
          "1124:         output = nn_ops.depthwise_conv2d_native(",
          "1125:             t1, t2, strides=[1, stride, stride, 1], padding=padding)",
          "1126:         ret = self.evaluate(output)",
          "1127:         self.assertShapeEqual(ret, output)",
          "1128:         return ret",
          "1130:     gpu_value = _GetVal(use_gpu=True, dtype=dtype)",
          "1131:     cpu_value = _GetVal(use_gpu=False, dtype=dtype)",
          "1132:     self.assertAllCloseAccordingToType(",
          "1133:         cpu_value, gpu_value, rtol=1e-4, atol=1e-4, bfloat16_rtol=1e-1)",
          "1135:   @test_util.run_gpu_only",
          "1136:   def testDepthwiseConv2DForwardCompare(self):",
          "1137:     for index, (input_size, filter_size, output_size, stride, padding,",
          "1138:                 dilations) in enumerate(ConfigsToTest()):",
          "1139:       if dilations:",
          "1140:         continue",
          "1141:       tf_logging.info(",
          "1142:           \"Testing DepthwiseConv2DForwardCompare, %dth config: %r * %r, \"",
          "1143:           \"stride: %d, padding: %s\", index, input_size, filter_size, stride,",
          "1144:           padding)",
          "1145:       self._CompareForward(input_size, filter_size, output_size, stride,",
          "1146:                            padding, \"float32\")",
          "1147:       # Convolutions on the ROCm platform don't support double dtype. And its",
          "1148:       # support for bf16 is unknown. So, we skip these tests.",
          "1149:       if test.is_built_with_rocm():",
          "1150:         continue",
          "1151:       self._CompareForward(input_size, filter_size, output_size, stride,",
          "1152:                            padding, \"float64\")",
          "1154:       self._CompareForward(input_size, filter_size, output_size, stride,",
          "1155:                            padding, \"bfloat16\")",
          "1157:   @test_util.run_gpu_only",
          "1158:   def testDepthwiseConv2DForwardExplicitCompare(self):",
          "1159:     for index, (input_size, filter_size, output_size, stride, padding,",
          "1160:                 dilations) in enumerate(ConfigsToTestExplicit()):",
          "1161:       if dilations:",
          "1162:         continue",
          "1163:       tf_logging.info(",
          "1164:           \"Testing DepthwiseConv2DForwardCompare, %dth config: %r * %r, \"",
          "1165:           \"stride: %d, padding: %s\", index, input_size, filter_size, stride,",
          "1166:           padding)",
          "1167:       # Convolutions on the ROCm platform don't support double dtype. And its",
          "1168:       # support for bf16 is unknown. So, we skip these tests.",
          "1169:       if test.is_built_with_rocm():",
          "1170:         continue",
          "1171:       self._CompareForward(input_size, filter_size, output_size, stride,",
          "1172:                            padding, \"float64\")",
          "1173:       self._CompareForward(input_size, filter_size, output_size, stride,",
          "1174:                            padding, \"float32\")",
          "1176:       self._CompareForward(input_size, filter_size, output_size, stride,",
          "1177:                            padding, \"bfloat16\")",
          "",
          "[Added Lines]",
          "1084:       # double datatype is currently not supported for convolution ops",
          "1085:       # on the ROCm platform",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "d787cdf78f3e9574a14b6d774ab2308a35228d9c",
      "candidate_info": {
        "commit_hash": "d787cdf78f3e9574a14b6d774ab2308a35228d9c",
        "repo": "tensorflow/tensorflow",
        "commit_url": "https://github.com/tensorflow/tensorflow/commit/d787cdf78f3e9574a14b6d774ab2308a35228d9c",
        "files": [
          "tensorflow/core/kernels/BUILD",
          "tensorflow/core/kernels/conv_2d_gpu_bfloat16.cu.cc",
          "tensorflow/core/kernels/conv_grad_filter_ops.cc",
          "tensorflow/core/kernels/conv_grad_input_ops.cc",
          "tensorflow/core/kernels/conv_grad_ops_3d.cc",
          "tensorflow/core/kernels/conv_ops.cc",
          "tensorflow/core/kernels/conv_ops_3d.cc",
          "tensorflow/core/kernels/conv_ops_gpu.cc",
          "tensorflow/core/kernels/depthwise_conv_grad_op.cc",
          "tensorflow/core/kernels/depthwise_conv_op.cc",
          "tensorflow/core/kernels/depthwise_conv_op.h",
          "tensorflow/core/kernels/depthwise_conv_op_gpu.h",
          "tensorflow/core/kernels/depthwise_conv_op_gpu_bfloat16.cu.cc",
          "tensorflow/core/util/gpu_device_functions.h",
          "tensorflow/python/kernel_tests/nn_ops/conv_ops_test.py",
          "tensorflow/python/kernel_tests/nn_ops/depthwise_conv_op_base.py"
        ],
        "message": "PR #57674: [NVIDIA TF] Enable BF16 Conv Ops\n\nImported from GitHub PR https://github.com/tensorflow/tensorflow/pull/57674\n\nThis PR enables BF16 Conv ops via cuDNN or cuBLASLt.\n\nThis includes the fwd and bwd ops for the Conv2D/3D and the Depthwise Conv.\n\ncc. @nluehr @pjannaty @kushanam\nCopybara import of the project:\n\n--\n0834a4266cc054a19184e66ddafc556a47329d33 by Kaixi Hou <kaixih@nvidia.com>:\n\nSupport bf16 conv and add unit tests\n\n--\n7857ff73e50768cec43edbe363deb7fd8c8747a7 by Kaixi Hou <kaixih@nvidia.com>:\n\nSupport bf16 depthwise conv and add unit tests\n\n--\n6993ea93b0e4bfd367e934fc43c80ed785ea039a by Kaixi Hou <kaixih@nvidia.com>:\n\nUse one function to check NHWC use\n\n--\n14d47f31335c9371e8e2700f8cad164d9c892192 by Kaixi Hou <kaixih@nvidia.com>:\n\nUse macro to control building with bfloat16\n\n--\ncd7c703df5a315786e53870a31739def65d61bf1 by Kaixi Hou <kaixih@nvidia.com>:\n\nRevert \"Use macro to control building with bfloat16\"\n\nThis reverts commit 5a2f4c7f289f5505f2853381b49ff0278b51a686.\n\n--\ne12df140de53872834baadc079c9b4eb1061f4c4 by Kaixi Hou <kaixih@nvidia.com>:\n\nAddress comments\n\n--\nbbe97aea4180659804f8cd47097c4acffcd5b767 by Kaixi Hou <kaixih@nvidia.com>:\n\nAdd more depthwise tests\n\n--\n8f4d24f559469fe9327d4144cbad91f01e46f776 by Kaixi Hou <kaixih@nvidia.com>:\n\nCleanup unused variables\n\nMerging this change closes #57674\n\nCOPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/57674 from kaixih:bf16_conv_gpu 8f4d24f559469fe9327d4144cbad91f01e46f776\nPiperOrigin-RevId: 486829530",
        "before_after_code_files": [
          "tensorflow/core/kernels/conv_2d_gpu_bfloat16.cu.cc||tensorflow/core/kernels/conv_2d_gpu_bfloat16.cu.cc",
          "tensorflow/core/kernels/conv_grad_filter_ops.cc||tensorflow/core/kernels/conv_grad_filter_ops.cc",
          "tensorflow/core/kernels/conv_grad_input_ops.cc||tensorflow/core/kernels/conv_grad_input_ops.cc",
          "tensorflow/core/kernels/conv_grad_ops_3d.cc||tensorflow/core/kernels/conv_grad_ops_3d.cc",
          "tensorflow/core/kernels/conv_ops.cc||tensorflow/core/kernels/conv_ops.cc",
          "tensorflow/core/kernels/conv_ops_3d.cc||tensorflow/core/kernels/conv_ops_3d.cc",
          "tensorflow/core/kernels/conv_ops_gpu.cc||tensorflow/core/kernels/conv_ops_gpu.cc",
          "tensorflow/core/kernels/depthwise_conv_grad_op.cc||tensorflow/core/kernels/depthwise_conv_grad_op.cc",
          "tensorflow/core/kernels/depthwise_conv_op.cc||tensorflow/core/kernels/depthwise_conv_op.cc",
          "tensorflow/core/kernels/depthwise_conv_op.h||tensorflow/core/kernels/depthwise_conv_op.h",
          "tensorflow/core/kernels/depthwise_conv_op_gpu.h||tensorflow/core/kernels/depthwise_conv_op_gpu.h",
          "tensorflow/core/kernels/depthwise_conv_op_gpu_bfloat16.cu.cc||tensorflow/core/kernels/depthwise_conv_op_gpu_bfloat16.cu.cc",
          "tensorflow/core/util/gpu_device_functions.h||tensorflow/core/util/gpu_device_functions.h",
          "tensorflow/python/kernel_tests/nn_ops/conv_ops_test.py||tensorflow/python/kernel_tests/nn_ops/conv_ops_test.py",
          "tensorflow/python/kernel_tests/nn_ops/depthwise_conv_op_base.py||tensorflow/python/kernel_tests/nn_ops/depthwise_conv_op_base.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "tensorflow/core/kernels/conv_grad_ops_3d.cc||tensorflow/core/kernels/conv_grad_ops_3d.cc"
          ],
          "candidate": [
            "tensorflow/core/kernels/conv_grad_ops_3d.cc||tensorflow/core/kernels/conv_grad_ops_3d.cc"
          ]
        }
      },
      "candidate_diff": {
        "tensorflow/core/kernels/conv_2d_gpu_bfloat16.cu.cc||tensorflow/core/kernels/conv_2d_gpu_bfloat16.cu.cc": [
          "File: tensorflow/core/kernels/conv_2d_gpu_bfloat16.cu.cc -> tensorflow/core/kernels/conv_2d_gpu_bfloat16.cu.cc",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "3: Licensed under the Apache License, Version 2.0 (the \"License\");",
          "4: you may not use this file except in compliance with the License.",
          "5: You may obtain a copy of the License at",
          "7:     http://www.apache.org/licenses/LICENSE-2.0",
          "9: Unless required by applicable law or agreed to in writing, software",
          "10: distributed under the License is distributed on an \"AS IS\" BASIS,",
          "11: WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
          "12: See the License for the specific language governing permissions and",
          "13: limitations under the License.",
          "16: #if GOOGLE_CUDA || TENSORFLOW_USE_ROCM",
          "18: #define EIGEN_USE_GPU",
          "20: #include <algorithm>",
          "21: #include <array>",
          "22: #include <limits>",
          "23: #include <utility>",
          "25: #include \"tensorflow/core/kernels/conv_2d.h\"",
          "26: #include \"tensorflow/core/kernels/conv_2d_gpu.h\"",
          "28: namespace tensorflow {",
          "30: namespace functor {",
          "32: template struct SwapDimension1And2InTensor3<Eigen::GpuDevice, Eigen::bfloat16>;",
          "35: template struct TransformFilter<Eigen::GpuDevice, Eigen::bfloat16, int, 4>;",
          "36: template struct ReverseTransformFilter<Eigen::GpuDevice, Eigen::bfloat16, 4>;",
          "37: template struct NHWCToNCHW<Eigen::GpuDevice, Eigen::bfloat16, 4>;",
          "38: template struct NCHWToNHWC<Eigen::GpuDevice, Eigen::bfloat16, 4>;",
          "39: template struct PadInput<Eigen::GpuDevice, Eigen::bfloat16, int, 4>;",
          "42: template struct TransformFilter<Eigen::GpuDevice, Eigen::bfloat16, int, 5>;",
          "43: template struct ReverseTransformFilter<Eigen::GpuDevice, Eigen::bfloat16, 5>;",
          "44: template struct NHWCToNCHW<Eigen::GpuDevice, Eigen::bfloat16, 5>;",
          "45: template struct NCHWToNHWC<Eigen::GpuDevice, Eigen::bfloat16, 5>;",
          "46: template struct PadInput<Eigen::GpuDevice, Eigen::bfloat16, int, 5>;",
          "48: }  // namespace functor",
          "49: }  // namespace tensorflow",
          "51: #endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM",
          "",
          "---------------"
        ],
        "tensorflow/core/kernels/conv_grad_filter_ops.cc||tensorflow/core/kernels/conv_grad_filter_ops.cc": [
          "File: tensorflow/core/kernels/conv_grad_filter_ops.cc -> tensorflow/core/kernels/conv_grad_filter_ops.cc",
          "--- Hunk 1 ---",
          "[Context before]",
          "49: #endif",
          "51: #if GOOGLE_CUDA || TENSORFLOW_USE_ROCM",
          "52: #include \"tensorflow/core/kernels/conv_ops_gpu.h\"",
          "53: #include \"tensorflow/core/platform/stream_executor.h\"",
          "54: #include \"tensorflow/core/protobuf/autotuning.pb.h\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "52: #include \"tensorflow/core/kernels/cast_op.h\"",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "923:       filter_backprop->tensor<T, 4>());",
          "924: }",
          "927: namespace functor {",
          "928: #define DECLARE_GPU_SPEC(T)                                             \\",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "927: template <>",
          "928: void LaunchConv2DBackpropFilterOp<Eigen::GpuDevice, Eigen::bfloat16>::",
          "929: operator()(OpKernelContext* ctx, bool use_cudnn, bool cudnn_use_autotune,",
          "930:            const Tensor& out_backprop, const Tensor& input, int row_dilation,",
          "931:            int col_dilation, int row_stride, int col_stride,",
          "932:            const Padding& padding,",
          "933:            const std::vector<int64_t>& explicit_paddings,",
          "934:            Tensor* filter_backprop, TensorFormat data_format) {",
          "937:   auto* stream = ctx->op_device_context()->stream();",
          "938:   const bool cast_to_float = !stream->GetCudaComputeCapability().IsAtLeast(",
          "939:       se::CudaComputeCapability::AMPERE);",
          "940:   Tensor casted_input = input;",
          "941:   Tensor casted_out_backprop = out_backprop;",
          "942:   Tensor casted_filter_backprop = *filter_backprop;",
          "944:   if (cast_to_float) {",
          "945:     const GPUDevice& device = ctx->eigen_device<GPUDevice>();",
          "946:     functor::CastFunctor<GPUDevice, float, Eigen::bfloat16> cast;",
          "947:     OP_REQUIRES_OK(ctx,",
          "948:                    ctx->allocate_temp(DT_FLOAT, input.shape(), &casted_input));",
          "949:     cast(device, casted_input.template flat<float>(),",
          "950:          input.template flat<Eigen::bfloat16>());",
          "952:     OP_REQUIRES_OK(ctx, ctx->allocate_temp(DT_FLOAT, out_backprop.shape(),",
          "953:                                            &casted_out_backprop));",
          "954:     cast(device, casted_out_backprop.template flat<float>(),",
          "955:          out_backprop.template flat<Eigen::bfloat16>());",
          "957:     OP_REQUIRES_OK(ctx, ctx->allocate_temp(DT_FLOAT, filter_backprop->shape(),",
          "958:                                            &casted_filter_backprop));",
          "960:     LaunchConv2DBackpropFilterOp<Eigen::GpuDevice, float>()(",
          "961:         ctx, use_cudnn, cudnn_use_autotune, casted_out_backprop, casted_input,",
          "962:         row_dilation, col_dilation, row_stride, col_stride, padding,",
          "963:         explicit_paddings, &casted_filter_backprop, data_format);",
          "965:     functor::CastFunctor<GPUDevice, Eigen::bfloat16, float> cast_back;",
          "966:     const Tensor& casted_filter_backprop_const = casted_filter_backprop;",
          "967:     cast_back(device, filter_backprop->template flat<Eigen::bfloat16>(),",
          "968:               casted_filter_backprop_const.template flat<float>());",
          "969:     return;",
          "970:   }",
          "972:   LaunchConv2DBackpropFilterOp<Eigen::GpuDevice, Eigen::bfloat16>()(",
          "973:       ctx, use_cudnn, cudnn_use_autotune, casted_out_backprop, casted_input,",
          "974:       row_dilation, col_dilation, row_stride, col_stride, padding,",
          "975:       explicit_paddings, &casted_filter_backprop, data_format);",
          "976: }",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "944: DECLARE_GPU_SPEC(float);",
          "945: DECLARE_GPU_SPEC(Eigen::half);",
          "946: DECLARE_GPU_SPEC(double);",
          "947: #undef DECLARE_GPU_SPEC",
          "948: }  // namespace functor",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "998: DECLARE_GPU_SPEC(Eigen::bfloat16);",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "962:                             .TypeConstraint<Eigen::half>(\"T\")",
          "963:                             .HostMemory(\"filter_sizes\"),",
          "964:                         Conv2DBackpropFilterOp<GPUDevice, Eigen::half>);",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1018: REGISTER_KERNEL_BUILDER(Name(\"Conv2DBackpropFilter\")",
          "1019:                             .Device(DEVICE_GPU)",
          "1020:                             .TypeConstraint<Eigen::bfloat16>(\"T\")",
          "1021:                             .HostMemory(\"filter_sizes\"),",
          "1022:                         Conv2DBackpropFilterOp<GPUDevice, Eigen::bfloat16>);",
          "",
          "---------------"
        ],
        "tensorflow/core/kernels/conv_grad_input_ops.cc||tensorflow/core/kernels/conv_grad_input_ops.cc": [
          "File: tensorflow/core/kernels/conv_grad_input_ops.cc -> tensorflow/core/kernels/conv_grad_input_ops.cc",
          "--- Hunk 1 ---",
          "[Context before]",
          "22: #include \"tensorflow/core/profiler/lib/scoped_annotation.h\"",
          "24: #if GOOGLE_CUDA || TENSORFLOW_USE_ROCM",
          "25: #include \"tensorflow/core/protobuf/autotuning.pb.h\"",
          "26: #include \"tensorflow/core/util/autotune_maps/conv_parameters.h\"",
          "27: #endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "25: #include \"tensorflow/core/kernels/cast_op.h\"",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "432:   }",
          "433: }",
          "436: namespace functor {",
          "437: #define DECLARE_GPU_SPEC(T)                                             \\",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "436: template <>",
          "437: void LaunchConv2DBackpropInputOp<GPUDevice, Eigen::bfloat16>::operator()(",
          "438:     OpKernelContext* ctx, bool use_cudnn, bool cudnn_use_autotune,",
          "439:     const Tensor& out_backprop, const Tensor& filter, int row_dilation,",
          "440:     int col_dilation, int row_stride, int col_stride, const Padding& padding,",
          "441:     const std::vector<int64_t>& explicit_paddings, Tensor* in_backprop,",
          "442:     TensorFormat data_format) {",
          "445:   auto* stream = ctx->op_device_context()->stream();",
          "446:   const bool cast_to_float = !stream->GetCudaComputeCapability().IsAtLeast(",
          "447:       se::CudaComputeCapability::AMPERE);",
          "448:   Tensor casted_out_backprop = out_backprop;",
          "449:   Tensor casted_filter = filter;",
          "450:   Tensor casted_in_backprop = *in_backprop;",
          "452:   if (cast_to_float) {",
          "453:     const GPUDevice& device = ctx->eigen_device<GPUDevice>();",
          "454:     functor::CastFunctor<GPUDevice, float, Eigen::bfloat16> cast;",
          "455:     OP_REQUIRES_OK(ctx, ctx->allocate_temp(DT_FLOAT, out_backprop.shape(),",
          "456:                                            &casted_out_backprop));",
          "457:     cast(device, casted_out_backprop.template flat<float>(),",
          "458:          out_backprop.template flat<Eigen::bfloat16>());",
          "460:     OP_REQUIRES_OK(",
          "461:         ctx, ctx->allocate_temp(DT_FLOAT, filter.shape(), &casted_filter));",
          "462:     cast(device, casted_filter.template flat<float>(),",
          "463:          filter.template flat<Eigen::bfloat16>());",
          "465:     OP_REQUIRES_OK(ctx, ctx->allocate_temp(DT_FLOAT, in_backprop->shape(),",
          "466:                                            &casted_in_backprop));",
          "468:     LaunchConv2DBackpropInputOp<Eigen::GpuDevice, float>()(",
          "469:         ctx, use_cudnn, cudnn_use_autotune, casted_out_backprop, casted_filter,",
          "470:         row_dilation, col_dilation, row_stride, col_stride, padding,",
          "471:         explicit_paddings, &casted_in_backprop, data_format);",
          "473:     functor::CastFunctor<GPUDevice, Eigen::bfloat16, float> cast_back;",
          "474:     const Tensor& casted_in_backprop_const = casted_in_backprop;",
          "475:     cast_back(device, in_backprop->template flat<Eigen::bfloat16>(),",
          "476:               casted_in_backprop_const.template flat<float>());",
          "477:     return;",
          "478:   }",
          "480:   LaunchConv2DBackpropInputOp<Eigen::GpuDevice, Eigen::bfloat16>()(",
          "481:       ctx, use_cudnn, cudnn_use_autotune, casted_out_backprop, casted_filter,",
          "482:       row_dilation, col_dilation, row_stride, col_stride, padding,",
          "483:       explicit_paddings, &casted_in_backprop, data_format);",
          "484: }",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "453: DECLARE_GPU_SPEC(float);",
          "454: DECLARE_GPU_SPEC(Eigen::half);",
          "455: DECLARE_GPU_SPEC(double);",
          "456: #undef DECLARE_GPU_SPEC",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "506: DECLARE_GPU_SPEC(Eigen::bfloat16);",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "493:                             .TypeConstraint<Eigen::half>(\"T\")",
          "494:                             .HostMemory(\"input_sizes\"),",
          "495:                         Conv2DBackpropInputOp<GPUDevice, Eigen::half>);",
          "496: REGISTER_KERNEL_BUILDER(Name(\"Conv2DBackpropInput\")",
          "497:                             .Device(DEVICE_GPU)",
          "498:                             .TypeConstraint<int32>(\"T\")",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "548: REGISTER_KERNEL_BUILDER(Name(\"Conv2DBackpropInput\")",
          "549:                             .Device(DEVICE_GPU)",
          "550:                             .TypeConstraint<Eigen::bfloat16>(\"T\")",
          "551:                             .HostMemory(\"input_sizes\"),",
          "552:                         Conv2DBackpropInputOp<GPUDevice, Eigen::bfloat16>);",
          "",
          "---------------"
        ],
        "tensorflow/core/kernels/conv_grad_ops_3d.cc||tensorflow/core/kernels/conv_grad_ops_3d.cc": [
          "File: tensorflow/core/kernels/conv_grad_ops_3d.cc -> tensorflow/core/kernels/conv_grad_ops_3d.cc",
          "--- Hunk 1 ---",
          "[Context before]",
          "44: #endif",
          "46: #if GOOGLE_CUDA || TENSORFLOW_USE_ROCM",
          "47: #include \"tensorflow/core/platform/stream_executor.h\"",
          "48: using stream_executor::dnn::DimIndex;",
          "49: #include \"tensorflow/core/protobuf/autotuning.pb.h\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "47: #include \"tensorflow/core/kernels/cast_op.h\"",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1233:       const T& padding_value);",
          "1235: DECLARE_GPU_SPEC(Eigen::half);",
          "1236: DECLARE_GPU_SPEC(float);",
          "1237: DECLARE_GPU_SPEC(double);",
          "1238: #undef DECLARE_GPU_SPEC",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1237: DECLARE_GPU_SPEC(Eigen::bfloat16);",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "1249:     AutotuneConv3dBwdData;",
          "1251: template <typename T>",
          "1302:     const TensorShape& filter_shape = filter.shape();",
          "1305:     const TensorShape& out_backprop_shape = out_backprop.shape();",
          "1315:     ConvBackpropDimensions dims;",
          "1316:     OP_REQUIRES_OK(context, ConvBackpropComputeDimensionsV2(",
          "1317:                                 \"Conv3DBackpropInputOp\", /*num_spatial_dims=*/3,",
          "1318:                                 input_shape, filter_shape, out_backprop_shape,",
          "1326:     auto* stream = context->op_device_context()->stream();",
          "1327:     OP_REQUIRES(context, stream, errors::Internal(\"No GPU stream available.\"));",
          "",
          "[Removed Lines]",
          "1252: class Conv3DBackpropInputOp<GPUDevice, T> : public OpKernel {",
          "1253:  public:",
          "1254:   explicit Conv3DBackpropInputOp(OpKernelConstruction* context)",
          "1255:       : OpKernel(context),",
          "1256:         data_format_(FORMAT_NHWC),",
          "1257:         takes_shape_(type_string().find(\"V2\") != std::string::npos) {",
          "1259:     if (takes_shape_) {",
          "1260:       string data_format;",
          "1261:       OP_REQUIRES_OK(context, context->GetAttr(\"data_format\", &data_format));",
          "1262:       OP_REQUIRES(context, FormatFromString(data_format, &data_format_),",
          "1263:                   errors::InvalidArgument(\"Invalid data format\"));",
          "1264:     }",
          "1265:     OP_REQUIRES_OK(context, context->GetAttr(\"dilations\", &dilation_));",
          "1266:     OP_REQUIRES(context, dilation_.size() == 5,",
          "1267:                 errors::InvalidArgument(\"Dilation rates field must \"",
          "1268:                                         \"specify 5 dimensions\"));",
          "1269:     OP_REQUIRES(context,",
          "1270:                 (GetTensorDim(dilation_, data_format_, 'C') == 1 &&",
          "1271:                  GetTensorDim(dilation_, data_format_, 'N') == 1),",
          "1272:                 errors::InvalidArgument(",
          "1273:                     \"Current implementation does not yet support \"",
          "1274:                     \"dilation rates in the batch and depth dimensions.\"));",
          "1275:     OP_REQUIRES(",
          "1276:         context,",
          "1277:         (GetTensorDim(dilation_, data_format_, '0') > 0 &&",
          "1278:          GetTensorDim(dilation_, data_format_, '1') > 0 &&",
          "1279:          GetTensorDim(dilation_, data_format_, '2') > 0),",
          "1280:         errors::InvalidArgument(\"Dilated rates should be larger than 0.\"));",
          "1281:     OP_REQUIRES_OK(context, context->GetAttr(\"strides\", &stride_));",
          "1282:     OP_REQUIRES(context, stride_.size() == 5,",
          "1283:                 errors::InvalidArgument(\"Sliding window strides field must \"",
          "1284:                                         \"specify 5 dimensions\"));",
          "1285:     OP_REQUIRES(",
          "1286:         context,",
          "1287:         (GetTensorDim(stride_, data_format_, 'C') == 1 &&",
          "1288:          GetTensorDim(stride_, data_format_, 'N') == 1),",
          "1289:         errors::InvalidArgument(\"Current implementation does not yet support \"",
          "1290:                                 \"strides in the batch and depth dimensions.\"));",
          "1291:     OP_REQUIRES(",
          "1292:         context,",
          "1293:         (GetTensorDim(stride_, data_format_, '0') > 0 &&",
          "1294:          GetTensorDim(stride_, data_format_, '1') > 0 &&",
          "1295:          GetTensorDim(stride_, data_format_, '2') > 0),",
          "1296:         errors::InvalidArgument(\"Spatial strides should be larger than 0.\"));",
          "1297:     OP_REQUIRES_OK(context, context->GetAttr(\"padding\", &padding_));",
          "1298:     cudnn_use_autotune_ = CudnnUseAutotune();",
          "1299:   }",
          "1300:   void Compute(OpKernelContext* context) override {",
          "1301:     const Tensor& filter = context->input(1);",
          "1304:     const Tensor& out_backprop = context->input(2);",
          "1307:     TensorShape input_shape;",
          "1308:     if (takes_shape_) {",
          "1309:       const Tensor& input_sizes = context->input(0);",
          "1310:       OP_REQUIRES_OK(context, tensor::MakeShape(input_sizes, &input_shape));",
          "1311:     } else {",
          "1312:       input_shape = context->input(0).shape();",
          "1313:     }",
          "1319:                                 dilation_, stride_, padding_,",
          "1322:     Tensor* in_backprop;",
          "1323:     OP_REQUIRES_OK(context,",
          "1324:                    context->allocate_output(0, input_shape, &in_backprop));",
          "",
          "[Added Lines]",
          "1254: struct LaunchConvBackpropInputOp {",
          "1255:   static void launch(OpKernelContext* context, bool cudnn_use_autotune,",
          "1256:                      const Tensor& out_backprop, const Tensor& filter,",
          "1257:                      const std::vector<int32>& dilation,",
          "1258:                      const std::vector<int32>& stride, const Padding& padding,",
          "1259:                      Tensor* in_backprop, TensorFormat data_format) {",
          "1262:     const TensorShape& input_shape = in_backprop->shape();",
          "1268:                                 dilation, stride, padding,",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "1331:         dims.filter_size(1) == 1 && dims.filter_size(2) == 1 &&",
          "1332:         dims.dilation(0) == 1 && dims.dilation(1) == 1 &&",
          "1333:         dims.dilation(2) == 1 && dims.stride(0) == 1 && dims.stride(1) == 1 &&",
          "1335:       const uint64 m = dims.batch_size * dims.input_size(0) *",
          "1336:                        dims.input_size(1) * dims.input_size(2);",
          "1337:       const uint64 k = dims.out_depth;",
          "",
          "[Removed Lines]",
          "1334:         dims.stride(2) == 1 && data_format_ == FORMAT_NHWC) {",
          "",
          "[Added Lines]",
          "1279:         dims.stride(2) == 1 && data_format == FORMAT_NHWC) {",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "1356:                dims.filter_size(0) == dims.input_size(0) &&",
          "1357:                dims.filter_size(1) == dims.input_size(1) &&",
          "1358:                dims.filter_size(2) == dims.input_size(2) &&",
          "1360:       const uint64 m = dims.batch_size;",
          "1361:       const uint64 k = dims.out_depth;",
          "1362:       const uint64 n = dims.input_size(0) * dims.input_size(1) *",
          "",
          "[Removed Lines]",
          "1359:                padding_ == Padding::VALID && data_format_ == FORMAT_NHWC) {",
          "",
          "[Added Lines]",
          "1304:                padding == Padding::VALID && data_format == FORMAT_NHWC) {",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "1379:       return;",
          "1380:     }",
          "1385:     const bool planes_odd = (padding_planes % 2 != 0);",
          "1386:     const bool rows_odd = (padding_rows % 2 != 0);",
          "1387:     const bool cols_odd = (padding_cols % 2 != 0);",
          "",
          "[Removed Lines]",
          "1382:     int padding_planes = dims.SpatialPadding(padding_, 0);",
          "1383:     int padding_rows = dims.SpatialPadding(padding_, 1);",
          "1384:     int padding_cols = dims.SpatialPadding(padding_, 2);",
          "",
          "[Added Lines]",
          "1327:     int padding_planes = dims.SpatialPadding(padding, 0);",
          "1328:     int padding_rows = dims.SpatialPadding(padding, 1);",
          "1329:     int padding_cols = dims.SpatialPadding(padding, 2);",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "1409:     const bool compute_in_nhwc = ComputeInNhwcEnabled(",
          "1410:         DataTypeToEnum<T>::value, stream, /*use_4d_tensor=*/false);",
          "1411:     const TensorFormat compute_data_format =",
          "1415:     VLOG(3) << \"Compute Conv3DBackpropInput with cuDNN:\"",
          "1417:             << \" compute_data_format=\" << ToString(compute_data_format);",
          "1419:     constexpr auto kComputeInNHWC =",
          "",
          "[Removed Lines]",
          "1412:         (compute_in_nhwc && data_format_ == FORMAT_NHWC) ? FORMAT_NHWC",
          "1413:                                                          : FORMAT_NCHW;",
          "1416:             << \" data_format=\" << ToString(data_format_)",
          "",
          "[Added Lines]",
          "1358:         (compute_in_nhwc && data_format == FORMAT_NHWC) ? FORMAT_NHWC",
          "1359:                                                         : FORMAT_NCHW;",
          "1362:             << \" data_format=\" << ToString(data_format)",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "1487:     Tensor transformed_out_backprop;",
          "1489:       TensorShape nchw_shape = {dims.batch_size, dims.out_depth,",
          "1490:                                 dims.output_size(0), dims.output_size(1),",
          "1491:                                 dims.output_size(2)};",
          "",
          "[Removed Lines]",
          "1488:     if (data_format_ == FORMAT_NHWC && compute_data_format == FORMAT_NCHW) {",
          "",
          "[Added Lines]",
          "1434:     if (data_format == FORMAT_NHWC && compute_data_format == FORMAT_NCHW) {",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "1551:     using se::dnn::ProfileResult;",
          "1553:     auto entry_or = AutotuneUnfusedConv(",
          "1555:         conv_parameters, context, se::dnn::ConvolutionKind::BACKWARD_DATA,",
          "1556:         input_desc, in_backprop_ptr, filter_desc, filter_ptr, conv_desc,",
          "1557:         output_desc, out_backprop_ptr, ConvolveBackwardDataScratchSize);",
          "",
          "[Removed Lines]",
          "1554:         cudnn_use_autotune_, AutotuneConv3dBwdData::GetInstance(),",
          "",
          "[Added Lines]",
          "1500:         cudnn_use_autotune, AutotuneConv3dBwdData::GetInstance(),",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "1593:       pre_transformed_in_backprop = in_backprop_remove_padding;",
          "1594:     }",
          "1597:       auto toConstTensor = [](const Tensor& x) -> const Tensor { return x; };",
          "1598:       functor::NCHWToNHWC<GPUDevice, T, 5>()(",
          "1599:           context->eigen_device<GPUDevice>(),",
          "",
          "[Removed Lines]",
          "1596:     if (data_format_ == FORMAT_NHWC && compute_data_format == FORMAT_NCHW) {",
          "",
          "[Added Lines]",
          "1542:     if (data_format == FORMAT_NHWC && compute_data_format == FORMAT_NCHW) {",
          "",
          "---------------",
          "--- Hunk 11 ---",
          "[Context before]",
          "1604:     }",
          "1605:   }",
          "1614: };",
          "1625: template <typename T>",
          "1627:  public:",
          "1629:       : OpKernel(context),",
          "1630:         data_format_(FORMAT_NHWC),",
          "1631:         takes_shape_(type_string().find(\"V2\") != std::string::npos) {",
          "",
          "[Removed Lines]",
          "1607:  private:",
          "1608:   std::vector<int32> dilation_;",
          "1609:   std::vector<int32> stride_;",
          "1610:   Padding padding_;",
          "1611:   TensorFormat data_format_;",
          "1612:   bool takes_shape_;",
          "1613:   bool cudnn_use_autotune_;",
          "1617: struct Conv3dBackwardFilterAutotuneGroup {",
          "1618:   static string name() { return \"Conv3dBwdFilter\"; }",
          "1619: };",
          "1621: typedef AutotuneSingleton<Conv3dBackwardFilterAutotuneGroup, ConvParameters,",
          "1622:                           AutotuneEntry<se::dnn::ConvOp>>",
          "1623:     AutotuneConv3dBwdFilter;",
          "1626: class Conv3DBackpropFilterOp<GPUDevice, T> : public OpKernel {",
          "1628:   explicit Conv3DBackpropFilterOp(OpKernelConstruction* context)",
          "",
          "[Added Lines]",
          "1554: template <>",
          "1555: struct LaunchConvBackpropInputOp<Eigen::bfloat16> {",
          "1556:   static void launch(OpKernelContext* ctx, bool cudnn_use_autotune,",
          "1557:                      const Tensor& out_backprop, const Tensor& filter,",
          "1558:                      const std::vector<int32>& dilation,",
          "1559:                      const std::vector<int32>& strides, const Padding& padding,",
          "1560:                      Tensor* in_backprop, TensorFormat data_format) {",
          "1563:     auto* stream = ctx->op_device_context()->stream();",
          "1564:     const bool cast_to_float = !stream->GetCudaComputeCapability().IsAtLeast(",
          "1565:         se::CudaComputeCapability::AMPERE);",
          "1566:     Tensor casted_out_backprop = out_backprop;",
          "1567:     Tensor casted_filter = filter;",
          "1568:     Tensor casted_in_backprop = *in_backprop;",
          "1570:     if (cast_to_float) {",
          "1571:       const GPUDevice& device = ctx->eigen_device<GPUDevice>();",
          "1572:       functor::CastFunctor<GPUDevice, float, Eigen::bfloat16> cast;",
          "1573:       OP_REQUIRES_OK(ctx, ctx->allocate_temp(DT_FLOAT, out_backprop.shape(),",
          "1574:                                              &casted_out_backprop));",
          "1575:       cast(device, casted_out_backprop.template flat<float>(),",
          "1576:            out_backprop.template flat<Eigen::bfloat16>());",
          "1578:       OP_REQUIRES_OK(",
          "1579:           ctx, ctx->allocate_temp(DT_FLOAT, filter.shape(), &casted_filter));",
          "1580:       cast(device, casted_filter.template flat<float>(),",
          "1581:            filter.template flat<Eigen::bfloat16>());",
          "1583:       OP_REQUIRES_OK(ctx, ctx->allocate_temp(DT_FLOAT, in_backprop->shape(),",
          "1584:                                              &casted_in_backprop));",
          "1586:       LaunchConvBackpropInputOp<float>::launch(",
          "1587:           ctx, cudnn_use_autotune, casted_out_backprop, casted_filter, dilation,",
          "1588:           strides, padding, &casted_in_backprop, data_format);",
          "1590:       functor::CastFunctor<GPUDevice, Eigen::bfloat16, float> cast_back;",
          "1591:       const Tensor& casted_in_backprop_const = casted_in_backprop;",
          "1592:       cast_back(device, in_backprop->template flat<Eigen::bfloat16>(),",
          "1593:                 casted_in_backprop_const.template flat<float>());",
          "1594:       return;",
          "1595:     }",
          "1597:     LaunchConvBackpropInputOp<Eigen::bfloat16>::launch(",
          "1598:         ctx, cudnn_use_autotune, casted_out_backprop, casted_filter, dilation,",
          "1599:         strides, padding, &casted_in_backprop, data_format);",
          "1600:   }",
          "1601: };",
          "1604: class Conv3DBackpropInputOp<GPUDevice, T> : public OpKernel {",
          "1606:   explicit Conv3DBackpropInputOp(OpKernelConstruction* context)",
          "",
          "---------------",
          "--- Hunk 12 ---",
          "[Context before]",
          "1671:     OP_REQUIRES_OK(context, context->GetAttr(\"padding\", &padding_));",
          "1672:     cudnn_use_autotune_ = CudnnUseAutotune();",
          "1673:   }",
          "1675:   void Compute(OpKernelContext* context) override {",
          "1679:     const Tensor& out_backprop = context->input(2);",
          "1683:     if (takes_shape_) {",
          "1690:     } else {",
          "1692:     }",
          "1694:     ConvBackpropDimensions dims;",
          "1695:     OP_REQUIRES_OK(",
          "1696:         context,",
          "1697:         ConvBackpropComputeDimensionsV2(",
          "1698:             \"Conv3DBackpropFilterOp\", /*num_spatial_dims=*/3, input_shape,",
          "1706:     auto* stream = context->op_device_context()->stream();",
          "1707:     OP_REQUIRES(context, stream, errors::Internal(\"No GPU stream available.\"));",
          "1709:     bool is_grouped_convolution = filter_shape.dim_size(3) != dims.in_depth;",
          "1710:     if (!is_grouped_convolution && dims.filter_size(1) == 1 &&",
          "1711:         dims.filter_size(2) == 1 && dims.filter_size(0) == 1 &&",
          "1712:         dims.dilation(2) == 1 && dims.dilation(1) == 1 &&",
          "1713:         dims.dilation(0) == 1 && dims.stride(2) == 1 && dims.stride(1) == 1 &&",
          "1715:       const uint64 m = dims.in_depth;",
          "1716:       const uint64 k = dims.batch_size * dims.input_size(1) *",
          "1717:                        dims.input_size(2) * dims.input_size(0);",
          "",
          "[Removed Lines]",
          "1676:     const Tensor& input = context->input(0);",
          "1677:     const TensorShape& input_shape = input.shape();",
          "1680:     const TensorShape& out_backprop_shape = out_backprop.shape();",
          "1682:     TensorShape filter_shape;",
          "1684:       const Tensor& filter_sizes = context->input(1);",
          "1685:       OP_REQUIRES(context, TensorShapeUtils::IsVector(filter_sizes.shape()),",
          "1686:                   errors::InvalidArgument(",
          "1687:                       \"filter_sizes shape must be rank 1 but is rank \",",
          "1688:                       filter_sizes.shape().dims()));",
          "1689:       OP_REQUIRES_OK(context, tensor::MakeShape(filter_sizes, &filter_shape));",
          "1691:       filter_shape = context->input(1).shape();",
          "1699:             filter_shape, out_backprop_shape, dilation_, stride_, padding_,",
          "1702:     Tensor* filter_backprop;",
          "1703:     OP_REQUIRES_OK(context,",
          "1704:                    context->allocate_output(0, filter_shape, &filter_backprop));",
          "1714:         dims.stride(0) == 1 && data_format_ == FORMAT_NHWC) {",
          "",
          "[Added Lines]",
          "1653:     const Tensor& filter = context->input(1);",
          "1657:     TensorShape input_shape;",
          "1659:       const Tensor& input_sizes = context->input(0);",
          "1660:       OP_REQUIRES_OK(context, tensor::MakeShape(input_sizes, &input_shape));",
          "1662:       input_shape = context->input(0).shape();",
          "1665:     Tensor* in_backprop;",
          "1666:     OP_REQUIRES_OK(context,",
          "1667:                    context->allocate_output(0, input_shape, &in_backprop));",
          "1669:     LaunchConvBackpropInputOp<T>::launch(",
          "1670:         context, cudnn_use_autotune_, out_backprop, filter, dilation_, stride_,",
          "1671:         padding_, in_backprop, data_format_);",
          "1672:   }",
          "1674:  private:",
          "1675:   std::vector<int32> dilation_;",
          "1676:   std::vector<int32> stride_;",
          "1677:   Padding padding_;",
          "1678:   TensorFormat data_format_;",
          "1679:   bool takes_shape_;",
          "1680:   bool cudnn_use_autotune_;",
          "1681: };",
          "1684: struct Conv3dBackwardFilterAutotuneGroup {",
          "1685:   static string name() { return \"Conv3dBwdFilter\"; }",
          "1686: };",
          "1688: typedef AutotuneSingleton<Conv3dBackwardFilterAutotuneGroup, ConvParameters,",
          "1689:                           AutotuneEntry<se::dnn::ConvOp>>",
          "1690:     AutotuneConv3dBwdFilter;",
          "1692: template <typename T>",
          "1693: struct LaunchConvBackpropFilterOp {",
          "1694:   static void launch(OpKernelContext* context, bool cudnn_use_autotune,",
          "1695:                      const Tensor& input, const Tensor& out_backprop,",
          "1696:                      const std::vector<int32>& dilation,",
          "1697:                      const std::vector<int32>& stride, const Padding& padding,",
          "1698:                      Tensor* filter_backprop, TensorFormat data_format) {",
          "1699:     const TensorShape& input_shape = input.shape();",
          "1700:     const TensorShape& out_backprop_shape = out_backprop.shape();",
          "1701:     const TensorShape& filter_shape = filter_backprop->shape();",
          "1708:             filter_shape, out_backprop_shape, dilation, stride, padding,",
          "1714:     if (DataTypeToEnum<T>::value == DT_BFLOAT16 &&",
          "1715:         !stream->GetCudaComputeCapability().IsAtLeast(",
          "1716:             se::CudaComputeCapability::AMPERE)) {",
          "1717:       context->SetStatus(errors::Unimplemented(",
          "1718:           \"Conv3DBackpropFilter for GPU with bfloat16 is only supported \"",
          "1719:           \"with cuDNN on Ampere GPUs or later.\"));",
          "1720:       return;",
          "1721:     }",
          "1728:         dims.stride(0) == 1 && data_format == FORMAT_NHWC) {",
          "",
          "---------------",
          "--- Hunk 13 ---",
          "[Context before]",
          "1745:                dims.filter_size(0) == dims.input_size(0) &&",
          "1746:                dims.filter_size(1) == dims.input_size(1) &&",
          "1747:                dims.filter_size(2) == dims.input_size(2) &&",
          "1749:       const uint64 m = dims.input_size(0) * dims.input_size(1) *",
          "1750:                        dims.input_size(2) * dims.in_depth;",
          "1751:       const uint64 k = dims.batch_size;",
          "",
          "[Removed Lines]",
          "1748:                padding_ == Padding::VALID && data_format_ == FORMAT_NHWC) {",
          "",
          "[Added Lines]",
          "1762:                padding == Padding::VALID && data_format == FORMAT_NHWC) {",
          "",
          "---------------",
          "--- Hunk 14 ---",
          "[Context before]",
          "1766:       return;",
          "1767:     }",
          "1772:     const bool planes_odd = (padding_planes % 2 != 0);",
          "1773:     const bool rows_odd = (padding_rows % 2 != 0);",
          "1774:     const bool cols_odd = (padding_cols % 2 != 0);",
          "",
          "[Removed Lines]",
          "1769:     int padding_planes = dims.SpatialPadding(padding_, 0);",
          "1770:     int padding_rows = dims.SpatialPadding(padding_, 1);",
          "1771:     int padding_cols = dims.SpatialPadding(padding_, 2);",
          "",
          "[Added Lines]",
          "1783:     int padding_planes = dims.SpatialPadding(padding, 0);",
          "1784:     int padding_rows = dims.SpatialPadding(padding, 1);",
          "1785:     int padding_cols = dims.SpatialPadding(padding, 2);",
          "",
          "---------------",
          "--- Hunk 15 ---",
          "[Context before]",
          "1778:       OP_REQUIRES_OK(context,",
          "1779:                      context->allocate_temp(",
          "1780:                          DataTypeToEnum<T>::value,",
          "1782:                                          {{dims.input_size(0) + planes_odd,",
          "1783:                                            dims.input_size(1) + rows_odd,",
          "1784:                                            dims.input_size(2) + cols_odd}},",
          "",
          "[Removed Lines]",
          "1781:                          ShapeFromFormat(data_format_, dims.batch_size,",
          "",
          "[Added Lines]",
          "1795:                          ShapeFromFormat(data_format, dims.batch_size,",
          "",
          "---------------",
          "--- Hunk 16 ---",
          "[Context before]",
          "1788:           context->template eigen_device<GPUDevice>(),",
          "1789:           To32Bit(input.tensor<T, 5>()), {{0, 0, 0}},",
          "1790:           {{planes_odd, rows_odd, cols_odd}},",
          "1792:     } else {",
          "1793:       compatible_input = input;",
          "1794:     }",
          "",
          "[Removed Lines]",
          "1791:           To32Bit(compatible_input.tensor<T, 5>()), data_format_, T{});",
          "",
          "[Added Lines]",
          "1805:           To32Bit(compatible_input.tensor<T, 5>()), data_format, T{});",
          "",
          "---------------",
          "--- Hunk 17 ---",
          "[Context before]",
          "1805:     const bool compute_in_nhwc = false;",
          "1806: #endif",
          "1807:     const TensorFormat compute_data_format =",
          "1811:     VLOG(3) << \"Compute Conv3DBackpropFilter with cuDNN:\"",
          "1813:             << \" compute_data_format=\" << ToString(compute_data_format);",
          "1815:     constexpr auto kComputeInNHWC =",
          "",
          "[Removed Lines]",
          "1808:         (compute_in_nhwc && data_format_ == FORMAT_NHWC) ? FORMAT_NHWC",
          "1809:                                                          : FORMAT_NCHW;",
          "1812:             << \" data_format=\" << ToString(data_format_)",
          "",
          "[Added Lines]",
          "1822:         (compute_in_nhwc && data_format == FORMAT_NHWC) ? FORMAT_NHWC",
          "1823:                                                         : FORMAT_NCHW;",
          "1826:             << \" data_format=\" << ToString(data_format)",
          "",
          "---------------",
          "--- Hunk 18 ---",
          "[Context before]",
          "1828:     se::dnn::BatchDescriptor input_desc(3);",
          "1829:     input_desc.set_count(dims.batch_size)",
          "1830:         .set_spatial_dim(DimIndex::X,",
          "1832:         .set_spatial_dim(DimIndex::Y,",
          "1834:         .set_spatial_dim(DimIndex::Z,",
          "1836:         .set_feature_map_count(dims.in_depth)",
          "1837:         .set_layout(compute_data_layout);",
          "1838:     se::dnn::BatchDescriptor output_desc(3);",
          "",
          "[Removed Lines]",
          "1831:                          GetTensorDim(compatible_input, data_format_, '2'))",
          "1833:                          GetTensorDim(compatible_input, data_format_, '1'))",
          "1835:                          GetTensorDim(compatible_input, data_format_, '0'))",
          "",
          "[Added Lines]",
          "1845:                          GetTensorDim(compatible_input, data_format, '2'))",
          "1847:                          GetTensorDim(compatible_input, data_format, '1'))",
          "1849:                          GetTensorDim(compatible_input, data_format, '0'))",
          "",
          "---------------",
          "--- Hunk 19 ---",
          "[Context before]",
          "1877:                                           &pre_transformed_filter_backprop));",
          "1879:     Tensor transformed_out_backprop;",
          "1881:       VLOG(4) << \"Convert the `out_backprop` tensor from NDHWC to NCDHW.\";",
          "1882:       TensorShape nchw_shape = {dims.batch_size, dims.out_depth,",
          "1883:                                 dims.output_size(0), dims.output_size(1),",
          "",
          "[Removed Lines]",
          "1880:     if (data_format_ == FORMAT_NHWC && compute_data_format == FORMAT_NCHW) {",
          "",
          "[Added Lines]",
          "1894:     if (data_format == FORMAT_NHWC && compute_data_format == FORMAT_NCHW) {",
          "",
          "---------------",
          "--- Hunk 20 ---",
          "[Context before]",
          "1896:       transformed_out_backprop = out_backprop;",
          "1897:     }",
          "1898:     Tensor transformed_input;",
          "1900:       VLOG(4) << \"Convert the `input` tensor from NDHWC to NCDHW.\";",
          "1901:       TensorShape nchw_shape = {",
          "1902:           dims.batch_size, dims.in_depth, compatible_input.dim_size(1),",
          "",
          "[Removed Lines]",
          "1899:     if (data_format_ == FORMAT_NHWC && compute_data_format == FORMAT_NCHW) {",
          "",
          "[Added Lines]",
          "1913:     if (data_format == FORMAT_NHWC && compute_data_format == FORMAT_NCHW) {",
          "",
          "---------------",
          "--- Hunk 21 ---",
          "[Context before]",
          "1950:     using se::dnn::ProfileResult;",
          "1952:     auto entry_or = AutotuneUnfusedConv(",
          "1954:         conv_parameters, context, se::dnn::ConvolutionKind::BACKWARD_FILTER,",
          "1955:         input_desc, input_ptr, filter_desc, filter_backprop_ptr, conv_desc,",
          "1956:         output_desc, out_backprop_ptr, ConvolveBackwardFilterScratchSize);",
          "",
          "[Removed Lines]",
          "1953:         cudnn_use_autotune_, AutotuneConv3dBwdFilter::GetInstance(),",
          "",
          "[Added Lines]",
          "1967:         cudnn_use_autotune, AutotuneConv3dBwdFilter::GetInstance(),",
          "",
          "---------------",
          "--- Hunk 22 ---",
          "[Context before]",
          "1975:         toConstTensor(pre_transformed_filter_backprop).template tensor<T, 5>(),",
          "1976:         filter_backprop->tensor<T, 5>());",
          "1977:   }",
          "1979:  private:",
          "1980:   std::vector<int32> dilation_;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1992: };",
          "1994: template <>",
          "1995: struct LaunchConvBackpropFilterOp<Eigen::bfloat16> {",
          "1996:   static void launch(OpKernelContext* ctx, bool cudnn_use_autotune,",
          "1997:                      const Tensor& input, const Tensor& out_backprop,",
          "1998:                      const std::vector<int32>& dilation,",
          "1999:                      const std::vector<int32>& stride, const Padding& padding,",
          "2000:                      Tensor* filter_backprop, TensorFormat data_format) {",
          "2003:     auto* stream = ctx->op_device_context()->stream();",
          "2004:     const bool cast_to_float = !stream->GetCudaComputeCapability().IsAtLeast(",
          "2005:         se::CudaComputeCapability::AMPERE);",
          "2006:     Tensor casted_input = input;",
          "2007:     Tensor casted_out_backprop = out_backprop;",
          "2008:     Tensor casted_filter_backprop = *filter_backprop;",
          "2010:     if (cast_to_float) {",
          "2011:       const GPUDevice& device = ctx->eigen_device<GPUDevice>();",
          "2012:       functor::CastFunctor<GPUDevice, float, Eigen::bfloat16> cast;",
          "2013:       OP_REQUIRES_OK(",
          "2014:           ctx, ctx->allocate_temp(DT_FLOAT, input.shape(), &casted_input));",
          "2015:       cast(device, casted_input.template flat<float>(),",
          "2016:            input.template flat<Eigen::bfloat16>());",
          "2018:       OP_REQUIRES_OK(ctx, ctx->allocate_temp(DT_FLOAT, out_backprop.shape(),",
          "2019:                                              &casted_out_backprop));",
          "2020:       cast(device, casted_out_backprop.template flat<float>(),",
          "2021:            out_backprop.template flat<Eigen::bfloat16>());",
          "2023:       OP_REQUIRES_OK(ctx, ctx->allocate_temp(DT_FLOAT, filter_backprop->shape(),",
          "2024:                                              &casted_filter_backprop));",
          "2026:       LaunchConvBackpropFilterOp<float>::launch(",
          "2027:           ctx, cudnn_use_autotune, casted_input, casted_out_backprop, dilation,",
          "2028:           stride, padding, &casted_filter_backprop, data_format);",
          "2030:       functor::CastFunctor<GPUDevice, Eigen::bfloat16, float> cast_back;",
          "2031:       const Tensor& casted_filter_backprop_const = casted_filter_backprop;",
          "2032:       cast_back(device, filter_backprop->template flat<Eigen::bfloat16>(),",
          "2033:                 casted_filter_backprop_const.template flat<float>());",
          "2034:       return;",
          "2035:     }",
          "2037:     LaunchConvBackpropFilterOp<Eigen::bfloat16>::launch(",
          "2038:         ctx, cudnn_use_autotune, casted_input, casted_out_backprop, dilation,",
          "2039:         stride, padding, &casted_filter_backprop, data_format);",
          "2040:   }",
          "2041: };",
          "2043: template <typename T>",
          "2044: class Conv3DBackpropFilterOp<GPUDevice, T> : public OpKernel {",
          "2045:  public:",
          "2046:   explicit Conv3DBackpropFilterOp(OpKernelConstruction* context)",
          "2047:       : OpKernel(context),",
          "2048:         data_format_(FORMAT_NHWC),",
          "2049:         takes_shape_(type_string().find(\"V2\") != std::string::npos) {",
          "2051:     if (takes_shape_) {",
          "2052:       string data_format;",
          "2053:       OP_REQUIRES_OK(context, context->GetAttr(\"data_format\", &data_format));",
          "2054:       OP_REQUIRES(context, FormatFromString(data_format, &data_format_),",
          "2055:                   errors::InvalidArgument(\"Invalid data format\"));",
          "2056:     }",
          "2057:     OP_REQUIRES_OK(context, context->GetAttr(\"dilations\", &dilation_));",
          "2058:     OP_REQUIRES(context, dilation_.size() == 5,",
          "2059:                 errors::InvalidArgument(\"Dilation rates field must \"",
          "2060:                                         \"specify 5 dimensions\"));",
          "2061:     OP_REQUIRES(context,",
          "2062:                 (GetTensorDim(dilation_, data_format_, 'C') == 1 &&",
          "2063:                  GetTensorDim(dilation_, data_format_, 'N') == 1),",
          "2064:                 errors::InvalidArgument(",
          "2065:                     \"Current implementation does not yet support \"",
          "2066:                     \"dilation rates in the batch and depth dimensions.\"));",
          "2067:     OP_REQUIRES(",
          "2068:         context,",
          "2069:         (GetTensorDim(dilation_, data_format_, '0') > 0 &&",
          "2070:          GetTensorDim(dilation_, data_format_, '1') > 0 &&",
          "2071:          GetTensorDim(dilation_, data_format_, '2') > 0),",
          "2072:         errors::InvalidArgument(\"Dilated rates should be larger than 0.\"));",
          "2073:     OP_REQUIRES_OK(context, context->GetAttr(\"strides\", &stride_));",
          "2074:     OP_REQUIRES(context, stride_.size() == 5,",
          "2075:                 errors::InvalidArgument(\"Sliding window strides field must \"",
          "2076:                                         \"specify 5 dimensions\"));",
          "2077:     OP_REQUIRES(",
          "2078:         context,",
          "2079:         (GetTensorDim(stride_, data_format_, 'C') == 1 &&",
          "2080:          GetTensorDim(stride_, data_format_, 'N') == 1),",
          "2081:         errors::InvalidArgument(\"Current implementation does not yet support \"",
          "2082:                                 \"strides in the batch and depth dimensions.\"));",
          "2083:     OP_REQUIRES(",
          "2084:         context,",
          "2085:         (GetTensorDim(stride_, data_format_, '0') > 0 &&",
          "2086:          GetTensorDim(stride_, data_format_, '1') > 0 &&",
          "2087:          GetTensorDim(stride_, data_format_, '2') > 0),",
          "2088:         errors::InvalidArgument(\"Spatial strides should be larger than 0.\"));",
          "2089:     OP_REQUIRES_OK(context, context->GetAttr(\"padding\", &padding_));",
          "2090:     cudnn_use_autotune_ = CudnnUseAutotune();",
          "2091:   }",
          "2093:   void Compute(OpKernelContext* context) override {",
          "2094:     const Tensor& input = context->input(0);",
          "2095:     const Tensor& out_backprop = context->input(2);",
          "2097:     TensorShape filter_shape;",
          "2098:     if (takes_shape_) {",
          "2099:       const Tensor& filter_sizes = context->input(1);",
          "2100:       OP_REQUIRES(context, TensorShapeUtils::IsVector(filter_sizes.shape()),",
          "2101:                   errors::InvalidArgument(",
          "2102:                       \"filter_sizes shape must be rank 1 but is rank \",",
          "2103:                       filter_sizes.shape().dims()));",
          "2104:       OP_REQUIRES_OK(context, tensor::MakeShape(filter_sizes, &filter_shape));",
          "2105:     } else {",
          "2106:       filter_shape = context->input(1).shape();",
          "2107:     }",
          "2109:     Tensor* filter_backprop;",
          "2110:     OP_REQUIRES_OK(context,",
          "2111:                    context->allocate_output(0, filter_shape, &filter_backprop));",
          "2113:     LaunchConvBackpropFilterOp<T>::launch(",
          "2114:         context, cudnn_use_autotune_, input, out_backprop, dilation_, stride_,",
          "2115:         padding_, filter_backprop, data_format_);",
          "2116:   }",
          "",
          "---------------",
          "--- Hunk 23 ---",
          "[Context before]",
          "2003:                               .HostMemory(\"filter_sizes\"),                    \\",
          "2004:                           Conv3DBackpropFilterOp<GPUDevice, T>);",
          "2005: TF_CALL_half(REGISTER_GPU_KERNEL);",
          "2006: TF_CALL_float(REGISTER_GPU_KERNEL);",
          "2007: TF_CALL_double(REGISTER_GPU_KERNEL);",
          "2008: #undef REGISTER_GPU_KERNEL",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "2145: TF_CALL_bfloat16(REGISTER_GPU_KERNEL);",
          "",
          "---------------"
        ],
        "tensorflow/core/kernels/conv_ops.cc||tensorflow/core/kernels/conv_ops.cc": [
          "File: tensorflow/core/kernels/conv_ops.cc -> tensorflow/core/kernels/conv_ops.cc",
          "--- Hunk 1 ---",
          "[Context before]",
          "58: #include \"tensorflow/core/util/use_cudnn.h\"",
          "60: #if GOOGLE_CUDA || TENSORFLOW_USE_ROCM",
          "61: #include \"tensorflow/core/kernels/conv_ops_gpu.h\"",
          "62: #include \"tensorflow/core/platform/stream_executor.h\"",
          "63: #include \"tensorflow/core/protobuf/autotuning.pb.h\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "61: #include \"tensorflow/core/kernels/cast_op.h\"",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1077:   }",
          "1078: }",
          "1081: namespace functor {",
          "1082: #define DECLARE_GPU_SPEC(T)                                                 \\",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1081: template <>",
          "1082: void LaunchConv2DOp<GPUDevice, Eigen::bfloat16>::operator()(",
          "1083:     OpKernelContext* ctx, bool use_cudnn, bool cudnn_use_autotune,",
          "1084:     const Tensor& input_param, const Tensor& filter, int row_dilation,",
          "1085:     int col_dilation, int row_stride, int col_stride, const Padding& padding,",
          "1086:     const std::vector<int64_t>& explicit_paddings, Tensor* output,",
          "1087:     TensorFormat data_format) {",
          "1090:   auto* stream = ctx->op_device_context()->stream();",
          "1091:   const bool cast_to_float = !stream->GetCudaComputeCapability().IsAtLeast(",
          "1092:       se::CudaComputeCapability::AMPERE);",
          "1093:   Tensor casted_input = input_param;",
          "1094:   Tensor casted_filter = filter;",
          "1095:   Tensor casted_out = *output;",
          "1097:   if (cast_to_float) {",
          "1098:     const GPUDevice& device = ctx->eigen_device<GPUDevice>();",
          "1099:     functor::CastFunctor<GPUDevice, float, Eigen::bfloat16> cast;",
          "1100:     OP_REQUIRES_OK(",
          "1101:         ctx, ctx->allocate_temp(DT_FLOAT, input_param.shape(), &casted_input));",
          "1102:     cast(device, casted_input.template flat<float>(),",
          "1103:          input_param.template flat<Eigen::bfloat16>());",
          "1105:     OP_REQUIRES_OK(",
          "1106:         ctx, ctx->allocate_temp(DT_FLOAT, filter.shape(), &casted_filter));",
          "1107:     cast(device, casted_filter.template flat<float>(),",
          "1108:          filter.template flat<Eigen::bfloat16>());",
          "1110:     OP_REQUIRES_OK(ctx,",
          "1111:                    ctx->allocate_temp(DT_FLOAT, output->shape(), &casted_out));",
          "1113:     LaunchConv2DOp<GPUDevice, float>()(",
          "1114:         ctx, use_cudnn, cudnn_use_autotune, casted_input, casted_filter,",
          "1115:         row_dilation, col_dilation, row_stride, col_stride, padding,",
          "1116:         explicit_paddings, &casted_out, data_format);",
          "1118:     functor::CastFunctor<GPUDevice, Eigen::bfloat16, float> cast_back;",
          "1119:     const Tensor& casted_out_const = casted_out;",
          "1120:     cast_back(device, output->template flat<Eigen::bfloat16>(),",
          "1121:               casted_out_const.template flat<float>());",
          "1122:     return;",
          "1123:   }",
          "1125:   LaunchConv2DOp<GPUDevice, Eigen::bfloat16>()(",
          "1126:       ctx, use_cudnn, cudnn_use_autotune, casted_input, casted_filter,",
          "1127:       row_dilation, col_dilation, row_stride, col_stride, padding,",
          "1128:       explicit_paddings, &casted_out, data_format);",
          "1129: }",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "1123: DECLARE_GPU_SPEC(float);",
          "1124: DECLARE_GPU_SPEC(Eigen::half);",
          "1125: DECLARE_GPU_SPEC(double);",
          "1126: DECLARE_GPU_SPEC(int32);",
          "1127: #undef DECLARE_GPU_SPEC",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1176: DECLARE_GPU_SPEC(Eigen::bfloat16);",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "1132: REGISTER_KERNEL_BUILDER(",
          "1133:     Name(\"Conv2D\").Device(DEVICE_GPU).TypeConstraint<Eigen::half>(\"T\"),",
          "1134:     Conv2DOp<GPUDevice, Eigen::half>);",
          "1135: REGISTER_KERNEL_BUILDER(",
          "1136:     Name(\"Conv2D\").Device(DEVICE_GPU).TypeConstraint<float>(\"T\"),",
          "1137:     Conv2DOp<GPUDevice, float>);",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1187: REGISTER_KERNEL_BUILDER(",
          "1188:     Name(\"Conv2D\").Device(DEVICE_GPU).TypeConstraint<Eigen::bfloat16>(\"T\"),",
          "1189:     Conv2DOp<GPUDevice, Eigen::bfloat16>);",
          "",
          "---------------"
        ],
        "tensorflow/core/kernels/conv_ops_3d.cc||tensorflow/core/kernels/conv_ops_3d.cc": [
          "File: tensorflow/core/kernels/conv_ops_3d.cc -> tensorflow/core/kernels/conv_ops_3d.cc",
          "--- Hunk 1 ---",
          "[Context before]",
          "36: #include \"tensorflow/core/util/use_cudnn.h\"",
          "38: #if GOOGLE_CUDA || TENSORFLOW_USE_ROCM",
          "39: #include \"tensorflow/core/platform/stream_executor.h\"",
          "40: #include \"tensorflow/core/protobuf/autotuning.pb.h\"",
          "41: #include \"tensorflow/core/util/autotune_maps/conv_parameters.h\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "39: #include \"tensorflow/core/kernels/cast_op.h\"",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "527:   }",
          "528: };",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "532: template <>",
          "533: struct LaunchConvOp<GPUDevice, Eigen::bfloat16> {",
          "534:   static void launch(OpKernelContext* ctx, bool cudnn_use_autotune,",
          "535:                      const Tensor& input_param, const Tensor& filter,",
          "536:                      const std::array<int64, 3>& dilations,",
          "537:                      const std::array<int64, 3>& strides, const Padding padding,",
          "538:                      TensorFormat data_format, Tensor* output) {",
          "541:     auto* stream = ctx->op_device_context()->stream();",
          "542:     const bool cast_to_float = !stream->GetCudaComputeCapability().IsAtLeast(",
          "543:         se::CudaComputeCapability::AMPERE);",
          "544:     Tensor casted_input = input_param;",
          "545:     Tensor casted_filter = filter;",
          "546:     Tensor casted_out = *output;",
          "548:     if (cast_to_float) {",
          "549:       const GPUDevice& device = ctx->eigen_device<GPUDevice>();",
          "550:       functor::CastFunctor<GPUDevice, float, Eigen::bfloat16> cast;",
          "551:       OP_REQUIRES_OK(ctx, ctx->allocate_temp(DT_FLOAT, input_param.shape(),",
          "552:                                              &casted_input));",
          "553:       cast(device, casted_input.template flat<float>(),",
          "554:            input_param.template flat<Eigen::bfloat16>());",
          "556:       OP_REQUIRES_OK(",
          "557:           ctx, ctx->allocate_temp(DT_FLOAT, filter.shape(), &casted_filter));",
          "558:       cast(device, casted_filter.template flat<float>(),",
          "559:            filter.template flat<Eigen::bfloat16>());",
          "561:       OP_REQUIRES_OK(",
          "562:           ctx, ctx->allocate_temp(DT_FLOAT, output->shape(), &casted_out));",
          "564:       LaunchConvOp<GPUDevice, float>::launch(",
          "565:           ctx, cudnn_use_autotune, casted_input, casted_filter, dilations,",
          "566:           strides, padding, data_format, &casted_out);",
          "568:       functor::CastFunctor<GPUDevice, Eigen::bfloat16, float> cast_back;",
          "569:       const Tensor& casted_out_const = casted_out;",
          "570:       cast_back(device, output->template flat<Eigen::bfloat16>(),",
          "571:                 casted_out_const.template flat<float>());",
          "572:       return;",
          "573:     }",
          "575:     LaunchConvOp<GPUDevice, Eigen::bfloat16>::launch(",
          "576:         ctx, cudnn_use_autotune, casted_input, casted_filter, dilations,",
          "577:         strides, padding, data_format, &casted_out);",
          "578:   }",
          "579: };",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "559:       typename TTypes<T, 5>::Tensor out);",
          "561: DECLARE_GPU_SPEC(Eigen::half);",
          "562: DECLARE_GPU_SPEC(float);",
          "563: DECLARE_GPU_SPEC(double);",
          "564: #undef DECLARE_GPU_SPEC",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "613: DECLARE_GPU_SPEC(Eigen::bfloat16);",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "569: REGISTER_KERNEL_BUILDER(",
          "570:     Name(\"Conv3D\").Device(DEVICE_GPU).TypeConstraint<Eigen::half>(\"T\"),",
          "571:     Conv3DOp<GPUDevice, Eigen::half>);",
          "572: REGISTER_KERNEL_BUILDER(",
          "573:     Name(\"Conv3D\").Device(DEVICE_GPU).TypeConstraint<float>(\"T\"),",
          "574:     Conv3DOp<GPUDevice, float>);",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "624: REGISTER_KERNEL_BUILDER(",
          "625:     Name(\"Conv3D\").Device(DEVICE_GPU).TypeConstraint<Eigen::bfloat16>(\"T\"),",
          "626:     Conv3DOp<GPUDevice, Eigen::bfloat16>);",
          "",
          "---------------"
        ],
        "tensorflow/core/kernels/conv_ops_gpu.cc||tensorflow/core/kernels/conv_ops_gpu.cc": [
          "File: tensorflow/core/kernels/conv_ops_gpu.cc -> tensorflow/core/kernels/conv_ops_gpu.cc",
          "--- Hunk 1 ---",
          "[Context before]",
          "46:   bool use_nhwc_fp16 =",
          "47:       data_type == DT_HALF && stream->GetCudaComputeCapability().IsAtLeast(",
          "48:                                   se::CudaComputeCapability::VOLTA);",
          "49:   if (use_4d_tensor) {",
          "51:   }",
          "53: #else",
          "55:   return false;",
          "",
          "[Removed Lines]",
          "50:     return use_nhwc_fp16 || use_nhwc_tf32;",
          "52:   return CUDNN_VERSION >= 8000 && (use_nhwc_fp16 || use_nhwc_tf32);",
          "",
          "[Added Lines]",
          "49:   bool use_nhwc_bf16 =",
          "50:       data_type == DT_BFLOAT16 && stream->GetCudaComputeCapability().IsAtLeast(",
          "51:                                       se::CudaComputeCapability::AMPERE);",
          "53:     return use_nhwc_fp16 || use_nhwc_tf32 || use_nhwc_bf16;",
          "55:   return CUDNN_VERSION >= 8000 &&",
          "56:          (use_nhwc_fp16 || use_nhwc_tf32 || use_nhwc_bf16);",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "397:   return autotune_entry;",
          "398: }",
          "437: }  // namespace tensorflow",
          "",
          "[Removed Lines]",
          "400: template StatusOr<AutotuneEntry<se::dnn::ConvOp>> AutotuneUnfusedConv<double>(",
          "401:     bool cudnn_use_autotune,",
          "402:     AutotuneMap<ConvParameters, AutotuneEntry<se::dnn::ConvOp>>* autotune_map,",
          "403:     const ConvParameters& conv_parameters, OpKernelContext* ctx,",
          "404:     se::dnn::ConvolutionKind kind, const se::dnn::BatchDescriptor& input_desc,",
          "405:     se::DeviceMemory<double> input_ptr,",
          "406:     const se::dnn::FilterDescriptor& filter_desc,",
          "407:     se::DeviceMemory<double> filter_ptr,",
          "408:     const se::dnn::ConvolutionDescriptor& conv_desc,",
          "409:     const se::dnn::BatchDescriptor& output_desc,",
          "410:     se::DeviceMemory<double> output_ptr, int64_t scratch_size_limit);",
          "412: template StatusOr<AutotuneEntry<se::dnn::ConvOp>> AutotuneUnfusedConv<float>(",
          "413:     bool cudnn_use_autotune,",
          "414:     AutotuneMap<ConvParameters, AutotuneEntry<se::dnn::ConvOp>>* autotune_map,",
          "415:     const ConvParameters& conv_parameters, OpKernelContext* ctx,",
          "416:     se::dnn::ConvolutionKind kind, const se::dnn::BatchDescriptor& input_desc,",
          "417:     se::DeviceMemory<float> input_ptr,",
          "418:     const se::dnn::FilterDescriptor& filter_desc,",
          "419:     se::DeviceMemory<float> filter_ptr,",
          "420:     const se::dnn::ConvolutionDescriptor& conv_desc,",
          "421:     const se::dnn::BatchDescriptor& output_desc,",
          "422:     se::DeviceMemory<float> output_ptr, int64_t scratch_size_limit);",
          "424: template StatusOr<AutotuneEntry<se::dnn::ConvOp>>",
          "425: AutotuneUnfusedConv<Eigen::half>(",
          "426:     bool cudnn_use_autotune,",
          "427:     AutotuneMap<ConvParameters, AutotuneEntry<se::dnn::ConvOp>>* autotune_map,",
          "428:     const ConvParameters& conv_parameters, OpKernelContext* ctx,",
          "429:     se::dnn::ConvolutionKind kind, const se::dnn::BatchDescriptor& input_desc,",
          "430:     se::DeviceMemory<Eigen::half> input_ptr,",
          "431:     const se::dnn::FilterDescriptor& filter_desc,",
          "432:     se::DeviceMemory<Eigen::half> filter_ptr,",
          "433:     const se::dnn::ConvolutionDescriptor& conv_desc,",
          "434:     const se::dnn::BatchDescriptor& output_desc,",
          "435:     se::DeviceMemory<Eigen::half> output_ptr, int64_t scratch_size_limit);",
          "",
          "[Added Lines]",
          "404: #define DECLARE_GPU_SPEC(T)                                                 \\",
          "405:   template StatusOr<AutotuneEntry<se::dnn::ConvOp>> AutotuneUnfusedConv<T>( \\",
          "406:       bool cudnn_use_autotune,                                              \\",
          "407:       AutotuneMap<ConvParameters, AutotuneEntry<se::dnn::ConvOp>>*          \\",
          "408:           autotune_map,                                                     \\",
          "409:       const ConvParameters& conv_parameters, OpKernelContext* ctx,          \\",
          "410:       se::dnn::ConvolutionKind kind,                                        \\",
          "411:       const se::dnn::BatchDescriptor& input_desc,                           \\",
          "412:       se::DeviceMemory<T> input_ptr,                                        \\",
          "413:       const se::dnn::FilterDescriptor& filter_desc,                         \\",
          "414:       se::DeviceMemory<T> filter_ptr,                                       \\",
          "415:       const se::dnn::ConvolutionDescriptor& conv_desc,                      \\",
          "416:       const se::dnn::BatchDescriptor& output_desc,                          \\",
          "417:       se::DeviceMemory<T> output_ptr, int64_t scratch_size_limit);",
          "419: DECLARE_GPU_SPEC(double);",
          "420: DECLARE_GPU_SPEC(float);",
          "421: DECLARE_GPU_SPEC(Eigen::half);",
          "422: DECLARE_GPU_SPEC(Eigen::bfloat16);",
          "424: #undef DECLARE_GPU_SPEC",
          "",
          "---------------"
        ],
        "tensorflow/core/kernels/depthwise_conv_grad_op.cc||tensorflow/core/kernels/depthwise_conv_grad_op.cc": [
          "File: tensorflow/core/kernels/depthwise_conv_grad_op.cc -> tensorflow/core/kernels/depthwise_conv_grad_op.cc",
          "--- Hunk 1 ---",
          "[Context before]",
          "4: you may not use this file except in compliance with the License.",
          "5: You may obtain a copy of the License at",
          "9: Unless required by applicable law or agreed to in writing, software",
          "10: distributed under the License is distributed on an \"AS IS\" BASIS,",
          "",
          "[Removed Lines]",
          "7:     http://www.apache.org/licenses/LICENSE-2.0",
          "",
          "[Added Lines]",
          "7:    http://www.apache.org/licenses/LICENSE-2.0",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "39: #include \"tensorflow/core/util/use_cudnn.h\"",
          "40: #include \"tensorflow/core/util/work_sharder.h\"",
          "44: #if GOOGLE_CUDA",
          "45: #include \"third_party/gpus/cudnn/cudnn.h\"",
          "46: #endif",
          "51: namespace tensorflow {",
          "",
          "[Removed Lines]",
          "42: #if GOOGLE_CUDA || TENSORFLOW_USE_ROCM",
          "48: #include \"tensorflow/core/platform/stream_executor.h\"",
          "49: #endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "537: #if GOOGLE_CUDA || TENSORFLOW_USE_ROCM",
          "540: extern template struct LaunchConv2DBackpropInputOp<GPUDevice, Eigen::half>;",
          "541: extern template struct LaunchConv2DBackpropInputOp<GPUDevice, float>;",
          "542: extern template struct LaunchConv2DBackpropInputOp<GPUDevice, double>;",
          "545: extern template struct LaunchDepthwiseConvBackpropInputOp<GPUDevice,",
          "546:                                                           Eigen::half>;",
          "547: extern template struct LaunchDepthwiseConvBackpropInputOp<GPUDevice, float>;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "535: extern template struct LaunchConv2DBackpropInputOp<GPUDevice, Eigen::bfloat16>;",
          "541: extern template struct LaunchDepthwiseConvBackpropInputOp<GPUDevice,",
          "542:                                                           Eigen::bfloat16>;",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "596:     use_cudnn_grouped_conv_ =",
          "598:         ((data_format_ == FORMAT_NCHW && stride_ == 1 && stride_w == 1) ||",
          "599:          (data_format_ == FORMAT_NHWC && stride_ == stride_w &&",
          "600:           (stride_ == 1 || stride_ == 2)));",
          "",
          "[Removed Lines]",
          "597:         dtype_ == DT_HALF &&",
          "",
          "[Added Lines]",
          "595:         (dtype_ == DT_HALF || dtype_ == DT_BFLOAT16) &&",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "644:     bool use_cudnn =",
          "645:         std::is_same<Device, GPUDevice>::value &&",
          "650:     VLOG(2) << \"DepthwiseConv2dNativeBackpropInput: \"",
          "651:             << \" Input: [\" << batch << \", \" << input_rows << \", \" << input_cols",
          "",
          "[Removed Lines]",
          "646:         (in_depth == 1 || (use_cudnn_grouped_conv_ &&",
          "647:                            ShouldCudnnGroupedConvolutionBeUsed(",
          "648:                                filter_rows, filter_cols, in_depth, out_depth)));",
          "",
          "[Added Lines]",
          "644:         (in_depth == 1 ||",
          "645:          (use_cudnn_grouped_conv_ && UseCudnnWith16BitFloat(context, dtype_) &&",
          "646:           ShouldCudnnGroupedConvolutionBeUsed(filter_rows, filter_cols,",
          "647:                                               in_depth, out_depth)));",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "731:                               .HostMemory(\"input_sizes\"),            \\",
          "732:                           DepthwiseConv2dNativeBackpropInputOp<GPUDevice, T>)",
          "734: TF_CALL_half(REGISTER_GPU_KERNEL);",
          "735: TF_CALL_float(REGISTER_GPU_KERNEL);",
          "736: TF_CALL_double(REGISTER_GPU_KERNEL);",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "733: TF_CALL_bfloat16(REGISTER_GPU_KERNEL);",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "755:                               .Label(\"cudnn_grouped_convolution\"),   \\",
          "756:                           DepthwiseConv2dGroupedConvBackpropInputOp<T>)",
          "758: TF_CALL_half(REGISTER_GROUPED_CONV_KERNEL);",
          "759: TF_CALL_float(REGISTER_GROUPED_CONV_KERNEL);",
          "760: TF_CALL_double(REGISTER_GROUPED_CONV_KERNEL);",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "758: TF_CALL_bfloat16(REGISTER_GROUPED_CONV_KERNEL);",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "1040: #if GOOGLE_CUDA || TENSORFLOW_USE_ROCM",
          "1043: extern template struct LaunchConv2DBackpropFilterOp<GPUDevice, Eigen::half>;",
          "1044: extern template struct LaunchConv2DBackpropFilterOp<GPUDevice, float>;",
          "1045: extern template struct LaunchConv2DBackpropFilterOp<GPUDevice, double>;",
          "1048: extern template struct LaunchDepthwiseConvBackpropFilterOp<GPUDevice,",
          "1049:                                                            Eigen::half>;",
          "1050: extern template struct LaunchDepthwiseConvBackpropFilterOp<GPUDevice, float>;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1044: extern template struct LaunchConv2DBackpropFilterOp<GPUDevice, Eigen::bfloat16>;",
          "1050: extern template struct LaunchDepthwiseConvBackpropFilterOp<GPUDevice,",
          "1051:                                                            Eigen::bfloat16>;",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "1100:     } else {",
          "1101:       LOG(ERROR) << \"Only bfloat16, half, float, and double are supported.\";",
          "1102:     }",
          "1113: #else",
          "1114:     use_cudnn_grouped_conv_ = false;",
          "1115: #endif",
          "",
          "[Removed Lines]",
          "1103: #if CUDNN_VERSION >= 7603",
          "1112:     use_cudnn_grouped_conv_ = OpDeterminismRequired() || dtype_ == DT_HALF;",
          "",
          "[Added Lines]",
          "1107: #if CUDNN_VERSION >= 8000",
          "1108:     use_cudnn_grouped_conv_ = dtype_ == DT_HALF || dtype_ == DT_BFLOAT16;",
          "1109: #elif CUDNN_VERSION >= 7603",
          "1113:     use_cudnn_grouped_conv_ = dtype_ == DT_HALF;",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "1156:     VLOG(2) << \"DepthwiseConv2dNativeBackpropFilter: \"",
          "1157:             << \" Input: [\" << batch << \", \" << input_rows << \", \" << input_cols",
          "",
          "[Removed Lines]",
          "1149:     bool use_cudnn = std::is_same<Device, GPUDevice>::value &&",
          "1150:                      (in_depth == 1 ||",
          "1151:                       (use_cudnn_grouped_conv_ &&",
          "1152:                        (ShouldCudnnGroupedConvolutionBeUsed(",
          "1153:                             filter_rows, filter_cols, in_depth, out_depth) ||",
          "1154:                         OpDeterminismRequired())));",
          "",
          "[Added Lines]",
          "1155:     bool determinism_required = false;",
          "1156: #if CUDNN_VERSION >= 7603",
          "1157:     determinism_required = OpDeterminismRequired();",
          "1158: #endif  // CUDNN_VERSION >= 7603",
          "1159:     bool use_cudnn =",
          "1160:         std::is_same<Device, GPUDevice>::value &&",
          "1161:         (in_depth == 1 || determinism_required ||",
          "1162:          (use_cudnn_grouped_conv_ && UseCudnnWith16BitFloat(context, dtype_) &&",
          "1163:           ShouldCudnnGroupedConvolutionBeUsed(filter_rows, filter_cols,",
          "1164:                                               in_depth, out_depth)));",
          "",
          "---------------",
          "--- Hunk 11 ---",
          "[Context before]",
          "1189:       return;",
          "1190:     }",
          "1196:                                    std::is_same<Device, GPUDevice>::value;",
          "1197:     using U = typename std::conditional<cast_to_float, float, T>::type;",
          "1198:     Tensor casted_out_backprop = out_backprop;",
          "",
          "[Removed Lines]",
          "1195:     constexpr bool cast_to_float = std::is_same<T, Eigen::half>::value &&",
          "",
          "[Added Lines]",
          "1205:     constexpr bool cast_to_float = (std::is_same<T, Eigen::half>::value ||",
          "1206:                                     std::is_same<T, Eigen::bfloat16>::value) &&",
          "",
          "---------------",
          "--- Hunk 12 ---",
          "[Context before]",
          "1200:     Tensor casted_filter_backprop = *filter_backprop;",
          "1201:     const Device& device = context->template eigen_device<Device>();",
          "1202:     if (cast_to_float) {",
          "1204:       OP_REQUIRES_OK(context,",
          "1205:                      context->allocate_temp(DT_FLOAT, out_backprop.shape(),",
          "1206:                                             &casted_out_backprop));",
          "1207:       cast(device, casted_out_backprop.template flat<float>(),",
          "1209:       OP_REQUIRES_OK(context, context->allocate_temp(DT_FLOAT, input.shape(),",
          "1210:                                                      &casted_input));",
          "1211:       cast(device, casted_input.template flat<float>(),",
          "1213:       OP_REQUIRES_OK(context,",
          "1214:                      context->allocate_temp(DT_FLOAT, filter_backprop->shape(),",
          "1215:                                             &casted_filter_backprop));",
          "",
          "[Removed Lines]",
          "1203:       functor::CastFunctor<Device, float, Eigen::half> cast;",
          "1208:            out_backprop.template flat<Eigen::half>());",
          "1212:            input.template flat<Eigen::half>());",
          "",
          "[Added Lines]",
          "1214:       functor::CastFunctor<Device, float, T> cast;",
          "1219:            out_backprop.template flat<T>());",
          "1223:            input.template flat<T>());",
          "",
          "---------------",
          "--- Hunk 13 ---",
          "[Context before]",
          "1223:         data_format_);",
          "1225:     if (cast_to_float) {",
          "1227:       const Tensor& casted_filter_backprop_const = casted_filter_backprop;",
          "1229:            casted_filter_backprop_const.template flat<float>());",
          "1230:     }",
          "1231:   }",
          "",
          "[Removed Lines]",
          "1226:       functor::CastFunctor<Device, Eigen::half, float> cast;",
          "1228:       cast(device, filter_backprop->template flat<Eigen::half>(),",
          "",
          "[Added Lines]",
          "1237:       functor::CastFunctor<Device, T, float> cast;",
          "1239:       cast(device, filter_backprop->template flat<T>(),",
          "",
          "---------------",
          "--- Hunk 14 ---",
          "[Context before]",
          "1270:                               .HostMemory(\"filter_sizes\"),            \\",
          "1271:                           DepthwiseConv2dNativeBackpropFilterOp<GPUDevice, T>)",
          "1273: TF_CALL_half(REGISTER_GPU_KERNEL);",
          "1274: TF_CALL_float(REGISTER_GPU_KERNEL);",
          "1275: TF_CALL_double(REGISTER_GPU_KERNEL);",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1284: TF_CALL_bfloat16(REGISTER_GPU_KERNEL);",
          "",
          "---------------",
          "--- Hunk 15 ---",
          "[Context before]",
          "1294:                               .Label(\"cudnn_grouped_convolution\"),    \\",
          "1295:                           DepthwiseConv2dGroupedConvBackpropFilterOp<T>)",
          "1297: TF_CALL_half(REGISTER_GROUPED_CONV_KERNEL);",
          "1298: TF_CALL_float(REGISTER_GROUPED_CONV_KERNEL);",
          "1299: TF_CALL_double(REGISTER_GROUPED_CONV_KERNEL);",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1309: TF_CALL_bfloat16(REGISTER_GROUPED_CONV_KERNEL);",
          "",
          "---------------"
        ],
        "tensorflow/core/kernels/depthwise_conv_op.cc||tensorflow/core/kernels/depthwise_conv_op.cc": [
          "File: tensorflow/core/kernels/depthwise_conv_op.cc -> tensorflow/core/kernels/depthwise_conv_op.cc",
          "--- Hunk 1 ---",
          "[Context before]",
          "41: #include \"tensorflow/core/util/use_cudnn.h\"",
          "42: #include \"tensorflow/core/util/work_sharder.h\"",
          "46: #if GOOGLE_CUDA",
          "47: #include \"third_party/gpus/cudnn/cudnn.h\"",
          "48: #endif",
          "53: namespace tensorflow {",
          "",
          "[Removed Lines]",
          "44: #if GOOGLE_CUDA || TENSORFLOW_USE_ROCM",
          "50: #include \"tensorflow/core/platform/stream_executor.h\"",
          "51: #endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "62: typedef Eigen::ThreadPoolDevice CPUDevice;",
          "63: typedef Eigen::GpuDevice GPUDevice;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "60: bool UseCudnnWith16BitFloat(OpKernelContext* ctx, DataType dtype) {",
          "61: #if GOOGLE_CUDA",
          "62:   if (dtype == DT_HALF) {",
          "63:     return true;",
          "64:   } else if (dtype == DT_BFLOAT16) {",
          "65:     auto* stream = ctx->op_device_context()->stream();",
          "66:     if (!stream) return false;",
          "67:     return stream->GetCudaComputeCapability().IsAtLeast(",
          "68:         se::CudaComputeCapability::AMPERE);",
          "69:   }",
          "70: #endif",
          "71:   return false;",
          "72: }",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "258: #if GOOGLE_CUDA || TENSORFLOW_USE_ROCM",
          "261: extern template struct LaunchConv2DOp<GPUDevice, Eigen::half>;",
          "262: extern template struct LaunchConv2DOp<GPUDevice, float>;",
          "263: extern template struct LaunchConv2DOp<GPUDevice, double>;",
          "266: extern template struct LaunchDepthwiseConvOp<GPUDevice, Eigen::half>;",
          "267: extern template struct LaunchDepthwiseConvOp<GPUDevice, float>;",
          "268: extern template struct LaunchDepthwiseConvOp<GPUDevice, double>;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "270: extern template struct LaunchConv2DOp<GPUDevice, Eigen::bfloat16>;",
          "276: extern template struct LaunchDepthwiseConvOp<GPUDevice, Eigen::bfloat16>;",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "331:     use_cudnn_grouped_conv_ =",
          "333:         (data_format_ == FORMAT_NCHW ||",
          "334:          (data_format_ == FORMAT_NHWC && stride_ == stride_w &&",
          "335:           (stride_ == 1 || stride_ == 2)));",
          "",
          "[Removed Lines]",
          "332:         dtype_ == DT_HALF &&",
          "",
          "[Added Lines]",
          "343:         (dtype_ == DT_HALF || dtype_ == DT_BFLOAT16) &&",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "429:     bool use_cudnn =",
          "430:         std::is_same<Device, GPUDevice>::value &&",
          "435:     VLOG(2) << \"DepthwiseConv2dNative: \"",
          "436:             << \" Input: [\" << batch << \", \" << input_rows << \", \" << input_cols",
          "",
          "[Removed Lines]",
          "431:         (in_depth == 1 || (use_cudnn_grouped_conv_ &&",
          "432:                            ShouldCudnnGroupedConvolutionBeUsed(",
          "433:                                filter_rows, filter_cols, in_depth, out_depth)));",
          "",
          "[Added Lines]",
          "442:         (in_depth == 1 ||",
          "443:          (use_cudnn_grouped_conv_ && UseCudnnWith16BitFloat(context, dtype_) &&",
          "444:           ShouldCudnnGroupedConvolutionBeUsed(filter_rows, filter_cols,",
          "445:                                               in_depth, out_depth)));",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "527:       Name(\"DepthwiseConv2dNative\").Device(DEVICE_GPU).TypeConstraint<T>(\"T\"), \\",
          "528:       DepthwiseConv2dNativeOp<GPUDevice, T>)",
          "530: TF_CALL_half(REGISTER_GPU_KERNEL);",
          "531: TF_CALL_float(REGISTER_GPU_KERNEL);",
          "532: TF_CALL_double(REGISTER_GPU_KERNEL);",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "542: TF_CALL_bfloat16(REGISTER_GPU_KERNEL);",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "549:                               .Label(\"cudnn_grouped_convolution\"), \\",
          "550:                           DepthwiseConv2dGroupedConvOp<T>)",
          "552: TF_CALL_half(REGISTER_GROUPED_CONV_KERNEL);",
          "553: TF_CALL_float(REGISTER_GROUPED_CONV_KERNEL);",
          "554: TF_CALL_double(REGISTER_GROUPED_CONV_KERNEL);",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "565: TF_CALL_bfloat16(REGISTER_GROUPED_CONV_KERNEL);",
          "",
          "---------------"
        ],
        "tensorflow/core/kernels/depthwise_conv_op.h||tensorflow/core/kernels/depthwise_conv_op.h": [
          "File: tensorflow/core/kernels/depthwise_conv_op.h -> tensorflow/core/kernels/depthwise_conv_op.h",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: #include \"tensorflow/core/framework/types.h\"",
          "21: #include \"tensorflow/core/util/tensor_format.h\"",
          "23: namespace tensorflow {",
          "25: struct DepthwiseArgs {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "23: #if GOOGLE_CUDA || TENSORFLOW_USE_ROCM",
          "24: #include \"tensorflow/core/platform/stream_executor.h\"",
          "25: #endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "80:                   TensorFormat data_format);",
          "81: };",
          "83: #if GOOGLE_CUDA || TENSORFLOW_USE_ROCM",
          "84: template <typename T>",
          "85: struct LaunchDepthwiseConvOp<Eigen::GpuDevice, T> {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "87: bool UseCudnnWith16BitFloat(OpKernelContext* ctx, DataType dtype);",
          "",
          "---------------"
        ],
        "tensorflow/core/kernels/depthwise_conv_op_gpu.h||tensorflow/core/kernels/depthwise_conv_op_gpu.h": [
          "File: tensorflow/core/kernels/depthwise_conv_op_gpu.h -> tensorflow/core/kernels/depthwise_conv_op_gpu.h",
          "--- Hunk 1 ---",
          "[Context before]",
          "47: struct PseudoHalfType<Eigen::half> {",
          "48:   using Type = float;",
          "49: };",
          "50: }  // namespace detail",
          "52: using Eigen::GpuDevice;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "50: template <>",
          "51: struct PseudoHalfType<Eigen::bfloat16> {",
          "52:   using Type = float;",
          "53: };",
          "",
          "---------------"
        ],
        "tensorflow/core/kernels/depthwise_conv_op_gpu_bfloat16.cu.cc||tensorflow/core/kernels/depthwise_conv_op_gpu_bfloat16.cu.cc": [
          "File: tensorflow/core/kernels/depthwise_conv_op_gpu_bfloat16.cu.cc -> tensorflow/core/kernels/depthwise_conv_op_gpu_bfloat16.cu.cc",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "3: Licensed under the Apache License, Version 2.0 (the \"License\");",
          "4: you may not use this file except in compliance with the License.",
          "5: You may obtain a copy of the License at",
          "7:     http://www.apache.org/licenses/LICENSE-2.0",
          "9: Unless required by applicable law or agreed to in writing, software",
          "10: distributed under the License is distributed on an \"AS IS\" BASIS,",
          "11: WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
          "12: See the License for the specific language governing permissions and",
          "13: limitations under the License.",
          "16: #if GOOGLE_CUDA || TENSORFLOW_USE_ROCM",
          "17: #define EIGEN_USE_GPU",
          "19: #include \"tensorflow/core/kernels/depthwise_conv_op.h\"",
          "20: #include \"tensorflow/core/kernels/depthwise_conv_op_gpu.h\"",
          "22: namespace tensorflow {",
          "23: using Eigen::GpuDevice;",
          "25: template struct LaunchDepthwiseConvOp<GpuDevice, Eigen::bfloat16>;",
          "26: template struct LaunchDepthwiseConvBackpropInputOp<GpuDevice, Eigen::bfloat16>;",
          "27: template struct LaunchDepthwiseConvBackpropFilterOp<GpuDevice, Eigen::bfloat16>;",
          "28: }  // namespace tensorflow",
          "30: #endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM",
          "",
          "---------------"
        ],
        "tensorflow/core/util/gpu_device_functions.h||tensorflow/core/util/gpu_device_functions.h": [
          "File: tensorflow/core/util/gpu_device_functions.h -> tensorflow/core/util/gpu_device_functions.h",
          "--- Hunk 1 ---",
          "[Context before]",
          "663:   }",
          "664: }",
          "666: template <typename F>",
          "667: __device__ long long GpuAtomicCasHelper(long long* ptr, F accumulate) {",
          "668:   return static_cast<long long>(",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "666: template <typename F>",
          "667: __device__ Eigen::bfloat16 GpuAtomicCasHelper(Eigen::bfloat16* ptr,",
          "668:                                               F accumulate) {",
          "669:   Eigen::half ret = detail::GpuAtomicCasHelper(",
          "670:       reinterpret_cast<Eigen::half*>(ptr), [accumulate](Eigen::half a) {",
          "671:         Eigen::bfloat16 acc =",
          "672:             accumulate(Eigen::numext::bit_cast<Eigen::bfloat16>(a));",
          "673:         return Eigen::numext::bit_cast<Eigen::half>(acc);",
          "674:       });",
          "675:   return Eigen::numext::bit_cast<Eigen::bfloat16>(ret);",
          "676: }",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "725:       ptr, [value](Eigen::half a) { return a + value; });",
          "726: }",
          "728: #if (__CUDA_ARCH__ < 600) || TENSORFLOW_USE_ROCM",
          "729: __device__ inline double GpuAtomicAdd(double* ptr, double value) {",
          "730:   return detail::GpuAtomicCasHelper(ptr,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "740: __device__ inline Eigen::bfloat16 GpuAtomicAdd(Eigen::bfloat16* ptr,",
          "741:                                                Eigen::bfloat16 value) {",
          "742:   return detail::GpuAtomicCasHelper(",
          "743:       ptr, [value](Eigen::bfloat16 a) { return a + value; });",
          "744: }",
          "",
          "---------------"
        ],
        "tensorflow/python/kernel_tests/nn_ops/conv_ops_test.py||tensorflow/python/kernel_tests/nn_ops/conv_ops_test.py": [
          "File: tensorflow/python/kernel_tests/nn_ops/conv_ops_test.py -> tensorflow/python/kernel_tests/nn_ops/conv_ops_test.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "172:         out.append(dtypes.float16)",
          "173:       if not test.is_built_with_rocm():",
          "174:         out.append(dtypes.float64)",
          "175:       return out",
          "177:     return [dtypes.float32, dtypes.float64, dtypes.float16, dtypes.bfloat16]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "175:         out.append(dtypes.bfloat16)",
          "",
          "---------------"
        ],
        "tensorflow/python/kernel_tests/nn_ops/depthwise_conv_op_base.py||tensorflow/python/kernel_tests/nn_ops/depthwise_conv_op_base.py": [
          "File: tensorflow/python/kernel_tests/nn_ops/depthwise_conv_op_base.py -> tensorflow/python/kernel_tests/nn_ops/depthwise_conv_op_base.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "416:       tf_logging.info(",
          "417:           \"Testing DepthwiseConv2DCudnn, %dth config: %r * %r, stride: %d, \"",
          "418:           \"padding: %s\", index, input_size, filter_size, stride, padding)",
          "430:   @test_util.run_v1_only(\"b/120545219\")",
          "431:   def testDepthwiseConv2D(self):",
          "",
          "[Removed Lines]",
          "419:       data_type = dtypes.float16",
          "420:       self._VerifyValues(",
          "421:           input_size,",
          "422:           filter_size,",
          "423:           stride,",
          "424:           padding,",
          "425:           data_type,",
          "426:           use_gpu=True,",
          "427:           data_format=\"NCHW\",",
          "428:           dilations=dilations)",
          "",
          "[Added Lines]",
          "419:       data_types = [dtypes.float16, dtypes.bfloat16]",
          "420:       for data_type in data_types:",
          "421:         self._VerifyValues(",
          "422:             input_size,",
          "423:             filter_size,",
          "424:             stride,",
          "425:             padding,",
          "426:             data_type,",
          "427:             use_gpu=True,",
          "428:             data_format=\"NCHW\",",
          "429:             dilations=dilations)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "509:           \"Testing DepthwiseConv2D, %dth config: %r * %r, stride: %d, padding: \"",
          "510:           \"%s\", index, input_size, filter_size, stride, padding)",
          "511:       # double datatype is currently not supported for convolution ops",
          "514:       data_formats = [\"NHWC\", \"NCHW\"] if test.is_gpu_available() else [\"NHWC\"]",
          "516:         for data_format in data_formats:",
          "517:           self._VerifyValues(",
          "518:               input_size,",
          "",
          "[Removed Lines]",
          "512:       # on the ROCm platform",
          "513:       optional_float64 = [] if test.is_built_with_rocm() else [dtypes.float64]",
          "515:       for data_type in [dtypes.float16, dtypes.float32] + optional_float64:",
          "",
          "[Added Lines]",
          "513:       # on the ROCm platform and its support for bfloat16 is unknown.",
          "514:       data_types = [dtypes.float16, dtypes.float32]",
          "515:       if not test.is_built_with_rocm():",
          "516:         data_types.extend([dtypes.float64, dtypes.bfloat16])",
          "518:       for data_type in data_types:",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "662:           dtypes.float16: 4e-0,",
          "663:           dtypes.float32: 8e-4,",
          "664:           dtypes.float64: 1e-12,",
          "665:       }[data_type]",
          "667:       input_tensor = constant_op.constant(",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "668:           dtypes.bfloat16: 1e-0,",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "743:           \"Testing DepthwiseConv2DInputGradCudnn, %dth config: %r * %r, \"",
          "744:           \"stride: %d, padding: %s\", index, input_size, filter_size, stride,",
          "745:           padding)",
          "759:   @test_util.run_v1_only(\"b/120545219\")",
          "760:   def testDepthwiseConv2DInputGrad(self):",
          "",
          "[Removed Lines]",
          "746:       data_type = dtypes.float16",
          "747:       self._ConstructAndTestGradient(",
          "748:           input_size,",
          "749:           filter_size,",
          "750:           output_size,",
          "751:           stride,",
          "752:           padding,",
          "753:           data_type,",
          "754:           test_input=True,",
          "755:           use_gpu=True,",
          "756:           data_format=\"NCHW\",",
          "757:           dilations=dilations)",
          "",
          "[Added Lines]",
          "750:       data_types = [dtypes.float16, dtypes.bfloat16]",
          "751:       for data_type in data_types:",
          "752:         self._ConstructAndTestGradient(",
          "753:             input_size,",
          "754:             filter_size,",
          "755:             output_size,",
          "756:             stride,",
          "757:             padding,",
          "758:             data_type,",
          "759:             test_input=True,",
          "760:             use_gpu=True,",
          "761:             data_format=\"NCHW\",",
          "762:             dilations=dilations)",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "825:           \"stride: %d, padding: %s\", index, input_size, filter_size, stride,",
          "826:           padding)",
          "827:       # double datatype is currently not supported for convolution ops",
          "830:       data_formats = [\"NHWC\", \"NCHW\"] if test.is_gpu_available() else [\"NHWC\"]",
          "832:         for data_format in data_formats:",
          "833:           self._ConstructAndTestGradient(",
          "834:               input_size,",
          "",
          "[Removed Lines]",
          "828:       # on the ROCm platform",
          "829:       optional_float64 = [] if test.is_built_with_rocm() else [dtypes.float64]",
          "831:       for data_type in [dtypes.float16, dtypes.float32] + optional_float64:",
          "",
          "[Added Lines]",
          "833:       # on the ROCm platform and its support for bfloat16 is unknown.",
          "834:       data_types = [dtypes.float16, dtypes.float32]",
          "835:       if not test.is_built_with_rocm():",
          "836:         data_types.extend([dtypes.float64, dtypes.bfloat16])",
          "838:       for data_type in data_types:",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "853:           \"Testing DepthwiseConv2DFilterGradCudnn, %dth config: %r * %r, \"",
          "854:           \"stride: %d, padding: %s\", index, input_size, filter_size, stride,",
          "855:           padding)",
          "880:   @test_util.run_v1_only(\"b/120545219\")",
          "881:   def testDepthwiseConv2DFilterGrad(self):",
          "",
          "[Removed Lines]",
          "856:       data_type = dtypes.float16",
          "857:       self._ConstructAndTestGradient(",
          "858:           input_size,",
          "859:           filter_size,",
          "860:           output_size,",
          "861:           stride,",
          "862:           padding,",
          "863:           data_type,",
          "864:           test_input=False,",
          "865:           use_gpu=True,",
          "866:           data_format=\"NCHW\",",
          "867:           dilations=dilations)",
          "868:       self._ConstructAndTestGradient(",
          "869:           input_size,",
          "870:           filter_size,",
          "871:           output_size,",
          "872:           stride,",
          "873:           padding,",
          "874:           data_type,",
          "875:           test_input=False,",
          "876:           use_gpu=True,",
          "877:           data_format=\"NHWC\",",
          "878:           dilations=dilations)",
          "",
          "[Added Lines]",
          "863:       data_types = [dtypes.float16, dtypes.bfloat16]",
          "864:       for data_type in data_types:",
          "865:         self._ConstructAndTestGradient(",
          "866:             input_size,",
          "867:             filter_size,",
          "868:             output_size,",
          "869:             stride,",
          "870:             padding,",
          "871:             data_type,",
          "872:             test_input=False,",
          "873:             use_gpu=True,",
          "874:             data_format=\"NCHW\",",
          "875:             dilations=dilations)",
          "876:         self._ConstructAndTestGradient(",
          "877:             input_size,",
          "878:             filter_size,",
          "879:             output_size,",
          "880:             stride,",
          "881:             padding,",
          "882:             data_type,",
          "883:             test_input=False,",
          "884:             use_gpu=True,",
          "885:             data_format=\"NHWC\",",
          "886:             dilations=dilations)",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "935:           \"stride: %d, padding: %s\", index, input_size, filter_size, stride,",
          "936:           padding)",
          "937:       # double datatype is currently not supported for convolution ops",
          "940:       data_formats = [\"NHWC\", \"NCHW\"] if test.is_gpu_available() else [\"NHWC\"]",
          "942:         for data_format in data_formats:",
          "943:           self._ConstructAndTestGradient(",
          "944:               input_size,",
          "",
          "[Removed Lines]",
          "938:       # on the ROCm platform",
          "939:       optional_float64 = [] if test.is_built_with_rocm() else [dtypes.float64]",
          "941:       for data_type in [dtypes.float16, dtypes.float32] + optional_float64:",
          "",
          "[Added Lines]",
          "946:       # on the ROCm platform and its support for bfloat16 is unknown.",
          "947:       data_types = [dtypes.float16, dtypes.float32]",
          "948:       if not test.is_built_with_rocm():",
          "949:         data_types.extend([dtypes.float64, dtypes.bfloat16])",
          "951:       for data_type in data_types:",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "955:   def _CompareBackpropInput(self, input_sizes, filter_sizes, output_sizes,",
          "956:                             stride, padding, dtype):",
          "959:     if isinstance(padding, list):",
          "960:       padding = [(0, 0)] + padding + [(0, 0)]",
          "963:       with self.cached_session(use_gpu=use_gpu):",
          "964:         t0 = constant_op.constant(input_sizes, shape=[len(input_sizes)])",
          "967:         backprop = nn_ops.depthwise_conv2d_native_backprop_input(",
          "968:             t0, t1, t2, strides=[1, stride, stride, 1], padding=padding)",
          "969:         ret = self.evaluate(backprop)",
          "970:         self.assertShapeEqual(ret, backprop)",
          "971:         return ret",
          "977:   @test_util.run_gpu_only",
          "978:   def testDepthwiseConv2DInputGradCompare(self):",
          "",
          "[Removed Lines]",
          "957:     x1 = np.random.rand(*filter_sizes).astype(dtype)",
          "958:     x2 = np.random.rand(*output_sizes).astype(dtype)",
          "962:     def _GetVal(use_gpu):",
          "965:         t1 = constant_op.constant(x1, shape=filter_sizes)",
          "966:         t2 = constant_op.constant(x2, shape=output_sizes)",
          "973:     gpu_value = _GetVal(use_gpu=True)",
          "974:     cpu_value = _GetVal(use_gpu=False)",
          "975:     self.assertAllClose(cpu_value, gpu_value, rtol=1e-4, atol=1e-4)",
          "",
          "[Added Lines]",
          "967:     x1 = np.random.rand(*filter_sizes)",
          "968:     x2 = np.random.rand(*output_sizes)",
          "972:     def _GetVal(use_gpu, dtype):",
          "975:         t1 = constant_op.constant(x1, shape=filter_sizes, dtype=dtype)",
          "976:         t2 = constant_op.constant(x2, shape=output_sizes, dtype=dtype)",
          "983:     rtol, atol = (1e-1, 1e-1) if dtype == \"bfloat16\" else (1e-4, 1e-4)",
          "984:     gpu_value = _GetVal(use_gpu=True, dtype=dtype)",
          "985:     cpu_value = _GetVal(use_gpu=False, dtype=dtype)",
          "986:     self.assertAllClose(cpu_value, gpu_value, rtol=rtol, atol=atol)",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "986:           padding)",
          "987:       self._CompareBackpropInput(input_size, filter_size, output_size, stride,",
          "988:                                  padding, \"float32\")",
          "991:       if test.is_built_with_rocm():",
          "992:         continue",
          "993:       self._CompareBackpropInput(input_size, filter_size, output_size, stride,",
          "994:                                  padding, \"float64\")",
          "996:   @test_util.run_gpu_only",
          "997:   def testDepthwiseConv2DInputGradExplicitCompare(self):",
          "",
          "[Removed Lines]",
          "989:       # double datatype is currently not supported for convolution ops",
          "990:       # on the ROCm platform",
          "",
          "[Added Lines]",
          "1000:       # Convolutions on the ROCm platform don't support double dtype. And its",
          "1001:       # support for bf16 is unknown. So, we skip these tests.",
          "1006:       self._CompareBackpropInput(input_size, filter_size, output_size, stride,",
          "1007:                                  padding, \"bfloat16\")",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "1005:           padding)",
          "1006:       self._CompareBackpropInput(input_size, filter_size, output_size, stride,",
          "1007:                                  padding, \"float32\")",
          "1010:       if test.is_built_with_rocm():",
          "1011:         continue",
          "1012:       self._CompareBackpropInput(input_size, filter_size, output_size, stride,",
          "1013:                                  padding, \"float64\")",
          "1015:   def _CompareBackpropFilter(self, input_sizes, filter_sizes, output_sizes,",
          "1016:                              stride, padding, dtype):",
          "1019:     padding_nhwc = padding",
          "1020:     padding_nchw = padding",
          "1021:     if isinstance(padding, list):",
          "1022:       padding_nhwc = [(0, 0)] + padding + [(0, 0)]",
          "1023:       padding_nchw = [(0, 0)] + [(0, 0)] + padding",
          "1026:       with self.cached_session(use_gpu=use_gpu):",
          "1028:         t1 = constant_op.constant(filter_sizes, shape=[len(filter_sizes)])",
          "1030:         strides = [1, stride, stride, 1]",
          "1031:         padding = padding_nhwc",
          "1032:         if data_format == \"NCHW\":",
          "",
          "[Removed Lines]",
          "1008:       # double datatype is currently not supported for convolution ops",
          "1009:       # on the ROCm platform",
          "1017:     x0 = np.random.rand(*input_sizes).astype(dtype)",
          "1018:     x2 = np.random.rand(*output_sizes).astype(dtype)",
          "1025:     def _GetVal(use_gpu, data_format=\"NHWC\"):",
          "1027:         t0 = constant_op.constant(x0, shape=input_sizes)",
          "1029:         t2 = constant_op.constant(x2, shape=output_sizes)",
          "",
          "[Added Lines]",
          "1021:       # Convolutions on the ROCm platform don't support double dtype. And its",
          "1022:       # support for bf16 is unknown. So, we skip these tests.",
          "1027:       self._CompareBackpropInput(input_size, filter_size, output_size, stride,",
          "1028:                                  padding, \"bfloat16\")",
          "1032:     x0 = np.random.rand(*input_sizes)",
          "1033:     x2 = np.random.rand(*output_sizes)",
          "1040:     def _GetVal(use_gpu, dtype, data_format=\"NHWC\"):",
          "1042:         t0 = constant_op.constant(x0, shape=input_sizes, dtype=dtype)",
          "1044:         t2 = constant_op.constant(x2, shape=output_sizes, dtype=dtype)",
          "",
          "---------------",
          "--- Hunk 11 ---",
          "[Context before]",
          "1045:         self.assertShapeEqual(ret, backprop)",
          "1046:         return ret",
          "1049:     for data_format in [\"NHWC\", \"NCHW\"]:",
          "1053:   @test_util.run_gpu_only",
          "1054:   def testDepthwiseConv2DFilterGradCompare(self):",
          "",
          "[Removed Lines]",
          "1048:     cpu_value = _GetVal(use_gpu=False)",
          "1050:       gpu_value = _GetVal(use_gpu=True, data_format=data_format)",
          "1051:       self.assertAllClose(cpu_value, gpu_value, rtol=1e-4, atol=1e-4)",
          "",
          "[Added Lines]",
          "1063:     cpu_value = _GetVal(use_gpu=False, dtype=dtype)",
          "1065:       gpu_value = _GetVal(use_gpu=True, dtype=dtype, data_format=data_format)",
          "1066:       self.assertAllCloseAccordingToType(",
          "1067:           cpu_value, gpu_value, rtol=1e-4, atol=1e-4, bfloat16_rtol=1e-0)",
          "",
          "---------------",
          "--- Hunk 12 ---",
          "[Context before]",
          "1062:           padding)",
          "1063:       self._CompareBackpropFilter(input_size, filter_size, output_size, stride,",
          "1064:                                   padding, \"float32\")",
          "1067:       if test.is_built_with_rocm():",
          "1068:         continue",
          "1069:       self._CompareBackpropFilter(input_size, filter_size, output_size, stride,",
          "1070:                                   padding, \"float64\")",
          "1072:   @test_util.run_gpu_only",
          "1073:   def testDepthwiseConv2DFilterGradExplicitCompare(self):",
          "1074:     for index, (input_size, filter_size, output_size, stride, padding,",
          "",
          "[Removed Lines]",
          "1065:       # double datatype is currently not supported for convolution ops",
          "1066:       # on the ROCm platform",
          "",
          "[Added Lines]",
          "1081:       # Convolutions on the ROCm platform don't support double dtype. And its",
          "1082:       # support for bf16 is unknown. So, we skip these tests.",
          "1088:       self._CompareBackpropFilter(input_size, filter_size, output_size, stride,",
          "1089:                                   padding, \"bfloat16\")",
          "",
          "---------------",
          "--- Hunk 13 ---",
          "[Context before]",
          "1081:           padding)",
          "1082:       self._CompareBackpropFilter(input_size, filter_size, output_size, stride,",
          "1083:                                   padding, \"float32\")",
          "1086:       if test.is_built_with_rocm():",
          "1087:         continue",
          "1088:       self._CompareBackpropFilter(input_size, filter_size, output_size, stride,",
          "1089:                                   padding, \"float64\")",
          "",
          "[Removed Lines]",
          "1084:       # double datatype is currently not supported for convolution ops",
          "1085:       # on the ROCm platform",
          "",
          "[Added Lines]",
          "1103:       # Convolutions on the ROCm platform don't support double dtype. And its",
          "1104:       # support for bf16 is unknown. So, we skip these tests.",
          "1110:       self._CompareBackpropFilter(input_size, filter_size, output_size, stride,",
          "1111:                                   padding, \"bfloat16\")",
          "1113:   def _CompareForward(self, input_sizes, filter_sizes, output_sizes, stride,",
          "1114:                       padding, dtype):",
          "1115:     x1 = np.random.rand(*input_sizes)",
          "1116:     x2 = np.random.rand(*filter_sizes)",
          "1117:     if isinstance(padding, list):",
          "1118:       padding = [(0, 0)] + padding + [(0, 0)]",
          "1120:     def _GetVal(use_gpu, dtype):",
          "1121:       with self.cached_session(use_gpu=use_gpu):",
          "1122:         t1 = constant_op.constant(x1, shape=input_sizes, dtype=dtype)",
          "1123:         t2 = constant_op.constant(x2, shape=filter_sizes, dtype=dtype)",
          "1124:         output = nn_ops.depthwise_conv2d_native(",
          "1125:             t1, t2, strides=[1, stride, stride, 1], padding=padding)",
          "1126:         ret = self.evaluate(output)",
          "1127:         self.assertShapeEqual(ret, output)",
          "1128:         return ret",
          "1130:     gpu_value = _GetVal(use_gpu=True, dtype=dtype)",
          "1131:     cpu_value = _GetVal(use_gpu=False, dtype=dtype)",
          "1132:     self.assertAllCloseAccordingToType(",
          "1133:         cpu_value, gpu_value, rtol=1e-4, atol=1e-4, bfloat16_rtol=1e-1)",
          "1135:   @test_util.run_gpu_only",
          "1136:   def testDepthwiseConv2DForwardCompare(self):",
          "1137:     for index, (input_size, filter_size, output_size, stride, padding,",
          "1138:                 dilations) in enumerate(ConfigsToTest()):",
          "1139:       if dilations:",
          "1140:         continue",
          "1141:       tf_logging.info(",
          "1142:           \"Testing DepthwiseConv2DForwardCompare, %dth config: %r * %r, \"",
          "1143:           \"stride: %d, padding: %s\", index, input_size, filter_size, stride,",
          "1144:           padding)",
          "1145:       self._CompareForward(input_size, filter_size, output_size, stride,",
          "1146:                            padding, \"float32\")",
          "1147:       # Convolutions on the ROCm platform don't support double dtype. And its",
          "1148:       # support for bf16 is unknown. So, we skip these tests.",
          "1149:       if test.is_built_with_rocm():",
          "1150:         continue",
          "1151:       self._CompareForward(input_size, filter_size, output_size, stride,",
          "1152:                            padding, \"float64\")",
          "1154:       self._CompareForward(input_size, filter_size, output_size, stride,",
          "1155:                            padding, \"bfloat16\")",
          "1157:   @test_util.run_gpu_only",
          "1158:   def testDepthwiseConv2DForwardExplicitCompare(self):",
          "1159:     for index, (input_size, filter_size, output_size, stride, padding,",
          "1160:                 dilations) in enumerate(ConfigsToTestExplicit()):",
          "1161:       if dilations:",
          "1162:         continue",
          "1163:       tf_logging.info(",
          "1164:           \"Testing DepthwiseConv2DForwardCompare, %dth config: %r * %r, \"",
          "1165:           \"stride: %d, padding: %s\", index, input_size, filter_size, stride,",
          "1166:           padding)",
          "1167:       # Convolutions on the ROCm platform don't support double dtype. And its",
          "1168:       # support for bf16 is unknown. So, we skip these tests.",
          "1169:       if test.is_built_with_rocm():",
          "1170:         continue",
          "1171:       self._CompareForward(input_size, filter_size, output_size, stride,",
          "1172:                            padding, \"float64\")",
          "1173:       self._CompareForward(input_size, filter_size, output_size, stride,",
          "1174:                            padding, \"float32\")",
          "1176:       self._CompareForward(input_size, filter_size, output_size, stride,",
          "1177:                            padding, \"bfloat16\")",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "c3f826b9f673cb580f3cfd5b2614f07b096fdccb",
      "candidate_info": {
        "commit_hash": "c3f826b9f673cb580f3cfd5b2614f07b096fdccb",
        "repo": "tensorflow/tensorflow",
        "commit_url": "https://github.com/tensorflow/tensorflow/commit/c3f826b9f673cb580f3cfd5b2614f07b096fdccb",
        "files": [
          "tensorflow/core/kernels/conv_grad_ops_3d.cc",
          "tensorflow/python/kernel_tests/nn_ops/conv3d_backprop_filter_v2_grad_test.py"
        ],
        "message": "Fix failed check in Conv3DBackpropFilterV2.\n\nPassing in a rank-0 `filter_size` causes a check fail and crash,\ncoming from a `filter_size.vec<>()` call.  Here we check the size\nfirst.\n\nPiperOrigin-RevId: 445517122",
        "before_after_code_files": [
          "tensorflow/core/kernels/conv_grad_ops_3d.cc||tensorflow/core/kernels/conv_grad_ops_3d.cc",
          "tensorflow/python/kernel_tests/nn_ops/conv3d_backprop_filter_v2_grad_test.py||tensorflow/python/kernel_tests/nn_ops/conv3d_backprop_filter_v2_grad_test.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "diff_branch_same_aad": 1,
        "olp_code_files": {
          "patch": [
            "tensorflow/core/kernels/conv_grad_ops_3d.cc||tensorflow/core/kernels/conv_grad_ops_3d.cc",
            "tensorflow/python/kernel_tests/nn_ops/conv3d_backprop_filter_v2_grad_test.py||tensorflow/python/kernel_tests/nn_ops/conv3d_backprop_filter_v2_grad_test.py"
          ],
          "candidate": [
            "tensorflow/core/kernels/conv_grad_ops_3d.cc||tensorflow/core/kernels/conv_grad_ops_3d.cc",
            "tensorflow/python/kernel_tests/nn_ops/conv3d_backprop_filter_v2_grad_test.py||tensorflow/python/kernel_tests/nn_ops/conv3d_backprop_filter_v2_grad_test.py"
          ]
        }
      },
      "candidate_diff": {
        "tensorflow/core/kernels/conv_grad_ops_3d.cc||tensorflow/core/kernels/conv_grad_ops_3d.cc": [
          "File: tensorflow/core/kernels/conv_grad_ops_3d.cc -> tensorflow/core/kernels/conv_grad_ops_3d.cc",
          "--- Hunk 1 ---",
          "[Context before]",
          "741:     TensorShape filter_shape;",
          "742:     if (takes_shape_) {",
          "743:       const Tensor& filter_sizes = context->input(1);",
          "744:       OP_REQUIRES_OK(context, TensorShapeUtils::MakeShape(",
          "745:                                   filter_sizes.vec<int32>(), &filter_shape));",
          "746:     } else {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "744:       OP_REQUIRES(context, TensorShapeUtils::IsVector(filter_sizes.shape()),",
          "745:                   errors::InvalidArgument(",
          "746:                       \"filter_sizes shape must be rank 1 but is rank \",",
          "747:                       filter_sizes.shape().dims()));",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "875:     TensorShape filter_shape;",
          "876:     if (takes_shape_) {",
          "877:       const Tensor& filter_sizes = context->input(1);",
          "878:       OP_REQUIRES_OK(context, TensorShapeUtils::MakeShape(",
          "879:                                   filter_sizes.vec<int32>(), &filter_shape));",
          "880:     } else {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "882:       OP_REQUIRES(context, TensorShapeUtils::IsVector(filter_sizes.shape()),",
          "883:                   errors::InvalidArgument(",
          "884:                       \"filter_sizes shape must be rank 1 but is rank \",",
          "885:                       filter_sizes.shape().dims()));",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "1638:     TensorShape filter_shape;",
          "1639:     if (takes_shape_) {",
          "1640:       const Tensor& filter_sizes = context->input(1);",
          "1641:       OP_REQUIRES_OK(context, tensor::MakeShape(filter_sizes, &filter_shape));",
          "1642:     } else {",
          "1643:       filter_shape = context->input(1).shape();",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1649:       OP_REQUIRES(context, TensorShapeUtils::IsVector(filter_sizes.shape()),",
          "1650:                   errors::InvalidArgument(",
          "1651:                       \"filter_sizes shape must be rank 1 but is rank \",",
          "1652:                       filter_sizes.shape().dims()));",
          "",
          "---------------"
        ],
        "tensorflow/python/kernel_tests/nn_ops/conv3d_backprop_filter_v2_grad_test.py||tensorflow/python/kernel_tests/nn_ops/conv3d_backprop_filter_v2_grad_test.py": [
          "File: tensorflow/python/kernel_tests/nn_ops/conv3d_backprop_filter_v2_grad_test.py -> tensorflow/python/kernel_tests/nn_ops/conv3d_backprop_filter_v2_grad_test.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "19: from tensorflow.python.framework import constant_op",
          "20: from tensorflow.python.framework import dtypes",
          "21: from tensorflow.python.framework import test_util",
          "22: from tensorflow.python.ops import array_ops",
          "23: from tensorflow.python.ops import gradient_checker",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "21: from tensorflow.python.framework import errors",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "58:           err_tolerance = 1e-3",
          "59:           self.assertLess(err, err_tolerance)",
          "62: if __name__ == \"__main__\":",
          "63:   test.main()",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "62:   def testBadFilterShape(self):",
          "63:     strides = [1, 1, 1, 1, 1]",
          "64:     padding = \"VALID\"",
          "65:     tin = constant_op.constant(",
          "66:         .5053710941, shape=[2, 2, 2, 2, 1], dtype=dtypes.float32)",
          "67:     filter_sizes = constant_op.constant(0, shape=[], dtype=dtypes.int32)",
          "68:     out_backprop = constant_op.constant(",
          "69:         .5053710941, shape=[2, 2, 2, 2, 1], dtype=dtypes.float32)",
          "71:     with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
          "72:                                 \"must be rank 1\"):",
          "73:       nn_ops.conv3d_backprop_filter_v2(",
          "74:           input=tin,",
          "75:           filter_sizes=filter_sizes,",
          "76:           out_backprop=out_backprop,",
          "77:           strides=strides,",
          "78:           padding=padding)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "0879be2bef63c8ba33864054bee25a5f2b9ac5dd",
      "candidate_info": {
        "commit_hash": "0879be2bef63c8ba33864054bee25a5f2b9ac5dd",
        "repo": "tensorflow/tensorflow",
        "commit_url": "https://github.com/tensorflow/tensorflow/commit/0879be2bef63c8ba33864054bee25a5f2b9ac5dd",
        "files": [
          "tensorflow/core/kernels/conv_grad_ops_3d.cc",
          "tensorflow/python/kernel_tests/conv3d_backprop_filter_v2_grad_test.py"
        ],
        "message": "Fix failed check in Conv3DBackpropFilterV2.\n\nPassing in a rank-0 `filter_size` causes a check fail and crash,\ncoming from a `filter_size.vec<>()` call.  Here we check the size\nfirst.\n\nPiperOrigin-RevId: 445517122",
        "before_after_code_files": [
          "tensorflow/core/kernels/conv_grad_ops_3d.cc||tensorflow/core/kernels/conv_grad_ops_3d.cc",
          "tensorflow/python/kernel_tests/conv3d_backprop_filter_v2_grad_test.py||tensorflow/python/kernel_tests/conv3d_backprop_filter_v2_grad_test.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "diff_branch_same_aad": 1,
        "olp_code_files": {
          "patch": [
            "tensorflow/core/kernels/conv_grad_ops_3d.cc||tensorflow/core/kernels/conv_grad_ops_3d.cc"
          ],
          "candidate": [
            "tensorflow/core/kernels/conv_grad_ops_3d.cc||tensorflow/core/kernels/conv_grad_ops_3d.cc"
          ]
        }
      },
      "candidate_diff": {
        "tensorflow/core/kernels/conv_grad_ops_3d.cc||tensorflow/core/kernels/conv_grad_ops_3d.cc": [
          "File: tensorflow/core/kernels/conv_grad_ops_3d.cc -> tensorflow/core/kernels/conv_grad_ops_3d.cc",
          "--- Hunk 1 ---",
          "[Context before]",
          "741:     TensorShape filter_shape;",
          "742:     if (takes_shape_) {",
          "743:       const Tensor& filter_sizes = context->input(1);",
          "744:       OP_REQUIRES_OK(context, TensorShapeUtils::MakeShape(",
          "745:                                   filter_sizes.vec<int32>(), &filter_shape));",
          "746:     } else {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "744:       OP_REQUIRES(context, TensorShapeUtils::IsVector(filter_sizes.shape()),",
          "745:                   errors::InvalidArgument(",
          "746:                       \"filter_sizes shape must be rank 1 but is rank \",",
          "747:                       filter_sizes.shape().dims()));",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "875:     TensorShape filter_shape;",
          "876:     if (takes_shape_) {",
          "877:       const Tensor& filter_sizes = context->input(1);",
          "878:       OP_REQUIRES_OK(context, TensorShapeUtils::MakeShape(",
          "879:                                   filter_sizes.vec<int32>(), &filter_shape));",
          "880:     } else {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "882:       OP_REQUIRES(context, TensorShapeUtils::IsVector(filter_sizes.shape()),",
          "883:                   errors::InvalidArgument(",
          "884:                       \"filter_sizes shape must be rank 1 but is rank \",",
          "885:                       filter_sizes.shape().dims()));",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "1651:     TensorShape filter_shape;",
          "1652:     if (takes_shape_) {",
          "1653:       const Tensor& filter_sizes = context->input(1);",
          "1654:       OP_REQUIRES_OK(context, tensor::MakeShape(filter_sizes, &filter_shape));",
          "1655:     } else {",
          "1656:       filter_shape = context->input(1).shape();",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1662:       OP_REQUIRES(context, TensorShapeUtils::IsVector(filter_sizes.shape()),",
          "1663:                   errors::InvalidArgument(",
          "1664:                       \"filter_sizes shape must be rank 1 but is rank \",",
          "1665:                       filter_sizes.shape().dims()));",
          "",
          "---------------"
        ],
        "tensorflow/python/kernel_tests/conv3d_backprop_filter_v2_grad_test.py||tensorflow/python/kernel_tests/conv3d_backprop_filter_v2_grad_test.py": [
          "File: tensorflow/python/kernel_tests/conv3d_backprop_filter_v2_grad_test.py -> tensorflow/python/kernel_tests/conv3d_backprop_filter_v2_grad_test.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "23: from tensorflow.python.framework import constant_op",
          "24: from tensorflow.python.framework import dtypes",
          "25: from tensorflow.python.framework import test_util",
          "26: from tensorflow.python.ops import array_ops",
          "27: from tensorflow.python.ops import gradient_checker",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "25: from tensorflow.python.framework import errors",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "62:           err_tolerance = 1e-3",
          "63:           self.assertLess(err, err_tolerance)",
          "66: if __name__ == \"__main__\":",
          "67:   test.main()",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "66:   def testBadFilterShape(self):",
          "67:     strides = [1, 1, 1, 1, 1]",
          "68:     padding = \"VALID\"",
          "69:     tin = constant_op.constant(",
          "70:         .5053710941, shape=[2, 2, 2, 2, 1], dtype=dtypes.float32)",
          "71:     filter_sizes = constant_op.constant(0, shape=[], dtype=dtypes.int32)",
          "72:     out_backprop = constant_op.constant(",
          "73:         .5053710941, shape=[2, 2, 2, 2, 1], dtype=dtypes.float32)",
          "75:     with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
          "76:                                 \"must be rank 1\"):",
          "77:       nn_ops.conv3d_backprop_filter_v2(",
          "78:           input=tin,",
          "79:           filter_sizes=filter_sizes,",
          "80:           out_backprop=out_backprop,",
          "81:           strides=strides,",
          "82:           padding=padding)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "bca5be65b3c27b1e9dcf0f6dc63f778225377f33",
      "candidate_info": {
        "commit_hash": "bca5be65b3c27b1e9dcf0f6dc63f778225377f33",
        "repo": "tensorflow/tensorflow",
        "commit_url": "https://github.com/tensorflow/tensorflow/commit/bca5be65b3c27b1e9dcf0f6dc63f778225377f33",
        "files": [
          "tensorflow/core/kernels/conv_grad_ops_3d.cc",
          "tensorflow/python/kernel_tests/conv3d_backprop_filter_v2_grad_test.py"
        ],
        "message": "Fix failed check in Conv3DBackpropFilterV2.\n\nPassing in a rank-0 `filter_size` causes a check fail and crash,\ncoming from a `filter_size.vec<>()` call.  Here we check the size\nfirst.\n\nPiperOrigin-RevId: 445517122",
        "before_after_code_files": [
          "tensorflow/core/kernels/conv_grad_ops_3d.cc||tensorflow/core/kernels/conv_grad_ops_3d.cc",
          "tensorflow/python/kernel_tests/conv3d_backprop_filter_v2_grad_test.py||tensorflow/python/kernel_tests/conv3d_backprop_filter_v2_grad_test.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "diff_branch_same_aad": 1,
        "olp_code_files": {
          "patch": [
            "tensorflow/core/kernels/conv_grad_ops_3d.cc||tensorflow/core/kernels/conv_grad_ops_3d.cc"
          ],
          "candidate": [
            "tensorflow/core/kernels/conv_grad_ops_3d.cc||tensorflow/core/kernels/conv_grad_ops_3d.cc"
          ]
        }
      },
      "candidate_diff": {
        "tensorflow/core/kernels/conv_grad_ops_3d.cc||tensorflow/core/kernels/conv_grad_ops_3d.cc": [
          "File: tensorflow/core/kernels/conv_grad_ops_3d.cc -> tensorflow/core/kernels/conv_grad_ops_3d.cc",
          "--- Hunk 1 ---",
          "[Context before]",
          "740:     TensorShape filter_shape;",
          "741:     if (takes_shape_) {",
          "742:       const Tensor& filter_sizes = context->input(1);",
          "743:       OP_REQUIRES_OK(context, TensorShapeUtils::MakeShape(",
          "744:                                   filter_sizes.vec<int32>(), &filter_shape));",
          "745:     } else {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "743:       OP_REQUIRES(context, TensorShapeUtils::IsVector(filter_sizes.shape()),",
          "744:                   errors::InvalidArgument(",
          "745:                       \"filter_sizes shape must be rank 1 but is rank \",",
          "746:                       filter_sizes.shape().dims()));",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "874:     TensorShape filter_shape;",
          "875:     if (takes_shape_) {",
          "876:       const Tensor& filter_sizes = context->input(1);",
          "877:       OP_REQUIRES_OK(context, TensorShapeUtils::MakeShape(",
          "878:                                   filter_sizes.vec<int32>(), &filter_shape));",
          "879:     } else {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "881:       OP_REQUIRES(context, TensorShapeUtils::IsVector(filter_sizes.shape()),",
          "882:                   errors::InvalidArgument(",
          "883:                       \"filter_sizes shape must be rank 1 but is rank \",",
          "884:                       filter_sizes.shape().dims()));",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "1805:     TensorShape filter_shape;",
          "1806:     if (takes_shape_) {",
          "1807:       const Tensor& filter_sizes = context->input(1);",
          "1808:       OP_REQUIRES_OK(context, tensor::MakeShape(filter_sizes, &filter_shape));",
          "1809:     } else {",
          "1810:       filter_shape = context->input(1).shape();",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1816:       OP_REQUIRES(context, TensorShapeUtils::IsVector(filter_sizes.shape()),",
          "1817:                   errors::InvalidArgument(",
          "1818:                       \"filter_sizes shape must be rank 1 but is rank \",",
          "1819:                       filter_sizes.shape().dims()));",
          "",
          "---------------"
        ],
        "tensorflow/python/kernel_tests/conv3d_backprop_filter_v2_grad_test.py||tensorflow/python/kernel_tests/conv3d_backprop_filter_v2_grad_test.py": [
          "File: tensorflow/python/kernel_tests/conv3d_backprop_filter_v2_grad_test.py -> tensorflow/python/kernel_tests/conv3d_backprop_filter_v2_grad_test.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "23: from tensorflow.python.framework import constant_op",
          "24: from tensorflow.python.framework import dtypes",
          "25: from tensorflow.python.framework import test_util",
          "26: from tensorflow.python.ops import array_ops",
          "27: from tensorflow.python.ops import gradient_checker",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "25: from tensorflow.python.framework import errors",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "62:           err_tolerance = 1e-3",
          "63:           self.assertLess(err, err_tolerance)",
          "66: if __name__ == \"__main__\":",
          "67:   test.main()",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "66:   def testBadFilterShape(self):",
          "67:     strides = [1, 1, 1, 1, 1]",
          "68:     padding = \"VALID\"",
          "69:     tin = constant_op.constant(",
          "70:         .5053710941, shape=[2, 2, 2, 2, 1], dtype=dtypes.float32)",
          "71:     filter_sizes = constant_op.constant(0, shape=[], dtype=dtypes.int32)",
          "72:     out_backprop = constant_op.constant(",
          "73:         .5053710941, shape=[2, 2, 2, 2, 1], dtype=dtypes.float32)",
          "75:     with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
          "76:                                 \"must be rank 1\"):",
          "77:       nn_ops.conv3d_backprop_filter_v2(",
          "78:           input=tin,",
          "79:           filter_sizes=filter_sizes,",
          "80:           out_backprop=out_backprop,",
          "81:           strides=strides,",
          "82:           padding=padding)",
          "",
          "---------------"
        ]
      }
    }
  ]
}