{
  "cve_id": "CVE-2021-41220",
  "cve_desc": "TensorFlow is an open source platform for machine learning. In affected versions the async implementation of `CollectiveReduceV2` suffers from a memory leak and a use after free. This occurs due to the asynchronous computation and the fact that objects that have been `std::move()`d from are still accessed. The fix will be included in TensorFlow 2.7.0. We will also cherrypick this commit on TensorFlow 2.6.1, as this version is the only one that is also affected.",
  "repo": "tensorflow/tensorflow",
  "patch_hash": "ca38dab9d3ee66c5de06f11af9a4b1200da5ef75",
  "patch_info": {
    "commit_hash": "ca38dab9d3ee66c5de06f11af9a4b1200da5ef75",
    "repo": "tensorflow/tensorflow",
    "commit_url": "https://github.com/tensorflow/tensorflow/commit/ca38dab9d3ee66c5de06f11af9a4b1200da5ef75",
    "files": [
      "tensorflow/core/kernels/collective_ops.cc",
      "tensorflow/python/kernel_tests/collective_ops_test.py"
    ],
    "message": "Fix undefined behavior in CollectiveReduceV2 and others\n\nWe should not call done after it's moved.\n\nPiperOrigin-RevId: 400838185\nChange-Id: Ifc979740054b8f8c6f4d50acc89472fe60c4fdb1",
    "before_after_code_files": [
      "tensorflow/core/kernels/collective_ops.cc||tensorflow/core/kernels/collective_ops.cc",
      "tensorflow/python/kernel_tests/collective_ops_test.py||tensorflow/python/kernel_tests/collective_ops_test.py"
    ]
  },
  "patch_diff": {
    "tensorflow/core/kernels/collective_ops.cc||tensorflow/core/kernels/collective_ops.cc": [
      "File: tensorflow/core/kernels/collective_ops.cc -> tensorflow/core/kernels/collective_ops.cc",
      "--- Hunk 1 ---",
      "[Context before]",
      "494:                               const Tensor& group_size, const Tensor& group_key,",
      "495:                               const Tensor& instance_key) {",
      "496:     if (group_size.dims() > 0) {",
      "499:     }",
      "500:     if (group_key.dims() > 0) {",
      "503:     }",
      "504:     if (instance_key.dims() > 0) {",
      "506:           \"Unexpected dimensions on input instance_key, got \",",
      "507:           instance_key.shape().DebugString());",
      "508:     }",
      "",
      "[Removed Lines]",
      "497:       return errors::Internal(\"Unexpected dimensions on input group_size, got \",",
      "498:                               group_size.shape().DebugString());",
      "501:       return errors::Internal(\"Unexpected dimensions on input group_key, got \",",
      "502:                               group_key.shape().DebugString());",
      "505:       return errors::Internal(",
      "",
      "[Added Lines]",
      "497:       return errors::InvalidArgument(",
      "498:           \"Unexpected dimensions on input group_size, got \",",
      "499:           group_size.shape().DebugString());",
      "502:       return errors::InvalidArgument(",
      "503:           \"Unexpected dimensions on input group_key, got \",",
      "504:           group_key.shape().DebugString());",
      "507:       return errors::InvalidArgument(",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "629:     col_params->instance.shape = c->input(0).shape();",
      "630:     col_params->merge_op = merge_op_.get();",
      "631:     col_params->final_op = final_op_.get();",
      "",
      "[Removed Lines]",
      "628:                          done);",
      "",
      "[Added Lines]",
      "630:                          done_with_cleanup);",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "856:   Status CheckInputs(Tensor group_size_t, Tensor group_key_t) {",
      "857:     if (group_size_t.dims() > 0) {",
      "859:           \"Unexpected dimensions on input group_size. \"",
      "860:           \"It shoulbe a scalar, got tensor with shape \",",
      "861:           group_size_t.shape().DebugString());",
      "862:     }",
      "863:     if (group_key_t.dims() > 0) {",
      "866:     }",
      "868:     auto group_size = group_size_t.unaligned_flat<int32>()(0);",
      "",
      "[Removed Lines]",
      "858:       return errors::Internal(",
      "864:       return errors::Internal(\"Unexpected dimensions on input group_key, got \",",
      "865:                               group_key_t.shape().DebugString());",
      "",
      "[Added Lines]",
      "860:       return errors::InvalidArgument(",
      "866:       return errors::InvalidArgument(",
      "867:           \"Unexpected dimensions on input group_key, got \",",
      "868:           group_key_t.shape().DebugString());",
      "",
      "---------------",
      "--- Hunk 4 ---",
      "[Context before]",
      "1084:     };",
      "1085:     core::RefCountPtr<CollectiveGroupResource> resource;",
      "1086:     OP_REQUIRES_OK_ASYNC(c, LookupResource(c, HandleFromInput(c, 1), &resource),",
      "1089:     Tensor group_assignment = c->input(2);",
      "",
      "[Removed Lines]",
      "1087:                          done);",
      "",
      "[Added Lines]",
      "1090:                          done_with_cleanup);",
      "",
      "---------------",
      "--- Hunk 5 ---",
      "[Context before]",
      "1134:     };",
      "1135:     core::RefCountPtr<CollectiveGroupResource> resource;",
      "1136:     OP_REQUIRES_OK_ASYNC(c, LookupResource(c, HandleFromInput(c, 1), &resource),",
      "1139:     Tensor group_assignment = c->input(2);",
      "",
      "[Removed Lines]",
      "1137:                          done);",
      "",
      "[Added Lines]",
      "1140:                          done_with_cleanup);",
      "",
      "---------------"
    ],
    "tensorflow/python/kernel_tests/collective_ops_test.py||tensorflow/python/kernel_tests/collective_ops_test.py": [
      "File: tensorflow/python/kernel_tests/collective_ops_test.py -> tensorflow/python/kernel_tests/collective_ops_test.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "1182:     self.assertAllEqual(self.evaluate(f()), [[3.], [3.]])",
      "1185: class CollectiveOpsV3Test(test.TestCase, parameterized.TestCase):",
      "1187:   def setUp(self):",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "1185: @combinations.generate(",
      "1186:     combinations.times(",
      "1187:         combinations.combine(collective_op=[",
      "1188:             combinations.NamedObject('all_reduce_v2',",
      "1189:                                      CollectiveOpsV2.all_reduce),",
      "1190:             combinations.NamedObject('all_gather_v2',",
      "1191:                                      CollectiveOpsV2.all_gather)",
      "1192:         ]), device_combination))",
      "1193: class InvalidInputTest(test.TestCase, parameterized.TestCase):",
      "1195:   def setUp(self):",
      "1196:     _setup_context()",
      "1197:     super().setUp()",
      "1199:   def testInvalidGroupKey(self, collective_op, device, communication):",
      "1200:     dev0 = '/device:%s:0' % device",
      "1201:     group_size = 2",
      "1202:     group_key = [100]",
      "1203:     instance_key = 100",
      "1204:     in_tensor = constant_op.constant([1.])",
      "1206:     with self.assertRaises(errors.InvalidArgumentError):",
      "1207:       with ops.device(dev0):",
      "1208:         collective_op(",
      "1209:             in_tensor,",
      "1210:             group_size,",
      "1211:             group_key,",
      "1212:             instance_key,",
      "1213:             communication_hint=communication)",
      "1215:   def testInvalidGroupSize(self, collective_op, device, communication):",
      "1216:     dev0 = '/device:%s:0' % device",
      "1217:     group_size = -2",
      "1218:     group_key = 100",
      "1219:     instance_key = 100",
      "1220:     in_tensor = constant_op.constant([1.])",
      "1222:     with self.assertRaises(errors.InvalidArgumentError):",
      "1223:       with ops.device(dev0):",
      "1224:         collective_op(",
      "1225:             in_tensor,",
      "1226:             group_size,",
      "1227:             group_key,",
      "1228:             instance_key,",
      "1229:             communication_hint=communication)",
      "1231:   def testInvalidInstanceKey(self, collective_op, device, communication):",
      "1232:     dev0 = '/device:%s:0' % device",
      "1233:     group_size = 2",
      "1234:     group_key = 100",
      "1235:     instance_key = [100]",
      "1236:     in_tensor = constant_op.constant([1.])",
      "1238:     with self.assertRaises(errors.InvalidArgumentError):",
      "1239:       with ops.device(dev0):",
      "1240:         collective_op(",
      "1241:             in_tensor,",
      "1242:             group_size,",
      "1243:             group_key,",
      "1244:             instance_key,",
      "1245:             communication_hint=communication)",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "54bc354d2eee2f930763a99ff80331e7e294b68e",
      "candidate_info": {
        "commit_hash": "54bc354d2eee2f930763a99ff80331e7e294b68e",
        "repo": "tensorflow/tensorflow",
        "commit_url": "https://github.com/tensorflow/tensorflow/commit/54bc354d2eee2f930763a99ff80331e7e294b68e",
        "files": [
          "tensorflow/core/common_runtime/ring_reducer.cc",
          "tensorflow/core/common_runtime/ring_reducer_test.cc",
          "tensorflow/core/framework/collective.h",
          "tensorflow/core/kernels/collective_nccl_reducer.cc",
          "tensorflow/core/kernels/collective_nccl_test.cc",
          "tensorflow/core/kernels/collective_ops.cc"
        ],
        "message": "Make V2 collective ComputeAsync thread safe\n\nPiperOrigin-RevId: 338291666\nChange-Id: I041f88b8b9f805f3660fe55b3ed0c5c04d7075d4",
        "before_after_code_files": [
          "tensorflow/core/common_runtime/ring_reducer.cc||tensorflow/core/common_runtime/ring_reducer.cc",
          "tensorflow/core/common_runtime/ring_reducer_test.cc||tensorflow/core/common_runtime/ring_reducer_test.cc",
          "tensorflow/core/framework/collective.h||tensorflow/core/framework/collective.h",
          "tensorflow/core/kernels/collective_nccl_reducer.cc||tensorflow/core/kernels/collective_nccl_reducer.cc",
          "tensorflow/core/kernels/collective_nccl_test.cc||tensorflow/core/kernels/collective_nccl_test.cc",
          "tensorflow/core/kernels/collective_ops.cc||tensorflow/core/kernels/collective_ops.cc"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "tensorflow/core/kernels/collective_ops.cc||tensorflow/core/kernels/collective_ops.cc"
          ],
          "candidate": [
            "tensorflow/core/kernels/collective_ops.cc||tensorflow/core/kernels/collective_ops.cc"
          ]
        }
      },
      "candidate_diff": {
        "tensorflow/core/common_runtime/ring_reducer.cc||tensorflow/core/common_runtime/ring_reducer.cc": [
          "File: tensorflow/core/common_runtime/ring_reducer.cc -> tensorflow/core/common_runtime/ring_reducer.cc",
          "--- Hunk 1 ---",
          "[Context before]",
          "256:               rf->action = RF_REDUCE;",
          "257:               Status s = collective_util::ComputeBinOp(",
          "258:                   col_ctx_->op_ctx, col_ctx_->op_params, col_ctx_->device,",
          "260:               if (!s.ok()) {",
          "261:                 aborted = true;",
          "262:                 StartAbort(s);",
          "",
          "[Removed Lines]",
          "259:                   col_params_->merge_op.get(), &rf->chunk, &rf->tmp_chunk);",
          "",
          "[Added Lines]",
          "259:                   col_params_->merge_op, &rf->chunk, &rf->tmp_chunk);",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "266:             }",
          "267:             break;",
          "268:           case RF_REDUCE:",
          "271:               rf->action = RF_FINALIZE;",
          "272:               group_size_tensor_ready_.WaitForNotification();",
          "273:               Status s = collective_util::ComputeBinOp(",
          "274:                   col_ctx_->op_ctx, col_ctx_->op_params, col_ctx_->device,",
          "276:               if (!s.ok()) {",
          "277:                 aborted = true;",
          "278:                 StartAbort(s);",
          "",
          "[Removed Lines]",
          "269:             if (!rf->second_pass && col_params_->final_op.get() &&",
          "270:                 rf->is_final) {",
          "275:                   col_params_->final_op.get(), &rf->chunk, &group_size_tensor_);",
          "",
          "[Added Lines]",
          "269:             if (!rf->second_pass && col_params_->final_op && rf->is_final) {",
          "274:                   col_params_->final_op, &rf->chunk, &group_size_tensor_);",
          "",
          "---------------"
        ],
        "tensorflow/core/common_runtime/ring_reducer_test.cc||tensorflow/core/common_runtime/ring_reducer_test.cc": [
          "File: tensorflow/core/common_runtime/ring_reducer_test.cc -> tensorflow/core/common_runtime/ring_reducer_test.cc",
          "--- Hunk 1 ---",
          "[Context before]",
          "466:     }",
          "468:     void DoReduce() {",
          "475:       OpKernelContext::Params op_params;",
          "",
          "[Removed Lines]",
          "469:       col_params_.merge_op =",
          "470:           GetAdd(col_params_.instance.data_type, device_type_, device_);",
          "471:       col_params_.final_op =",
          "472:           GetDiv(col_params_.instance.data_type, device_type_, device_);",
          "",
          "[Added Lines]",
          "469:       merge_op_ = GetAdd(col_params_.instance.data_type, device_type_, device_);",
          "470:       final_op_ = GetDiv(col_params_.instance.data_type, device_type_, device_);",
          "471:       col_params_.merge_op = merge_op_.get();",
          "472:       col_params_.final_op = final_op_.get();",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "536:     Tensor tensor_;",
          "537:     Device* device_;",
          "538:     CollectiveParams col_params_;",
          "539:     std::unique_ptr<CollectiveAdapter> ca_;",
          "540:     std::unique_ptr<OpKernelContext> ctx_;",
          "541:     Status status_;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "539:     std::unique_ptr<OpKernel> merge_op_;",
          "540:     std::unique_ptr<OpKernel> final_op_;",
          "",
          "---------------"
        ],
        "tensorflow/core/framework/collective.h||tensorflow/core/framework/collective.h": [
          "File: tensorflow/core/framework/collective.h -> tensorflow/core/framework/collective.h",
          "--- Hunk 1 ---",
          "[Context before]",
          "143:   int source_rank = -1;    // broadcast only",
          "145:   std::vector<int> subdiv_rank;",
          "148:   string ToString() const;",
          "149: };",
          "",
          "[Removed Lines]",
          "146:   std::unique_ptr<OpKernel> merge_op;  // reduction only",
          "147:   std::unique_ptr<OpKernel> final_op;  // reduction only",
          "",
          "[Added Lines]",
          "146:   OpKernel* merge_op = nullptr;  // reduction only",
          "147:   OpKernel* final_op = nullptr;  // reduction only",
          "",
          "---------------"
        ],
        "tensorflow/core/kernels/collective_nccl_reducer.cc||tensorflow/core/kernels/collective_nccl_reducer.cc": [
          "File: tensorflow/core/kernels/collective_nccl_reducer.cc -> tensorflow/core/kernels/collective_nccl_reducer.cc",
          "--- Hunk 1 ---",
          "[Context before]",
          "113:   if (final_status.ok()) {",
          "114:     final_status = collective_util::ComputeBinOp(",
          "115:         col_ctx_->op_ctx, col_ctx_->op_params, col_ctx_->device,",
          "117:   }",
          "118:   done(final_status);",
          "119: }",
          "",
          "[Removed Lines]",
          "116:         col_params_->final_op.get(), col_ctx_->output, &group_size);",
          "",
          "[Added Lines]",
          "116:         col_params_->final_op, col_ctx_->output, &group_size);",
          "",
          "---------------"
        ],
        "tensorflow/core/kernels/collective_nccl_test.cc||tensorflow/core/kernels/collective_nccl_test.cc": [
          "File: tensorflow/core/kernels/collective_nccl_test.cc -> tensorflow/core/kernels/collective_nccl_test.cc",
          "--- Hunk 1 ---",
          "[Context before]",
          "248:       TF_CHECK_OK(parent_->dev_mgr_->LookupDevice(device_name_, &device_))",
          "249:           << \"Could not find device \" << device_name_ << \" existing devices \"",
          "250:           << parent_->dev_mgr_->DebugString();",
          "251:       col_params_.name = parent_->col_params_.name;",
          "252:       col_params_.default_rank = rank;",
          "253:       col_params_.group = parent_->col_params_.group;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "251:       merge_op_ = GetAdd(device_);",
          "252:       final_op_ = GetDiv(device_);",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "414:     Tensor output_;",
          "415:     Device* device_;",
          "416:     CollectiveParams col_params_;",
          "417:     Status status_;",
          "418:   };",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "419:     std::unique_ptr<OpKernel> merge_op_;",
          "420:     std::unique_ptr<OpKernel> final_op_;",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "459:   }",
          "461:   void InitDevice(DeviceInstance* di) override {",
          "464:   }",
          "466:   void RunCollectiveOnDevice(DeviceInstance* di) override { di->RunReduce(); }",
          "",
          "[Removed Lines]",
          "462:     di->col_params_.merge_op = GetAdd(di->device_);",
          "463:     di->col_params_.final_op = GetDiv(di->device_);",
          "",
          "[Added Lines]",
          "466:     di->col_params_.merge_op = di->merge_op_.get();",
          "467:     di->col_params_.final_op = di->final_op_.get();",
          "",
          "---------------"
        ],
        "tensorflow/core/kernels/collective_ops.cc||tensorflow/core/kernels/collective_ops.cc": [
          "File: tensorflow/core/kernels/collective_ops.cc -> tensorflow/core/kernels/collective_ops.cc",
          "--- Hunk 1 ---",
          "[Context before]",
          "271:     sub_node.set_device(real_node.device());",
          "272:     SetAttrValue(col_params_.instance.data_type,",
          "273:                  &(*sub_node.mutable_attr())[\"T\"]);",
          "276:   }",
          "278:  protected:",
          "",
          "[Removed Lines]",
          "274:     col_params_.merge_op = BuildOpKernel(c, merge_op_name, &sub_node);",
          "275:     col_params_.final_op = BuildOpKernel(c, final_op_name, &sub_node);",
          "",
          "[Added Lines]",
          "274:     merge_op_ = BuildOpKernel(c, merge_op_name, &sub_node);",
          "275:     final_op_ = BuildOpKernel(c, final_op_name, &sub_node);",
          "276:     col_params_.merge_op = merge_op_.get();",
          "277:     col_params_.final_op = final_op_.get();",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "311:   }",
          "313:  private:",
          "314:   TF_DISALLOW_COPY_AND_ASSIGN(CollectiveReduceOpKernel);",
          "315: };",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "316:   std::unique_ptr<OpKernel> merge_op_;",
          "317:   std::unique_ptr<OpKernel> final_op_;",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "469: class CollectiveReduceV2OpKernel : public CollectiveOpKernel {",
          "470:  public:",
          "471:   explicit CollectiveReduceV2OpKernel(OpKernelConstruction* c)",
          "475:     string merge_op_name;",
          "476:     OP_REQUIRES_OK(c, c->GetAttr(\"merge_op\", &merge_op_name));",
          "477:     OP_REQUIRES_OK(c, c->GetAttr(\"merge_op\", &merge_op_name));",
          "",
          "[Removed Lines]",
          "472:       : CollectiveOpKernel(c) {",
          "473:     col_params_ = std::make_shared<CollectiveParams>();",
          "474:     OP_REQUIRES_OK(c, c->GetAttr(\"T\", &col_params_->instance.data_type));",
          "",
          "[Added Lines]",
          "476:       : CollectiveOpKernel(c), device_type_(DEVICE_DEFAULT) {",
          "477:     OP_REQUIRES_OK(c, c->GetAttr(\"T\", &data_type_));",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "482:     }",
          "483:     string final_op_name;",
          "484:     OP_REQUIRES_OK(c, c->GetAttr(\"final_op\", &final_op_name));",
          "493:     NodeDef sub_node;",
          "494:     sub_node.add_input(c->def().input(0));",
          "495:     sub_node.add_input(c->def().input(0));",
          "496:     sub_node.set_device(c->def().device());",
          "511:   }",
          "513:  protected:",
          "",
          "[Removed Lines]",
          "485:     OP_REQUIRES_OK(",
          "486:         c, c->GetAttr(\"communication_hint\",",
          "487:                       &col_params_->instance.impl_details.communication_hint));",
          "488:     OP_REQUIRES_OK(",
          "489:         c, c->GetAttr(\"timeout_seconds\",",
          "490:                       &col_params_->instance.impl_details.timeout_seconds));",
          "497:     SetAttrValue(col_params_->instance.data_type,",
          "498:                  &(*sub_node.mutable_attr())[\"T\"]);",
          "499:     col_params_->merge_op = BuildOpKernel(c, merge_op_name, &sub_node);",
          "500:     col_params_->final_op = BuildOpKernel(c, final_op_name, &sub_node);",
          "502:     col_params_->name = strings::StrCat(c->def().name(), \": ReduceV2(\",",
          "503:                                         merge_op_name, \",\", final_op_name, \")\");",
          "504:     col_params_->group.device_type = c->device_type();",
          "507:     col_params_->instance.impl_details.subdiv_offsets.push_back(0);",
          "508:     VLOG(2) << \"CollectiveReduceV2 \" << this << \" name \" << col_params_->name",
          "509:             << \" communication_hint \"",
          "510:             << col_params_->instance.impl_details.communication_hint;",
          "",
          "[Added Lines]",
          "488:     OP_REQUIRES_OK(c, c->GetAttr(\"communication_hint\", &communication_hint_));",
          "489:     OP_REQUIRES_OK(c, c->GetAttr(\"timeout_seconds\", &timeout_seconds_));",
          "496:     SetAttrValue(data_type_, &(*sub_node.mutable_attr())[\"T\"]);",
          "497:     merge_op_ = BuildOpKernel(c, merge_op_name, &sub_node);",
          "498:     final_op_ = BuildOpKernel(c, final_op_name, &sub_node);",
          "500:     name_ = strings::StrCat(c->def().name(), \": ReduceV2(\", merge_op_name, \",\",",
          "501:                             final_op_name, \")\");",
          "502:     device_type_ = c->device_type();",
          "503:     VLOG(2) << \"CollectiveReduceV2 \" << this << \" name \" << name_",
          "504:             << \" communication_hint \" << communication_hint_;",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "527:         c, instance_key.dims() == 0,",
          "528:         errors::Internal(\"Unexpected dimensions on input instance_key\"), done);",
          "533:     col_params->group.group_size = group_size.unaligned_flat<int32>()(0);",
          "534:     col_params->group.group_key = group_key.unaligned_flat<int32>()(0);",
          "535:     col_params->instance.type = REDUCTION_COLLECTIVE;",
          "536:     col_params->instance.instance_key = instance_key.unaligned_flat<int32>()(0);",
          "546:     VLOG(1) << \"CollectiveReduceV2 group_size \" << col_params->group.group_size",
          "547:             << \" group_key \" << col_params->group.group_key << \" instance_key \"",
          "548:             << col_params->instance.instance_key;",
          "551:     Tensor* output = nullptr;",
          "552:     OP_REQUIRES_OK_ASYNC(",
          "553:         c, c->forward_input_or_allocate_output({0}, 0, input.shape(), &output),",
          "555:     col_params->instance.shape = input.shape();",
          "565:       VLOG(1) << \"CollectiveReduceV2 CompleteParams for collective \"",
          "566:               << col_params->name << \" device \" << c->device()->name()",
          "567:               << \" group \" << col_params->group.group_key << \" instance \"",
          "568:               << col_params->instance.instance_key;",
          "569:       col_exec->CompleteParamsAsync(",
          "572:           [c, done = std::move(done), col_params, col_exec](const Status& s) {",
          "573:             if (s.ok()) {",
          "574:               auto actual_done = [c, group_key = col_params->group.group_key,",
          "",
          "[Removed Lines]",
          "530:     auto col_params = std::make_shared<CollectiveParams>();",
          "531:     col_params->name = col_params_->name;",
          "532:     col_params->group.device_type = col_params_->group.device_type;",
          "537:     col_params->instance.data_type = col_params_->instance.data_type;",
          "538:     col_params->instance.impl_details.communication_hint =",
          "539:         col_params_->instance.impl_details.communication_hint;",
          "540:     col_params->instance.impl_details.timeout_seconds =",
          "541:         col_params_->instance.impl_details.timeout_seconds;",
          "542:     col_params->instance.impl_details.subdiv_offsets =",
          "543:         col_params_->instance.impl_details.subdiv_offsets;",
          "544:     col_params->merge_op = std::move(col_params_->merge_op);",
          "545:     col_params->final_op = std::move(col_params_->final_op);",
          "554:         done);",
          "558:     col_params_ = col_params;",
          "563:     c->collective_executor()->RunClosure([c, done = std::move(done), col_params,",
          "564:                                           col_exec]() {",
          "570:           c->device()->attributes(), col_params.get(),",
          "571:           c->cancellation_manager(),",
          "",
          "[Added Lines]",
          "524:     auto col_params = new CollectiveParams();",
          "525:     col_params->name = name_;",
          "526:     col_params->group.device_type = device_type_;",
          "531:     col_params->instance.data_type = data_type_;",
          "532:     col_params->instance.impl_details.communication_hint = communication_hint_;",
          "533:     col_params->instance.impl_details.timeout_seconds = timeout_seconds_;",
          "536:     col_params->instance.impl_details.subdiv_offsets.push_back(0);",
          "537:     col_params->merge_op = merge_op_.get();",
          "538:     col_params->final_op = final_op_.get();",
          "543:     auto done_with_cleanup = [col_params, done = std::move(done)]() {",
          "544:       delete col_params;",
          "545:       done();",
          "546:     };",
          "552:         done_with_cleanup);",
          "558:     c->collective_executor()->RunClosure([c,",
          "559:                                           done = std::move(done_with_cleanup),",
          "560:                                           col_params, col_exec]() {",
          "566:           c->device()->attributes(), col_params, c->cancellation_manager(),",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "602:   }",
          "604:  private:",
          "606: };",
          "608: REGISTER_KERNEL_BUILDER(Name(\"CollectiveReduceV2\").Device(DEVICE_CPU),",
          "",
          "[Removed Lines]",
          "605:   std::shared_ptr<CollectiveParams> col_params_;",
          "",
          "[Added Lines]",
          "600:   DataType data_type_ = DT_INVALID;",
          "601:   string communication_hint_;",
          "602:   float timeout_seconds_ = 0;",
          "603:   DeviceType device_type_;",
          "604:   std::unique_ptr<OpKernel> merge_op_;",
          "605:   std::unique_ptr<OpKernel> final_op_;",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "673:         0, output_shape.dim_size(0) * col_params->group.group_size);",
          "674:     col_params->instance.shape = output_shape;",
          "680:     auto done_with_cleanup = [col_params, done = std::move(done)]() {",
          "681:       delete col_params;",
          "682:       done();",
          "683:     };",
          "",
          "[Removed Lines]",
          "676:     Tensor* output = nullptr;",
          "677:     OP_REQUIRES_OK_ASYNC(",
          "678:         c, c->allocate_output(0, col_params->instance.shape, &output), done);",
          "",
          "[Added Lines]",
          "681:     Tensor* output = nullptr;",
          "682:     OP_REQUIRES_OK_ASYNC(",
          "683:         c, c->allocate_output(0, col_params->instance.shape, &output),",
          "684:         done_with_cleanup);",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "727:   }",
          "729:  private:",
          "731:   string communication_hint_;",
          "733:   DeviceType device_type_;",
          "734: };",
          "",
          "[Removed Lines]",
          "730:   DataType data_type_;",
          "732:   float timeout_seconds_;",
          "",
          "[Added Lines]",
          "731:   DataType data_type_ = DT_INVALID;",
          "733:   float timeout_seconds_ = 0;",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "54e8bbbad20d562cd444ec154852288298e5d97b",
      "candidate_info": {
        "commit_hash": "54e8bbbad20d562cd444ec154852288298e5d97b",
        "repo": "tensorflow/tensorflow",
        "commit_url": "https://github.com/tensorflow/tensorflow/commit/54e8bbbad20d562cd444ec154852288298e5d97b",
        "files": [
          "tensorflow/core/common_runtime/ring_reducer.cc",
          "tensorflow/core/common_runtime/ring_reducer_test.cc",
          "tensorflow/core/framework/collective.h",
          "tensorflow/core/kernels/collective_nccl_reducer.cc",
          "tensorflow/core/kernels/collective_nccl_test.cc",
          "tensorflow/core/kernels/collective_ops.cc"
        ],
        "message": "Make V2 collective ComputeAsync thread safe\n\nPiperOrigin-RevId: 338183795\nChange-Id: Ic9a4aeb1d11acfd5ea07b82f77b27402cb8124d0",
        "before_after_code_files": [
          "tensorflow/core/common_runtime/ring_reducer.cc||tensorflow/core/common_runtime/ring_reducer.cc",
          "tensorflow/core/common_runtime/ring_reducer_test.cc||tensorflow/core/common_runtime/ring_reducer_test.cc",
          "tensorflow/core/framework/collective.h||tensorflow/core/framework/collective.h",
          "tensorflow/core/kernels/collective_nccl_reducer.cc||tensorflow/core/kernels/collective_nccl_reducer.cc",
          "tensorflow/core/kernels/collective_nccl_test.cc||tensorflow/core/kernels/collective_nccl_test.cc",
          "tensorflow/core/kernels/collective_ops.cc||tensorflow/core/kernels/collective_ops.cc"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "tensorflow/core/kernels/collective_ops.cc||tensorflow/core/kernels/collective_ops.cc"
          ],
          "candidate": [
            "tensorflow/core/kernels/collective_ops.cc||tensorflow/core/kernels/collective_ops.cc"
          ]
        }
      },
      "candidate_diff": {
        "tensorflow/core/common_runtime/ring_reducer.cc||tensorflow/core/common_runtime/ring_reducer.cc": [
          "File: tensorflow/core/common_runtime/ring_reducer.cc -> tensorflow/core/common_runtime/ring_reducer.cc",
          "--- Hunk 1 ---",
          "[Context before]",
          "256:               rf->action = RF_REDUCE;",
          "257:               Status s = collective_util::ComputeBinOp(",
          "258:                   col_ctx_->op_ctx, col_ctx_->op_params, col_ctx_->device,",
          "260:               if (!s.ok()) {",
          "261:                 aborted = true;",
          "262:                 StartAbort(s);",
          "",
          "[Removed Lines]",
          "259:                   col_params_->merge_op.get(), &rf->chunk, &rf->tmp_chunk);",
          "",
          "[Added Lines]",
          "259:                   col_params_->merge_op, &rf->chunk, &rf->tmp_chunk);",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "266:             }",
          "267:             break;",
          "268:           case RF_REDUCE:",
          "271:               rf->action = RF_FINALIZE;",
          "272:               group_size_tensor_ready_.WaitForNotification();",
          "273:               Status s = collective_util::ComputeBinOp(",
          "274:                   col_ctx_->op_ctx, col_ctx_->op_params, col_ctx_->device,",
          "276:               if (!s.ok()) {",
          "277:                 aborted = true;",
          "278:                 StartAbort(s);",
          "",
          "[Removed Lines]",
          "269:             if (!rf->second_pass && col_params_->final_op.get() &&",
          "270:                 rf->is_final) {",
          "275:                   col_params_->final_op.get(), &rf->chunk, &group_size_tensor_);",
          "",
          "[Added Lines]",
          "269:             if (!rf->second_pass && col_params_->final_op && rf->is_final) {",
          "274:                   col_params_->final_op, &rf->chunk, &group_size_tensor_);",
          "",
          "---------------"
        ],
        "tensorflow/core/common_runtime/ring_reducer_test.cc||tensorflow/core/common_runtime/ring_reducer_test.cc": [
          "File: tensorflow/core/common_runtime/ring_reducer_test.cc -> tensorflow/core/common_runtime/ring_reducer_test.cc",
          "--- Hunk 1 ---",
          "[Context before]",
          "466:     }",
          "468:     void DoReduce() {",
          "475:       OpKernelContext::Params op_params;",
          "",
          "[Removed Lines]",
          "469:       col_params_.merge_op =",
          "470:           GetAdd(col_params_.instance.data_type, device_type_, device_);",
          "471:       col_params_.final_op =",
          "472:           GetDiv(col_params_.instance.data_type, device_type_, device_);",
          "",
          "[Added Lines]",
          "469:       merge_op_ = GetAdd(col_params_.instance.data_type, device_type_, device_);",
          "470:       final_op_ = GetDiv(col_params_.instance.data_type, device_type_, device_);",
          "471:       col_params_.merge_op = merge_op_.get();",
          "472:       col_params_.final_op = final_op_.get();",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "536:     Tensor tensor_;",
          "537:     Device* device_;",
          "538:     CollectiveParams col_params_;",
          "539:     std::unique_ptr<CollectiveAdapter> ca_;",
          "540:     std::unique_ptr<OpKernelContext> ctx_;",
          "541:     Status status_;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "539:     std::unique_ptr<OpKernel> merge_op_;",
          "540:     std::unique_ptr<OpKernel> final_op_;",
          "",
          "---------------"
        ],
        "tensorflow/core/framework/collective.h||tensorflow/core/framework/collective.h": [
          "File: tensorflow/core/framework/collective.h -> tensorflow/core/framework/collective.h",
          "--- Hunk 1 ---",
          "[Context before]",
          "143:   int source_rank = -1;    // broadcast only",
          "145:   std::vector<int> subdiv_rank;",
          "148:   string ToString() const;",
          "149: };",
          "",
          "[Removed Lines]",
          "146:   std::unique_ptr<OpKernel> merge_op;  // reduction only",
          "147:   std::unique_ptr<OpKernel> final_op;  // reduction only",
          "",
          "[Added Lines]",
          "146:   OpKernel* merge_op = nullptr;  // reduction only",
          "147:   OpKernel* final_op = nullptr;  // reduction only",
          "",
          "---------------"
        ],
        "tensorflow/core/kernels/collective_nccl_reducer.cc||tensorflow/core/kernels/collective_nccl_reducer.cc": [
          "File: tensorflow/core/kernels/collective_nccl_reducer.cc -> tensorflow/core/kernels/collective_nccl_reducer.cc",
          "--- Hunk 1 ---",
          "[Context before]",
          "113:   if (final_status.ok()) {",
          "114:     final_status = collective_util::ComputeBinOp(",
          "115:         col_ctx_->op_ctx, col_ctx_->op_params, col_ctx_->device,",
          "117:   }",
          "118:   done(final_status);",
          "119: }",
          "",
          "[Removed Lines]",
          "116:         col_params_->final_op.get(), col_ctx_->output, &group_size);",
          "",
          "[Added Lines]",
          "116:         col_params_->final_op, col_ctx_->output, &group_size);",
          "",
          "---------------"
        ],
        "tensorflow/core/kernels/collective_nccl_test.cc||tensorflow/core/kernels/collective_nccl_test.cc": [
          "File: tensorflow/core/kernels/collective_nccl_test.cc -> tensorflow/core/kernels/collective_nccl_test.cc",
          "--- Hunk 1 ---",
          "[Context before]",
          "248:       TF_CHECK_OK(parent_->dev_mgr_->LookupDevice(device_name_, &device_))",
          "249:           << \"Could not find device \" << device_name_ << \" existing devices \"",
          "250:           << parent_->dev_mgr_->DebugString();",
          "251:       col_params_.name = parent_->col_params_.name;",
          "252:       col_params_.default_rank = rank;",
          "253:       col_params_.group = parent_->col_params_.group;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "251:       merge_op_ = GetAdd(device_);",
          "252:       final_op_ = GetDiv(device_);",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "414:     Tensor output_;",
          "415:     Device* device_;",
          "416:     CollectiveParams col_params_;",
          "417:     Status status_;",
          "418:   };",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "419:     std::unique_ptr<OpKernel> merge_op_;",
          "420:     std::unique_ptr<OpKernel> final_op_;",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "459:   }",
          "461:   void InitDevice(DeviceInstance* di) override {",
          "464:   }",
          "466:   void RunCollectiveOnDevice(DeviceInstance* di) override { di->RunReduce(); }",
          "",
          "[Removed Lines]",
          "462:     di->col_params_.merge_op = GetAdd(di->device_);",
          "463:     di->col_params_.final_op = GetDiv(di->device_);",
          "",
          "[Added Lines]",
          "466:     di->col_params_.merge_op = di->merge_op_.get();",
          "467:     di->col_params_.final_op = di->final_op_.get();",
          "",
          "---------------"
        ],
        "tensorflow/core/kernels/collective_ops.cc||tensorflow/core/kernels/collective_ops.cc": [
          "File: tensorflow/core/kernels/collective_ops.cc -> tensorflow/core/kernels/collective_ops.cc",
          "--- Hunk 1 ---",
          "[Context before]",
          "271:     sub_node.set_device(real_node.device());",
          "272:     SetAttrValue(col_params_.instance.data_type,",
          "273:                  &(*sub_node.mutable_attr())[\"T\"]);",
          "276:   }",
          "278:  protected:",
          "",
          "[Removed Lines]",
          "274:     col_params_.merge_op = BuildOpKernel(c, merge_op_name, &sub_node);",
          "275:     col_params_.final_op = BuildOpKernel(c, final_op_name, &sub_node);",
          "",
          "[Added Lines]",
          "274:     merge_op_ = BuildOpKernel(c, merge_op_name, &sub_node);",
          "275:     final_op_ = BuildOpKernel(c, final_op_name, &sub_node);",
          "276:     col_params_.merge_op = merge_op_.get();",
          "277:     col_params_.final_op = final_op_.get();",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "311:   }",
          "313:  private:",
          "314:   TF_DISALLOW_COPY_AND_ASSIGN(CollectiveReduceOpKernel);",
          "315: };",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "316:   std::unique_ptr<OpKernel> merge_op_;",
          "317:   std::unique_ptr<OpKernel> final_op_;",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "469: class CollectiveReduceV2OpKernel : public CollectiveOpKernel {",
          "470:  public:",
          "471:   explicit CollectiveReduceV2OpKernel(OpKernelConstruction* c)",
          "475:     string merge_op_name;",
          "476:     OP_REQUIRES_OK(c, c->GetAttr(\"merge_op\", &merge_op_name));",
          "477:     OP_REQUIRES_OK(c, c->GetAttr(\"merge_op\", &merge_op_name));",
          "",
          "[Removed Lines]",
          "472:       : CollectiveOpKernel(c) {",
          "473:     col_params_ = std::make_shared<CollectiveParams>();",
          "474:     OP_REQUIRES_OK(c, c->GetAttr(\"T\", &col_params_->instance.data_type));",
          "",
          "[Added Lines]",
          "476:       : CollectiveOpKernel(c), device_type_(DEVICE_DEFAULT) {",
          "477:     OP_REQUIRES_OK(c, c->GetAttr(\"T\", &data_type_));",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "482:     }",
          "483:     string final_op_name;",
          "484:     OP_REQUIRES_OK(c, c->GetAttr(\"final_op\", &final_op_name));",
          "493:     NodeDef sub_node;",
          "494:     sub_node.add_input(c->def().input(0));",
          "495:     sub_node.add_input(c->def().input(0));",
          "496:     sub_node.set_device(c->def().device());",
          "498:                  &(*sub_node.mutable_attr())[\"T\"]);",
          "509:             << \" communication_hint \"",
          "511:   }",
          "513:  protected:",
          "",
          "[Removed Lines]",
          "485:     OP_REQUIRES_OK(",
          "486:         c, c->GetAttr(\"communication_hint\",",
          "487:                       &col_params_->instance.impl_details.communication_hint));",
          "488:     OP_REQUIRES_OK(",
          "489:         c, c->GetAttr(\"timeout_seconds\",",
          "490:                       &col_params_->instance.impl_details.timeout_seconds));",
          "497:     SetAttrValue(col_params_->instance.data_type,",
          "499:     col_params_->merge_op = BuildOpKernel(c, merge_op_name, &sub_node);",
          "500:     col_params_->final_op = BuildOpKernel(c, final_op_name, &sub_node);",
          "502:     col_params_->name = strings::StrCat(c->def().name(), \": ReduceV2(\",",
          "503:                                         merge_op_name, \",\", final_op_name, \")\");",
          "504:     col_params_->group.device_type = c->device_type();",
          "507:     col_params_->instance.impl_details.subdiv_offsets.push_back(0);",
          "508:     VLOG(2) << \"CollectiveReduceV2 \" << this << \" name \" << col_params_->name",
          "510:             << col_params_->instance.impl_details.communication_hint;",
          "",
          "[Added Lines]",
          "488:     OP_REQUIRES_OK(c, c->GetAttr(\"communication_hint\", &communication_hint_));",
          "489:     OP_REQUIRES_OK(c, c->GetAttr(\"timeout_seconds\", &timeout_seconds_));",
          "496:     SetAttrValue(col_params_.instance.data_type,",
          "498:     merge_op_ = BuildOpKernel(c, merge_op_name, &sub_node);",
          "499:     final_op_ = BuildOpKernel(c, final_op_name, &sub_node);",
          "501:     name_ = strings::StrCat(c->def().name(), \": ReduceV2(\", merge_op_name, \",\",",
          "502:                             final_op_name, \")\");",
          "503:     device_type_ = c->device_type();",
          "504:     VLOG(2) << \"CollectiveReduceV2 \" << this << \" name \" << col_params_.name",
          "506:             << col_params_.instance.impl_details.communication_hint;",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "527:         c, instance_key.dims() == 0,",
          "528:         errors::Internal(\"Unexpected dimensions on input instance_key\"), done);",
          "533:     col_params->group.group_size = group_size.unaligned_flat<int32>()(0);",
          "534:     col_params->group.group_key = group_key.unaligned_flat<int32>()(0);",
          "535:     col_params->instance.type = REDUCTION_COLLECTIVE;",
          "536:     col_params->instance.instance_key = instance_key.unaligned_flat<int32>()(0);",
          "546:     VLOG(1) << \"CollectiveReduceV2 group_size \" << col_params->group.group_size",
          "547:             << \" group_key \" << col_params->group.group_key << \" instance_key \"",
          "548:             << col_params->instance.instance_key;",
          "551:     Tensor* output = nullptr;",
          "552:     OP_REQUIRES_OK_ASYNC(",
          "553:         c, c->forward_input_or_allocate_output({0}, 0, input.shape(), &output),",
          "555:     col_params->instance.shape = input.shape();",
          "565:       VLOG(1) << \"CollectiveReduceV2 CompleteParams for collective \"",
          "566:               << col_params->name << \" device \" << c->device()->name()",
          "567:               << \" group \" << col_params->group.group_key << \" instance \"",
          "568:               << col_params->instance.instance_key;",
          "569:       col_exec->CompleteParamsAsync(",
          "572:           [c, done = std::move(done), col_params, col_exec](const Status& s) {",
          "573:             if (s.ok()) {",
          "574:               auto actual_done = [c, group_key = col_params->group.group_key,",
          "",
          "[Removed Lines]",
          "530:     auto col_params = std::make_shared<CollectiveParams>();",
          "531:     col_params->name = col_params_->name;",
          "532:     col_params->group.device_type = col_params_->group.device_type;",
          "537:     col_params->instance.data_type = col_params_->instance.data_type;",
          "538:     col_params->instance.impl_details.communication_hint =",
          "539:         col_params_->instance.impl_details.communication_hint;",
          "540:     col_params->instance.impl_details.timeout_seconds =",
          "541:         col_params_->instance.impl_details.timeout_seconds;",
          "542:     col_params->instance.impl_details.subdiv_offsets =",
          "543:         col_params_->instance.impl_details.subdiv_offsets;",
          "544:     col_params->merge_op = std::move(col_params_->merge_op);",
          "545:     col_params->final_op = std::move(col_params_->final_op);",
          "554:         done);",
          "558:     col_params_ = col_params;",
          "563:     c->collective_executor()->RunClosure([c, done = std::move(done), col_params,",
          "564:                                           col_exec]() {",
          "570:           c->device()->attributes(), col_params.get(),",
          "571:           c->cancellation_manager(),",
          "",
          "[Added Lines]",
          "526:     auto col_params = new CollectiveParams();",
          "527:     col_params->name = name_;",
          "528:     col_params->group.device_type = device_type_;",
          "533:     col_params->instance.data_type = data_type_;",
          "534:     col_params->instance.impl_details.communication_hint = communication_hint_;",
          "535:     col_params->instance.impl_details.timeout_seconds = timeout_seconds_;",
          "538:     col_params->instance.impl_details.subdiv_offsets.push_back(0);",
          "539:     col_params->merge_op = merge_op_.get();",
          "540:     col_params->final_op = final_op_.get();",
          "545:     auto done_with_cleanup = [col_params, done = std::move(done)]() {",
          "546:       delete col_params;",
          "547:       done();",
          "548:     };",
          "554:         done_with_cleanup);",
          "560:     c->collective_executor()->RunClosure([c,",
          "561:                                           done = std::move(done_with_cleanup),",
          "562:                                           col_params, col_exec]() {",
          "568:           c->device()->attributes(), col_params, c->cancellation_manager(),",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "602:   }",
          "604:  private:",
          "606: };",
          "608: REGISTER_KERNEL_BUILDER(Name(\"CollectiveReduceV2\").Device(DEVICE_CPU),",
          "",
          "[Removed Lines]",
          "605:   std::shared_ptr<CollectiveParams> col_params_;",
          "",
          "[Added Lines]",
          "602:   DataType data_type_ = DT_INVALID;",
          "603:   string communication_hint_;",
          "604:   float timeout_seconds_ = 0;",
          "605:   DeviceType device_type_;",
          "606:   std::unique_ptr<OpKernel> merge_op_;",
          "607:   std::unique_ptr<OpKernel> final_op_;",
          "608:   CollectiveParams col_params_;",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "673:         0, output_shape.dim_size(0) * col_params->group.group_size);",
          "674:     col_params->instance.shape = output_shape;",
          "680:     auto done_with_cleanup = [col_params, done = std::move(done)]() {",
          "681:       delete col_params;",
          "682:       done();",
          "683:     };",
          "",
          "[Removed Lines]",
          "676:     Tensor* output = nullptr;",
          "677:     OP_REQUIRES_OK_ASYNC(",
          "678:         c, c->allocate_output(0, col_params->instance.shape, &output), done);",
          "",
          "[Added Lines]",
          "684:     Tensor* output = nullptr;",
          "685:     OP_REQUIRES_OK_ASYNC(",
          "686:         c, c->allocate_output(0, col_params->instance.shape, &output),",
          "687:         done_with_cleanup);",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "727:   }",
          "729:  private:",
          "731:   string communication_hint_;",
          "733:   DeviceType device_type_;",
          "734: };",
          "",
          "[Removed Lines]",
          "730:   DataType data_type_;",
          "732:   float timeout_seconds_;",
          "",
          "[Added Lines]",
          "734:   DataType data_type_ = DT_INVALID;",
          "736:   float timeout_seconds_ = 0;",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "d345c40688a43345eebd5a440985daf77643ae3e",
      "candidate_info": {
        "commit_hash": "d345c40688a43345eebd5a440985daf77643ae3e",
        "repo": "tensorflow/tensorflow",
        "commit_url": "https://github.com/tensorflow/tensorflow/commit/d345c40688a43345eebd5a440985daf77643ae3e",
        "files": [
          "tensorflow/core/common_runtime/executor.cc",
          "tensorflow/core/kernels/collective_ops.cc",
          "tensorflow/python/distribute/cross_device_utils.py",
          "tensorflow/python/keras/distribute/distribute_strategy_test.py",
          "tensorflow/python/keras/engine/training.py",
          "tensorflow/python/kernel_tests/BUILD",
          "tensorflow/python/kernel_tests/collective_ops_test.py"
        ],
        "message": "[retry]Use cancellation manager to abort collectives\n\nThe previous change may cause a use-after-free since StartAbort() runs a separate thread but accesses resources owned by CollectiveExecutiveMgr. Once all cancellation callbacks finish, the CollectiveExecutorMgr may already be deallocated while StartAbort() is in progress. Fixing the ownership is not trivial so we now call StartAbort() in the cancellation callback instead to ensure all resources are valid. Note that with this we need to use TryDeregisterCallback in done() instead of DeregisterCallback(), because the latter blocks until all cancellation callback is done.\n\nWe used to always abort collective ops in executor when there're errors in graph execution. However there're some errors that are intended for the user to catch, and if we abort collective ops, the user program cannot continue. It's also not necessary\nto abort collective ops if there's no active ones.\n\nIdeally we should have a cancellation story for collectives. Before that, we can at least only abort collectives when it's necessary, i.e. when there're pending collective ops or failed collective ops.\n\nTo make the the catching EOF workflow work, we also need to make all collectives in gather depend on the input tensors, so there's better chance they fire after iterator GetNext. Without that the shape gathering may run in parallel with GetNext.\n\nPiperOrigin-RevId: 337997169\nChange-Id: I4a374f9ff00bdba38e012a96fb7f5837e049c85c",
        "before_after_code_files": [
          "tensorflow/core/common_runtime/executor.cc||tensorflow/core/common_runtime/executor.cc",
          "tensorflow/core/kernels/collective_ops.cc||tensorflow/core/kernels/collective_ops.cc",
          "tensorflow/python/distribute/cross_device_utils.py||tensorflow/python/distribute/cross_device_utils.py",
          "tensorflow/python/keras/distribute/distribute_strategy_test.py||tensorflow/python/keras/distribute/distribute_strategy_test.py",
          "tensorflow/python/keras/engine/training.py||tensorflow/python/keras/engine/training.py",
          "tensorflow/python/kernel_tests/collective_ops_test.py||tensorflow/python/kernel_tests/collective_ops_test.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "tensorflow/core/kernels/collective_ops.cc||tensorflow/core/kernels/collective_ops.cc",
            "tensorflow/python/kernel_tests/collective_ops_test.py||tensorflow/python/kernel_tests/collective_ops_test.py"
          ],
          "candidate": [
            "tensorflow/core/kernels/collective_ops.cc||tensorflow/core/kernels/collective_ops.cc",
            "tensorflow/python/kernel_tests/collective_ops_test.py||tensorflow/python/kernel_tests/collective_ops_test.py"
          ]
        }
      },
      "candidate_diff": {
        "tensorflow/core/common_runtime/executor.cc||tensorflow/core/common_runtime/executor.cc": [
          "File: tensorflow/core/common_runtime/executor.cc -> tensorflow/core/common_runtime/executor.cc",
          "--- Hunk 1 ---",
          "[Context before]",
          "1119:       if (rendezvous_) {",
          "1120:         rendezvous_->StartAbort(s);",
          "1121:       }",
          "1125:       if (cancellation_manager_) {",
          "1126:         cancellation_manager_->StartCancel();",
          "1127:       }",
          "1128:     }",
          "",
          "[Removed Lines]",
          "1122:       if (collective_executor_) {",
          "1123:         collective_executor_->StartAbort(s);",
          "1124:       }",
          "",
          "[Added Lines]",
          "1124:       } else if (collective_executor_) {",
          "1128:         collective_executor_->StartAbort(s);",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1267:       if (rendezvous_) {",
          "1268:         rendezvous_->StartAbort(status);",
          "1269:       }",
          "1273:       if (cancellation_manager_) {",
          "1274:         cancellation_manager_->StartCancel();",
          "1275:       }",
          "1276:     }",
          "1277:     delete this;",
          "",
          "[Removed Lines]",
          "1270:       if (collective_executor_) {",
          "1271:         collective_executor_->StartAbort(status);",
          "1272:       }",
          "",
          "[Added Lines]",
          "1274:       } else if (collective_executor_) {",
          "1278:         collective_executor_->StartAbort(status);",
          "",
          "---------------"
        ],
        "tensorflow/core/kernels/collective_ops.cc||tensorflow/core/kernels/collective_ops.cc": [
          "File: tensorflow/core/kernels/collective_ops.cc -> tensorflow/core/kernels/collective_ops.cc",
          "--- Hunk 1 ---",
          "[Context before]",
          "52: class CollectiveOpKernel : public AsyncOpKernel {",
          "53:  public:",
          "",
          "[Removed Lines]",
          "54:   explicit CollectiveOpKernel(OpKernelConstruction* c) : AsyncOpKernel(c) {}",
          "",
          "[Added Lines]",
          "54:   explicit CollectiveOpKernel(OpKernelConstruction* c)",
          "55:       : AsyncOpKernel(c), name_(name()) {}",
          "57:   void ComputeAsync(OpKernelContext* c, DoneCallback done) override {",
          "58:     CollectiveExecutor* col_exec = c->collective_executor();",
          "59:     OP_REQUIRES_ASYNC(",
          "60:         c, col_exec,",
          "61:         errors::Internal(",
          "62:             \"Failed to get CollectiveExecutor from OpKernelContext for Op \",",
          "63:             name_),",
          "64:         done);",
          "65:     const CancellationToken token =",
          "66:         c->cancellation_manager()->get_cancellation_token();",
          "67:     const bool already_cancelled =",
          "68:         !c->cancellation_manager()->RegisterCallback(token, [col_exec]() {",
          "72:           col_exec->StartAbort(errors::Cancelled(\"op cancelled\"));",
          "73:         });",
          "74:     OP_REQUIRES_ASYNC(c, !already_cancelled,",
          "75:                       errors::Cancelled(\"op cancelled \", name_), done);",
          "77:     auto deregister_and_done = [c, col_exec, token, done = std::move(done)]() {",
          "81:       c->cancellation_manager()->TryDeregisterCallback(token);",
          "84:       if (!c->status().ok()) {",
          "85:         col_exec->StartAbort(c->status());",
          "86:       }",
          "87:       done();",
          "88:     };",
          "89:     ComputeAsyncImpl(c, col_exec, std::move(deregister_and_done));",
          "90:   }",
          "92:  protected:",
          "93:   virtual void ComputeAsyncImpl(OpKernelContext* c,",
          "94:                                 CollectiveExecutor* col_exec,",
          "95:                                 DoneCallback done) = 0;",
          "97:   string name_;",
          "98: };",
          "100: class CollectiveOpV1Kernel : public CollectiveOpKernel {",
          "101:  public:",
          "102:   explicit CollectiveOpV1Kernel(OpKernelConstruction* c)",
          "103:       : CollectiveOpKernel(c) {}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "90:     return true;",
          "91:   }",
          "93:   CollectiveParams col_params_;",
          "94:   std::vector<int32> dependencies_;",
          "95: };",
          "98:  public:",
          "99:   explicit CollectiveGatherOpKernel(OpKernelConstruction* c)",
          "101:     col_params_.instance.type = GATHER_COLLECTIVE;",
          "102:     OP_REQUIRES_OK(c, c->GetAttr(\"group_size\", &col_params_.group.group_size));",
          "103:     OP_REQUIRES(",
          "",
          "[Removed Lines]",
          "97: class CollectiveGatherOpKernel : public CollectiveOpKernel {",
          "100:       : CollectiveOpKernel(c) {",
          "",
          "[Added Lines]",
          "142:  protected:",
          "147: class CollectiveGatherOpKernel : public CollectiveOpV1Kernel {",
          "150:       : CollectiveOpV1Kernel(c) {",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "119:     col_params_.group.device_type = c->device_type();",
          "120:   }",
          "131:     auto output_shape = c->input(0).shape();",
          "132:     output_shape.set_dim(",
          "133:         0, output_shape.dim_size(0) * col_params_.group.group_size);",
          "",
          "[Removed Lines]",
          "122:   void ComputeAsync(OpKernelContext* c, DoneCallback done) override {",
          "123:     CollectiveExecutor* col_exec = c->collective_executor();",
          "124:     OP_REQUIRES_ASYNC(",
          "125:         c, col_exec,",
          "126:         errors::Internal(",
          "127:             \"Failed to get CollectiveExecutor from OpKernelContext for Op \",",
          "128:             col_params_.name),",
          "129:         done);",
          "",
          "[Added Lines]",
          "172:  protected:",
          "173:   void ComputeAsyncImpl(OpKernelContext* c, CollectiveExecutor* col_exec,",
          "174:                         DoneCallback done) override {",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "171: REGISTER_KERNEL_BUILDER(Name(\"CollectiveGather\").Device(DEVICE_GPU),",
          "172:                         CollectiveGatherOpKernel);",
          "175:  public:",
          "176:   explicit CollectiveReduceOpKernel(OpKernelConstruction* c)",
          "178:     col_params_.instance.type = REDUCTION_COLLECTIVE;",
          "179:     OP_REQUIRES_OK(c, c->GetAttr(\"group_size\", &col_params_.group.group_size));",
          "180:     OP_REQUIRES(",
          "",
          "[Removed Lines]",
          "174: class CollectiveReduceOpKernel : public CollectiveOpKernel {",
          "177:       : CollectiveOpKernel(c) {",
          "",
          "[Added Lines]",
          "218: class CollectiveReduceOpKernel : public CollectiveOpV1Kernel {",
          "221:       : CollectiveOpV1Kernel(c) {",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "231:     col_params_.final_op = BuildOpKernel(c, final_op_name, &sub_node);",
          "232:   }",
          "",
          "[Removed Lines]",
          "234:   void ComputeAsync(OpKernelContext* c, DoneCallback done) override {",
          "235:     CollectiveExecutor* col_exec = c->collective_executor();",
          "236:     OP_REQUIRES_ASYNC(",
          "237:         c, col_exec,",
          "238:         errors::Internal(",
          "239:             \"Failed to get CollectiveExecutor from OpKernelContext for Op \",",
          "240:             col_params_.name),",
          "241:         done);",
          "",
          "[Added Lines]",
          "278:  protected:",
          "279:   void ComputeAsyncImpl(OpKernelContext* c, CollectiveExecutor* col_exec,",
          "280:                         DoneCallback done) override {",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "280: REGISTER_KERNEL_BUILDER(Name(\"CollectiveReduce\").Device(DEVICE_GPU),",
          "281:                         CollectiveReduceOpKernel);",
          "284:  public:",
          "285:   explicit CollectiveBcastSendOpKernel(OpKernelConstruction* c)",
          "287:     col_params_.instance.type = BROADCAST_COLLECTIVE;",
          "288:     OP_REQUIRES_OK(c, c->GetAttr(\"group_size\", &col_params_.group.group_size));",
          "289:     OP_REQUIRES(",
          "",
          "[Removed Lines]",
          "283: class CollectiveBcastSendOpKernel : public CollectiveOpKernel {",
          "286:       : CollectiveOpKernel(c) {",
          "",
          "[Added Lines]",
          "322: class CollectiveBcastSendOpKernel : public CollectiveOpV1Kernel {",
          "325:       : CollectiveOpV1Kernel(c) {",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "309:     col_params_.group.device_type = c->device_type();",
          "310:   }",
          "",
          "[Removed Lines]",
          "312:   void ComputeAsync(OpKernelContext* c, DoneCallback done) override {",
          "313:     CollectiveExecutor* col_exec = c->collective_executor();",
          "314:     OP_REQUIRES_ASYNC(",
          "315:         c, col_exec,",
          "316:         errors::Internal(",
          "317:             \"Failed to get CollectiveExecutor from OpKernelContext for Op \",",
          "318:             col_params_.name),",
          "319:         done);",
          "",
          "[Added Lines]",
          "351:  protected:",
          "352:   void ComputeAsyncImpl(OpKernelContext* c, CollectiveExecutor* col_exec,",
          "353:                         DoneCallback done) override {",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "362: REGISTER_KERNEL_BUILDER(Name(\"CollectiveBcastSend\").Device(DEVICE_GPU),",
          "363:                         CollectiveBcastSendOpKernel);",
          "366:  public:",
          "367:   explicit CollectiveBcastRecvOpKernel(OpKernelConstruction* c)",
          "369:     col_params_.instance.type = BROADCAST_COLLECTIVE;",
          "370:     OP_REQUIRES_OK(c, c->GetAttr(\"group_size\", &col_params_.group.group_size));",
          "371:     OP_REQUIRES(",
          "",
          "[Removed Lines]",
          "365: class CollectiveBcastRecvOpKernel : public CollectiveOpKernel {",
          "368:       : CollectiveOpKernel(c) {",
          "",
          "[Added Lines]",
          "399: class CollectiveBcastRecvOpKernel : public CollectiveOpV1Kernel {",
          "402:       : CollectiveOpV1Kernel(c) {",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "391:     col_params_.group.device_type = c->device_type();",
          "392:   }",
          "",
          "[Removed Lines]",
          "394:   void ComputeAsync(OpKernelContext* c, DoneCallback done) override {",
          "395:     CollectiveExecutor* col_exec = c->collective_executor();",
          "396:     OP_REQUIRES_ASYNC(",
          "397:         c, col_exec,",
          "398:         errors::Internal(",
          "399:             \"Failed to get CollectiveExecutor from OpKernelContext for Op \",",
          "400:             col_params_.name),",
          "401:         done);",
          "",
          "[Added Lines]",
          "428:  protected:",
          "429:   void ComputeAsyncImpl(OpKernelContext* c, CollectiveExecutor* col_exec,",
          "430:                         DoneCallback done) override {",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "437: REGISTER_KERNEL_BUILDER(Name(\"CollectiveBcastRecv\").Device(DEVICE_GPU),",
          "438:                         CollectiveBcastRecvOpKernel);",
          "441:  public:",
          "442:   explicit CollectiveReduceV2OpKernel(OpKernelConstruction* c)",
          "444:     col_params_ = std::make_shared<CollectiveParams>();",
          "445:     OP_REQUIRES_OK(c, c->GetAttr(\"T\", &col_params_->instance.data_type));",
          "446:     string merge_op_name;",
          "",
          "[Removed Lines]",
          "440: class CollectiveReduceV2OpKernel : public AsyncOpKernel {",
          "443:       : AsyncOpKernel(c) {",
          "",
          "[Added Lines]",
          "469: class CollectiveReduceV2OpKernel : public CollectiveOpKernel {",
          "472:       : CollectiveOpKernel(c) {",
          "",
          "---------------",
          "--- Hunk 11 ---",
          "[Context before]",
          "481:             << col_params_->instance.impl_details.communication_hint;",
          "482:   }",
          "492:     const Tensor& input = c->input(0);",
          "493:     const Tensor& group_size = c->input(1);",
          "494:     const Tensor& group_key = c->input(2);",
          "",
          "[Removed Lines]",
          "484:   void ComputeAsync(OpKernelContext* c, DoneCallback done) override {",
          "485:     CollectiveExecutor* col_exec = c->collective_executor();",
          "486:     OP_REQUIRES_ASYNC(",
          "487:         c, col_exec,",
          "488:         errors::Internal(",
          "489:             \"Failed to get CollectiveExecutor from OpKernelContext for Op \",",
          "490:             col_params_->name),",
          "491:         done);",
          "",
          "[Added Lines]",
          "513:  protected:",
          "514:   void ComputeAsyncImpl(OpKernelContext* c, CollectiveExecutor* col_exec,",
          "515:                         DoneCallback done) override {",
          "",
          "---------------",
          "--- Hunk 12 ---",
          "[Context before]",
          "590:                             .HostMemory(\"instance_key\"),",
          "591:                         CollectiveReduceV2OpKernel);",
          "594:  public:",
          "595:   explicit CollectiveGatherV2OpKernel(OpKernelConstruction* c)",
          "597:     OP_REQUIRES_OK(c, c->GetAttr(\"T\", &data_type_));",
          "598:     OP_REQUIRES_OK(c, c->GetAttr(\"communication_hint\", &communication_hint_));",
          "599:     OP_REQUIRES_OK(c, c->GetAttr(\"timeout_seconds\", &timeout_seconds_));",
          "",
          "[Removed Lines]",
          "593: class CollectiveGatherV2OpKernel : public AsyncOpKernel {",
          "596:       : AsyncOpKernel(c), device_type_(DEVICE_DEFAULT) {",
          "",
          "[Added Lines]",
          "617: class CollectiveGatherV2OpKernel : public CollectiveOpKernel {",
          "620:       : CollectiveOpKernel(c), device_type_(DEVICE_DEFAULT) {",
          "",
          "---------------",
          "--- Hunk 13 ---",
          "[Context before]",
          "603:             << \" communication_hint \" << communication_hint_;",
          "604:   }",
          "614:     const Tensor& input = c->input(0);",
          "615:     const Tensor& group_size = c->input(1);",
          "616:     const Tensor& group_key = c->input(2);",
          "",
          "[Removed Lines]",
          "606:   void ComputeAsync(OpKernelContext* c, DoneCallback done) override {",
          "607:     CollectiveExecutor* col_exec = c->collective_executor();",
          "608:     OP_REQUIRES_ASYNC(",
          "609:         c, col_exec,",
          "610:         errors::Internal(",
          "611:             \"Failed to get CollectiveExecutor from OpKernelContext for Op \",",
          "612:             name_),",
          "613:         done);",
          "",
          "[Added Lines]",
          "630:  protected:",
          "631:   void ComputeAsyncImpl(OpKernelContext* c, CollectiveExecutor* col_exec,",
          "632:                         DoneCallback done) override {",
          "",
          "---------------",
          "--- Hunk 14 ---",
          "[Context before]",
          "712:   string communication_hint_;",
          "713:   float timeout_seconds_;",
          "714:   DeviceType device_type_;",
          "716: };",
          "718: REGISTER_KERNEL_BUILDER(Name(\"CollectiveGatherV2\").Device(DEVICE_CPU),",
          "",
          "[Removed Lines]",
          "715:   string name_;",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "tensorflow/python/distribute/cross_device_utils.py||tensorflow/python/distribute/cross_device_utils.py": [
          "File: tensorflow/python/distribute/cross_device_utils.py -> tensorflow/python/distribute/cross_device_utils.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "412:         self._group_key, self._device)",
          "413:     instance_key_shape = self._collective_keys.get_instance_key(",
          "414:         self._group_key, self._device)",
          "416:       # 1. Transpose",
          "417:       # E.g. Given an input_tensor with shape [2,2,5,1] and axis to gather is 3,",
          "418:       # we use perm_pre=[3 0 1 2] to reshape it to [1,2,2,5], which",
          "",
          "[Removed Lines]",
          "415:     with ops.device(self._device):",
          "",
          "[Added Lines]",
          "415:     with ops.device(self._device), \\",
          "416:          ops.control_dependencies([array_ops.identity(input_tensor)]):",
          "",
          "---------------"
        ],
        "tensorflow/python/keras/distribute/distribute_strategy_test.py||tensorflow/python/keras/distribute/distribute_strategy_test.py": [
          "File: tensorflow/python/keras/distribute/distribute_strategy_test.py -> tensorflow/python/keras/distribute/distribute_strategy_test.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "29: from tensorflow.python.data.ops import dataset_ops",
          "30: from tensorflow.python.data.ops import readers",
          "31: from tensorflow.python.distribute import central_storage_strategy",
          "33: from tensorflow.python.distribute import combinations as ds_combinations",
          "34: from tensorflow.python.distribute import distribution_strategy_context",
          "35: from tensorflow.python.distribute import mirrored_strategy",
          "",
          "[Removed Lines]",
          "32: from tensorflow.python.distribute import collective_all_reduce_strategy",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1151:     if mode == 'graph' and _is_tpu_strategy(distribution):",
          "1152:       self.skipTest('partial batch not supported with TPU in graph mode.')",
          "1157:     with self.cached_session():",
          "1158:       with distribution.scope():",
          "1159:         optimizer_fn = gradient_descent_keras.SGD",
          "",
          "[Removed Lines]",
          "1154:     if isinstance(distribution,",
          "1155:                   collective_all_reduce_strategy.CollectiveAllReduceStrategy):",
          "1156:       self.skipTest('EOF error causes subsequent collective ops fail.')",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "1166:             loss,",
          "1167:             metrics=metrics)",
          "1171:       # steps/steps_per_epoch are calculated when using numpy arrays as",
          "1172:       # input data.",
          "1173:       fit_with_numpy = model.fit(",
          "",
          "[Removed Lines]",
          "1169:       inputs = np.zeros((1000, 3), dtype=np.float32)",
          "1170:       targets = np.zeros((1000, 4), dtype=np.float32)",
          "",
          "[Added Lines]",
          "1165:       inputs = np.zeros((100, 3), dtype=np.float32)",
          "1166:       targets = np.zeros((100, 4), dtype=np.float32)",
          "",
          "---------------"
        ],
        "tensorflow/python/keras/engine/training.py||tensorflow/python/keras/engine/training.py": [
          "File: tensorflow/python/keras/engine/training.py -> tensorflow/python/keras/engine/training.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "2756: def _multi_worker_concat(v, strategy):",
          "2757:   \"\"\"Order PerReplica objects for CollectiveAllReduceStrategy and concat.\"\"\"",
          "2758:   replicas = strategy.gather(v, axis=0)  # pylint: disable=protected-access",
          "2773:   replicas = array_ops.split(",
          "2774:       replicas,",
          "",
          "[Removed Lines]",
          "2759:   # v might not have the same shape on different replicas",
          "2760:   if isinstance(v, ds_values.PerReplica):",
          "2761:     shapes = array_ops.concat([",
          "2762:         array_ops.expand_dims_v2(array_ops.shape(single_value)[0], axis=0)",
          "2763:         for single_value in v.values",
          "2764:     ],",
          "2765:                               axis=0)",
          "2766:     all_shapes = strategy.gather(shapes, axis=0)  # pylint: disable=protected-access",
          "2767:   else:",
          "2768:     # v is a tensor. This may happen when, say, we have 2x1 multi-worker.",
          "2769:     all_shapes = strategy.gather(  # pylint: disable=protected-access",
          "2770:         array_ops.expand_dims_v2(array_ops.shape(v)[0], axis=0),",
          "2771:         axis=0)",
          "",
          "[Added Lines]",
          "2759:   # TODO(b/170435030): We now need to make sure these run after the iterator",
          "2760:   # GetNext, so that we don't trigger aborting collective ops in the case of",
          "2761:   # EOF. Remove after the issue is fixed.",
          "2762:   with ops.control_dependencies([replicas]):",
          "2763:     # v might not have the same shape on different replicas",
          "2764:     if isinstance(v, ds_values.PerReplica):",
          "2765:       shapes = array_ops.concat([",
          "2766:           array_ops.expand_dims_v2(array_ops.shape(single_value)[0], axis=0)",
          "2767:           for single_value in v.values",
          "2768:       ],",
          "2769:                                 axis=0)",
          "2770:       all_shapes = strategy.gather(shapes, axis=0)",
          "2771:     else:",
          "2772:       # v is a tensor. This may happen when, say, we have 2x1 multi-worker.",
          "2773:       all_shapes = strategy.gather(",
          "2774:           array_ops.expand_dims_v2(array_ops.shape(v)[0], axis=0),",
          "2775:           axis=0)",
          "",
          "---------------"
        ],
        "tensorflow/python/kernel_tests/collective_ops_test.py||tensorflow/python/kernel_tests/collective_ops_test.py": [
          "File: tensorflow/python/kernel_tests/collective_ops_test.py -> tensorflow/python/kernel_tests/collective_ops_test.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "24: from absl.testing import parameterized",
          "26: from tensorflow.python.compat import v2_compat",
          "27: from tensorflow.python.distribute import combinations",
          "28: from tensorflow.python.distribute import test_util",
          "29: from tensorflow.python.eager import context",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "27: from tensorflow.python.data.experimental.ops import testing as dataset_testing",
          "28: from tensorflow.python.data.ops import dataset_ops",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "469:     _setup_context()",
          "470:     def_function.function(collective_fn)()",
          "473: @combinations.generate(",
          "474:     combinations.times(",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "474:   def testOpErrorNotAbort(self, collective_op, device, communication):",
          "475:     # Do not abort if there's no active collective ops. There could be",
          "476:     # exceptions like EOF which we expect users to catch, aborting collective",
          "477:     # ops on all op errors intervenes with this workflow.",
          "478:     dev0 = '/device:%s:0' % device",
          "479:     dev1 = '/device:%s:1' % device",
          "480:     group_size = 2",
          "481:     group_key = 100",
          "482:     instance_key = 100",
          "483:     dataset = dataset_ops.Dataset.from_tensors([1.])",
          "485:     @def_function.function",
          "486:     def collective_fn(in_tensor):",
          "487:       for device in [dev0, dev1]:",
          "488:         with ops.device(device):",
          "489:           collective_op(",
          "490:               in_tensor,",
          "491:               group_size,",
          "492:               group_key,",
          "493:               instance_key,",
          "494:               communication_hint=communication)",
          "496:     @def_function.function",
          "497:     def f():",
          "498:       iterator = iter(dataset)",
          "499:       collective_fn(next(iterator))",
          "500:       # This next(iterator) should raise EOF.",
          "501:       collective_fn(next(iterator))",
          "503:     with self.assertRaises(errors.OutOfRangeError):",
          "504:       f()",
          "505:     collective_fn(constant_op.constant([1.]))",
          "507:   def testOpErrorAbort(self, collective_op, device, communication):",
          "508:     # Abort collective ops if there're active collective ops at the time of an",
          "509:     # op error. This is due to the inability to cancel collective ops, and op",
          "510:     # errors may cause running collective ops to hang.",
          "511:     dev0 = '/device:%s:0' % device",
          "512:     group_size = 2",
          "513:     group_key = 100",
          "514:     instance_key = 100",
          "515:     in_tensor = constant_op.constant([1.])",
          "516:     # Make the dataset sleep a while so that the collective is being executed",
          "517:     # when the EOF happens.",
          "518:     dataset = dataset_ops.Dataset.from_tensors([1.]).apply(",
          "519:         dataset_testing.sleep(sleep_microseconds=200))",
          "521:     @def_function.function",
          "522:     def f():",
          "523:       # Launch a collective op that won't be able to finish to test abortion",
          "524:       # when other ops error.",
          "525:       with ops.device(dev0):",
          "526:         ret = collective_op(",
          "527:             in_tensor,",
          "528:             group_size,",
          "529:             group_key,",
          "530:             instance_key,",
          "531:             communication_hint=communication)",
          "532:       iterator = iter(dataset)",
          "533:       next(iterator)",
          "534:       # This should raise EOF.",
          "535:       next(iterator)",
          "536:       return ret",
          "538:     with self.assertRaises(errors.OutOfRangeError):",
          "539:       f()",
          "540:     # Now collective ops is aborted, subsequent collective ops should fail with",
          "541:     # the previous error.",
          "542:     with self.assertRaises(errors.CancelledError):",
          "543:       with ops.device(dev0):",
          "544:         collective_op(",
          "545:             in_tensor,",
          "546:             group_size,",
          "547:             group_key,",
          "548:             instance_key,",
          "549:             communication_hint=communication)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "da2e275a21ffd42634a97de93e42528d717b0b24",
      "candidate_info": {
        "commit_hash": "da2e275a21ffd42634a97de93e42528d717b0b24",
        "repo": "tensorflow/tensorflow",
        "commit_url": "https://github.com/tensorflow/tensorflow/commit/da2e275a21ffd42634a97de93e42528d717b0b24",
        "files": [
          "tensorflow/core/api_def/base_api/api_def_CollectiveAllToAllV3.pbtxt",
          "tensorflow/core/api_def/base_api/api_def_CollectiveInitializeCommunicator.pbtxt",
          "tensorflow/core/api_def/base_api/api_def_CollectiveReduceV3.pbtxt",
          "tensorflow/core/common_runtime/collective_param_resolver_local.cc",
          "tensorflow/core/common_runtime/collective_param_resolver_local.h",
          "tensorflow/core/distributed_runtime/collective_param_resolver_distributed.cc",
          "tensorflow/core/framework/collective.h",
          "tensorflow/core/grappler/optimizers/function_optimizer.cc",
          "tensorflow/core/kernels/collective_ops.cc",
          "tensorflow/core/ops/collective_ops.cc",
          "tensorflow/python/framework/auto_control_deps.py",
          "tensorflow/python/kernel_tests/collective_ops_test.py",
          "tensorflow/python/ops/collective_ops.py",
          "tensorflow/tools/api/golden/v1/tensorflow.raw_ops.pbtxt",
          "tensorflow/tools/api/golden/v2/tensorflow.raw_ops.pbtxt"
        ],
        "message": "Add an implementation of group initialization op. The op creates a resource for holding group information such as group_size, group_key etc. after initializing the group.\n\nAllToAll and AllReduce ops consume the resource as input and lookup group information for the already initialized group.\n\nPiperOrigin-RevId: 398291817\nChange-Id: I6e5eaf8530b53934f7501cbeaf4f4729b7011222",
        "before_after_code_files": [
          "tensorflow/core/api_def/base_api/api_def_CollectiveAllToAllV3.pbtxt||tensorflow/core/api_def/base_api/api_def_CollectiveAllToAllV3.pbtxt",
          "tensorflow/core/api_def/base_api/api_def_CollectiveInitializeCommunicator.pbtxt||tensorflow/core/api_def/base_api/api_def_CollectiveInitializeCommunicator.pbtxt",
          "tensorflow/core/api_def/base_api/api_def_CollectiveReduceV3.pbtxt||tensorflow/core/api_def/base_api/api_def_CollectiveReduceV3.pbtxt",
          "tensorflow/core/common_runtime/collective_param_resolver_local.cc||tensorflow/core/common_runtime/collective_param_resolver_local.cc",
          "tensorflow/core/common_runtime/collective_param_resolver_local.h||tensorflow/core/common_runtime/collective_param_resolver_local.h",
          "tensorflow/core/distributed_runtime/collective_param_resolver_distributed.cc||tensorflow/core/distributed_runtime/collective_param_resolver_distributed.cc",
          "tensorflow/core/framework/collective.h||tensorflow/core/framework/collective.h",
          "tensorflow/core/grappler/optimizers/function_optimizer.cc||tensorflow/core/grappler/optimizers/function_optimizer.cc",
          "tensorflow/core/kernels/collective_ops.cc||tensorflow/core/kernels/collective_ops.cc",
          "tensorflow/core/ops/collective_ops.cc||tensorflow/core/ops/collective_ops.cc",
          "tensorflow/python/framework/auto_control_deps.py||tensorflow/python/framework/auto_control_deps.py",
          "tensorflow/python/kernel_tests/collective_ops_test.py||tensorflow/python/kernel_tests/collective_ops_test.py",
          "tensorflow/python/ops/collective_ops.py||tensorflow/python/ops/collective_ops.py",
          "tensorflow/tools/api/golden/v1/tensorflow.raw_ops.pbtxt||tensorflow/tools/api/golden/v1/tensorflow.raw_ops.pbtxt",
          "tensorflow/tools/api/golden/v2/tensorflow.raw_ops.pbtxt||tensorflow/tools/api/golden/v2/tensorflow.raw_ops.pbtxt"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "tensorflow/core/kernels/collective_ops.cc||tensorflow/core/kernels/collective_ops.cc",
            "tensorflow/python/kernel_tests/collective_ops_test.py||tensorflow/python/kernel_tests/collective_ops_test.py"
          ],
          "candidate": [
            "tensorflow/core/kernels/collective_ops.cc||tensorflow/core/kernels/collective_ops.cc",
            "tensorflow/python/kernel_tests/collective_ops_test.py||tensorflow/python/kernel_tests/collective_ops_test.py"
          ]
        }
      },
      "candidate_diff": {
        "tensorflow/core/api_def/base_api/api_def_CollectiveAllToAllV3.pbtxt||tensorflow/core/api_def/base_api/api_def_CollectiveAllToAllV3.pbtxt": [
          "File: tensorflow/core/api_def/base_api/api_def_CollectiveAllToAllV3.pbtxt -> tensorflow/core/api_def/base_api/api_def_CollectiveAllToAllV3.pbtxt",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: op {",
          "2:   graph_op_name: \"CollectiveAllToAllV3\"",
          "3:   summary: \"Mutually exchanges multiple tensors of identical type and shape.\"",
          "4:   visibility: HIDDEN",
          "5: }",
          "",
          "---------------"
        ],
        "tensorflow/core/api_def/base_api/api_def_CollectiveInitializeCommunicator.pbtxt||tensorflow/core/api_def/base_api/api_def_CollectiveInitializeCommunicator.pbtxt": [
          "File: tensorflow/core/api_def/base_api/api_def_CollectiveInitializeCommunicator.pbtxt -> tensorflow/core/api_def/base_api/api_def_CollectiveInitializeCommunicator.pbtxt",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: op {",
          "2:   graph_op_name: \"CollectiveInitializeCommunicator\"",
          "3:   summary: \"Initializes a group for collective operations.\"",
          "4:   visibility: HIDDEN",
          "5: }",
          "",
          "---------------"
        ],
        "tensorflow/core/api_def/base_api/api_def_CollectiveReduceV3.pbtxt||tensorflow/core/api_def/base_api/api_def_CollectiveReduceV3.pbtxt": [
          "File: tensorflow/core/api_def/base_api/api_def_CollectiveReduceV3.pbtxt -> tensorflow/core/api_def/base_api/api_def_CollectiveReduceV3.pbtxt",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: op {",
          "2:   graph_op_name: \"CollectiveReduceV3\"",
          "3:   summary: \"Mutually reduces multiple tensors of identical type and shape.\"",
          "4:   visibility: HIDDEN",
          "5: }",
          "",
          "---------------"
        ],
        "tensorflow/core/common_runtime/collective_param_resolver_local.cc||tensorflow/core/common_runtime/collective_param_resolver_local.cc": [
          "File: tensorflow/core/common_runtime/collective_param_resolver_local.cc -> tensorflow/core/common_runtime/collective_param_resolver_local.cc",
          "--- Hunk 1 ---",
          "[Context before]",
          "23: #include \"tensorflow/core/common_runtime/device_mgr.h\"",
          "24: #include \"tensorflow/core/framework/cancellation.h\"",
          "25: #include \"tensorflow/core/framework/device_attributes.pb.h\"",
          "26: #include \"tensorflow/core/framework/types.h\"",
          "27: #include \"tensorflow/core/lib/core/errors.h\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "25: #include \"tensorflow/core/framework/collective.h\"",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "589:   return irec;",
          "590: }",
          "592: void CollectiveParamResolverLocal::CompleteParamsAsync(",
          "593:     const DeviceAttributes& device, CollectiveParams* cp,",
          "594:     CancellationManager* cancel_mgr, const StatusCallback& done) {",
          "595:   VLOG(1) << \"CompleteParams local \" << device.name() << \" for \" << cp << \": \"",
          "596:           << cp->ToString();",
          "605: }",
          "607: void CollectiveParamResolverLocal::CompleteInstanceAsync(",
          "",
          "[Removed Lines]",
          "597:   CompleteGroupLocal(device, &cp->group, cancel_mgr,",
          "598:                      [this, device, cp, done](const Status& s) {",
          "599:                        if (s.ok()) {",
          "600:                          CompleteInstanceLocal(device.name(), cp, done);",
          "601:                        } else {",
          "602:                          done(s);",
          "603:                        }",
          "604:                      });",
          "",
          "[Added Lines]",
          "593: Status CollectiveParamResolverLocal::LookupAndPopulateGroupParams(",
          "594:     CollGroupParams* group) {",
          "595:   mutex_lock l(group_mu_);",
          "596:   auto group_rec = group_table_.find(group->group_key);",
          "597:   if (group_rec == group_table_.end()) {",
          "598:     return errors::InvalidArgument(\"Group \", group->group_key,",
          "599:                                    \" is not \"",
          "600:                                    \"initialized. Please call group \"",
          "601:                                    \"initialization op first before invoking \"",
          "602:                                    \"collective op.\");",
          "603:   }",
          "604:   mutex_lock lock(group_rec->second->mu);",
          "605:   if (!group_rec->second->status.ok()) {",
          "606:     return errors::FailedPrecondition(",
          "607:         \"Failed to run collective due to \"",
          "608:         \"unsuccessful group initialization. \"",
          "609:         \"Group initialization failed with error \",",
          "610:         group_rec->second->status.ToString());",
          "611:   }",
          "613:   return Status::OK();",
          "614: }",
          "621:   if (cp->run_group_initialization) {",
          "622:     CompleteGroupLocal(device, &cp->group, cancel_mgr,",
          "623:                        [this, device, cp, done](const Status& s) {",
          "624:                          if (s.ok()) {",
          "625:                            CompleteInstanceLocal(device.name(), cp, done);",
          "626:                          } else {",
          "627:                            done(s);",
          "628:                          }",
          "629:                        });",
          "630:   } else {",
          "633:     auto s = LookupAndPopulateGroupParams(&cp->group);",
          "634:     if (s.ok()) {",
          "635:       CompleteInstanceLocal(device.name(), cp, done);",
          "636:     } else {",
          "637:       done(s);",
          "638:     }",
          "639:   }",
          "",
          "---------------"
        ],
        "tensorflow/core/common_runtime/collective_param_resolver_local.h||tensorflow/core/common_runtime/collective_param_resolver_local.h": [
          "File: tensorflow/core/common_runtime/collective_param_resolver_local.h -> tensorflow/core/common_runtime/collective_param_resolver_local.h",
          "--- Hunk 1 ---",
          "[Context before]",
          "97:   void CancelGroup(int32 group_key) TF_LOCKS_EXCLUDED(group_mu_);",
          "100:   struct InstanceRec;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "100:   Status LookupAndPopulateGroupParams(CollGroupParams* group_params);",
          "",
          "---------------"
        ],
        "tensorflow/core/distributed_runtime/collective_param_resolver_distributed.cc||tensorflow/core/distributed_runtime/collective_param_resolver_distributed.cc": [
          "File: tensorflow/core/distributed_runtime/collective_param_resolver_distributed.cc -> tensorflow/core/distributed_runtime/collective_param_resolver_distributed.cc",
          "--- Hunk 1 ---",
          "[Context before]",
          "109:     CancellationManager* cancel_mgr, const StatusCallback& done) {",
          "110:   VLOG(1) << \"CompleteParams distributed \" << device.name() << \" for \" << cp",
          "111:           << \": \" << cp->ToString();",
          "124: }",
          "126: void CollectiveParamResolverDistributed::CompleteGroupAsync(",
          "",
          "[Removed Lines]",
          "112:   CompleteGroupDistributed(",
          "113:       device, &cp->group, cancel_mgr,",
          "114:       [this, device, cp, cancel_mgr, done](Status s) {",
          "115:         if (s.ok()) {",
          "116:           s = dev_resolver_->UpdateDeviceAttributes(cp->group.devices);",
          "117:         }",
          "118:         if (s.ok()) {",
          "119:           CompleteInstanceDistributed(device.name(), cp, cancel_mgr, done);",
          "120:         } else {",
          "121:           done(s);",
          "122:         }",
          "123:       });",
          "",
          "[Added Lines]",
          "112:   if (cp->run_group_initialization) {",
          "113:     CompleteGroupDistributed(",
          "114:         device, &cp->group, cancel_mgr,",
          "115:         [this, device, cp, cancel_mgr, done](Status s) {",
          "116:           if (s.ok()) {",
          "117:             s = dev_resolver_->UpdateDeviceAttributes(cp->group.devices);",
          "118:           }",
          "119:           if (s.ok()) {",
          "120:             CompleteInstanceDistributed(device.name(), cp, cancel_mgr, done);",
          "121:           } else {",
          "122:             done(s);",
          "123:           }",
          "124:         });",
          "125:   } else {",
          "128:     auto s = LookupAndPopulateGroupParams(&cp->group);",
          "129:     if (s.ok()) {",
          "130:       CompleteInstanceDistributed(device.name(), cp, cancel_mgr, done);",
          "131:     } else {",
          "132:       done(s);",
          "133:     }",
          "134:   }",
          "",
          "---------------"
        ],
        "tensorflow/core/framework/collective.h||tensorflow/core/framework/collective.h": [
          "File: tensorflow/core/framework/collective.h -> tensorflow/core/framework/collective.h",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: #include <string>",
          "19: #include <vector>",
          "21: #include \"tensorflow/core/framework/device_attributes.pb.h\"",
          "22: #include \"tensorflow/core/framework/device_base.h\"",
          "23: #include \"tensorflow/core/framework/op_kernel.h\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "21: #include \"absl/container/flat_hash_set.h\"",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "151:   OpKernel* merge_op = nullptr;  // reduction only",
          "152:   OpKernel* final_op = nullptr;  // reduction only",
          "153:   string ToString() const;",
          "154: };",
          "156: class CollectiveExecutor;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "155:   bool run_group_initialization = true;",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "321:         \"a CollectiveExecutor has not been provided.\"));",
          "322:   }",
          "325:   virtual void RunClosure(std::function<void()> closure) = 0;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "326:   virtual void CompleteGroupAsync(const DeviceAttributes& device,",
          "327:                                   CollGroupParams* group_params,",
          "328:                                   CancellationManager* cancel_mgr,",
          "329:                                   StatusCallback done) {",
          "330:     return cem_->GetParamResolver()->CompleteGroupAsync(device, group_params,",
          "331:                                                         cancel_mgr, done);",
          "332:   }",
          "",
          "---------------"
        ],
        "tensorflow/core/grappler/optimizers/function_optimizer.cc||tensorflow/core/grappler/optimizers/function_optimizer.cc": [
          "File: tensorflow/core/grappler/optimizers/function_optimizer.cc -> tensorflow/core/grappler/optimizers/function_optimizer.cc",
          "--- Hunk 1 ---",
          "[Context before]",
          "834:        \"CollectiveGather\", \"CollectiveGatherV2\", \"CollectiveReduce\",",
          "835:        \"CollectiveReduceV2\", \"CollectiveBcastSend\", \"CollectiveBcastRecv\",",
          "836:        \"CollectiveBcastSendV2\", \"CollectiveBcastRecvV2\", \"NcclAllReduce\",",
          "",
          "[Removed Lines]",
          "837:        \"Send\", \"Recv\",",
          "",
          "[Added Lines]",
          "837:        \"Send\", \"Recv\", \"CollectiveInitializeCommunicator\",",
          "",
          "---------------"
        ],
        "tensorflow/core/kernels/collective_ops.cc||tensorflow/core/kernels/collective_ops.cc": [
          "File: tensorflow/core/kernels/collective_ops.cc -> tensorflow/core/kernels/collective_ops.cc",
          "--- Hunk 1 ---",
          "[Context before]",
          "12: See the License for the specific language governing permissions and",
          "13: limitations under the License.",
          "15: #include \"tensorflow/core/framework/attr_value.pb.h\"",
          "16: #include \"tensorflow/core/framework/collective.h\"",
          "17: #include \"tensorflow/core/framework/node_def.pb.h\"",
          "18: #include \"tensorflow/core/framework/op_kernel.h\"",
          "19: #include \"tensorflow/core/framework/tensor_util.h\"",
          "20: #include \"tensorflow/core/framework/types.h\"",
          "21: #include \"tensorflow/core/framework/types.pb.h\"",
          "22: #include \"tensorflow/core/lib/core/errors.h\"",
          "24: namespace tensorflow {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "15: #include <string>",
          "16: #include <utility>",
          "18: #include \"absl/strings/str_cat.h\"",
          "19: #include \"absl/strings/str_format.h\"",
          "22: #include \"tensorflow/core/framework/device_attributes.pb.h\"",
          "25: #include \"tensorflow/core/framework/op_requires.h\"",
          "26: #include \"tensorflow/core/framework/resource_handle.h\"",
          "27: #include \"tensorflow/core/framework/resource_mgr.h\"",
          "32: #include \"tensorflow/core/platform/errors.h\"",
          "33: #include \"tensorflow/core/platform/refcount.h\"",
          "34: #include \"tensorflow/core/platform/status.h\"",
          "35: #include \"tensorflow/core/platform/types.h\"",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "789:                             .HostMemory(\"shape\"),",
          "790:                         CollectiveBcastRecvV2OpKernel);",
          "792: }  // namespace",
          "793: }  // namespace tensorflow",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "810: class CollectiveGroupResource : public ResourceBase {",
          "811:  public:",
          "812:   CollectiveGroupResource(int32 group_key, int32 rank, int32 group_size,",
          "813:                           string communication_hint, float timeout_seconds)",
          "814:       : group_key_(group_key),",
          "815:         rank_(rank),",
          "816:         group_size_(group_size),",
          "817:         communication_hint_(communication_hint),",
          "818:         timeout_seconds_(timeout_seconds) {}",
          "820:   std::string DebugString() const override {",
          "821:     return absl::StrFormat(",
          "822:         \"Collective Group with group_key = %d, group_size = %d, rank = %d\",",
          "823:         group_key_, group_size_, rank_);",
          "824:   }",
          "826:   int get_next_instance_key() {",
          "827:     return instance_key_.fetch_add(1, std::memory_order_relaxed);",
          "828:   }",
          "830:   int32 group_key() const { return group_key_; }",
          "832:   int32 rank() const { return rank_; }",
          "834:   int32 group_size() const { return group_size_; }",
          "836:   string communication_hint() const { return communication_hint_; }",
          "838:   float timeout_seconds() const { return timeout_seconds_; }",
          "840:  private:",
          "841:   int32 group_key_, rank_, group_size_;",
          "842:   string communication_hint_;",
          "843:   std::atomic<int> instance_key_{0};",
          "844:   float timeout_seconds_ = 0;",
          "845: };",
          "847: class CollectiveInitializeCommunicatorOpKernel : public AsyncOpKernel {",
          "848:  public:",
          "849:   explicit CollectiveInitializeCommunicatorOpKernel(OpKernelConstruction* c)",
          "850:       : AsyncOpKernel(c), device_type_(DEVICE_DEFAULT) {",
          "851:     OP_REQUIRES_OK(c, c->GetAttr(\"communication_hint\", &communication_hint_));",
          "852:     OP_REQUIRES_OK(c, c->GetAttr(\"timeout_seconds\", &timeout_seconds_));",
          "853:     device_type_ = c->device_type();",
          "854:   }",
          "856:   Status CheckInputs(Tensor group_size_t, Tensor group_key_t) {",
          "857:     if (group_size_t.dims() > 0) {",
          "858:       return errors::Internal(",
          "859:           \"Unexpected dimensions on input group_size. \"",
          "860:           \"It shoulbe a scalar, got tensor with shape \",",
          "861:           group_size_t.shape().DebugString());",
          "862:     }",
          "863:     if (group_key_t.dims() > 0) {",
          "864:       return errors::Internal(\"Unexpected dimensions on input group_key, got \",",
          "865:                               group_key_t.shape().DebugString());",
          "866:     }",
          "868:     auto group_size = group_size_t.unaligned_flat<int32>()(0);",
          "869:     if (group_size <= 0) {",
          "870:       return errors::InvalidArgument(",
          "871:           \"group_size must be positive integer but got \", group_size);",
          "872:     }",
          "873:     return Status::OK();",
          "874:   }",
          "876:   void ComputeAsync(OpKernelContext* c, DoneCallback done) override {",
          "877:     auto group_key_t = c->input(0);",
          "878:     auto rank_t = c->input(1);",
          "879:     auto group_size_t = c->input(2);",
          "881:     OP_REQUIRES_OK_ASYNC(c, CheckInputs(group_size_t, group_key_t), done);",
          "883:     auto group_size = group_size_t.unaligned_flat<int32>()(0);",
          "884:     auto group_key = group_key_t.unaligned_flat<int32>()(0);",
          "885:     auto rank = rank_t.unaligned_flat<int32>()(0);",
          "887:     ResourceHandle resource_handle =",
          "888:         MakeResourceHandle<CollectiveGroupResource>(",
          "889:             c, \"collective_op_group\", absl::StrFormat(\"%d\", group_key));",
          "891:     Tensor* output_handle = nullptr;",
          "892:     OP_REQUIRES_OK_ASYNC(",
          "893:         c, c->allocate_output(0, TensorShape({}), &output_handle), done);",
          "894:     output_handle->scalar<ResourceHandle>()() = resource_handle;",
          "896:     CollectiveGroupResource* resource = new CollectiveGroupResource(",
          "897:         group_key, rank, group_size, this->communication_hint_,",
          "898:         this->timeout_seconds_);",
          "899:     OP_REQUIRES_OK_ASYNC(",
          "900:         c,",
          "901:         CreateResource<CollectiveGroupResource>(c, resource_handle, resource),",
          "902:         done);",
          "903:     auto group_params = new CollGroupParams();",
          "904:     group_params->device_type = device_type_;",
          "905:     group_params->group_size = resource->group_size();",
          "906:     group_params->group_key = resource->group_key();",
          "908:     auto* col_exec = c->collective_executor();",
          "910:     c->collective_executor()->RunClosure([c, done = std::move(done),",
          "911:                                           group_params, col_exec]() {",
          "912:       VLOG(1) << \"Collective Group initialization for \"",
          "913:               << \" device \" << c->device()->name() << \" group \"",
          "914:               << group_params->group_key;",
          "915:       col_exec->CompleteGroupAsync(",
          "916:           c->device()->attributes(), group_params, c->cancellation_manager(),",
          "917:           [c, done = std::move(done), group_params](const Status& s) {",
          "918:             if (s.ok()) {",
          "919:               VLOG(1) << \"Collective Group initialization done for device \"",
          "920:                       << c->device()->name() << \" group \"",
          "921:                       << group_params->group_key << \" status \" << s;",
          "922:             } else {",
          "923:               c->SetStatus(s);",
          "924:             }",
          "925:             delete group_params;",
          "926:             done();",
          "927:           });",
          "928:     });",
          "929:   }",
          "931:  private:",
          "932:   string communication_hint_;",
          "933:   DeviceType device_type_;",
          "934:   float timeout_seconds_ = 0;",
          "935: };",
          "937: REGISTER_KERNEL_BUILDER(",
          "938:     Name(\"CollectiveInitializeCommunicator\").Device(DEVICE_CPU),",
          "939:     CollectiveInitializeCommunicatorOpKernel);",
          "940: REGISTER_KERNEL_BUILDER(Name(\"CollectiveInitializeCommunicator\")",
          "941:                             .Device(DEVICE_GPU)",
          "942:                             .HostMemory(\"group_size\")",
          "943:                             .HostMemory(\"group_key\")",
          "944:                             .HostMemory(\"rank\"),",
          "945:                         CollectiveInitializeCommunicatorOpKernel);",
          "947: class CollectiveOpV3Kernel : public AsyncOpKernel {",
          "948:  public:",
          "949:   explicit CollectiveOpV3Kernel(OpKernelConstruction* c)",
          "950:       : AsyncOpKernel(c), name_(name()), device_type_(DEVICE_DEFAULT) {",
          "951:     OP_REQUIRES_OK(c, c->GetAttr(\"T\", &data_type_));",
          "952:     if (c->HasAttr(\"timeout_seconds\")) {",
          "953:       OP_REQUIRES_OK(c, c->GetAttr(\"timeout_seconds\", &timeout_seconds_));",
          "954:     } else {",
          "955:       timeout_seconds_ = -1;",
          "956:     }",
          "957:     device_type_ = c->device_type();",
          "958:   }",
          "960:  protected:",
          "964:   Status FillCollectiveParams(CollectiveParams* col_params,",
          "965:                               const Tensor& group_assignment,",
          "966:                               CollectiveType collective_type,",
          "967:                               CollectiveGroupResource* resource) {",
          "968:     int64 group_id;",
          "969:     int64 group_size;",
          "970:     if (group_assignment.NumElements() == 0) {",
          "972:       group_id = 0;",
          "973:       group_size = resource->group_size();",
          "974:     } else {",
          "975:       return errors::Unimplemented(\"Group assignments are not supported yet.\");",
          "976:     }",
          "980:     int32 instance_key = group_id << 21 | resource->get_next_instance_key();",
          "981:     col_params->name = name_;",
          "982:     col_params->group.device_type = device_type_;",
          "983:     col_params->group.group_size = group_size;",
          "984:     col_params->group.group_key = resource->group_key();",
          "985:     col_params->instance.type = collective_type;",
          "986:     col_params->instance.instance_key = instance_key;",
          "987:     col_params->instance.data_type = data_type_;",
          "988:     col_params->instance.impl_details.communication_hint =",
          "989:         resource->communication_hint();",
          "990:     col_params->instance.impl_details.timeout_seconds =",
          "991:         timeout_seconds_ > 0 ? resource->timeout_seconds() : timeout_seconds_;",
          "992:     col_params->run_group_initialization = false;",
          "993:     return Status::OK();",
          "994:   }",
          "998:   void Run(OpKernelContext* c, CollectiveParams* col_params,",
          "999:            DoneCallback done) {",
          "1000:     CollectiveExecutor* col_exec = c->collective_executor();",
          "1001:     OP_REQUIRES_ASYNC(",
          "1002:         c, col_exec,",
          "1003:         errors::Internal(",
          "1004:             \"Failed to get CollectiveExecutor from OpKernelContext for Op \",",
          "1005:             name_),",
          "1006:         done);",
          "1010:     col_exec->RunClosure([c, done = std::move(done), col_params, col_exec]() {",
          "1011:       VLOG(1) << \"Collective CompleteParams for \" << col_params->name",
          "1012:               << \" device \" << c->device()->name() << \" group \"",
          "1013:               << col_params->group.group_key << \" instance \"",
          "1014:               << col_params->instance.instance_key;",
          "1015:       col_exec->CompleteParamsAsync(",
          "1016:           c->device()->attributes(), col_params, c->cancellation_manager(),",
          "1017:           [c, done = std::move(done), col_params, col_exec](const Status& s) {",
          "1018:             if (s.ok()) {",
          "1019:               auto actual_done = [c, col_params,",
          "1020:                                   done = std::move(done)](const Status& s) {",
          "1021:                 VLOG(1) << \"Collective ExecuteAsync done for \"",
          "1022:                         << col_params->name << \" device \" << c->device()->name()",
          "1023:                         << \" group \" << col_params->group.group_key",
          "1024:                         << \" instance \" << col_params->instance.instance_key",
          "1025:                         << \" status \" << s;",
          "1026:                 if (!s.ok()) {",
          "1027:                   c->SetStatus(s);",
          "1028:                 }",
          "1029:                 done();",
          "1030:               };",
          "1031:               VLOG(1) << \"Collective ExecuteAsync start for \"",
          "1032:                       << col_params->name << \" device \" << c->device()->name()",
          "1033:                       << \" group \" << col_params->group.group_key",
          "1034:                       << \" instance \" << col_params->instance.instance_key;",
          "1035:               col_exec->ExecuteAsync(",
          "1036:                   c, col_params,",
          "1037:                   CollectiveKey(c, col_params->group.group_key,",
          "1038:                                 col_params->instance.instance_key),",
          "1039:                   actual_done);",
          "1040:             } else {",
          "1041:               c->SetStatus(s);",
          "1042:               done();",
          "1043:             }",
          "1044:           });",
          "1045:     });",
          "1046:   }",
          "1048:  protected:",
          "1049:   string name_;",
          "1050:   DataType data_type_ = DT_INVALID;",
          "1051:   DeviceType device_type_;",
          "1052:   float timeout_seconds_ = 0;",
          "1053: };",
          "1055: class CollectiveReduceV3OpKernel : public CollectiveOpV3Kernel {",
          "1056:  public:",
          "1057:   explicit CollectiveReduceV3OpKernel(OpKernelConstruction* c)",
          "1058:       : CollectiveOpV3Kernel(c) {",
          "1059:     string reduction;",
          "1060:     OP_REQUIRES_OK(c, c->GetAttr(\"reduction\", &reduction));",
          "1061:     if (reduction == \"Max\") {",
          "1062:       reduction = \"Maximum\";",
          "1063:     } else if (reduction == \"Min\") {",
          "1064:       reduction = \"Minimum\";",
          "1065:     }",
          "1068:     NodeDef sub_node;",
          "1069:     sub_node.add_input(c->def().input(0));",
          "1070:     sub_node.add_input(c->def().input(0));",
          "1071:     sub_node.set_device(c->def().device());",
          "1072:     SetAttrValue(data_type_, &(*sub_node.mutable_attr())[\"T\"]);",
          "1073:     merge_op_ = BuildOpKernel(c, reduction, &sub_node);",
          "1074:     final_op_ = BuildOpKernel(c, \"Id\", &sub_node);",
          "1075:     name_ = strings::StrCat(c->def().name(), \": ReduceV3(\", reduction, \")\");",
          "1076:     VLOG(2) << \"CollectiveReduceV3 \" << this << \" name \" << name_;",
          "1077:   }",
          "1079:   void ComputeAsync(OpKernelContext* c, DoneCallback done) override {",
          "1080:     auto col_params = new CollectiveParams();",
          "1081:     auto done_with_cleanup = [col_params, done = std::move(done)]() {",
          "1082:       done();",
          "1083:       col_params->Unref();",
          "1084:     };",
          "1085:     core::RefCountPtr<CollectiveGroupResource> resource;",
          "1086:     OP_REQUIRES_OK_ASYNC(c, LookupResource(c, HandleFromInput(c, 1), &resource),",
          "1087:                          done);",
          "1089:     Tensor group_assignment = c->input(2);",
          "1091:     OP_REQUIRES_OK_ASYNC(",
          "1092:         c,",
          "1093:         FillCollectiveParams(col_params, group_assignment, REDUCTION_COLLECTIVE,",
          "1094:                              resource.get()),",
          "1095:         done);",
          "1096:     col_params->instance.shape = c->input(0).shape();",
          "1097:     col_params->merge_op = merge_op_.get();",
          "1098:     col_params->final_op = final_op_.get();",
          "1099:     VLOG(1) << \"CollectiveReduceV3 group_size \" << col_params->group.group_size",
          "1100:             << \" group_key \" << col_params->group.group_key << \" instance_key \"",
          "1101:             << col_params->instance.instance_key;",
          "1103:     Tensor* output = nullptr;",
          "1104:     OP_REQUIRES_OK_ASYNC(c,",
          "1105:                          c->forward_input_or_allocate_output(",
          "1106:                              {0}, 0, col_params->instance.shape, &output),",
          "1107:                          done_with_cleanup);",
          "1108:     Run(c, col_params, std::move(done_with_cleanup));",
          "1109:   }",
          "1111:  private:",
          "1112:   std::unique_ptr<OpKernel> merge_op_;",
          "1113:   std::unique_ptr<OpKernel> final_op_;",
          "1114: };",
          "1116: REGISTER_KERNEL_BUILDER(Name(\"CollectiveReduceV3\").Device(DEVICE_CPU),",
          "1117:                         CollectiveReduceV3OpKernel);",
          "1118: REGISTER_KERNEL_BUILDER(Name(\"CollectiveReduceV3\").Device(DEVICE_GPU),",
          "1119:                         CollectiveReduceV3OpKernel);",
          "1121: class CollectiveAllToAllV3OpKernel : public CollectiveOpV3Kernel {",
          "1122:  public:",
          "1123:   explicit CollectiveAllToAllV3OpKernel(OpKernelConstruction* c)",
          "1124:       : CollectiveOpV3Kernel(c) {",
          "1125:     name_ = strings::StrCat(c->def().name(), \": AllToAllV3\");",
          "1126:     VLOG(2) << \"CollectiveAllToAllV3 \" << this << \" name \" << name_;",
          "1127:   }",
          "1129:   void ComputeAsync(OpKernelContext* c, DoneCallback done) override {",
          "1130:     auto col_params = new CollectiveParams();",
          "1131:     auto done_with_cleanup = [col_params, done = std::move(done)]() {",
          "1132:       done();",
          "1133:       col_params->Unref();",
          "1134:     };",
          "1135:     core::RefCountPtr<CollectiveGroupResource> resource;",
          "1136:     OP_REQUIRES_OK_ASYNC(c, LookupResource(c, HandleFromInput(c, 1), &resource),",
          "1137:                          done);",
          "1139:     Tensor group_assignment = c->input(2);",
          "1141:     OP_REQUIRES_OK_ASYNC(",
          "1142:         c,",
          "1143:         FillCollectiveParams(col_params, group_assignment,",
          "1144:                              ALL_TO_ALL_COLLECTIVE, resource.get()),",
          "1145:         done);",
          "1146:     col_params->instance.shape = c->input(0).shape();",
          "1147:     VLOG(1) << \"CollectiveAllToAll group_size \" << col_params->group.group_size",
          "1148:             << \" group_key \" << col_params->group.group_key << \" instance_key \"",
          "1149:             << col_params->instance.instance_key;",
          "1151:     Tensor* output = nullptr;",
          "1152:     OP_REQUIRES_OK_ASYNC(c,",
          "1153:                          c->forward_input_or_allocate_output(",
          "1154:                              {0}, 0, col_params->instance.shape, &output),",
          "1155:                          done_with_cleanup);",
          "1156:     Run(c, col_params, std::move(done_with_cleanup));",
          "1157:   }",
          "1158: };",
          "1160: REGISTER_KERNEL_BUILDER(Name(\"CollectiveAllToAllV3\").Device(DEVICE_CPU),",
          "1161:                         CollectiveAllToAllV3OpKernel);",
          "1162: REGISTER_KERNEL_BUILDER(Name(\"CollectiveAllToAllV3\").Device(DEVICE_GPU),",
          "1163:                         CollectiveAllToAllV3OpKernel);",
          "",
          "---------------"
        ],
        "tensorflow/core/ops/collective_ops.cc||tensorflow/core/ops/collective_ops.cc": [
          "File: tensorflow/core/ops/collective_ops.cc -> tensorflow/core/ops/collective_ops.cc",
          "--- Hunk 1 ---",
          "[Context before]",
          "185:       return Status::OK();",
          "186:     });",
          "188: }  // namespace tensorflow",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "188: REGISTER_OP(\"CollectiveInitializeCommunicator\")",
          "189:     .Input(\"group_key: int32\")",
          "190:     .Input(\"rank: int32\")",
          "191:     .Input(\"group_size: int32\")",
          "192:     .Attr(\"communication_hint: string = 'auto'\")",
          "193:     .Attr(\"timeout_seconds: float = 0\")",
          "194:     .Output(\"communicator: resource\")",
          "195:     .SetIsStateful()",
          "196:     .SetIsDistributedCommunication()",
          "197:     .SetShapeFn(shape_inference::ScalarShape);",
          "199: REGISTER_OP(\"CollectiveReduceV3\")",
          "200:     .Input(\"input: T\")",
          "201:     .Input(\"communicator: resource\")",
          "202:     .Input(\"group_assignment: int32\")",
          "203:     .Output(\"data: T\")",
          "204:     .Attr(\"T: {bfloat16, float, float16, float64, int32, int64}\")",
          "205:     .Attr(\"reduction: {'Min', 'Max', 'Mul', 'Add'}\")",
          "206:     .Attr(\"timeout_seconds: float = 0\")",
          "207:     .SetIsStateful()",
          "208:     .SetIsDistributedCommunication()",
          "209:     .SetShapeFn(shape_inference::UnchangedShape);",
          "211: REGISTER_OP(\"CollectiveAllToAllV3\")",
          "212:     .Input(\"input: T\")",
          "213:     .Input(\"communicator: resource\")",
          "214:     .Input(\"group_assignment: int32\")",
          "215:     .Output(\"data: T\")",
          "216:     .Attr(\"T: {bfloat16, float, float16, float64, int32, int64}\")",
          "217:     .Attr(\"timeout_seconds: float = 0\")",
          "218:     .SetIsStateful()",
          "219:     .SetIsDistributedCommunication()",
          "220:     .SetShapeFn(shape_inference::UnchangedShape);",
          "",
          "---------------"
        ],
        "tensorflow/python/framework/auto_control_deps.py||tensorflow/python/framework/auto_control_deps.py": [
          "File: tensorflow/python/framework/auto_control_deps.py -> tensorflow/python/framework/auto_control_deps.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "53:     # We do not add \"Send\" here since we want it to be added as a control output",
          "54:     # in order to avoid being pruned.",
          "55:     \"Recv\",",
          "56: ]",
          "58: LEGACY_RANDOM_OPS = [",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "56:     \"CollectiveInitializeCommunicator\",",
          "",
          "---------------"
        ],
        "tensorflow/python/kernel_tests/collective_ops_test.py||tensorflow/python/kernel_tests/collective_ops_test.py": [
          "File: tensorflow/python/kernel_tests/collective_ops_test.py -> tensorflow/python/kernel_tests/collective_ops_test.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1186:     self.assertAllEqual(self.evaluate(f()), [[3.], [3.]])",
          "1189: def _setup_context():",
          "1190:   context._reset_context()",
          "1191:   test_util.set_logical_devices_to_at_least('CPU', 4)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1189: class CollectiveOpsV3Test(test.TestCase, parameterized.TestCase):",
          "1191:   def setUp(self):",
          "1192:     super().setUp()",
          "1193:     _setup_context()",
          "1195:   def testGroupInitialization(self):",
          "1196:     group_size = 2",
          "1197:     group_key = 100",
          "1199:     @def_function.function",
          "1200:     def f():",
          "1201:       with ops.device('CPU:0'):",
          "1202:         _collective_ops.initialize_communicator(",
          "1203:             group_key=group_key, rank=0, group_size=group_size)",
          "1204:       with ops.device('CPU:1'):",
          "1205:         _collective_ops.initialize_communicator(",
          "1206:             group_key=group_key, rank=1, group_size=group_size)",
          "1208:       # TODO(b/193864859): Add validation with reduction op.",
          "1210:     self.evaluate(f())",
          "1212:   @combinations.generate(device_combination)",
          "1213:   def testAllReduceV3(self, device, communication):",
          "1214:     group_size = 2",
          "1215:     group_key = 101",
          "1217:     dev0 = '/device:%s:0' % device",
          "1218:     dev1 = '/device:%s:1' % device",
          "1220:     @def_function.function",
          "1221:     def run_all_reduce_2devices():",
          "1222:       collectives = []",
          "1223:       with ops.device(dev0):",
          "1224:         group_handle0 = _collective_ops.initialize_communicator(",
          "1225:             group_key=group_key,",
          "1226:             rank=0,",
          "1227:             group_size=group_size,",
          "1228:             communication_hint=communication)",
          "1229:         collectives.append(",
          "1230:             _collective_ops.all_reduce_v3(",
          "1231:                 group_handle0, [1.0], reduction='Add'))",
          "1232:       with ops.device(dev1):",
          "1233:         group_handle1 = _collective_ops.initialize_communicator(",
          "1234:             group_key=group_key,",
          "1235:             rank=1,",
          "1236:             group_size=group_size,",
          "1237:             communication_hint=communication)",
          "1238:         collectives.append(",
          "1239:             _collective_ops.all_reduce_v3(",
          "1240:                 group_handle1, [2.0], reduction='Add'))",
          "1241:       return collectives",
          "1243:     for result in run_all_reduce_2devices():",
          "1244:       self.assertAllClose(result, [3.], rtol=1e-5, atol=1e-5)",
          "1246:   @combinations.generate(device_combination)",
          "1247:   def testAllToAllV3(self, device, communication):",
          "1248:     group_size = 2",
          "1249:     group_key = 104",
          "1251:     dev0 = '/device:%s:0' % device",
          "1252:     dev1 = '/device:%s:1' % device",
          "1254:     @def_function.function",
          "1255:     def run_all_to_all_2devices():",
          "1256:       collectives = []",
          "1257:       with ops.device(dev0):",
          "1258:         group_handle0 = _collective_ops.initialize_communicator(",
          "1259:             group_key=group_key,",
          "1260:             rank=0,",
          "1261:             group_size=group_size,",
          "1262:             communication_hint=communication)",
          "1263:         collectives.append(",
          "1264:             _collective_ops.all_to_all_v3(group_handle0, [1.0, 3.0]))",
          "1265:       with ops.device(dev1):",
          "1266:         group_handle1 = _collective_ops.initialize_communicator(",
          "1267:             group_key=group_key,",
          "1268:             rank=1,",
          "1269:             group_size=group_size,",
          "1270:             communication_hint=communication)",
          "1271:         collectives.append(",
          "1272:             _collective_ops.all_to_all_v3(group_handle1, [2.0, 4.0]))",
          "1273:       return collectives",
          "1275:     result = run_all_to_all_2devices()",
          "1276:     self.assertAllClose(result[0], [1.0, 2.0], rtol=1e-5, atol=1e-5)",
          "1277:     self.assertAllClose(result[1], [3.0, 4.0], rtol=1e-5, atol=1e-5)",
          "",
          "---------------"
        ],
        "tensorflow/python/ops/collective_ops.py||tensorflow/python/ops/collective_ops.py": [
          "File: tensorflow/python/ops/collective_ops.py -> tensorflow/python/ops/collective_ops.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "386:       shape=shape,",
          "387:       communication_hint=communication_hint.lower(),",
          "388:       timeout_seconds=timeout)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "391: def initialize_communicator(group_key,",
          "392:                             rank,",
          "393:                             group_size,",
          "394:                             communication_hint='auto',",
          "395:                             timeout_seconds=0):",
          "396:   \"\"\"Initializes a collective communicator.",
          "398:   This creates a collective communicator, which represents membership to a",
          "399:   collective group. It should be called once per member of the group, and each",
          "400:   member needs to be on a different device. It blocks until all members of the",
          "401:   group run this op.",
          "403:   Communicators of a group can only be initialized once. Trying to initialize",
          "404:   communicators of an existing group will result in an error.",
          "406:   Args:",
          "407:     group_key: an int32 `tf.Tensor` identifying the group.",
          "408:     rank: an `tf.Tensor` specifying the rank of this device in the group. If",
          "409:       specified, the rank is required to be unique in the group.",
          "410:     group_size: an int32 `tf.Tensor`. The size of the group.",
          "411:     communication_hint: preferred collective communication.  The implementation",
          "412:       may fall back to another mechanism.  Options include `auto`, `ring`, and",
          "413:       `nccl`.",
          "414:     timeout_seconds: If set to a non zero, set a completion timeout to detect",
          "415:       staleness. If the timer goes off, a DeadlineExceededError is raised. The",
          "416:       timeout value in seconds. This feature is experimental.",
          "419:   Returns:",
          "420:     A resource `tf.Tensor`.",
          "421:   \"\"\"",
          "422:   return gen_collective_ops.collective_initialize_communicator(",
          "423:       group_key=group_key,",
          "424:       rank=rank,",
          "425:       group_size=group_size,",
          "426:       communication_hint=communication_hint,",
          "427:       timeout_seconds=timeout_seconds)",
          "430: def all_reduce_v3(communicator,",
          "431:                   t,",
          "432:                   reduction='Add',",
          "433:                   group_assignment=None,",
          "434:                   timeout_seconds=None):",
          "435:   \"\"\"Reduces tensors mutually.",
          "437:   Args:",
          "438:     communicator: the resource `tf.Tensor` returned from",
          "439:       `initialize_communicator`.",
          "440:     t: the `tf.Tensor` to be reduced.",
          "441:     reduction: a string. The name of the operation to reduce the values.",
          "442:       Accpeted values are `\"min\"`, `\"max\"`, `\"mul\"`, `\"add\"`.",
          "443:     group_assignment: Optional int32 `tf.Tensor` with shape [num_groups,",
          "444:       num_ranks_per_group]. `group_assignment[i]` represents the ranks in the",
          "445:       `ith` subgroup.",
          "446:     timeout_seconds: If set to a non zero, set a completion timeout to detect",
          "447:       staleness. If the timer goes off, a DeadlineExceededError is raised. The",
          "448:       timeout value in seconds. This feature is experimental.",
          "450:   Returns:",
          "451:     The reduced `tf.Tensor`.",
          "452:   \"\"\"",
          "453:   if group_assignment is None:",
          "454:     group_assignment = []",
          "455:   return gen_collective_ops.collective_reduce_v3(",
          "456:       communicator=communicator,",
          "457:       input=t,",
          "458:       group_assignment=group_assignment,",
          "459:       reduction=reduction,",
          "460:       timeout_seconds=timeout_seconds)",
          "463: def all_to_all_v3(communicator, t, group_assignment=None, timeout_seconds=None):",
          "464:   \"\"\"Exchanges tensors mutually.",
          "466:   Args:",
          "467:     communicator: the resource `tf.Tensor` returned from",
          "468:       `initialize_communicator`.",
          "469:     t: a `tf.Tensor`. The first dimension should have the length as the size of",
          "470:       the group. `t[i]` is sent to `rank i` within the group.",
          "471:     group_assignment: Optional int32 `tf.Tensor` with shape [num_groups,",
          "472:       num_ranks_per_group]. `group_assignment[i]` represents the ranks in the",
          "473:       `ith` subgroup.",
          "474:     timeout_seconds: If set to a non zero, set a completion timeout to detect",
          "475:       staleness. If the timer goes off, a DeadlineExceededError is raised. The",
          "476:       timeout value in seconds. This feature is experimental.",
          "478:   Returns:",
          "479:     a `tf.Tensor`. `t[i]` is sent from `rank i` within the group.",
          "480:   \"\"\"",
          "481:   if group_assignment is None:",
          "482:     group_assignment = []",
          "483:   return gen_collective_ops.collective_all_to_all_v3(",
          "484:       communicator=communicator,",
          "485:       input=t,",
          "486:       group_assignment=group_assignment,",
          "487:       timeout_seconds=timeout_seconds)",
          "",
          "---------------"
        ],
        "tensorflow/tools/api/golden/v1/tensorflow.raw_ops.pbtxt||tensorflow/tools/api/golden/v1/tensorflow.raw_ops.pbtxt": [
          "File: tensorflow/tools/api/golden/v1/tensorflow.raw_ops.pbtxt -> tensorflow/tools/api/golden/v1/tensorflow.raw_ops.pbtxt",
          "--- Hunk 1 ---",
          "[Context before]",
          "760:     name: \"CloseSummaryWriter\"",
          "761:     argspec: \"args=[\\'writer\\', \\'name\\'], varargs=None, keywords=None, defaults=[\\'None\\'], \"",
          "762:   }",
          "763:   member_method {",
          "764:     name: \"CollectiveBcastRecv\"",
          "765:     argspec: \"args=[\\'T\\', \\'group_size\\', \\'group_key\\', \\'instance_key\\', \\'shape\\', \\'communication_hint\\', \\'timeout_seconds\\', \\'name\\'], varargs=None, keywords=None, defaults=[\\'auto\\', \\'0\\', \\'None\\'], \"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "763:   member_method {",
          "764:     name: \"CollectiveAllToAllV3\"",
          "765:     argspec: \"args=[\\'input\\', \\'communicator\\', \\'group_assignment\\', \\'timeout_seconds\\', \\'name\\'], varargs=None, keywords=None, defaults=[\\'0\\', \\'None\\'], \"",
          "766:   }",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "784:     name: \"CollectiveGatherV2\"",
          "785:     argspec: \"args=[\\'input\\', \\'group_size\\', \\'group_key\\', \\'instance_key\\', \\'ordering_token\\', \\'communication_hint\\', \\'timeout_seconds\\', \\'name\\'], varargs=None, keywords=None, defaults=[\\'auto\\', \\'0\\', \\'None\\'], \"",
          "786:   }",
          "787:   member_method {",
          "788:     name: \"CollectivePermute\"",
          "789:     argspec: \"args=[\\'input\\', \\'source_target_pairs\\', \\'name\\'], varargs=None, keywords=None, defaults=[\\'None\\'], \"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "791:   member_method {",
          "792:     name: \"CollectiveInitializeCommunicator\"",
          "793:     argspec: \"args=[\\'group_key\\', \\'rank\\', \\'group_size\\', \\'communication_hint\\', \\'timeout_seconds\\', \\'name\\'], varargs=None, keywords=None, defaults=[\\'auto\\', \\'0\\', \\'None\\'], \"",
          "794:   }",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "796:     name: \"CollectiveReduceV2\"",
          "797:     argspec: \"args=[\\'input\\', \\'group_size\\', \\'group_key\\', \\'instance_key\\', \\'ordering_token\\', \\'merge_op\\', \\'final_op\\', \\'communication_hint\\', \\'timeout_seconds\\', \\'max_subdivs_per_device\\', \\'name\\'], varargs=None, keywords=None, defaults=[\\'auto\\', \\'0\\', \\'-1\\', \\'None\\'], \"",
          "798:   }",
          "799:   member_method {",
          "800:     name: \"CombinedNonMaxSuppression\"",
          "801:     argspec: \"args=[\\'boxes\\', \\'scores\\', \\'max_output_size_per_class\\', \\'max_total_size\\', \\'iou_threshold\\', \\'score_threshold\\', \\'pad_per_class\\', \\'clip_boxes\\', \\'name\\'], varargs=None, keywords=None, defaults=[\\'False\\', \\'True\\', \\'None\\'], \"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "807:   member_method {",
          "808:     name: \"CollectiveReduceV3\"",
          "809:     argspec: \"args=[\\'input\\', \\'communicator\\', \\'group_assignment\\', \\'reduction\\', \\'timeout_seconds\\', \\'name\\'], varargs=None, keywords=None, defaults=[\\'0\\', \\'None\\'], \"",
          "810:   }",
          "",
          "---------------"
        ],
        "tensorflow/tools/api/golden/v2/tensorflow.raw_ops.pbtxt||tensorflow/tools/api/golden/v2/tensorflow.raw_ops.pbtxt": [
          "File: tensorflow/tools/api/golden/v2/tensorflow.raw_ops.pbtxt -> tensorflow/tools/api/golden/v2/tensorflow.raw_ops.pbtxt",
          "--- Hunk 1 ---",
          "[Context before]",
          "760:     name: \"CloseSummaryWriter\"",
          "761:     argspec: \"args=[\\'writer\\', \\'name\\'], varargs=None, keywords=None, defaults=[\\'None\\'], \"",
          "762:   }",
          "763:   member_method {",
          "764:     name: \"CollectiveBcastRecv\"",
          "765:     argspec: \"args=[\\'T\\', \\'group_size\\', \\'group_key\\', \\'instance_key\\', \\'shape\\', \\'communication_hint\\', \\'timeout_seconds\\', \\'name\\'], varargs=None, keywords=None, defaults=[\\'auto\\', \\'0\\', \\'None\\'], \"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "763:   member_method {",
          "764:     name: \"CollectiveAllToAllV3\"",
          "765:     argspec: \"args=[\\'input\\', \\'communicator\\', \\'group_assignment\\', \\'timeout_seconds\\', \\'name\\'], varargs=None, keywords=None, defaults=[\\'0\\', \\'None\\'], \"",
          "766:   }",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "784:     name: \"CollectiveGatherV2\"",
          "785:     argspec: \"args=[\\'input\\', \\'group_size\\', \\'group_key\\', \\'instance_key\\', \\'ordering_token\\', \\'communication_hint\\', \\'timeout_seconds\\', \\'name\\'], varargs=None, keywords=None, defaults=[\\'auto\\', \\'0\\', \\'None\\'], \"",
          "786:   }",
          "787:   member_method {",
          "788:     name: \"CollectivePermute\"",
          "789:     argspec: \"args=[\\'input\\', \\'source_target_pairs\\', \\'name\\'], varargs=None, keywords=None, defaults=[\\'None\\'], \"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "791:   member_method {",
          "792:     name: \"CollectiveInitializeCommunicator\"",
          "793:     argspec: \"args=[\\'group_key\\', \\'rank\\', \\'group_size\\', \\'communication_hint\\', \\'timeout_seconds\\', \\'name\\'], varargs=None, keywords=None, defaults=[\\'auto\\', \\'0\\', \\'None\\'], \"",
          "794:   }",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "796:     name: \"CollectiveReduceV2\"",
          "797:     argspec: \"args=[\\'input\\', \\'group_size\\', \\'group_key\\', \\'instance_key\\', \\'ordering_token\\', \\'merge_op\\', \\'final_op\\', \\'communication_hint\\', \\'timeout_seconds\\', \\'max_subdivs_per_device\\', \\'name\\'], varargs=None, keywords=None, defaults=[\\'auto\\', \\'0\\', \\'-1\\', \\'None\\'], \"",
          "798:   }",
          "799:   member_method {",
          "800:     name: \"CombinedNonMaxSuppression\"",
          "801:     argspec: \"args=[\\'boxes\\', \\'scores\\', \\'max_output_size_per_class\\', \\'max_total_size\\', \\'iou_threshold\\', \\'score_threshold\\', \\'pad_per_class\\', \\'clip_boxes\\', \\'name\\'], varargs=None, keywords=None, defaults=[\\'False\\', \\'True\\', \\'None\\'], \"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "807:   member_method {",
          "808:     name: \"CollectiveReduceV3\"",
          "809:     argspec: \"args=[\\'input\\', \\'communicator\\', \\'group_assignment\\', \\'reduction\\', \\'timeout_seconds\\', \\'name\\'], varargs=None, keywords=None, defaults=[\\'0\\', \\'None\\'], \"",
          "810:   }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "649b0e983c7d7a9a217201abc5a9e0245a5e65b5",
      "candidate_info": {
        "commit_hash": "649b0e983c7d7a9a217201abc5a9e0245a5e65b5",
        "repo": "tensorflow/tensorflow",
        "commit_url": "https://github.com/tensorflow/tensorflow/commit/649b0e983c7d7a9a217201abc5a9e0245a5e65b5",
        "files": [
          "tensorflow/core/kernels/collective_ops.cc"
        ],
        "message": "Fix undefined behavior in CollectiveReduceV2 and others\n\nWe should not call done after it's moved.\n\nPiperOrigin-RevId: 400838185\nChange-Id: Ifc979740054b8f8c6f4d50acc89472fe60c4fdb1",
        "before_after_code_files": [
          "tensorflow/core/kernels/collective_ops.cc||tensorflow/core/kernels/collective_ops.cc"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "diff_branch_same_aad": 1,
        "olp_code_files": {
          "patch": [
            "tensorflow/core/kernels/collective_ops.cc||tensorflow/core/kernels/collective_ops.cc"
          ],
          "candidate": [
            "tensorflow/core/kernels/collective_ops.cc||tensorflow/core/kernels/collective_ops.cc"
          ]
        }
      },
      "candidate_diff": {
        "tensorflow/core/kernels/collective_ops.cc||tensorflow/core/kernels/collective_ops.cc": [
          "File: tensorflow/core/kernels/collective_ops.cc -> tensorflow/core/kernels/collective_ops.cc",
          "--- Hunk 1 ---",
          "[Context before]",
          "482:                               const Tensor& group_size, const Tensor& group_key,",
          "483:                               const Tensor& instance_key) {",
          "484:     if (group_size.dims() > 0) {",
          "487:     }",
          "488:     if (group_key.dims() > 0) {",
          "491:     }",
          "492:     if (instance_key.dims() > 0) {",
          "494:           \"Unexpected dimensions on input instance_key, got \",",
          "495:           instance_key.shape().DebugString());",
          "496:     }",
          "",
          "[Removed Lines]",
          "485:       return errors::Internal(\"Unexpected dimensions on input group_size, got \",",
          "486:                               group_size.shape().DebugString());",
          "489:       return errors::Internal(\"Unexpected dimensions on input group_key, got \",",
          "490:                               group_key.shape().DebugString());",
          "493:       return errors::Internal(",
          "",
          "[Added Lines]",
          "485:       return errors::InvalidArgument(",
          "486:           \"Unexpected dimensions on input group_size, got \",",
          "487:           group_size.shape().DebugString());",
          "490:       return errors::InvalidArgument(",
          "491:           \"Unexpected dimensions on input group_key, got \",",
          "492:           group_key.shape().DebugString());",
          "495:       return errors::InvalidArgument(",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "617:     col_params->instance.shape = c->input(0).shape();",
          "618:     col_params->merge_op = merge_op_.get();",
          "619:     col_params->final_op = final_op_.get();",
          "",
          "[Removed Lines]",
          "616:                          done);",
          "",
          "[Added Lines]",
          "618:                          done_with_cleanup);",
          "",
          "---------------"
        ]
      }
    }
  ]
}