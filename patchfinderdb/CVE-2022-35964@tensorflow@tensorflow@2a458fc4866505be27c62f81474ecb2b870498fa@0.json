{
  "cve_id": "CVE-2022-35964",
  "cve_desc": "TensorFlow is an open source platform for machine learning. The implementation of `BlockLSTMGradV2` does not fully validate its inputs. This results in a a segfault that can be used to trigger a denial of service attack. We have patched the issue in GitHub commit 2a458fc4866505be27c62f81474ecb2b870498fa. The fix will be included in TensorFlow 2.10.0. We will also cherrypick this commit on TensorFlow 2.9.1, TensorFlow 2.8.1, and TensorFlow 2.7.2, as these are also affected and still in supported range. There are no known workarounds for this issue.",
  "repo": "tensorflow/tensorflow",
  "patch_hash": "2a458fc4866505be27c62f81474ecb2b870498fa",
  "patch_info": {
    "commit_hash": "2a458fc4866505be27c62f81474ecb2b870498fa",
    "repo": "tensorflow/tensorflow",
    "commit_url": "https://github.com/tensorflow/tensorflow/commit/2a458fc4866505be27c62f81474ecb2b870498fa",
    "files": [
      "tensorflow/core/kernels/rnn/lstm_ops.cc",
      "tensorflow/python/kernel_tests/nn_ops/rnn_cell_test.py"
    ],
    "message": "Add size input validation to BlockLSTMGradV2.\n\nInvalid sizes lead to a security vulnerability crash.\nThe added size checks are copied from the shape function assigned\nin `REGISTER_OP`.\n\nPiperOrigin-RevId: 462886105",
    "before_after_code_files": [
      "tensorflow/core/kernels/rnn/lstm_ops.cc||tensorflow/core/kernels/rnn/lstm_ops.cc",
      "tensorflow/python/kernel_tests/nn_ops/rnn_cell_test.py||tensorflow/python/kernel_tests/nn_ops/rnn_cell_test.py"
    ]
  },
  "patch_diff": {
    "tensorflow/core/kernels/rnn/lstm_ops.cc||tensorflow/core/kernels/rnn/lstm_ops.cc": [
      "File: tensorflow/core/kernels/rnn/lstm_ops.cc -> tensorflow/core/kernels/rnn/lstm_ops.cc",
      "--- Hunk 1 ---",
      "[Context before]",
      "1139:     const Tensor* x;",
      "1140:     OP_REQUIRES_OK(ctx, ctx->input(\"x\", &x));",
      "1142:     const int64_t timelen = x->dim_size(0);",
      "1143:     const int64_t batch_size = x->dim_size(1);",
      "1144:     const int64_t input_size = x->dim_size(2);",
      "1146:     const Tensor* cs_prev_tensor = nullptr;",
      "1147:     OP_REQUIRES_OK(ctx, ctx->input(\"cs_prev\", &cs_prev_tensor));",
      "1149:     const Tensor* h_prev_tensor = nullptr;",
      "1150:     OP_REQUIRES_OK(ctx, ctx->input(\"h_prev\", &h_prev_tensor));",
      "1152:     const Tensor* w_tensor = nullptr;",
      "1153:     OP_REQUIRES_OK(ctx, ctx->input(\"w\", &w_tensor));",
      "1154:     const int64_t cell_size = w_tensor->dim_size(1) / 4;",
      "1155:     OP_REQUIRES(ctx, input_size + cell_size == w_tensor->dim_size(0),",
      "1156:                 errors::InvalidArgument(",
      "",
      "[Removed Lines]",
      "1141:     OP_REQUIRES(ctx, x->dims() == 3, errors::InvalidArgument(\"x must be 3D\"));",
      "",
      "[Added Lines]",
      "1141:     OP_REQUIRES(",
      "1142:         ctx, x->dims() == 3,",
      "1143:         errors::InvalidArgument(\"x must be rank 3 but is rank \", x->dims()));",
      "1150:     OP_REQUIRES(ctx, cs_prev_tensor->dims() == 2,",
      "1151:                 errors::InvalidArgument(\"cs_prev must be rank 2 but is rank \",",
      "1152:                                         cs_prev_tensor->dims()));",
      "1156:     OP_REQUIRES(ctx, h_prev_tensor->dims() == 2,",
      "1157:                 errors::InvalidArgument(\"h_prev must be rank 2 but is rank \",",
      "1158:                                         h_prev_tensor->dims()));",
      "1162:     OP_REQUIRES(ctx, w_tensor->dims() == 2,",
      "1163:                 errors::InvalidArgument(\"w must be rank 2 but is rank \",",
      "1164:                                         w_tensor->dims()));",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "1160:     const Tensor* wci_tensor = nullptr;",
      "1161:     OP_REQUIRES_OK(ctx, ctx->input(\"wci\", &wci_tensor));",
      "1163:     const Tensor* wcf_tensor = nullptr;",
      "1164:     OP_REQUIRES_OK(ctx, ctx->input(\"wcf\", &wcf_tensor));",
      "1166:     const Tensor* wco_tensor = nullptr;",
      "1167:     OP_REQUIRES_OK(ctx, ctx->input(\"wco\", &wco_tensor));",
      "1169:     const Tensor* b_tensor = nullptr;",
      "1170:     OP_REQUIRES_OK(ctx, ctx->input(\"b\", &b_tensor));",
      "1171:     OP_REQUIRES(",
      "1172:         ctx, cell_size == b_tensor->dim_size(0) / 4,",
      "1173:         errors::InvalidArgument(\"w and b cell_size don't match: \", cell_size,",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "1173:     OP_REQUIRES(ctx, wci_tensor->dims() == 1,",
      "1174:                 errors::InvalidArgument(\"wci must be rank 1 but is rank \",",
      "1175:                                         wci_tensor->dims()));",
      "1179:     OP_REQUIRES(ctx, wcf_tensor->dims() == 1,",
      "1180:                 errors::InvalidArgument(\"wcf must be rank 1 but is rank \",",
      "1181:                                         wcf_tensor->dims()));",
      "1185:     OP_REQUIRES(ctx, wco_tensor->dims() == 1,",
      "1186:                 errors::InvalidArgument(\"wco must be rank 1 but is rank \",",
      "1187:                                         wco_tensor->dims()));",
      "1191:     OP_REQUIRES(ctx, b_tensor->dims() == 1,",
      "1192:                 errors::InvalidArgument(\"b must be rank 1 but is rank \",",
      "1193:                                         b_tensor->dims()));",
      "",
      "---------------"
    ],
    "tensorflow/python/kernel_tests/nn_ops/rnn_cell_test.py||tensorflow/python/kernel_tests/nn_ops/rnn_cell_test.py": [
      "File: tensorflow/python/kernel_tests/nn_ops/rnn_cell_test.py -> tensorflow/python/kernel_tests/nn_ops/rnn_cell_test.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "1354:               cell_clip=cell_clip,",
      "1355:               use_peephole=use_peephole))",
      "1358: class BidirectionalRNNTest(test.TestCase):",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "1357:   @test_util.run_in_graph_and_eager_modes",
      "1358:   def testLSTMBlockCellGradErrorHandling(self):",
      "1359:     use_peephole = False",
      "1360:     seq_len_max = constant_op.constant(1, shape=[], dtype=dtypes.int64)",
      "1361:     x = constant_op.constant(0.504355371, shape=[1, 1, 1], dtype=dtypes.float32)",
      "1362:     cs_prev = constant_op.constant(",
      "1363:         0.504355371, shape=[1, 1, 1], dtype=dtypes.float32)",
      "1364:     h_prev = constant_op.constant(",
      "1365:         0.504355371, shape=[1, 1], dtype=dtypes.float32)",
      "1366:     w = constant_op.constant(0.504355371, shape=[1, 1], dtype=dtypes.float32)",
      "1367:     wci = constant_op.constant(0.504355371, shape=[1], dtype=dtypes.float32)",
      "1368:     wcf = constant_op.constant(0.504355371, shape=[1], dtype=dtypes.float32)",
      "1369:     wco = constant_op.constant(0.504355371, shape=[1], dtype=dtypes.float32)",
      "1370:     b = constant_op.constant(0.504355371, shape=[1], dtype=dtypes.float32)",
      "1371:     i = constant_op.constant(0.504355371, shape=[1, 1, 1], dtype=dtypes.float32)",
      "1372:     cs = constant_op.constant(",
      "1373:         0.504355371, shape=[1, 1, 1], dtype=dtypes.float32)",
      "1374:     f = constant_op.constant(0.504355371, shape=[1, 1, 1], dtype=dtypes.float32)",
      "1375:     o = constant_op.constant(0.504355371, shape=[1, 1, 1], dtype=dtypes.float32)",
      "1376:     ci = constant_op.constant(",
      "1377:         0.504355371, shape=[1, 1, 1], dtype=dtypes.float32)",
      "1378:     co = constant_op.constant(",
      "1379:         0.504355371, shape=[1, 1, 1], dtype=dtypes.float32)",
      "1380:     h = constant_op.constant(0.504355371, shape=[1, 1, 1], dtype=dtypes.float32)",
      "1381:     cs_grad = constant_op.constant(",
      "1382:         0.504355371, shape=[1, 1, 1], dtype=dtypes.float32)",
      "1383:     h_grad = constant_op.constant(",
      "1384:         0.504355371, shape=[1, 1, 1], dtype=dtypes.float32)",
      "1385:     with self.assertRaisesRegex((ValueError, errors_impl.InvalidArgumentError),",
      "1386:                                 \"must be rank\"):",
      "1387:       self.evaluate(",
      "1388:           gen_rnn_ops.block_lstm_grad_v2(",
      "1389:               seq_len_max=seq_len_max,",
      "1390:               x=x,",
      "1391:               cs_prev=cs_prev,",
      "1392:               h_prev=h_prev,",
      "1393:               w=w,",
      "1394:               wci=wci,",
      "1395:               wcf=wcf,",
      "1396:               wco=wco,",
      "1397:               b=b,",
      "1398:               i=i,",
      "1399:               cs=cs,",
      "1400:               f=f,",
      "1401:               o=o,",
      "1402:               ci=ci,",
      "1403:               co=co,",
      "1404:               h=h,",
      "1405:               cs_grad=cs_grad,",
      "1406:               h_grad=h_grad,",
      "1407:               use_peephole=use_peephole))",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "8ab66d366b550dadb9476d433644294ab1f6dea0",
      "candidate_info": {
        "commit_hash": "8ab66d366b550dadb9476d433644294ab1f6dea0",
        "repo": "tensorflow/tensorflow",
        "commit_url": "https://github.com/tensorflow/tensorflow/commit/8ab66d366b550dadb9476d433644294ab1f6dea0",
        "files": [
          "tensorflow/core/kernels/rnn/lstm_ops.cc",
          "tensorflow/python/kernel_tests/nn_ops/rnn_cell_test.py"
        ],
        "message": "Add size input validation to BlockLSTMGradV2.\n\nInvalid sizes lead to a security vulnerability crash.\nThe added size checks are copied from the shape function assigned\nin `REGISTER_OP`.\n\nPiperOrigin-RevId: 462886105",
        "before_after_code_files": [
          "tensorflow/core/kernels/rnn/lstm_ops.cc||tensorflow/core/kernels/rnn/lstm_ops.cc",
          "tensorflow/python/kernel_tests/nn_ops/rnn_cell_test.py||tensorflow/python/kernel_tests/nn_ops/rnn_cell_test.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "diff_branch_same_aad": 1,
        "olp_code_files": {
          "patch": [
            "tensorflow/core/kernels/rnn/lstm_ops.cc||tensorflow/core/kernels/rnn/lstm_ops.cc",
            "tensorflow/python/kernel_tests/nn_ops/rnn_cell_test.py||tensorflow/python/kernel_tests/nn_ops/rnn_cell_test.py"
          ],
          "candidate": [
            "tensorflow/core/kernels/rnn/lstm_ops.cc||tensorflow/core/kernels/rnn/lstm_ops.cc",
            "tensorflow/python/kernel_tests/nn_ops/rnn_cell_test.py||tensorflow/python/kernel_tests/nn_ops/rnn_cell_test.py"
          ]
        }
      },
      "candidate_diff": {
        "tensorflow/core/kernels/rnn/lstm_ops.cc||tensorflow/core/kernels/rnn/lstm_ops.cc": [
          "File: tensorflow/core/kernels/rnn/lstm_ops.cc -> tensorflow/core/kernels/rnn/lstm_ops.cc",
          "--- Hunk 1 ---",
          "[Context before]",
          "1139:     const Tensor* x;",
          "1140:     OP_REQUIRES_OK(ctx, ctx->input(\"x\", &x));",
          "1142:     const int64_t timelen = x->dim_size(0);",
          "1143:     const int64_t batch_size = x->dim_size(1);",
          "1144:     const int64_t input_size = x->dim_size(2);",
          "1146:     const Tensor* cs_prev_tensor = nullptr;",
          "1147:     OP_REQUIRES_OK(ctx, ctx->input(\"cs_prev\", &cs_prev_tensor));",
          "1149:     const Tensor* h_prev_tensor = nullptr;",
          "1150:     OP_REQUIRES_OK(ctx, ctx->input(\"h_prev\", &h_prev_tensor));",
          "1152:     const Tensor* w_tensor = nullptr;",
          "1153:     OP_REQUIRES_OK(ctx, ctx->input(\"w\", &w_tensor));",
          "1154:     const int64_t cell_size = w_tensor->dim_size(1) / 4;",
          "1155:     OP_REQUIRES(ctx, input_size + cell_size == w_tensor->dim_size(0),",
          "1156:                 errors::InvalidArgument(",
          "",
          "[Removed Lines]",
          "1141:     OP_REQUIRES(ctx, x->dims() == 3, errors::InvalidArgument(\"x must be 3D\"));",
          "",
          "[Added Lines]",
          "1141:     OP_REQUIRES(",
          "1142:         ctx, x->dims() == 3,",
          "1143:         errors::InvalidArgument(\"x must be rank 3 but is rank \", x->dims()));",
          "1150:     OP_REQUIRES(ctx, cs_prev_tensor->dims() == 2,",
          "1151:                 errors::InvalidArgument(\"cs_prev must be rank 2 but is rank \",",
          "1152:                                         cs_prev_tensor->dims()));",
          "1156:     OP_REQUIRES(ctx, h_prev_tensor->dims() == 2,",
          "1157:                 errors::InvalidArgument(\"h_prev must be rank 2 but is rank \",",
          "1158:                                         h_prev_tensor->dims()));",
          "1162:     OP_REQUIRES(ctx, w_tensor->dims() == 2,",
          "1163:                 errors::InvalidArgument(\"w must be rank 2 but is rank \",",
          "1164:                                         w_tensor->dims()));",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1160:     const Tensor* wci_tensor = nullptr;",
          "1161:     OP_REQUIRES_OK(ctx, ctx->input(\"wci\", &wci_tensor));",
          "1163:     const Tensor* wcf_tensor = nullptr;",
          "1164:     OP_REQUIRES_OK(ctx, ctx->input(\"wcf\", &wcf_tensor));",
          "1166:     const Tensor* wco_tensor = nullptr;",
          "1167:     OP_REQUIRES_OK(ctx, ctx->input(\"wco\", &wco_tensor));",
          "1169:     const Tensor* b_tensor = nullptr;",
          "1170:     OP_REQUIRES_OK(ctx, ctx->input(\"b\", &b_tensor));",
          "1171:     OP_REQUIRES(",
          "1172:         ctx, cell_size == b_tensor->dim_size(0) / 4,",
          "1173:         errors::InvalidArgument(\"w and b cell_size don't match: \", cell_size,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1173:     OP_REQUIRES(ctx, wci_tensor->dims() == 1,",
          "1174:                 errors::InvalidArgument(\"wci must be rank 1 but is rank \",",
          "1175:                                         wci_tensor->dims()));",
          "1179:     OP_REQUIRES(ctx, wcf_tensor->dims() == 1,",
          "1180:                 errors::InvalidArgument(\"wcf must be rank 1 but is rank \",",
          "1181:                                         wcf_tensor->dims()));",
          "1185:     OP_REQUIRES(ctx, wco_tensor->dims() == 1,",
          "1186:                 errors::InvalidArgument(\"wco must be rank 1 but is rank \",",
          "1187:                                         wco_tensor->dims()));",
          "1191:     OP_REQUIRES(ctx, b_tensor->dims() == 1,",
          "1192:                 errors::InvalidArgument(\"b must be rank 1 but is rank \",",
          "1193:                                         b_tensor->dims()));",
          "",
          "---------------"
        ],
        "tensorflow/python/kernel_tests/nn_ops/rnn_cell_test.py||tensorflow/python/kernel_tests/nn_ops/rnn_cell_test.py": [
          "File: tensorflow/python/kernel_tests/nn_ops/rnn_cell_test.py -> tensorflow/python/kernel_tests/nn_ops/rnn_cell_test.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1354:               cell_clip=cell_clip,",
          "1355:               use_peephole=use_peephole))",
          "1358: class BidirectionalRNNTest(test.TestCase):",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1357:   @test_util.run_in_graph_and_eager_modes",
          "1358:   def testLSTMBlockCellGradErrorHandling(self):",
          "1359:     use_peephole = False",
          "1360:     seq_len_max = constant_op.constant(1, shape=[], dtype=dtypes.int64)",
          "1361:     x = constant_op.constant(0.504355371, shape=[1, 1, 1], dtype=dtypes.float32)",
          "1362:     cs_prev = constant_op.constant(",
          "1363:         0.504355371, shape=[1, 1, 1], dtype=dtypes.float32)",
          "1364:     h_prev = constant_op.constant(",
          "1365:         0.504355371, shape=[1, 1], dtype=dtypes.float32)",
          "1366:     w = constant_op.constant(0.504355371, shape=[1, 1], dtype=dtypes.float32)",
          "1367:     wci = constant_op.constant(0.504355371, shape=[1], dtype=dtypes.float32)",
          "1368:     wcf = constant_op.constant(0.504355371, shape=[1], dtype=dtypes.float32)",
          "1369:     wco = constant_op.constant(0.504355371, shape=[1], dtype=dtypes.float32)",
          "1370:     b = constant_op.constant(0.504355371, shape=[1], dtype=dtypes.float32)",
          "1371:     i = constant_op.constant(0.504355371, shape=[1, 1, 1], dtype=dtypes.float32)",
          "1372:     cs = constant_op.constant(",
          "1373:         0.504355371, shape=[1, 1, 1], dtype=dtypes.float32)",
          "1374:     f = constant_op.constant(0.504355371, shape=[1, 1, 1], dtype=dtypes.float32)",
          "1375:     o = constant_op.constant(0.504355371, shape=[1, 1, 1], dtype=dtypes.float32)",
          "1376:     ci = constant_op.constant(",
          "1377:         0.504355371, shape=[1, 1, 1], dtype=dtypes.float32)",
          "1378:     co = constant_op.constant(",
          "1379:         0.504355371, shape=[1, 1, 1], dtype=dtypes.float32)",
          "1380:     h = constant_op.constant(0.504355371, shape=[1, 1, 1], dtype=dtypes.float32)",
          "1381:     cs_grad = constant_op.constant(",
          "1382:         0.504355371, shape=[1, 1, 1], dtype=dtypes.float32)",
          "1383:     h_grad = constant_op.constant(",
          "1384:         0.504355371, shape=[1, 1, 1], dtype=dtypes.float32)",
          "1385:     with self.assertRaisesRegex((ValueError, errors_impl.InvalidArgumentError),",
          "1386:                                 \"must be rank\"):",
          "1387:       self.evaluate(",
          "1388:           gen_rnn_ops.block_lstm_grad_v2(",
          "1389:               seq_len_max=seq_len_max,",
          "1390:               x=x,",
          "1391:               cs_prev=cs_prev,",
          "1392:               h_prev=h_prev,",
          "1393:               w=w,",
          "1394:               wci=wci,",
          "1395:               wcf=wcf,",
          "1396:               wco=wco,",
          "1397:               b=b,",
          "1398:               i=i,",
          "1399:               cs=cs,",
          "1400:               f=f,",
          "1401:               o=o,",
          "1402:               ci=ci,",
          "1403:               co=co,",
          "1404:               h=h,",
          "1405:               cs_grad=cs_grad,",
          "1406:               h_grad=h_grad,",
          "1407:               use_peephole=use_peephole))",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "d06148684e9e5d0cea6a918a7179a167dd2e5547",
      "candidate_info": {
        "commit_hash": "d06148684e9e5d0cea6a918a7179a167dd2e5547",
        "repo": "tensorflow/tensorflow",
        "commit_url": "https://github.com/tensorflow/tensorflow/commit/d06148684e9e5d0cea6a918a7179a167dd2e5547",
        "files": [
          "tensorflow/core/kernels/rnn/lstm_ops.cc",
          "tensorflow/python/kernel_tests/rnn_cell_test.py"
        ],
        "message": "Add size input validation to BlockLSTMGradV2.\n\nInvalid sizes lead to a security vulnerability crash.\nThe added size checks are copied from the shape function assigned\nin `REGISTER_OP`.\n\nPiperOrigin-RevId: 462886105",
        "before_after_code_files": [
          "tensorflow/core/kernels/rnn/lstm_ops.cc||tensorflow/core/kernels/rnn/lstm_ops.cc",
          "tensorflow/python/kernel_tests/rnn_cell_test.py||tensorflow/python/kernel_tests/rnn_cell_test.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "diff_branch_same_aad": 1,
        "olp_code_files": {
          "patch": [
            "tensorflow/core/kernels/rnn/lstm_ops.cc||tensorflow/core/kernels/rnn/lstm_ops.cc"
          ],
          "candidate": [
            "tensorflow/core/kernels/rnn/lstm_ops.cc||tensorflow/core/kernels/rnn/lstm_ops.cc"
          ]
        }
      },
      "candidate_diff": {
        "tensorflow/core/kernels/rnn/lstm_ops.cc||tensorflow/core/kernels/rnn/lstm_ops.cc": [
          "File: tensorflow/core/kernels/rnn/lstm_ops.cc -> tensorflow/core/kernels/rnn/lstm_ops.cc",
          "--- Hunk 1 ---",
          "[Context before]",
          "1139:     const Tensor* x;",
          "1140:     OP_REQUIRES_OK(ctx, ctx->input(\"x\", &x));",
          "1142:     const int64_t timelen = x->dim_size(0);",
          "1143:     const int64_t batch_size = x->dim_size(1);",
          "1144:     const int64_t input_size = x->dim_size(2);",
          "1146:     const Tensor* cs_prev_tensor = nullptr;",
          "1147:     OP_REQUIRES_OK(ctx, ctx->input(\"cs_prev\", &cs_prev_tensor));",
          "1149:     const Tensor* h_prev_tensor = nullptr;",
          "1150:     OP_REQUIRES_OK(ctx, ctx->input(\"h_prev\", &h_prev_tensor));",
          "1152:     const Tensor* w_tensor = nullptr;",
          "1153:     OP_REQUIRES_OK(ctx, ctx->input(\"w\", &w_tensor));",
          "1154:     const int64_t cell_size = w_tensor->dim_size(1) / 4;",
          "1155:     OP_REQUIRES(ctx, input_size + cell_size == w_tensor->dim_size(0),",
          "1156:                 errors::InvalidArgument(",
          "",
          "[Removed Lines]",
          "1141:     OP_REQUIRES(ctx, x->dims() == 3, errors::InvalidArgument(\"x must be 3D\"));",
          "",
          "[Added Lines]",
          "1141:     OP_REQUIRES(",
          "1142:         ctx, x->dims() == 3,",
          "1143:         errors::InvalidArgument(\"x must be rank 3 but is rank \", x->dims()));",
          "1150:     OP_REQUIRES(ctx, cs_prev_tensor->dims() == 2,",
          "1151:                 errors::InvalidArgument(\"cs_prev must be rank 2 but is rank \",",
          "1152:                                         cs_prev_tensor->dims()));",
          "1156:     OP_REQUIRES(ctx, h_prev_tensor->dims() == 2,",
          "1157:                 errors::InvalidArgument(\"h_prev must be rank 2 but is rank \",",
          "1158:                                         h_prev_tensor->dims()));",
          "1162:     OP_REQUIRES(ctx, w_tensor->dims() == 2,",
          "1163:                 errors::InvalidArgument(\"w must be rank 2 but is rank \",",
          "1164:                                         w_tensor->dims()));",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1160:     const Tensor* wci_tensor = nullptr;",
          "1161:     OP_REQUIRES_OK(ctx, ctx->input(\"wci\", &wci_tensor));",
          "1163:     const Tensor* wcf_tensor = nullptr;",
          "1164:     OP_REQUIRES_OK(ctx, ctx->input(\"wcf\", &wcf_tensor));",
          "1166:     const Tensor* wco_tensor = nullptr;",
          "1167:     OP_REQUIRES_OK(ctx, ctx->input(\"wco\", &wco_tensor));",
          "1169:     const Tensor* b_tensor = nullptr;",
          "1170:     OP_REQUIRES_OK(ctx, ctx->input(\"b\", &b_tensor));",
          "1171:     OP_REQUIRES(",
          "1172:         ctx, cell_size == b_tensor->dim_size(0) / 4,",
          "1173:         errors::InvalidArgument(\"w and b cell_size don't match: \", cell_size,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1173:     OP_REQUIRES(ctx, wci_tensor->dims() == 1,",
          "1174:                 errors::InvalidArgument(\"wci must be rank 1 but is rank \",",
          "1175:                                         wci_tensor->dims()));",
          "1179:     OP_REQUIRES(ctx, wcf_tensor->dims() == 1,",
          "1180:                 errors::InvalidArgument(\"wcf must be rank 1 but is rank \",",
          "1181:                                         wcf_tensor->dims()));",
          "1185:     OP_REQUIRES(ctx, wco_tensor->dims() == 1,",
          "1186:                 errors::InvalidArgument(\"wco must be rank 1 but is rank \",",
          "1187:                                         wco_tensor->dims()));",
          "1191:     OP_REQUIRES(ctx, b_tensor->dims() == 1,",
          "1192:                 errors::InvalidArgument(\"b must be rank 1 but is rank \",",
          "1193:                                         b_tensor->dims()));",
          "",
          "---------------"
        ],
        "tensorflow/python/kernel_tests/rnn_cell_test.py||tensorflow/python/kernel_tests/rnn_cell_test.py": [
          "File: tensorflow/python/kernel_tests/rnn_cell_test.py -> tensorflow/python/kernel_tests/rnn_cell_test.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1359:               cell_clip=cell_clip,",
          "1360:               use_peephole=use_peephole))",
          "1363: class BidirectionalRNNTest(test.TestCase):",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1362:   @test_util.run_in_graph_and_eager_modes",
          "1363:   def testLSTMBlockCellGradErrorHandling(self):",
          "1364:     use_peephole = False",
          "1365:     seq_len_max = constant_op.constant(1, shape=[], dtype=dtypes.int64)",
          "1366:     x = constant_op.constant(0.504355371, shape=[1, 1, 1], dtype=dtypes.float32)",
          "1367:     cs_prev = constant_op.constant(",
          "1368:         0.504355371, shape=[1, 1, 1], dtype=dtypes.float32)",
          "1369:     h_prev = constant_op.constant(",
          "1370:         0.504355371, shape=[1, 1], dtype=dtypes.float32)",
          "1371:     w = constant_op.constant(0.504355371, shape=[1, 1], dtype=dtypes.float32)",
          "1372:     wci = constant_op.constant(0.504355371, shape=[1], dtype=dtypes.float32)",
          "1373:     wcf = constant_op.constant(0.504355371, shape=[1], dtype=dtypes.float32)",
          "1374:     wco = constant_op.constant(0.504355371, shape=[1], dtype=dtypes.float32)",
          "1375:     b = constant_op.constant(0.504355371, shape=[1], dtype=dtypes.float32)",
          "1376:     i = constant_op.constant(0.504355371, shape=[1, 1, 1], dtype=dtypes.float32)",
          "1377:     cs = constant_op.constant(",
          "1378:         0.504355371, shape=[1, 1, 1], dtype=dtypes.float32)",
          "1379:     f = constant_op.constant(0.504355371, shape=[1, 1, 1], dtype=dtypes.float32)",
          "1380:     o = constant_op.constant(0.504355371, shape=[1, 1, 1], dtype=dtypes.float32)",
          "1381:     ci = constant_op.constant(",
          "1382:         0.504355371, shape=[1, 1, 1], dtype=dtypes.float32)",
          "1383:     co = constant_op.constant(",
          "1384:         0.504355371, shape=[1, 1, 1], dtype=dtypes.float32)",
          "1385:     h = constant_op.constant(0.504355371, shape=[1, 1, 1], dtype=dtypes.float32)",
          "1386:     cs_grad = constant_op.constant(",
          "1387:         0.504355371, shape=[1, 1, 1], dtype=dtypes.float32)",
          "1388:     h_grad = constant_op.constant(",
          "1389:         0.504355371, shape=[1, 1, 1], dtype=dtypes.float32)",
          "1390:     with self.assertRaisesRegex((ValueError, errors_impl.InvalidArgumentError),",
          "1391:                                 \"must be rank\"):",
          "1392:       self.evaluate(",
          "1393:           gen_rnn_ops.block_lstm_grad_v2(",
          "1394:               seq_len_max=seq_len_max,",
          "1395:               x=x,",
          "1396:               cs_prev=cs_prev,",
          "1397:               h_prev=h_prev,",
          "1398:               w=w,",
          "1399:               wci=wci,",
          "1400:               wcf=wcf,",
          "1401:               wco=wco,",
          "1402:               b=b,",
          "1403:               i=i,",
          "1404:               cs=cs,",
          "1405:               f=f,",
          "1406:               o=o,",
          "1407:               ci=ci,",
          "1408:               co=co,",
          "1409:               h=h,",
          "1410:               cs_grad=cs_grad,",
          "1411:               h_grad=h_grad,",
          "1412:               use_peephole=use_peephole))",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "794f4c83ffa7bf4e97f12d5ccbf4fb025a574573",
      "candidate_info": {
        "commit_hash": "794f4c83ffa7bf4e97f12d5ccbf4fb025a574573",
        "repo": "tensorflow/tensorflow",
        "commit_url": "https://github.com/tensorflow/tensorflow/commit/794f4c83ffa7bf4e97f12d5ccbf4fb025a574573",
        "files": [
          "tensorflow/core/kernels/rnn/lstm_ops.cc",
          "tensorflow/python/kernel_tests/nn_ops/rnn_cell_test.py"
        ],
        "message": "Add size input validation to BlockLSTMGradV2.\n\nInvalid sizes lead to a security vulnerability crash.\nThe added size checks are copied from the shape function assigned\nin `REGISTER_OP`.\n\nPiperOrigin-RevId: 462886105",
        "before_after_code_files": [
          "tensorflow/core/kernels/rnn/lstm_ops.cc||tensorflow/core/kernels/rnn/lstm_ops.cc",
          "tensorflow/python/kernel_tests/nn_ops/rnn_cell_test.py||tensorflow/python/kernel_tests/nn_ops/rnn_cell_test.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "diff_branch_same_aad": 1,
        "olp_code_files": {
          "patch": [
            "tensorflow/core/kernels/rnn/lstm_ops.cc||tensorflow/core/kernels/rnn/lstm_ops.cc",
            "tensorflow/python/kernel_tests/nn_ops/rnn_cell_test.py||tensorflow/python/kernel_tests/nn_ops/rnn_cell_test.py"
          ],
          "candidate": [
            "tensorflow/core/kernels/rnn/lstm_ops.cc||tensorflow/core/kernels/rnn/lstm_ops.cc",
            "tensorflow/python/kernel_tests/nn_ops/rnn_cell_test.py||tensorflow/python/kernel_tests/nn_ops/rnn_cell_test.py"
          ]
        }
      },
      "candidate_diff": {
        "tensorflow/core/kernels/rnn/lstm_ops.cc||tensorflow/core/kernels/rnn/lstm_ops.cc": [
          "File: tensorflow/core/kernels/rnn/lstm_ops.cc -> tensorflow/core/kernels/rnn/lstm_ops.cc",
          "--- Hunk 1 ---",
          "[Context before]",
          "1139:     const Tensor* x;",
          "1140:     OP_REQUIRES_OK(ctx, ctx->input(\"x\", &x));",
          "1142:     const int64_t timelen = x->dim_size(0);",
          "1143:     const int64_t batch_size = x->dim_size(1);",
          "1144:     const int64_t input_size = x->dim_size(2);",
          "1146:     const Tensor* cs_prev_tensor = nullptr;",
          "1147:     OP_REQUIRES_OK(ctx, ctx->input(\"cs_prev\", &cs_prev_tensor));",
          "1149:     const Tensor* h_prev_tensor = nullptr;",
          "1150:     OP_REQUIRES_OK(ctx, ctx->input(\"h_prev\", &h_prev_tensor));",
          "1152:     const Tensor* w_tensor = nullptr;",
          "1153:     OP_REQUIRES_OK(ctx, ctx->input(\"w\", &w_tensor));",
          "1154:     const int64_t cell_size = w_tensor->dim_size(1) / 4;",
          "1155:     OP_REQUIRES(ctx, input_size + cell_size == w_tensor->dim_size(0),",
          "1156:                 errors::InvalidArgument(",
          "",
          "[Removed Lines]",
          "1141:     OP_REQUIRES(ctx, x->dims() == 3, errors::InvalidArgument(\"x must be 3D\"));",
          "",
          "[Added Lines]",
          "1141:     OP_REQUIRES(",
          "1142:         ctx, x->dims() == 3,",
          "1143:         errors::InvalidArgument(\"x must be rank 3 but is rank \", x->dims()));",
          "1150:     OP_REQUIRES(ctx, cs_prev_tensor->dims() == 2,",
          "1151:                 errors::InvalidArgument(\"cs_prev must be rank 2 but is rank \",",
          "1152:                                         cs_prev_tensor->dims()));",
          "1156:     OP_REQUIRES(ctx, h_prev_tensor->dims() == 2,",
          "1157:                 errors::InvalidArgument(\"h_prev must be rank 2 but is rank \",",
          "1158:                                         h_prev_tensor->dims()));",
          "1162:     OP_REQUIRES(ctx, w_tensor->dims() == 2,",
          "1163:                 errors::InvalidArgument(\"w must be rank 2 but is rank \",",
          "1164:                                         w_tensor->dims()));",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1160:     const Tensor* wci_tensor = nullptr;",
          "1161:     OP_REQUIRES_OK(ctx, ctx->input(\"wci\", &wci_tensor));",
          "1163:     const Tensor* wcf_tensor = nullptr;",
          "1164:     OP_REQUIRES_OK(ctx, ctx->input(\"wcf\", &wcf_tensor));",
          "1166:     const Tensor* wco_tensor = nullptr;",
          "1167:     OP_REQUIRES_OK(ctx, ctx->input(\"wco\", &wco_tensor));",
          "1169:     const Tensor* b_tensor = nullptr;",
          "1170:     OP_REQUIRES_OK(ctx, ctx->input(\"b\", &b_tensor));",
          "1171:     OP_REQUIRES(",
          "1172:         ctx, cell_size == b_tensor->dim_size(0) / 4,",
          "1173:         errors::InvalidArgument(\"w and b cell_size don't match: \", cell_size,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1173:     OP_REQUIRES(ctx, wci_tensor->dims() == 1,",
          "1174:                 errors::InvalidArgument(\"wci must be rank 1 but is rank \",",
          "1175:                                         wci_tensor->dims()));",
          "1179:     OP_REQUIRES(ctx, wcf_tensor->dims() == 1,",
          "1180:                 errors::InvalidArgument(\"wcf must be rank 1 but is rank \",",
          "1181:                                         wcf_tensor->dims()));",
          "1185:     OP_REQUIRES(ctx, wco_tensor->dims() == 1,",
          "1186:                 errors::InvalidArgument(\"wco must be rank 1 but is rank \",",
          "1187:                                         wco_tensor->dims()));",
          "1191:     OP_REQUIRES(ctx, b_tensor->dims() == 1,",
          "1192:                 errors::InvalidArgument(\"b must be rank 1 but is rank \",",
          "1193:                                         b_tensor->dims()));",
          "",
          "---------------"
        ],
        "tensorflow/python/kernel_tests/nn_ops/rnn_cell_test.py||tensorflow/python/kernel_tests/nn_ops/rnn_cell_test.py": [
          "File: tensorflow/python/kernel_tests/nn_ops/rnn_cell_test.py -> tensorflow/python/kernel_tests/nn_ops/rnn_cell_test.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1355:               cell_clip=cell_clip,",
          "1356:               use_peephole=use_peephole))",
          "1359: class BidirectionalRNNTest(test.TestCase):",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1358:   @test_util.run_in_graph_and_eager_modes",
          "1359:   def testLSTMBlockCellGradErrorHandling(self):",
          "1360:     use_peephole = False",
          "1361:     seq_len_max = constant_op.constant(1, shape=[], dtype=dtypes.int64)",
          "1362:     x = constant_op.constant(0.504355371, shape=[1, 1, 1], dtype=dtypes.float32)",
          "1363:     cs_prev = constant_op.constant(",
          "1364:         0.504355371, shape=[1, 1, 1], dtype=dtypes.float32)",
          "1365:     h_prev = constant_op.constant(",
          "1366:         0.504355371, shape=[1, 1], dtype=dtypes.float32)",
          "1367:     w = constant_op.constant(0.504355371, shape=[1, 1], dtype=dtypes.float32)",
          "1368:     wci = constant_op.constant(0.504355371, shape=[1], dtype=dtypes.float32)",
          "1369:     wcf = constant_op.constant(0.504355371, shape=[1], dtype=dtypes.float32)",
          "1370:     wco = constant_op.constant(0.504355371, shape=[1], dtype=dtypes.float32)",
          "1371:     b = constant_op.constant(0.504355371, shape=[1], dtype=dtypes.float32)",
          "1372:     i = constant_op.constant(0.504355371, shape=[1, 1, 1], dtype=dtypes.float32)",
          "1373:     cs = constant_op.constant(",
          "1374:         0.504355371, shape=[1, 1, 1], dtype=dtypes.float32)",
          "1375:     f = constant_op.constant(0.504355371, shape=[1, 1, 1], dtype=dtypes.float32)",
          "1376:     o = constant_op.constant(0.504355371, shape=[1, 1, 1], dtype=dtypes.float32)",
          "1377:     ci = constant_op.constant(",
          "1378:         0.504355371, shape=[1, 1, 1], dtype=dtypes.float32)",
          "1379:     co = constant_op.constant(",
          "1380:         0.504355371, shape=[1, 1, 1], dtype=dtypes.float32)",
          "1381:     h = constant_op.constant(0.504355371, shape=[1, 1, 1], dtype=dtypes.float32)",
          "1382:     cs_grad = constant_op.constant(",
          "1383:         0.504355371, shape=[1, 1, 1], dtype=dtypes.float32)",
          "1384:     h_grad = constant_op.constant(",
          "1385:         0.504355371, shape=[1, 1, 1], dtype=dtypes.float32)",
          "1386:     with self.assertRaisesRegex((ValueError, errors_impl.InvalidArgumentError),",
          "1387:                                 \"must be rank\"):",
          "1388:       self.evaluate(",
          "1389:           gen_rnn_ops.block_lstm_grad_v2(",
          "1390:               seq_len_max=seq_len_max,",
          "1391:               x=x,",
          "1392:               cs_prev=cs_prev,",
          "1393:               h_prev=h_prev,",
          "1394:               w=w,",
          "1395:               wci=wci,",
          "1396:               wcf=wcf,",
          "1397:               wco=wco,",
          "1398:               b=b,",
          "1399:               i=i,",
          "1400:               cs=cs,",
          "1401:               f=f,",
          "1402:               o=o,",
          "1403:               ci=ci,",
          "1404:               co=co,",
          "1405:               h=h,",
          "1406:               cs_grad=cs_grad,",
          "1407:               h_grad=h_grad,",
          "1408:               use_peephole=use_peephole))",
          "",
          "---------------"
        ]
      }
    }
  ]
}