{
  "cve_id": "CVE-2022-23595",
  "cve_desc": "Tensorflow is an Open Source Machine Learning Framework. When building an XLA compilation cache, if default settings are used, TensorFlow triggers a null pointer dereference. In the default scenario, all devices are allowed, so `flr->config_proto` is `nullptr`. The fix will be included in TensorFlow 2.8.0. We will also cherrypick this commit on TensorFlow 2.7.1, TensorFlow 2.6.3, and TensorFlow 2.5.3, as these are also affected and still in supported range.",
  "repo": "tensorflow/tensorflow",
  "patch_hash": "e21af685e1828f7ca65038307df5cc06de4479e8",
  "patch_info": {
    "commit_hash": "e21af685e1828f7ca65038307df5cc06de4479e8",
    "repo": "tensorflow/tensorflow",
    "commit_url": "https://github.com/tensorflow/tensorflow/commit/e21af685e1828f7ca65038307df5cc06de4479e8",
    "files": [
      "tensorflow/compiler/jit/xla_platform_info.cc"
    ],
    "message": "Fix Null-pointer dereference in BuildXlaCompilationCache\n\nIf ConfigProto is not used, then use the default settings which is to allow all devices.\n\nPiperOrigin-RevId: 420391800\nChange-Id: I88161ad7042990aef678e77b597a2fb2c8f815be",
    "before_after_code_files": [
      "tensorflow/compiler/jit/xla_platform_info.cc||tensorflow/compiler/jit/xla_platform_info.cc"
    ]
  },
  "patch_diff": {
    "tensorflow/compiler/jit/xla_platform_info.cc||tensorflow/compiler/jit/xla_platform_info.cc": [
      "File: tensorflow/compiler/jit/xla_platform_info.cc -> tensorflow/compiler/jit/xla_platform_info.cc",
      "--- Hunk 1 ---",
      "[Context before]",
      "82:   client_options.set_intra_op_parallelism_threads(",
      "83:       device->tensorflow_cpu_worker_threads()->num_threads);",
      "91:   auto client = xla::ClientLibrary::GetOrCreateLocalClient(client_options);",
      "92:   if (!client.ok()) {",
      "",
      "[Removed Lines]",
      "85:   string allowed_gpus =",
      "86:       flr->config_proto()->gpu_options().visible_device_list();",
      "87:   TF_ASSIGN_OR_RETURN(absl::optional<std::set<int>> gpu_ids,",
      "88:                       ParseVisibleDeviceList(allowed_gpus));",
      "89:   client_options.set_allowed_devices(gpu_ids);",
      "",
      "[Added Lines]",
      "85:   if (flr->config_proto()) {",
      "86:     string allowed_gpus =",
      "87:         flr->config_proto()->gpu_options().visible_device_list();",
      "88:     TF_ASSIGN_OR_RETURN(absl::optional<std::set<int>> gpu_ids,",
      "89:                         ParseVisibleDeviceList(allowed_gpus));",
      "90:     client_options.set_allowed_devices(gpu_ids);",
      "91:   }",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "eaa0c588cf44e0c436560738ec361cbc4861cb2d",
      "candidate_info": {
        "commit_hash": "eaa0c588cf44e0c436560738ec361cbc4861cb2d",
        "repo": "tensorflow/tensorflow",
        "commit_url": "https://github.com/tensorflow/tensorflow/commit/eaa0c588cf44e0c436560738ec361cbc4861cb2d",
        "files": [
          "tensorflow/compiler/jit/BUILD",
          "tensorflow/compiler/jit/get_compiler_ir.cc",
          "tensorflow/compiler/jit/kernels/xla_ops.cc",
          "tensorflow/compiler/jit/xla_compile_on_demand_op.cc",
          "tensorflow/compiler/jit/xla_gpu_device.cc",
          "tensorflow/compiler/jit/xla_platform_info.cc",
          "tensorflow/compiler/jit/xla_platform_info.h"
        ],
        "message": "[TF/XLA] Restrict GPUs to those selected with `tf.config.set_visible_devices`\n\nPiperOrigin-RevId: 371800645\nChange-Id: I6d4ac73b836997d16b132dfcebe312f17afe19b3",
        "before_after_code_files": [
          "tensorflow/compiler/jit/get_compiler_ir.cc||tensorflow/compiler/jit/get_compiler_ir.cc",
          "tensorflow/compiler/jit/kernels/xla_ops.cc||tensorflow/compiler/jit/kernels/xla_ops.cc",
          "tensorflow/compiler/jit/xla_compile_on_demand_op.cc||tensorflow/compiler/jit/xla_compile_on_demand_op.cc",
          "tensorflow/compiler/jit/xla_gpu_device.cc||tensorflow/compiler/jit/xla_gpu_device.cc",
          "tensorflow/compiler/jit/xla_platform_info.cc||tensorflow/compiler/jit/xla_platform_info.cc",
          "tensorflow/compiler/jit/xla_platform_info.h||tensorflow/compiler/jit/xla_platform_info.h"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "tensorflow/compiler/jit/xla_platform_info.cc||tensorflow/compiler/jit/xla_platform_info.cc"
          ],
          "candidate": [
            "tensorflow/compiler/jit/xla_platform_info.cc||tensorflow/compiler/jit/xla_platform_info.cc"
          ]
        }
      },
      "candidate_diff": {
        "tensorflow/compiler/jit/get_compiler_ir.cc||tensorflow/compiler/jit/get_compiler_ir.cc": [
          "File: tensorflow/compiler/jit/get_compiler_ir.cc -> tensorflow/compiler/jit/get_compiler_ir.cc",
          "--- Hunk 1 ---",
          "[Context before]",
          "97:   TF_RETURN_IF_ERROR(rmgr->LookupOrCreate<XlaCompilationCache>(",
          "98:       rmgr->default_container(), \"xla_cache\", &cache,",
          "99:       [&](XlaCompilationCache** cache_write_into) {",
          "101:       }));",
          "102:   core::ScopedUnref cache_ref(cache);",
          "",
          "[Removed Lines]",
          "100:         return BuildXlaCompilationCache(dev, platform_info, cache_write_into);",
          "",
          "[Added Lines]",
          "100:         return BuildXlaCompilationCache(dev, flr, platform_info,",
          "101:                                         cache_write_into);",
          "",
          "---------------"
        ],
        "tensorflow/compiler/jit/kernels/xla_ops.cc||tensorflow/compiler/jit/kernels/xla_ops.cc": [
          "File: tensorflow/compiler/jit/kernels/xla_ops.cc -> tensorflow/compiler/jit/kernels/xla_ops.cc",
          "--- Hunk 1 ---",
          "[Context before]",
          "189:   TF_RETURN_IF_ERROR(rm->LookupOrCreate<XlaCompilationCache>(",
          "190:       rm->default_container(), \"xla_cache\", &cache,",
          "191:       [&](XlaCompilationCache** cache) {",
          "193:       }));",
          "",
          "[Removed Lines]",
          "192:         return BuildXlaCompilationCache(ctx->device(), platform_info, cache);",
          "",
          "[Added Lines]",
          "192:         return BuildXlaCompilationCache(ctx->device(), ctx->function_library(),",
          "193:                                         platform_info, cache);",
          "",
          "---------------"
        ],
        "tensorflow/compiler/jit/xla_compile_on_demand_op.cc||tensorflow/compiler/jit/xla_compile_on_demand_op.cc": [
          "File: tensorflow/compiler/jit/xla_compile_on_demand_op.cc -> tensorflow/compiler/jit/xla_compile_on_demand_op.cc",
          "--- Hunk 1 ---",
          "[Context before]",
          "119:   TF_RETURN_IF_ERROR(rm->LookupOrCreate<XlaCompilationCache>(",
          "120:       rm->default_container(), \"xla_cache\", cache,",
          "121:       [&](XlaCompilationCache** write_into_cache) {",
          "124:       }));",
          "126:   XlaCompiler::Options options = GenerateCompilerOptions(",
          "",
          "[Removed Lines]",
          "122:         return BuildXlaCompilationCache(ctx->device(), platform_info_,",
          "123:                                         write_into_cache);",
          "",
          "[Added Lines]",
          "122:         return BuildXlaCompilationCache(ctx->device(), ctx->function_library(),",
          "123:                                         platform_info_, write_into_cache);",
          "",
          "---------------"
        ],
        "tensorflow/compiler/jit/xla_gpu_device.cc||tensorflow/compiler/jit/xla_gpu_device.cc": [
          "File: tensorflow/compiler/jit/xla_gpu_device.cc -> tensorflow/compiler/jit/xla_gpu_device.cc",
          "--- Hunk 1 ---",
          "[Context before]",
          "25: #include \"tensorflow/compiler/jit/kernels/xla_ops.h\"",
          "26: #include \"tensorflow/compiler/jit/xla_device.h\"",
          "27: #include \"tensorflow/compiler/jit/xla_device_ops.h\"",
          "28: #include \"tensorflow/compiler/tf2xla/xla_op_registry.h\"",
          "29: #include \"tensorflow/core/common_runtime/device_factory.h\"",
          "30: #include \"tensorflow/core/common_runtime/gpu/gpu_init.h\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "28: #include \"tensorflow/compiler/jit/xla_platform_info.h\"",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "33: namespace tensorflow {",
          "59: class XlaGpuDeviceFactory : public DeviceFactory {",
          "60:  public:",
          "61:   Status ListPhysicalDevices(std::vector<string>* devices) override;",
          "",
          "[Removed Lines]",
          "38: static xla::StatusOr<absl::optional<std::set<int>>> ParseVisibleDeviceList(",
          "39:     const string& visible_device_list) {",
          "40:   std::set<int> gpu_ids;",
          "41:   if (visible_device_list.empty()) {",
          "42:     return {{absl::nullopt}};",
          "43:   }",
          "44:   const std::vector<string> visible_devices =",
          "45:       absl::StrSplit(visible_device_list, ',');",
          "46:   for (const string& platform_device_id_str : visible_devices) {",
          "47:     int32 platform_device_id;",
          "48:     if (!absl::SimpleAtoi(platform_device_id_str, &platform_device_id)) {",
          "49:       return errors::InvalidArgument(",
          "50:           \"Could not parse entry in 'visible_device_list': '\",",
          "51:           platform_device_id_str,",
          "52:           \"'. visible_device_list = \", visible_device_list);",
          "53:     }",
          "54:     gpu_ids.insert(platform_device_id);",
          "55:   }",
          "56:   return {{gpu_ids}};",
          "57: }",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "tensorflow/compiler/jit/xla_platform_info.cc||tensorflow/compiler/jit/xla_platform_info.cc": [
          "File: tensorflow/compiler/jit/xla_platform_info.cc -> tensorflow/compiler/jit/xla_platform_info.cc",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: namespace tensorflow {",
          "23:                                 const XlaPlatformInfo& platform_info,",
          "24:                                 XlaCompilationCache** cache) {",
          "25:   if (platform_info.xla_device_metadata()) {",
          "",
          "[Removed Lines]",
          "22: Status BuildXlaCompilationCache(DeviceBase* device,",
          "",
          "[Added Lines]",
          "22: xla::StatusOr<absl::optional<std::set<int>>> ParseVisibleDeviceList(",
          "23:     absl::string_view visible_device_list) {",
          "24:   std::set<int> gpu_ids;",
          "25:   if (visible_device_list.empty()) {",
          "26:     return {{absl::nullopt}};",
          "27:   }",
          "28:   const std::vector<string> visible_devices =",
          "29:       absl::StrSplit(visible_device_list, ',');",
          "30:   for (const string& platform_device_id_str : visible_devices) {",
          "31:     int32 platform_device_id;",
          "32:     if (!absl::SimpleAtoi(platform_device_id_str, &platform_device_id)) {",
          "33:       return errors::InvalidArgument(",
          "34:           \"Could not parse entry in 'visible_device_list': '\",",
          "35:           platform_device_id_str,",
          "36:           \"'. visible_device_list = \", visible_device_list);",
          "37:     }",
          "38:     gpu_ids.insert(platform_device_id);",
          "39:   }",
          "40:   return {{gpu_ids}};",
          "41: }",
          "43: Status BuildXlaCompilationCache(DeviceBase* device, FunctionLibraryRuntime* flr,",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "60:   client_options.set_platform(platform.ValueOrDie());",
          "61:   client_options.set_intra_op_parallelism_threads(",
          "62:       device->tensorflow_cpu_worker_threads()->num_threads);",
          "63:   auto client = xla::ClientLibrary::GetOrCreateLocalClient(client_options);",
          "64:   if (!client.ok()) {",
          "65:     return client.status();",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "85:   string allowed_gpus =",
          "86:       flr->config_proto()->gpu_options().visible_device_list();",
          "87:   TF_ASSIGN_OR_RETURN(absl::optional<std::set<int>> gpu_ids,",
          "88:                       ParseVisibleDeviceList(allowed_gpus));",
          "89:   client_options.set_allowed_devices(gpu_ids);",
          "",
          "---------------"
        ],
        "tensorflow/compiler/jit/xla_platform_info.h||tensorflow/compiler/jit/xla_platform_info.h": [
          "File: tensorflow/compiler/jit/xla_platform_info.h -> tensorflow/compiler/jit/xla_platform_info.h",
          "--- Hunk 1 ---",
          "[Context before]",
          "81:   TF_DISALLOW_COPY_AND_ASSIGN(XlaPlatformInfo);",
          "82: };",
          "86:                                 const XlaPlatformInfo& platform_info,",
          "87:                                 XlaCompilationCache** cache);",
          "",
          "[Removed Lines]",
          "85: Status BuildXlaCompilationCache(DeviceBase* dev,",
          "",
          "[Added Lines]",
          "87: StatusOr<absl::optional<std::set<int>>> ParseVisibleDeviceList(",
          "88:     absl::string_view visible_device_list);",
          "91: Status BuildXlaCompilationCache(DeviceBase* dev, FunctionLibraryRuntime* flr,",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "c83aeca106be58680521e7fa02a8bd27b61d48f5",
      "candidate_info": {
        "commit_hash": "c83aeca106be58680521e7fa02a8bd27b61d48f5",
        "repo": "tensorflow/tensorflow",
        "commit_url": "https://github.com/tensorflow/tensorflow/commit/c83aeca106be58680521e7fa02a8bd27b61d48f5",
        "files": [
          "tensorflow/compiler/jit/xla_platform_info.cc"
        ],
        "message": "Fix Null-pointer dereference in BuildXlaCompilationCache\n\nIf ConfigProto is not used, then use the default settings which is to allow all devices.\n\nPiperOrigin-RevId: 420391800\nChange-Id: I88161ad7042990aef678e77b597a2fb2c8f815be",
        "before_after_code_files": [
          "tensorflow/compiler/jit/xla_platform_info.cc||tensorflow/compiler/jit/xla_platform_info.cc"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "diff_branch_same_aad": 1,
        "olp_code_files": {
          "patch": [
            "tensorflow/compiler/jit/xla_platform_info.cc||tensorflow/compiler/jit/xla_platform_info.cc"
          ],
          "candidate": [
            "tensorflow/compiler/jit/xla_platform_info.cc||tensorflow/compiler/jit/xla_platform_info.cc"
          ]
        }
      },
      "candidate_diff": {
        "tensorflow/compiler/jit/xla_platform_info.cc||tensorflow/compiler/jit/xla_platform_info.cc": [
          "File: tensorflow/compiler/jit/xla_platform_info.cc -> tensorflow/compiler/jit/xla_platform_info.cc",
          "--- Hunk 1 ---",
          "[Context before]",
          "82:   client_options.set_intra_op_parallelism_threads(",
          "83:       device->tensorflow_cpu_worker_threads()->num_threads);",
          "91:   auto client = xla::ClientLibrary::GetOrCreateLocalClient(client_options);",
          "92:   if (!client.ok()) {",
          "",
          "[Removed Lines]",
          "85:   string allowed_gpus =",
          "86:       flr->config_proto()->gpu_options().visible_device_list();",
          "87:   TF_ASSIGN_OR_RETURN(absl::optional<std::set<int>> gpu_ids,",
          "88:                       ParseVisibleDeviceList(allowed_gpus));",
          "89:   client_options.set_allowed_devices(gpu_ids);",
          "",
          "[Added Lines]",
          "85:   if (flr->config_proto()) {",
          "86:     string allowed_gpus =",
          "87:         flr->config_proto()->gpu_options().visible_device_list();",
          "88:     TF_ASSIGN_OR_RETURN(absl::optional<std::set<int>> gpu_ids,",
          "89:                         ParseVisibleDeviceList(allowed_gpus));",
          "90:     client_options.set_allowed_devices(gpu_ids);",
          "91:   }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "6e00a6efc6fadf3fe1a70e4dc11a5c2297723110",
      "candidate_info": {
        "commit_hash": "6e00a6efc6fadf3fe1a70e4dc11a5c2297723110",
        "repo": "tensorflow/tensorflow",
        "commit_url": "https://github.com/tensorflow/tensorflow/commit/6e00a6efc6fadf3fe1a70e4dc11a5c2297723110",
        "files": [
          "tensorflow/compiler/jit/xla_platform_info.cc"
        ],
        "message": "Fix Null-pointer dereference in BuildXlaCompilationCache\n\nIf ConfigProto is not used, then use the default settings which is to allow all devices.\n\nPiperOrigin-RevId: 420391800\nChange-Id: I88161ad7042990aef678e77b597a2fb2c8f815be",
        "before_after_code_files": [
          "tensorflow/compiler/jit/xla_platform_info.cc||tensorflow/compiler/jit/xla_platform_info.cc"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "diff_branch_same_aad": 1,
        "olp_code_files": {
          "patch": [
            "tensorflow/compiler/jit/xla_platform_info.cc||tensorflow/compiler/jit/xla_platform_info.cc"
          ],
          "candidate": [
            "tensorflow/compiler/jit/xla_platform_info.cc||tensorflow/compiler/jit/xla_platform_info.cc"
          ]
        }
      },
      "candidate_diff": {
        "tensorflow/compiler/jit/xla_platform_info.cc||tensorflow/compiler/jit/xla_platform_info.cc": [
          "File: tensorflow/compiler/jit/xla_platform_info.cc -> tensorflow/compiler/jit/xla_platform_info.cc",
          "--- Hunk 1 ---",
          "[Context before]",
          "82:   client_options.set_intra_op_parallelism_threads(",
          "83:       device->tensorflow_cpu_worker_threads()->num_threads);",
          "91:   auto client = xla::ClientLibrary::GetOrCreateLocalClient(client_options);",
          "92:   if (!client.ok()) {",
          "",
          "[Removed Lines]",
          "85:   string allowed_gpus =",
          "86:       flr->config_proto()->gpu_options().visible_device_list();",
          "87:   TF_ASSIGN_OR_RETURN(absl::optional<std::set<int>> gpu_ids,",
          "88:                       ParseVisibleDeviceList(allowed_gpus));",
          "89:   client_options.set_allowed_devices(gpu_ids);",
          "",
          "[Added Lines]",
          "85:   if (flr->config_proto()) {",
          "86:     string allowed_gpus =",
          "87:         flr->config_proto()->gpu_options().visible_device_list();",
          "88:     TF_ASSIGN_OR_RETURN(absl::optional<std::set<int>> gpu_ids,",
          "89:                         ParseVisibleDeviceList(allowed_gpus));",
          "90:     client_options.set_allowed_devices(gpu_ids);",
          "91:   }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "23ca6e0d815fae1856daea51e7f052a185532a23",
      "candidate_info": {
        "commit_hash": "23ca6e0d815fae1856daea51e7f052a185532a23",
        "repo": "tensorflow/tensorflow",
        "commit_url": "https://github.com/tensorflow/tensorflow/commit/23ca6e0d815fae1856daea51e7f052a185532a23",
        "files": [
          "tensorflow/compiler/jit/xla_platform_info.cc"
        ],
        "message": "Fix Null-pointer dereference in BuildXlaCompilationCache\n\nIf ConfigProto is not used, then use the default settings which is to allow all devices.\n\nPiperOrigin-RevId: 420391800\nChange-Id: I88161ad7042990aef678e77b597a2fb2c8f815be",
        "before_after_code_files": [
          "tensorflow/compiler/jit/xla_platform_info.cc||tensorflow/compiler/jit/xla_platform_info.cc"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "diff_branch_same_aad": 1,
        "olp_code_files": {
          "patch": [
            "tensorflow/compiler/jit/xla_platform_info.cc||tensorflow/compiler/jit/xla_platform_info.cc"
          ],
          "candidate": [
            "tensorflow/compiler/jit/xla_platform_info.cc||tensorflow/compiler/jit/xla_platform_info.cc"
          ]
        }
      },
      "candidate_diff": {
        "tensorflow/compiler/jit/xla_platform_info.cc||tensorflow/compiler/jit/xla_platform_info.cc": [
          "File: tensorflow/compiler/jit/xla_platform_info.cc -> tensorflow/compiler/jit/xla_platform_info.cc",
          "--- Hunk 1 ---",
          "[Context before]",
          "82:   client_options.set_intra_op_parallelism_threads(",
          "83:       device->tensorflow_cpu_worker_threads()->num_threads);",
          "91:   auto client = xla::ClientLibrary::GetOrCreateLocalClient(client_options);",
          "92:   if (!client.ok()) {",
          "",
          "[Removed Lines]",
          "85:   string allowed_gpus =",
          "86:       flr->config_proto()->gpu_options().visible_device_list();",
          "87:   TF_ASSIGN_OR_RETURN(absl::optional<std::set<int>> gpu_ids,",
          "88:                       ParseVisibleDeviceList(allowed_gpus));",
          "89:   client_options.set_allowed_devices(gpu_ids);",
          "",
          "[Added Lines]",
          "85:   if (flr->config_proto()) {",
          "86:     string allowed_gpus =",
          "87:         flr->config_proto()->gpu_options().visible_device_list();",
          "88:     TF_ASSIGN_OR_RETURN(absl::optional<std::set<int>> gpu_ids,",
          "89:                         ParseVisibleDeviceList(allowed_gpus));",
          "90:     client_options.set_allowed_devices(gpu_ids);",
          "91:   }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "d22277271c747d6de421387c7b38425db7cfbb74",
      "candidate_info": {
        "commit_hash": "d22277271c747d6de421387c7b38425db7cfbb74",
        "repo": "tensorflow/tensorflow",
        "commit_url": "https://github.com/tensorflow/tensorflow/commit/d22277271c747d6de421387c7b38425db7cfbb74",
        "files": [
          "tensorflow/compiler/jit/xla_platform_info.cc"
        ],
        "message": "Fix Null-pointer dereference in BuildXlaCompilationCache\n\nIf ConfigProto is not used, then use the default settings which is to allow all devices.\n\nPiperOrigin-RevId: 420391800\nChange-Id: I88161ad7042990aef678e77b597a2fb2c8f815be",
        "before_after_code_files": [
          "tensorflow/compiler/jit/xla_platform_info.cc||tensorflow/compiler/jit/xla_platform_info.cc"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "diff_branch_same_aad": 1,
        "olp_code_files": {
          "patch": [
            "tensorflow/compiler/jit/xla_platform_info.cc||tensorflow/compiler/jit/xla_platform_info.cc"
          ],
          "candidate": [
            "tensorflow/compiler/jit/xla_platform_info.cc||tensorflow/compiler/jit/xla_platform_info.cc"
          ]
        }
      },
      "candidate_diff": {
        "tensorflow/compiler/jit/xla_platform_info.cc||tensorflow/compiler/jit/xla_platform_info.cc": [
          "File: tensorflow/compiler/jit/xla_platform_info.cc -> tensorflow/compiler/jit/xla_platform_info.cc",
          "--- Hunk 1 ---",
          "[Context before]",
          "60:   client_options.set_platform(platform.ValueOrDie());",
          "61:   client_options.set_intra_op_parallelism_threads(",
          "62:       device->tensorflow_cpu_worker_threads()->num_threads);",
          "63:   auto client = xla::ClientLibrary::GetOrCreateLocalClient(client_options);",
          "64:   if (!client.ok()) {",
          "65:     return client.status();",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "64:   if (flr->config_proto()) {",
          "65:     string allowed_gpus =",
          "66:         flr->config_proto()->gpu_options().visible_device_list();",
          "67:     TF_ASSIGN_OR_RETURN(absl::optional<std::set<int>> gpu_ids,",
          "68:                         ParseVisibleDeviceList(allowed_gpus));",
          "69:     client_options.set_allowed_devices(gpu_ids);",
          "70:   }",
          "",
          "---------------"
        ]
      }
    }
  ]
}