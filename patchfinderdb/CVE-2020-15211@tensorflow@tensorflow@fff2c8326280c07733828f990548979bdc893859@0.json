{
  "cve_id": "CVE-2020-15211",
  "cve_desc": "In TensorFlow Lite before versions 1.15.4, 2.0.3, 2.1.2, 2.2.1 and 2.3.1, saved models in the flatbuffer format use a double indexing scheme: a model has a set of subgraphs, each subgraph has a set of operators and each operator has a set of input/output tensors. The flatbuffer format uses indices for the tensors, indexing into an array of tensors that is owned by the subgraph. This results in a pattern of double array indexing when trying to get the data of each tensor. However, some operators can have some tensors be optional. To handle this scenario, the flatbuffer model uses a negative `-1` value as index for these tensors. This results in special casing during validation at model loading time. Unfortunately, this means that the `-1` index is a valid tensor index for any operator, including those that don't expect optional inputs and including for output tensors. Thus, this allows writing and reading from outside the bounds of heap allocated arrays, although only at a specific offset from the start of these arrays. This results in both read and write gadgets, albeit very limited in scope. The issue is patched in several commits (46d5b0852, 00302787b7, e11f5558, cd31fd0ce, 1970c21, and fff2c83), and is released in TensorFlow versions 1.15.4, 2.0.3, 2.1.2, 2.2.1, or 2.3.1. A potential workaround would be to add a custom `Verifier` to the model loading code to ensure that only operators which accept optional inputs use the `-1` special value and only for the tensors that they expect to be optional. Since this allow-list type approach is erro-prone, we advise upgrading to the patched code.",
  "repo": "tensorflow/tensorflow",
  "patch_hash": "fff2c8326280c07733828f990548979bdc893859",
  "patch_info": {
    "commit_hash": "fff2c8326280c07733828f990548979bdc893859",
    "repo": "tensorflow/tensorflow",
    "commit_url": "https://github.com/tensorflow/tensorflow/commit/fff2c8326280c07733828f990548979bdc893859",
    "files": [
      "tensorflow/lite/micro/kernels/activations.cc",
      "tensorflow/lite/micro/kernels/add.cc",
      "tensorflow/lite/micro/kernels/ceil.cc",
      "tensorflow/lite/micro/kernels/circular_buffer.cc",
      "tensorflow/lite/micro/kernels/comparisons.cc",
      "tensorflow/lite/micro/kernels/concatenation.cc",
      "tensorflow/lite/micro/kernels/conv.cc",
      "tensorflow/lite/micro/kernels/depthwise_conv.cc",
      "tensorflow/lite/micro/kernels/dequantize.cc",
      "tensorflow/lite/micro/kernels/elementwise.cc",
      "tensorflow/lite/micro/kernels/fully_connected.cc",
      "tensorflow/lite/micro/kernels/hard_swish.cc",
      "tensorflow/lite/micro/kernels/l2norm.cc",
      "tensorflow/lite/micro/kernels/logistic.cc",
      "tensorflow/lite/micro/kernels/mul.cc",
      "tensorflow/lite/micro/kernels/pad.cc",
      "tensorflow/lite/micro/kernels/pooling.cc",
      "tensorflow/lite/micro/kernels/prelu.cc",
      "tensorflow/lite/micro/kernels/quantize.cc",
      "tensorflow/lite/micro/kernels/reduce.cc",
      "tensorflow/lite/micro/kernels/reshape.cc",
      "tensorflow/lite/micro/kernels/round.cc",
      "tensorflow/lite/micro/kernels/softmax.cc",
      "tensorflow/lite/micro/kernels/split.cc",
      "tensorflow/lite/micro/kernels/sub.cc",
      "tensorflow/lite/micro/kernels/svdf.cc",
      "tensorflow/lite/micro/kernels/tanh.cc"
    ],
    "message": "[tflite]: Insert `nullptr` checks when obtaining tensors.\n\nAs part of ongoing refactoring, `tflite::GetInput`, `tflite::GetOutput`, `tflite::GetTemporary` and `tflite::GetIntermediates` will return `nullptr` in some cases. Hence, we insert the `nullptr` checks on all usages.\n\nWe also insert `nullptr` checks on usages of `tflite::GetVariableInput` and `tflite::GetOptionalInputTensor` but only in the cases where there is no obvious check that `nullptr` is acceptable (that is, we only insert the check for the output of these two functions if the tensor is accessed as if it is always not `nullptr`).\n\nPiperOrigin-RevId: 332520146\nChange-Id: I405d986cfc653aaafcfdf4162c0acbd46220b921",
    "before_after_code_files": [
      "tensorflow/lite/micro/kernels/activations.cc||tensorflow/lite/micro/kernels/activations.cc",
      "tensorflow/lite/micro/kernels/add.cc||tensorflow/lite/micro/kernels/add.cc",
      "tensorflow/lite/micro/kernels/ceil.cc||tensorflow/lite/micro/kernels/ceil.cc",
      "tensorflow/lite/micro/kernels/circular_buffer.cc||tensorflow/lite/micro/kernels/circular_buffer.cc",
      "tensorflow/lite/micro/kernels/comparisons.cc||tensorflow/lite/micro/kernels/comparisons.cc",
      "tensorflow/lite/micro/kernels/concatenation.cc||tensorflow/lite/micro/kernels/concatenation.cc",
      "tensorflow/lite/micro/kernels/conv.cc||tensorflow/lite/micro/kernels/conv.cc",
      "tensorflow/lite/micro/kernels/depthwise_conv.cc||tensorflow/lite/micro/kernels/depthwise_conv.cc",
      "tensorflow/lite/micro/kernels/dequantize.cc||tensorflow/lite/micro/kernels/dequantize.cc",
      "tensorflow/lite/micro/kernels/elementwise.cc||tensorflow/lite/micro/kernels/elementwise.cc",
      "tensorflow/lite/micro/kernels/fully_connected.cc||tensorflow/lite/micro/kernels/fully_connected.cc",
      "tensorflow/lite/micro/kernels/hard_swish.cc||tensorflow/lite/micro/kernels/hard_swish.cc",
      "tensorflow/lite/micro/kernels/l2norm.cc||tensorflow/lite/micro/kernels/l2norm.cc",
      "tensorflow/lite/micro/kernels/logistic.cc||tensorflow/lite/micro/kernels/logistic.cc",
      "tensorflow/lite/micro/kernels/mul.cc||tensorflow/lite/micro/kernels/mul.cc",
      "tensorflow/lite/micro/kernels/pad.cc||tensorflow/lite/micro/kernels/pad.cc",
      "tensorflow/lite/micro/kernels/pooling.cc||tensorflow/lite/micro/kernels/pooling.cc",
      "tensorflow/lite/micro/kernels/prelu.cc||tensorflow/lite/micro/kernels/prelu.cc",
      "tensorflow/lite/micro/kernels/quantize.cc||tensorflow/lite/micro/kernels/quantize.cc",
      "tensorflow/lite/micro/kernels/reduce.cc||tensorflow/lite/micro/kernels/reduce.cc",
      "tensorflow/lite/micro/kernels/reshape.cc||tensorflow/lite/micro/kernels/reshape.cc",
      "tensorflow/lite/micro/kernels/round.cc||tensorflow/lite/micro/kernels/round.cc",
      "tensorflow/lite/micro/kernels/softmax.cc||tensorflow/lite/micro/kernels/softmax.cc",
      "tensorflow/lite/micro/kernels/split.cc||tensorflow/lite/micro/kernels/split.cc",
      "tensorflow/lite/micro/kernels/sub.cc||tensorflow/lite/micro/kernels/sub.cc",
      "tensorflow/lite/micro/kernels/svdf.cc||tensorflow/lite/micro/kernels/svdf.cc",
      "tensorflow/lite/micro/kernels/tanh.cc||tensorflow/lite/micro/kernels/tanh.cc"
    ]
  },
  "patch_diff": {
    "tensorflow/lite/micro/kernels/activations.cc||tensorflow/lite/micro/kernels/activations.cc": [
      "File: tensorflow/lite/micro/kernels/activations.cc -> tensorflow/lite/micro/kernels/activations.cc",
      "--- Hunk 1 ---",
      "[Context before]",
      "139:   ReluOpData* data = static_cast<ReluOpData*>(node->user_data);",
      "141:   const TfLiteTensor* input = GetInput(context, node, kInputTensor);",
      "142:   TfLiteTensor* output = GetOutput(context, node, kOutputTensor);",
      "144:   if (input->type == kTfLiteInt8) {",
      "145:     CalculateReluOpData<int8_t>(input, output, data);",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "142:   TF_LITE_ENSURE(context, input != nullptr);",
      "144:   TF_LITE_ENSURE(context, output != nullptr);",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "200:   Relu6OpData* data = static_cast<Relu6OpData*>(node->user_data);",
      "202:   const TfLiteTensor* input = GetInput(context, node, kInputTensor);",
      "204:   if (input->type == kTfLiteInt8) {",
      "205:     data->six_int8 = FloatToAsymmetricQuantizedInt8(6.0f, input->params.scale,",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "205:   TF_LITE_ENSURE(context, input != nullptr);",
      "",
      "---------------"
    ],
    "tensorflow/lite/micro/kernels/add.cc||tensorflow/lite/micro/kernels/add.cc": [
      "File: tensorflow/lite/micro/kernels/add.cc -> tensorflow/lite/micro/kernels/add.cc",
      "--- Hunk 1 ---",
      "[Context before]",
      "201:   TFLITE_DCHECK(node->builtin_data != nullptr);",
      "203:   const TfLiteTensor* input1 = GetInput(context, node, kInputTensor1);",
      "204:   const TfLiteTensor* input2 = GetInput(context, node, kInputTensor2);",
      "205:   TfLiteTensor* output = GetOutput(context, node, kOutputTensor);",
      "207:   OpData* data = static_cast<OpData*>(node->user_data);",
      "208:   auto* params = reinterpret_cast<TfLiteAddParams*>(node->builtin_data);",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "204:   TF_LITE_ENSURE(context, input1 != nullptr);",
      "206:   TF_LITE_ENSURE(context, input2 != nullptr);",
      "208:   TF_LITE_ENSURE(context, output != nullptr);",
      "",
      "---------------"
    ],
    "tensorflow/lite/micro/kernels/ceil.cc||tensorflow/lite/micro/kernels/ceil.cc": [
      "File: tensorflow/lite/micro/kernels/ceil.cc -> tensorflow/lite/micro/kernels/ceil.cc",
      "--- Hunk 1 ---",
      "[Context before]",
      "31: TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {",
      "32:   const TfLiteTensor* input = GetInput(context, node, kInputTensor);",
      "33:   TfLiteTensor* output = GetOutput(context, node, kOutputTensor);",
      "34:   TF_LITE_ENSURE_EQ(context, NumInputs(node), 1);",
      "35:   TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);",
      "36:   TF_LITE_ENSURE_TYPES_EQ(context, input->type, kTfLiteFloat32);",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "33:   TF_LITE_ENSURE(context, input != nullptr);",
      "35:   TF_LITE_ENSURE(context, output != nullptr);",
      "",
      "---------------"
    ],
    "tensorflow/lite/micro/kernels/circular_buffer.cc||tensorflow/lite/micro/kernels/circular_buffer.cc": [
      "File: tensorflow/lite/micro/kernels/circular_buffer.cc -> tensorflow/lite/micro/kernels/circular_buffer.cc",
      "--- Hunk 1 ---",
      "[Context before]",
      "78: TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {",
      "79:   const TfLiteTensor* input = GetInput(context, node, kInputTensor);",
      "80:   TfLiteTensor* output = GetOutput(context, node, kOutputTensor);",
      "82:   TF_LITE_ENSURE(context, input != nullptr);",
      "83:   TF_LITE_ENSURE(context, output != nullptr);",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "80:   TF_LITE_ENSURE(context, input != nullptr);",
      "82:   TF_LITE_ENSURE(context, output != nullptr);",
      "",
      "---------------"
    ],
    "tensorflow/lite/micro/kernels/comparisons.cc||tensorflow/lite/micro/kernels/comparisons.cc": [
      "File: tensorflow/lite/micro/kernels/comparisons.cc -> tensorflow/lite/micro/kernels/comparisons.cc",
      "--- Hunk 1 ---",
      "[Context before]",
      "619:   OpData* data = static_cast<OpData*>(node->user_data);",
      "621:   const TfLiteTensor* input1 = GetInput(context, node, kInputTensor1);",
      "622:   const TfLiteTensor* input2 = GetInput(context, node, kInputTensor2);",
      "624:   if (input1->type == kTfLiteUInt8 || input1->type == kTfLiteInt8) {",
      "625:     auto input1_offset = -input1->params.zero_point;",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "622:   TF_LITE_ENSURE(context, input1 != nullptr);",
      "624:   TF_LITE_ENSURE(context, input2 != nullptr);",
      "",
      "---------------"
    ],
    "tensorflow/lite/micro/kernels/concatenation.cc||tensorflow/lite/micro/kernels/concatenation.cc": [
      "File: tensorflow/lite/micro/kernels/concatenation.cc -> tensorflow/lite/micro/kernels/concatenation.cc",
      "--- Hunk 1 ---",
      "[Context before]",
      "136:   const TfLiteConcatenationParams* params =",
      "137:       reinterpret_cast<TfLiteConcatenationParams*>(node->builtin_data);",
      "143:   TF_LITE_ENSURE_EQ(context, params->activation, kTfLiteActNone);",
      "",
      "[Removed Lines]",
      "139:   TfLiteType input_type = GetInput(context, node, 0)->type;",
      "140:   TfLiteType output_type = GetOutput(context, node, kOutputTensor)->type;",
      "",
      "[Added Lines]",
      "139:   const TfLiteTensor* input_tensor = GetInput(context, node, 0);",
      "140:   TF_LITE_ENSURE(context, input_tensor != nullptr);",
      "141:   TfLiteType input_type = input_tensor->type;",
      "142:   const TfLiteTensor* output_tensor = GetOutput(context, node, kOutputTensor);",
      "143:   TF_LITE_ENSURE(context, output_tensor != nullptr);",
      "144:   TfLiteType output_type = output_tensor->type;",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "157:   for (int i = 0; i < num_inputs; ++i) {",
      "158:     const TfLiteTensor* input = GetInput(context, node, i);",
      "159:     int num_dimensions = NumDimensions(input);",
      "161:     if (num_dimensions > 4) {",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "163:     TF_LITE_ENSURE(context, input != nullptr);",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "173:   OpData* data = static_cast<OpData*>(node->user_data);",
      "175:   TfLiteTensor* output = GetOutput(context, node, kOutputTensor);",
      "177:   switch (output_type) {  // Already know in/outtypes are same.",
      "178:     case kTfLiteFloat32:",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "181:   TF_LITE_ENSURE(context, output != nullptr);",
      "",
      "---------------",
      "--- Hunk 4 ---",
      "[Context before]",
      "200:       for (int i = 0; i < node->inputs->size; ++i) {",
      "201:         const TfLiteTensor* t = GetInput(context, node, i);",
      "202:         input_scales[i] = t->params.scale;",
      "203:         input_zero_points[i] = t->params.zero_point;",
      "204:       }",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "208:         TF_LITE_ENSURE(context, t != nullptr);",
      "",
      "---------------",
      "--- Hunk 5 ---",
      "[Context before]",
      "220: }",
      "222: TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {",
      "225:   switch (output_type) {  // Already know in/outtypes are same.",
      "226:     case kTfLiteFloat32:",
      "",
      "[Removed Lines]",
      "223:   TfLiteType output_type = GetOutput(context, node, kOutputTensor)->type;",
      "",
      "[Added Lines]",
      "230:   const TfLiteTensor* output_tensor = GetOutput(context, node, kOutputTensor);",
      "231:   TF_LITE_ENSURE(context, output_tensor != nullptr);",
      "232:   TfLiteType output_type = output_tensor->type;",
      "",
      "---------------"
    ],
    "tensorflow/lite/micro/kernels/conv.cc||tensorflow/lite/micro/kernels/conv.cc": [
      "File: tensorflow/lite/micro/kernels/conv.cc -> tensorflow/lite/micro/kernels/conv.cc",
      "--- Hunk 1 ---",
      "[Context before]",
      "98:   if (data_type != kTfLiteFloat32) {",
      "99:     const TfLiteTensor* input = GetInput(context, node, kInputTensor);",
      "100:     const TfLiteTensor* filter = GetInput(context, node, kFilterTensor);",
      "101:     const TfLiteTensor* bias =",
      "102:         GetOptionalInputTensor(context, node, kBiasTensor);",
      "103:     TfLiteTensor* output = GetOutput(context, node, kOutputTensor);",
      "104:     int output_channels = filter->dims->data[kConvQuantizedDimension];",
      "106:     TF_LITE_ENSURE_STATUS(tflite::PopulateConvolutionQuantizationParams(",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "100:     TF_LITE_ENSURE(context, input != nullptr);",
      "102:     TF_LITE_ENSURE(context, filter != nullptr);",
      "106:     TF_LITE_ENSURE(context, output != nullptr);",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "127:   const auto params = static_cast<const TfLiteConvParams*>(node->builtin_data);",
      "129:   TfLiteTensor* output = GetOutput(context, node, kOutputTensor);",
      "130:   const TfLiteTensor* input = GetInput(context, node, kInputTensor);",
      "131:   const TfLiteTensor* filter = GetInput(context, node, kFilterTensor);",
      "133:   int input_width = input->dims->data[2];",
      "134:   int input_height = input->dims->data[1];",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "133:   TF_LITE_ENSURE(context, output != nullptr);",
      "135:   TF_LITE_ENSURE(context, input != nullptr);",
      "137:   TF_LITE_ENSURE(context, filter != nullptr);",
      "",
      "---------------"
    ],
    "tensorflow/lite/micro/kernels/depthwise_conv.cc||tensorflow/lite/micro/kernels/depthwise_conv.cc": [
      "File: tensorflow/lite/micro/kernels/depthwise_conv.cc -> tensorflow/lite/micro/kernels/depthwise_conv.cc",
      "--- Hunk 1 ---",
      "[Context before]",
      "83:   if (data_type != kTfLiteFloat32) {",
      "84:     const TfLiteTensor* input = GetInput(context, node, kInputTensor);",
      "85:     const TfLiteTensor* filter = GetInput(context, node, kFilterTensor);",
      "86:     const TfLiteTensor* bias =",
      "87:         GetOptionalInputTensor(context, node, kBiasTensor);",
      "88:     TfLiteTensor* output = GetOutput(context, node, kOutputTensor);",
      "89:     int num_channels = filter->dims->data[kDepthwiseConvQuantizedDimension];",
      "91:     return tflite::PopulateConvolutionQuantizationParams(",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "85:     TF_LITE_ENSURE(context, input != nullptr);",
      "87:     TF_LITE_ENSURE(context, filter != nullptr);",
      "91:     TF_LITE_ENSURE(context, output != nullptr);",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "114:   OpData* data = static_cast<OpData*>(node->user_data);",
      "116:   TfLiteTensor* output = GetOutput(context, node, kOutputTensor);",
      "117:   const TfLiteTensor* input = GetInput(context, node, kInputTensor);",
      "118:   const TfLiteTensor* filter = GetInput(context, node, kFilterTensor);",
      "120:   const TfLiteType data_type = input->type;",
      "121:   int width = SizeOfDimension(input, 2);",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "120:   TF_LITE_ENSURE(context, output != nullptr);",
      "122:   TF_LITE_ENSURE(context, input != nullptr);",
      "124:   TF_LITE_ENSURE(context, filter != nullptr);",
      "",
      "---------------"
    ],
    "tensorflow/lite/micro/kernels/dequantize.cc||tensorflow/lite/micro/kernels/dequantize.cc": [
      "File: tensorflow/lite/micro/kernels/dequantize.cc -> tensorflow/lite/micro/kernels/dequantize.cc",
      "--- Hunk 1 ---",
      "[Context before]",
      "54:   const TfLiteTensor* input = GetInput(context, node, 0);",
      "55:   TfLiteTensor* output = GetOutput(context, node, 0);",
      "57:   TF_LITE_ENSURE(context, input->type == kTfLiteUInt8 ||",
      "58:                               input->type == kTfLiteInt8 ||",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "55:   TF_LITE_ENSURE(context, input != nullptr);",
      "57:   TF_LITE_ENSURE(context, output != nullptr);",
      "",
      "---------------"
    ],
    "tensorflow/lite/micro/kernels/elementwise.cc||tensorflow/lite/micro/kernels/elementwise.cc": [
      "File: tensorflow/lite/micro/kernels/elementwise.cc -> tensorflow/lite/micro/kernels/elementwise.cc",
      "--- Hunk 1 ---",
      "[Context before]",
      "41:   TF_LITE_ENSURE_EQ(context, NumInputs(node), 1);",
      "42:   TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);",
      "43:   const TfLiteTensor* input = GetInput(context, node, 0);",
      "44:   TfLiteTensor* output = GetOutput(context, node, 0);",
      "45:   TF_LITE_ENSURE_TYPES_EQ(context, input->type, output->type);",
      "46:   if (!IsSupportedType(input->type)) {",
      "47:     TF_LITE_KERNEL_LOG(context, \"Input data type %s (%d) is not supported.\",",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "44:   TF_LITE_ENSURE(context, input != nullptr);",
      "46:   TF_LITE_ENSURE(context, output != nullptr);",
      "",
      "---------------"
    ],
    "tensorflow/lite/micro/kernels/fully_connected.cc||tensorflow/lite/micro/kernels/fully_connected.cc": [
      "File: tensorflow/lite/micro/kernels/fully_connected.cc -> tensorflow/lite/micro/kernels/fully_connected.cc",
      "--- Hunk 1 ---",
      "[Context before]",
      "93:       static_cast<const TfLiteFullyConnectedParams*>(node->builtin_data);",
      "95:   const TfLiteTensor* input = GetInput(context, node, kInputTensor);",
      "96:   const TfLiteTensor* filter = GetInput(context, node, kWeightsTensor);",
      "97:   const TfLiteTensor* bias = GetOptionalInputTensor(context, node, kBiasTensor);",
      "98:   TfLiteTensor* output = GetOutput(context, node, kOutputTensor);",
      "100:   TF_LITE_ENSURE_TYPES_EQ(context, input->type, output->type);",
      "101:   TF_LITE_ENSURE_MSG(context, input->type == filter->type,",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "96:   TF_LITE_ENSURE(context, input != nullptr);",
      "98:   TF_LITE_ENSURE(context, filter != nullptr);",
      "101:   TF_LITE_ENSURE(context, output != nullptr);",
      "",
      "---------------"
    ],
    "tensorflow/lite/micro/kernels/hard_swish.cc||tensorflow/lite/micro/kernels/hard_swish.cc": [
      "File: tensorflow/lite/micro/kernels/hard_swish.cc -> tensorflow/lite/micro/kernels/hard_swish.cc",
      "--- Hunk 1 ---",
      "[Context before]",
      "45:   TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);",
      "47:   const TfLiteTensor* input = GetInput(context, node, kInputTensor);",
      "48:   TfLiteTensor* output = GetOutput(context, node, kOutputTensor);",
      "50:   if (input->type == kTfLiteUInt8 || input->type == kTfLiteInt8) {",
      "51:     HardSwishParams* params = static_cast<HardSwishParams*>(node->user_data);",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "48:   TF_LITE_ENSURE(context, input != nullptr);",
      "50:   TF_LITE_ENSURE(context, output != nullptr);",
      "",
      "---------------"
    ],
    "tensorflow/lite/micro/kernels/l2norm.cc||tensorflow/lite/micro/kernels/l2norm.cc": [
      "File: tensorflow/lite/micro/kernels/l2norm.cc -> tensorflow/lite/micro/kernels/l2norm.cc",
      "--- Hunk 1 ---",
      "[Context before]",
      "50:   TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);",
      "52:   const TfLiteTensor* input = GetInput(context, node, kInputTensor);",
      "53:   TfLiteTensor* output = GetOutput(context, node, kOutputTensor);",
      "55:   TF_LITE_ENSURE(context, NumDimensions(input) <= 4);",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "53:   TF_LITE_ENSURE(context, input != nullptr);",
      "55:   TF_LITE_ENSURE(context, output != nullptr);",
      "",
      "---------------"
    ],
    "tensorflow/lite/micro/kernels/logistic.cc||tensorflow/lite/micro/kernels/logistic.cc": [
      "File: tensorflow/lite/micro/kernels/logistic.cc -> tensorflow/lite/micro/kernels/logistic.cc",
      "--- Hunk 1 ---",
      "[Context before]",
      "43: TfLiteStatus CalculateArithmeticOpData(TfLiteContext* context, TfLiteNode* node,",
      "44:                                        OpData* data) {",
      "45:   const TfLiteTensor* input = GetInput(context, node, kInputTensor);",
      "46:   TfLiteTensor* output = GetOutput(context, node, kOutputTensor);",
      "48:   TF_LITE_ENSURE_TYPES_EQ(context, input->type, output->type);",
      "49:   if (input->type == kTfLiteInt8) {",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "46:   TF_LITE_ENSURE(context, input != nullptr);",
      "48:   TF_LITE_ENSURE(context, output != nullptr);",
      "",
      "---------------"
    ],
    "tensorflow/lite/micro/kernels/mul.cc||tensorflow/lite/micro/kernels/mul.cc": [
      "File: tensorflow/lite/micro/kernels/mul.cc -> tensorflow/lite/micro/kernels/mul.cc",
      "--- Hunk 1 ---",
      "[Context before]",
      "51: TfLiteStatus CalculateOpData(TfLiteContext* context, TfLiteNode* node,",
      "52:                              TfLiteMulParams* params, OpData* data) {",
      "53:   const TfLiteTensor* input1 = GetInput(context, node, kInput1Tensor);",
      "54:   const TfLiteTensor* input2 = GetInput(context, node, kInput2Tensor);",
      "55:   TfLiteTensor* output = GetOutput(context, node, kOutputTensor);",
      "57:   TF_LITE_ENSURE_EQ(context, NumInputs(node), 2);",
      "58:   TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "54:   TF_LITE_ENSURE(context, input1 != nullptr);",
      "56:   TF_LITE_ENSURE(context, input2 != nullptr);",
      "58:   TF_LITE_ENSURE(context, output != nullptr);",
      "",
      "---------------"
    ],
    "tensorflow/lite/micro/kernels/pad.cc||tensorflow/lite/micro/kernels/pad.cc": [
      "File: tensorflow/lite/micro/kernels/pad.cc -> tensorflow/lite/micro/kernels/pad.cc",
      "--- Hunk 1 ---",
      "[Context before]",
      "50:   TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);",
      "52:   const TfLiteTensor* input = GetInput(context, node, /*index=*/0);",
      "53:   const TfLiteTensor* paddings = GetInput(context, node, /*index=*/1);",
      "54:   const TfLiteTensor* constant_values =",
      "55:       NumInputs(node) == 3 ? GetInput(context, node, /*index=*/2) : nullptr;",
      "56:   TfLiteTensor* output = GetOutput(context, node, /*index=*/0);",
      "58:   TF_LITE_ENSURE_EQ(context, input->type, output->type);",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "53:   TF_LITE_ENSURE(context, input != nullptr);",
      "55:   TF_LITE_ENSURE(context, paddings != nullptr);",
      "59:   TF_LITE_ENSURE(context, output != nullptr);",
      "",
      "---------------"
    ],
    "tensorflow/lite/micro/kernels/pooling.cc||tensorflow/lite/micro/kernels/pooling.cc": [
      "File: tensorflow/lite/micro/kernels/pooling.cc -> tensorflow/lite/micro/kernels/pooling.cc",
      "--- Hunk 1 ---",
      "[Context before]",
      "222:   OpData* data = static_cast<OpData*>(node->user_data);",
      "224:   const TfLiteTensor* input = GetInput(context, node, kInputTensor);",
      "225:   TfLiteTensor* output = GetOutput(context, node, kOutputTensor);",
      "227:   TF_LITE_ENSURE_STATUS(CalculateOpData(context, params, input, output, data));",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "225:   TF_LITE_ENSURE(context, input != nullptr);",
      "227:   TF_LITE_ENSURE(context, output != nullptr);",
      "",
      "---------------"
    ],
    "tensorflow/lite/micro/kernels/prelu.cc||tensorflow/lite/micro/kernels/prelu.cc": [
      "File: tensorflow/lite/micro/kernels/prelu.cc -> tensorflow/lite/micro/kernels/prelu.cc",
      "--- Hunk 1 ---",
      "[Context before]",
      "95:   PreluParams* params = static_cast<PreluParams*>(node->user_data);",
      "97:   const TfLiteTensor* input = GetInput(context, node, 0);",
      "98:   const TfLiteTensor* alpha = GetInput(context, node, 1);",
      "99:   TfLiteTensor* output = GetOutput(context, node, 0);",
      "101:   return CalculatePreluParams(input, alpha, output, params);",
      "102: }",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "98:   TF_LITE_ENSURE(context, input != nullptr);",
      "100:   TF_LITE_ENSURE(context, alpha != nullptr);",
      "102:   TF_LITE_ENSURE(context, output != nullptr);",
      "",
      "---------------"
    ],
    "tensorflow/lite/micro/kernels/quantize.cc||tensorflow/lite/micro/kernels/quantize.cc": [
      "File: tensorflow/lite/micro/kernels/quantize.cc -> tensorflow/lite/micro/kernels/quantize.cc",
      "--- Hunk 1 ---",
      "[Context before]",
      "50:   TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);",
      "52:   const TfLiteTensor* input = GetInput(context, node, 0);",
      "53:   TfLiteTensor* output = GetOutput(context, node, 0);",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "53:   TF_LITE_ENSURE(context, input != nullptr);",
      "55:   TF_LITE_ENSURE(context, output != nullptr);",
      "",
      "---------------"
    ],
    "tensorflow/lite/micro/kernels/reduce.cc||tensorflow/lite/micro/kernels/reduce.cc": [
      "File: tensorflow/lite/micro/kernels/reduce.cc -> tensorflow/lite/micro/kernels/reduce.cc",
      "--- Hunk 1 ---",
      "[Context before]",
      "66:   const TfLiteTensor* axis = GetInput(context, node, 1);",
      "67:   TF_LITE_ENSURE_TYPES_EQ(context, axis->type, kTfLiteInt32);",
      "69:   if (input->type == kTfLiteInt8) {",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "67:   TF_LITE_ENSURE(context, axis != nullptr);",
      "",
      "---------------"
    ],
    "tensorflow/lite/micro/kernels/reshape.cc||tensorflow/lite/micro/kernels/reshape.cc": [
      "File: tensorflow/lite/micro/kernels/reshape.cc -> tensorflow/lite/micro/kernels/reshape.cc",
      "--- Hunk 1 ---",
      "[Context before]",
      "33: TfLiteStatus ReshapeOutput(TfLiteContext* context, TfLiteNode* node) {",
      "34:   const TfLiteTensor* input = GetInput(context, node, kInputTensor);",
      "35:   TfLiteTensor* output = GetOutput(context, node, kOutputTensor);",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "35:   TF_LITE_ENSURE(context, input != nullptr);",
      "37:   TF_LITE_ENSURE(context, output != nullptr);",
      "",
      "---------------"
    ],
    "tensorflow/lite/micro/kernels/round.cc||tensorflow/lite/micro/kernels/round.cc": [
      "File: tensorflow/lite/micro/kernels/round.cc -> tensorflow/lite/micro/kernels/round.cc",
      "--- Hunk 1 ---",
      "[Context before]",
      "31: TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {",
      "32:   const TfLiteTensor* input = GetInput(context, node, kInputTensor);",
      "33:   TfLiteTensor* output = GetOutput(context, node, kOutputTensor);",
      "34:   TF_LITE_ENSURE_EQ(context, NumInputs(node), 1);",
      "35:   TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);",
      "36:   TF_LITE_ENSURE_TYPES_EQ(context, input->type, kTfLiteFloat32);",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "33:   TF_LITE_ENSURE(context, input != nullptr);",
      "35:   TF_LITE_ENSURE(context, output != nullptr);",
      "",
      "---------------"
    ],
    "tensorflow/lite/micro/kernels/softmax.cc||tensorflow/lite/micro/kernels/softmax.cc": [
      "File: tensorflow/lite/micro/kernels/softmax.cc -> tensorflow/lite/micro/kernels/softmax.cc",
      "--- Hunk 1 ---",
      "[Context before]",
      "119:   TF_LITE_ENSURE_EQ(context, NumInputs(node), 1);",
      "120:   TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);",
      "121:   const TfLiteTensor* input = GetInput(context, node, 0);",
      "122:   TF_LITE_ENSURE(context, NumDimensions(input) >= 1);",
      "124:   TfLiteTensor* output = GetOutput(context, node, 0);",
      "126:   TFLITE_DCHECK(node->user_data != nullptr);",
      "127:   SoftmaxParams* data = static_cast<SoftmaxParams*>(node->user_data);",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "122:   TF_LITE_ENSURE(context, input != nullptr);",
      "126:   TF_LITE_ENSURE(context, output != nullptr);",
      "",
      "---------------"
    ],
    "tensorflow/lite/micro/kernels/split.cc||tensorflow/lite/micro/kernels/split.cc": [
      "File: tensorflow/lite/micro/kernels/split.cc -> tensorflow/lite/micro/kernels/split.cc",
      "--- Hunk 1 ---",
      "[Context before]",
      "70: TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {",
      "71:   const TfLiteTensor* axis = GetInput(context, node, 0);",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "72:   TF_LITE_ENSURE(context, axis != nullptr);",
      "",
      "---------------"
    ],
    "tensorflow/lite/micro/kernels/sub.cc||tensorflow/lite/micro/kernels/sub.cc": [
      "File: tensorflow/lite/micro/kernels/sub.cc -> tensorflow/lite/micro/kernels/sub.cc",
      "--- Hunk 1 ---",
      "[Context before]",
      "108:   auto* params = reinterpret_cast<TfLiteSubParams*>(node->builtin_data);",
      "110:   const TfLiteTensor* input1 = GetInput(context, node, kInputTensor1);",
      "111:   const TfLiteTensor* input2 = GetInput(context, node, kInputTensor2);",
      "112:   TfLiteTensor* output = GetOutput(context, node, kOutputTensor);",
      "114:   TF_LITE_ENSURE_STATUS(",
      "115:       CalculateOpData(context, params, input1, input2, output, data));",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "111:   TF_LITE_ENSURE(context, input1 != nullptr);",
      "113:   TF_LITE_ENSURE(context, input2 != nullptr);",
      "115:   TF_LITE_ENSURE(context, output != nullptr);",
      "",
      "---------------"
    ],
    "tensorflow/lite/micro/kernels/svdf.cc||tensorflow/lite/micro/kernels/svdf.cc": [
      "File: tensorflow/lite/micro/kernels/svdf.cc -> tensorflow/lite/micro/kernels/svdf.cc",
      "--- Hunk 1 ---",
      "[Context before]",
      "368:   const TfLiteTensor* input = GetInput(context, node, kInputTensor);",
      "369:   const TfLiteTensor* weights_feature =",
      "370:       GetInput(context, node, kWeightsFeatureTensor);",
      "371:   const TfLiteTensor* weights_time =",
      "372:       GetInput(context, node, kWeightsTimeTensor);",
      "373:   const TfLiteTensor* bias = GetOptionalInputTensor(context, node, kBiasTensor);",
      "374:   const TfLiteTensor* activation_state =",
      "375:       GetInput(context, node, kInputActivationStateTensor);",
      "378:   const int rank = params->rank;",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "369:   TF_LITE_ENSURE(context, input != nullptr);",
      "372:   TF_LITE_ENSURE(context, weights_feature != nullptr);",
      "375:   TF_LITE_ENSURE(context, weights_time != nullptr);",
      "379:   TF_LITE_ENSURE(context, activation_state != nullptr);",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "393:   TF_LITE_ENSURE_EQ(context, node->outputs->size, 1);",
      "394:   TfLiteTensor* output = GetOutput(context, node, kOutputTensor);",
      "395:   TF_LITE_ENSURE_EQ(context, NumDimensions(output), 2);",
      "396:   TF_LITE_ENSURE_EQ(context, output->dims->data[0], batch_size);",
      "397:   TF_LITE_ENSURE_EQ(context, output->dims->data[1], num_units);",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "399:   TF_LITE_ENSURE(context, output != nullptr);",
      "",
      "---------------"
    ],
    "tensorflow/lite/micro/kernels/tanh.cc||tensorflow/lite/micro/kernels/tanh.cc": [
      "File: tensorflow/lite/micro/kernels/tanh.cc -> tensorflow/lite/micro/kernels/tanh.cc",
      "--- Hunk 1 ---",
      "[Context before]",
      "51:   TF_LITE_ENSURE_EQ(context, NumInputs(node), 1);",
      "52:   TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);",
      "53:   const TfLiteTensor* input = GetInput(context, node, kInputTensor);",
      "54:   TfLiteTensor* output = GetOutput(context, node, kOutputTensor);",
      "56:   TF_LITE_ENSURE_TYPES_EQ(context, input->type, output->type);",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "54:   TF_LITE_ENSURE(context, input != nullptr);",
      "56:   TF_LITE_ENSURE(context, output != nullptr);",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "76:   OpData* data = static_cast<OpData*>(node->user_data);",
      "78:   const TfLiteTensor* input = GetInput(context, node, kInputTensor);",
      "79:   data->input_zero_point = input->params.zero_point;",
      "80:   return CalculateArithmeticOpData(context, node, data);",
      "81: }",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "81:   TF_LITE_ENSURE(context, input != nullptr);",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "05ae49f2f66e67f2fc7733b490ada3038603685a",
      "candidate_info": {
        "commit_hash": "05ae49f2f66e67f2fc7733b490ada3038603685a",
        "repo": "tensorflow/tensorflow",
        "commit_url": "https://github.com/tensorflow/tensorflow/commit/05ae49f2f66e67f2fc7733b490ada3038603685a",
        "files": [
          "tensorflow/lite/micro/kernels/circular_buffer.cc"
        ],
        "message": "Generalize circular buffer operator.\n\nPiperOrigin-RevId: 343587123\nChange-Id: Ifaea8ffe1e067b4119e529053aa13874c87c984b",
        "before_after_code_files": [
          "tensorflow/lite/micro/kernels/circular_buffer.cc||tensorflow/lite/micro/kernels/circular_buffer.cc"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "tensorflow/lite/micro/kernels/circular_buffer.cc||tensorflow/lite/micro/kernels/circular_buffer.cc"
          ],
          "candidate": [
            "tensorflow/lite/micro/kernels/circular_buffer.cc||tensorflow/lite/micro/kernels/circular_buffer.cc"
          ]
        }
      },
      "candidate_diff": {
        "tensorflow/lite/micro/kernels/circular_buffer.cc||tensorflow/lite/micro/kernels/circular_buffer.cc": [
          "File: tensorflow/lite/micro/kernels/circular_buffer.cc -> tensorflow/lite/micro/kernels/circular_buffer.cc",
          "--- Hunk 1 ---",
          "[Context before]",
          "65:   int cycles_max;",
          "66: };",
          "74: }  // namespace",
          "78: TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {",
          "79:   const TfLiteTensor* input = GetInput(context, node, kInputTensor);",
          "81:   TfLiteTensor* output = GetOutput(context, node, kOutputTensor);",
          "84:   TF_LITE_ENSURE(context, input != nullptr);",
          "85:   TF_LITE_ENSURE(context, output != nullptr);",
          "88:   TF_LITE_ENSURE_EQ(context, 1, input->dims->data[1]);",
          "91:   TF_LITE_ENSURE_EQ(context, output->dims->data[3], input->dims->data[3]);",
          "93:   TF_LITE_ENSURE_TYPES_EQ(context, input->type, output->type);",
          "96:   TF_LITE_ENSURE_TYPES_EQ(context, input->type, kTfLiteInt8);",
          "106:     op_data->cycles_max = 1;",
          "107:   } else {",
          "108:     op_data->cycles_max = 2;",
          "109:   }",
          "",
          "[Removed Lines]",
          "70: constexpr int kMaxOpDataSize = 7;",
          "71: int op_data_counter = 0;",
          "72: OpData op_data_array[kMaxOpDataSize];",
          "76: void Free(TfLiteContext* context, void* buffer) { op_data_counter = 0; }",
          "80:   TF_LITE_ENSURE(context, input != nullptr);",
          "82:   TF_LITE_ENSURE(context, output != nullptr);",
          "86:   TF_LITE_ENSURE_EQ(context, 1, output->dims->data[0]);",
          "87:   TF_LITE_ENSURE_EQ(context, 1, input->dims->data[0]);",
          "89:   TF_LITE_ENSURE_EQ(context, 1, output->dims->data[2]);",
          "90:   TF_LITE_ENSURE_EQ(context, 1, input->dims->data[2]);",
          "100:   TFLITE_DCHECK_LE(op_data_counter, kMaxOpDataSize);",
          "101:   OpData* op_data = &op_data_array[op_data_counter++];",
          "105:   if (output->dims->data[1] == 5) {",
          "",
          "[Added Lines]",
          "70: void* Init(TfLiteContext* context, const char* buffer, size_t length) {",
          "71:   TFLITE_DCHECK(context->AllocatePersistentBuffer != nullptr);",
          "72:   return context->AllocatePersistentBuffer(context, sizeof(OpData));",
          "73: }",
          "79:   TFLITE_DCHECK(node->user_data != nullptr);",
          "80:   OpData* op_data = static_cast<OpData*>(node->user_data);",
          "84:   TF_LITE_ENSURE_EQ(context, input->dims->data[0], output->dims->data[0]);",
          "86:   TF_LITE_ENSURE_EQ(context, input->dims->data[2], output->dims->data[2]);",
          "97:   static int cb_prepare_count = 0;",
          "98:   cb_prepare_count++;",
          "106:   if (output->dims->data[1] == 5 || output->dims->data[1] == 13 ||",
          "107:       (cb_prepare_count == 5 && output->dims->data[2] == 2 &&",
          "108:        output->dims->data[3] == 96)) {",
          "110:     cb_prepare_count = 0;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "127:   TfLiteEvalTensor* output =",
          "128:       tflite::micro::GetEvalOutput(context, node, kOutputTensor);",
          "130:   OpData* data = reinterpret_cast<OpData*>(node->user_data);",
          "132:   int num_slots = output->dims->data[1];",
          "135:   if (input->type == kTfLiteInt8) {",
          "136:     EvalInt8(tflite::micro::GetTensorData<int8_t>(input), num_slots, depth,",
          "",
          "[Removed Lines]",
          "133:   int depth = output->dims->data[3];",
          "",
          "[Added Lines]",
          "134:   TFLITE_DCHECK(node->user_data != nullptr);",
          "138:   int depth = output->dims->data[2] * output->dims->data[3];",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "148:     return static_cast<TfLiteStatus>(kTfLiteAbort);",
          "149:   }",
          "157:   data->cycles_until_run = data->cycles_max;",
          "159:   return kTfLiteOk;",
          "",
          "[Removed Lines]",
          "155:   op_data_counter = 0;",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "162: }  // namespace circular_buffer",
          "164: TfLiteRegistration* Register_CIRCULAR_BUFFER() {",
          "",
          "[Removed Lines]",
          "165:   static TfLiteRegistration r = {/*init=*/nullptr,",
          "",
          "[Added Lines]",
          "164:   static TfLiteRegistration r = {/*init=*/circular_buffer::Init,",
          "",
          "---------------"
        ]
      }
    }
  ]
}