{
  "cve_id": "CVE-2021-41220",
  "cve_desc": "TensorFlow is an open source platform for machine learning. In affected versions the async implementation of `CollectiveReduceV2` suffers from a memory leak and a use after free. This occurs due to the asynchronous computation and the fact that objects that have been `std::move()`d from are still accessed. The fix will be included in TensorFlow 2.7.0. We will also cherrypick this commit on TensorFlow 2.6.1, as this version is the only one that is also affected.",
  "repo": "tensorflow/tensorflow",
  "patch_hash": "ca38dab9d3ee66c5de06f11af9a4b1200da5ef75",
  "patch_info": {
    "commit_hash": "ca38dab9d3ee66c5de06f11af9a4b1200da5ef75",
    "repo": "tensorflow/tensorflow",
    "commit_url": "https://github.com/tensorflow/tensorflow/commit/ca38dab9d3ee66c5de06f11af9a4b1200da5ef75",
    "files": [
      "tensorflow/core/kernels/collective_ops.cc",
      "tensorflow/python/kernel_tests/collective_ops_test.py"
    ],
    "message": "Fix undefined behavior in CollectiveReduceV2 and others\n\nWe should not call done after it's moved.\n\nPiperOrigin-RevId: 400838185\nChange-Id: Ifc979740054b8f8c6f4d50acc89472fe60c4fdb1",
    "before_after_code_files": [
      "tensorflow/core/kernels/collective_ops.cc||tensorflow/core/kernels/collective_ops.cc",
      "tensorflow/python/kernel_tests/collective_ops_test.py||tensorflow/python/kernel_tests/collective_ops_test.py"
    ]
  },
  "patch_diff": {
    "tensorflow/core/kernels/collective_ops.cc||tensorflow/core/kernels/collective_ops.cc": [
      "File: tensorflow/core/kernels/collective_ops.cc -> tensorflow/core/kernels/collective_ops.cc",
      "--- Hunk 1 ---",
      "[Context before]",
      "494:                               const Tensor& group_size, const Tensor& group_key,",
      "495:                               const Tensor& instance_key) {",
      "496:     if (group_size.dims() > 0) {",
      "499:     }",
      "500:     if (group_key.dims() > 0) {",
      "503:     }",
      "504:     if (instance_key.dims() > 0) {",
      "506:           \"Unexpected dimensions on input instance_key, got \",",
      "507:           instance_key.shape().DebugString());",
      "508:     }",
      "",
      "[Removed Lines]",
      "497:       return errors::Internal(\"Unexpected dimensions on input group_size, got \",",
      "498:                               group_size.shape().DebugString());",
      "501:       return errors::Internal(\"Unexpected dimensions on input group_key, got \",",
      "502:                               group_key.shape().DebugString());",
      "505:       return errors::Internal(",
      "",
      "[Added Lines]",
      "497:       return errors::InvalidArgument(",
      "498:           \"Unexpected dimensions on input group_size, got \",",
      "499:           group_size.shape().DebugString());",
      "502:       return errors::InvalidArgument(",
      "503:           \"Unexpected dimensions on input group_key, got \",",
      "504:           group_key.shape().DebugString());",
      "507:       return errors::InvalidArgument(",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "629:     col_params->instance.shape = c->input(0).shape();",
      "630:     col_params->merge_op = merge_op_.get();",
      "631:     col_params->final_op = final_op_.get();",
      "",
      "[Removed Lines]",
      "628:                          done);",
      "",
      "[Added Lines]",
      "630:                          done_with_cleanup);",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "856:   Status CheckInputs(Tensor group_size_t, Tensor group_key_t) {",
      "857:     if (group_size_t.dims() > 0) {",
      "859:           \"Unexpected dimensions on input group_size. \"",
      "860:           \"It shoulbe a scalar, got tensor with shape \",",
      "861:           group_size_t.shape().DebugString());",
      "862:     }",
      "863:     if (group_key_t.dims() > 0) {",
      "866:     }",
      "868:     auto group_size = group_size_t.unaligned_flat<int32>()(0);",
      "",
      "[Removed Lines]",
      "858:       return errors::Internal(",
      "864:       return errors::Internal(\"Unexpected dimensions on input group_key, got \",",
      "865:                               group_key_t.shape().DebugString());",
      "",
      "[Added Lines]",
      "860:       return errors::InvalidArgument(",
      "866:       return errors::InvalidArgument(",
      "867:           \"Unexpected dimensions on input group_key, got \",",
      "868:           group_key_t.shape().DebugString());",
      "",
      "---------------",
      "--- Hunk 4 ---",
      "[Context before]",
      "1084:     };",
      "1085:     core::RefCountPtr<CollectiveGroupResource> resource;",
      "1086:     OP_REQUIRES_OK_ASYNC(c, LookupResource(c, HandleFromInput(c, 1), &resource),",
      "1089:     Tensor group_assignment = c->input(2);",
      "",
      "[Removed Lines]",
      "1087:                          done);",
      "",
      "[Added Lines]",
      "1090:                          done_with_cleanup);",
      "",
      "---------------",
      "--- Hunk 5 ---",
      "[Context before]",
      "1134:     };",
      "1135:     core::RefCountPtr<CollectiveGroupResource> resource;",
      "1136:     OP_REQUIRES_OK_ASYNC(c, LookupResource(c, HandleFromInput(c, 1), &resource),",
      "1139:     Tensor group_assignment = c->input(2);",
      "",
      "[Removed Lines]",
      "1137:                          done);",
      "",
      "[Added Lines]",
      "1140:                          done_with_cleanup);",
      "",
      "---------------"
    ],
    "tensorflow/python/kernel_tests/collective_ops_test.py||tensorflow/python/kernel_tests/collective_ops_test.py": [
      "File: tensorflow/python/kernel_tests/collective_ops_test.py -> tensorflow/python/kernel_tests/collective_ops_test.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "1182:     self.assertAllEqual(self.evaluate(f()), [[3.], [3.]])",
      "1185: class CollectiveOpsV3Test(test.TestCase, parameterized.TestCase):",
      "1187:   def setUp(self):",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "1185: @combinations.generate(",
      "1186:     combinations.times(",
      "1187:         combinations.combine(collective_op=[",
      "1188:             combinations.NamedObject('all_reduce_v2',",
      "1189:                                      CollectiveOpsV2.all_reduce),",
      "1190:             combinations.NamedObject('all_gather_v2',",
      "1191:                                      CollectiveOpsV2.all_gather)",
      "1192:         ]), device_combination))",
      "1193: class InvalidInputTest(test.TestCase, parameterized.TestCase):",
      "1195:   def setUp(self):",
      "1196:     _setup_context()",
      "1197:     super().setUp()",
      "1199:   def testInvalidGroupKey(self, collective_op, device, communication):",
      "1200:     dev0 = '/device:%s:0' % device",
      "1201:     group_size = 2",
      "1202:     group_key = [100]",
      "1203:     instance_key = 100",
      "1204:     in_tensor = constant_op.constant([1.])",
      "1206:     with self.assertRaises(errors.InvalidArgumentError):",
      "1207:       with ops.device(dev0):",
      "1208:         collective_op(",
      "1209:             in_tensor,",
      "1210:             group_size,",
      "1211:             group_key,",
      "1212:             instance_key,",
      "1213:             communication_hint=communication)",
      "1215:   def testInvalidGroupSize(self, collective_op, device, communication):",
      "1216:     dev0 = '/device:%s:0' % device",
      "1217:     group_size = -2",
      "1218:     group_key = 100",
      "1219:     instance_key = 100",
      "1220:     in_tensor = constant_op.constant([1.])",
      "1222:     with self.assertRaises(errors.InvalidArgumentError):",
      "1223:       with ops.device(dev0):",
      "1224:         collective_op(",
      "1225:             in_tensor,",
      "1226:             group_size,",
      "1227:             group_key,",
      "1228:             instance_key,",
      "1229:             communication_hint=communication)",
      "1231:   def testInvalidInstanceKey(self, collective_op, device, communication):",
      "1232:     dev0 = '/device:%s:0' % device",
      "1233:     group_size = 2",
      "1234:     group_key = 100",
      "1235:     instance_key = [100]",
      "1236:     in_tensor = constant_op.constant([1.])",
      "1238:     with self.assertRaises(errors.InvalidArgumentError):",
      "1239:       with ops.device(dev0):",
      "1240:         collective_op(",
      "1241:             in_tensor,",
      "1242:             group_size,",
      "1243:             group_key,",
      "1244:             instance_key,",
      "1245:             communication_hint=communication)",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "f0844f406518c746eb732c4284f821f30c8f3a71",
      "candidate_info": {
        "commit_hash": "f0844f406518c746eb732c4284f821f30c8f3a71",
        "repo": "tensorflow/tensorflow",
        "commit_url": "https://github.com/tensorflow/tensorflow/commit/f0844f406518c746eb732c4284f821f30c8f3a71",
        "files": [
          "tensorflow/core/common_runtime/executor.cc",
          "tensorflow/core/kernels/collective_ops.cc",
          "tensorflow/python/distribute/cross_device_utils.py",
          "tensorflow/python/keras/distribute/distribute_strategy_test.py",
          "tensorflow/python/keras/engine/training.py",
          "tensorflow/python/kernel_tests/BUILD",
          "tensorflow/python/kernel_tests/collective_ops_test.py"
        ],
        "message": "Use cancellation manager to abort collectives\n\nWe used to always abort collective ops in executor when there're errors in graph execution. However there're some errors that are intended for the user to catch, and if we abort collective ops, the user program cannot continue. It's also not necessary\nto abort collective ops if there's no active ones.\n\nIdeally we should have a cancellation story for collectives. Before that, we can at least only abort collectives when it's necessary, i.e. when there're pending collective ops or failed collective ops.\n\nTo make the the catching EOF workflow work, we also need to make all collectives in gather depend on the input tensors, so there's better chance they fire after iterator GetNext. Without that the shape gathering may run in parallel with GetNext.\n\nPiperOrigin-RevId: 337440792\nChange-Id: I7caea917c858bcf99f6eb471abf46d94d5c255b3",
        "before_after_code_files": [
          "tensorflow/core/common_runtime/executor.cc||tensorflow/core/common_runtime/executor.cc",
          "tensorflow/core/kernels/collective_ops.cc||tensorflow/core/kernels/collective_ops.cc",
          "tensorflow/python/distribute/cross_device_utils.py||tensorflow/python/distribute/cross_device_utils.py",
          "tensorflow/python/keras/distribute/distribute_strategy_test.py||tensorflow/python/keras/distribute/distribute_strategy_test.py",
          "tensorflow/python/keras/engine/training.py||tensorflow/python/keras/engine/training.py",
          "tensorflow/python/kernel_tests/collective_ops_test.py||tensorflow/python/kernel_tests/collective_ops_test.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "tensorflow/core/kernels/collective_ops.cc||tensorflow/core/kernels/collective_ops.cc",
            "tensorflow/python/kernel_tests/collective_ops_test.py||tensorflow/python/kernel_tests/collective_ops_test.py"
          ],
          "candidate": [
            "tensorflow/core/kernels/collective_ops.cc||tensorflow/core/kernels/collective_ops.cc",
            "tensorflow/python/kernel_tests/collective_ops_test.py||tensorflow/python/kernel_tests/collective_ops_test.py"
          ]
        }
      },
      "candidate_diff": {
        "tensorflow/core/common_runtime/executor.cc||tensorflow/core/common_runtime/executor.cc": [
          "File: tensorflow/core/common_runtime/executor.cc -> tensorflow/core/common_runtime/executor.cc",
          "--- Hunk 1 ---",
          "[Context before]",
          "1119:       if (rendezvous_) {",
          "1120:         rendezvous_->StartAbort(s);",
          "1121:       }",
          "1125:       if (cancellation_manager_) {",
          "1126:         cancellation_manager_->StartCancel();",
          "1127:       }",
          "1128:     }",
          "",
          "[Removed Lines]",
          "1122:       if (collective_executor_) {",
          "1123:         collective_executor_->StartAbort(s);",
          "1124:       }",
          "",
          "[Added Lines]",
          "1124:       } else {",
          "1128:         if (collective_executor_) {",
          "1129:           collective_executor_->StartAbort(s);",
          "1130:         }",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1267:       if (rendezvous_) {",
          "1268:         rendezvous_->StartAbort(status);",
          "1269:       }",
          "1273:       if (cancellation_manager_) {",
          "1274:         cancellation_manager_->StartCancel();",
          "1275:       }",
          "1276:     }",
          "1277:     delete this;",
          "",
          "[Removed Lines]",
          "1270:       if (collective_executor_) {",
          "1271:         collective_executor_->StartAbort(status);",
          "1272:       }",
          "",
          "[Added Lines]",
          "1276:       } else {",
          "1280:         if (collective_executor_) {",
          "1281:           collective_executor_->StartAbort(status);",
          "1282:         }",
          "",
          "---------------"
        ],
        "tensorflow/core/kernels/collective_ops.cc||tensorflow/core/kernels/collective_ops.cc": [
          "File: tensorflow/core/kernels/collective_ops.cc -> tensorflow/core/kernels/collective_ops.cc",
          "--- Hunk 1 ---",
          "[Context before]",
          "52: class CollectiveOpKernel : public AsyncOpKernel {",
          "53:  public:",
          "",
          "[Removed Lines]",
          "54:   explicit CollectiveOpKernel(OpKernelConstruction* c) : AsyncOpKernel(c) {}",
          "",
          "[Added Lines]",
          "54:   explicit CollectiveOpKernel(OpKernelConstruction* c)",
          "55:       : AsyncOpKernel(c), name_(name()) {}",
          "57:   void ComputeAsync(OpKernelContext* c, DoneCallback done) override {",
          "58:     CollectiveExecutor* col_exec = c->collective_executor();",
          "59:     OP_REQUIRES_ASYNC(",
          "60:         c, col_exec,",
          "61:         errors::Internal(",
          "62:             \"Failed to get CollectiveExecutor from OpKernelContext for Op \",",
          "63:             name_),",
          "64:         done);",
          "65:     CancellationToken token =",
          "66:         c->cancellation_manager()->get_cancellation_token();",
          "67:     bool cancel_registered =",
          "68:         c->cancellation_manager()->RegisterCallback(token, [col_exec]() {",
          "71:           col_exec->RunClosure([col_exec]() {",
          "72:             col_exec->StartAbort(errors::Cancelled(\"op cancelled\"));",
          "73:           });",
          "74:         });",
          "75:     OP_REQUIRES_ASYNC(c, cancel_registered,",
          "76:                       errors::Cancelled(\"op cancelled \", name_), done);",
          "78:     auto deregister_and_done = [c, col_exec, token, done = std::move(done)]() {",
          "79:       c->cancellation_manager()->DeregisterCallback(token);",
          "82:       if (!c->status().ok()) {",
          "83:         col_exec->StartAbort(c->status());",
          "84:       }",
          "85:       done();",
          "86:     };",
          "87:     ComputeAsyncImpl(c, col_exec, std::move(deregister_and_done));",
          "88:   }",
          "90:  protected:",
          "91:   virtual void ComputeAsyncImpl(OpKernelContext* c,",
          "92:                                 CollectiveExecutor* col_exec,",
          "93:                                 DoneCallback done) = 0;",
          "95:   string name_;",
          "96: };",
          "98: class CollectiveOpV1Kernel : public CollectiveOpKernel {",
          "99:  public:",
          "100:   explicit CollectiveOpV1Kernel(OpKernelConstruction* c)",
          "101:       : CollectiveOpKernel(c) {}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "90:     return true;",
          "91:   }",
          "93:   CollectiveParams col_params_;",
          "94:   std::vector<int32> dependencies_;",
          "95: };",
          "98:  public:",
          "99:   explicit CollectiveGatherOpKernel(OpKernelConstruction* c)",
          "101:     col_params_.instance.type = GATHER_COLLECTIVE;",
          "102:     OP_REQUIRES_OK(c, c->GetAttr(\"group_size\", &col_params_.group.group_size));",
          "103:     OP_REQUIRES(",
          "",
          "[Removed Lines]",
          "97: class CollectiveGatherOpKernel : public CollectiveOpKernel {",
          "100:       : CollectiveOpKernel(c) {",
          "",
          "[Added Lines]",
          "140:  protected:",
          "145: class CollectiveGatherOpKernel : public CollectiveOpV1Kernel {",
          "148:       : CollectiveOpV1Kernel(c) {",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "119:     col_params_.group.device_type = c->device_type();",
          "120:   }",
          "131:     auto output_shape = c->input(0).shape();",
          "132:     output_shape.set_dim(",
          "133:         0, output_shape.dim_size(0) * col_params_.group.group_size);",
          "",
          "[Removed Lines]",
          "122:   void ComputeAsync(OpKernelContext* c, DoneCallback done) override {",
          "123:     CollectiveExecutor* col_exec = c->collective_executor();",
          "124:     OP_REQUIRES_ASYNC(",
          "125:         c, col_exec,",
          "126:         errors::Internal(",
          "127:             \"Failed to get CollectiveExecutor from OpKernelContext for Op \",",
          "128:             col_params_.name),",
          "129:         done);",
          "",
          "[Added Lines]",
          "170:  protected:",
          "171:   void ComputeAsyncImpl(OpKernelContext* c, CollectiveExecutor* col_exec,",
          "172:                         DoneCallback done) override {",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "171: REGISTER_KERNEL_BUILDER(Name(\"CollectiveGather\").Device(DEVICE_GPU),",
          "172:                         CollectiveGatherOpKernel);",
          "175:  public:",
          "176:   explicit CollectiveReduceOpKernel(OpKernelConstruction* c)",
          "178:     col_params_.instance.type = REDUCTION_COLLECTIVE;",
          "179:     OP_REQUIRES_OK(c, c->GetAttr(\"group_size\", &col_params_.group.group_size));",
          "180:     OP_REQUIRES(",
          "",
          "[Removed Lines]",
          "174: class CollectiveReduceOpKernel : public CollectiveOpKernel {",
          "177:       : CollectiveOpKernel(c) {",
          "",
          "[Added Lines]",
          "216: class CollectiveReduceOpKernel : public CollectiveOpV1Kernel {",
          "219:       : CollectiveOpV1Kernel(c) {",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "231:     col_params_.final_op = BuildOpKernel(c, final_op_name, &sub_node);",
          "232:   }",
          "",
          "[Removed Lines]",
          "234:   void ComputeAsync(OpKernelContext* c, DoneCallback done) override {",
          "235:     CollectiveExecutor* col_exec = c->collective_executor();",
          "236:     OP_REQUIRES_ASYNC(",
          "237:         c, col_exec,",
          "238:         errors::Internal(",
          "239:             \"Failed to get CollectiveExecutor from OpKernelContext for Op \",",
          "240:             col_params_.name),",
          "241:         done);",
          "",
          "[Added Lines]",
          "276:  protected:",
          "277:   void ComputeAsyncImpl(OpKernelContext* c, CollectiveExecutor* col_exec,",
          "278:                         DoneCallback done) override {",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "280: REGISTER_KERNEL_BUILDER(Name(\"CollectiveReduce\").Device(DEVICE_GPU),",
          "281:                         CollectiveReduceOpKernel);",
          "284:  public:",
          "285:   explicit CollectiveBcastSendOpKernel(OpKernelConstruction* c)",
          "287:     col_params_.instance.type = BROADCAST_COLLECTIVE;",
          "288:     OP_REQUIRES_OK(c, c->GetAttr(\"group_size\", &col_params_.group.group_size));",
          "289:     OP_REQUIRES(",
          "",
          "[Removed Lines]",
          "283: class CollectiveBcastSendOpKernel : public CollectiveOpKernel {",
          "286:       : CollectiveOpKernel(c) {",
          "",
          "[Added Lines]",
          "320: class CollectiveBcastSendOpKernel : public CollectiveOpV1Kernel {",
          "323:       : CollectiveOpV1Kernel(c) {",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "309:     col_params_.group.device_type = c->device_type();",
          "310:   }",
          "",
          "[Removed Lines]",
          "312:   void ComputeAsync(OpKernelContext* c, DoneCallback done) override {",
          "313:     CollectiveExecutor* col_exec = c->collective_executor();",
          "314:     OP_REQUIRES_ASYNC(",
          "315:         c, col_exec,",
          "316:         errors::Internal(",
          "317:             \"Failed to get CollectiveExecutor from OpKernelContext for Op \",",
          "318:             col_params_.name),",
          "319:         done);",
          "",
          "[Added Lines]",
          "349:  protected:",
          "350:   void ComputeAsyncImpl(OpKernelContext* c, CollectiveExecutor* col_exec,",
          "351:                         DoneCallback done) override {",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "362: REGISTER_KERNEL_BUILDER(Name(\"CollectiveBcastSend\").Device(DEVICE_GPU),",
          "363:                         CollectiveBcastSendOpKernel);",
          "366:  public:",
          "367:   explicit CollectiveBcastRecvOpKernel(OpKernelConstruction* c)",
          "369:     col_params_.instance.type = BROADCAST_COLLECTIVE;",
          "370:     OP_REQUIRES_OK(c, c->GetAttr(\"group_size\", &col_params_.group.group_size));",
          "371:     OP_REQUIRES(",
          "",
          "[Removed Lines]",
          "365: class CollectiveBcastRecvOpKernel : public CollectiveOpKernel {",
          "368:       : CollectiveOpKernel(c) {",
          "",
          "[Added Lines]",
          "397: class CollectiveBcastRecvOpKernel : public CollectiveOpV1Kernel {",
          "400:       : CollectiveOpV1Kernel(c) {",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "391:     col_params_.group.device_type = c->device_type();",
          "392:   }",
          "",
          "[Removed Lines]",
          "394:   void ComputeAsync(OpKernelContext* c, DoneCallback done) override {",
          "395:     CollectiveExecutor* col_exec = c->collective_executor();",
          "396:     OP_REQUIRES_ASYNC(",
          "397:         c, col_exec,",
          "398:         errors::Internal(",
          "399:             \"Failed to get CollectiveExecutor from OpKernelContext for Op \",",
          "400:             col_params_.name),",
          "401:         done);",
          "",
          "[Added Lines]",
          "426:  protected:",
          "427:   void ComputeAsyncImpl(OpKernelContext* c, CollectiveExecutor* col_exec,",
          "428:                         DoneCallback done) override {",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "437: REGISTER_KERNEL_BUILDER(Name(\"CollectiveBcastRecv\").Device(DEVICE_GPU),",
          "438:                         CollectiveBcastRecvOpKernel);",
          "441:  public:",
          "442:   explicit CollectiveReduceV2OpKernel(OpKernelConstruction* c)",
          "444:     col_params_ = std::make_shared<CollectiveParams>();",
          "445:     OP_REQUIRES_OK(c, c->GetAttr(\"T\", &col_params_->instance.data_type));",
          "446:     string merge_op_name;",
          "",
          "[Removed Lines]",
          "440: class CollectiveReduceV2OpKernel : public AsyncOpKernel {",
          "443:       : AsyncOpKernel(c) {",
          "",
          "[Added Lines]",
          "467: class CollectiveReduceV2OpKernel : public CollectiveOpKernel {",
          "470:       : CollectiveOpKernel(c) {",
          "",
          "---------------",
          "--- Hunk 11 ---",
          "[Context before]",
          "481:             << col_params_->instance.impl_details.communication_hint;",
          "482:   }",
          "492:     const Tensor& input = c->input(0);",
          "493:     const Tensor& group_size = c->input(1);",
          "494:     const Tensor& group_key = c->input(2);",
          "",
          "[Removed Lines]",
          "484:   void ComputeAsync(OpKernelContext* c, DoneCallback done) override {",
          "485:     CollectiveExecutor* col_exec = c->collective_executor();",
          "486:     OP_REQUIRES_ASYNC(",
          "487:         c, col_exec,",
          "488:         errors::Internal(",
          "489:             \"Failed to get CollectiveExecutor from OpKernelContext for Op \",",
          "490:             col_params_->name),",
          "491:         done);",
          "",
          "[Added Lines]",
          "511:  protected:",
          "512:   void ComputeAsyncImpl(OpKernelContext* c, CollectiveExecutor* col_exec,",
          "513:                         DoneCallback done) override {",
          "",
          "---------------",
          "--- Hunk 12 ---",
          "[Context before]",
          "590:                             .HostMemory(\"instance_key\"),",
          "591:                         CollectiveReduceV2OpKernel);",
          "594:  public:",
          "595:   explicit CollectiveGatherV2OpKernel(OpKernelConstruction* c)",
          "597:     OP_REQUIRES_OK(c, c->GetAttr(\"T\", &data_type_));",
          "598:     OP_REQUIRES_OK(c, c->GetAttr(\"communication_hint\", &communication_hint_));",
          "599:     OP_REQUIRES_OK(c, c->GetAttr(\"timeout_seconds\", &timeout_seconds_));",
          "",
          "[Removed Lines]",
          "593: class CollectiveGatherV2OpKernel : public AsyncOpKernel {",
          "596:       : AsyncOpKernel(c), device_type_(DEVICE_DEFAULT) {",
          "",
          "[Added Lines]",
          "615: class CollectiveGatherV2OpKernel : public CollectiveOpKernel {",
          "618:       : CollectiveOpKernel(c), device_type_(DEVICE_DEFAULT) {",
          "",
          "---------------",
          "--- Hunk 13 ---",
          "[Context before]",
          "603:             << \" communication_hint \" << communication_hint_;",
          "604:   }",
          "614:     const Tensor& input = c->input(0);",
          "615:     const Tensor& group_size = c->input(1);",
          "616:     const Tensor& group_key = c->input(2);",
          "",
          "[Removed Lines]",
          "606:   void ComputeAsync(OpKernelContext* c, DoneCallback done) override {",
          "607:     CollectiveExecutor* col_exec = c->collective_executor();",
          "608:     OP_REQUIRES_ASYNC(",
          "609:         c, col_exec,",
          "610:         errors::Internal(",
          "611:             \"Failed to get CollectiveExecutor from OpKernelContext for Op \",",
          "612:             name_),",
          "613:         done);",
          "",
          "[Added Lines]",
          "628:  protected:",
          "629:   void ComputeAsyncImpl(OpKernelContext* c, CollectiveExecutor* col_exec,",
          "630:                         DoneCallback done) override {",
          "",
          "---------------",
          "--- Hunk 14 ---",
          "[Context before]",
          "712:   string communication_hint_;",
          "713:   float timeout_seconds_;",
          "714:   DeviceType device_type_;",
          "716: };",
          "718: REGISTER_KERNEL_BUILDER(Name(\"CollectiveGatherV2\").Device(DEVICE_CPU),",
          "",
          "[Removed Lines]",
          "715:   string name_;",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "tensorflow/python/distribute/cross_device_utils.py||tensorflow/python/distribute/cross_device_utils.py": [
          "File: tensorflow/python/distribute/cross_device_utils.py -> tensorflow/python/distribute/cross_device_utils.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "412:         self._group_key, self._device)",
          "413:     instance_key_shape = self._collective_keys.get_instance_key(",
          "414:         self._group_key, self._device)",
          "416:       # 1. Transpose",
          "417:       # E.g. Given an input_tensor with shape [2,2,5,1] and axis to gather is 3,",
          "418:       # we use perm_pre=[3 0 1 2] to reshape it to [1,2,2,5], which",
          "",
          "[Removed Lines]",
          "415:     with ops.device(self._device):",
          "",
          "[Added Lines]",
          "415:     with ops.device(self._device), \\",
          "416:          ops.control_dependencies([array_ops.identity(input_tensor)]):",
          "",
          "---------------"
        ],
        "tensorflow/python/keras/distribute/distribute_strategy_test.py||tensorflow/python/keras/distribute/distribute_strategy_test.py": [
          "File: tensorflow/python/keras/distribute/distribute_strategy_test.py -> tensorflow/python/keras/distribute/distribute_strategy_test.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "29: from tensorflow.python.data.ops import dataset_ops",
          "30: from tensorflow.python.data.ops import readers",
          "31: from tensorflow.python.distribute import central_storage_strategy",
          "33: from tensorflow.python.distribute import combinations as ds_combinations",
          "34: from tensorflow.python.distribute import distribution_strategy_context",
          "35: from tensorflow.python.distribute import mirrored_strategy",
          "",
          "[Removed Lines]",
          "32: from tensorflow.python.distribute import collective_all_reduce_strategy",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1151:     if mode == 'graph' and _is_tpu_strategy(distribution):",
          "1152:       self.skipTest('partial batch not supported with TPU in graph mode.')",
          "1157:     with self.cached_session():",
          "1158:       with distribution.scope():",
          "1159:         optimizer_fn = gradient_descent_keras.SGD",
          "",
          "[Removed Lines]",
          "1154:     if isinstance(distribution,",
          "1155:                   collective_all_reduce_strategy.CollectiveAllReduceStrategy):",
          "1156:       self.skipTest('EOF error causes subsequent collective ops fail.')",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "1166:             loss,",
          "1167:             metrics=metrics)",
          "1171:       # steps/steps_per_epoch are calculated when using numpy arrays as",
          "1172:       # input data.",
          "1173:       fit_with_numpy = model.fit(",
          "",
          "[Removed Lines]",
          "1169:       inputs = np.zeros((1000, 3), dtype=np.float32)",
          "1170:       targets = np.zeros((1000, 4), dtype=np.float32)",
          "",
          "[Added Lines]",
          "1165:       inputs = np.zeros((10, 3), dtype=np.float32)",
          "1166:       targets = np.zeros((10, 4), dtype=np.float32)",
          "",
          "---------------"
        ],
        "tensorflow/python/keras/engine/training.py||tensorflow/python/keras/engine/training.py": [
          "File: tensorflow/python/keras/engine/training.py -> tensorflow/python/keras/engine/training.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "2733: def _multi_worker_concat(v, strategy):",
          "2734:   \"\"\"Order PerReplica objects for CollectiveAllReduceStrategy and concat.\"\"\"",
          "2735:   replicas = strategy._gather(v, axis=0)  # pylint: disable=protected-access",
          "2750:   replicas = array_ops.split(",
          "2751:       replicas,",
          "",
          "[Removed Lines]",
          "2736:   # v might not have the same shape on different replicas",
          "2737:   if isinstance(v, ds_values.PerReplica):",
          "2738:     shapes = array_ops.concat([",
          "2739:         array_ops.expand_dims_v2(array_ops.shape(single_value)[0], axis=0)",
          "2740:         for single_value in v.values",
          "2741:     ],",
          "2742:                               axis=0)",
          "2743:     all_shapes = strategy._gather(shapes, axis=0)  # pylint: disable=protected-access",
          "2744:   else:",
          "2745:     # v is a tensor. This may happen when, say, we have 2x1 multi-worker.",
          "2746:     all_shapes = strategy._gather(  # pylint: disable=protected-access",
          "2747:         array_ops.expand_dims_v2(array_ops.shape(v)[0], axis=0),",
          "2748:         axis=0)",
          "",
          "[Added Lines]",
          "2736:   # TODO(b/170435030): We now need to make sure these run after the iterator",
          "2737:   # GetNext, so that we don't trigger aborting collective ops in the case of",
          "2738:   # EOF. Remove after the issue is fixed.",
          "2739:   with ops.control_dependencies([replicas]):",
          "2740:     # v might not have the same shape on different replicas",
          "2741:     if isinstance(v, ds_values.PerReplica):",
          "2742:       shapes = array_ops.concat([",
          "2743:           array_ops.expand_dims_v2(array_ops.shape(single_value)[0], axis=0)",
          "2744:           for single_value in v.values",
          "2745:       ],",
          "2746:                                 axis=0)",
          "2747:       all_shapes = strategy._gather(shapes, axis=0)  # pylint: disable=protected-access",
          "2748:     else:",
          "2749:       # v is a tensor. This may happen when, say, we have 2x1 multi-worker.",
          "2750:       all_shapes = strategy._gather(  # pylint: disable=protected-access",
          "2751:           array_ops.expand_dims_v2(array_ops.shape(v)[0], axis=0),",
          "2752:           axis=0)",
          "",
          "---------------"
        ],
        "tensorflow/python/kernel_tests/collective_ops_test.py||tensorflow/python/kernel_tests/collective_ops_test.py": [
          "File: tensorflow/python/kernel_tests/collective_ops_test.py -> tensorflow/python/kernel_tests/collective_ops_test.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "24: from absl.testing import parameterized",
          "26: from tensorflow.python.compat import v2_compat",
          "27: from tensorflow.python.distribute import combinations",
          "28: from tensorflow.python.distribute import test_util",
          "29: from tensorflow.python.eager import context",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "27: from tensorflow.python.data.experimental.ops import testing as dataset_testing",
          "28: from tensorflow.python.data.ops import dataset_ops",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "469:     _setup_context()",
          "470:     def_function.function(collective_fn)()",
          "473: @combinations.generate(",
          "474:     combinations.times(",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "474:   def testOpErrorNotAbort(self, collective_op, device, communication):",
          "475:     # Do not abort if there's no active collective ops. There could be",
          "476:     # exceptions like EOF which we expect users to catch, aborting collective",
          "477:     # ops on all op errors intervenes with this workflow.",
          "478:     dev0 = '/device:%s:0' % device",
          "479:     dev1 = '/device:%s:1' % device",
          "480:     group_size = 2",
          "481:     group_key = 100",
          "482:     instance_key = 100",
          "483:     dataset = dataset_ops.Dataset.from_tensors([1.])",
          "485:     @def_function.function",
          "486:     def collective_fn(in_tensor):",
          "487:       for device in [dev0, dev1]:",
          "488:         with ops.device(device):",
          "489:           collective_op(",
          "490:               in_tensor,",
          "491:               group_size,",
          "492:               group_key,",
          "493:               instance_key,",
          "494:               communication_hint=communication)",
          "496:     @def_function.function",
          "497:     def f():",
          "498:       iterator = iter(dataset)",
          "499:       collective_fn(next(iterator))",
          "500:       # This next(iterator) should raise EOF.",
          "501:       collective_fn(next(iterator))",
          "503:     with self.assertRaises(errors.OutOfRangeError):",
          "504:       f()",
          "505:     collective_fn(constant_op.constant([1.]))",
          "507:   def testOpErrorAbort(self, collective_op, device, communication):",
          "508:     # Abort collective ops if there're active collective ops at the time of an",
          "509:     # op error. This is due to the inability to cancel collective ops, and op",
          "510:     # errors may cause running collective ops to hang.",
          "511:     dev0 = '/device:%s:0' % device",
          "512:     group_size = 2",
          "513:     group_key = 100",
          "514:     instance_key = 100",
          "515:     in_tensor = constant_op.constant([1.])",
          "516:     # Make the dataset sleep a while so that the collective is being executed",
          "517:     # when the EOF happens.",
          "518:     dataset = dataset_ops.Dataset.from_tensors([1.]).apply(",
          "519:         dataset_testing.sleep(sleep_microseconds=200))",
          "521:     @def_function.function",
          "522:     def f():",
          "523:       # Launch a collective op that won't be able to finish to test abortion",
          "524:       # when other ops error.",
          "525:       with ops.device(dev0):",
          "526:         ret = collective_op(",
          "527:             in_tensor,",
          "528:             group_size,",
          "529:             group_key,",
          "530:             instance_key,",
          "531:             communication_hint=communication)",
          "532:       iterator = iter(dataset)",
          "533:       next(iterator)",
          "534:       # This should raise EOF.",
          "535:       next(iterator)",
          "536:       return ret",
          "538:     with self.assertRaises(errors.OutOfRangeError):",
          "539:       f()",
          "540:     # Now collective ops is aborted, subsequent collective ops should fail with",
          "541:     # the previous error.",
          "542:     with self.assertRaises(errors.CancelledError):",
          "543:       with ops.device(dev0):",
          "544:         collective_op(",
          "545:             in_tensor,",
          "546:             group_size,",
          "547:             group_key,",
          "548:             instance_key,",
          "549:             communication_hint=communication)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "861764c4067afdde8a571883c3e4b78409bb2075",
      "candidate_info": {
        "commit_hash": "861764c4067afdde8a571883c3e4b78409bb2075",
        "repo": "tensorflow/tensorflow",
        "commit_url": "https://github.com/tensorflow/tensorflow/commit/861764c4067afdde8a571883c3e4b78409bb2075",
        "files": [
          "tensorflow/core/common_runtime/base_collective_executor.cc",
          "tensorflow/core/common_runtime/ring_alg.cc",
          "tensorflow/core/kernels/collective_ops.cc",
          "tensorflow/core/nccl/collective_communicator.cc",
          "tensorflow/core/nccl/nccl_manager.cc",
          "tensorflow/python/kernel_tests/collective_ops_test.py"
        ],
        "message": "Rely on cancellation in collective V2 kernels\n\nFor collective v2 kernels we stop aborting collective ops if they're cancelled.\nMost componenets except param resolution is able to respond to cancellation.\nParam resolution is not large concern in practice, since group resolution is\nlikely not needed, and most instance resolution do not block.\n\nTechnically we can do this to v1 kernels as well, but it doesn't seem safe since\nwe reuse instance keys in v1 collectives.\n\nPiperOrigin-RevId: 338508607\nChange-Id: Iab2f4e1061d7b384b83bc2712b849e42ba3677fc",
        "before_after_code_files": [
          "tensorflow/core/common_runtime/base_collective_executor.cc||tensorflow/core/common_runtime/base_collective_executor.cc",
          "tensorflow/core/common_runtime/ring_alg.cc||tensorflow/core/common_runtime/ring_alg.cc",
          "tensorflow/core/kernels/collective_ops.cc||tensorflow/core/kernels/collective_ops.cc",
          "tensorflow/core/nccl/collective_communicator.cc||tensorflow/core/nccl/collective_communicator.cc",
          "tensorflow/core/nccl/nccl_manager.cc||tensorflow/core/nccl/nccl_manager.cc",
          "tensorflow/python/kernel_tests/collective_ops_test.py||tensorflow/python/kernel_tests/collective_ops_test.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "tensorflow/core/kernels/collective_ops.cc||tensorflow/core/kernels/collective_ops.cc",
            "tensorflow/python/kernel_tests/collective_ops_test.py||tensorflow/python/kernel_tests/collective_ops_test.py"
          ],
          "candidate": [
            "tensorflow/core/kernels/collective_ops.cc||tensorflow/core/kernels/collective_ops.cc",
            "tensorflow/python/kernel_tests/collective_ops_test.py||tensorflow/python/kernel_tests/collective_ops_test.py"
          ]
        }
      },
      "candidate_diff": {
        "tensorflow/core/common_runtime/base_collective_executor.cc||tensorflow/core/common_runtime/base_collective_executor.cc": [
          "File: tensorflow/core/common_runtime/base_collective_executor.cc -> tensorflow/core/common_runtime/base_collective_executor.cc",
          "--- Hunk 1 ---",
          "[Context before]",
          "23: #include \"tensorflow/core/common_runtime/dma_helper.h\"",
          "24: #include \"tensorflow/core/common_runtime/process_util.h\"",
          "25: #include \"tensorflow/core/framework/allocator.h\"",
          "26: #include \"tensorflow/core/framework/op_kernel.h\"",
          "27: #include \"tensorflow/core/framework/tensor.h\"",
          "28: #include \"tensorflow/core/framework/tensor_shape.h\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "26: #include \"tensorflow/core/framework/cancellation.h\"",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "42: #define VALUE_IN_DEBUG_STRING false",
          "44: namespace tensorflow {",
          "46: int64 CollectiveAdapter::AlignedChunkElts(int64 elt_bytes, int64 total_elts,",
          "47:                                           int64 num_chunks) {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "47: namespace {",
          "48: bool IsCancelled(CancellationManager* cancel_mgr) {",
          "49:   return cancel_mgr != nullptr &&",
          "50:          (cancel_mgr->IsCancelled() || cancel_mgr->IsCancelling());",
          "51: }",
          "52: }  // namespace",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "215: BaseCollectiveExecutor::~BaseCollectiveExecutor() {}",
          "217: void BaseCollectiveExecutor::StartAbort(const Status& s) {",
          "219:   Status status;",
          "220:   {",
          "221:     mutex_lock l(status_mu_);",
          "222:     if (!status_.ok()) {",
          "226:       return;",
          "227:     }",
          "228:     status_ = StatusGroup::MakeDerived(Status(",
          "",
          "[Removed Lines]",
          "218:   VLOG(1) << \"BaseCollectiveExecutor::StartAbort \" << s;",
          "223:       LOG(WARNING)",
          "224:           << \"BaseCollectiveExecutor already aborted, ignoring StartAbort: \"",
          "225:           << s;",
          "",
          "[Added Lines]",
          "231:       VLOG(2) << \"BaseCollectiveExecutor already aborted, ignoring StartAbort: \"",
          "232:               << s;",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "233:             \"program to reset.\")));",
          "234:     status = status_;",
          "235:   }",
          "236:   cem_->GetParamResolver()->StartAbort(status);",
          "237:   remote_access_->StartAbort(status);",
          "238:   if (cem_->GetNcclCommunicator() != nullptr) {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "243:   LOG(ERROR) << \"BaseCollectiveExecutor::StartAbort \" << s;",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "261:                                           StatusCallback done) {",
          "263:   const auto is_callback_called = std::make_shared<std::atomic<bool>>(false);",
          "265:     bool called = is_callback_called->exchange(true);",
          "266:     if (!called) {",
          "267:       done(GetStatus(s));",
          "268:     }",
          "269:   };",
          "",
          "[Removed Lines]",
          "264:   auto done_safe = [this, done, is_callback_called](const Status& s) {",
          "",
          "[Added Lines]",
          "272:   auto done_safe = [this, done, ctx, is_callback_called](const Status& s) {",
          "275:       if (!s.ok() && !IsCancelled(ctx->cancellation_manager())) {",
          "278:         StartAbort(s);",
          "279:       }",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "343:   const auto is_callback_called = std::make_shared<std::atomic<bool>>(false);",
          "345:     bool called = is_callback_called->exchange(true);",
          "346:     if (!called) {",
          "347:       done(GetStatus(s));",
          "348:     }",
          "349:   };",
          "",
          "[Removed Lines]",
          "344:   auto done_safe = [this, is_callback_called, done](const Status& s) {",
          "",
          "[Added Lines]",
          "357:   auto done_safe = [this, is_callback_called, cancel_mgr,",
          "358:                     done](const Status& s) {",
          "361:       if (!s.ok() && !IsCancelled(cancel_mgr)) {",
          "364:         StartAbort(s);",
          "365:       }",
          "",
          "---------------"
        ],
        "tensorflow/core/common_runtime/ring_alg.cc||tensorflow/core/common_runtime/ring_alg.cc": [
          "File: tensorflow/core/common_runtime/ring_alg.cc -> tensorflow/core/common_runtime/ring_alg.cc",
          "--- Hunk 1 ---",
          "[Context before]",
          "278:       status_.Update(s);",
          "279:     }",
          "280:   }",
          "285:   if (abort_started) {",
          "287:   }",
          "288: }",
          "",
          "[Removed Lines]",
          "286:     col_ctx_->col_exec->StartAbort(s);",
          "",
          "[Added Lines]",
          "287:     if (col_ctx_->op_ctx->cancellation_manager() == nullptr ||",
          "288:         (!col_ctx_->op_ctx->cancellation_manager()->IsCancelled() &&",
          "289:          !col_ctx_->op_ctx->cancellation_manager()->IsCancelling())) {",
          "290:       col_ctx_->col_exec->StartAbort(s);",
          "291:     }",
          "",
          "---------------"
        ],
        "tensorflow/core/kernels/collective_ops.cc||tensorflow/core/kernels/collective_ops.cc": [
          "File: tensorflow/core/kernels/collective_ops.cc -> tensorflow/core/kernels/collective_ops.cc",
          "--- Hunk 1 ---",
          "[Context before]",
          "49:   return k;",
          "50: }",
          "53:  public:",
          "55:       : AsyncOpKernel(c), name_(name()) {}",
          "57:   void ComputeAsync(OpKernelContext* c, DoneCallback done) override {",
          "",
          "[Removed Lines]",
          "52: class CollectiveOpKernel : public AsyncOpKernel {",
          "54:   explicit CollectiveOpKernel(OpKernelConstruction* c)",
          "",
          "[Added Lines]",
          "52: class CollectiveOpV1Kernel : public AsyncOpKernel {",
          "54:   explicit CollectiveOpV1Kernel(OpKernelConstruction* c)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "81:       c->cancellation_manager()->TryDeregisterCallback(token);",
          "87:       done();",
          "88:     };",
          "89:     ComputeAsyncImpl(c, col_exec, std::move(deregister_and_done));",
          "90:   }",
          "107:   string GetCollectiveKey(OpKernelContext* c) {",
          "",
          "[Removed Lines]",
          "84:       if (!c->status().ok()) {",
          "85:         col_exec->StartAbort(c->status());",
          "86:       }",
          "92:  protected:",
          "93:   virtual void ComputeAsyncImpl(OpKernelContext* c,",
          "94:                                 CollectiveExecutor* col_exec,",
          "95:                                 DoneCallback done) = 0;",
          "97:   string name_;",
          "98: };",
          "100: class CollectiveOpV1Kernel : public CollectiveOpKernel {",
          "101:  public:",
          "102:   explicit CollectiveOpV1Kernel(OpKernelConstruction* c)",
          "103:       : CollectiveOpKernel(c) {}",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "140:   }",
          "142:  protected:",
          "143:   CollectiveParams col_params_;",
          "144:   std::vector<int32> dependencies_;",
          "145: };",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "125:   virtual void ComputeAsyncImpl(OpKernelContext* c,",
          "126:                                 CollectiveExecutor* col_exec,",
          "127:                                 DoneCallback done) = 0;",
          "129:   string name_;",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "470: REGISTER_KERNEL_BUILDER(Name(\"CollectiveBcastRecv\").Device(DEVICE_GPU),",
          "471:                         CollectiveBcastRecvOpKernel);",
          "474:  public:",
          "475:   explicit CollectiveReduceV2OpKernel(OpKernelConstruction* c)",
          "477:     OP_REQUIRES_OK(c, c->GetAttr(\"T\", &data_type_));",
          "478:     string merge_op_name;",
          "479:     OP_REQUIRES_OK(c, c->GetAttr(\"merge_op\", &merge_op_name));",
          "",
          "[Removed Lines]",
          "473: class CollectiveReduceV2OpKernel : public CollectiveOpKernel {",
          "476:       : CollectiveOpKernel(c), device_type_(DEVICE_DEFAULT) {",
          "",
          "[Added Lines]",
          "460: class CollectiveReduceV2OpKernel : public AsyncOpKernel {",
          "463:       : AsyncOpKernel(c), device_type_(DEVICE_DEFAULT) {",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "504:             << \" communication_hint \" << communication_hint_;",
          "505:   }",
          "510:     const Tensor& input = c->input(0);",
          "511:     const Tensor& group_size = c->input(1);",
          "512:     const Tensor& group_key = c->input(2);",
          "",
          "[Removed Lines]",
          "507:  protected:",
          "508:   void ComputeAsyncImpl(OpKernelContext* c, CollectiveExecutor* col_exec,",
          "509:                         DoneCallback done) override {",
          "",
          "[Added Lines]",
          "494:   void ComputeAsync(OpKernelContext* c, DoneCallback done) override {",
          "495:     CollectiveExecutor* col_exec = c->collective_executor();",
          "496:     OP_REQUIRES_ASYNC(",
          "497:         c, col_exec,",
          "498:         errors::Internal(",
          "499:             \"Failed to get CollectiveExecutor from OpKernelContext for Op \",",
          "500:             name_),",
          "501:         done);",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "597:   }",
          "599:  private:",
          "600:   DataType data_type_ = DT_INVALID;",
          "601:   string communication_hint_;",
          "602:   float timeout_seconds_ = 0;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "592:   string name_;",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "614:                             .HostMemory(\"instance_key\"),",
          "615:                         CollectiveReduceV2OpKernel);",
          "618:  public:",
          "619:   explicit CollectiveGatherV2OpKernel(OpKernelConstruction* c)",
          "621:     OP_REQUIRES_OK(c, c->GetAttr(\"T\", &data_type_));",
          "622:     OP_REQUIRES_OK(c, c->GetAttr(\"communication_hint\", &communication_hint_));",
          "623:     OP_REQUIRES_OK(c, c->GetAttr(\"timeout_seconds\", &timeout_seconds_));",
          "",
          "[Removed Lines]",
          "617: class CollectiveGatherV2OpKernel : public CollectiveOpKernel {",
          "620:       : CollectiveOpKernel(c), device_type_(DEVICE_DEFAULT) {",
          "",
          "[Added Lines]",
          "610: class CollectiveGatherV2OpKernel : public AsyncOpKernel {",
          "613:       : AsyncOpKernel(c), device_type_(DEVICE_DEFAULT) {",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "627:             << \" communication_hint \" << communication_hint_;",
          "628:   }",
          "633:     const Tensor& input = c->input(0);",
          "634:     const Tensor& group_size = c->input(1);",
          "635:     const Tensor& group_key = c->input(2);",
          "",
          "[Removed Lines]",
          "630:  protected:",
          "631:   void ComputeAsyncImpl(OpKernelContext* c, CollectiveExecutor* col_exec,",
          "632:                         DoneCallback done) override {",
          "",
          "[Added Lines]",
          "623:   void ComputeAsync(OpKernelContext* c, DoneCallback done) override {",
          "624:     CollectiveExecutor* col_exec = c->collective_executor();",
          "625:     OP_REQUIRES_ASYNC(",
          "626:         c, col_exec,",
          "627:         errors::Internal(",
          "628:             \"Failed to get CollectiveExecutor from OpKernelContext for Op \",",
          "629:             name_),",
          "630:         done);",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "728:   }",
          "730:  private:",
          "731:   DataType data_type_ = DT_INVALID;",
          "732:   string communication_hint_;",
          "733:   float timeout_seconds_ = 0;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "729:   string name_;",
          "",
          "---------------"
        ],
        "tensorflow/core/nccl/collective_communicator.cc||tensorflow/core/nccl/collective_communicator.cc": [
          "File: tensorflow/core/nccl/collective_communicator.cc -> tensorflow/core/nccl/collective_communicator.cc",
          "--- Hunk 1 ---",
          "[Context before]",
          "16: #include \"tensorflow/core/nccl/collective_communicator.h\"",
          "18: #if TENSORFLOW_USE_NCCL && (GOOGLE_CUDA || TENSORFLOW_USE_ROCM)",
          "20: #include \"absl/memory/memory.h\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "18: #include \"tensorflow/core/framework/cancellation.h\"",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "77:   auto* gpu_info = col_ctx->op_ctx->device()->tensorflow_gpu_device_info();",
          "78:   auto participant = absl::make_unique<NcclManager::Participant>(",
          "79:       compute_stream->parent(), compute_stream, gpu_info, col_ctx->input,",
          "81:   NcclManager::Context context(",
          "82:       nccl_collective_key, num_local_devices, num_global_devices,",
          "83:       col_params.group.runtime_details.communicator_key,",
          "",
          "[Removed Lines]",
          "80:       col_ctx->output, col_ctx->col_params.default_rank, std::move(done));",
          "",
          "[Added Lines]",
          "82:       col_ctx->output, col_ctx->col_params.default_rank,",
          "84:   CancellationManager* cancel_mgr = col_ctx->op_ctx->cancellation_manager();",
          "85:   if (cancel_mgr == nullptr) {",
          "86:     participant->done_callback = std::move(done);",
          "87:   } else {",
          "88:     CancellationToken cancel_token = cancel_mgr->get_cancellation_token();",
          "89:     cancel_mgr->RegisterCallback(cancel_token, [this]() {",
          "90:       nccl_manager_.StartAbort(errors::Cancelled(\"op cancelled\"));",
          "91:       nccl_manager_.Reset();",
          "92:     });",
          "93:     participant->done_callback = [cancel_mgr, cancel_token,",
          "94:                                   done = std::move(done)](const Status& s) {",
          "97:       cancel_mgr->TryDeregisterCallback(cancel_token);",
          "98:       done(s);",
          "99:     };",
          "100:   }",
          "",
          "---------------"
        ],
        "tensorflow/core/nccl/nccl_manager.cc||tensorflow/core/nccl/nccl_manager.cc": [
          "File: tensorflow/core/nccl/nccl_manager.cc -> tensorflow/core/nccl/nccl_manager.cc"
        ],
        "tensorflow/python/kernel_tests/collective_ops_test.py||tensorflow/python/kernel_tests/collective_ops_test.py": [
          "File: tensorflow/python/kernel_tests/collective_ops_test.py -> tensorflow/python/kernel_tests/collective_ops_test.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "471:     _setup_context()",
          "472:     def_function.function(collective_fn)()",
          "475:     # Do not abort if there's no active collective ops. There could be",
          "476:     # exceptions like EOF which we expect users to catch, aborting collective",
          "477:     # ops on all op errors intervenes with this workflow.",
          "",
          "[Removed Lines]",
          "474:   def testOpErrorNotAbort(self, collective_op, device, communication):",
          "",
          "[Added Lines]",
          "475: class OpCancellationTest(test.TestCase, parameterized.TestCase):",
          "477:   def setUp(self):",
          "478:     _setup_context()",
          "479:     super().setUp()",
          "481:   @combinations.generate(",
          "482:       combinations.times(",
          "483:           combinations.combine(",
          "484:               collective_op=[",
          "485:                   combinations.NamedObject('all_reduce',",
          "486:                                            CollectiveOpsV1.all_reduce),",
          "487:                   combinations.NamedObject('all_reduce_v2',",
          "488:                                            CollectiveOpsV2.all_reduce),",
          "489:                   combinations.NamedObject('all_gather',",
          "490:                                            CollectiveOpsV1.all_gather),",
          "491:                   combinations.NamedObject('all_gather_v2',",
          "492:                                            CollectiveOpsV2.all_gather),",
          "493:               ],",
          "494:               mode='eager'), device_combination))",
          "495:   def testOpErrorNotAbortIfNoCollective(self, collective_op, device,",
          "496:                                         communication):",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "504:       f()",
          "505:     collective_fn(constant_op.constant([1.]))",
          "510:     # errors may cause running collective ops to hang.",
          "511:     dev0 = '/device:%s:0' % device",
          "512:     group_size = 2",
          "",
          "[Removed Lines]",
          "507:   def testOpErrorAbort(self, collective_op, device, communication):",
          "508:     # Abort collective ops if there're active collective ops at the time of an",
          "509:     # op error. This is due to the inability to cancel collective ops, and op",
          "",
          "[Added Lines]",
          "529:   @combinations.generate(",
          "530:       combinations.times(",
          "531:           combinations.combine(",
          "532:               collective_op=[",
          "533:                   combinations.NamedObject('all_reduce',",
          "534:                                            CollectiveOpsV1.all_reduce),",
          "535:                   combinations.NamedObject('all_gather',",
          "536:                                            CollectiveOpsV1.all_gather),",
          "537:               ],",
          "538:               mode='eager'), device_combination))",
          "539:   def testOpErrorAbortWithCollective(self, collective_op, device,",
          "540:                                      communication):",
          "541:     # Abort v1 collective ops if there're active collective ops at the time of",
          "542:     # an op error. This is due to the inability to cancel collective ops, and op",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "548:             instance_key,",
          "549:             communication_hint=communication)",
          "552: @combinations.generate(",
          "553:     combinations.times(",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "584:   @combinations.generate(",
          "585:       combinations.times(",
          "586:           combinations.combine(",
          "587:               collective_op=[",
          "588:                   combinations.NamedObject('all_reduce_v2',",
          "589:                                            CollectiveOpsV2.all_reduce),",
          "590:                   combinations.NamedObject('all_gather_v2',",
          "591:                                            CollectiveOpsV2.all_gather),",
          "592:               ],",
          "593:               mode='eager'), device_combination))",
          "594:   def testOpErrorNotAbortWithCollective(self, collective_op, device,",
          "595:                                         communication):",
          "596:     # Do not abort v2 collective ops even if there're active collective ops at",
          "597:     # the time of an op error. We rely cancellation to terminate active",
          "598:     # collective ops.",
          "599:     dev0 = '/device:%s:0' % device",
          "600:     dev1 = '/device:%s:1' % device",
          "601:     group_size = 2",
          "602:     group_key = 100",
          "603:     instance_key = 100",
          "604:     in_tensor = constant_op.constant([1.])",
          "606:     @def_function.function",
          "607:     def collective_fn():",
          "608:       for device in [dev0, dev1]:",
          "609:         with ops.device(device):",
          "610:           collective_op(",
          "611:               in_tensor,",
          "612:               group_size,",
          "613:               group_key,",
          "614:               instance_key,",
          "615:               communication_hint=communication)",
          "617:     # Local params resolution cannot be cancelled yet, so we perform a normal",
          "618:     # collective so that the group is resolved.",
          "619:     collective_fn()",
          "621:     # Make the dataset sleep a while so that the collective is being executed",
          "622:     # when the EOF happens.",
          "623:     dataset = dataset_ops.Dataset.from_tensors([1.]).apply(",
          "624:         dataset_testing.sleep(sleep_microseconds=200))",
          "626:     @def_function.function",
          "627:     def f():",
          "628:       # Launch a collective op that won't be able to finish to test cancellation",
          "629:       # when other ops error.",
          "630:       with ops.device(dev0):",
          "631:         ret = collective_op(",
          "632:             in_tensor,",
          "633:             group_size,",
          "634:             group_key,",
          "635:             instance_key,",
          "636:             communication_hint=communication)",
          "637:       iterator = iter(dataset)",
          "638:       next(iterator)",
          "639:       # This should raise EOF.",
          "640:       next(iterator)",
          "641:       return ret",
          "643:     with self.assertRaises(errors.OutOfRangeError):",
          "644:       f()",
          "645:     # Collective ops shouldn't be aborted and new collectives should be able to",
          "646:     # proceed.",
          "647:     collective_fn()",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "c0f7f14ba6769089fc9d01697e8be158fa566ef2",
      "candidate_info": {
        "commit_hash": "c0f7f14ba6769089fc9d01697e8be158fa566ef2",
        "repo": "tensorflow/tensorflow",
        "commit_url": "https://github.com/tensorflow/tensorflow/commit/c0f7f14ba6769089fc9d01697e8be158fa566ef2",
        "files": [
          "tensorflow/core/common_runtime/ring_reducer.cc",
          "tensorflow/core/common_runtime/ring_reducer_test.cc",
          "tensorflow/core/framework/collective.h",
          "tensorflow/core/kernels/collective_nccl_reducer.cc",
          "tensorflow/core/kernels/collective_nccl_test.cc",
          "tensorflow/core/kernels/collective_ops.cc"
        ],
        "message": "Make V2 collective ComputeAsync thread safe\n\nPiperOrigin-RevId: 338193956\nChange-Id: I2121340af74c2c756897baf23d391446ae4d4407",
        "before_after_code_files": [
          "tensorflow/core/common_runtime/ring_reducer.cc||tensorflow/core/common_runtime/ring_reducer.cc",
          "tensorflow/core/common_runtime/ring_reducer_test.cc||tensorflow/core/common_runtime/ring_reducer_test.cc",
          "tensorflow/core/framework/collective.h||tensorflow/core/framework/collective.h",
          "tensorflow/core/kernels/collective_nccl_reducer.cc||tensorflow/core/kernels/collective_nccl_reducer.cc",
          "tensorflow/core/kernels/collective_nccl_test.cc||tensorflow/core/kernels/collective_nccl_test.cc",
          "tensorflow/core/kernels/collective_ops.cc||tensorflow/core/kernels/collective_ops.cc"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "tensorflow/core/kernels/collective_ops.cc||tensorflow/core/kernels/collective_ops.cc"
          ],
          "candidate": [
            "tensorflow/core/kernels/collective_ops.cc||tensorflow/core/kernels/collective_ops.cc"
          ]
        }
      },
      "candidate_diff": {
        "tensorflow/core/common_runtime/ring_reducer.cc||tensorflow/core/common_runtime/ring_reducer.cc": [
          "File: tensorflow/core/common_runtime/ring_reducer.cc -> tensorflow/core/common_runtime/ring_reducer.cc",
          "--- Hunk 1 ---",
          "[Context before]",
          "256:               rf->action = RF_REDUCE;",
          "257:               Status s = collective_util::ComputeBinOp(",
          "258:                   col_ctx_->op_ctx, col_ctx_->op_params, col_ctx_->device,",
          "260:               if (!s.ok()) {",
          "261:                 aborted = true;",
          "262:                 StartAbort(s);",
          "",
          "[Removed Lines]",
          "259:                   col_params_->merge_op, &rf->chunk, &rf->tmp_chunk);",
          "",
          "[Added Lines]",
          "259:                   col_params_->merge_op.get(), &rf->chunk, &rf->tmp_chunk);",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "266:             }",
          "267:             break;",
          "268:           case RF_REDUCE:",
          "270:               rf->action = RF_FINALIZE;",
          "271:               group_size_tensor_ready_.WaitForNotification();",
          "272:               Status s = collective_util::ComputeBinOp(",
          "273:                   col_ctx_->op_ctx, col_ctx_->op_params, col_ctx_->device,",
          "275:               if (!s.ok()) {",
          "276:                 aborted = true;",
          "277:                 StartAbort(s);",
          "",
          "[Removed Lines]",
          "269:             if (!rf->second_pass && col_params_->final_op && rf->is_final) {",
          "274:                   col_params_->final_op, &rf->chunk, &group_size_tensor_);",
          "",
          "[Added Lines]",
          "269:             if (!rf->second_pass && col_params_->final_op.get() &&",
          "270:                 rf->is_final) {",
          "275:                   col_params_->final_op.get(), &rf->chunk, &group_size_tensor_);",
          "",
          "---------------"
        ],
        "tensorflow/core/common_runtime/ring_reducer_test.cc||tensorflow/core/common_runtime/ring_reducer_test.cc": [
          "File: tensorflow/core/common_runtime/ring_reducer_test.cc -> tensorflow/core/common_runtime/ring_reducer_test.cc",
          "--- Hunk 1 ---",
          "[Context before]",
          "466:     }",
          "468:     void DoReduce() {",
          "475:       OpKernelContext::Params op_params;",
          "",
          "[Removed Lines]",
          "469:       merge_op_ = GetAdd(col_params_.instance.data_type, device_type_, device_);",
          "470:       final_op_ = GetDiv(col_params_.instance.data_type, device_type_, device_);",
          "471:       col_params_.merge_op = merge_op_.get();",
          "472:       col_params_.final_op = final_op_.get();",
          "",
          "[Added Lines]",
          "469:       col_params_.merge_op =",
          "470:           GetAdd(col_params_.instance.data_type, device_type_, device_);",
          "471:       col_params_.final_op =",
          "472:           GetDiv(col_params_.instance.data_type, device_type_, device_);",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "536:     Tensor tensor_;",
          "537:     Device* device_;",
          "538:     CollectiveParams col_params_;",
          "541:     std::unique_ptr<CollectiveAdapter> ca_;",
          "542:     std::unique_ptr<OpKernelContext> ctx_;",
          "543:     Status status_;",
          "",
          "[Removed Lines]",
          "539:     std::unique_ptr<OpKernel> merge_op_;",
          "540:     std::unique_ptr<OpKernel> final_op_;",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "tensorflow/core/framework/collective.h||tensorflow/core/framework/collective.h": [
          "File: tensorflow/core/framework/collective.h -> tensorflow/core/framework/collective.h",
          "--- Hunk 1 ---",
          "[Context before]",
          "143:   int source_rank = -1;    // broadcast only",
          "145:   std::vector<int> subdiv_rank;",
          "148:   string ToString() const;",
          "149: };",
          "",
          "[Removed Lines]",
          "146:   OpKernel* merge_op = nullptr;  // reduction only",
          "147:   OpKernel* final_op = nullptr;  // reduction only",
          "",
          "[Added Lines]",
          "146:   std::unique_ptr<OpKernel> merge_op;  // reduction only",
          "147:   std::unique_ptr<OpKernel> final_op;  // reduction only",
          "",
          "---------------"
        ],
        "tensorflow/core/kernels/collective_nccl_reducer.cc||tensorflow/core/kernels/collective_nccl_reducer.cc": [
          "File: tensorflow/core/kernels/collective_nccl_reducer.cc -> tensorflow/core/kernels/collective_nccl_reducer.cc",
          "--- Hunk 1 ---",
          "[Context before]",
          "113:   if (final_status.ok()) {",
          "114:     final_status = collective_util::ComputeBinOp(",
          "115:         col_ctx_->op_ctx, col_ctx_->op_params, col_ctx_->device,",
          "117:   }",
          "118:   done(final_status);",
          "119: }",
          "",
          "[Removed Lines]",
          "116:         col_params_->final_op, col_ctx_->output, &group_size);",
          "",
          "[Added Lines]",
          "116:         col_params_->final_op.get(), col_ctx_->output, &group_size);",
          "",
          "---------------"
        ],
        "tensorflow/core/kernels/collective_nccl_test.cc||tensorflow/core/kernels/collective_nccl_test.cc": [
          "File: tensorflow/core/kernels/collective_nccl_test.cc -> tensorflow/core/kernels/collective_nccl_test.cc",
          "--- Hunk 1 ---",
          "[Context before]",
          "248:       TF_CHECK_OK(parent_->dev_mgr_->LookupDevice(device_name_, &device_))",
          "249:           << \"Could not find device \" << device_name_ << \" existing devices \"",
          "250:           << parent_->dev_mgr_->DebugString();",
          "253:       col_params_.name = parent_->col_params_.name;",
          "254:       col_params_.default_rank = rank;",
          "255:       col_params_.group = parent_->col_params_.group;",
          "",
          "[Removed Lines]",
          "251:       merge_op_ = GetAdd(device_);",
          "252:       final_op_ = GetDiv(device_);",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "416:     Tensor output_;",
          "417:     Device* device_;",
          "418:     CollectiveParams col_params_;",
          "421:     Status status_;",
          "422:   };",
          "",
          "[Removed Lines]",
          "419:     std::unique_ptr<OpKernel> merge_op_;",
          "420:     std::unique_ptr<OpKernel> final_op_;",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "463:   }",
          "465:   void InitDevice(DeviceInstance* di) override {",
          "468:   }",
          "470:   void RunCollectiveOnDevice(DeviceInstance* di) override { di->RunReduce(); }",
          "",
          "[Removed Lines]",
          "466:     di->col_params_.merge_op = di->merge_op_.get();",
          "467:     di->col_params_.final_op = di->final_op_.get();",
          "",
          "[Added Lines]",
          "462:     di->col_params_.merge_op = GetAdd(di->device_);",
          "463:     di->col_params_.final_op = GetDiv(di->device_);",
          "",
          "---------------"
        ],
        "tensorflow/core/kernels/collective_ops.cc||tensorflow/core/kernels/collective_ops.cc": [
          "File: tensorflow/core/kernels/collective_ops.cc -> tensorflow/core/kernels/collective_ops.cc",
          "--- Hunk 1 ---",
          "[Context before]",
          "271:     sub_node.set_device(real_node.device());",
          "272:     SetAttrValue(col_params_.instance.data_type,",
          "273:                  &(*sub_node.mutable_attr())[\"T\"]);",
          "278:   }",
          "280:  protected:",
          "",
          "[Removed Lines]",
          "274:     merge_op_ = BuildOpKernel(c, merge_op_name, &sub_node);",
          "275:     final_op_ = BuildOpKernel(c, final_op_name, &sub_node);",
          "276:     col_params_.merge_op = merge_op_.get();",
          "277:     col_params_.final_op = final_op_.get();",
          "",
          "[Added Lines]",
          "274:     col_params_.merge_op = BuildOpKernel(c, merge_op_name, &sub_node);",
          "275:     col_params_.final_op = BuildOpKernel(c, final_op_name, &sub_node);",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "313:   }",
          "315:  private:",
          "318:   TF_DISALLOW_COPY_AND_ASSIGN(CollectiveReduceOpKernel);",
          "319: };",
          "",
          "[Removed Lines]",
          "316:   std::unique_ptr<OpKernel> merge_op_;",
          "317:   std::unique_ptr<OpKernel> final_op_;",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "473: class CollectiveReduceV2OpKernel : public CollectiveOpKernel {",
          "474:  public:",
          "475:   explicit CollectiveReduceV2OpKernel(OpKernelConstruction* c)",
          "478:     string merge_op_name;",
          "479:     OP_REQUIRES_OK(c, c->GetAttr(\"merge_op\", &merge_op_name));",
          "480:     OP_REQUIRES_OK(c, c->GetAttr(\"merge_op\", &merge_op_name));",
          "",
          "[Removed Lines]",
          "476:       : CollectiveOpKernel(c), device_type_(DEVICE_DEFAULT) {",
          "477:     OP_REQUIRES_OK(c, c->GetAttr(\"T\", &data_type_));",
          "",
          "[Added Lines]",
          "472:       : CollectiveOpKernel(c) {",
          "473:     col_params_ = std::make_shared<CollectiveParams>();",
          "474:     OP_REQUIRES_OK(c, c->GetAttr(\"T\", &col_params_->instance.data_type));",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "485:     }",
          "486:     string final_op_name;",
          "487:     OP_REQUIRES_OK(c, c->GetAttr(\"final_op\", &final_op_name));",
          "492:     NodeDef sub_node;",
          "493:     sub_node.add_input(c->def().input(0));",
          "494:     sub_node.add_input(c->def().input(0));",
          "495:     sub_node.set_device(c->def().device());",
          "497:                  &(*sub_node.mutable_attr())[\"T\"]);",
          "505:             << \" communication_hint \"",
          "507:   }",
          "509:  protected:",
          "",
          "[Removed Lines]",
          "488:     OP_REQUIRES_OK(c, c->GetAttr(\"communication_hint\", &communication_hint_));",
          "489:     OP_REQUIRES_OK(c, c->GetAttr(\"timeout_seconds\", &timeout_seconds_));",
          "496:     SetAttrValue(col_params_.instance.data_type,",
          "498:     merge_op_ = BuildOpKernel(c, merge_op_name, &sub_node);",
          "499:     final_op_ = BuildOpKernel(c, final_op_name, &sub_node);",
          "501:     name_ = strings::StrCat(c->def().name(), \": ReduceV2(\", merge_op_name, \",\",",
          "502:                             final_op_name, \")\");",
          "503:     device_type_ = c->device_type();",
          "504:     VLOG(2) << \"CollectiveReduceV2 \" << this << \" name \" << col_params_.name",
          "506:             << col_params_.instance.impl_details.communication_hint;",
          "",
          "[Added Lines]",
          "485:     OP_REQUIRES_OK(",
          "486:         c, c->GetAttr(\"communication_hint\",",
          "487:                       &col_params_->instance.impl_details.communication_hint));",
          "488:     OP_REQUIRES_OK(",
          "489:         c, c->GetAttr(\"timeout_seconds\",",
          "490:                       &col_params_->instance.impl_details.timeout_seconds));",
          "497:     SetAttrValue(col_params_->instance.data_type,",
          "499:     col_params_->merge_op = BuildOpKernel(c, merge_op_name, &sub_node);",
          "500:     col_params_->final_op = BuildOpKernel(c, final_op_name, &sub_node);",
          "502:     col_params_->name = strings::StrCat(c->def().name(), \": ReduceV2(\",",
          "503:                                         merge_op_name, \",\", final_op_name, \")\");",
          "504:     col_params_->group.device_type = c->device_type();",
          "507:     col_params_->instance.impl_details.subdiv_offsets.push_back(0);",
          "508:     VLOG(2) << \"CollectiveReduceV2 \" << this << \" name \" << col_params_->name",
          "510:             << col_params_->instance.impl_details.communication_hint;",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "523:         c, instance_key.dims() == 0,",
          "524:         errors::Internal(\"Unexpected dimensions on input instance_key\"), done);",
          "529:     col_params->group.group_size = group_size.unaligned_flat<int32>()(0);",
          "530:     col_params->group.group_key = group_key.unaligned_flat<int32>()(0);",
          "531:     col_params->instance.type = REDUCTION_COLLECTIVE;",
          "532:     col_params->instance.instance_key = instance_key.unaligned_flat<int32>()(0);",
          "541:     VLOG(1) << \"CollectiveReduceV2 group_size \" << col_params->group.group_size",
          "542:             << \" group_key \" << col_params->group.group_key << \" instance_key \"",
          "543:             << col_params->instance.instance_key;",
          "551:     Tensor* output = nullptr;",
          "552:     OP_REQUIRES_OK_ASYNC(",
          "553:         c, c->forward_input_or_allocate_output({0}, 0, input.shape(), &output),",
          "555:     col_params->instance.shape = input.shape();",
          "563:       VLOG(1) << \"CollectiveReduceV2 CompleteParams for collective \"",
          "564:               << col_params->name << \" device \" << c->device()->name()",
          "565:               << \" group \" << col_params->group.group_key << \" instance \"",
          "566:               << col_params->instance.instance_key;",
          "567:       col_exec->CompleteParamsAsync(",
          "569:           [c, done = std::move(done), col_params, col_exec](const Status& s) {",
          "570:             if (s.ok()) {",
          "571:               auto actual_done = [c, group_key = col_params->group.group_key,",
          "",
          "[Removed Lines]",
          "526:     auto col_params = new CollectiveParams();",
          "527:     col_params->name = name_;",
          "528:     col_params->group.device_type = device_type_;",
          "533:     col_params->instance.data_type = data_type_;",
          "534:     col_params->instance.impl_details.communication_hint = communication_hint_;",
          "535:     col_params->instance.impl_details.timeout_seconds = timeout_seconds_;",
          "538:     col_params->instance.impl_details.subdiv_offsets.push_back(0);",
          "539:     col_params->merge_op = merge_op_.get();",
          "540:     col_params->final_op = final_op_.get();",
          "545:     auto done_with_cleanup = [col_params, done = std::move(done)]() {",
          "546:       delete col_params;",
          "547:       done();",
          "548:     };",
          "554:         done_with_cleanup);",
          "560:     c->collective_executor()->RunClosure([c,",
          "561:                                           done = std::move(done_with_cleanup),",
          "562:                                           col_params, col_exec]() {",
          "568:           c->device()->attributes(), col_params, c->cancellation_manager(),",
          "",
          "[Added Lines]",
          "530:     auto col_params = std::make_shared<CollectiveParams>();",
          "531:     col_params->name = col_params_->name;",
          "532:     col_params->group.device_type = col_params_->group.device_type;",
          "537:     col_params->instance.data_type = col_params_->instance.data_type;",
          "538:     col_params->instance.impl_details.communication_hint =",
          "539:         col_params_->instance.impl_details.communication_hint;",
          "540:     col_params->instance.impl_details.timeout_seconds =",
          "541:         col_params_->instance.impl_details.timeout_seconds;",
          "542:     col_params->instance.impl_details.subdiv_offsets =",
          "543:         col_params_->instance.impl_details.subdiv_offsets;",
          "544:     col_params->merge_op = std::move(col_params_->merge_op);",
          "545:     col_params->final_op = std::move(col_params_->final_op);",
          "554:         done);",
          "558:     col_params_ = col_params;",
          "563:     c->collective_executor()->RunClosure([c, done = std::move(done), col_params,",
          "564:                                           col_exec]() {",
          "570:           c->device()->attributes(), col_params.get(),",
          "571:           c->cancellation_manager(),",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "599:   }",
          "601:  private:",
          "609: };",
          "611: REGISTER_KERNEL_BUILDER(Name(\"CollectiveReduceV2\").Device(DEVICE_CPU),",
          "",
          "[Removed Lines]",
          "602:   DataType data_type_ = DT_INVALID;",
          "603:   string communication_hint_;",
          "604:   float timeout_seconds_ = 0;",
          "605:   DeviceType device_type_;",
          "606:   std::unique_ptr<OpKernel> merge_op_;",
          "607:   std::unique_ptr<OpKernel> final_op_;",
          "608:   CollectiveParams col_params_;",
          "",
          "[Added Lines]",
          "605:   std::shared_ptr<CollectiveParams> col_params_;",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "676:         0, output_shape.dim_size(0) * col_params->group.group_size);",
          "677:     col_params->instance.shape = output_shape;",
          "679:     auto done_with_cleanup = [col_params, done = std::move(done)]() {",
          "680:       delete col_params;",
          "681:       done();",
          "682:     };",
          "",
          "[Removed Lines]",
          "684:     Tensor* output = nullptr;",
          "685:     OP_REQUIRES_OK_ASYNC(",
          "686:         c, c->allocate_output(0, col_params->instance.shape, &output),",
          "687:         done_with_cleanup);",
          "",
          "[Added Lines]",
          "676:     Tensor* output = nullptr;",
          "677:     OP_REQUIRES_OK_ASYNC(",
          "678:         c, c->allocate_output(0, col_params->instance.shape, &output), done);",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "731:   }",
          "733:  private:",
          "735:   string communication_hint_;",
          "737:   DeviceType device_type_;",
          "738: };",
          "",
          "[Removed Lines]",
          "734:   DataType data_type_ = DT_INVALID;",
          "736:   float timeout_seconds_ = 0;",
          "",
          "[Added Lines]",
          "730:   DataType data_type_;",
          "732:   float timeout_seconds_;",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "4c5d73cb63e65beb82cce606f184b4e8423efc48",
      "candidate_info": {
        "commit_hash": "4c5d73cb63e65beb82cce606f184b4e8423efc48",
        "repo": "tensorflow/tensorflow",
        "commit_url": "https://github.com/tensorflow/tensorflow/commit/4c5d73cb63e65beb82cce606f184b4e8423efc48",
        "files": [
          "tensorflow/core/common_runtime/executor.cc",
          "tensorflow/core/kernels/collective_ops.cc",
          "tensorflow/python/distribute/cross_device_utils.py",
          "tensorflow/python/keras/distribute/distribute_strategy_test.py",
          "tensorflow/python/keras/engine/training.py",
          "tensorflow/python/kernel_tests/BUILD",
          "tensorflow/python/kernel_tests/collective_ops_test.py"
        ],
        "message": "Rollback: Use cancellation manager to abort collectives\n\nPiperOrigin-RevId: 337512779\nChange-Id: Iced42d2245b4362bfe23fa4e3a9e2d86a8dd3d4e",
        "before_after_code_files": [
          "tensorflow/core/common_runtime/executor.cc||tensorflow/core/common_runtime/executor.cc",
          "tensorflow/core/kernels/collective_ops.cc||tensorflow/core/kernels/collective_ops.cc",
          "tensorflow/python/distribute/cross_device_utils.py||tensorflow/python/distribute/cross_device_utils.py",
          "tensorflow/python/keras/distribute/distribute_strategy_test.py||tensorflow/python/keras/distribute/distribute_strategy_test.py",
          "tensorflow/python/keras/engine/training.py||tensorflow/python/keras/engine/training.py",
          "tensorflow/python/kernel_tests/collective_ops_test.py||tensorflow/python/kernel_tests/collective_ops_test.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "tensorflow/core/kernels/collective_ops.cc||tensorflow/core/kernels/collective_ops.cc",
            "tensorflow/python/kernel_tests/collective_ops_test.py||tensorflow/python/kernel_tests/collective_ops_test.py"
          ],
          "candidate": [
            "tensorflow/core/kernels/collective_ops.cc||tensorflow/core/kernels/collective_ops.cc",
            "tensorflow/python/kernel_tests/collective_ops_test.py||tensorflow/python/kernel_tests/collective_ops_test.py"
          ]
        }
      },
      "candidate_diff": {
        "tensorflow/core/common_runtime/executor.cc||tensorflow/core/common_runtime/executor.cc": [
          "File: tensorflow/core/common_runtime/executor.cc -> tensorflow/core/common_runtime/executor.cc",
          "--- Hunk 1 ---",
          "[Context before]",
          "1119:       if (rendezvous_) {",
          "1120:         rendezvous_->StartAbort(s);",
          "1121:       }",
          "1122:       if (cancellation_manager_) {",
          "1123:         cancellation_manager_->StartCancel();",
          "1131:       }",
          "1132:     }",
          "",
          "[Removed Lines]",
          "1124:       } else {",
          "1128:         if (collective_executor_) {",
          "1129:           collective_executor_->StartAbort(s);",
          "1130:         }",
          "",
          "[Added Lines]",
          "1122:       if (collective_executor_) {",
          "1123:         collective_executor_->StartAbort(s);",
          "1124:       }",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1271:       if (rendezvous_) {",
          "1272:         rendezvous_->StartAbort(status);",
          "1273:       }",
          "1274:       if (cancellation_manager_) {",
          "1275:         cancellation_manager_->StartCancel();",
          "1283:       }",
          "1284:     }",
          "1285:     delete this;",
          "",
          "[Removed Lines]",
          "1276:       } else {",
          "1280:         if (collective_executor_) {",
          "1281:           collective_executor_->StartAbort(status);",
          "1282:         }",
          "",
          "[Added Lines]",
          "1270:       if (collective_executor_) {",
          "1271:         collective_executor_->StartAbort(status);",
          "1272:       }",
          "",
          "---------------"
        ],
        "tensorflow/core/kernels/collective_ops.cc||tensorflow/core/kernels/collective_ops.cc": [
          "File: tensorflow/core/kernels/collective_ops.cc -> tensorflow/core/kernels/collective_ops.cc",
          "--- Hunk 1 ---",
          "[Context before]",
          "52: class CollectiveOpKernel : public AsyncOpKernel {",
          "53:  public:",
          "",
          "[Removed Lines]",
          "54:   explicit CollectiveOpKernel(OpKernelConstruction* c)",
          "55:       : AsyncOpKernel(c), name_(name()) {}",
          "57:   void ComputeAsync(OpKernelContext* c, DoneCallback done) override {",
          "58:     CollectiveExecutor* col_exec = c->collective_executor();",
          "59:     OP_REQUIRES_ASYNC(",
          "60:         c, col_exec,",
          "61:         errors::Internal(",
          "62:             \"Failed to get CollectiveExecutor from OpKernelContext for Op \",",
          "63:             name_),",
          "64:         done);",
          "65:     CancellationToken token =",
          "66:         c->cancellation_manager()->get_cancellation_token();",
          "67:     bool cancel_registered =",
          "68:         c->cancellation_manager()->RegisterCallback(token, [col_exec]() {",
          "71:           col_exec->RunClosure([col_exec]() {",
          "72:             col_exec->StartAbort(errors::Cancelled(\"op cancelled\"));",
          "73:           });",
          "74:         });",
          "75:     OP_REQUIRES_ASYNC(c, cancel_registered,",
          "76:                       errors::Cancelled(\"op cancelled \", name_), done);",
          "78:     auto deregister_and_done = [c, col_exec, token, done = std::move(done)]() {",
          "79:       c->cancellation_manager()->DeregisterCallback(token);",
          "82:       if (!c->status().ok()) {",
          "83:         col_exec->StartAbort(c->status());",
          "84:       }",
          "85:       done();",
          "86:     };",
          "87:     ComputeAsyncImpl(c, col_exec, std::move(deregister_and_done));",
          "88:   }",
          "90:  protected:",
          "91:   virtual void ComputeAsyncImpl(OpKernelContext* c,",
          "92:                                 CollectiveExecutor* col_exec,",
          "93:                                 DoneCallback done) = 0;",
          "95:   string name_;",
          "96: };",
          "98: class CollectiveOpV1Kernel : public CollectiveOpKernel {",
          "99:  public:",
          "100:   explicit CollectiveOpV1Kernel(OpKernelConstruction* c)",
          "101:       : CollectiveOpKernel(c) {}",
          "",
          "[Added Lines]",
          "54:   explicit CollectiveOpKernel(OpKernelConstruction* c) : AsyncOpKernel(c) {}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "137:     return true;",
          "138:   }",
          "141:   CollectiveParams col_params_;",
          "142:   std::vector<int32> dependencies_;",
          "143: };",
          "146:  public:",
          "147:   explicit CollectiveGatherOpKernel(OpKernelConstruction* c)",
          "149:     col_params_.instance.type = GATHER_COLLECTIVE;",
          "150:     OP_REQUIRES_OK(c, c->GetAttr(\"group_size\", &col_params_.group.group_size));",
          "151:     OP_REQUIRES(",
          "",
          "[Removed Lines]",
          "140:  protected:",
          "145: class CollectiveGatherOpKernel : public CollectiveOpV1Kernel {",
          "148:       : CollectiveOpV1Kernel(c) {",
          "",
          "[Added Lines]",
          "97: class CollectiveGatherOpKernel : public CollectiveOpKernel {",
          "100:       : CollectiveOpKernel(c) {",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "167:     col_params_.group.device_type = c->device_type();",
          "168:   }",
          "173:     auto output_shape = c->input(0).shape();",
          "174:     output_shape.set_dim(",
          "175:         0, output_shape.dim_size(0) * col_params_.group.group_size);",
          "",
          "[Removed Lines]",
          "170:  protected:",
          "171:   void ComputeAsyncImpl(OpKernelContext* c, CollectiveExecutor* col_exec,",
          "172:                         DoneCallback done) override {",
          "",
          "[Added Lines]",
          "122:   void ComputeAsync(OpKernelContext* c, DoneCallback done) override {",
          "123:     CollectiveExecutor* col_exec = c->collective_executor();",
          "124:     OP_REQUIRES_ASYNC(",
          "125:         c, col_exec,",
          "126:         errors::Internal(",
          "127:             \"Failed to get CollectiveExecutor from OpKernelContext for Op \",",
          "128:             col_params_.name),",
          "129:         done);",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "213: REGISTER_KERNEL_BUILDER(Name(\"CollectiveGather\").Device(DEVICE_GPU),",
          "214:                         CollectiveGatherOpKernel);",
          "217:  public:",
          "218:   explicit CollectiveReduceOpKernel(OpKernelConstruction* c)",
          "220:     col_params_.instance.type = REDUCTION_COLLECTIVE;",
          "221:     OP_REQUIRES_OK(c, c->GetAttr(\"group_size\", &col_params_.group.group_size));",
          "222:     OP_REQUIRES(",
          "",
          "[Removed Lines]",
          "216: class CollectiveReduceOpKernel : public CollectiveOpV1Kernel {",
          "219:       : CollectiveOpV1Kernel(c) {",
          "",
          "[Added Lines]",
          "174: class CollectiveReduceOpKernel : public CollectiveOpKernel {",
          "177:       : CollectiveOpKernel(c) {",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "273:     col_params_.final_op = BuildOpKernel(c, final_op_name, &sub_node);",
          "274:   }",
          "",
          "[Removed Lines]",
          "276:  protected:",
          "277:   void ComputeAsyncImpl(OpKernelContext* c, CollectiveExecutor* col_exec,",
          "278:                         DoneCallback done) override {",
          "",
          "[Added Lines]",
          "234:   void ComputeAsync(OpKernelContext* c, DoneCallback done) override {",
          "235:     CollectiveExecutor* col_exec = c->collective_executor();",
          "236:     OP_REQUIRES_ASYNC(",
          "237:         c, col_exec,",
          "238:         errors::Internal(",
          "239:             \"Failed to get CollectiveExecutor from OpKernelContext for Op \",",
          "240:             col_params_.name),",
          "241:         done);",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "317: REGISTER_KERNEL_BUILDER(Name(\"CollectiveReduce\").Device(DEVICE_GPU),",
          "318:                         CollectiveReduceOpKernel);",
          "321:  public:",
          "322:   explicit CollectiveBcastSendOpKernel(OpKernelConstruction* c)",
          "324:     col_params_.instance.type = BROADCAST_COLLECTIVE;",
          "325:     OP_REQUIRES_OK(c, c->GetAttr(\"group_size\", &col_params_.group.group_size));",
          "326:     OP_REQUIRES(",
          "",
          "[Removed Lines]",
          "320: class CollectiveBcastSendOpKernel : public CollectiveOpV1Kernel {",
          "323:       : CollectiveOpV1Kernel(c) {",
          "",
          "[Added Lines]",
          "283: class CollectiveBcastSendOpKernel : public CollectiveOpKernel {",
          "286:       : CollectiveOpKernel(c) {",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "346:     col_params_.group.device_type = c->device_type();",
          "347:   }",
          "",
          "[Removed Lines]",
          "349:  protected:",
          "350:   void ComputeAsyncImpl(OpKernelContext* c, CollectiveExecutor* col_exec,",
          "351:                         DoneCallback done) override {",
          "",
          "[Added Lines]",
          "312:   void ComputeAsync(OpKernelContext* c, DoneCallback done) override {",
          "313:     CollectiveExecutor* col_exec = c->collective_executor();",
          "314:     OP_REQUIRES_ASYNC(",
          "315:         c, col_exec,",
          "316:         errors::Internal(",
          "317:             \"Failed to get CollectiveExecutor from OpKernelContext for Op \",",
          "318:             col_params_.name),",
          "319:         done);",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "394: REGISTER_KERNEL_BUILDER(Name(\"CollectiveBcastSend\").Device(DEVICE_GPU),",
          "395:                         CollectiveBcastSendOpKernel);",
          "398:  public:",
          "399:   explicit CollectiveBcastRecvOpKernel(OpKernelConstruction* c)",
          "401:     col_params_.instance.type = BROADCAST_COLLECTIVE;",
          "402:     OP_REQUIRES_OK(c, c->GetAttr(\"group_size\", &col_params_.group.group_size));",
          "403:     OP_REQUIRES(",
          "",
          "[Removed Lines]",
          "397: class CollectiveBcastRecvOpKernel : public CollectiveOpV1Kernel {",
          "400:       : CollectiveOpV1Kernel(c) {",
          "",
          "[Added Lines]",
          "365: class CollectiveBcastRecvOpKernel : public CollectiveOpKernel {",
          "368:       : CollectiveOpKernel(c) {",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "423:     col_params_.group.device_type = c->device_type();",
          "424:   }",
          "",
          "[Removed Lines]",
          "426:  protected:",
          "427:   void ComputeAsyncImpl(OpKernelContext* c, CollectiveExecutor* col_exec,",
          "428:                         DoneCallback done) override {",
          "",
          "[Added Lines]",
          "394:   void ComputeAsync(OpKernelContext* c, DoneCallback done) override {",
          "395:     CollectiveExecutor* col_exec = c->collective_executor();",
          "396:     OP_REQUIRES_ASYNC(",
          "397:         c, col_exec,",
          "398:         errors::Internal(",
          "399:             \"Failed to get CollectiveExecutor from OpKernelContext for Op \",",
          "400:             col_params_.name),",
          "401:         done);",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "464: REGISTER_KERNEL_BUILDER(Name(\"CollectiveBcastRecv\").Device(DEVICE_GPU),",
          "465:                         CollectiveBcastRecvOpKernel);",
          "468:  public:",
          "469:   explicit CollectiveReduceV2OpKernel(OpKernelConstruction* c)",
          "471:     col_params_ = std::make_shared<CollectiveParams>();",
          "472:     OP_REQUIRES_OK(c, c->GetAttr(\"T\", &col_params_->instance.data_type));",
          "473:     string merge_op_name;",
          "",
          "[Removed Lines]",
          "467: class CollectiveReduceV2OpKernel : public CollectiveOpKernel {",
          "470:       : CollectiveOpKernel(c) {",
          "",
          "[Added Lines]",
          "440: class CollectiveReduceV2OpKernel : public AsyncOpKernel {",
          "443:       : AsyncOpKernel(c) {",
          "",
          "---------------",
          "--- Hunk 11 ---",
          "[Context before]",
          "508:             << col_params_->instance.impl_details.communication_hint;",
          "509:   }",
          "514:     const Tensor& input = c->input(0);",
          "515:     const Tensor& group_size = c->input(1);",
          "516:     const Tensor& group_key = c->input(2);",
          "",
          "[Removed Lines]",
          "511:  protected:",
          "512:   void ComputeAsyncImpl(OpKernelContext* c, CollectiveExecutor* col_exec,",
          "513:                         DoneCallback done) override {",
          "",
          "[Added Lines]",
          "484:   void ComputeAsync(OpKernelContext* c, DoneCallback done) override {",
          "485:     CollectiveExecutor* col_exec = c->collective_executor();",
          "486:     OP_REQUIRES_ASYNC(",
          "487:         c, col_exec,",
          "488:         errors::Internal(",
          "489:             \"Failed to get CollectiveExecutor from OpKernelContext for Op \",",
          "490:             col_params_->name),",
          "491:         done);",
          "",
          "---------------",
          "--- Hunk 12 ---",
          "[Context before]",
          "612:                             .HostMemory(\"instance_key\"),",
          "613:                         CollectiveReduceV2OpKernel);",
          "616:  public:",
          "617:   explicit CollectiveGatherV2OpKernel(OpKernelConstruction* c)",
          "619:     OP_REQUIRES_OK(c, c->GetAttr(\"T\", &data_type_));",
          "620:     OP_REQUIRES_OK(c, c->GetAttr(\"communication_hint\", &communication_hint_));",
          "621:     OP_REQUIRES_OK(c, c->GetAttr(\"timeout_seconds\", &timeout_seconds_));",
          "",
          "[Removed Lines]",
          "615: class CollectiveGatherV2OpKernel : public CollectiveOpKernel {",
          "618:       : CollectiveOpKernel(c), device_type_(DEVICE_DEFAULT) {",
          "",
          "[Added Lines]",
          "593: class CollectiveGatherV2OpKernel : public AsyncOpKernel {",
          "596:       : AsyncOpKernel(c), device_type_(DEVICE_DEFAULT) {",
          "",
          "---------------",
          "--- Hunk 13 ---",
          "[Context before]",
          "625:             << \" communication_hint \" << communication_hint_;",
          "626:   }",
          "631:     const Tensor& input = c->input(0);",
          "632:     const Tensor& group_size = c->input(1);",
          "633:     const Tensor& group_key = c->input(2);",
          "",
          "[Removed Lines]",
          "628:  protected:",
          "629:   void ComputeAsyncImpl(OpKernelContext* c, CollectiveExecutor* col_exec,",
          "630:                         DoneCallback done) override {",
          "",
          "[Added Lines]",
          "606:   void ComputeAsync(OpKernelContext* c, DoneCallback done) override {",
          "607:     CollectiveExecutor* col_exec = c->collective_executor();",
          "608:     OP_REQUIRES_ASYNC(",
          "609:         c, col_exec,",
          "610:         errors::Internal(",
          "611:             \"Failed to get CollectiveExecutor from OpKernelContext for Op \",",
          "612:             name_),",
          "613:         done);",
          "",
          "---------------",
          "--- Hunk 14 ---",
          "[Context before]",
          "729:   string communication_hint_;",
          "730:   float timeout_seconds_;",
          "731:   DeviceType device_type_;",
          "732: };",
          "734: REGISTER_KERNEL_BUILDER(Name(\"CollectiveGatherV2\").Device(DEVICE_CPU),",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "715:   string name_;",
          "",
          "---------------"
        ],
        "tensorflow/python/distribute/cross_device_utils.py||tensorflow/python/distribute/cross_device_utils.py": [
          "File: tensorflow/python/distribute/cross_device_utils.py -> tensorflow/python/distribute/cross_device_utils.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "412:         self._group_key, self._device)",
          "413:     instance_key_shape = self._collective_keys.get_instance_key(",
          "414:         self._group_key, self._device)",
          "417:       # 1. Transpose",
          "418:       # E.g. Given an input_tensor with shape [2,2,5,1] and axis to gather is 3,",
          "419:       # we use perm_pre=[3 0 1 2] to reshape it to [1,2,2,5], which",
          "",
          "[Removed Lines]",
          "415:     with ops.device(self._device), \\",
          "416:          ops.control_dependencies([array_ops.identity(input_tensor)]):",
          "",
          "[Added Lines]",
          "415:     with ops.device(self._device):",
          "",
          "---------------"
        ],
        "tensorflow/python/keras/distribute/distribute_strategy_test.py||tensorflow/python/keras/distribute/distribute_strategy_test.py": [
          "File: tensorflow/python/keras/distribute/distribute_strategy_test.py -> tensorflow/python/keras/distribute/distribute_strategy_test.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "29: from tensorflow.python.data.ops import dataset_ops",
          "30: from tensorflow.python.data.ops import readers",
          "31: from tensorflow.python.distribute import central_storage_strategy",
          "32: from tensorflow.python.distribute import combinations as ds_combinations",
          "33: from tensorflow.python.distribute import distribution_strategy_context",
          "34: from tensorflow.python.distribute import mirrored_strategy",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "32: from tensorflow.python.distribute import collective_all_reduce_strategy",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1150:     if mode == 'graph' and _is_tpu_strategy(distribution):",
          "1151:       self.skipTest('partial batch not supported with TPU in graph mode.')",
          "1153:     with self.cached_session():",
          "1154:       with distribution.scope():",
          "1155:         optimizer_fn = gradient_descent_keras.SGD",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1154:     if isinstance(distribution,",
          "1155:                   collective_all_reduce_strategy.CollectiveAllReduceStrategy):",
          "1156:       self.skipTest('EOF error causes subsequent collective ops fail.')",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "1162:             loss,",
          "1163:             metrics=metrics)",
          "1167:       # steps/steps_per_epoch are calculated when using numpy arrays as",
          "1168:       # input data.",
          "1169:       fit_with_numpy = model.fit(",
          "",
          "[Removed Lines]",
          "1165:       inputs = np.zeros((10, 3), dtype=np.float32)",
          "1166:       targets = np.zeros((10, 4), dtype=np.float32)",
          "",
          "[Added Lines]",
          "1169:       inputs = np.zeros((1000, 3), dtype=np.float32)",
          "1170:       targets = np.zeros((1000, 4), dtype=np.float32)",
          "",
          "---------------"
        ],
        "tensorflow/python/keras/engine/training.py||tensorflow/python/keras/engine/training.py": [
          "File: tensorflow/python/keras/engine/training.py -> tensorflow/python/keras/engine/training.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "2733: def _multi_worker_concat(v, strategy):",
          "2734:   \"\"\"Order PerReplica objects for CollectiveAllReduceStrategy and concat.\"\"\"",
          "2735:   replicas = strategy._gather(v, axis=0)  # pylint: disable=protected-access",
          "2754:   replicas = array_ops.split(",
          "2755:       replicas,",
          "",
          "[Removed Lines]",
          "2736:   # TODO(b/170435030): We now need to make sure these run after the iterator",
          "2737:   # GetNext, so that we don't trigger aborting collective ops in the case of",
          "2738:   # EOF. Remove after the issue is fixed.",
          "2739:   with ops.control_dependencies([replicas]):",
          "2740:     # v might not have the same shape on different replicas",
          "2741:     if isinstance(v, ds_values.PerReplica):",
          "2742:       shapes = array_ops.concat([",
          "2743:           array_ops.expand_dims_v2(array_ops.shape(single_value)[0], axis=0)",
          "2744:           for single_value in v.values",
          "2745:       ],",
          "2746:                                 axis=0)",
          "2747:       all_shapes = strategy._gather(shapes, axis=0)  # pylint: disable=protected-access",
          "2748:     else:",
          "2749:       # v is a tensor. This may happen when, say, we have 2x1 multi-worker.",
          "2750:       all_shapes = strategy._gather(  # pylint: disable=protected-access",
          "2751:           array_ops.expand_dims_v2(array_ops.shape(v)[0], axis=0),",
          "2752:           axis=0)",
          "",
          "[Added Lines]",
          "2736:   # v might not have the same shape on different replicas",
          "2737:   if isinstance(v, ds_values.PerReplica):",
          "2738:     shapes = array_ops.concat([",
          "2739:         array_ops.expand_dims_v2(array_ops.shape(single_value)[0], axis=0)",
          "2740:         for single_value in v.values",
          "2741:     ],",
          "2742:                               axis=0)",
          "2743:     all_shapes = strategy._gather(shapes, axis=0)  # pylint: disable=protected-access",
          "2744:   else:",
          "2745:     # v is a tensor. This may happen when, say, we have 2x1 multi-worker.",
          "2746:     all_shapes = strategy._gather(  # pylint: disable=protected-access",
          "2747:         array_ops.expand_dims_v2(array_ops.shape(v)[0], axis=0),",
          "2748:         axis=0)",
          "",
          "---------------"
        ],
        "tensorflow/python/kernel_tests/collective_ops_test.py||tensorflow/python/kernel_tests/collective_ops_test.py": [
          "File: tensorflow/python/kernel_tests/collective_ops_test.py -> tensorflow/python/kernel_tests/collective_ops_test.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "24: from absl.testing import parameterized",
          "26: from tensorflow.python.compat import v2_compat",
          "29: from tensorflow.python.distribute import combinations",
          "30: from tensorflow.python.distribute import test_util",
          "31: from tensorflow.python.eager import context",
          "",
          "[Removed Lines]",
          "27: from tensorflow.python.data.experimental.ops import testing as dataset_testing",
          "28: from tensorflow.python.data.ops import dataset_ops",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "471:     _setup_context()",
          "472:     def_function.function(collective_fn)()",
          "552: @combinations.generate(",
          "553:     combinations.times(",
          "",
          "[Removed Lines]",
          "474:   def testOpErrorNotAbort(self, collective_op, device, communication):",
          "475:     # Do not abort if there's no active collective ops. There could be",
          "476:     # exceptions like EOF which we expect users to catch, aborting collective",
          "477:     # ops on all op errors intervenes with this workflow.",
          "478:     dev0 = '/device:%s:0' % device",
          "479:     dev1 = '/device:%s:1' % device",
          "480:     group_size = 2",
          "481:     group_key = 100",
          "482:     instance_key = 100",
          "483:     dataset = dataset_ops.Dataset.from_tensors([1.])",
          "485:     @def_function.function",
          "486:     def collective_fn(in_tensor):",
          "487:       for device in [dev0, dev1]:",
          "488:         with ops.device(device):",
          "489:           collective_op(",
          "490:               in_tensor,",
          "491:               group_size,",
          "492:               group_key,",
          "493:               instance_key,",
          "494:               communication_hint=communication)",
          "496:     @def_function.function",
          "497:     def f():",
          "498:       iterator = iter(dataset)",
          "499:       collective_fn(next(iterator))",
          "500:       # This next(iterator) should raise EOF.",
          "501:       collective_fn(next(iterator))",
          "503:     with self.assertRaises(errors.OutOfRangeError):",
          "504:       f()",
          "505:     collective_fn(constant_op.constant([1.]))",
          "507:   def testOpErrorAbort(self, collective_op, device, communication):",
          "508:     # Abort collective ops if there're active collective ops at the time of an",
          "509:     # op error. This is due to the inability to cancel collective ops, and op",
          "510:     # errors may cause running collective ops to hang.",
          "511:     dev0 = '/device:%s:0' % device",
          "512:     group_size = 2",
          "513:     group_key = 100",
          "514:     instance_key = 100",
          "515:     in_tensor = constant_op.constant([1.])",
          "516:     # Make the dataset sleep a while so that the collective is being executed",
          "517:     # when the EOF happens.",
          "518:     dataset = dataset_ops.Dataset.from_tensors([1.]).apply(",
          "519:         dataset_testing.sleep(sleep_microseconds=200))",
          "521:     @def_function.function",
          "522:     def f():",
          "523:       # Launch a collective op that won't be able to finish to test abortion",
          "524:       # when other ops error.",
          "525:       with ops.device(dev0):",
          "526:         ret = collective_op(",
          "527:             in_tensor,",
          "528:             group_size,",
          "529:             group_key,",
          "530:             instance_key,",
          "531:             communication_hint=communication)",
          "532:       iterator = iter(dataset)",
          "533:       next(iterator)",
          "534:       # This should raise EOF.",
          "535:       next(iterator)",
          "536:       return ret",
          "538:     with self.assertRaises(errors.OutOfRangeError):",
          "539:       f()",
          "540:     # Now collective ops is aborted, subsequent collective ops should fail with",
          "541:     # the previous error.",
          "542:     with self.assertRaises(errors.CancelledError):",
          "543:       with ops.device(dev0):",
          "544:         collective_op(",
          "545:             in_tensor,",
          "546:             group_size,",
          "547:             group_key,",
          "548:             instance_key,",
          "549:             communication_hint=communication)",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "e303715fe3bfc976c224825c1701c992b7a9966a",
      "candidate_info": {
        "commit_hash": "e303715fe3bfc976c224825c1701c992b7a9966a",
        "repo": "tensorflow/tensorflow",
        "commit_url": "https://github.com/tensorflow/tensorflow/commit/e303715fe3bfc976c224825c1701c992b7a9966a",
        "files": [
          "tensorflow/core/kernels/collective_ops.cc"
        ],
        "message": "Refactor collective v2 kernels\n\nCommon logic are moved to a base class.\n\nPiperOrigin-RevId: 380739857\nChange-Id: I8ba2b90bd77afb09db6eb66426df067db00d3335",
        "before_after_code_files": [
          "tensorflow/core/kernels/collective_ops.cc||tensorflow/core/kernels/collective_ops.cc"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "tensorflow/core/kernels/collective_ops.cc||tensorflow/core/kernels/collective_ops.cc"
          ],
          "candidate": [
            "tensorflow/core/kernels/collective_ops.cc||tensorflow/core/kernels/collective_ops.cc"
          ]
        }
      },
      "candidate_diff": {
        "tensorflow/core/kernels/collective_ops.cc||tensorflow/core/kernels/collective_ops.cc": [
          "File: tensorflow/core/kernels/collective_ops.cc -> tensorflow/core/kernels/collective_ops.cc",
          "--- Hunk 1 ---",
          "[Context before]",
          "463: REGISTER_KERNEL_BUILDER(Name(\"CollectiveBcastRecv\").Device(DEVICE_GPU),",
          "464:                         CollectiveBcastRecvOpKernel);",
          "467:  public:",
          "470:     OP_REQUIRES_OK(c, c->GetAttr(\"T\", &data_type_));",
          "481:     OP_REQUIRES_OK(c, c->GetAttr(\"communication_hint\", &communication_hint_));",
          "482:     OP_REQUIRES_OK(c, c->GetAttr(\"timeout_seconds\", &timeout_seconds_));",
          "497:     device_type_ = c->device_type();",
          "500:   }",
          "525:     col_params->name = name_;",
          "526:     col_params->group.device_type = device_type_;",
          "527:     col_params->group.group_size = group_size.unaligned_flat<int32>()(0);",
          "528:     col_params->group.group_key = group_key.unaligned_flat<int32>()(0);",
          "530:     col_params->instance.instance_key = instance_key.unaligned_flat<int32>()(0);",
          "531:     col_params->instance.data_type = data_type_;",
          "532:     col_params->instance.impl_details.communication_hint = communication_hint_;",
          "533:     col_params->instance.impl_details.timeout_seconds = timeout_seconds_;",
          "563:               << col_params->instance.instance_key;",
          "564:       col_exec->CompleteParamsAsync(",
          "565:           c->device()->attributes(), col_params, c->cancellation_manager(),",
          "566:           [c, done = std::move(done), col_params, col_exec](const Status& s) {",
          "567:             if (s.ok()) {",
          "571:                                   done = std::move(done)](const Status& s) {",
          "578:                 done();",
          "579:               };",
          "582:                       << col_params->name << \" device \" << c->device()->name()",
          "583:                       << \" group \" << col_params->group.group_key",
          "584:                       << \" instance \" << col_params->instance.instance_key;",
          "",
          "[Removed Lines]",
          "466: class CollectiveReduceV2OpKernel : public AsyncOpKernel {",
          "468:   explicit CollectiveReduceV2OpKernel(OpKernelConstruction* c)",
          "469:       : AsyncOpKernel(c), device_type_(DEVICE_DEFAULT) {",
          "471:     string merge_op_name;",
          "472:     OP_REQUIRES_OK(c, c->GetAttr(\"merge_op\", &merge_op_name));",
          "473:     OP_REQUIRES_OK(c, c->GetAttr(\"merge_op\", &merge_op_name));",
          "474:     if (merge_op_name == \"Max\") {",
          "475:       merge_op_name = \"Maximum\";",
          "476:     } else if (merge_op_name == \"Min\") {",
          "477:       merge_op_name = \"Minimum\";",
          "478:     }",
          "479:     string final_op_name;",
          "480:     OP_REQUIRES_OK(c, c->GetAttr(\"final_op\", &final_op_name));",
          "483:     OP_REQUIRES_OK(",
          "484:         c, c->GetAttr(\"max_subdivs_per_device\", &max_subdivs_per_device_));",
          "487:     NodeDef sub_node;",
          "488:     sub_node.add_input(c->def().input(0));",
          "489:     sub_node.add_input(c->def().input(0));",
          "490:     sub_node.set_device(c->def().device());",
          "491:     SetAttrValue(data_type_, &(*sub_node.mutable_attr())[\"T\"]);",
          "492:     merge_op_ = BuildOpKernel(c, merge_op_name, &sub_node);",
          "493:     final_op_ = BuildOpKernel(c, final_op_name, &sub_node);",
          "495:     name_ = strings::StrCat(c->def().name(), \": ReduceV2(\", merge_op_name, \",\",",
          "496:                             final_op_name, \")\");",
          "498:     VLOG(2) << \"CollectiveReduceV2 \" << this << \" name \" << name_",
          "499:             << \" communication_hint \" << communication_hint_;",
          "502:   void ComputeAsync(OpKernelContext* c, DoneCallback done) override {",
          "503:     CollectiveExecutor* col_exec = c->collective_executor();",
          "504:     OP_REQUIRES_ASYNC(",
          "505:         c, col_exec,",
          "506:         errors::Internal(",
          "507:             \"Failed to get CollectiveExecutor from OpKernelContext for Op \",",
          "508:             name_),",
          "509:         done);",
          "510:     const Tensor& input = c->input(0);",
          "511:     const Tensor& group_size = c->input(1);",
          "512:     const Tensor& group_key = c->input(2);",
          "513:     const Tensor& instance_key = c->input(3);",
          "514:     OP_REQUIRES_ASYNC(",
          "515:         c, group_size.dims() == 0,",
          "516:         errors::Internal(\"Unexpected dimensions on input group_size\"), done);",
          "517:     OP_REQUIRES_ASYNC(",
          "518:         c, group_key.dims() == 0,",
          "519:         errors::Internal(\"Unexpected dimensions on input group_key\"), done);",
          "520:     OP_REQUIRES_ASYNC(",
          "521:         c, instance_key.dims() == 0,",
          "522:         errors::Internal(\"Unexpected dimensions on input instance_key\"), done);",
          "524:     auto col_params = new CollectiveParams();",
          "529:     col_params->instance.type = REDUCTION_COLLECTIVE;",
          "534:     col_params->instance.impl_details.max_subdivs_per_device =",
          "535:         max_subdivs_per_device_;",
          "536:     col_params->merge_op = merge_op_.get();",
          "537:     col_params->final_op = final_op_.get();",
          "538:     VLOG(1) << \"CollectiveReduceV2 group_size \" << col_params->group.group_size",
          "539:             << \" group_key \" << col_params->group.group_key << \" instance_key \"",
          "540:             << col_params->instance.instance_key;",
          "542:     auto done_with_cleanup = [col_params, done = std::move(done)]() {",
          "543:       done();",
          "544:       col_params->Unref();",
          "545:     };",
          "548:     Tensor* output = nullptr;",
          "549:     OP_REQUIRES_OK_ASYNC(",
          "550:         c, c->forward_input_or_allocate_output({0}, 0, input.shape(), &output),",
          "551:         done_with_cleanup);",
          "552:     col_params->instance.shape = input.shape();",
          "557:     c->collective_executor()->RunClosure([c,",
          "558:                                           done = std::move(done_with_cleanup),",
          "559:                                           col_params, col_exec]() {",
          "560:       VLOG(1) << \"CollectiveReduceV2 CompleteParams for collective \"",
          "561:               << col_params->name << \" device \" << c->device()->name()",
          "562:               << \" group \" << col_params->group.group_key << \" instance \"",
          "568:               auto actual_done = [c, group_key = col_params->group.group_key,",
          "569:                                   instance_key =",
          "570:                                       col_params->instance.instance_key,",
          "572:                 VLOG(1) << \"CollectiveReduceV2 ExecuteAsync done for \"",
          "573:                            \"collective \"",
          "574:                         << c->op_kernel().name() << \" device \"",
          "575:                         << c->device()->name() << \" group \" << group_key",
          "576:                         << \" instance \" << instance_key << \" status \" << s;",
          "577:                 OP_REQUIRES_OK_ASYNC(c, s, done);",
          "580:               VLOG(1) << \"CollectiveReduceV2 ExecuteAsync start for \"",
          "581:                          \"collective \"",
          "",
          "[Added Lines]",
          "466: class CollectiveOpV2Kernel : public AsyncOpKernel {",
          "468:   explicit CollectiveOpV2Kernel(OpKernelConstruction* c)",
          "469:       : AsyncOpKernel(c), name_(name()), device_type_(DEVICE_DEFAULT) {",
          "476:  protected:",
          "480:   Status FillCollectiveParams(CollectiveParams* col_params,",
          "481:                               CollectiveType collective_type,",
          "482:                               const Tensor& group_size, const Tensor& group_key,",
          "483:                               const Tensor& instance_key) {",
          "484:     if (group_size.dims() > 0) {",
          "485:       return errors::Internal(\"Unexpected dimensions on input group_size, got \",",
          "486:                               group_size.shape().DebugString());",
          "487:     }",
          "488:     if (group_key.dims() > 0) {",
          "489:       return errors::Internal(\"Unexpected dimensions on input group_key, got \",",
          "490:                               group_key.shape().DebugString());",
          "491:     }",
          "492:     if (instance_key.dims() > 0) {",
          "493:       return errors::Internal(",
          "494:           \"Unexpected dimensions on input instance_key, got \",",
          "495:           instance_key.shape().DebugString());",
          "496:     }",
          "500:     if (col_params->group.group_size <= 0) {",
          "501:       return errors::InvalidArgument(",
          "502:           \"group_size must be positive integer but got \",",
          "503:           col_params->group.group_size);",
          "504:     }",
          "506:     col_params->instance.type = collective_type;",
          "511:     return Status::OK();",
          "512:   }",
          "516:   void Run(OpKernelContext* c, CollectiveParams* col_params,",
          "517:            DoneCallback done) {",
          "518:     CollectiveExecutor* col_exec = c->collective_executor();",
          "519:     OP_REQUIRES_ASYNC(",
          "520:         c, col_exec,",
          "521:         errors::Internal(",
          "522:             \"Failed to get CollectiveExecutor from OpKernelContext for Op \",",
          "523:             name_),",
          "524:         done);",
          "528:     c->collective_executor()->RunClosure([c, done = std::move(done), col_params,",
          "529:                                           col_exec]() {",
          "530:       VLOG(1) << \"Collective CompleteParams for \" << col_params->name",
          "531:               << \" device \" << c->device()->name() << \" group \"",
          "532:               << col_params->group.group_key << \" instance \"",
          "538:               auto actual_done = [c, col_params,",
          "540:                 VLOG(1) << \"Collective ExecuteAsync done for \"",
          "541:                         << col_params->name << \" device \" << c->device()->name()",
          "542:                         << \" group \" << col_params->group.group_key",
          "543:                         << \" instance \" << col_params->instance.instance_key",
          "544:                         << \" status \" << s;",
          "545:                 if (!s.ok()) {",
          "546:                   c->SetStatus(s);",
          "547:                 }",
          "550:               VLOG(1) << \"Collective ExecuteAsync start for \"",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "595:     });",
          "596:   }",
          "599:   string name_;",
          "600:   DataType data_type_ = DT_INVALID;",
          "601:   string communication_hint_;",
          "602:   float timeout_seconds_ = 0;",
          "604:   DeviceType device_type_;",
          "605:   std::unique_ptr<OpKernel> merge_op_;",
          "606:   std::unique_ptr<OpKernel> final_op_;",
          "607: };",
          "",
          "[Removed Lines]",
          "598:  private:",
          "603:   int max_subdivs_per_device_;",
          "",
          "[Added Lines]",
          "567:  protected:",
          "573: };",
          "575: class CollectiveReduceV2OpKernel : public CollectiveOpV2Kernel {",
          "576:  public:",
          "577:   explicit CollectiveReduceV2OpKernel(OpKernelConstruction* c)",
          "578:       : CollectiveOpV2Kernel(c) {",
          "579:     string merge_op_name;",
          "580:     OP_REQUIRES_OK(c, c->GetAttr(\"merge_op\", &merge_op_name));",
          "581:     if (merge_op_name == \"Max\") {",
          "582:       merge_op_name = \"Maximum\";",
          "583:     } else if (merge_op_name == \"Min\") {",
          "584:       merge_op_name = \"Minimum\";",
          "585:     }",
          "586:     string final_op_name;",
          "587:     OP_REQUIRES_OK(c, c->GetAttr(\"final_op\", &final_op_name));",
          "588:     OP_REQUIRES_OK(",
          "589:         c, c->GetAttr(\"max_subdivs_per_device\", &max_subdivs_per_device_));",
          "592:     NodeDef sub_node;",
          "593:     sub_node.add_input(c->def().input(0));",
          "594:     sub_node.add_input(c->def().input(0));",
          "595:     sub_node.set_device(c->def().device());",
          "596:     SetAttrValue(data_type_, &(*sub_node.mutable_attr())[\"T\"]);",
          "597:     merge_op_ = BuildOpKernel(c, merge_op_name, &sub_node);",
          "598:     final_op_ = BuildOpKernel(c, final_op_name, &sub_node);",
          "599:     name_ = strings::StrCat(c->def().name(), \": ReduceV2(\", merge_op_name, \",\",",
          "600:                             final_op_name, \")\");",
          "601:     VLOG(2) << \"CollectiveReduceV2 \" << this << \" name \" << name_",
          "602:             << \" communication_hint \" << communication_hint_;",
          "603:   }",
          "605:   void ComputeAsync(OpKernelContext* c, DoneCallback done) override {",
          "606:     auto col_params = new CollectiveParams();",
          "607:     auto done_with_cleanup = [col_params, done = std::move(done)]() {",
          "608:       done();",
          "609:       col_params->Unref();",
          "610:     };",
          "611:     OP_REQUIRES_OK_ASYNC(c,",
          "612:                          FillCollectiveParams(col_params, REDUCTION_COLLECTIVE,",
          "616:                          done);",
          "617:     col_params->instance.shape = c->input(0).shape();",
          "618:     col_params->merge_op = merge_op_.get();",
          "619:     col_params->final_op = final_op_.get();",
          "620:     VLOG(1) << \"CollectiveReduceV2 group_size \" << col_params->group.group_size",
          "621:             << \" group_key \" << col_params->group.group_key << \" instance_key \"",
          "622:             << col_params->instance.instance_key;",
          "624:     Tensor* output = nullptr;",
          "625:     OP_REQUIRES_OK_ASYNC(c,",
          "626:                          c->forward_input_or_allocate_output(",
          "627:                              {0}, 0, col_params->instance.shape, &output),",
          "628:                          done_with_cleanup);",
          "629:     Run(c, col_params, std::move(done_with_cleanup));",
          "630:   }",
          "632:  private:",
          "633:   int max_subdivs_per_device_;",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "615:                             .HostMemory(\"instance_key\"),",
          "616:                         CollectiveReduceV2OpKernel);",
          "619:  public:",
          "620:   explicit CollectiveGatherV2OpKernel(OpKernelConstruction* c)",
          "625:     name_ = strings::StrCat(c->def().name(), \": GatherV2\");",
          "627:     VLOG(2) << \"CollectiveGatherV2 \" << this << \" name \" << name_",
          "628:             << \" communication_hint \" << communication_hint_;",
          "629:   }",
          "631:   void ComputeAsync(OpKernelContext* c, DoneCallback done) override {",
          "659:     auto col_params = new CollectiveParams();",
          "682:     auto done_with_cleanup = [col_params, done = std::move(done)]() {",
          "683:       done();",
          "684:       col_params->Unref();",
          "685:     };",
          "687:     Tensor* output = nullptr;",
          "688:     OP_REQUIRES_OK_ASYNC(",
          "689:         c, c->allocate_output(0, col_params->instance.shape, &output),",
          "690:         done_with_cleanup);",
          "734:   }",
          "742: };",
          "744: REGISTER_KERNEL_BUILDER(Name(\"CollectiveGatherV2\").Device(DEVICE_CPU),",
          "",
          "[Removed Lines]",
          "618: class CollectiveGatherV2OpKernel : public AsyncOpKernel {",
          "621:       : AsyncOpKernel(c), device_type_(DEVICE_DEFAULT) {",
          "622:     OP_REQUIRES_OK(c, c->GetAttr(\"T\", &data_type_));",
          "623:     OP_REQUIRES_OK(c, c->GetAttr(\"communication_hint\", &communication_hint_));",
          "624:     OP_REQUIRES_OK(c, c->GetAttr(\"timeout_seconds\", &timeout_seconds_));",
          "626:     device_type_ = c->device_type();",
          "632:     CollectiveExecutor* col_exec = c->collective_executor();",
          "633:     OP_REQUIRES_ASYNC(",
          "634:         c, col_exec,",
          "635:         errors::Internal(",
          "636:             \"Failed to get CollectiveExecutor from OpKernelContext for Op \",",
          "637:             name_),",
          "638:         done);",
          "639:     const Tensor& input = c->input(0);",
          "640:     const Tensor& group_size = c->input(1);",
          "641:     const Tensor& group_key = c->input(2);",
          "642:     const Tensor& instance_key = c->input(3);",
          "643:     OP_REQUIRES_ASYNC(c, group_size.dims() == 0,",
          "644:                       errors::InvalidArgument(",
          "645:                           \"Unexpected dimensions on input group_size, got \",",
          "646:                           group_size.shape().DebugString()),",
          "647:                       done);",
          "648:     OP_REQUIRES_ASYNC(c, group_key.dims() == 0,",
          "649:                       errors::InvalidArgument(",
          "650:                           \"Unexpected dimensions on input group_key, got \",",
          "651:                           group_key.shape().DebugString()),",
          "652:                       done);",
          "653:     OP_REQUIRES_ASYNC(c, instance_key.dims() == 0,",
          "654:                       errors::InvalidArgument(",
          "655:                           \"Unexpected dimensions on input instance_key, got \",",
          "656:                           instance_key.shape().DebugString()),",
          "657:                       done);",
          "660:     col_params->name = name_;",
          "661:     col_params->group.device_type = device_type_;",
          "662:     col_params->group.group_size = group_size.unaligned_flat<int32>()(0);",
          "663:     OP_REQUIRES(",
          "664:         c, col_params->group.group_size > 0,",
          "665:         errors::InvalidArgument(\"group_size must be positive integer but got \",",
          "666:                                 col_params->group.group_size));",
          "667:     col_params->group.group_key = group_key.unaligned_flat<int32>()(0);",
          "668:     col_params->instance.type = GATHER_COLLECTIVE;",
          "669:     col_params->instance.instance_key = instance_key.unaligned_flat<int32>()(0);",
          "670:     col_params->instance.data_type = data_type_;",
          "671:     col_params->instance.impl_details.communication_hint = communication_hint_;",
          "672:     col_params->instance.impl_details.timeout_seconds = timeout_seconds_;",
          "673:     VLOG(1) << \"CollectiveGatherV2 group_size \" << col_params->group.group_size",
          "674:             << \" group_key \" << col_params->group.group_key << \" instance_key \"",
          "675:             << col_params->instance.instance_key;",
          "677:     auto output_shape = input.shape();",
          "678:     output_shape.set_dim(",
          "679:         0, output_shape.dim_size(0) * col_params->group.group_size);",
          "680:     col_params->instance.shape = output_shape;",
          "695:     c->collective_executor()->RunClosure([c,",
          "696:                                           done = std::move(done_with_cleanup),",
          "697:                                           col_params, col_exec]() {",
          "698:       VLOG(1) << \"CollectiveGatherV2 CompleteParams for collective \"",
          "699:               << col_params->name << \" device \" << c->device()->name()",
          "700:               << \" group \" << col_params->group.group_key << \" instance \"",
          "701:               << col_params->instance.instance_key;",
          "702:       col_exec->CompleteParamsAsync(",
          "703:           c->device()->attributes(), col_params, c->cancellation_manager(),",
          "704:           [c, done = std::move(done), col_params, col_exec](const Status& s) {",
          "705:             if (s.ok()) {",
          "706:               auto actual_done = [c, group_key = col_params->group.group_key,",
          "707:                                   instance_key =",
          "708:                                       col_params->instance.instance_key,",
          "709:                                   done = std::move(done)](const Status& s) {",
          "710:                 VLOG(1) << \"CollectiveGatherV2 ExecuteAsync done for \"",
          "711:                            \"collective \"",
          "712:                         << c->op_kernel().name() << \" device \"",
          "713:                         << c->device()->name() << \" group \" << group_key",
          "714:                         << \" instance \" << instance_key << \" status \" << s;",
          "715:                 OP_REQUIRES_OK_ASYNC(c, s, done);",
          "716:                 done();",
          "717:               };",
          "718:               VLOG(1) << \"CollectiveGatherV2 ExecuteAsync start for \"",
          "719:                          \"collective \"",
          "720:                       << col_params->name << \" device \" << c->device()->name()",
          "721:                       << \" group \" << col_params->group.group_key",
          "722:                       << \" instance \" << col_params->instance.instance_key;",
          "723:               col_exec->ExecuteAsync(",
          "724:                   c, col_params,",
          "725:                   CollectiveKey(c, col_params->group.group_key,",
          "726:                                 col_params->instance.instance_key),",
          "727:                   actual_done);",
          "728:             } else {",
          "729:               c->SetStatus(s);",
          "730:               done();",
          "731:             }",
          "732:           });",
          "733:     });",
          "736:  private:",
          "737:   string name_;",
          "738:   DataType data_type_ = DT_INVALID;",
          "739:   string communication_hint_;",
          "740:   float timeout_seconds_ = 0;",
          "741:   DeviceType device_type_;",
          "",
          "[Added Lines]",
          "647: class CollectiveGatherV2OpKernel : public CollectiveOpV2Kernel {",
          "650:       : CollectiveOpV2Kernel(c) {",
          "662:     OP_REQUIRES_OK_ASYNC(c,",
          "663:                          FillCollectiveParams(col_params, GATHER_COLLECTIVE,",
          "667:                                               c->input(3)),",
          "668:                          done_with_cleanup);",
          "669:     auto output_shape = c->input(0).shape();",
          "670:     output_shape.set_dim(",
          "671:         0, output_shape.dim_size(0) * col_params->group.group_size);",
          "672:     col_params->instance.shape = output_shape;",
          "673:     VLOG(1) << \"CollectiveGatherV2 group_size \" << col_params->group.group_size",
          "674:             << \" group_key \" << col_params->group.group_key << \" instance_key \"",
          "675:             << col_params->instance.instance_key;",
          "680:     Run(c, col_params, std::move(done_with_cleanup));",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "750:                             .HostMemory(\"instance_key\"),",
          "751:                         CollectiveGatherV2OpKernel);",
          "754:  public:",
          "755:   explicit CollectiveBcastSendV2OpKernel(OpKernelConstruction* c)",
          "760:     const bool is_source = true;",
          "761:     name_ = strings::StrCat(name(), \": Broadcast(\", is_source, \")\");",
          "762:   }",
          "764:  protected:",
          "765:   void ComputeAsync(OpKernelContext* c, DoneCallback done) override {",
          "787:     auto col_params = new CollectiveParams();",
          "797:     col_params->is_source = true;",
          "800:     col_params->instance.impl_details.subdiv_offsets.push_back(0);",
          "",
          "[Removed Lines]",
          "753: class CollectiveBcastSendV2OpKernel : public AsyncOpKernel {",
          "756:       : AsyncOpKernel(c), device_type_(DEVICE_DEFAULT) {",
          "757:     OP_REQUIRES_OK(c, c->GetAttr(\"T\", &data_type_));",
          "758:     OP_REQUIRES_OK(c, c->GetAttr(\"communication_hint\", &communication_hint_));",
          "759:     OP_REQUIRES_OK(c, c->GetAttr(\"timeout_seconds\", &timeout_seconds_));",
          "766:     CollectiveExecutor* col_exec = c->collective_executor();",
          "767:     OP_REQUIRES_ASYNC(",
          "768:         c, col_exec,",
          "769:         errors::Internal(",
          "770:             \"Failed to get CollectiveExecutor from OpKernelContext for Op \",",
          "771:             name_),",
          "772:         done);",
          "773:     const Tensor& input = c->input(0);",
          "774:     const Tensor& group_size = c->input(1);",
          "775:     const Tensor& group_key = c->input(2);",
          "776:     const Tensor& instance_key = c->input(3);",
          "777:     OP_REQUIRES_ASYNC(",
          "778:         c, group_size.dims() == 0,",
          "779:         errors::Internal(\"Unexpected dimensions on input group_size\"), done);",
          "780:     OP_REQUIRES_ASYNC(",
          "781:         c, group_key.dims() == 0,",
          "782:         errors::Internal(\"Unexpected dimensions on input group_key\"), done);",
          "783:     OP_REQUIRES_ASYNC(",
          "784:         c, instance_key.dims() == 0,",
          "785:         errors::Internal(\"Unexpected dimensions on input instance_key\"), done);",
          "788:     col_params->name = name_;",
          "789:     col_params->group.device_type = device_type_;",
          "790:     col_params->group.group_size = group_size.unaligned_flat<int32>()(0);",
          "791:     col_params->group.group_key = group_key.unaligned_flat<int32>()(0);",
          "792:     col_params->instance.type = BROADCAST_COLLECTIVE;",
          "793:     col_params->instance.instance_key = instance_key.unaligned_flat<int32>()(0);",
          "794:     col_params->instance.data_type = data_type_;",
          "795:     col_params->instance.impl_details.communication_hint = communication_hint_;",
          "796:     col_params->instance.impl_details.timeout_seconds = timeout_seconds_;",
          "",
          "[Added Lines]",
          "693: class CollectiveBcastSendV2OpKernel : public CollectiveOpV2Kernel {",
          "696:       : CollectiveOpV2Kernel(c) {",
          "704:     auto done_with_cleanup = [col_params, done = std::move(done)]() {",
          "705:       done();",
          "706:       col_params->Unref();",
          "707:     };",
          "708:     OP_REQUIRES_OK_ASYNC(c,",
          "709:                          FillCollectiveParams(col_params, BROADCAST_COLLECTIVE,",
          "713:                          done_with_cleanup);",
          "715:     col_params->instance.shape = c->input(0).shape();",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "802:             << col_params->group.group_size << \" group_key \"",
          "803:             << col_params->group.group_key << \" instance_key \"",
          "804:             << col_params->instance.instance_key;",
          "812:     Tensor* output = nullptr;",
          "860:   }",
          "868: };",
          "870: REGISTER_KERNEL_BUILDER(Name(\"CollectiveBcastSendV2\").Device(DEVICE_CPU),",
          "",
          "[Removed Lines]",
          "806:     auto done_with_cleanup = [col_params, done = std::move(done)]() {",
          "807:       done();",
          "808:       col_params->Unref();",
          "809:     };",
          "813:     OP_REQUIRES_OK_ASYNC(",
          "814:         c, c->forward_input_or_allocate_output({0}, 0, input.shape(), &output),",
          "815:         done_with_cleanup);",
          "816:     col_params->instance.shape = input.shape();",
          "821:     c->collective_executor()->RunClosure([c,",
          "822:                                           done = std::move(done_with_cleanup),",
          "823:                                           col_params, col_exec]() {",
          "824:       VLOG(1) << \"CollectiveBcastSendV2 CompleteParams for collective \"",
          "825:               << col_params->name << \" device \" << c->device()->name()",
          "826:               << \" group \" << col_params->group.group_key << \" instance \"",
          "827:               << col_params->instance.instance_key;",
          "828:       col_exec->CompleteParamsAsync(",
          "829:           c->device()->attributes(), col_params, c->cancellation_manager(),",
          "830:           [c, done = std::move(done), col_params, col_exec](const Status& s) {",
          "831:             if (s.ok()) {",
          "832:               auto actual_done = [c, group_key = col_params->group.group_key,",
          "833:                                   instance_key =",
          "834:                                       col_params->instance.instance_key,",
          "835:                                   done = std::move(done)](const Status& s) {",
          "836:                 VLOG(1) << \"CollectiveBcastSendV2 ExecuteAsync done for \"",
          "837:                            \"collective \"",
          "838:                         << c->op_kernel().name() << \" device \"",
          "839:                         << c->device()->name() << \" group \" << group_key",
          "840:                         << \" instance \" << instance_key << \" status \" << s;",
          "841:                 OP_REQUIRES_OK_ASYNC(c, s, done);",
          "842:                 done();",
          "843:               };",
          "844:               VLOG(1) << \"CollectiveBcastSendV2 ExecuteAsync start for \"",
          "845:                          \"collective \"",
          "846:                       << col_params->name << \" device \" << c->device()->name()",
          "847:                       << \" group \" << col_params->group.group_key",
          "848:                       << \" instance \" << col_params->instance.instance_key;",
          "849:               col_exec->ExecuteAsync(",
          "850:                   c, col_params,",
          "851:                   CollectiveKey(c, col_params->group.group_key,",
          "852:                                 col_params->instance.instance_key),",
          "853:                   actual_done);",
          "854:             } else {",
          "855:               c->SetStatus(s);",
          "856:               done();",
          "857:             }",
          "858:           });",
          "859:     });",
          "862:  private:",
          "863:   DeviceType device_type_;",
          "864:   DataType data_type_ = DT_INVALID;",
          "865:   string communication_hint_;",
          "866:   float timeout_seconds_ = 0;",
          "867:   string name_;",
          "",
          "[Added Lines]",
          "725:     OP_REQUIRES_OK_ASYNC(c,",
          "726:                          c->forward_input_or_allocate_output(",
          "727:                              {0}, 0, col_params->instance.shape, &output),",
          "728:                          done_with_cleanup);",
          "729:     Run(c, col_params, std::move(done_with_cleanup));",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "876:                             .HostMemory(\"instance_key\"),",
          "877:                         CollectiveBcastSendV2OpKernel);",
          "880:  public:",
          "881:   explicit CollectiveBcastRecvV2OpKernel(OpKernelConstruction* c)",
          "886:     const bool is_source = false;",
          "887:     name_ = strings::StrCat(name(), \": Broadcast(\", is_source, \")\");",
          "888:   }",
          "890:  protected:",
          "891:   void ComputeAsync(OpKernelContext* c, DoneCallback done) override {",
          "913:     auto col_params = new CollectiveParams();",
          "914:     auto done_with_cleanup = [col_params, done = std::move(done)]() {",
          "915:       done();",
          "916:       col_params->Unref();",
          "917:     };",
          "931:     col_params->is_source = false;",
          "934:     col_params->instance.impl_details.subdiv_offsets.push_back(0);",
          "",
          "[Removed Lines]",
          "879: class CollectiveBcastRecvV2OpKernel : public AsyncOpKernel {",
          "882:       : AsyncOpKernel(c), device_type_(DEVICE_DEFAULT) {",
          "883:     OP_REQUIRES_OK(c, c->GetAttr(\"T\", &data_type_));",
          "884:     OP_REQUIRES_OK(c, c->GetAttr(\"communication_hint\", &communication_hint_));",
          "885:     OP_REQUIRES_OK(c, c->GetAttr(\"timeout_seconds\", &timeout_seconds_));",
          "892:     CollectiveExecutor* col_exec = c->collective_executor();",
          "893:     OP_REQUIRES_ASYNC(",
          "894:         c, col_exec,",
          "895:         errors::Internal(",
          "896:             \"Failed to get CollectiveExecutor from OpKernelContext for Op \",",
          "897:             name_),",
          "898:         done);",
          "899:     const Tensor& group_size = c->input(0);",
          "900:     const Tensor& group_key = c->input(1);",
          "901:     const Tensor& instance_key = c->input(2);",
          "902:     const Tensor& shape_tensor = c->input(3);",
          "903:     OP_REQUIRES_ASYNC(",
          "904:         c, group_size.dims() == 0,",
          "905:         errors::Internal(\"Unexpected dimensions on input group_size\"), done);",
          "906:     OP_REQUIRES_ASYNC(",
          "907:         c, group_key.dims() == 0,",
          "908:         errors::Internal(\"Unexpected dimensions on input group_key\"), done);",
          "909:     OP_REQUIRES_ASYNC(",
          "910:         c, instance_key.dims() == 0,",
          "911:         errors::Internal(\"Unexpected dimensions on input instance_key\"), done);",
          "919:     OP_REQUIRES_OK_ASYNC(",
          "920:         c, tensor::MakeShape(shape_tensor, &col_params->instance.shape),",
          "921:         done_with_cleanup);",
          "922:     col_params->name = name_;",
          "923:     col_params->group.device_type = device_type_;",
          "924:     col_params->group.group_size = group_size.unaligned_flat<int32>()(0);",
          "925:     col_params->group.group_key = group_key.unaligned_flat<int32>()(0);",
          "926:     col_params->instance.type = BROADCAST_COLLECTIVE;",
          "927:     col_params->instance.instance_key = instance_key.unaligned_flat<int32>()(0);",
          "928:     col_params->instance.data_type = data_type_;",
          "929:     col_params->instance.impl_details.communication_hint = communication_hint_;",
          "930:     col_params->instance.impl_details.timeout_seconds = timeout_seconds_;",
          "",
          "[Added Lines]",
          "742: class CollectiveBcastRecvV2OpKernel : public CollectiveOpV2Kernel {",
          "745:       : CollectiveOpV2Kernel(c) {",
          "757:     OP_REQUIRES_OK_ASYNC(c,",
          "758:                          FillCollectiveParams(col_params, BROADCAST_COLLECTIVE,",
          "762:                          done_with_cleanup);",
          "764:     TensorShape output_shape;",
          "765:     OP_REQUIRES_OK_ASYNC(c, tensor::MakeShape(c->input(3), &output_shape),",
          "766:                          done_with_cleanup);",
          "767:     col_params->instance.shape = output_shape;",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "936:             << col_params->group.group_size << \" group_key \"",
          "937:             << col_params->group.group_key << \" instance_key \"",
          "938:             << col_params->instance.instance_key;",
          "941:     Tensor* output = nullptr;",
          "989:   }",
          "997: };",
          "999: REGISTER_KERNEL_BUILDER(Name(\"CollectiveBcastRecvV2\").Device(DEVICE_CPU),",
          "",
          "[Removed Lines]",
          "942:     OP_REQUIRES_OK_ASYNC(c,",
          "943:                          c->forward_input_or_allocate_output(",
          "944:                              {0}, 0, col_params->instance.shape, &output),",
          "945:                          done_with_cleanup);",
          "950:     c->collective_executor()->RunClosure([c,",
          "951:                                           done = std::move(done_with_cleanup),",
          "952:                                           col_params, col_exec]() {",
          "953:       VLOG(1) << \"CollectiveBcastRecvV2 CompleteParams for collective \"",
          "954:               << col_params->name << \" device \" << c->device()->name()",
          "955:               << \" group \" << col_params->group.group_key << \" instance \"",
          "956:               << col_params->instance.instance_key;",
          "957:       col_exec->CompleteParamsAsync(",
          "958:           c->device()->attributes(), col_params, c->cancellation_manager(),",
          "959:           [c, done = std::move(done), col_params, col_exec](const Status& s) {",
          "960:             if (s.ok()) {",
          "961:               auto actual_done = [c, group_key = col_params->group.group_key,",
          "962:                                   instance_key =",
          "963:                                       col_params->instance.instance_key,",
          "964:                                   done = std::move(done)](const Status& s) {",
          "965:                 VLOG(1) << \"CollectiveBcastRecvV2 ExecuteAsync done for \"",
          "966:                            \"collective \"",
          "967:                         << c->op_kernel().name() << \" device \"",
          "968:                         << c->device()->name() << \" group \" << group_key",
          "969:                         << \" instance \" << instance_key << \" status \" << s;",
          "970:                 OP_REQUIRES_OK_ASYNC(c, s, done);",
          "971:                 done();",
          "972:               };",
          "973:               VLOG(1) << \"CollectiveBcastRecvV2 ExecuteAsync start for \"",
          "974:                          \"collective \"",
          "975:                       << col_params->name << \" device \" << c->device()->name()",
          "976:                       << \" group \" << col_params->group.group_key",
          "977:                       << \" instance \" << col_params->instance.instance_key;",
          "978:               col_exec->ExecuteAsync(",
          "979:                   c, col_params,",
          "980:                   CollectiveKey(c, col_params->group.group_key,",
          "981:                                 col_params->instance.instance_key),",
          "982:                   actual_done);",
          "983:             } else {",
          "984:               c->SetStatus(s);",
          "985:               done();",
          "986:             }",
          "987:           });",
          "988:     });",
          "991:  private:",
          "992:   DeviceType device_type_;",
          "993:   DataType data_type_ = DT_INVALID;",
          "994:   string communication_hint_;",
          "995:   float timeout_seconds_ = 0;",
          "996:   string name_;",
          "",
          "[Added Lines]",
          "776:     OP_REQUIRES_OK_ASYNC(",
          "777:         c, c->allocate_output(0, col_params->instance.shape, &output),",
          "778:         done_with_cleanup);",
          "779:     Run(c, col_params, std::move(done_with_cleanup));",
          "",
          "---------------"
        ]
      }
    }
  ]
}