{
  "cve_id": "CVE-2024-48944",
  "cve_desc": "Server-Side Request Forgery (SSRF) vulnerability in Apache Kylin. Through a kylin server, an attacker may forge a request to invoke \"/kylin/api/xxx/diag\" api on another internal host and possibly get leaked information. There are two preconditions: 1) The attacker has got admin access to a kylin server; 2) Another internal host has the \"/kylin/api/xxx/diag\" api\n\nendpoint open for service.\n\n\nThis issue affects Apache Kylin: from 5.0.0 \nthrough \n\n5.0.1.\n\nUsers are recommended to upgrade to version 5.0.2, which fixes the issue.",
  "repo": "apache/kylin",
  "patch_hash": "4e6a5acd799ae7543c7161e72ef1019c74d5b4ad",
  "patch_info": {
    "commit_hash": "4e6a5acd799ae7543c7161e72ef1019c74d5b4ad",
    "repo": "apache/kylin",
    "commit_url": "https://github.com/apache/kylin/commit/4e6a5acd799ae7543c7161e72ef1019c74d5b4ad",
    "files": [
      "src/common-server/src/main/java/org/apache/kylin/rest/controller/NBasicController.java",
      "src/common-server/src/test/java/org/apache/kylin/rest/controller/NBasicControllerTest.java"
    ],
    "message": "KYLIN-5644 fix diag api security, encryption changed from base64 to AES\n\nCo-authored-by: liang.hua <liang.hua@kyligence.io>",
    "before_after_code_files": [
      "src/common-server/src/main/java/org/apache/kylin/rest/controller/NBasicController.java||src/common-server/src/main/java/org/apache/kylin/rest/controller/NBasicController.java",
      "src/common-server/src/test/java/org/apache/kylin/rest/controller/NBasicControllerTest.java||src/common-server/src/test/java/org/apache/kylin/rest/controller/NBasicControllerTest.java"
    ]
  },
  "patch_diff": {
    "src/common-server/src/main/java/org/apache/kylin/rest/controller/NBasicController.java||src/common-server/src/main/java/org/apache/kylin/rest/controller/NBasicController.java": [
      "File: src/common-server/src/main/java/org/apache/kylin/rest/controller/NBasicController.java -> src/common-server/src/main/java/org/apache/kylin/rest/controller/NBasicController.java",
      "--- Hunk 1 ---",
      "[Context before]",
      "81: import org.apache.kylin.common.msg.Message;",
      "82: import org.apache.kylin.common.msg.MsgPicker;",
      "83: import org.apache.kylin.common.persistence.transaction.TransactionException;",
      "84: import org.apache.kylin.common.util.JsonUtil;",
      "85: import org.apache.kylin.common.util.Pair;",
      "86: import org.apache.kylin.job.constant.JobStatusEnum;",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "84: import org.apache.kylin.common.util.EncryptUtil;",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "643:             if (StringUtils.isBlank(host) || host.startsWith(\"http://\")) {",
      "644:                 return host;",
      "645:             }",
      "647:         } catch (Exception e) {",
      "648:             logger.error(\"Failed to decode host, will use the original host name\");",
      "649:         }",
      "",
      "[Removed Lines]",
      "646:             return new String(Base64.decodeBase64(host), Charset.defaultCharset());",
      "",
      "[Added Lines]",
      "647:             String decryptValue = EncryptUtil.decrypt(new String(Base64.decodeBase64(host), Charset.defaultCharset()));",
      "648:             return StringUtils.isBlank(decryptValue) ? host : decryptValue;",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "659:             if (!host.toLowerCase().startsWith(\"http\")) {",
      "660:                 host = \"http://\" + host;",
      "661:             }",
      "663:         } catch (Exception e) {",
      "664:             logger.error(\"Failed to encode host, will use the original host name\");",
      "665:         }",
      "",
      "[Removed Lines]",
      "662:             return Base64.encodeBase64String(host.getBytes(Charset.defaultCharset()));",
      "",
      "[Added Lines]",
      "664:             return Base64.encodeBase64String(EncryptUtil.encrypt(host).getBytes(Charset.defaultCharset()));",
      "",
      "---------------"
    ],
    "src/common-server/src/test/java/org/apache/kylin/rest/controller/NBasicControllerTest.java||src/common-server/src/test/java/org/apache/kylin/rest/controller/NBasicControllerTest.java": [
      "File: src/common-server/src/test/java/org/apache/kylin/rest/controller/NBasicControllerTest.java -> src/common-server/src/test/java/org/apache/kylin/rest/controller/NBasicControllerTest.java",
      "--- Hunk 1 ---",
      "[Context before]",
      "311:         }",
      "312:         Assert.assertEquals(3, mockDataResponse.get(\"size\"));",
      "313:     }",
      "315: }",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "315:     @Test",
      "316:     public void testEncodeAndDecodeHost() {",
      "317:         Assert.assertTrue(nBasicController.encodeHost(\"\").isEmpty());",
      "318:         String host = \"localhost:7070\";",
      "319:         String encodeHost = nBasicController.encodeHost(host);",
      "320:         Assert.assertNotNull(encodeHost);",
      "321:         Assert.assertNotEquals(host, encodeHost);",
      "322:         String decodeHost = nBasicController.decodeHost(encodeHost);",
      "323:         Assert.assertEquals(\"http://\" + host, decodeHost);",
      "324:         Assert.assertEquals(\"ip\", nBasicController.decodeHost(\"ip\"));",
      "325:     }",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "c22a0aeb594990060688eb5726fd1d4bc8210886",
      "candidate_info": {
        "commit_hash": "c22a0aeb594990060688eb5726fd1d4bc8210886",
        "repo": "apache/kylin",
        "commit_url": "https://github.com/apache/kylin/commit/c22a0aeb594990060688eb5726fd1d4bc8210886",
        "files": [
          "pom.xml",
          "src/common-booter/src/main/resources/config/init.properties",
          "src/core-common/src/main/resources/kylin-defaults0.properties",
          "src/data-loading-booter/src/main/resources/config/init.properties",
          "src/kylin-it/src/test/java/org/apache/kylin/newten/BloomFilterTest.java",
          "src/query-booter/src/main/resources/config/init.properties",
          "src/query-service/src/main/java/org/apache/kylin/rest/service/QueryService.java",
          "src/server/src/main/resources/config/init.properties",
          "src/spark-project/sparder/src/main/scala/org/apache/spark/sql/SparderEnv.scala",
          "src/spark-project/spark-common/src/main/java/org/apache/kylin/engine/spark/filter/ParquetPageFilterCollector.java",
          "src/spark-project/spark-common/src/test/java/org/apache/kylin/engine/spark/filter/ParquetPageFilterCollectorTest.java"
        ],
        "message": "KYLIN-5653 add page index filter log (#30361)",
        "before_after_code_files": [
          "src/common-booter/src/main/resources/config/init.properties||src/common-booter/src/main/resources/config/init.properties",
          "src/core-common/src/main/resources/kylin-defaults0.properties||src/core-common/src/main/resources/kylin-defaults0.properties",
          "src/data-loading-booter/src/main/resources/config/init.properties||src/data-loading-booter/src/main/resources/config/init.properties",
          "src/kylin-it/src/test/java/org/apache/kylin/newten/BloomFilterTest.java||src/kylin-it/src/test/java/org/apache/kylin/newten/BloomFilterTest.java",
          "src/query-booter/src/main/resources/config/init.properties||src/query-booter/src/main/resources/config/init.properties",
          "src/query-service/src/main/java/org/apache/kylin/rest/service/QueryService.java||src/query-service/src/main/java/org/apache/kylin/rest/service/QueryService.java",
          "src/server/src/main/resources/config/init.properties||src/server/src/main/resources/config/init.properties",
          "src/spark-project/sparder/src/main/scala/org/apache/spark/sql/SparderEnv.scala||src/spark-project/sparder/src/main/scala/org/apache/spark/sql/SparderEnv.scala",
          "src/spark-project/spark-common/src/main/java/org/apache/kylin/engine/spark/filter/ParquetPageFilterCollector.java||src/spark-project/spark-common/src/main/java/org/apache/kylin/engine/spark/filter/ParquetPageFilterCollector.java",
          "src/spark-project/spark-common/src/test/java/org/apache/kylin/engine/spark/filter/ParquetPageFilterCollectorTest.java||src/spark-project/spark-common/src/test/java/org/apache/kylin/engine/spark/filter/ParquetPageFilterCollectorTest.java"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/kylin/pull/2138"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "src/common-booter/src/main/resources/config/init.properties||src/common-booter/src/main/resources/config/init.properties": [
          "File: src/common-booter/src/main/resources/config/init.properties -> src/common-booter/src/main/resources/config/init.properties",
          "--- Hunk 1 ---",
          "[Context before]",
          "160: # to avoid cartesian partition oom, set to -1 or empty to turn off",
          "161: kylin.storage.columnar.spark-conf.spark.sql.cartesianPartitionNumThreshold=-1",
          "167: # spark3 legacy config after calendar switch",
          "168: kylin.storage.columnar.spark-conf.spark.sql.legacy.parquet.int96RebaseModeInWrite=LEGACY",
          "",
          "[Removed Lines]",
          "163: # disable parquet columnindex to save the overhead on read column index",
          "164: # as column index won't be used by spark vectorized reader for now",
          "165: kylin.storage.columnar.spark-conf.parquet.filter.columnindex.enabled=false",
          "",
          "[Added Lines]",
          "163: kylin.storage.columnar.spark-conf.parquet.filter.columnindex.enabled=true",
          "",
          "---------------"
        ],
        "src/core-common/src/main/resources/kylin-defaults0.properties||src/core-common/src/main/resources/kylin-defaults0.properties": [
          "File: src/core-common/src/main/resources/kylin-defaults0.properties -> src/core-common/src/main/resources/kylin-defaults0.properties",
          "--- Hunk 1 ---",
          "[Context before]",
          "163: # to avoid cartesian partition oom, set to -1 or empty to turn off",
          "164: kylin.storage.columnar.spark-conf.spark.sql.cartesianPartitionNumThreshold=-1",
          "170: # spark3 legacy config after calendar switch",
          "171: kylin.storage.columnar.spark-conf.spark.sql.parquet.int96RebaseModeInWrite=LEGACY",
          "",
          "[Removed Lines]",
          "166: # disable parquet columnindex to save the overhead on read column index",
          "167: # as column index won't be used by spark vectorized reader for now",
          "168: kylin.storage.columnar.spark-conf.parquet.filter.columnindex.enabled=false",
          "",
          "[Added Lines]",
          "166: kylin.storage.columnar.spark-conf.parquet.filter.columnindex.enabled=true",
          "",
          "---------------"
        ],
        "src/data-loading-booter/src/main/resources/config/init.properties||src/data-loading-booter/src/main/resources/config/init.properties": [
          "File: src/data-loading-booter/src/main/resources/config/init.properties -> src/data-loading-booter/src/main/resources/config/init.properties",
          "--- Hunk 1 ---",
          "[Context before]",
          "160: # to avoid cartesian partition oom, set to -1 or empty to turn off",
          "161: kylin.storage.columnar.spark-conf.spark.sql.cartesianPartitionNumThreshold=-1",
          "167: # spark3 legacy config after calendar switch",
          "168: kylin.storage.columnar.spark-conf.spark.sql.legacy.parquet.int96RebaseModeInWrite=LEGACY",
          "",
          "[Removed Lines]",
          "163: # disable parquet columnindex to save the overhead on read column index",
          "164: # as column index won't be used by spark vectorized reader for now",
          "165: kylin.storage.columnar.spark-conf.parquet.filter.columnindex.enabled=false",
          "",
          "[Added Lines]",
          "163: kylin.storage.columnar.spark-conf.parquet.filter.columnindex.enabled=true",
          "",
          "---------------"
        ],
        "src/kylin-it/src/test/java/org/apache/kylin/newten/BloomFilterTest.java||src/kylin-it/src/test/java/org/apache/kylin/newten/BloomFilterTest.java": [
          "File: src/kylin-it/src/test/java/org/apache/kylin/newten/BloomFilterTest.java -> src/kylin-it/src/test/java/org/apache/kylin/newten/BloomFilterTest.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "116:     @Test",
          "117:     public void testBuildBloomFilter() throws Exception {",
          "118:         String dfID = \"c41390c5-b93d-4db3-b167-029874b85a2c\";",
          "119:         NDataflow dataflow = dfMgr.getDataflow(dfID);",
          "120:         LayoutEntity layout = dataflow.getIndexPlan().getLayoutEntity(20000000001L);",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "118:         Path projectFilterPath = getProjectFiltersFile(SERVER_HOST, getProject());",
          "119:         FileSystem fs = HadoopUtil.getFileSystem(KylinConfig.getInstanceFromEnv().getHdfsWorkingDirectory());",
          "120:         if (fs.exists(projectFilterPath)) {",
          "121:             fs.delete(projectFilterPath, true);",
          "122:         }",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "137:         String sql1 = \"select * from SSB.P_LINEORDER where LO_CUSTKEY in (13,8) and LO_SHIPPRIOTITY = 0 \";",
          "138:         query.add(Pair.newPair(\"bloomfilter\", sql1));",
          "139:         ExecAndComp.execAndCompare(query, getProject(), ExecAndComp.CompareLevel.NONE, \"inner\");",
          "143:         await().atMost(120, TimeUnit.SECONDS).until(() -> {",
          "144:             try {",
          "",
          "[Removed Lines]",
          "140:         Path projectFilterPath = getProjectFiltersFile(SERVER_HOST, getProject());",
          "141:         FileSystem fs = HadoopUtil.getFileSystem(KylinConfig.getInstanceFromEnv().getHdfsWorkingDirectory());",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "src/query-booter/src/main/resources/config/init.properties||src/query-booter/src/main/resources/config/init.properties": [
          "File: src/query-booter/src/main/resources/config/init.properties -> src/query-booter/src/main/resources/config/init.properties",
          "--- Hunk 1 ---",
          "[Context before]",
          "160: # to avoid cartesian partition oom, set to -1 or empty to turn off",
          "161: kylin.storage.columnar.spark-conf.spark.sql.cartesianPartitionNumThreshold=-1",
          "167: # spark3 legacy config after calendar switch",
          "168: kylin.storage.columnar.spark-conf.spark.sql.legacy.parquet.int96RebaseModeInWrite=LEGACY",
          "",
          "[Removed Lines]",
          "163: # disable parquet columnindex to save the overhead on read column index",
          "164: # as column index won't be used by spark vectorized reader for now",
          "165: kylin.storage.columnar.spark-conf.parquet.filter.columnindex.enabled=false",
          "",
          "[Added Lines]",
          "163: kylin.storage.columnar.spark-conf.parquet.filter.columnindex.enabled=true",
          "",
          "---------------"
        ],
        "src/query-service/src/main/java/org/apache/kylin/rest/service/QueryService.java||src/query-service/src/main/java/org/apache/kylin/rest/service/QueryService.java": [
          "File: src/query-service/src/main/java/org/apache/kylin/rest/service/QueryService.java -> src/query-service/src/main/java/org/apache/kylin/rest/service/QueryService.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "81: import org.apache.kylin.common.util.Pair;",
          "82: import org.apache.kylin.common.util.SetThreadName;",
          "83: import org.apache.kylin.engine.spark.filter.BloomFilterSkipCollector;",
          "84: import org.apache.kylin.guava30.shaded.common.annotations.VisibleForTesting;",
          "85: import org.apache.kylin.guava30.shaded.common.base.Joiner;",
          "86: import org.apache.kylin.guava30.shaded.common.collect.Collections2;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "84: import org.apache.kylin.engine.spark.filter.ParquetPageFilterCollector;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "438:         }",
          "440:         BloomFilterSkipCollector.logAndCleanStatus(QueryContext.current().getQueryId());",
          "441:         LogReport report = new LogReport().put(LogReport.QUERY_ID, QueryContext.current().getQueryId())",
          "442:                 .put(LogReport.SQL, sql).put(LogReport.USER, user)",
          "443:                 .put(LogReport.SUCCESS, null == response.getExceptionMessage()).put(LogReport.DURATION, duration)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "442:         ParquetPageFilterCollector.logParquetPages(QueryContext.current().getQueryId());",
          "",
          "---------------"
        ],
        "src/server/src/main/resources/config/init.properties||src/server/src/main/resources/config/init.properties": [
          "File: src/server/src/main/resources/config/init.properties -> src/server/src/main/resources/config/init.properties",
          "--- Hunk 1 ---",
          "[Context before]",
          "160: # to avoid cartesian partition oom, set to -1 or empty to turn off",
          "161: kylin.storage.columnar.spark-conf.spark.sql.cartesianPartitionNumThreshold=-1",
          "167: # spark3 legacy config after calendar switch",
          "168: kylin.storage.columnar.spark-conf.spark.sql.legacy.parquet.int96RebaseModeInWrite=LEGACY",
          "",
          "[Removed Lines]",
          "163: # disable parquet columnindex to save the overhead on read column index",
          "164: # as column index won't be used by spark vectorized reader for now",
          "165: kylin.storage.columnar.spark-conf.parquet.filter.columnindex.enabled=false",
          "",
          "[Added Lines]",
          "163: kylin.storage.columnar.spark-conf.parquet.filter.columnindex.enabled=true",
          "",
          "---------------"
        ],
        "src/spark-project/sparder/src/main/scala/org/apache/spark/sql/SparderEnv.scala||src/spark-project/sparder/src/main/scala/org/apache/spark/sql/SparderEnv.scala": [
          "File: src/spark-project/sparder/src/main/scala/org/apache/spark/sql/SparderEnv.scala -> src/spark-project/sparder/src/main/scala/org/apache/spark/sql/SparderEnv.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "31: import org.apache.kylin.common.msg.MsgPicker",
          "32: import org.apache.kylin.common.util.{DefaultHostInfoFetcher, HadoopUtil, S3AUtil}",
          "33: import org.apache.kylin.common.{KapConfig, KylinConfig, QueryContext}",
          "35: import org.apache.kylin.metadata.model.{NTableMetadataManager, TableExtDesc}",
          "36: import org.apache.kylin.metadata.project.NProjectManager",
          "37: import org.apache.kylin.query.runtime.plan.QueryToExecutionIDCache",
          "",
          "[Removed Lines]",
          "34: import org.apache.kylin.engine.spark.filter.BloomFilterSkipCollector",
          "",
          "[Added Lines]",
          "34: import org.apache.kylin.engine.spark.filter.{BloomFilterSkipCollector, ParquetPageFilterCollector}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "326:               inputMetrics.totalBloomBlocks, inputMetrics.totalSkipBloomBlocks,",
          "327:               inputMetrics.totalSkipBloomRows, inputMetrics.footerReadTime,",
          "328:               inputMetrics.footerReadNumber)",
          "329:           }",
          "330:         } catch {",
          "331:           case e: Throwable => logWarning(\"error when add metrics for query\", e)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "329:             ParquetPageFilterCollector.addQueryMetrics(taskEnd.queryId, inputMetrics.totalPagesCount,",
          "330:               inputMetrics.filteredPagesCount, inputMetrics.afterFilterPagesCount)",
          "",
          "---------------"
        ],
        "src/spark-project/spark-common/src/main/java/org/apache/kylin/engine/spark/filter/ParquetPageFilterCollector.java||src/spark-project/spark-common/src/main/java/org/apache/kylin/engine/spark/filter/ParquetPageFilterCollector.java": [
          "File: src/spark-project/spark-common/src/main/java/org/apache/kylin/engine/spark/filter/ParquetPageFilterCollector.java -> src/spark-project/spark-common/src/main/java/org/apache/kylin/engine/spark/filter/ParquetPageFilterCollector.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "19: package org.apache.kylin.engine.spark.filter;",
          "21: import java.util.concurrent.ExecutionException;",
          "22: import java.util.concurrent.TimeUnit;",
          "23: import java.util.concurrent.atomic.AtomicLong;",
          "24: import java.util.concurrent.locks.ReentrantReadWriteLock;",
          "26: import org.apache.kylin.common.util.AutoReadWriteLock;",
          "27: import org.apache.kylin.guava30.shaded.common.cache.Cache;",
          "28: import org.apache.kylin.guava30.shaded.common.cache.CacheBuilder;",
          "29: import org.slf4j.Logger;",
          "30: import org.slf4j.LoggerFactory;",
          "32: public class ParquetPageFilterCollector {",
          "34:     public static final Logger LOGGER = LoggerFactory.getLogger(ParquetPageFilterCollector.class);",
          "35:     public static final Cache<String, AtomicLong> queryTotalParquetPages = CacheBuilder.newBuilder()",
          "36:             .expireAfterAccess(10, TimeUnit.MINUTES).build();",
          "37:     public static final Cache<String, AtomicLong> queryFilteredParquetPages = CacheBuilder.newBuilder()",
          "38:             .expireAfterAccess(10, TimeUnit.MINUTES).build();",
          "39:     public static final Cache<String, AtomicLong> queryAfterFilterParquetPages = CacheBuilder.newBuilder()",
          "40:             .expireAfterAccess(10, TimeUnit.MINUTES).build();",
          "41:     private static final AutoReadWriteLock LOCK = new AutoReadWriteLock(new ReentrantReadWriteLock());",
          "43:     private ParquetPageFilterCollector() {",
          "45:     }",
          "47:     public static void addQueryMetrics(String queryId, long totalPages, long filteredPages, long afterFilterPages) {",
          "48:         long start = System.currentTimeMillis();",
          "49:         try (AutoReadWriteLock.AutoLock writeLock = LOCK.lockForWrite()) {",
          "50:             addQueryCounter(queryId, queryTotalParquetPages, totalPages);",
          "51:             addQueryCounter(queryId, queryFilteredParquetPages, filteredPages);",
          "52:             addQueryCounter(queryId, queryAfterFilterParquetPages, afterFilterPages);",
          "53:         } catch (Exception e) {",
          "54:             LOGGER.error(\"Error when add query metrics.\", e);",
          "55:         }",
          "56:         long end = System.currentTimeMillis();",
          "57:         if ((end - start) > 100) {",
          "58:             LOGGER.warn(\"Parquet page filter collector cost too much time: {} ms \", (end - start));",
          "59:         }",
          "60:     }",
          "62:     private static void addQueryCounter(String queryId, Cache<String, AtomicLong> counter, long step)",
          "63:             throws ExecutionException {",
          "64:         AtomicLong pageCnt = counter.get(queryId, () -> new AtomicLong(0L));",
          "65:         pageCnt.addAndGet(step);",
          "66:     }",
          "68:     public static void logParquetPages(String queryId) {",
          "69:         try {",
          "70:             AtomicLong totalPages = queryTotalParquetPages.getIfPresent(queryId);",
          "71:             if (totalPages != null && totalPages.get() > 0) {",
          "72:                 AtomicLong filteredPages = queryFilteredParquetPages.get(queryId, () -> new AtomicLong(0L));",
          "73:                 AtomicLong afterFilteredPages = queryAfterFilterParquetPages.get(queryId, () -> new AtomicLong(0L));",
          "74:                 LOGGER.info(\"Query total parquet pages {}, filtered pages {}, after filter pages {}, filter rate {}\",",
          "75:                         totalPages.get(), filteredPages.get(), afterFilteredPages.get(),",
          "76:                         (double) filteredPages.get() / totalPages.get());",
          "77:             }",
          "78:             queryTotalParquetPages.invalidate(queryId);",
          "79:             queryFilteredParquetPages.invalidate(queryId);",
          "80:             queryAfterFilterParquetPages.invalidate(queryId);",
          "82:         } catch (ExecutionException e) {",
          "83:             LOGGER.error(\"Error when log query metrics.\", e);",
          "84:         }",
          "86:     }",
          "87: }",
          "",
          "---------------"
        ],
        "src/spark-project/spark-common/src/test/java/org/apache/kylin/engine/spark/filter/ParquetPageFilterCollectorTest.java||src/spark-project/spark-common/src/test/java/org/apache/kylin/engine/spark/filter/ParquetPageFilterCollectorTest.java": [
          "File: src/spark-project/spark-common/src/test/java/org/apache/kylin/engine/spark/filter/ParquetPageFilterCollectorTest.java -> src/spark-project/spark-common/src/test/java/org/apache/kylin/engine/spark/filter/ParquetPageFilterCollectorTest.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "19: package org.apache.kylin.engine.spark.filter;",
          "21: import static org.junit.Assert.assertEquals;",
          "22: import static org.junit.Assert.assertNotNull;",
          "23: import static org.junit.Assert.assertNull;",
          "25: import org.junit.Test;",
          "27: public class ParquetPageFilterCollectorTest {",
          "29:     @Test",
          "30:     public void testParquetPageFilterCollector() {",
          "31:         String queryId = \"test_query_id\";",
          "32:         long totalPages = 10L;",
          "33:         long filteredPages = 5L;",
          "34:         long afterFilterPages = 3L;",
          "36:         ParquetPageFilterCollector.addQueryMetrics(queryId, totalPages, filteredPages, afterFilterPages);",
          "37:         assertNotNull(ParquetPageFilterCollector.queryTotalParquetPages.getIfPresent(queryId));",
          "38:         assertEquals(ParquetPageFilterCollector.queryTotalParquetPages.getIfPresent(queryId).get(), totalPages);",
          "39:         assertNotNull(ParquetPageFilterCollector.queryFilteredParquetPages.getIfPresent(queryId));",
          "40:         assertEquals(ParquetPageFilterCollector.queryFilteredParquetPages.getIfPresent(queryId).get(), filteredPages);",
          "41:         assertNotNull(ParquetPageFilterCollector.queryAfterFilterParquetPages.getIfPresent(queryId));",
          "42:         assertEquals(ParquetPageFilterCollector.queryAfterFilterParquetPages.getIfPresent(queryId).get(), afterFilterPages);",
          "44:         ParquetPageFilterCollector.logParquetPages(queryId);",
          "45:         assertNull(ParquetPageFilterCollector.queryTotalParquetPages.getIfPresent(queryId));",
          "46:         assertNull(ParquetPageFilterCollector.queryFilteredParquetPages.getIfPresent(queryId));",
          "47:         assertNull(ParquetPageFilterCollector.queryAfterFilterParquetPages.getIfPresent(queryId));",
          "48:     }",
          "49: }",
          "",
          "---------------"
        ]
      }
    }
  ]
}