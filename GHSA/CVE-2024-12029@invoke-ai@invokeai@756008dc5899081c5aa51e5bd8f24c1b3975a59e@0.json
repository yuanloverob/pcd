{
  "cve_id": "CVE-2024-12029",
  "cve_desc": "A remote code execution vulnerability exists in invoke-ai/invokeai versions 5.3.1 through 5.4.2 via the /api/v2/models/install API. The vulnerability arises from unsafe deserialization of model files using torch.load without proper validation. Attackers can exploit this by embedding malicious code in model files, which is executed upon loading. This issue is fixed in version 5.4.3.",
  "repo": "invoke-ai/invokeai",
  "patch_hash": "756008dc5899081c5aa51e5bd8f24c1b3975a59e",
  "patch_info": {
    "commit_hash": "756008dc5899081c5aa51e5bd8f24c1b3975a59e",
    "repo": "invoke-ai/invokeai",
    "commit_url": "https://github.com/invoke-ai/invokeai/commit/756008dc5899081c5aa51e5bd8f24c1b3975a59e",
    "files": [
      "invokeai/app/services/model_load/model_load_default.py",
      "invokeai/backend/model_manager/probe.py",
      "invokeai/backend/model_manager/util/model_util.py"
    ],
    "message": "fix: Fail scan on InvalidMagicError in picklescan, update default for read_checkpoint_meta to scan unless explicitly told not to",
    "before_after_code_files": [
      "invokeai/app/services/model_load/model_load_default.py||invokeai/app/services/model_load/model_load_default.py",
      "invokeai/backend/model_manager/probe.py||invokeai/backend/model_manager/probe.py",
      "invokeai/backend/model_manager/util/model_util.py||invokeai/backend/model_manager/util/model_util.py"
    ]
  },
  "patch_diff": {
    "invokeai/app/services/model_load/model_load_default.py||invokeai/app/services/model_load/model_load_default.py": [
      "File: invokeai/app/services/model_load/model_load_default.py -> invokeai/app/services/model_load/model_load_default.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "87:         def torch_load_file(checkpoint: Path) -> AnyModel:",
      "88:             scan_result = scan_file_path(checkpoint)",
      "90:                 raise Exception(\"The model at {checkpoint} is potentially infected by malware. Aborting load.\")",
      "91:             result = torch_load(checkpoint, map_location=\"cpu\")",
      "92:             return result",
      "",
      "[Removed Lines]",
      "89:             if scan_result.infected_files != 0:",
      "",
      "[Added Lines]",
      "89:             if scan_result.infected_files != 0 or scan_result.scan_err:",
      "",
      "---------------"
    ],
    "invokeai/backend/model_manager/probe.py||invokeai/backend/model_manager/probe.py": [
      "File: invokeai/backend/model_manager/probe.py -> invokeai/backend/model_manager/probe.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "469:         \"\"\"",
      "470:         # scan model",
      "471:         scan_result = scan_file_path(checkpoint)",
      "473:             raise Exception(\"The model {model_name} is potentially infected by malware. Aborting import.\")",
      "",
      "[Removed Lines]",
      "472:         if scan_result.infected_files != 0:",
      "",
      "[Added Lines]",
      "472:         if scan_result.infected_files != 0 or scan_result.scan_err:",
      "",
      "---------------"
    ],
    "invokeai/backend/model_manager/util/model_util.py||invokeai/backend/model_manager/util/model_util.py": [
      "File: invokeai/backend/model_manager/util/model_util.py -> invokeai/backend/model_manager/util/model_util.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "44:     return checkpoint",
      "48:     if str(path).endswith(\".safetensors\"):",
      "49:         try:",
      "50:             path_str = path.as_posix() if isinstance(path, Path) else path",
      "",
      "[Removed Lines]",
      "47: def read_checkpoint_meta(path: Union[str, Path], scan: bool = False) -> Dict[str, torch.Tensor]:",
      "",
      "[Added Lines]",
      "47: def read_checkpoint_meta(path: Union[str, Path], scan: bool = True) -> Dict[str, torch.Tensor]:",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "55:     else:",
      "56:         if scan:",
      "57:             scan_result = scan_file_path(path)",
      "59:                 raise Exception(f'The model file \"{path}\" is potentially infected by malware. Aborting import.')",
      "60:         if str(path).endswith(\".gguf\"):",
      "61:             # The GGUF reader used here uses numpy memmap, so these tensors are not loaded into memory during this function",
      "",
      "[Removed Lines]",
      "58:             if scan_result.infected_files != 0:",
      "",
      "[Added Lines]",
      "58:             if scan_result.infected_files != 0 or scan_result.scan_err:",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "ed46acee79460189b38c164321b14bdfbf3073c9",
      "candidate_info": {
        "commit_hash": "ed46acee79460189b38c164321b14bdfbf3073c9",
        "repo": "invoke-ai/invokeai",
        "commit_url": "https://github.com/invoke-ai/invokeai/commit/ed46acee79460189b38c164321b14bdfbf3073c9",
        "files": [
          "invokeai/app/services/model_load/model_load_default.py",
          "invokeai/backend/model_manager/probe.py",
          "invokeai/backend/model_manager/util/model_util.py"
        ],
        "message": "fix: Fail scan on InvalidMagicError in picklescan, update default for read_checkpoint_meta to scan unless explicitly told not to",
        "before_after_code_files": [
          "invokeai/app/services/model_load/model_load_default.py||invokeai/app/services/model_load/model_load_default.py",
          "invokeai/backend/model_manager/probe.py||invokeai/backend/model_manager/probe.py",
          "invokeai/backend/model_manager/util/model_util.py||invokeai/backend/model_manager/util/model_util.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "diff_branch_same_aad": 1,
        "olp_code_files": {
          "patch": [
            "invokeai/app/services/model_load/model_load_default.py||invokeai/app/services/model_load/model_load_default.py",
            "invokeai/backend/model_manager/probe.py||invokeai/backend/model_manager/probe.py",
            "invokeai/backend/model_manager/util/model_util.py||invokeai/backend/model_manager/util/model_util.py"
          ],
          "candidate": [
            "invokeai/app/services/model_load/model_load_default.py||invokeai/app/services/model_load/model_load_default.py",
            "invokeai/backend/model_manager/probe.py||invokeai/backend/model_manager/probe.py",
            "invokeai/backend/model_manager/util/model_util.py||invokeai/backend/model_manager/util/model_util.py"
          ]
        }
      },
      "candidate_diff": {
        "invokeai/app/services/model_load/model_load_default.py||invokeai/app/services/model_load/model_load_default.py": [
          "File: invokeai/app/services/model_load/model_load_default.py -> invokeai/app/services/model_load/model_load_default.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "87:         def torch_load_file(checkpoint: Path) -> AnyModel:",
          "88:             scan_result = scan_file_path(checkpoint)",
          "90:                 raise Exception(\"The model at {checkpoint} is potentially infected by malware. Aborting load.\")",
          "91:             result = torch_load(checkpoint, map_location=\"cpu\")",
          "92:             return result",
          "",
          "[Removed Lines]",
          "89:             if scan_result.infected_files != 0:",
          "",
          "[Added Lines]",
          "89:             if scan_result.infected_files != 0 or scan_result.scan_err:",
          "",
          "---------------"
        ],
        "invokeai/backend/model_manager/probe.py||invokeai/backend/model_manager/probe.py": [
          "File: invokeai/backend/model_manager/probe.py -> invokeai/backend/model_manager/probe.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "469:         \"\"\"",
          "470:         # scan model",
          "471:         scan_result = scan_file_path(checkpoint)",
          "473:             raise Exception(\"The model {model_name} is potentially infected by malware. Aborting import.\")",
          "",
          "[Removed Lines]",
          "472:         if scan_result.infected_files != 0:",
          "",
          "[Added Lines]",
          "472:         if scan_result.infected_files != 0 or scan_result.scan_err:",
          "",
          "---------------"
        ],
        "invokeai/backend/model_manager/util/model_util.py||invokeai/backend/model_manager/util/model_util.py": [
          "File: invokeai/backend/model_manager/util/model_util.py -> invokeai/backend/model_manager/util/model_util.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "44:     return checkpoint",
          "48:     if str(path).endswith(\".safetensors\"):",
          "49:         try:",
          "50:             path_str = path.as_posix() if isinstance(path, Path) else path",
          "",
          "[Removed Lines]",
          "47: def read_checkpoint_meta(path: Union[str, Path], scan: bool = False) -> Dict[str, torch.Tensor]:",
          "",
          "[Added Lines]",
          "47: def read_checkpoint_meta(path: Union[str, Path], scan: bool = True) -> Dict[str, torch.Tensor]:",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "55:     else:",
          "56:         if scan:",
          "57:             scan_result = scan_file_path(path)",
          "59:                 raise Exception(f'The model file \"{path}\" is potentially infected by malware. Aborting import.')",
          "60:         if str(path).endswith(\".gguf\"):",
          "61:             # The GGUF reader used here uses numpy memmap, so these tensors are not loaded into memory during this function",
          "",
          "[Removed Lines]",
          "58:             if scan_result.infected_files != 0:",
          "",
          "[Added Lines]",
          "58:             if scan_result.infected_files != 0 or scan_result.scan_err:",
          "",
          "---------------"
        ]
      }
    }
  ]
}