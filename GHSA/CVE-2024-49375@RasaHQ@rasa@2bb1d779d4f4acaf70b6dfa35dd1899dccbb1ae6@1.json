{
  "cve_id": "CVE-2024-49375",
  "cve_desc": "Open source machine learning framework. A vulnerability has been identified in Rasa that enables an attacker who has the ability to load a maliciously crafted model remotely into a Rasa instance to achieve Remote Code Execution. The prerequisites for this are: 1. The HTTP API must be enabled on the Rasa instance eg with `--enable-api`. This is not the default configuration. 2. For unauthenticated RCE to be exploitable, the user must not have configured any authentication or other security controls recommended in our documentation. 3. For authenticated RCE, the attacker must posses a valid authentication token or JWT to interact with the Rasa API. This issue has been addressed in rasa version 3.6.21 and all users are advised to upgrade. Users unable to upgrade should ensure that they require authentication and that only trusted users are given access.",
  "repo": "RasaHQ/rasa",
  "patch_hash": "2bb1d779d4f4acaf70b6dfa35dd1899dccbb1ae6",
  "patch_info": {
    "commit_hash": "2bb1d779d4f4acaf70b6dfa35dd1899dccbb1ae6",
    "repo": "RasaHQ/rasa",
    "commit_url": "https://github.com/RasaHQ/rasa/commit/2bb1d779d4f4acaf70b6dfa35dd1899dccbb1ae6",
    "files": [
      ".github/workflows/continous-integration.yml",
      "changelog/1424.bugfix.md",
      "poetry.lock",
      "pyproject.toml",
      "rasa/core/featurizers/single_state_featurizer.py",
      "rasa/core/featurizers/tracker_featurizers.py",
      "rasa/core/policies/ted_policy.py",
      "rasa/core/policies/unexpected_intent_policy.py",
      "rasa/nlu/classifiers/diet_classifier.py",
      "rasa/nlu/classifiers/logistic_regression_classifier.py",
      "rasa/nlu/classifiers/sklearn_intent_classifier.py",
      "rasa/nlu/extractors/crf_entity_extractor.py",
      "rasa/nlu/featurizers/sparse_featurizer/count_vectors_featurizer.py",
      "rasa/nlu/featurizers/sparse_featurizer/lexical_syntactic_featurizer.py",
      "rasa/nlu/featurizers/sparse_featurizer/regex_featurizer.py",
      "rasa/shared/nlu/training_data/features.py",
      "rasa/shared/utils/io.py",
      "rasa/utils/common.py",
      "rasa/utils/io.py",
      "rasa/utils/tensorflow/feature_array.py",
      "rasa/utils/tensorflow/model_data.py",
      "scripts/ping_slack_about_package_release.sh",
      "tests/core/featurizers/test_tracker_featurizer.py",
      "tests/nlu/extractors/test_crf_entity_extractor.py",
      "tests/shared/nlu/training_data/test_features.py",
      "tests/utils/tensorflow/test_feature_array.py",
      "tests/utils/test_io.py"
    ],
    "message": "Replace pickle with safer alternatives (#13067)\n\n* Update slack release notification step\n\n* [ENG-1424] Use `pickle` alternatives (#1453)\n\n* use json.dump and json.load in count_vectors_featurizer and lexical_syntactic_featurizer instead of pickle\n\n* update load and persist in sklearn intent classifier\n\n* update persist and load in dietclassifier\n\n* update load and persist in sklearn intent classifier\n\n* use json.dump and json.load in tracker featurizers\n\n* update persist and load of TEDPolicy\n\n* updated unexpected intent policy persist and load of model utilities.\n\n* save and load fake features\n\n* rename patterns.pkl to patterns.json\n\n* update poetry.lock\n\n* ruff formatting\n\n* move skops import\n\n* add comments\n\n* clean up save_features and load_features\n\n* WIP: update model data saving and loading\n\n* add tests for save and load features\n\n* update tests for test_tracker_featurizer\n\n* update tests for test_tracker_featurizer\n\n* WIP: serialization of feature arrays.\n\n* update serialization and deserialization for feature array\n\n* remove not needed tests/utils/tensorflow/test_model_data_storage.py\n\n* start writing tests for feature array\n\n* update feature array tests\n\n* update tests\n\n* fix linting\n\n* add changelog\n\n* add new dependencies to .github/dependabot.yml\n\n* fix some tests\n\n* fix loading and saving of unexpected intent ted policy\n\n* fix linting issue\n\n* fix converting of features in cvf and lsf\n\n* fix lint issues\n\n* convert vocab in cvf\n\n* fix linting\n\n* update crf entity extractor\n\n* fix to_dict of crf_token\n\n* addressed type issues\n\n* ruff formatting\n\n* fix typing and lint issues\n\n* remove cloudpickle dependency\n\n* update logistic_regression_classifier and remove joblib as dependency\n\n* update formatting of pyproject.toml\n\n* next try: update formatting of pyproject.toml\n\n* update logging\n\n* update poetry.lock\n\n* refactor loading of lexical_syntactic_featurizer\n\n* rename FeatureMetadata.type -> FeatureMetadata.data_type\n\n* clean up tests test_features.py and test_crf_entity_extractor.py\n\n* update test_feature_array.py\n\n* check for type when loading tracker featurizer.\n\n* update changelog\n\n* fix line too long\n\n* move import of skops\n\n* Prepared release of version 3.10.9.dev1 (#1496)\n\n* prepared release of version 3.10.9.dev1\n\n* update minimum model version\n\n* Check for 'step_id' and 'active_flow' keys in the metadata when adding 'ActionExecuted' event to flows paths stack.\n\n* fix parsing of commands\n\n* improve logging\n\n* formatting\n\n* add changelog\n\n* fix parse commands for multi step\n\n* [ATO-2985] - Windows model loading test (#1537)\n\n* Add test for model loading on windows\n\n* Improve the error message logged when handling the user message\n\n* Add a changelog\n\n* Fix Code Quality - line too long\n\n* Rasa-sdk-update (#1546)\n\n* all rasa-sdk micro updates\n\n* update poetry lock\n\n* update rasa-sdk in lock file\n\n* Remove trailing white sapce\n\n* Prepared release of version 3.10.11 (#1570)\n\n* prepared release of version 3.10.11\n\n* add comments again in pyproject.toml\n\n* update poetry.lock\n\n* revert changes in github workflows\n\n* undo changes in pyproject.toml\n\n* update changelog\n\n* revert changes in github workflows\n\n* update poetry.lock\n\n* update poetry.lock\n\n* update pyproject.toml\n\n* update poetry.lock\n\n* update setuptools = '>=65.5.1,<75.6.0'\n\n* update setuptools = '~75.3.0'\n\n* reformat code\n\n* undo deleting of ping_slack_about_package_release.sh\n\n* fix formatting and type issues\n\n* downgrade setuptools to 70.3.0\n\n* fixing logging issues (?)\n\n---------\n\nCo-authored-by: sancharigr <s.ghosh@rasa.com>",
    "before_after_code_files": [
      "poetry.lock||poetry.lock",
      "rasa/core/featurizers/single_state_featurizer.py||rasa/core/featurizers/single_state_featurizer.py",
      "rasa/core/featurizers/tracker_featurizers.py||rasa/core/featurizers/tracker_featurizers.py",
      "rasa/core/policies/ted_policy.py||rasa/core/policies/ted_policy.py",
      "rasa/core/policies/unexpected_intent_policy.py||rasa/core/policies/unexpected_intent_policy.py",
      "rasa/nlu/classifiers/diet_classifier.py||rasa/nlu/classifiers/diet_classifier.py",
      "rasa/nlu/classifiers/logistic_regression_classifier.py||rasa/nlu/classifiers/logistic_regression_classifier.py",
      "rasa/nlu/classifiers/sklearn_intent_classifier.py||rasa/nlu/classifiers/sklearn_intent_classifier.py",
      "rasa/nlu/extractors/crf_entity_extractor.py||rasa/nlu/extractors/crf_entity_extractor.py",
      "rasa/nlu/featurizers/sparse_featurizer/count_vectors_featurizer.py||rasa/nlu/featurizers/sparse_featurizer/count_vectors_featurizer.py",
      "rasa/nlu/featurizers/sparse_featurizer/lexical_syntactic_featurizer.py||rasa/nlu/featurizers/sparse_featurizer/lexical_syntactic_featurizer.py",
      "rasa/nlu/featurizers/sparse_featurizer/regex_featurizer.py||rasa/nlu/featurizers/sparse_featurizer/regex_featurizer.py",
      "rasa/shared/nlu/training_data/features.py||rasa/shared/nlu/training_data/features.py",
      "rasa/shared/utils/io.py||rasa/shared/utils/io.py",
      "rasa/utils/common.py||rasa/utils/common.py",
      "rasa/utils/io.py||rasa/utils/io.py",
      "rasa/utils/tensorflow/feature_array.py||rasa/utils/tensorflow/feature_array.py",
      "rasa/utils/tensorflow/model_data.py||rasa/utils/tensorflow/model_data.py",
      "scripts/ping_slack_about_package_release.sh||scripts/ping_slack_about_package_release.sh",
      "tests/core/featurizers/test_tracker_featurizer.py||tests/core/featurizers/test_tracker_featurizer.py",
      "tests/nlu/extractors/test_crf_entity_extractor.py||tests/nlu/extractors/test_crf_entity_extractor.py",
      "tests/shared/nlu/training_data/test_features.py||tests/shared/nlu/training_data/test_features.py",
      "tests/utils/tensorflow/test_feature_array.py||tests/utils/tensorflow/test_feature_array.py",
      "tests/utils/test_io.py||tests/utils/test_io.py"
    ]
  },
  "patch_diff": {
    "poetry.lock||poetry.lock": [
      "File: poetry.lock -> poetry.lock",
      "--- Hunk 1 ---",
      "[Context before]",
      "1412: name = \"filelock\"",
      "1413: version = \"3.12.2\"",
      "1414: description = \"A platform independent file lock.\"",
      "1416: python-versions = \">=3.7\"",
      "1417: files = [",
      "1418:     {file = \"filelock-3.12.2-py3-none-any.whl\", hash = \"sha256:cbb791cdea2a72f23da6ac5b5269ab0a0d161e9ef0100e653b69049a7706d1ec\"},",
      "",
      "[Removed Lines]",
      "1415: optional = true",
      "",
      "[Added Lines]",
      "1415: optional = false",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "2215: [[package]]",
      "2216: name = \"huggingface-hub\"",
      "2218: description = \"Client library to download and publish models, datasets and other repos on the huggingface.co hub\"",
      "2221: files = [",
      "2224: ]",
      "2226: [package.dependencies]",
      "2227: filelock = \"*\"",
      "2229: packaging = \">=20.9\"",
      "2230: pyyaml = \">=5.1\"",
      "2231: requests = \"*\"",
      "",
      "[Removed Lines]",
      "2217: version = \"0.16.2\"",
      "2219: optional = true",
      "2220: python-versions = \">=3.7.0\"",
      "2222:     {file = \"huggingface_hub-0.16.2-py3-none-any.whl\", hash = \"sha256:92facff575c11a8cf4b35d184ae67867a577a1b30865edcd8a9c5a48d2202133\"},",
      "2223:     {file = \"huggingface_hub-0.16.2.tar.gz\", hash = \"sha256:205abbf02a3408129a309f09e6d1a88d0c82de296b498682a813d9baa91c272f\"},",
      "2228: fsspec = \"*\"",
      "",
      "[Added Lines]",
      "2217: version = \"0.27.0\"",
      "2219: optional = false",
      "2220: python-versions = \">=3.8.0\"",
      "2222:     {file = \"huggingface_hub-0.27.0-py3-none-any.whl\", hash = \"sha256:8f2e834517f1f1ddf1ecc716f91b120d7333011b7485f665a9a412eacb1a2a81\"},",
      "2223:     {file = \"huggingface_hub-0.27.0.tar.gz\", hash = \"sha256:902cce1a1be5739f5589e560198a65a8edcfd3b830b1666f36e4b961f0454fac\"},",
      "2228: fsspec = \">=2023.5.0\"",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "2233: typing-extensions = \">=3.7.4.3\"",
      "2235: [package.extras]",
      "2237: cli = [\"InquirerPy (==0.3.4)\"]",
      "2239: fastai = [\"fastai (>=2.4)\", \"fastcore (>=1.3.27)\", \"toml\"]",
      "2242: tensorflow = [\"graphviz\", \"pydot\", \"tensorflow\"]",
      "2247: [[package]]",
      "2248: name = \"humanfriendly\"",
      "",
      "[Removed Lines]",
      "2236: all = [\"InquirerPy (==0.3.4)\", \"Jinja2\", \"Pillow\", \"aiohttp\", \"black (>=23.1,<24.0)\", \"gradio\", \"jedi\", \"mypy (==0.982)\", \"numpy\", \"pydantic\", \"pytest\", \"pytest-asyncio\", \"pytest-cov\", \"pytest-env\", \"pytest-vcr\", \"pytest-xdist\", \"ruff (>=0.0.241)\", \"soundfile\", \"types-PyYAML\", \"types-requests\", \"types-simplejson\", \"types-toml\", \"types-tqdm\", \"types-urllib3\", \"urllib3 (<2.0)\"]",
      "2238: dev = [\"InquirerPy (==0.3.4)\", \"Jinja2\", \"Pillow\", \"aiohttp\", \"black (>=23.1,<24.0)\", \"gradio\", \"jedi\", \"mypy (==0.982)\", \"numpy\", \"pydantic\", \"pytest\", \"pytest-asyncio\", \"pytest-cov\", \"pytest-env\", \"pytest-vcr\", \"pytest-xdist\", \"ruff (>=0.0.241)\", \"soundfile\", \"types-PyYAML\", \"types-requests\", \"types-simplejson\", \"types-toml\", \"types-tqdm\", \"types-urllib3\", \"urllib3 (<2.0)\"]",
      "2240: inference = [\"aiohttp\", \"pydantic\"]",
      "2241: quality = [\"black (>=23.1,<24.0)\", \"mypy (==0.982)\", \"ruff (>=0.0.241)\"]",
      "2243: testing = [\"InquirerPy (==0.3.4)\", \"Jinja2\", \"Pillow\", \"aiohttp\", \"gradio\", \"jedi\", \"numpy\", \"pydantic\", \"pytest\", \"pytest-asyncio\", \"pytest-cov\", \"pytest-env\", \"pytest-vcr\", \"pytest-xdist\", \"soundfile\", \"urllib3 (<2.0)\"]",
      "2244: torch = [\"torch\"]",
      "2245: typing = [\"pydantic\", \"types-PyYAML\", \"types-requests\", \"types-simplejson\", \"types-toml\", \"types-tqdm\", \"types-urllib3\"]",
      "",
      "[Added Lines]",
      "2236: all = [\"InquirerPy (==0.3.4)\", \"Jinja2\", \"Pillow\", \"aiohttp\", \"fastapi\", \"gradio (>=4.0.0)\", \"jedi\", \"libcst (==1.4.0)\", \"mypy (==1.5.1)\", \"numpy\", \"pytest (>=8.1.1,<8.2.2)\", \"pytest-asyncio\", \"pytest-cov\", \"pytest-env\", \"pytest-mock\", \"pytest-rerunfailures\", \"pytest-vcr\", \"pytest-xdist\", \"ruff (>=0.5.0)\", \"soundfile\", \"types-PyYAML\", \"types-requests\", \"types-simplejson\", \"types-toml\", \"types-tqdm\", \"types-urllib3\", \"typing-extensions (>=4.8.0)\", \"urllib3 (<2.0)\"]",
      "2238: dev = [\"InquirerPy (==0.3.4)\", \"Jinja2\", \"Pillow\", \"aiohttp\", \"fastapi\", \"gradio (>=4.0.0)\", \"jedi\", \"libcst (==1.4.0)\", \"mypy (==1.5.1)\", \"numpy\", \"pytest (>=8.1.1,<8.2.2)\", \"pytest-asyncio\", \"pytest-cov\", \"pytest-env\", \"pytest-mock\", \"pytest-rerunfailures\", \"pytest-vcr\", \"pytest-xdist\", \"ruff (>=0.5.0)\", \"soundfile\", \"types-PyYAML\", \"types-requests\", \"types-simplejson\", \"types-toml\", \"types-tqdm\", \"types-urllib3\", \"typing-extensions (>=4.8.0)\", \"urllib3 (<2.0)\"]",
      "2240: hf-transfer = [\"hf-transfer (>=0.1.4)\"]",
      "2241: inference = [\"aiohttp\"]",
      "2242: quality = [\"libcst (==1.4.0)\", \"mypy (==1.5.1)\", \"ruff (>=0.5.0)\"]",
      "2244: tensorflow-testing = [\"keras (<3.0)\", \"tensorflow\"]",
      "2245: testing = [\"InquirerPy (==0.3.4)\", \"Jinja2\", \"Pillow\", \"aiohttp\", \"fastapi\", \"gradio (>=4.0.0)\", \"jedi\", \"numpy\", \"pytest (>=8.1.1,<8.2.2)\", \"pytest-asyncio\", \"pytest-cov\", \"pytest-env\", \"pytest-mock\", \"pytest-rerunfailures\", \"pytest-vcr\", \"pytest-xdist\", \"soundfile\", \"urllib3 (<2.0)\"]",
      "2246: torch = [\"safetensors[torch]\", \"torch\"]",
      "2247: typing = [\"types-PyYAML\", \"types-requests\", \"types-simplejson\", \"types-toml\", \"types-tqdm\", \"types-urllib3\", \"typing-extensions (>=4.8.0)\"]",
      "",
      "---------------",
      "--- Hunk 4 ---",
      "[Context before]",
      "2417: [[package]]",
      "2418: name = \"joblib\"",
      "2420: description = \"Lightweight pipelining with Python functions\"",
      "2421: optional = false",
      "2423: files = [",
      "2426: ]",
      "2428: [[package]]",
      "2429: name = \"jsonpickle\"",
      "2432: optional = false",
      "2433: python-versions = \">=3.7\"",
      "2434: files = [",
      "2437: ]",
      "2439: [package.extras]",
      "2444: [[package]]",
      "2445: name = \"jsonschema\"",
      "",
      "[Removed Lines]",
      "2419: version = \"1.2.0\"",
      "2422: python-versions = \">=3.7\"",
      "2424:     {file = \"joblib-1.2.0-py3-none-any.whl\", hash = \"sha256:091138ed78f800342968c523bdde947e7a305b8594b910a0fea2ab83c3c6d385\"},",
      "2425:     {file = \"joblib-1.2.0.tar.gz\", hash = \"sha256:e1cee4a79e4af22881164f218d4311f60074197fb707e082e803b61f6d137018\"},",
      "2430: version = \"3.0.1\"",
      "2431: description = \"Python library for serializing any arbitrary object graph into JSON\"",
      "2435:     {file = \"jsonpickle-3.0.1-py2.py3-none-any.whl\", hash = \"sha256:130d8b293ea0add3845de311aaba55e6d706d0bb17bc123bd2c8baf8a39ac77c\"},",
      "2436:     {file = \"jsonpickle-3.0.1.tar.gz\", hash = \"sha256:032538804795e73b94ead410800ac387fdb6de98f8882ac957fcd247e3a85200\"},",
      "2440: docs = [\"jaraco.packaging (>=3.2)\", \"rst.linker (>=1.9)\", \"sphinx\"]",
      "2441: testing = [\"ecdsa\", \"feedparser\", \"gmpy2\", \"numpy\", \"pandas\", \"pymongo\", \"pytest (>=3.5,!=3.7.3)\", \"pytest-black-multipy\", \"pytest-checkdocs (>=1.2.3)\", \"pytest-cov\", \"pytest-flake8 (>=1.1.1)\", \"scikit-learn\", \"sqlalchemy\"]",
      "2442: testing-libs = [\"simplejson\", \"ujson\"]",
      "",
      "[Added Lines]",
      "2421: version = \"1.4.2\"",
      "2424: python-versions = \">=3.8\"",
      "2426:     {file = \"joblib-1.4.2-py3-none-any.whl\", hash = \"sha256:06d478d5674cbc267e7496a410ee875abd68e4340feff4490bcb7afb88060ae6\"},",
      "2427:     {file = \"joblib-1.4.2.tar.gz\", hash = \"sha256:2382c5816b2636fbd20a09e0f4e9dad4736765fdfb7dca582943b9c1366b3f0e\"},",
      "2432: version = \"3.0.4\"",
      "2433: description = \"Serialize any Python object to JSON\"",
      "2437:     {file = \"jsonpickle-3.0.4-py3-none-any.whl\", hash = \"sha256:04ae7567a14269579e3af66b76bda284587458d7e8a204951ca8f71a3309952e\"},",
      "2438:     {file = \"jsonpickle-3.0.4.tar.gz\", hash = \"sha256:a1b14c8d6221cd8f394f2a97e735ea1d7edc927fbd135b26f2f8700657c8c62b\"},",
      "2442: docs = [\"furo\", \"rst.linker (>=1.9)\", \"sphinx\"]",
      "2443: packaging = [\"build\", \"twine\"]",
      "2444: testing = [\"bson\", \"ecdsa\", \"feedparser\", \"gmpy2\", \"numpy\", \"pandas\", \"pymongo\", \"pytest (>=3.5,!=3.7.3)\", \"pytest-benchmark\", \"pytest-benchmark[histogram]\", \"pytest-checkdocs (>=1.2.3)\", \"pytest-cov\", \"pytest-enabler (>=1.0.1)\", \"pytest-ruff (>=0.2.1)\", \"scikit-learn\", \"scipy\", \"scipy (>=1.9.3)\", \"simplejson\", \"sqlalchemy\", \"ujson\"]",
      "",
      "---------------",
      "--- Hunk 5 ---",
      "[Context before]",
      "4746: [package.extras]",
      "4747: crt = [\"botocore[crt] (>=1.20.29,<2.0a.0)\"]",
      "4749: [[package]]",
      "4750: name = \"sanic\"",
      "4751: version = \"21.12.2\"",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "4751: [[package]]",
      "4752: name = \"safetensors\"",
      "4753: version = \"0.4.5\"",
      "4754: description = \"\"",
      "4755: optional = false",
      "4756: python-versions = \">=3.7\"",
      "4757: files = [",
      "4758:     {file = \"safetensors-0.4.5-cp310-cp310-macosx_10_12_x86_64.whl\", hash = \"sha256:a63eaccd22243c67e4f2b1c3e258b257effc4acd78f3b9d397edc8cf8f1298a7\"},",
      "4759:     {file = \"safetensors-0.4.5-cp310-cp310-macosx_11_0_arm64.whl\", hash = \"sha256:23fc9b4ec7b602915cbb4ec1a7c1ad96d2743c322f20ab709e2c35d1b66dad27\"},",
      "4760:     {file = \"safetensors-0.4.5-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:6885016f34bef80ea1085b7e99b3c1f92cb1be78a49839203060f67b40aee761\"},",
      "4761:     {file = \"safetensors-0.4.5-cp310-cp310-manylinux_2_17_armv7l.manylinux2014_armv7l.whl\", hash = \"sha256:133620f443450429322f238fda74d512c4008621227fccf2f8cf4a76206fea7c\"},",
      "4762:     {file = \"safetensors-0.4.5-cp310-cp310-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:4fb3e0609ec12d2a77e882f07cced530b8262027f64b75d399f1504ffec0ba56\"},",
      "4763:     {file = \"safetensors-0.4.5-cp310-cp310-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:d0f1dd769f064adc33831f5e97ad07babbd728427f98e3e1db6902e369122737\"},",
      "4764:     {file = \"safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:c6d156bdb26732feada84f9388a9f135528c1ef5b05fae153da365ad4319c4c5\"},",
      "4765:     {file = \"safetensors-0.4.5-cp310-cp310-manylinux_2_5_i686.manylinux1_i686.whl\", hash = \"sha256:9e347d77e2c77eb7624400ccd09bed69d35c0332f417ce8c048d404a096c593b\"},",
      "4766:     {file = \"safetensors-0.4.5-cp310-cp310-musllinux_1_1_aarch64.whl\", hash = \"sha256:9f556eea3aec1d3d955403159fe2123ddd68e880f83954ee9b4a3f2e15e716b6\"},",
      "4767:     {file = \"safetensors-0.4.5-cp310-cp310-musllinux_1_1_x86_64.whl\", hash = \"sha256:9483f42be3b6bc8ff77dd67302de8ae411c4db39f7224dec66b0eb95822e4163\"},",
      "4768:     {file = \"safetensors-0.4.5-cp310-none-win32.whl\", hash = \"sha256:7389129c03fadd1ccc37fd1ebbc773f2b031483b04700923c3511d2a939252cc\"},",
      "4769:     {file = \"safetensors-0.4.5-cp310-none-win_amd64.whl\", hash = \"sha256:e98ef5524f8b6620c8cdef97220c0b6a5c1cef69852fcd2f174bb96c2bb316b1\"},",
      "4770:     {file = \"safetensors-0.4.5-cp311-cp311-macosx_10_12_x86_64.whl\", hash = \"sha256:21f848d7aebd5954f92538552d6d75f7c1b4500f51664078b5b49720d180e47c\"},",
      "4771:     {file = \"safetensors-0.4.5-cp311-cp311-macosx_11_0_arm64.whl\", hash = \"sha256:bb07000b19d41e35eecef9a454f31a8b4718a185293f0d0b1c4b61d6e4487971\"},",
      "4772:     {file = \"safetensors-0.4.5-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:09dedf7c2fda934ee68143202acff6e9e8eb0ddeeb4cfc24182bef999efa9f42\"},",
      "4773:     {file = \"safetensors-0.4.5-cp311-cp311-manylinux_2_17_armv7l.manylinux2014_armv7l.whl\", hash = \"sha256:59b77e4b7a708988d84f26de3ebead61ef1659c73dcbc9946c18f3b1786d2688\"},",
      "4774:     {file = \"safetensors-0.4.5-cp311-cp311-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:5d3bc83e14d67adc2e9387e511097f254bd1b43c3020440e708858c684cbac68\"},",
      "4775:     {file = \"safetensors-0.4.5-cp311-cp311-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:39371fc551c1072976073ab258c3119395294cf49cdc1f8476794627de3130df\"},",
      "4776:     {file = \"safetensors-0.4.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:a6c19feda32b931cae0acd42748a670bdf56bee6476a046af20181ad3fee4090\"},",
      "4777:     {file = \"safetensors-0.4.5-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.whl\", hash = \"sha256:a659467495de201e2f282063808a41170448c78bada1e62707b07a27b05e6943\"},",
      "4778:     {file = \"safetensors-0.4.5-cp311-cp311-musllinux_1_1_aarch64.whl\", hash = \"sha256:bad5e4b2476949bcd638a89f71b6916fa9a5cae5c1ae7eede337aca2100435c0\"},",
      "4779:     {file = \"safetensors-0.4.5-cp311-cp311-musllinux_1_1_x86_64.whl\", hash = \"sha256:a3a315a6d0054bc6889a17f5668a73f94f7fe55121ff59e0a199e3519c08565f\"},",
      "4780:     {file = \"safetensors-0.4.5-cp311-none-win32.whl\", hash = \"sha256:a01e232e6d3d5cf8b1667bc3b657a77bdab73f0743c26c1d3c5dd7ce86bd3a92\"},",
      "4781:     {file = \"safetensors-0.4.5-cp311-none-win_amd64.whl\", hash = \"sha256:cbd39cae1ad3e3ef6f63a6f07296b080c951f24cec60188378e43d3713000c04\"},",
      "4782:     {file = \"safetensors-0.4.5-cp312-cp312-macosx_10_12_x86_64.whl\", hash = \"sha256:473300314e026bd1043cef391bb16a8689453363381561b8a3e443870937cc1e\"},",
      "4783:     {file = \"safetensors-0.4.5-cp312-cp312-macosx_11_0_arm64.whl\", hash = \"sha256:801183a0f76dc647f51a2d9141ad341f9665602a7899a693207a82fb102cc53e\"},",
      "4784:     {file = \"safetensors-0.4.5-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:1524b54246e422ad6fb6aea1ac71edeeb77666efa67230e1faf6999df9b2e27f\"},",
      "4785:     {file = \"safetensors-0.4.5-cp312-cp312-manylinux_2_17_armv7l.manylinux2014_armv7l.whl\", hash = \"sha256:b3139098e3e8b2ad7afbca96d30ad29157b50c90861084e69fcb80dec7430461\"},",
      "4786:     {file = \"safetensors-0.4.5-cp312-cp312-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:65573dc35be9059770808e276b017256fa30058802c29e1038eb1c00028502ea\"},",
      "4787:     {file = \"safetensors-0.4.5-cp312-cp312-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:fd33da8e9407559f8779c82a0448e2133737f922d71f884da27184549416bfed\"},",
      "4788:     {file = \"safetensors-0.4.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:3685ce7ed036f916316b567152482b7e959dc754fcc4a8342333d222e05f407c\"},",
      "4789:     {file = \"safetensors-0.4.5-cp312-cp312-manylinux_2_5_i686.manylinux1_i686.whl\", hash = \"sha256:dde2bf390d25f67908278d6f5d59e46211ef98e44108727084d4637ee70ab4f1\"},",
      "4790:     {file = \"safetensors-0.4.5-cp312-cp312-musllinux_1_1_aarch64.whl\", hash = \"sha256:7469d70d3de970b1698d47c11ebbf296a308702cbaae7fcb993944751cf985f4\"},",
      "4791:     {file = \"safetensors-0.4.5-cp312-cp312-musllinux_1_1_x86_64.whl\", hash = \"sha256:3a6ba28118636a130ccbb968bc33d4684c48678695dba2590169d5ab03a45646\"},",
      "4792:     {file = \"safetensors-0.4.5-cp312-none-win32.whl\", hash = \"sha256:c859c7ed90b0047f58ee27751c8e56951452ed36a67afee1b0a87847d065eec6\"},",
      "4793:     {file = \"safetensors-0.4.5-cp312-none-win_amd64.whl\", hash = \"sha256:b5a8810ad6a6f933fff6c276eae92c1da217b39b4d8b1bc1c0b8af2d270dc532\"},",
      "4794:     {file = \"safetensors-0.4.5-cp313-cp313-macosx_10_12_x86_64.whl\", hash = \"sha256:25e5f8e2e92a74f05b4ca55686234c32aac19927903792b30ee6d7bd5653d54e\"},",
      "4795:     {file = \"safetensors-0.4.5-cp313-cp313-macosx_11_0_arm64.whl\", hash = \"sha256:81efb124b58af39fcd684254c645e35692fea81c51627259cdf6d67ff4458916\"},",
      "4796:     {file = \"safetensors-0.4.5-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:585f1703a518b437f5103aa9cf70e9bd437cb78eea9c51024329e4fb8a3e3679\"},",
      "4797:     {file = \"safetensors-0.4.5-cp313-cp313-manylinux_2_17_armv7l.manylinux2014_armv7l.whl\", hash = \"sha256:4b99fbf72e3faf0b2f5f16e5e3458b93b7d0a83984fe8d5364c60aa169f2da89\"},",
      "4798:     {file = \"safetensors-0.4.5-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:b17b299ca9966ca983ecda1c0791a3f07f9ca6ab5ded8ef3d283fff45f6bcd5f\"},",
      "4799:     {file = \"safetensors-0.4.5-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:76ded72f69209c9780fdb23ea89e56d35c54ae6abcdec67ccb22af8e696e449a\"},",
      "4800:     {file = \"safetensors-0.4.5-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:2783956926303dcfeb1de91a4d1204cd4089ab441e622e7caee0642281109db3\"},",
      "4801:     {file = \"safetensors-0.4.5-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.whl\", hash = \"sha256:d94581aab8c6b204def4d7320f07534d6ee34cd4855688004a4354e63b639a35\"},",
      "4802:     {file = \"safetensors-0.4.5-cp313-cp313-musllinux_1_1_aarch64.whl\", hash = \"sha256:67e1e7cb8678bb1b37ac48ec0df04faf689e2f4e9e81e566b5c63d9f23748523\"},",
      "4803:     {file = \"safetensors-0.4.5-cp313-cp313-musllinux_1_1_x86_64.whl\", hash = \"sha256:dbd280b07e6054ea68b0cb4b16ad9703e7d63cd6890f577cb98acc5354780142\"},",
      "4804:     {file = \"safetensors-0.4.5-cp37-cp37m-macosx_10_12_x86_64.whl\", hash = \"sha256:77d9b228da8374c7262046a36c1f656ba32a93df6cc51cd4453af932011e77f1\"},",
      "4805:     {file = \"safetensors-0.4.5-cp37-cp37m-macosx_11_0_arm64.whl\", hash = \"sha256:500cac01d50b301ab7bb192353317035011c5ceeef0fca652f9f43c000bb7f8d\"},",
      "4806:     {file = \"safetensors-0.4.5-cp37-cp37m-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:75331c0c746f03158ded32465b7d0b0e24c5a22121743662a2393439c43a45cf\"},",
      "4807:     {file = \"safetensors-0.4.5-cp37-cp37m-manylinux_2_17_armv7l.manylinux2014_armv7l.whl\", hash = \"sha256:670e95fe34e0d591d0529e5e59fd9d3d72bc77b1444fcaa14dccda4f36b5a38b\"},",
      "4808:     {file = \"safetensors-0.4.5-cp37-cp37m-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:098923e2574ff237c517d6e840acada8e5b311cb1fa226019105ed82e9c3b62f\"},",
      "4809:     {file = \"safetensors-0.4.5-cp37-cp37m-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:13ca0902d2648775089fa6a0c8fc9e6390c5f8ee576517d33f9261656f851e3f\"},",
      "4810:     {file = \"safetensors-0.4.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:5f0032bedc869c56f8d26259fe39cd21c5199cd57f2228d817a0e23e8370af25\"},",
      "4811:     {file = \"safetensors-0.4.5-cp37-cp37m-manylinux_2_5_i686.manylinux1_i686.whl\", hash = \"sha256:f4b15f51b4f8f2a512341d9ce3475cacc19c5fdfc5db1f0e19449e75f95c7dc8\"},",
      "4812:     {file = \"safetensors-0.4.5-cp37-cp37m-musllinux_1_1_aarch64.whl\", hash = \"sha256:f6594d130d0ad933d885c6a7b75c5183cb0e8450f799b80a39eae2b8508955eb\"},",
      "4813:     {file = \"safetensors-0.4.5-cp37-cp37m-musllinux_1_1_x86_64.whl\", hash = \"sha256:60c828a27e852ded2c85fc0f87bf1ec20e464c5cd4d56ff0e0711855cc2e17f8\"},",
      "4814:     {file = \"safetensors-0.4.5-cp37-none-win32.whl\", hash = \"sha256:6d3de65718b86c3eeaa8b73a9c3d123f9307a96bbd7be9698e21e76a56443af5\"},",
      "4815:     {file = \"safetensors-0.4.5-cp37-none-win_amd64.whl\", hash = \"sha256:5a2d68a523a4cefd791156a4174189a4114cf0bf9c50ceb89f261600f3b2b81a\"},",
      "4816:     {file = \"safetensors-0.4.5-cp38-cp38-macosx_10_12_x86_64.whl\", hash = \"sha256:e7a97058f96340850da0601a3309f3d29d6191b0702b2da201e54c6e3e44ccf0\"},",
      "4817:     {file = \"safetensors-0.4.5-cp38-cp38-macosx_11_0_arm64.whl\", hash = \"sha256:63bfd425e25f5c733f572e2246e08a1c38bd6f2e027d3f7c87e2e43f228d1345\"},",
      "4818:     {file = \"safetensors-0.4.5-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:f3664ac565d0e809b0b929dae7ccd74e4d3273cd0c6d1220c6430035befb678e\"},",
      "4819:     {file = \"safetensors-0.4.5-cp38-cp38-manylinux_2_17_armv7l.manylinux2014_armv7l.whl\", hash = \"sha256:313514b0b9b73ff4ddfb4edd71860696dbe3c1c9dc4d5cc13dbd74da283d2cbf\"},",
      "4820:     {file = \"safetensors-0.4.5-cp38-cp38-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:31fa33ee326f750a2f2134a6174773c281d9a266ccd000bd4686d8021f1f3dac\"},",
      "4821:     {file = \"safetensors-0.4.5-cp38-cp38-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:09566792588d77b68abe53754c9f1308fadd35c9f87be939e22c623eaacbed6b\"},",
      "4822:     {file = \"safetensors-0.4.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:309aaec9b66cbf07ad3a2e5cb8a03205663324fea024ba391594423d0f00d9fe\"},",
      "4823:     {file = \"safetensors-0.4.5-cp38-cp38-manylinux_2_5_i686.manylinux1_i686.whl\", hash = \"sha256:53946c5813b8f9e26103c5efff4a931cc45d874f45229edd68557ffb35ffb9f8\"},",
      "4824:     {file = \"safetensors-0.4.5-cp38-cp38-musllinux_1_1_aarch64.whl\", hash = \"sha256:868f9df9e99ad1e7f38c52194063a982bc88fedc7d05096f4f8160403aaf4bd6\"},",
      "4825:     {file = \"safetensors-0.4.5-cp38-cp38-musllinux_1_1_x86_64.whl\", hash = \"sha256:9cc9449bd0b0bc538bd5e268221f0c5590bc5c14c1934a6ae359d44410dc68c4\"},",
      "4826:     {file = \"safetensors-0.4.5-cp38-none-win32.whl\", hash = \"sha256:83c4f13a9e687335c3928f615cd63a37e3f8ef072a3f2a0599fa09f863fb06a2\"},",
      "4827:     {file = \"safetensors-0.4.5-cp38-none-win_amd64.whl\", hash = \"sha256:b98d40a2ffa560653f6274e15b27b3544e8e3713a44627ce268f419f35c49478\"},",
      "4828:     {file = \"safetensors-0.4.5-cp39-cp39-macosx_10_12_x86_64.whl\", hash = \"sha256:cf727bb1281d66699bef5683b04d98c894a2803442c490a8d45cd365abfbdeb2\"},",
      "4829:     {file = \"safetensors-0.4.5-cp39-cp39-macosx_11_0_arm64.whl\", hash = \"sha256:96f1d038c827cdc552d97e71f522e1049fef0542be575421f7684756a748e457\"},",
      "4830:     {file = \"safetensors-0.4.5-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:139fbee92570ecea774e6344fee908907db79646d00b12c535f66bc78bd5ea2c\"},",
      "4831:     {file = \"safetensors-0.4.5-cp39-cp39-manylinux_2_17_armv7l.manylinux2014_armv7l.whl\", hash = \"sha256:c36302c1c69eebb383775a89645a32b9d266878fab619819ce660309d6176c9b\"},",
      "4832:     {file = \"safetensors-0.4.5-cp39-cp39-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:d641f5b8149ea98deb5ffcf604d764aad1de38a8285f86771ce1abf8e74c4891\"},",
      "4833:     {file = \"safetensors-0.4.5-cp39-cp39-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:b4db6a61d968de73722b858038c616a1bebd4a86abe2688e46ca0cc2d17558f2\"},",
      "4834:     {file = \"safetensors-0.4.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:b75a616e02f21b6f1d5785b20cecbab5e2bd3f6358a90e8925b813d557666ec1\"},",
      "4835:     {file = \"safetensors-0.4.5-cp39-cp39-manylinux_2_5_i686.manylinux1_i686.whl\", hash = \"sha256:788ee7d04cc0e0e7f944c52ff05f52a4415b312f5efd2ee66389fb7685ee030c\"},",
      "4836:     {file = \"safetensors-0.4.5-cp39-cp39-musllinux_1_1_aarch64.whl\", hash = \"sha256:87bc42bd04fd9ca31396d3ca0433db0be1411b6b53ac5a32b7845a85d01ffc2e\"},",
      "4837:     {file = \"safetensors-0.4.5-cp39-cp39-musllinux_1_1_x86_64.whl\", hash = \"sha256:4037676c86365a721a8c9510323a51861d703b399b78a6b4486a54a65a975fca\"},",
      "4838:     {file = \"safetensors-0.4.5-cp39-none-win32.whl\", hash = \"sha256:1500418454529d0ed5c1564bda376c4ddff43f30fce9517d9bee7bcce5a8ef50\"},",
      "4839:     {file = \"safetensors-0.4.5-cp39-none-win_amd64.whl\", hash = \"sha256:9d1a94b9d793ed8fe35ab6d5cea28d540a46559bafc6aae98f30ee0867000cab\"},",
      "4840:     {file = \"safetensors-0.4.5-pp310-pypy310_pp73-macosx_10_12_x86_64.whl\", hash = \"sha256:fdadf66b5a22ceb645d5435a0be7a0292ce59648ca1d46b352f13cff3ea80410\"},",
      "4841:     {file = \"safetensors-0.4.5-pp310-pypy310_pp73-macosx_11_0_arm64.whl\", hash = \"sha256:d42ffd4c2259f31832cb17ff866c111684c87bd930892a1ba53fed28370c918c\"},",
      "4842:     {file = \"safetensors-0.4.5-pp310-pypy310_pp73-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:dd8a1f6d2063a92cd04145c7fd9e31a1c7d85fbec20113a14b487563fdbc0597\"},",
      "4843:     {file = \"safetensors-0.4.5-pp310-pypy310_pp73-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:951d2fcf1817f4fb0ef0b48f6696688a4e852a95922a042b3f96aaa67eedc920\"},",
      "4844:     {file = \"safetensors-0.4.5-pp310-pypy310_pp73-manylinux_2_5_i686.manylinux1_i686.whl\", hash = \"sha256:6ac85d9a8c1af0e3132371d9f2d134695a06a96993c2e2f0bbe25debb9e3f67a\"},",
      "4845:     {file = \"safetensors-0.4.5-pp310-pypy310_pp73-musllinux_1_1_aarch64.whl\", hash = \"sha256:e3cec4a29eb7fe8da0b1c7988bc3828183080439dd559f720414450de076fcab\"},",
      "4846:     {file = \"safetensors-0.4.5-pp310-pypy310_pp73-musllinux_1_1_x86_64.whl\", hash = \"sha256:21742b391b859e67b26c0b2ac37f52c9c0944a879a25ad2f9f9f3cd61e7fda8f\"},",
      "4847:     {file = \"safetensors-0.4.5-pp37-pypy37_pp73-macosx_10_12_x86_64.whl\", hash = \"sha256:c7db3006a4915151ce1913652e907cdede299b974641a83fbc092102ac41b644\"},",
      "4848:     {file = \"safetensors-0.4.5-pp37-pypy37_pp73-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:f68bf99ea970960a237f416ea394e266e0361895753df06e3e06e6ea7907d98b\"},",
      "4849:     {file = \"safetensors-0.4.5-pp37-pypy37_pp73-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:8158938cf3324172df024da511839d373c40fbfaa83e9abf467174b2910d7b4c\"},",
      "4850:     {file = \"safetensors-0.4.5-pp37-pypy37_pp73-manylinux_2_5_i686.manylinux1_i686.whl\", hash = \"sha256:540ce6c4bf6b58cb0fd93fa5f143bc0ee341c93bb4f9287ccd92cf898cc1b0dd\"},",
      "4851:     {file = \"safetensors-0.4.5-pp37-pypy37_pp73-musllinux_1_1_aarch64.whl\", hash = \"sha256:bfeaa1a699c6b9ed514bd15e6a91e74738b71125a9292159e3d6b7f0a53d2cde\"},",
      "4852:     {file = \"safetensors-0.4.5-pp37-pypy37_pp73-musllinux_1_1_x86_64.whl\", hash = \"sha256:01c8f00da537af711979e1b42a69a8ec9e1d7112f208e0e9b8a35d2c381085ef\"},",
      "4853:     {file = \"safetensors-0.4.5-pp38-pypy38_pp73-macosx_10_12_x86_64.whl\", hash = \"sha256:a0dd565f83b30f2ca79b5d35748d0d99dd4b3454f80e03dfb41f0038e3bdf180\"},",
      "4854:     {file = \"safetensors-0.4.5-pp38-pypy38_pp73-macosx_11_0_arm64.whl\", hash = \"sha256:023b6e5facda76989f4cba95a861b7e656b87e225f61811065d5c501f78cdb3f\"},",
      "4855:     {file = \"safetensors-0.4.5-pp38-pypy38_pp73-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:9633b663393d5796f0b60249549371e392b75a0b955c07e9c6f8708a87fc841f\"},",
      "4856:     {file = \"safetensors-0.4.5-pp38-pypy38_pp73-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:78dd8adfb48716233c45f676d6e48534d34b4bceb50162c13d1f0bdf6f78590a\"},",
      "4857:     {file = \"safetensors-0.4.5-pp38-pypy38_pp73-manylinux_2_5_i686.manylinux1_i686.whl\", hash = \"sha256:8e8deb16c4321d61ae72533b8451ec4a9af8656d1c61ff81aa49f966406e4b68\"},",
      "4858:     {file = \"safetensors-0.4.5-pp38-pypy38_pp73-musllinux_1_1_aarch64.whl\", hash = \"sha256:52452fa5999dc50c4decaf0c53aa28371f7f1e0fe5c2dd9129059fbe1e1599c7\"},",
      "4859:     {file = \"safetensors-0.4.5-pp38-pypy38_pp73-musllinux_1_1_x86_64.whl\", hash = \"sha256:d5f23198821e227cfc52d50fa989813513db381255c6d100927b012f0cfec63d\"},",
      "4860:     {file = \"safetensors-0.4.5-pp39-pypy39_pp73-macosx_10_12_x86_64.whl\", hash = \"sha256:f4beb84b6073b1247a773141a6331117e35d07134b3bb0383003f39971d414bb\"},",
      "4861:     {file = \"safetensors-0.4.5-pp39-pypy39_pp73-macosx_11_0_arm64.whl\", hash = \"sha256:68814d599d25ed2fdd045ed54d370d1d03cf35e02dce56de44c651f828fb9b7b\"},",
      "4862:     {file = \"safetensors-0.4.5-pp39-pypy39_pp73-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:f0b6453c54c57c1781292c46593f8a37254b8b99004c68d6c3ce229688931a22\"},",
      "4863:     {file = \"safetensors-0.4.5-pp39-pypy39_pp73-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:adaa9c6dead67e2dd90d634f89131e43162012479d86e25618e821a03d1eb1dc\"},",
      "4864:     {file = \"safetensors-0.4.5-pp39-pypy39_pp73-manylinux_2_5_i686.manylinux1_i686.whl\", hash = \"sha256:73e7d408e9012cd17511b382b43547850969c7979efc2bc353f317abaf23c84c\"},",
      "4865:     {file = \"safetensors-0.4.5-pp39-pypy39_pp73-musllinux_1_1_aarch64.whl\", hash = \"sha256:775409ce0fcc58b10773fdb4221ed1eb007de10fe7adbdf8f5e8a56096b6f0bc\"},",
      "4866:     {file = \"safetensors-0.4.5-pp39-pypy39_pp73-musllinux_1_1_x86_64.whl\", hash = \"sha256:834001bed193e4440c4a3950a31059523ee5090605c907c66808664c932b549c\"},",
      "4867:     {file = \"safetensors-0.4.5.tar.gz\", hash = \"sha256:d73de19682deabb02524b3d5d1f8b3aaba94c72f1bbfc7911b9b9d5d391c0310\"},",
      "4868: ]",
      "4870: [package.extras]",
      "4871: all = [\"safetensors[jax]\", \"safetensors[numpy]\", \"safetensors[paddlepaddle]\", \"safetensors[pinned-tf]\", \"safetensors[quality]\", \"safetensors[testing]\", \"safetensors[torch]\"]",
      "4872: dev = [\"safetensors[all]\"]",
      "4873: jax = [\"flax (>=0.6.3)\", \"jax (>=0.3.25)\", \"jaxlib (>=0.3.25)\", \"safetensors[numpy]\"]",
      "4874: mlx = [\"mlx (>=0.0.9)\"]",
      "4875: numpy = [\"numpy (>=1.21.6)\"]",
      "4876: paddlepaddle = [\"paddlepaddle (>=2.4.1)\", \"safetensors[numpy]\"]",
      "4877: pinned-tf = [\"safetensors[numpy]\", \"tensorflow (==2.11.0)\"]",
      "4878: quality = [\"black (==22.3)\", \"click (==8.0.4)\", \"flake8 (>=3.8.3)\", \"isort (>=5.5.4)\"]",
      "4879: tensorflow = [\"safetensors[numpy]\", \"tensorflow (>=2.11.0)\"]",
      "4880: testing = [\"h5py (>=3.7.0)\", \"huggingface-hub (>=0.12.1)\", \"hypothesis (>=6.70.2)\", \"pytest (>=7.2.0)\", \"pytest-benchmark (>=4.0.0)\", \"safetensors[numpy]\", \"setuptools-rust (>=1.5.2)\"]",
      "4881: torch = [\"safetensors[numpy]\", \"torch (>=1.10)\"]",
      "",
      "---------------",
      "--- Hunk 6 ---",
      "[Context before]",
      "5015: [[package]]",
      "5016: name = \"setuptools\"",
      "5018: description = \"Easily download, build, install, upgrade, and uninstall Python packages\"",
      "5019: optional = false",
      "5021: files = [",
      "5024: ]",
      "5026: [package.extras]",
      "5031: [[package]]",
      "5032: name = \"six\"",
      "",
      "[Removed Lines]",
      "5017: version = \"68.0.0\"",
      "5020: python-versions = \">=3.7\"",
      "5022:     {file = \"setuptools-68.0.0-py3-none-any.whl\", hash = \"sha256:11e52c67415a381d10d6b462ced9cfb97066179f0e871399e006c4ab101fc85f\"},",
      "5023:     {file = \"setuptools-68.0.0.tar.gz\", hash = \"sha256:baf1fdb41c6da4cd2eae722e135500da913332ab3f2f5c7d33af9b492acb5235\"},",
      "5027: docs = [\"furo\", \"jaraco.packaging (>=9)\", \"jaraco.tidelift (>=1.4)\", \"pygments-github-lexers (==0.0.5)\", \"rst.linker (>=1.9)\", \"sphinx (>=3.5)\", \"sphinx-favicon\", \"sphinx-hoverxref (<2)\", \"sphinx-inline-tabs\", \"sphinx-lint\", \"sphinx-notfound-page (==0.8.3)\", \"sphinx-reredirects\", \"sphinxcontrib-towncrier\"]",
      "5028: testing = [\"build[virtualenv]\", \"filelock (>=3.4.0)\", \"flake8-2020\", \"ini2toml[lite] (>=0.9)\", \"jaraco.envs (>=2.2)\", \"jaraco.path (>=3.2.0)\", \"pip (>=19.1)\", \"pip-run (>=8.8)\", \"pytest (>=6)\", \"pytest-black (>=0.3.7)\", \"pytest-checkdocs (>=2.4)\", \"pytest-cov\", \"pytest-enabler (>=1.3)\", \"pytest-mypy (>=0.9.1)\", \"pytest-perf\", \"pytest-ruff\", \"pytest-timeout\", \"pytest-xdist\", \"tomli-w (>=1.0.0)\", \"virtualenv (>=13.0.0)\", \"wheel\"]",
      "5029: testing-integration = [\"build[virtualenv]\", \"filelock (>=3.4.0)\", \"jaraco.envs (>=2.2)\", \"jaraco.path (>=3.2.0)\", \"pytest\", \"pytest-enabler\", \"pytest-xdist\", \"tomli\", \"virtualenv (>=13.0.0)\", \"wheel\"]",
      "",
      "[Added Lines]",
      "5151: version = \"70.3.0\"",
      "5154: python-versions = \">=3.8\"",
      "5156:     {file = \"setuptools-70.3.0-py3-none-any.whl\", hash = \"sha256:fe384da74336c398e0d956d1cae0669bc02eed936cdb1d49b57de1990dc11ffc\"},",
      "5157:     {file = \"setuptools-70.3.0.tar.gz\", hash = \"sha256:f171bab1dfbc86b132997f26a119f6056a57950d058587841a0082e8830f9dc5\"},",
      "5161: doc = [\"furo\", \"jaraco.packaging (>=9.3)\", \"jaraco.tidelift (>=1.4)\", \"pygments-github-lexers (==0.0.5)\", \"pyproject-hooks (!=1.1)\", \"rst.linker (>=1.9)\", \"sphinx (>=3.5)\", \"sphinx-favicon\", \"sphinx-inline-tabs\", \"sphinx-lint\", \"sphinx-notfound-page (>=1,<2)\", \"sphinx-reredirects\", \"sphinxcontrib-towncrier\"]",
      "5162: test = [\"build[virtualenv] (>=1.0.3)\", \"filelock (>=3.4.0)\", \"importlib-metadata\", \"ini2toml[lite] (>=0.14)\", \"jaraco.develop (>=7.21)\", \"jaraco.envs (>=2.2)\", \"jaraco.path (>=3.2.0)\", \"jaraco.test\", \"mypy (==1.10.0)\", \"packaging (>=23.2)\", \"pip (>=19.1)\", \"pyproject-hooks (!=1.1)\", \"pytest (>=6,!=8.1.*)\", \"pytest-checkdocs (>=2.4)\", \"pytest-cov\", \"pytest-enabler (>=2.2)\", \"pytest-home (>=0.5)\", \"pytest-mypy\", \"pytest-perf\", \"pytest-ruff (>=0.3.2)\", \"pytest-subprocess\", \"pytest-timeout\", \"pytest-xdist (>=3)\", \"tomli\", \"tomli-w (>=1.0.0)\", \"virtualenv (>=13.0.0)\", \"wheel\"]",
      "",
      "---------------",
      "--- Hunk 7 ---",
      "[Context before]",
      "5056: tabulate = \"*\"",
      "5057: tqdm = \">=2.0\"",
      "5059: [[package]]",
      "5060: name = \"slack-sdk\"",
      "5061: version = \"3.21.3\"",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "5192: [[package]]",
      "5193: name = \"skops\"",
      "5194: version = \"0.9.0\"",
      "5195: description = \"A set of tools to push scikit-learn based models to and pull from Hugging Face Hub\"",
      "5196: optional = false",
      "5197: python-versions = \">=3.8\"",
      "5198: files = [",
      "5199:     {file = \"skops-0.9.0-py3-none-any.whl\", hash = \"sha256:05645199bf6976e1f6dbba4a0704799cd5d2fcef18a98b069b4c84744e1a80a1\"},",
      "5200:     {file = \"skops-0.9.0.tar.gz\", hash = \"sha256:3e39333d65f26d5863ad44db5001b4cfe6a29642274ac37af54fb834813aee3f\"},",
      "5201: ]",
      "5203: [package.dependencies]",
      "5204: huggingface-hub = \">=0.17.0\"",
      "5205: packaging = \">=17.0\"",
      "5206: scikit-learn = \">=0.24\"",
      "5207: tabulate = \">=0.8.8\"",
      "5209: [package.extras]",
      "5210: docs = [\"fairlearn (>=0.7.0)\", \"matplotlib (>=3.3)\", \"numpydoc (>=1.0.0)\", \"pandas (>=1)\", \"scikit-learn-intelex (>=2021.7.1)\", \"sphinx (>=3.2.0)\", \"sphinx-gallery (>=0.7.0)\", \"sphinx-issues (>=1.2.0)\", \"sphinx-prompt (>=1.3.0)\", \"sphinx-rtd-theme (>=1)\"]",
      "5211: rich = [\"rich (>=12)\"]",
      "5212: tests = [\"catboost (>=1.0)\", \"fairlearn (>=0.7.0)\", \"flake8 (>=3.8.2)\", \"flaky (>=3.7.0)\", \"lightgbm (>=3)\", \"matplotlib (>=3.3)\", \"pandas (>=1)\", \"pytest (>=5.0.1)\", \"pytest-cov (>=2.9.0)\", \"quantile-forest (>=1.0.0)\", \"rich (>=12)\", \"types-requests (>=2.28.5)\", \"xgboost (>=1.6)\"]",
      "",
      "---------------",
      "--- Hunk 8 ---",
      "[Context before]",
      "5964: [[package]]",
      "5965: name = \"tokenizers\"",
      "5968: optional = true",
      "5970: files = [",
      "6016: testing = [\"black (==22.3)\", \"datasets\", \"numpy\", \"pytest\", \"requests\"]",
      "6018: [[package]]",
      "",
      "[Removed Lines]",
      "5966: version = \"0.13.3\"",
      "5967: description = \"Fast and Customizable Tokenizers\"",
      "5969: python-versions = \"*\"",
      "5971:     {file = \"tokenizers-0.13.3-cp310-cp310-macosx_10_11_x86_64.whl\", hash = \"sha256:f3835c5be51de8c0a092058a4d4380cb9244fb34681fd0a295fbf0a52a5fdf33\"},",
      "5972:     {file = \"tokenizers-0.13.3-cp310-cp310-macosx_12_0_arm64.whl\", hash = \"sha256:4ef4c3e821730f2692489e926b184321e887f34fb8a6b80b8096b966ba663d07\"},",
      "5973:     {file = \"tokenizers-0.13.3-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:c5fd1a6a25353e9aa762e2aae5a1e63883cad9f4e997c447ec39d071020459bc\"},",
      "5974:     {file = \"tokenizers-0.13.3-cp310-cp310-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:ee0b1b311d65beab83d7a41c56a1e46ab732a9eed4460648e8eb0bd69fc2d059\"},",
      "5975:     {file = \"tokenizers-0.13.3-cp310-cp310-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:5ef4215284df1277dadbcc5e17d4882bda19f770d02348e73523f7e7d8b8d396\"},",
      "5976:     {file = \"tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:a4d53976079cff8a033f778fb9adca2d9d69d009c02fa2d71a878b5f3963ed30\"},",
      "5977:     {file = \"tokenizers-0.13.3-cp310-cp310-win32.whl\", hash = \"sha256:1f0e3b4c2ea2cd13238ce43548959c118069db7579e5d40ec270ad77da5833ce\"},",
      "5978:     {file = \"tokenizers-0.13.3-cp310-cp310-win_amd64.whl\", hash = \"sha256:89649c00d0d7211e8186f7a75dfa1db6996f65edce4b84821817eadcc2d3c79e\"},",
      "5979:     {file = \"tokenizers-0.13.3-cp311-cp311-macosx_10_11_universal2.whl\", hash = \"sha256:56b726e0d2bbc9243872b0144515ba684af5b8d8cd112fb83ee1365e26ec74c8\"},",
      "5980:     {file = \"tokenizers-0.13.3-cp311-cp311-macosx_12_0_arm64.whl\", hash = \"sha256:cc5c022ce692e1f499d745af293ab9ee6f5d92538ed2faf73f9708c89ee59ce6\"},",
      "5981:     {file = \"tokenizers-0.13.3-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:f55c981ac44ba87c93e847c333e58c12abcbb377a0c2f2ef96e1a266e4184ff2\"},",
      "5982:     {file = \"tokenizers-0.13.3-cp311-cp311-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:f247eae99800ef821a91f47c5280e9e9afaeed9980fc444208d5aa6ba69ff148\"},",
      "5983:     {file = \"tokenizers-0.13.3-cp311-cp311-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:4b3e3215d048e94f40f1c95802e45dcc37c5b05eb46280fc2ccc8cd351bff839\"},",
      "5984:     {file = \"tokenizers-0.13.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:9ba2b0bf01777c9b9bc94b53764d6684554ce98551fec496f71bc5be3a03e98b\"},",
      "5985:     {file = \"tokenizers-0.13.3-cp311-cp311-win32.whl\", hash = \"sha256:cc78d77f597d1c458bf0ea7c2a64b6aa06941c7a99cb135b5969b0278824d808\"},",
      "5986:     {file = \"tokenizers-0.13.3-cp311-cp311-win_amd64.whl\", hash = \"sha256:ecf182bf59bd541a8876deccf0360f5ae60496fd50b58510048020751cf1724c\"},",
      "5987:     {file = \"tokenizers-0.13.3-cp37-cp37m-macosx_10_11_x86_64.whl\", hash = \"sha256:0527dc5436a1f6bf2c0327da3145687d3bcfbeab91fed8458920093de3901b44\"},",
      "5988:     {file = \"tokenizers-0.13.3-cp37-cp37m-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:07cbb2c307627dc99b44b22ef05ff4473aa7c7cc1fec8f0a8b37d8a64b1a16d2\"},",
      "5989:     {file = \"tokenizers-0.13.3-cp37-cp37m-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:4560dbdeaae5b7ee0d4e493027e3de6d53c991b5002d7ff95083c99e11dd5ac0\"},",
      "5990:     {file = \"tokenizers-0.13.3-cp37-cp37m-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:64064bd0322405c9374305ab9b4c07152a1474370327499911937fd4a76d004b\"},",
      "5991:     {file = \"tokenizers-0.13.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:b8c6e2ab0f2e3d939ca66aa1d596602105fe33b505cd2854a4c1717f704c51de\"},",
      "5992:     {file = \"tokenizers-0.13.3-cp37-cp37m-win32.whl\", hash = \"sha256:6cc29d410768f960db8677221e497226e545eaaea01aa3613fa0fdf2cc96cff4\"},",
      "5993:     {file = \"tokenizers-0.13.3-cp37-cp37m-win_amd64.whl\", hash = \"sha256:fc2a7fdf864554a0dacf09d32e17c0caa9afe72baf9dd7ddedc61973bae352d8\"},",
      "5994:     {file = \"tokenizers-0.13.3-cp38-cp38-macosx_10_11_x86_64.whl\", hash = \"sha256:8791dedba834c1fc55e5f1521be325ea3dafb381964be20684b92fdac95d79b7\"},",
      "5995:     {file = \"tokenizers-0.13.3-cp38-cp38-macosx_12_0_arm64.whl\", hash = \"sha256:d607a6a13718aeb20507bdf2b96162ead5145bbbfa26788d6b833f98b31b26e1\"},",
      "5996:     {file = \"tokenizers-0.13.3-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:3791338f809cd1bf8e4fee6b540b36822434d0c6c6bc47162448deee3f77d425\"},",
      "5997:     {file = \"tokenizers-0.13.3-cp38-cp38-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:c2f35f30e39e6aab8716f07790f646bdc6e4a853816cc49a95ef2a9016bf9ce6\"},",
      "5998:     {file = \"tokenizers-0.13.3-cp38-cp38-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:310204dfed5aa797128b65d63538a9837cbdd15da2a29a77d67eefa489edda26\"},",
      "5999:     {file = \"tokenizers-0.13.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:a0f9b92ea052305166559f38498b3b0cae159caea712646648aaa272f7160963\"},",
      "6000:     {file = \"tokenizers-0.13.3-cp38-cp38-win32.whl\", hash = \"sha256:9a3fa134896c3c1f0da6e762d15141fbff30d094067c8f1157b9fdca593b5806\"},",
      "6001:     {file = \"tokenizers-0.13.3-cp38-cp38-win_amd64.whl\", hash = \"sha256:8e7b0cdeace87fa9e760e6a605e0ae8fc14b7d72e9fc19c578116f7287bb873d\"},",
      "6002:     {file = \"tokenizers-0.13.3-cp39-cp39-macosx_10_11_x86_64.whl\", hash = \"sha256:00cee1e0859d55507e693a48fa4aef07060c4bb6bd93d80120e18fea9371c66d\"},",
      "6003:     {file = \"tokenizers-0.13.3-cp39-cp39-macosx_12_0_arm64.whl\", hash = \"sha256:a23ff602d0797cea1d0506ce69b27523b07e70f6dda982ab8cf82402de839088\"},",
      "6004:     {file = \"tokenizers-0.13.3-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:70ce07445050b537d2696022dafb115307abdffd2a5c106f029490f84501ef97\"},",
      "6005:     {file = \"tokenizers-0.13.3-cp39-cp39-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:280ffe95f50eaaf655b3a1dc7ff1d9cf4777029dbbc3e63a74e65a056594abc3\"},",
      "6006:     {file = \"tokenizers-0.13.3-cp39-cp39-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:97acfcec592f7e9de8cadcdcda50a7134423ac8455c0166b28c9ff04d227b371\"},",
      "6007:     {file = \"tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:dd7730c98a3010cd4f523465867ff95cd9d6430db46676ce79358f65ae39797b\"},",
      "6008:     {file = \"tokenizers-0.13.3-cp39-cp39-win32.whl\", hash = \"sha256:48625a108029cb1ddf42e17a81b5a3230ba6888a70c9dc14e81bc319e812652d\"},",
      "6009:     {file = \"tokenizers-0.13.3-cp39-cp39-win_amd64.whl\", hash = \"sha256:bc0a6f1ba036e482db6453571c9e3e60ecd5489980ffd95d11dc9f960483d783\"},",
      "6010:     {file = \"tokenizers-0.13.3.tar.gz\", hash = \"sha256:2e546dbb68b623008a5442353137fbb0123d311a6d7ba52f2667c8862a75af2e\"},",
      "6011: ]",
      "6013: [package.extras]",
      "6014: dev = [\"black (==22.3)\", \"datasets\", \"numpy\", \"pytest\", \"requests\"]",
      "6015: docs = [\"setuptools-rust\", \"sphinx\", \"sphinx-rtd-theme\"]",
      "",
      "[Added Lines]",
      "6121: version = \"0.15.2\"",
      "6122: description = \"\"",
      "6124: python-versions = \">=3.7\"",
      "6126:     {file = \"tokenizers-0.15.2-cp310-cp310-macosx_10_12_x86_64.whl\", hash = \"sha256:52f6130c9cbf70544287575a985bf44ae1bda2da7e8c24e97716080593638012\"},",
      "6127:     {file = \"tokenizers-0.15.2-cp310-cp310-macosx_11_0_arm64.whl\", hash = \"sha256:054c1cc9c6d68f7ffa4e810b3d5131e0ba511b6e4be34157aa08ee54c2f8d9ee\"},",
      "6128:     {file = \"tokenizers-0.15.2-cp310-cp310-manylinux_2_12_i686.manylinux2010_i686.whl\", hash = \"sha256:a9b9b070fdad06e347563b88c278995735292ded1132f8657084989a4c84a6d5\"},",
      "6129:     {file = \"tokenizers-0.15.2-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:ea621a7eef4b70e1f7a4e84dd989ae3f0eeb50fc8690254eacc08acb623e82f1\"},",
      "6130:     {file = \"tokenizers-0.15.2-cp310-cp310-manylinux_2_17_armv7l.manylinux2014_armv7l.whl\", hash = \"sha256:cf7fd9a5141634fa3aa8d6b7be362e6ae1b4cda60da81388fa533e0b552c98fd\"},",
      "6131:     {file = \"tokenizers-0.15.2-cp310-cp310-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:44f2a832cd0825295f7179eaf173381dc45230f9227ec4b44378322d900447c9\"},",
      "6132:     {file = \"tokenizers-0.15.2-cp310-cp310-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:8b9ec69247a23747669ec4b0ca10f8e3dfb3545d550258129bd62291aabe8605\"},",
      "6133:     {file = \"tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:40b6a4c78da863ff26dbd5ad9a8ecc33d8a8d97b535172601cf00aee9d7ce9ce\"},",
      "6134:     {file = \"tokenizers-0.15.2-cp310-cp310-musllinux_1_1_aarch64.whl\", hash = \"sha256:5ab2a4d21dcf76af60e05af8063138849eb1d6553a0d059f6534357bce8ba364\"},",
      "6135:     {file = \"tokenizers-0.15.2-cp310-cp310-musllinux_1_1_x86_64.whl\", hash = \"sha256:a47acfac7e511f6bbfcf2d3fb8c26979c780a91e06fb5b9a43831b2c0153d024\"},",
      "6136:     {file = \"tokenizers-0.15.2-cp310-none-win32.whl\", hash = \"sha256:064ff87bb6acdbd693666de9a4b692add41308a2c0ec0770d6385737117215f2\"},",
      "6137:     {file = \"tokenizers-0.15.2-cp310-none-win_amd64.whl\", hash = \"sha256:3b919afe4df7eb6ac7cafd2bd14fb507d3f408db7a68c43117f579c984a73843\"},",
      "6138:     {file = \"tokenizers-0.15.2-cp311-cp311-macosx_10_12_x86_64.whl\", hash = \"sha256:89cd1cb93e4b12ff39bb2d626ad77e35209de9309a71e4d3d4672667b4b256e7\"},",
      "6139:     {file = \"tokenizers-0.15.2-cp311-cp311-macosx_11_0_arm64.whl\", hash = \"sha256:cfed5c64e5be23d7ee0f0e98081a25c2a46b0b77ce99a4f0605b1ec43dd481fa\"},",
      "6140:     {file = \"tokenizers-0.15.2-cp311-cp311-manylinux_2_12_i686.manylinux2010_i686.whl\", hash = \"sha256:a907d76dcfda37023ba203ab4ceeb21bc5683436ebefbd895a0841fd52f6f6f2\"},",
      "6141:     {file = \"tokenizers-0.15.2-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:20ea60479de6fc7b8ae756b4b097572372d7e4032e2521c1bbf3d90c90a99ff0\"},",
      "6142:     {file = \"tokenizers-0.15.2-cp311-cp311-manylinux_2_17_armv7l.manylinux2014_armv7l.whl\", hash = \"sha256:48e2b9335be2bc0171df9281385c2ed06a15f5cf121c44094338306ab7b33f2c\"},",
      "6143:     {file = \"tokenizers-0.15.2-cp311-cp311-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:112a1dd436d2cc06e6ffdc0b06d55ac019a35a63afd26475205cb4b1bf0bfbff\"},",
      "6144:     {file = \"tokenizers-0.15.2-cp311-cp311-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:4620cca5c2817177ee8706f860364cc3a8845bc1e291aaf661fb899e5d1c45b0\"},",
      "6145:     {file = \"tokenizers-0.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:ccd73a82751c523b3fc31ff8194702e4af4db21dc20e55b30ecc2079c5d43cb7\"},",
      "6146:     {file = \"tokenizers-0.15.2-cp311-cp311-musllinux_1_1_aarch64.whl\", hash = \"sha256:107089f135b4ae7817affe6264f8c7a5c5b4fd9a90f9439ed495f54fcea56fb4\"},",
      "6147:     {file = \"tokenizers-0.15.2-cp311-cp311-musllinux_1_1_x86_64.whl\", hash = \"sha256:0ff110ecc57b7aa4a594396525a3451ad70988e517237fe91c540997c4e50e29\"},",
      "6148:     {file = \"tokenizers-0.15.2-cp311-none-win32.whl\", hash = \"sha256:6d76f00f5c32da36c61f41c58346a4fa7f0a61be02f4301fd30ad59834977cc3\"},",
      "6149:     {file = \"tokenizers-0.15.2-cp311-none-win_amd64.whl\", hash = \"sha256:cc90102ed17271cf0a1262babe5939e0134b3890345d11a19c3145184b706055\"},",
      "6150:     {file = \"tokenizers-0.15.2-cp312-cp312-macosx_10_12_x86_64.whl\", hash = \"sha256:f86593c18d2e6248e72fb91c77d413a815153b8ea4e31f7cd443bdf28e467670\"},",
      "6151:     {file = \"tokenizers-0.15.2-cp312-cp312-macosx_11_0_arm64.whl\", hash = \"sha256:0774bccc6608eca23eb9d620196687c8b2360624619623cf4ba9dc9bd53e8b51\"},",
      "6152:     {file = \"tokenizers-0.15.2-cp312-cp312-manylinux_2_12_i686.manylinux2010_i686.whl\", hash = \"sha256:d0222c5b7c9b26c0b4822a82f6a7011de0a9d3060e1da176f66274b70f846b98\"},",
      "6153:     {file = \"tokenizers-0.15.2-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:3835738be1de66624fff2f4f6f6684775da4e9c00bde053be7564cbf3545cc66\"},",
      "6154:     {file = \"tokenizers-0.15.2-cp312-cp312-manylinux_2_17_armv7l.manylinux2014_armv7l.whl\", hash = \"sha256:0143e7d9dcd811855c1ce1ab9bf5d96d29bf5e528fd6c7824d0465741e8c10fd\"},",
      "6155:     {file = \"tokenizers-0.15.2-cp312-cp312-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:db35825f6d54215f6b6009a7ff3eedee0848c99a6271c870d2826fbbedf31a38\"},",
      "6156:     {file = \"tokenizers-0.15.2-cp312-cp312-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:3f5e64b0389a2be47091d8cc53c87859783b837ea1a06edd9d8e04004df55a5c\"},",
      "6157:     {file = \"tokenizers-0.15.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:9e0480c452217edd35eca56fafe2029fb4d368b7c0475f8dfa3c5c9c400a7456\"},",
      "6158:     {file = \"tokenizers-0.15.2-cp312-cp312-musllinux_1_1_aarch64.whl\", hash = \"sha256:a33ab881c8fe70474980577e033d0bc9a27b7ab8272896e500708b212995d834\"},",
      "6159:     {file = \"tokenizers-0.15.2-cp312-cp312-musllinux_1_1_x86_64.whl\", hash = \"sha256:a308a607ca9de2c64c1b9ba79ec9a403969715a1b8ba5f998a676826f1a7039d\"},",
      "6160:     {file = \"tokenizers-0.15.2-cp312-none-win32.whl\", hash = \"sha256:b8fcfa81bcb9447df582c5bc96a031e6df4da2a774b8080d4f02c0c16b42be0b\"},",
      "6161:     {file = \"tokenizers-0.15.2-cp312-none-win_amd64.whl\", hash = \"sha256:38d7ab43c6825abfc0b661d95f39c7f8af2449364f01d331f3b51c94dcff7221\"},",
      "6162:     {file = \"tokenizers-0.15.2-cp313-cp313-macosx_10_12_x86_64.whl\", hash = \"sha256:38bfb0204ff3246ca4d5e726e8cc8403bfc931090151e6eede54d0e0cf162ef0\"},",
      "6163:     {file = \"tokenizers-0.15.2-cp313-cp313-macosx_11_0_arm64.whl\", hash = \"sha256:9c861d35e8286a53e06e9e28d030b5a05bcbf5ac9d7229e561e53c352a85b1fc\"},",
      "6164:     {file = \"tokenizers-0.15.2-cp313-cp313-manylinux_2_12_i686.manylinux2010_i686.whl\", hash = \"sha256:936bf3842db5b2048eaa53dade907b1160f318e7c90c74bfab86f1e47720bdd6\"},",
      "6165:     {file = \"tokenizers-0.15.2-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:620beacc3373277700d0e27718aa8b25f7b383eb8001fba94ee00aeea1459d89\"},",
      "6166:     {file = \"tokenizers-0.15.2-cp313-cp313-manylinux_2_17_armv7l.manylinux2014_armv7l.whl\", hash = \"sha256:2735ecbbf37e52db4ea970e539fd2d450d213517b77745114f92867f3fc246eb\"},",
      "6167:     {file = \"tokenizers-0.15.2-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:473c83c5e2359bb81b0b6fde870b41b2764fcdd36d997485e07e72cc3a62264a\"},",
      "6168:     {file = \"tokenizers-0.15.2-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:968fa1fb3c27398b28a4eca1cbd1e19355c4d3a6007f7398d48826bbe3a0f728\"},",
      "6169:     {file = \"tokenizers-0.15.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:865c60ae6eaebdde7da66191ee9b7db52e542ed8ee9d2c653b6d190a9351b980\"},",
      "6170:     {file = \"tokenizers-0.15.2-cp313-cp313-musllinux_1_1_aarch64.whl\", hash = \"sha256:7c0d8b52664ab2d4a8d6686eb5effc68b78608a9008f086a122a7b2996befbab\"},",
      "6171:     {file = \"tokenizers-0.15.2-cp313-cp313-musllinux_1_1_x86_64.whl\", hash = \"sha256:f33dfbdec3784093a9aebb3680d1f91336c56d86cc70ddf88708251da1fe9064\"},",
      "6172:     {file = \"tokenizers-0.15.2-cp37-cp37m-macosx_10_12_x86_64.whl\", hash = \"sha256:d44ba80988ff9424e33e0a49445072ac7029d8c0e1601ad25a0ca5f41ed0c1d6\"},",
      "6173:     {file = \"tokenizers-0.15.2-cp37-cp37m-macosx_11_0_arm64.whl\", hash = \"sha256:dce74266919b892f82b1b86025a613956ea0ea62a4843d4c4237be2c5498ed3a\"},",
      "6174:     {file = \"tokenizers-0.15.2-cp37-cp37m-manylinux_2_12_i686.manylinux2010_i686.whl\", hash = \"sha256:0ef06b9707baeb98b316577acb04f4852239d856b93e9ec3a299622f6084e4be\"},",
      "6175:     {file = \"tokenizers-0.15.2-cp37-cp37m-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:c73e2e74bbb07910da0d37c326869f34113137b23eadad3fc00856e6b3d9930c\"},",
      "6176:     {file = \"tokenizers-0.15.2-cp37-cp37m-manylinux_2_17_armv7l.manylinux2014_armv7l.whl\", hash = \"sha256:4eeb12daf02a59e29f578a865f55d87cd103ce62bd8a3a5874f8fdeaa82e336b\"},",
      "6177:     {file = \"tokenizers-0.15.2-cp37-cp37m-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:9ba9f6895af58487ca4f54e8a664a322f16c26bbb442effd01087eba391a719e\"},",
      "6178:     {file = \"tokenizers-0.15.2-cp37-cp37m-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:ccec77aa7150e38eec6878a493bf8c263ff1fa8a62404e16c6203c64c1f16a26\"},",
      "6179:     {file = \"tokenizers-0.15.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:f3f40604f5042ff210ba82743dda2b6aa3e55aa12df4e9f2378ee01a17e2855e\"},",
      "6180:     {file = \"tokenizers-0.15.2-cp37-cp37m-musllinux_1_1_aarch64.whl\", hash = \"sha256:5645938a42d78c4885086767c70923abad047163d809c16da75d6b290cb30bbe\"},",
      "6181:     {file = \"tokenizers-0.15.2-cp37-cp37m-musllinux_1_1_x86_64.whl\", hash = \"sha256:05a77cbfebe28a61ab5c3891f9939cc24798b63fa236d84e5f29f3a85a200c00\"},",
      "6182:     {file = \"tokenizers-0.15.2-cp37-none-win32.whl\", hash = \"sha256:361abdc068e8afe9c5b818769a48624687fb6aaed49636ee39bec4e95e1a215b\"},",
      "6183:     {file = \"tokenizers-0.15.2-cp37-none-win_amd64.whl\", hash = \"sha256:7ef789f83eb0f9baeb4d09a86cd639c0a5518528f9992f38b28e819df397eb06\"},",
      "6184:     {file = \"tokenizers-0.15.2-cp38-cp38-macosx_10_12_x86_64.whl\", hash = \"sha256:4fe1f74a902bee74a3b25aff180fbfbf4f8b444ab37c4d496af7afd13a784ed2\"},",
      "6185:     {file = \"tokenizers-0.15.2-cp38-cp38-macosx_11_0_arm64.whl\", hash = \"sha256:4c4b89038a684f40a6b15d6b09f49650ac64d951ad0f2a3ea9169687bbf2a8ba\"},",
      "6186:     {file = \"tokenizers-0.15.2-cp38-cp38-manylinux_2_12_i686.manylinux2010_i686.whl\", hash = \"sha256:d05a1b06f986d41aed5f2de464c003004b2df8aaf66f2b7628254bcbfb72a438\"},",
      "6187:     {file = \"tokenizers-0.15.2-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:508711a108684111ec8af89d3a9e9e08755247eda27d0ba5e3c50e9da1600f6d\"},",
      "6188:     {file = \"tokenizers-0.15.2-cp38-cp38-manylinux_2_17_armv7l.manylinux2014_armv7l.whl\", hash = \"sha256:daa348f02d15160cb35439098ac96e3a53bacf35885072611cd9e5be7d333daa\"},",
      "6189:     {file = \"tokenizers-0.15.2-cp38-cp38-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:494fdbe5932d3416de2a85fc2470b797e6f3226c12845cadf054dd906afd0442\"},",
      "6190:     {file = \"tokenizers-0.15.2-cp38-cp38-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:c2d60f5246f4da9373f75ff18d64c69cbf60c3bca597290cea01059c336d2470\"},",
      "6191:     {file = \"tokenizers-0.15.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:93268e788825f52de4c7bdcb6ebc1fcd4a5442c02e730faa9b6b08f23ead0e24\"},",
      "6192:     {file = \"tokenizers-0.15.2-cp38-cp38-musllinux_1_1_aarch64.whl\", hash = \"sha256:6fc7083ab404019fc9acafe78662c192673c1e696bd598d16dc005bd663a5cf9\"},",
      "6193:     {file = \"tokenizers-0.15.2-cp38-cp38-musllinux_1_1_x86_64.whl\", hash = \"sha256:41e39b41e5531d6b2122a77532dbea60e171ef87a3820b5a3888daa847df4153\"},",
      "6194:     {file = \"tokenizers-0.15.2-cp38-none-win32.whl\", hash = \"sha256:06cd0487b1cbfabefb2cc52fbd6b1f8d4c37799bd6c6e1641281adaa6b2504a7\"},",
      "6195:     {file = \"tokenizers-0.15.2-cp38-none-win_amd64.whl\", hash = \"sha256:5179c271aa5de9c71712e31cb5a79e436ecd0d7532a408fa42a8dbfa4bc23fd9\"},",
      "6196:     {file = \"tokenizers-0.15.2-cp39-cp39-macosx_10_12_x86_64.whl\", hash = \"sha256:82f8652a74cc107052328b87ea8b34291c0f55b96d8fb261b3880216a9f9e48e\"},",
      "6197:     {file = \"tokenizers-0.15.2-cp39-cp39-macosx_11_0_arm64.whl\", hash = \"sha256:02458bee6f5f3139f1ebbb6d042b283af712c0981f5bc50edf771d6b762d5e4f\"},",
      "6198:     {file = \"tokenizers-0.15.2-cp39-cp39-manylinux_2_12_i686.manylinux2010_i686.whl\", hash = \"sha256:c9a09cd26cca2e1c349f91aa665309ddb48d71636370749414fbf67bc83c5343\"},",
      "6199:     {file = \"tokenizers-0.15.2-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:158be8ea8554e5ed69acc1ce3fbb23a06060bd4bbb09029431ad6b9a466a7121\"},",
      "6200:     {file = \"tokenizers-0.15.2-cp39-cp39-manylinux_2_17_armv7l.manylinux2014_armv7l.whl\", hash = \"sha256:1ddba9a2b0c8c81633eca0bb2e1aa5b3a15362b1277f1ae64176d0f6eba78ab1\"},",
      "6201:     {file = \"tokenizers-0.15.2-cp39-cp39-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:3ef5dd1d39797044642dbe53eb2bc56435308432e9c7907728da74c69ee2adca\"},",
      "6202:     {file = \"tokenizers-0.15.2-cp39-cp39-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:454c203164e07a860dbeb3b1f4a733be52b0edbb4dd2e5bd75023ffa8b49403a\"},",
      "6203:     {file = \"tokenizers-0.15.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:0cf6b7f1d4dc59af960e6ffdc4faffe6460bbfa8dce27a58bf75755ffdb2526d\"},",
      "6204:     {file = \"tokenizers-0.15.2-cp39-cp39-musllinux_1_1_aarch64.whl\", hash = \"sha256:2ef09bbc16519f6c25d0c7fc0c6a33a6f62923e263c9d7cca4e58b8c61572afb\"},",
      "6205:     {file = \"tokenizers-0.15.2-cp39-cp39-musllinux_1_1_x86_64.whl\", hash = \"sha256:c9a2ebdd2ad4ec7a68e7615086e633857c85e2f18025bd05d2a4399e6c5f7169\"},",
      "6206:     {file = \"tokenizers-0.15.2-cp39-none-win32.whl\", hash = \"sha256:918fbb0eab96fe08e72a8c2b5461e9cce95585d82a58688e7f01c2bd546c79d0\"},",
      "6207:     {file = \"tokenizers-0.15.2-cp39-none-win_amd64.whl\", hash = \"sha256:524e60da0135e106b254bd71f0659be9f89d83f006ea9093ce4d1fab498c6d0d\"},",
      "6208:     {file = \"tokenizers-0.15.2-pp310-pypy310_pp73-macosx_10_12_x86_64.whl\", hash = \"sha256:6a9b648a58281c4672212fab04e60648fde574877d0139cd4b4f93fe28ca8944\"},",
      "6209:     {file = \"tokenizers-0.15.2-pp310-pypy310_pp73-macosx_11_0_arm64.whl\", hash = \"sha256:7c7d18b733be6bbca8a55084027f7be428c947ddf871c500ee603e375013ffba\"},",
      "6210:     {file = \"tokenizers-0.15.2-pp310-pypy310_pp73-manylinux_2_12_i686.manylinux2010_i686.whl\", hash = \"sha256:13ca3611de8d9ddfbc4dc39ef54ab1d2d4aaa114ac8727dfdc6a6ec4be017378\"},",
      "6211:     {file = \"tokenizers-0.15.2-pp310-pypy310_pp73-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:237d1bf3361cf2e6463e6c140628e6406766e8b27274f5fcc62c747ae3c6f094\"},",
      "6212:     {file = \"tokenizers-0.15.2-pp310-pypy310_pp73-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:67a0fe1e49e60c664915e9fb6b0cb19bac082ab1f309188230e4b2920230edb3\"},",
      "6213:     {file = \"tokenizers-0.15.2-pp310-pypy310_pp73-musllinux_1_1_aarch64.whl\", hash = \"sha256:4e022fe65e99230b8fd89ebdfea138c24421f91c1a4f4781a8f5016fd5cdfb4d\"},",
      "6214:     {file = \"tokenizers-0.15.2-pp310-pypy310_pp73-musllinux_1_1_x86_64.whl\", hash = \"sha256:d857be2df69763362ac699f8b251a8cd3fac9d21893de129bc788f8baaef2693\"},",
      "6215:     {file = \"tokenizers-0.15.2-pp37-pypy37_pp73-macosx_10_12_x86_64.whl\", hash = \"sha256:708bb3e4283177236309e698da5fcd0879ce8fd37457d7c266d16b550bcbbd18\"},",
      "6216:     {file = \"tokenizers-0.15.2-pp37-pypy37_pp73-manylinux_2_12_i686.manylinux2010_i686.whl\", hash = \"sha256:64c35e09e9899b72a76e762f9854e8750213f67567787d45f37ce06daf57ca78\"},",
      "6217:     {file = \"tokenizers-0.15.2-pp37-pypy37_pp73-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:c1257f4394be0d3b00de8c9e840ca5601d0a4a8438361ce9c2b05c7d25f6057b\"},",
      "6218:     {file = \"tokenizers-0.15.2-pp37-pypy37_pp73-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:02272fe48280e0293a04245ca5d919b2c94a48b408b55e858feae9618138aeda\"},",
      "6219:     {file = \"tokenizers-0.15.2-pp37-pypy37_pp73-musllinux_1_1_aarch64.whl\", hash = \"sha256:dc3ad9ebc76eabe8b1d7c04d38be884b8f9d60c0cdc09b0aa4e3bcf746de0388\"},",
      "6220:     {file = \"tokenizers-0.15.2-pp37-pypy37_pp73-musllinux_1_1_x86_64.whl\", hash = \"sha256:32e16bdeffa7c4f46bf2152172ca511808b952701d13e7c18833c0b73cb5c23f\"},",
      "6221:     {file = \"tokenizers-0.15.2-pp38-pypy38_pp73-macosx_10_12_x86_64.whl\", hash = \"sha256:fb16ba563d59003028b678d2361a27f7e4ae0ab29c7a80690efa20d829c81fdb\"},",
      "6222:     {file = \"tokenizers-0.15.2-pp38-pypy38_pp73-macosx_11_0_arm64.whl\", hash = \"sha256:2277c36d2d6cdb7876c274547921a42425b6810d38354327dd65a8009acf870c\"},",
      "6223:     {file = \"tokenizers-0.15.2-pp38-pypy38_pp73-manylinux_2_12_i686.manylinux2010_i686.whl\", hash = \"sha256:1cf75d32e8d250781940d07f7eece253f2fe9ecdb1dc7ba6e3833fa17b82fcbc\"},",
      "6224:     {file = \"tokenizers-0.15.2-pp38-pypy38_pp73-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:f1b3b31884dc8e9b21508bb76da80ebf7308fdb947a17affce815665d5c4d028\"},",
      "6225:     {file = \"tokenizers-0.15.2-pp38-pypy38_pp73-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:b10122d8d8e30afb43bb1fe21a3619f62c3e2574bff2699cf8af8b0b6c5dc4a3\"},",
      "6226:     {file = \"tokenizers-0.15.2-pp38-pypy38_pp73-musllinux_1_1_aarch64.whl\", hash = \"sha256:d88b96ff0fe8e91f6ef01ba50b0d71db5017fa4e3b1d99681cec89a85faf7bf7\"},",
      "6227:     {file = \"tokenizers-0.15.2-pp38-pypy38_pp73-musllinux_1_1_x86_64.whl\", hash = \"sha256:37aaec5a52e959892870a7c47cef80c53797c0db9149d458460f4f31e2fb250e\"},",
      "6228:     {file = \"tokenizers-0.15.2-pp39-pypy39_pp73-macosx_10_12_x86_64.whl\", hash = \"sha256:e2ea752f2b0fe96eb6e2f3adbbf4d72aaa1272079b0dfa1145507bd6a5d537e6\"},",
      "6229:     {file = \"tokenizers-0.15.2-pp39-pypy39_pp73-macosx_11_0_arm64.whl\", hash = \"sha256:4b19a808d8799fda23504a5cd31d2f58e6f52f140380082b352f877017d6342b\"},",
      "6230:     {file = \"tokenizers-0.15.2-pp39-pypy39_pp73-manylinux_2_12_i686.manylinux2010_i686.whl\", hash = \"sha256:64c86e5e068ac8b19204419ed8ca90f9d25db20578f5881e337d203b314f4104\"},",
      "6231:     {file = \"tokenizers-0.15.2-pp39-pypy39_pp73-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:de19c4dc503c612847edf833c82e9f73cd79926a384af9d801dcf93f110cea4e\"},",
      "6232:     {file = \"tokenizers-0.15.2-pp39-pypy39_pp73-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:ea09acd2fe3324174063d61ad620dec3bcf042b495515f27f638270a7d466e8b\"},",
      "6233:     {file = \"tokenizers-0.15.2-pp39-pypy39_pp73-musllinux_1_1_aarch64.whl\", hash = \"sha256:cf27fd43472e07b57cf420eee1e814549203d56de00b5af8659cb99885472f1f\"},",
      "6234:     {file = \"tokenizers-0.15.2-pp39-pypy39_pp73-musllinux_1_1_x86_64.whl\", hash = \"sha256:7ca22bd897537a0080521445d91a58886c8c04084a6a19e6c78c586e0cfa92a5\"},",
      "6235:     {file = \"tokenizers-0.15.2.tar.gz\", hash = \"sha256:e6e9c6e019dd5484be5beafc775ae6c925f4c69a3487040ed09b45e13df2cb91\"},",
      "6236: ]",
      "6238: [package.dependencies]",
      "6239: huggingface_hub = \">=0.16.4,<1.0\"",
      "6241: [package.extras]",
      "6242: dev = [\"tokenizers[testing]\"]",
      "6243: docs = [\"setuptools_rust\", \"sphinx\", \"sphinx_rtd_theme\"]",
      "",
      "---------------",
      "--- Hunk 9 ---",
      "[Context before]",
      "6104: [[package]]",
      "6105: name = \"transformers\"",
      "6107: description = \"State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\"",
      "6108: optional = true",
      "6110: files = [",
      "6113: ]",
      "6115: [package.dependencies]",
      "6116: filelock = \"*\"",
      "6118: numpy = \">=1.17\"",
      "6119: packaging = \">=20.0\"",
      "6120: pyyaml = \">=5.1\"",
      "6121: regex = \"!=2019.12.17\"",
      "6122: requests = \"*\"",
      "6124: tqdm = \">=4.27\"",
      "6126: [package.extras]",
      "6129: audio = [\"kenlm\", \"librosa\", \"phonemizer\", \"pyctcdecode (>=0.4.0)\"]",
      "6130: codecarbon = [\"codecarbon (==1.2.0)\"]",
      "6137: docs-specific = [\"hf-doc-builder\"]",
      "6140: flax-speech = [\"kenlm\", \"librosa\", \"phonemizer\", \"pyctcdecode (>=0.4.0)\"]",
      "6141: ftfy = [\"ftfy\"]",
      "6144: modelcreation = [\"cookiecutter (==1.7.3)\"]",
      "6146: onnx = [\"onnxconverter-common\", \"onnxruntime (>=1.4.0)\", \"onnxruntime-tools (>=1.4.2)\", \"tf2onnx\"]",
      "6147: onnxruntime = [\"onnxruntime (>=1.4.0)\", \"onnxruntime-tools (>=1.4.2)\"]",
      "6148: optuna = [\"optuna\"]",
      "6151: retrieval = [\"datasets (!=2.5.0)\", \"faiss-cpu\"]",
      "6152: sagemaker = [\"sagemaker (>=2.31.0)\"]",
      "6155: sigopt = [\"sigopt\"]",
      "6156: sklearn = [\"scikit-learn\"]",
      "6157: speech = [\"kenlm\", \"librosa\", \"phonemizer\", \"pyctcdecode (>=0.4.0)\", \"torchaudio\"]",
      "6161: tf-speech = [\"kenlm\", \"librosa\", \"phonemizer\", \"pyctcdecode (>=0.4.0)\"]",
      "6162: timm = [\"timm\"]",
      "6165: torch-speech = [\"kenlm\", \"librosa\", \"phonemizer\", \"pyctcdecode (>=0.4.0)\", \"torchaudio\"]",
      "6170: [[package]]",
      "6171: name = \"twilio\"",
      "",
      "[Removed Lines]",
      "6106: version = \"4.26.0\"",
      "6109: python-versions = \">=3.7.0\"",
      "6111:     {file = \"transformers-4.26.0-py3-none-any.whl\", hash = \"sha256:6a902eee6098d9a737faadf185b8df5a169acc695ebbde5a81b90528f43e665f\"},",
      "6112:     {file = \"transformers-4.26.0.tar.gz\", hash = \"sha256:d7859bd83829a3682ca632197ee5c72556e1063d199ab84eec35c4f23b3d73a3\"},",
      "6117: huggingface-hub = \">=0.11.0,<1.0\"",
      "6123: tokenizers = \">=0.11.1,<0.11.3 || >0.11.3,<0.14\"",
      "6127: accelerate = [\"accelerate (>=0.10.0)\"]",
      "6128: all = [\"Pillow\", \"accelerate (>=0.10.0)\", \"codecarbon (==1.2.0)\", \"decord (==0.6.0)\", \"flax (>=0.4.1)\", \"jax (>=0.2.8,!=0.3.2,<=0.3.6)\", \"jaxlib (>=0.1.65,<=0.3.6)\", \"kenlm\", \"keras-nlp (>=0.3.1)\", \"librosa\", \"onnxconverter-common\", \"optax (>=0.0.8)\", \"optuna\", \"phonemizer\", \"protobuf (<=3.20.2)\", \"pyctcdecode (>=0.4.0)\", \"ray[tune]\", \"sentencepiece (>=0.1.91,!=0.1.92)\", \"sigopt\", \"tensorflow (>=2.4,<2.12)\", \"tensorflow-text\", \"tf2onnx\", \"timm\", \"tokenizers (>=0.11.1,!=0.11.3,<0.14)\", \"torch (>=1.7,!=1.12.0)\", \"torchaudio\"]",
      "6131: deepspeed = [\"accelerate (>=0.10.0)\", \"deepspeed (>=0.6.5)\"]",
      "6132: deepspeed-testing = [\"GitPython (<3.1.19)\", \"accelerate (>=0.10.0)\", \"beautifulsoup4\", \"black (==22.3)\", \"cookiecutter (==1.7.3)\", \"datasets (!=2.5.0)\", \"deepspeed (>=0.6.5)\", \"dill (<0.3.5)\", \"evaluate (>=0.2.0)\", \"faiss-cpu\", \"hf-doc-builder (>=0.3.0)\", \"nltk\", \"optuna\", \"parameterized\", \"protobuf (<=3.20.2)\", \"psutil\", \"pytest\", \"pytest-timeout\", \"pytest-xdist\", \"rjieba\", \"rouge-score (!=0.0.7,!=0.0.8,!=0.1,!=0.1.1)\", \"sacrebleu (>=1.4.12,<2.0.0)\", \"sacremoses\", \"safetensors (>=0.2.1)\", \"sentencepiece (>=0.1.91,!=0.1.92)\", \"timeout-decorator\"]",
      "6133: dev = [\"GitPython (<3.1.19)\", \"Pillow\", \"accelerate (>=0.10.0)\", \"beautifulsoup4\", \"black (==22.3)\", \"codecarbon (==1.2.0)\", \"cookiecutter (==1.7.3)\", \"datasets (!=2.5.0)\", \"decord (==0.6.0)\", \"dill (<0.3.5)\", \"evaluate (>=0.2.0)\", \"faiss-cpu\", \"flake8 (>=3.8.3)\", \"flax (>=0.4.1)\", \"fugashi (>=1.0)\", \"hf-doc-builder\", \"hf-doc-builder (>=0.3.0)\", \"ipadic (>=1.0.0,<2.0)\", \"isort (>=5.5.4)\", \"jax (>=0.2.8,!=0.3.2,<=0.3.6)\", \"jaxlib (>=0.1.65,<=0.3.6)\", \"kenlm\", \"keras-nlp (>=0.3.1)\", \"librosa\", \"nltk\", \"onnxconverter-common\", \"optax (>=0.0.8)\", \"optuna\", \"parameterized\", \"phonemizer\", \"protobuf (<=3.20.2)\", \"psutil\", \"pyctcdecode (>=0.4.0)\", \"pytest\", \"pytest-timeout\", \"pytest-xdist\", \"ray[tune]\", \"rhoknp (>=1.1.0)\", \"rjieba\", \"rouge-score (!=0.0.7,!=0.0.8,!=0.1,!=0.1.1)\", \"sacrebleu (>=1.4.12,<2.0.0)\", \"sacremoses\", \"safetensors (>=0.2.1)\", \"scikit-learn\", \"sentencepiece (>=0.1.91,!=0.1.92)\", \"sigopt\", \"sudachidict-core (>=20220729)\", \"sudachipy (>=0.6.6)\", \"tensorflow (>=2.4,<2.12)\", \"tensorflow-text\", \"tf2onnx\", \"timeout-decorator\", \"timm\", \"tokenizers (>=0.11.1,!=0.11.3,<0.14)\", \"torch (>=1.7,!=1.12.0)\", \"torchaudio\", \"unidic (>=1.0.2)\", \"unidic-lite (>=1.0.7)\"]",
      "6134: dev-tensorflow = [\"GitPython (<3.1.19)\", \"Pillow\", \"beautifulsoup4\", \"black (==22.3)\", \"cookiecutter (==1.7.3)\", \"datasets (!=2.5.0)\", \"dill (<0.3.5)\", \"evaluate (>=0.2.0)\", \"faiss-cpu\", \"flake8 (>=3.8.3)\", \"hf-doc-builder\", \"hf-doc-builder (>=0.3.0)\", \"isort (>=5.5.4)\", \"kenlm\", \"keras-nlp (>=0.3.1)\", \"librosa\", \"nltk\", \"onnxconverter-common\", \"onnxruntime (>=1.4.0)\", \"onnxruntime-tools (>=1.4.2)\", \"parameterized\", \"phonemizer\", \"protobuf (<=3.20.2)\", \"psutil\", \"pyctcdecode (>=0.4.0)\", \"pytest\", \"pytest-timeout\", \"pytest-xdist\", \"rjieba\", \"rouge-score (!=0.0.7,!=0.0.8,!=0.1,!=0.1.1)\", \"sacrebleu (>=1.4.12,<2.0.0)\", \"sacremoses\", \"safetensors (>=0.2.1)\", \"scikit-learn\", \"sentencepiece (>=0.1.91,!=0.1.92)\", \"tensorflow (>=2.4,<2.12)\", \"tensorflow-text\", \"tf2onnx\", \"timeout-decorator\", \"tokenizers (>=0.11.1,!=0.11.3,<0.14)\"]",
      "6135: dev-torch = [\"GitPython (<3.1.19)\", \"Pillow\", \"beautifulsoup4\", \"black (==22.3)\", \"codecarbon (==1.2.0)\", \"cookiecutter (==1.7.3)\", \"datasets (!=2.5.0)\", \"dill (<0.3.5)\", \"evaluate (>=0.2.0)\", \"faiss-cpu\", \"flake8 (>=3.8.3)\", \"fugashi (>=1.0)\", \"hf-doc-builder\", \"hf-doc-builder (>=0.3.0)\", \"ipadic (>=1.0.0,<2.0)\", \"isort (>=5.5.4)\", \"kenlm\", \"librosa\", \"nltk\", \"onnxruntime (>=1.4.0)\", \"onnxruntime-tools (>=1.4.2)\", \"optuna\", \"parameterized\", \"phonemizer\", \"protobuf (<=3.20.2)\", \"psutil\", \"pyctcdecode (>=0.4.0)\", \"pytest\", \"pytest-timeout\", \"pytest-xdist\", \"ray[tune]\", \"rhoknp (>=1.1.0)\", \"rjieba\", \"rouge-score (!=0.0.7,!=0.0.8,!=0.1,!=0.1.1)\", \"sacrebleu (>=1.4.12,<2.0.0)\", \"sacremoses\", \"safetensors (>=0.2.1)\", \"scikit-learn\", \"sentencepiece (>=0.1.91,!=0.1.92)\", \"sigopt\", \"sudachidict-core (>=20220729)\", \"sudachipy (>=0.6.6)\", \"timeout-decorator\", \"timm\", \"tokenizers (>=0.11.1,!=0.11.3,<0.14)\", \"torch (>=1.7,!=1.12.0)\", \"torchaudio\", \"unidic (>=1.0.2)\", \"unidic-lite (>=1.0.7)\"]",
      "6136: docs = [\"Pillow\", \"accelerate (>=0.10.0)\", \"codecarbon (==1.2.0)\", \"decord (==0.6.0)\", \"flax (>=0.4.1)\", \"hf-doc-builder\", \"jax (>=0.2.8,!=0.3.2,<=0.3.6)\", \"jaxlib (>=0.1.65,<=0.3.6)\", \"kenlm\", \"keras-nlp (>=0.3.1)\", \"librosa\", \"onnxconverter-common\", \"optax (>=0.0.8)\", \"optuna\", \"phonemizer\", \"protobuf (<=3.20.2)\", \"pyctcdecode (>=0.4.0)\", \"ray[tune]\", \"sentencepiece (>=0.1.91,!=0.1.92)\", \"sigopt\", \"tensorflow (>=2.4,<2.12)\", \"tensorflow-text\", \"tf2onnx\", \"timm\", \"tokenizers (>=0.11.1,!=0.11.3,<0.14)\", \"torch (>=1.7,!=1.12.0)\", \"torchaudio\"]",
      "6138: fairscale = [\"fairscale (>0.3)\"]",
      "6139: flax = [\"flax (>=0.4.1)\", \"jax (>=0.2.8,!=0.3.2,<=0.3.6)\", \"jaxlib (>=0.1.65,<=0.3.6)\", \"optax (>=0.0.8)\"]",
      "6142: integrations = [\"optuna\", \"ray[tune]\", \"sigopt\"]",
      "6143: ja = [\"fugashi (>=1.0)\", \"ipadic (>=1.0.0,<2.0)\", \"rhoknp (>=1.1.0)\", \"sudachidict-core (>=20220729)\", \"sudachipy (>=0.6.6)\", \"unidic (>=1.0.2)\", \"unidic-lite (>=1.0.7)\"]",
      "6145: natten = [\"natten (>=0.14.4)\"]",
      "6149: quality = [\"GitPython (<3.1.19)\", \"black (==22.3)\", \"datasets (!=2.5.0)\", \"flake8 (>=3.8.3)\", \"hf-doc-builder (>=0.3.0)\", \"isort (>=5.5.4)\"]",
      "6150: ray = [\"ray[tune]\"]",
      "6153: sentencepiece = [\"protobuf (<=3.20.2)\", \"sentencepiece (>=0.1.91,!=0.1.92)\"]",
      "6154: serving = [\"fastapi\", \"pydantic\", \"starlette\", \"uvicorn\"]",
      "6158: testing = [\"GitPython (<3.1.19)\", \"beautifulsoup4\", \"black (==22.3)\", \"cookiecutter (==1.7.3)\", \"datasets (!=2.5.0)\", \"dill (<0.3.5)\", \"evaluate (>=0.2.0)\", \"faiss-cpu\", \"hf-doc-builder (>=0.3.0)\", \"nltk\", \"parameterized\", \"protobuf (<=3.20.2)\", \"psutil\", \"pytest\", \"pytest-timeout\", \"pytest-xdist\", \"rjieba\", \"rouge-score (!=0.0.7,!=0.0.8,!=0.1,!=0.1.1)\", \"sacrebleu (>=1.4.12,<2.0.0)\", \"sacremoses\", \"safetensors (>=0.2.1)\", \"timeout-decorator\"]",
      "6159: tf = [\"keras-nlp (>=0.3.1)\", \"onnxconverter-common\", \"tensorflow (>=2.4,<2.12)\", \"tensorflow-text\", \"tf2onnx\"]",
      "6160: tf-cpu = [\"keras-nlp (>=0.3.1)\", \"onnxconverter-common\", \"tensorflow-cpu (>=2.4,<2.12)\", \"tensorflow-text\", \"tf2onnx\"]",
      "6163: tokenizers = [\"tokenizers (>=0.11.1,!=0.11.3,<0.14)\"]",
      "6164: torch = [\"torch (>=1.7,!=1.12.0)\"]",
      "6166: torchhub = [\"filelock\", \"huggingface-hub (>=0.11.0,<1.0)\", \"importlib-metadata\", \"numpy (>=1.17)\", \"packaging (>=20.0)\", \"protobuf (<=3.20.2)\", \"regex (!=2019.12.17)\", \"requests\", \"sentencepiece (>=0.1.91,!=0.1.92)\", \"tokenizers (>=0.11.1,!=0.11.3,<0.14)\", \"torch (>=1.7,!=1.12.0)\", \"tqdm (>=4.27)\"]",
      "6167: video = [\"decord (==0.6.0)\"]",
      "6168: vision = [\"Pillow\"]",
      "",
      "[Added Lines]",
      "6334: version = \"4.36.2\"",
      "6337: python-versions = \">=3.8.0\"",
      "6339:     {file = \"transformers-4.36.2-py3-none-any.whl\", hash = \"sha256:462066c4f74ee52516f12890dcc9ec71d1a5e97998db621668455117a54330f6\"},",
      "6340:     {file = \"transformers-4.36.2.tar.gz\", hash = \"sha256:d8068e897e47793281501e547d2bbdfc5b8556409c2cb6c3d9e2ca77d4c0b4ec\"},",
      "6345: huggingface-hub = \">=0.19.3,<1.0\"",
      "6351: safetensors = \">=0.3.1\"",
      "6352: tokenizers = \">=0.14,<0.19\"",
      "6356: accelerate = [\"accelerate (>=0.21.0)\"]",
      "6357: agents = [\"Pillow (>=10.0.1,<=15.0)\", \"accelerate (>=0.21.0)\", \"datasets (!=2.5.0)\", \"diffusers\", \"opencv-python\", \"sentencepiece (>=0.1.91,!=0.1.92)\", \"torch (>=1.10,!=1.12.0)\"]",
      "6358: all = [\"Pillow (>=10.0.1,<=15.0)\", \"accelerate (>=0.21.0)\", \"av (==9.2.0)\", \"codecarbon (==1.2.0)\", \"decord (==0.6.0)\", \"flax (>=0.4.1,<=0.7.0)\", \"jax (>=0.4.1,<=0.4.13)\", \"jaxlib (>=0.4.1,<=0.4.13)\", \"kenlm\", \"keras-nlp (>=0.3.1)\", \"librosa\", \"onnxconverter-common\", \"optax (>=0.0.8,<=0.1.4)\", \"optuna\", \"phonemizer\", \"protobuf\", \"pyctcdecode (>=0.4.0)\", \"ray[tune] (>=2.7.0)\", \"sentencepiece (>=0.1.91,!=0.1.92)\", \"sigopt\", \"tensorflow (>=2.6,<2.16)\", \"tensorflow-text (<2.16)\", \"tf2onnx\", \"timm\", \"tokenizers (>=0.14,<0.19)\", \"torch (>=1.10,!=1.12.0)\", \"torchaudio\", \"torchvision\"]",
      "6361: deepspeed = [\"accelerate (>=0.21.0)\", \"deepspeed (>=0.9.3)\"]",
      "6362: deepspeed-testing = [\"GitPython (<3.1.19)\", \"accelerate (>=0.21.0)\", \"beautifulsoup4\", \"cookiecutter (==1.7.3)\", \"datasets (!=2.5.0)\", \"deepspeed (>=0.9.3)\", \"dill (<0.3.5)\", \"evaluate (>=0.2.0)\", \"faiss-cpu\", \"hf-doc-builder (>=0.3.0)\", \"nltk\", \"optuna\", \"parameterized\", \"protobuf\", \"psutil\", \"pydantic (<2)\", \"pytest (>=7.2.0)\", \"pytest-timeout\", \"pytest-xdist\", \"rjieba\", \"rouge-score (!=0.0.7,!=0.0.8,!=0.1,!=0.1.1)\", \"ruff (==0.1.5)\", \"sacrebleu (>=1.4.12,<2.0.0)\", \"sacremoses\", \"sentencepiece (>=0.1.91,!=0.1.92)\", \"tensorboard\", \"timeout-decorator\"]",
      "6363: dev = [\"GitPython (<3.1.19)\", \"Pillow (>=10.0.1,<=15.0)\", \"accelerate (>=0.21.0)\", \"av (==9.2.0)\", \"beautifulsoup4\", \"codecarbon (==1.2.0)\", \"cookiecutter (==1.7.3)\", \"datasets (!=2.5.0)\", \"decord (==0.6.0)\", \"dill (<0.3.5)\", \"evaluate (>=0.2.0)\", \"faiss-cpu\", \"flax (>=0.4.1,<=0.7.0)\", \"fugashi (>=1.0)\", \"hf-doc-builder\", \"hf-doc-builder (>=0.3.0)\", \"ipadic (>=1.0.0,<2.0)\", \"isort (>=5.5.4)\", \"jax (>=0.4.1,<=0.4.13)\", \"jaxlib (>=0.4.1,<=0.4.13)\", \"kenlm\", \"keras-nlp (>=0.3.1)\", \"librosa\", \"nltk\", \"onnxconverter-common\", \"optax (>=0.0.8,<=0.1.4)\", \"optuna\", \"parameterized\", \"phonemizer\", \"protobuf\", \"psutil\", \"pyctcdecode (>=0.4.0)\", \"pydantic (<2)\", \"pytest (>=7.2.0)\", \"pytest-timeout\", \"pytest-xdist\", \"ray[tune] (>=2.7.0)\", \"rhoknp (>=1.1.0,<1.3.1)\", \"rjieba\", \"rouge-score (!=0.0.7,!=0.0.8,!=0.1,!=0.1.1)\", \"ruff (==0.1.5)\", \"sacrebleu (>=1.4.12,<2.0.0)\", \"sacremoses\", \"scikit-learn\", \"sentencepiece (>=0.1.91,!=0.1.92)\", \"sigopt\", \"sudachidict-core (>=20220729)\", \"sudachipy (>=0.6.6)\", \"tensorboard\", \"tensorflow (>=2.6,<2.16)\", \"tensorflow-text (<2.16)\", \"tf2onnx\", \"timeout-decorator\", \"timm\", \"tokenizers (>=0.14,<0.19)\", \"torch (>=1.10,!=1.12.0)\", \"torchaudio\", \"torchvision\", \"unidic (>=1.0.2)\", \"unidic-lite (>=1.0.7)\", \"urllib3 (<2.0.0)\"]",
      "6364: dev-tensorflow = [\"GitPython (<3.1.19)\", \"Pillow (>=10.0.1,<=15.0)\", \"beautifulsoup4\", \"cookiecutter (==1.7.3)\", \"datasets (!=2.5.0)\", \"dill (<0.3.5)\", \"evaluate (>=0.2.0)\", \"faiss-cpu\", \"hf-doc-builder\", \"hf-doc-builder (>=0.3.0)\", \"isort (>=5.5.4)\", \"kenlm\", \"keras-nlp (>=0.3.1)\", \"librosa\", \"nltk\", \"onnxconverter-common\", \"onnxruntime (>=1.4.0)\", \"onnxruntime-tools (>=1.4.2)\", \"parameterized\", \"phonemizer\", \"protobuf\", \"psutil\", \"pyctcdecode (>=0.4.0)\", \"pydantic (<2)\", \"pytest (>=7.2.0)\", \"pytest-timeout\", \"pytest-xdist\", \"rjieba\", \"rouge-score (!=0.0.7,!=0.0.8,!=0.1,!=0.1.1)\", \"ruff (==0.1.5)\", \"sacrebleu (>=1.4.12,<2.0.0)\", \"sacremoses\", \"scikit-learn\", \"sentencepiece (>=0.1.91,!=0.1.92)\", \"tensorboard\", \"tensorflow (>=2.6,<2.16)\", \"tensorflow-text (<2.16)\", \"tf2onnx\", \"timeout-decorator\", \"tokenizers (>=0.14,<0.19)\", \"urllib3 (<2.0.0)\"]",
      "6365: dev-torch = [\"GitPython (<3.1.19)\", \"Pillow (>=10.0.1,<=15.0)\", \"accelerate (>=0.21.0)\", \"beautifulsoup4\", \"codecarbon (==1.2.0)\", \"cookiecutter (==1.7.3)\", \"datasets (!=2.5.0)\", \"dill (<0.3.5)\", \"evaluate (>=0.2.0)\", \"faiss-cpu\", \"fugashi (>=1.0)\", \"hf-doc-builder\", \"hf-doc-builder (>=0.3.0)\", \"ipadic (>=1.0.0,<2.0)\", \"isort (>=5.5.4)\", \"kenlm\", \"librosa\", \"nltk\", \"onnxruntime (>=1.4.0)\", \"onnxruntime-tools (>=1.4.2)\", \"optuna\", \"parameterized\", \"phonemizer\", \"protobuf\", \"psutil\", \"pyctcdecode (>=0.4.0)\", \"pydantic (<2)\", \"pytest (>=7.2.0)\", \"pytest-timeout\", \"pytest-xdist\", \"ray[tune] (>=2.7.0)\", \"rhoknp (>=1.1.0,<1.3.1)\", \"rjieba\", \"rouge-score (!=0.0.7,!=0.0.8,!=0.1,!=0.1.1)\", \"ruff (==0.1.5)\", \"sacrebleu (>=1.4.12,<2.0.0)\", \"sacremoses\", \"scikit-learn\", \"sentencepiece (>=0.1.91,!=0.1.92)\", \"sigopt\", \"sudachidict-core (>=20220729)\", \"sudachipy (>=0.6.6)\", \"tensorboard\", \"timeout-decorator\", \"timm\", \"tokenizers (>=0.14,<0.19)\", \"torch (>=1.10,!=1.12.0)\", \"torchaudio\", \"torchvision\", \"unidic (>=1.0.2)\", \"unidic-lite (>=1.0.7)\", \"urllib3 (<2.0.0)\"]",
      "6366: docs = [\"Pillow (>=10.0.1,<=15.0)\", \"accelerate (>=0.21.0)\", \"av (==9.2.0)\", \"codecarbon (==1.2.0)\", \"decord (==0.6.0)\", \"flax (>=0.4.1,<=0.7.0)\", \"hf-doc-builder\", \"jax (>=0.4.1,<=0.4.13)\", \"jaxlib (>=0.4.1,<=0.4.13)\", \"kenlm\", \"keras-nlp (>=0.3.1)\", \"librosa\", \"onnxconverter-common\", \"optax (>=0.0.8,<=0.1.4)\", \"optuna\", \"phonemizer\", \"protobuf\", \"pyctcdecode (>=0.4.0)\", \"ray[tune] (>=2.7.0)\", \"sentencepiece (>=0.1.91,!=0.1.92)\", \"sigopt\", \"tensorflow (>=2.6,<2.16)\", \"tensorflow-text (<2.16)\", \"tf2onnx\", \"timm\", \"tokenizers (>=0.14,<0.19)\", \"torch (>=1.10,!=1.12.0)\", \"torchaudio\", \"torchvision\"]",
      "6368: flax = [\"flax (>=0.4.1,<=0.7.0)\", \"jax (>=0.4.1,<=0.4.13)\", \"jaxlib (>=0.4.1,<=0.4.13)\", \"optax (>=0.0.8,<=0.1.4)\"]",
      "6371: integrations = [\"optuna\", \"ray[tune] (>=2.7.0)\", \"sigopt\"]",
      "6372: ja = [\"fugashi (>=1.0)\", \"ipadic (>=1.0.0,<2.0)\", \"rhoknp (>=1.1.0,<1.3.1)\", \"sudachidict-core (>=20220729)\", \"sudachipy (>=0.6.6)\", \"unidic (>=1.0.2)\", \"unidic-lite (>=1.0.7)\"]",
      "6374: natten = [\"natten (>=0.14.6)\"]",
      "6378: quality = [\"GitPython (<3.1.19)\", \"datasets (!=2.5.0)\", \"hf-doc-builder (>=0.3.0)\", \"isort (>=5.5.4)\", \"ruff (==0.1.5)\", \"urllib3 (<2.0.0)\"]",
      "6379: ray = [\"ray[tune] (>=2.7.0)\"]",
      "6382: sentencepiece = [\"protobuf\", \"sentencepiece (>=0.1.91,!=0.1.92)\"]",
      "6383: serving = [\"fastapi\", \"pydantic (<2)\", \"starlette\", \"uvicorn\"]",
      "6387: testing = [\"GitPython (<3.1.19)\", \"beautifulsoup4\", \"cookiecutter (==1.7.3)\", \"datasets (!=2.5.0)\", \"dill (<0.3.5)\", \"evaluate (>=0.2.0)\", \"faiss-cpu\", \"hf-doc-builder (>=0.3.0)\", \"nltk\", \"parameterized\", \"protobuf\", \"psutil\", \"pydantic (<2)\", \"pytest (>=7.2.0)\", \"pytest-timeout\", \"pytest-xdist\", \"rjieba\", \"rouge-score (!=0.0.7,!=0.0.8,!=0.1,!=0.1.1)\", \"ruff (==0.1.5)\", \"sacrebleu (>=1.4.12,<2.0.0)\", \"sacremoses\", \"tensorboard\", \"timeout-decorator\"]",
      "6388: tf = [\"keras-nlp (>=0.3.1)\", \"onnxconverter-common\", \"tensorflow (>=2.6,<2.16)\", \"tensorflow-text (<2.16)\", \"tf2onnx\"]",
      "6389: tf-cpu = [\"keras-nlp (>=0.3.1)\", \"onnxconverter-common\", \"tensorflow-cpu (>=2.6,<2.16)\", \"tensorflow-text (<2.16)\", \"tf2onnx\"]",
      "6392: tokenizers = [\"tokenizers (>=0.14,<0.19)\"]",
      "6393: torch = [\"accelerate (>=0.21.0)\", \"torch (>=1.10,!=1.12.0)\"]",
      "6395: torch-vision = [\"Pillow (>=10.0.1,<=15.0)\", \"torchvision\"]",
      "6396: torchhub = [\"filelock\", \"huggingface-hub (>=0.19.3,<1.0)\", \"importlib-metadata\", \"numpy (>=1.17)\", \"packaging (>=20.0)\", \"protobuf\", \"regex (!=2019.12.17)\", \"requests\", \"sentencepiece (>=0.1.91,!=0.1.92)\", \"tokenizers (>=0.14,<0.19)\", \"torch (>=1.10,!=1.12.0)\", \"tqdm (>=4.27)\"]",
      "6397: video = [\"av (==9.2.0)\", \"decord (==0.6.0)\"]",
      "6398: vision = [\"Pillow (>=10.0.1,<=15.0)\"]",
      "",
      "---------------",
      "--- Hunk 10 ---",
      "[Context before]",
      "6956: [metadata]",
      "6957: lock-version = \"2.0\"",
      "6958: python-versions = \">=3.8,<3.11\"",
      "",
      "[Removed Lines]",
      "6959: content-hash = \"4c84d994449f859816e48dd00d77f31f6f9d964e29a9f6060300c51d923786e0\"",
      "",
      "[Added Lines]",
      "7189: content-hash = \"c1c51259ab3b886039dcf7eb746a45815d4b8afcaa4bdbe179c891810aee553f\"",
      "",
      "---------------"
    ],
    "rasa/core/featurizers/single_state_featurizer.py||rasa/core/featurizers/single_state_featurizer.py": [
      "File: rasa/core/featurizers/single_state_featurizer.py -> rasa/core/featurizers/single_state_featurizer.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "1: import logging",
      "2: import numpy as np",
      "3: import scipy.sparse",
      "6: from rasa.core.featurizers.precomputation import MessageContainerForCoreFeaturization",
      "7: from rasa.nlu.extractors.extractor import EntityTagSpec",
      "",
      "[Removed Lines]",
      "4: from typing import List, Optional, Dict, Text, Set, Any",
      "",
      "[Added Lines]",
      "2: from typing import List, Optional, Dict, Text, Set, Any",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "362:             for action in domain.action_names_or_texts",
      "363:         ]",
      "366: class IntentTokenizerSingleStateFeaturizer(SingleStateFeaturizer):",
      "367:     \"\"\"A SingleStateFeaturizer for use with policies that predict intent labels.\"\"\"",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "366:     def to_dict(self) -> Dict[str, Any]:",
      "367:         return {",
      "368:             \"action_texts\": self.action_texts,",
      "369:             \"entity_tag_specs\": self.entity_tag_specs,",
      "370:             \"feature_states\": self._default_feature_states,",
      "371:         }",
      "373:     @classmethod",
      "374:     def create_from_dict(",
      "375:         cls, data: Dict[str, Any]",
      "376:     ) -> Optional[\"SingleStateFeaturizer\"]:",
      "377:         if not data:",
      "378:             return None",
      "380:         featurizer = SingleStateFeaturizer()",
      "381:         featurizer.action_texts = data[\"action_texts\"]",
      "382:         featurizer._default_feature_states = data[\"feature_states\"]",
      "383:         featurizer.entity_tag_specs = data[\"entity_tag_specs\"]",
      "384:         return featurizer",
      "",
      "---------------"
    ],
    "rasa/core/featurizers/tracker_featurizers.py||rasa/core/featurizers/tracker_featurizers.py": [
      "File: rasa/core/featurizers/tracker_featurizers.py -> rasa/core/featurizers/tracker_featurizers.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "1: from __future__ import annotations",
      "9: from typing import (",
      "10:     Tuple,",
      "11:     List,",
      "",
      "[Removed Lines]",
      "2: from pathlib import Path",
      "3: from collections import defaultdict",
      "4: from abc import abstractmethod",
      "5: import jsonpickle",
      "6: import logging",
      "8: from tqdm import tqdm",
      "",
      "[Added Lines]",
      "3: import logging",
      "4: from abc import abstractmethod",
      "5: from collections import defaultdict",
      "6: from pathlib import Path",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "18:     Set,",
      "19:     DefaultDict,",
      "20:     cast,",
      "21: )",
      "22: import numpy as np",
      "27: import rasa.shared.core.trackers",
      "28: import rasa.shared.utils.io",
      "34: from rasa.shared.core.constants import (",
      "35:     USER,",
      "36:     ACTION_UNLIKELY_INTENT_NAME,",
      "37:     PREVIOUS_ACTION,",
      "38: )",
      "39: from rasa.shared.exceptions import RasaException",
      "40: from rasa.utils.tensorflow.constants import LABEL_PAD_ID",
      "41: from rasa.utils.tensorflow.model_data import ragged_array_to_ndarray",
      "",
      "[Removed Lines]",
      "24: from rasa.core.featurizers.single_state_featurizer import SingleStateFeaturizer",
      "25: from rasa.core.featurizers.precomputation import MessageContainerForCoreFeaturization",
      "26: from rasa.core.exceptions import InvalidTrackerFeaturizerUsageError",
      "29: from rasa.shared.nlu.constants import TEXT, INTENT, ENTITIES, ACTION_NAME",
      "30: from rasa.shared.nlu.training_data.features import Features",
      "31: from rasa.shared.core.trackers import DialogueStateTracker",
      "32: from rasa.shared.core.domain import State, Domain",
      "33: from rasa.shared.core.events import Event, ActionExecuted, UserUttered",
      "",
      "[Added Lines]",
      "19:     Type,",
      "20:     Callable,",
      "21:     ClassVar,",
      "25: from tqdm import tqdm",
      "29: from rasa.core.exceptions import InvalidTrackerFeaturizerUsageError",
      "30: from rasa.core.featurizers.precomputation import MessageContainerForCoreFeaturization",
      "31: from rasa.core.featurizers.single_state_featurizer import SingleStateFeaturizer",
      "37: from rasa.shared.core.domain import State, Domain",
      "38: from rasa.shared.core.events import Event, ActionExecuted, UserUttered",
      "39: from rasa.shared.core.trackers import DialogueStateTracker",
      "41: from rasa.shared.nlu.constants import TEXT, INTENT, ENTITIES, ACTION_NAME",
      "42: from rasa.shared.nlu.training_data.features import Features",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "64: class TrackerFeaturizer:",
      "65:     \"\"\"Base class for actual tracker featurizers.\"\"\"",
      "67:     def __init__(",
      "68:         self, state_featurizer: Optional[SingleStateFeaturizer] = None",
      "69:     ) -> None:",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "70:     # Class registry to store all subclasses",
      "71:     _registry: ClassVar[Dict[str, Type[\"TrackerFeaturizer\"]]] = {}",
      "72:     _featurizer_type: str = \"TrackerFeaturizer\"",
      "",
      "---------------",
      "--- Hunk 4 ---",
      "[Context before]",
      "74:         \"\"\"",
      "75:         self.state_featurizer = state_featurizer",
      "77:     @staticmethod",
      "78:     def _create_states(",
      "79:         tracker: DialogueStateTracker,",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "84:     @classmethod",
      "85:     def register(cls, featurizer_type: str) -> Callable:",
      "86:         \"\"\"Decorator to register featurizer subclasses.\"\"\"",
      "88:         def wrapper(subclass: Type[\"TrackerFeaturizer\"]) -> Type[\"TrackerFeaturizer\"]:",
      "89:             cls._registry[featurizer_type] = subclass",
      "90:             # Store the type identifier in the class for serialization",
      "91:             subclass._featurizer_type = featurizer_type",
      "92:             return subclass",
      "94:         return wrapper",
      "96:     @classmethod",
      "97:     def from_dict(cls, data: Dict[str, Any]) -> \"TrackerFeaturizer\":",
      "98:         \"\"\"Create featurizer instance from dictionary.\"\"\"",
      "99:         featurizer_type = data.pop(\"type\")",
      "101:         if featurizer_type not in cls._registry:",
      "102:             raise ValueError(f\"Unknown featurizer type: {featurizer_type}\")",
      "104:         # Get the correct subclass and instantiate it",
      "105:         subclass = cls._registry[featurizer_type]",
      "106:         return subclass.create_from_dict(data)",
      "108:     @classmethod",
      "109:     @abstractmethod",
      "110:     def create_from_dict(cls, data: Dict[str, Any]) -> \"TrackerFeaturizer\":",
      "111:         \"\"\"Each subclass must implement its own creation from dict method.\"\"\"",
      "112:         pass",
      "",
      "---------------",
      "--- Hunk 5 ---",
      "[Context before]",
      "465:             self.state_featurizer.entity_tag_specs = []",
      "467:         # noinspection PyTypeChecker",
      "472:     @staticmethod",
      "473:     def load(path: Union[Text, Path]) -> Optional[TrackerFeaturizer]:",
      "",
      "[Removed Lines]",
      "468:         rasa.shared.utils.io.write_text_file(",
      "469:             str(jsonpickle.encode(self)), featurizer_file",
      "470:         )",
      "",
      "[Added Lines]",
      "505:         rasa.shared.utils.io.dump_obj_as_json_to_file(featurizer_file, self.to_dict())",
      "",
      "---------------",
      "--- Hunk 6 ---",
      "[Context before]",
      "481:         \"\"\"",
      "482:         featurizer_file = Path(path) / FEATURIZER_FILE",
      "483:         if featurizer_file.is_file():",
      "486:         logger.error(",
      "487:             f\"Couldn't load featurizer for policy. \"",
      "",
      "[Removed Lines]",
      "484:             return jsonpickle.decode(rasa.shared.utils.io.read_file(featurizer_file))",
      "",
      "[Added Lines]",
      "519:             data = rasa.shared.utils.io.read_json_file(featurizer_file)",
      "521:             if \"type\" not in data:",
      "522:                 logger.error(",
      "523:                     f\"Couldn't load featurizer for policy. \"",
      "524:                     f\"File '{featurizer_file}' does not contain all \"",
      "525:                     f\"necessary information. 'type' is missing.\"",
      "526:                 )",
      "527:                 return None",
      "529:             return TrackerFeaturizer.from_dict(data)",
      "",
      "---------------",
      "--- Hunk 7 ---",
      "[Context before]",
      "508:             )",
      "509:         ]",
      "512: class FullDialogueTrackerFeaturizer(TrackerFeaturizer):",
      "513:     \"\"\"Creates full dialogue training data for time distributed architectures.",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "556:     def to_dict(self) -> Dict[str, Any]:",
      "557:         return {",
      "558:             \"type\": self.__class__._featurizer_type,",
      "559:             \"state_featurizer\": (",
      "560:                 self.state_featurizer.to_dict() if self.state_featurizer else None",
      "561:             ),",
      "562:         }",
      "565: @TrackerFeaturizer.register(\"FullDialogueTrackerFeaturizer\")",
      "",
      "---------------",
      "--- Hunk 8 ---",
      "[Context before]",
      "647:         return trackers_as_states",
      "650: class MaxHistoryTrackerFeaturizer(TrackerFeaturizer):",
      "651:     \"\"\"Truncates the tracker history into `max_history` long sequences.",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "703:     def to_dict(self) -> Dict[str, Any]:",
      "704:         return super().to_dict()",
      "706:     @classmethod",
      "707:     def create_from_dict(cls, data: Dict[str, Any]) -> \"FullDialogueTrackerFeaturizer\":",
      "708:         state_featurizer = SingleStateFeaturizer.create_from_dict(",
      "709:             data[\"state_featurizer\"]",
      "710:         )",
      "711:         return cls(",
      "712:             state_featurizer,",
      "713:         )",
      "716: @TrackerFeaturizer.register(\"MaxHistoryTrackerFeaturizer\")",
      "",
      "---------------",
      "--- Hunk 9 ---",
      "[Context before]",
      "888:         return trackers_as_states",
      "891: class IntentMaxHistoryTrackerFeaturizer(MaxHistoryTrackerFeaturizer):",
      "892:     \"\"\"Truncates the tracker history into `max_history` long sequences.",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "957:     def to_dict(self) -> Dict[str, Any]:",
      "958:         data = super().to_dict()",
      "959:         data.update(",
      "960:             {",
      "961:                 \"remove_duplicates\": self.remove_duplicates,",
      "962:                 \"max_history\": self.max_history,",
      "963:             }",
      "964:         )",
      "965:         return data",
      "967:     @classmethod",
      "968:     def create_from_dict(cls, data: Dict[str, Any]) -> \"MaxHistoryTrackerFeaturizer\":",
      "969:         state_featurizer = SingleStateFeaturizer.create_from_dict(",
      "970:             data[\"state_featurizer\"]",
      "971:         )",
      "972:         return cls(state_featurizer, data[\"max_history\"], data[\"remove_duplicates\"])",
      "975: @TrackerFeaturizer.register(\"IntentMaxHistoryTrackerFeaturizer\")",
      "",
      "---------------",
      "--- Hunk 10 ---",
      "[Context before]",
      "1167:         return trackers_as_states",
      "1170: def _is_prev_action_unlikely_intent_in_state(state: State) -> bool:",
      "1171:     prev_action_name = state.get(PREVIOUS_ACTION, {}).get(ACTION_NAME)",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "1254:     def to_dict(self) -> Dict[str, Any]:",
      "1255:         return super().to_dict()",
      "1257:     @classmethod",
      "1258:     def create_from_dict(",
      "1259:         cls, data: Dict[str, Any]",
      "1260:     ) -> \"IntentMaxHistoryTrackerFeaturizer\":",
      "1261:         state_featurizer = SingleStateFeaturizer.create_from_dict(",
      "1262:             data[\"state_featurizer\"]",
      "1263:         )",
      "1264:         return cls(state_featurizer, data[\"max_history\"], data[\"remove_duplicates\"])",
      "",
      "---------------"
    ],
    "rasa/core/policies/ted_policy.py||rasa/core/policies/ted_policy.py": [
      "File: rasa/core/policies/ted_policy.py -> rasa/core/policies/ted_policy.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "1: from __future__ import annotations",
      "5: from pathlib import Path",
      "6: from collections import defaultdict",
      "7: import contextlib",
      "9: import numpy as np",
      "10: import tensorflow as tf",
      "13: from rasa.engine.graph import ExecutionContext",
      "14: from rasa.engine.storage.resource import Resource",
      "15: from rasa.engine.storage.storage import ModelStorage",
      "",
      "[Removed Lines]",
      "2: import logging",
      "4: from rasa.engine.recipes.default_recipe import DefaultV1Recipe",
      "11: from typing import Any, List, Optional, Text, Dict, Tuple, Union, Type",
      "",
      "[Added Lines]",
      "3: import logging",
      "7: from typing import Any, List, Optional, Text, Dict, Tuple, Union, Type",
      "12: from rasa.engine.recipes.default_recipe import DefaultV1Recipe",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "49: from rasa.shared.core.events import EntitiesAdded, Event",
      "50: from rasa.shared.core.domain import Domain",
      "51: from rasa.shared.nlu.training_data.message import Message",
      "53: import rasa.shared.utils.io",
      "54: import rasa.utils.io",
      "55: from rasa.utils import train_utils",
      "61:     FeatureArray,",
      "63: )",
      "64: from rasa.utils.tensorflow.model_data_utils import convert_to_data_format",
      "65: from rasa.utils.tensorflow.constants import (",
      "66:     LABEL,",
      "",
      "[Removed Lines]",
      "52: from rasa.shared.nlu.training_data.features import Features",
      "56: from rasa.utils.tensorflow.models import RasaModel, TransformerRasaModel",
      "57: from rasa.utils.tensorflow import rasa_layers",
      "58: from rasa.utils.tensorflow.model_data import (",
      "59:     RasaModelData,",
      "60:     FeatureSignature,",
      "62:     Data,",
      "",
      "[Added Lines]",
      "52: from rasa.shared.nlu.training_data.features import (",
      "53:     Features,",
      "54:     save_features,",
      "55:     load_features,",
      "56: )",
      "60: from rasa.utils.tensorflow.feature_array import (",
      "62:     serialize_nested_feature_arrays,",
      "63:     deserialize_nested_feature_arrays,",
      "65: from rasa.utils.tensorflow.models import RasaModel, TransformerRasaModel",
      "66: from rasa.utils.tensorflow import rasa_layers",
      "67: from rasa.utils.tensorflow.model_data import RasaModelData, FeatureSignature, Data",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "961:             model_path: Path where model is to be persisted",
      "962:         \"\"\"",
      "963:         model_filename = self._metadata_filename()",
      "969:         )",
      "972:         )",
      "975:         )",
      "978:             dict(self._label_data.data) if self._label_data is not None else {},",
      "979:         )",
      "980:         entity_tag_specs = (",
      "981:             [tag_spec._asdict() for tag_spec in self._entity_tag_specs]",
      "982:             if self._entity_tag_specs",
      "",
      "[Removed Lines]",
      "964:         rasa.utils.io.json_pickle(",
      "965:             model_path / f\"{model_filename}.priority.pkl\", self.priority",
      "966:         )",
      "967:         rasa.utils.io.pickle_dump(",
      "968:             model_path / f\"{model_filename}.meta.pkl\", self.config",
      "970:         rasa.utils.io.pickle_dump(",
      "971:             model_path / f\"{model_filename}.data_example.pkl\", self.data_example",
      "973:         rasa.utils.io.pickle_dump(",
      "974:             model_path / f\"{model_filename}.fake_features.pkl\", self.fake_features",
      "976:         rasa.utils.io.pickle_dump(",
      "977:             model_path / f\"{model_filename}.label_data.pkl\",",
      "",
      "[Added Lines]",
      "968:         rasa.shared.utils.io.dump_obj_as_json_to_file(",
      "969:             model_path / f\"{model_filename}.priority.json\", self.priority",
      "971:         rasa.shared.utils.io.dump_obj_as_json_to_file(",
      "972:             model_path / f\"{model_filename}.meta.json\", self.config",
      "974:         # save data example",
      "975:         serialize_nested_feature_arrays(",
      "976:             self.data_example,",
      "977:             str(model_path / f\"{model_filename}.data_example.st\"),",
      "978:             str(model_path / f\"{model_filename}.data_example_metadata.json\"),",
      "980:         # save label data",
      "981:         serialize_nested_feature_arrays(",
      "983:             str(model_path / f\"{model_filename}.label_data.st\"),",
      "984:             str(model_path / f\"{model_filename}.label_data_metadata.json\"),",
      "985:         )",
      "986:         # save fake features",
      "987:         metadata = save_features(",
      "988:             self.fake_features, str(model_path / f\"{model_filename}.fake_features.st\")",
      "989:         )",
      "990:         rasa.shared.utils.io.dump_obj_as_json_to_file(",
      "991:             model_path / f\"{model_filename}.fake_features_metadata.json\", metadata",
      "",
      "---------------",
      "--- Hunk 4 ---",
      "[Context before]",
      "994:             model_path: Path where model is to be persisted.",
      "995:         \"\"\"",
      "996:         tf_model_file = model_path / f\"{cls._metadata_filename()}.tf_model\"",
      "999:         )",
      "1002:         )",
      "1005:         )",
      "1009:         )",
      "1010:         entity_tag_specs = rasa.shared.utils.io.read_json_file(",
      "1011:             model_path / f\"{cls._metadata_filename()}.entity_tag_specs.json\"",
      "",
      "[Removed Lines]",
      "997:         loaded_data = rasa.utils.io.pickle_load(",
      "998:             model_path / f\"{cls._metadata_filename()}.data_example.pkl\"",
      "1000:         label_data = rasa.utils.io.pickle_load(",
      "1001:             model_path / f\"{cls._metadata_filename()}.label_data.pkl\"",
      "1003:         fake_features = rasa.utils.io.pickle_load(",
      "1004:             model_path / f\"{cls._metadata_filename()}.fake_features.pkl\"",
      "1006:         label_data = RasaModelData(data=label_data)",
      "1007:         priority = rasa.utils.io.json_unpickle(",
      "1008:             model_path / f\"{cls._metadata_filename()}.priority.pkl\"",
      "",
      "[Added Lines]",
      "1012:         # load data example",
      "1013:         loaded_data = deserialize_nested_feature_arrays(",
      "1014:             str(model_path / f\"{cls._metadata_filename()}.data_example.st\"),",
      "1015:             str(model_path / f\"{cls._metadata_filename()}.data_example_metadata.json\"),",
      "1017:         # load label data",
      "1018:         loaded_label_data = deserialize_nested_feature_arrays(",
      "1019:             str(model_path / f\"{cls._metadata_filename()}.label_data.st\"),",
      "1020:             str(model_path / f\"{cls._metadata_filename()}.label_data_metadata.json\"),",
      "1022:         label_data = RasaModelData(data=loaded_label_data)",
      "1024:         # load fake features",
      "1025:         metadata = rasa.shared.utils.io.read_json_file(",
      "1026:             model_path / f\"{cls._metadata_filename()}.fake_features_metadata.json\"",
      "1028:         fake_features = load_features(",
      "1029:             str(model_path / f\"{cls._metadata_filename()}.fake_features.st\"), metadata",
      "1030:         )",
      "1032:         priority = rasa.shared.utils.io.read_json_file(",
      "1033:             model_path / f\"{cls._metadata_filename()}.priority.json\"",
      "",
      "---------------",
      "--- Hunk 5 ---",
      "[Context before]",
      "1023:             )",
      "1024:             for tag_spec in entity_tag_specs",
      "1025:         ]",
      "1028:         )",
      "1030:         return {",
      "",
      "[Removed Lines]",
      "1026:         model_config = rasa.utils.io.pickle_load(",
      "1027:             model_path / f\"{cls._metadata_filename()}.meta.pkl\"",
      "",
      "[Added Lines]",
      "1051:         model_config = rasa.shared.utils.io.read_json_file(",
      "1052:             model_path / f\"{cls._metadata_filename()}.meta.json\"",
      "",
      "---------------",
      "--- Hunk 6 ---",
      "[Context before]",
      "1070:     ) -> TEDPolicy:",
      "1071:         featurizer = TrackerFeaturizer.load(model_path)",
      "1074:             return cls(",
      "1075:                 config,",
      "1076:                 model_storage,",
      "",
      "[Removed Lines]",
      "1073:         if not (model_path / f\"{cls._metadata_filename()}.data_example.pkl\").is_file():",
      "",
      "[Added Lines]",
      "1098:         if not (model_path / f\"{cls._metadata_filename()}.data_example.st\").is_file():",
      "",
      "---------------"
    ],
    "rasa/core/policies/unexpected_intent_policy.py||rasa/core/policies/unexpected_intent_policy.py": [
      "File: rasa/core/policies/unexpected_intent_policy.py -> rasa/core/policies/unexpected_intent_policy.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "16: from rasa.shared.core.trackers import DialogueStateTracker",
      "17: from rasa.shared.core.constants import SLOTS, ACTIVE_LOOP, ACTION_UNLIKELY_INTENT_NAME",
      "18: from rasa.shared.core.events import UserUttered, ActionExecuted",
      "19: from rasa.shared.nlu.constants import (",
      "20:     INTENT,",
      "21:     TEXT,",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "20: import rasa.shared.utils.io",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "103: )",
      "104: from rasa.utils.tensorflow import layers",
      "105: from rasa.utils.tensorflow.model_data import RasaModelData, FeatureArray, Data",
      "108: from rasa.core.exceptions import RasaCoreException",
      "109: from rasa.shared.utils import common",
      "",
      "[Removed Lines]",
      "107: import rasa.utils.io as io_utils",
      "",
      "[Added Lines]",
      "[None]",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "881:             model_path: Path where model is to be persisted",
      "882:         \"\"\"",
      "883:         super().persist_model_utilities(model_path)",
      "887:         )",
      "889:     @classmethod",
      "",
      "[Removed Lines]",
      "884:         io_utils.pickle_dump(",
      "885:             model_path / f\"{self._metadata_filename()}.label_quantiles.pkl\",",
      "886:             self.label_quantiles,",
      "",
      "[Added Lines]",
      "885:         from safetensors.numpy import save_file",
      "887:         save_file(",
      "888:             {str(k): np.array(v) for k, v in self.label_quantiles.items()},",
      "889:             model_path / f\"{self._metadata_filename()}.label_quantiles.st\",",
      "",
      "---------------",
      "--- Hunk 4 ---",
      "[Context before]",
      "894:             model_path: Path where model is to be persisted.",
      "895:         \"\"\"",
      "896:         model_utilties = super()._load_model_utilities(model_path)",
      "899:         )",
      "900:         model_utilties.update({\"label_quantiles\": label_quantiles})",
      "901:         return model_utilties",
      "",
      "[Removed Lines]",
      "897:         label_quantiles = io_utils.pickle_load(",
      "898:             model_path / f\"{cls._metadata_filename()}.label_quantiles.pkl\"",
      "",
      "[Added Lines]",
      "901:         from safetensors.numpy import load_file",
      "903:         loaded_label_quantiles = load_file(",
      "904:             model_path / f\"{cls._metadata_filename()}.label_quantiles.st\"",
      "906:         label_quantiles = {int(k): list(v) for k, v in loaded_label_quantiles.items()}",
      "",
      "---------------"
    ],
    "rasa/nlu/classifiers/diet_classifier.py||rasa/nlu/classifiers/diet_classifier.py": [
      "File: rasa/nlu/classifiers/diet_classifier.py -> rasa/nlu/classifiers/diet_classifier.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "1: from __future__ import annotations",
      "2: import copy",
      "3: import logging",
      "4: from collections import defaultdict",
      "5: from pathlib import Path",
      "10: import numpy as np",
      "11: import scipy.sparse",
      "12: import tensorflow as tf",
      "16: from rasa.engine.graph import ExecutionContext, GraphComponent",
      "17: from rasa.engine.recipes.default_recipe import DefaultV1Recipe",
      "18: from rasa.engine.storage.resource import Resource",
      "",
      "[Removed Lines]",
      "7: from rasa.exceptions import ModelNotFound",
      "8: from rasa.nlu.featurizers.featurizer import Featurizer",
      "14: from typing import Any, Dict, List, Optional, Text, Tuple, Union, TypeVar, Type",
      "",
      "[Added Lines]",
      "7: from typing import Any, Dict, List, Optional, Text, Tuple, Union, TypeVar, Type",
      "13: from rasa.exceptions import ModelNotFound",
      "14: from rasa.nlu.featurizers.featurizer import Featurizer",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "20: from rasa.nlu.extractors.extractor import EntityExtractorMixin",
      "21: from rasa.nlu.classifiers.classifier import IntentClassifier",
      "22: import rasa.shared.utils.io",
      "24: import rasa.nlu.utils.bilou_utils as bilou_utils",
      "25: from rasa.shared.constants import DIAGNOSTIC_DATA",
      "26: from rasa.nlu.extractors.extractor import EntityTagSpec",
      "27: from rasa.nlu.classifiers import LABEL_RANKING_LENGTH",
      "28: from rasa.utils import train_utils",
      "29: from rasa.utils.tensorflow import rasa_layers",
      "30: from rasa.utils.tensorflow.models import RasaModel, TransformerRasaModel",
      "31: from rasa.utils.tensorflow.model_data import (",
      "32:     RasaModelData,",
      "33:     FeatureSignature,",
      "35: )",
      "36: from rasa.nlu.constants import TOKENS_NAMES, DEFAULT_TRANSFORMER_SIZE",
      "37: from rasa.shared.nlu.constants import (",
      "",
      "[Removed Lines]",
      "23: import rasa.utils.io as io_utils",
      "34:     FeatureArray,",
      "",
      "[Added Lines]",
      "28: from rasa.utils.tensorflow.feature_array import (",
      "29:     FeatureArray,",
      "30:     serialize_nested_feature_arrays,",
      "31:     deserialize_nested_feature_arrays,",
      "32: )",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "1086:             self.model.save(str(tf_model_file))",
      "1094:             )",
      "1097:                 dict(self._label_data.data) if self._label_data is not None else {},",
      "1098:             )",
      "1100:                 model_path / f\"{file_name}.index_label_id_mapping.json\",",
      "1101:                 self.index_label_id_mapping,",
      "1102:             )",
      "",
      "[Removed Lines]",
      "1088:             io_utils.pickle_dump(",
      "1089:                 model_path / f\"{file_name}.data_example.pkl\", self._data_example",
      "1090:             )",
      "1091:             io_utils.pickle_dump(",
      "1092:                 model_path / f\"{file_name}.sparse_feature_sizes.pkl\",",
      "1093:                 self._sparse_feature_sizes,",
      "1095:             io_utils.pickle_dump(",
      "1096:                 model_path / f\"{file_name}.label_data.pkl\",",
      "1099:             io_utils.json_pickle(",
      "",
      "[Added Lines]",
      "1089:             # save data example",
      "1090:             serialize_nested_feature_arrays(",
      "1091:                 self._data_example,",
      "1092:                 model_path / f\"{file_name}.data_example.st\",",
      "1093:                 model_path / f\"{file_name}.data_example_metadata.json\",",
      "1095:             # save label data",
      "1096:             serialize_nested_feature_arrays(",
      "1098:                 model_path / f\"{file_name}.label_data.st\",",
      "1099:                 model_path / f\"{file_name}.label_data_metadata.json\",",
      "1102:             rasa.shared.utils.io.dump_obj_as_json_to_file(",
      "1103:                 model_path / f\"{file_name}.sparse_feature_sizes.json\",",
      "1104:                 self._sparse_feature_sizes,",
      "1105:             )",
      "1106:             rasa.shared.utils.io.dump_obj_as_json_to_file(",
      "",
      "---------------",
      "--- Hunk 4 ---",
      "[Context before]",
      "1185:     ]:",
      "1186:         file_name = cls.__name__",
      "1190:         )",
      "1195:         )",
      "1197:             model_path / f\"{file_name}.index_label_id_mapping.json\"",
      "1198:         )",
      "1199:         entity_tag_specs = rasa.shared.utils.io.read_json_file(",
      "",
      "[Removed Lines]",
      "1188:         data_example = io_utils.pickle_load(",
      "1189:             model_path / f\"{file_name}.data_example.pkl\"",
      "1191:         label_data = io_utils.pickle_load(model_path / f\"{file_name}.label_data.pkl\")",
      "1192:         label_data = RasaModelData(data=label_data)",
      "1193:         sparse_feature_sizes = io_utils.pickle_load(",
      "1194:             model_path / f\"{file_name}.sparse_feature_sizes.pkl\"",
      "1196:         index_label_id_mapping = io_utils.json_unpickle(",
      "",
      "[Added Lines]",
      "1195:         # load data example",
      "1196:         data_example = deserialize_nested_feature_arrays(",
      "1197:             str(model_path / f\"{file_name}.data_example.st\"),",
      "1198:             str(model_path / f\"{file_name}.data_example_metadata.json\"),",
      "1200:         # load label data",
      "1201:         loaded_label_data = deserialize_nested_feature_arrays(",
      "1202:             str(model_path / f\"{file_name}.label_data.st\"),",
      "1203:             str(model_path / f\"{file_name}.label_data_metadata.json\"),",
      "1204:         )",
      "1205:         label_data = RasaModelData(data=loaded_label_data)",
      "1207:         sparse_feature_sizes = rasa.shared.utils.io.read_json_file(",
      "1208:             model_path / f\"{file_name}.sparse_feature_sizes.json\"",
      "1210:         index_label_id_mapping = rasa.shared.utils.io.read_json_file(",
      "",
      "---------------",
      "--- Hunk 5 ---",
      "[Context before]",
      "1213:             for tag_spec in entity_tag_specs",
      "1214:         ]",
      "1217:         index_label_id_mapping = {",
      "1218:             int(key): value for key, value in index_label_id_mapping.items()",
      "1219:         }",
      "",
      "[Removed Lines]",
      "1216:         # jsonpickle converts dictionary keys to strings",
      "",
      "[Added Lines]",
      "[None]",
      "",
      "---------------"
    ],
    "rasa/nlu/classifiers/logistic_regression_classifier.py||rasa/nlu/classifiers/logistic_regression_classifier.py": [
      "File: rasa/nlu/classifiers/logistic_regression_classifier.py -> rasa/nlu/classifiers/logistic_regression_classifier.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "1: import logging",
      "2: from typing import Any, Text, Dict, List, Type, Tuple",
      "5: from scipy.sparse import hstack, vstack, csr_matrix",
      "6: from sklearn.linear_model import LogisticRegression",
      "8: from rasa.engine.storage.resource import Resource",
      "9: from rasa.engine.storage.storage import ModelStorage",
      "12: from rasa.nlu.classifiers import LABEL_RANKING_LENGTH",
      "14: from rasa.nlu.classifiers.classifier import IntentClassifier",
      "17: from rasa.shared.nlu.constants import TEXT, INTENT",
      "18: from rasa.utils.tensorflow.constants import RANKING_LENGTH",
      "20: logger = logging.getLogger(__name__)",
      "",
      "[Removed Lines]",
      "4: import joblib",
      "10: from rasa.engine.recipes.default_recipe import DefaultV1Recipe",
      "11: from rasa.engine.graph import ExecutionContext, GraphComponent",
      "13: from rasa.nlu.featurizers.featurizer import Featurizer",
      "15: from rasa.shared.nlu.training_data.training_data import TrainingData",
      "16: from rasa.shared.nlu.training_data.message import Message",
      "",
      "[Added Lines]",
      "7: from rasa.engine.graph import ExecutionContext, GraphComponent",
      "8: from rasa.engine.recipes.default_recipe import DefaultV1Recipe",
      "13: from rasa.nlu.featurizers.featurizer import Featurizer",
      "15: from rasa.shared.nlu.training_data.message import Message",
      "16: from rasa.shared.nlu.training_data.training_data import TrainingData",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "159:     def persist(self) -> None:",
      "160:         \"\"\"Persist this model into the passed directory.\"\"\"",
      "161:         with self._model_storage.write_to(self._resource) as model_dir:",
      "164:             logger.debug(f\"Saved intent classifier to '{path}'.\")",
      "166:     @classmethod",
      "",
      "[Removed Lines]",
      "162:             path = model_dir / f\"{self._resource.name}.joblib\"",
      "163:             joblib.dump(self.clf, path)",
      "",
      "[Added Lines]",
      "160:         import skops.io as sio",
      "163:             path = model_dir / f\"{self._resource.name}.skops\"",
      "164:             sio.dump(self.clf, path)",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "174:     ) -> \"LogisticRegressionClassifier\":",
      "175:         \"\"\"Loads trained component (see parent class for full docstring).\"\"\"",
      "176:         try:",
      "177:             with model_storage.read_from(resource) as model_dir:",
      "179:                 component = cls(",
      "180:                     config, execution_context.node_name, model_storage, resource",
      "181:                 )",
      "",
      "[Removed Lines]",
      "178:                 classifier = joblib.load(model_dir / f\"{resource.name}.joblib\")",
      "",
      "[Added Lines]",
      "177:         import skops.io as sio",
      "181:                 classifier_file = model_dir / f\"{resource.name}.skops\"",
      "182:                 unknown_types = sio.get_untrusted_types(file=classifier_file)",
      "184:                 if unknown_types:",
      "185:                     logger.debug(",
      "186:                         f\"Untrusted types ({unknown_types}) found when \"",
      "187:                         f\"loading {classifier_file}!\",",
      "188:                     )",
      "189:                     raise ValueError()",
      "191:                 classifier = sio.load(classifier_file, trusted=unknown_types)",
      "",
      "---------------"
    ],
    "rasa/nlu/classifiers/sklearn_intent_classifier.py||rasa/nlu/classifiers/sklearn_intent_classifier.py": [
      "File: rasa/nlu/classifiers/sklearn_intent_classifier.py -> rasa/nlu/classifiers/sklearn_intent_classifier.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "1: from __future__ import annotations",
      "2: import logging",
      "4: import typing",
      "5: import warnings",
      "6: from typing import Any, Dict, List, Optional, Text, Tuple, Type",
      "",
      "[Removed Lines]",
      "3: from rasa.nlu.featurizers.dense_featurizer.dense_featurizer import DenseFeaturizer",
      "",
      "[Added Lines]",
      "[None]",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "8: import numpy as np",
      "10: import rasa.shared.utils.io",
      "12: from rasa.engine.graph import GraphComponent, ExecutionContext",
      "13: from rasa.engine.recipes.default_recipe import DefaultV1Recipe",
      "14: from rasa.engine.storage.resource import Resource",
      "15: from rasa.engine.storage.storage import ModelStorage",
      "17: from rasa.nlu.classifiers import LABEL_RANKING_LENGTH",
      "18: from rasa.shared.exceptions import RasaException",
      "19: from rasa.shared.nlu.constants import TEXT",
      "22: from rasa.shared.nlu.training_data.message import Message",
      "23: from rasa.utils.tensorflow.constants import FEATURIZERS",
      "25: logger = logging.getLogger(__name__)",
      "",
      "[Removed Lines]",
      "11: import rasa.utils.io as io_utils",
      "16: from rasa.shared.constants import DOCS_URL_TRAINING_DATA_NLU",
      "20: from rasa.nlu.classifiers.classifier import IntentClassifier",
      "21: from rasa.shared.nlu.training_data.training_data import TrainingData",
      "",
      "[Added Lines]",
      "16: from rasa.nlu.classifiers.classifier import IntentClassifier",
      "17: from rasa.nlu.featurizers.dense_featurizer.dense_featurizer import DenseFeaturizer",
      "18: from rasa.shared.constants import DOCS_URL_TRAINING_DATA_NLU",
      "22: from rasa.shared.nlu.training_data.training_data import TrainingData",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "267:     def persist(self) -> None:",
      "268:         \"\"\"Persist this model into the passed directory.\"\"\"",
      "269:         with self._model_storage.write_to(self._resource) as model_dir:",
      "270:             file_name = self.__class__.__name__",
      "274:             if self.clf and self.le:",
      "278:     @classmethod",
      "279:     def load(",
      "",
      "[Removed Lines]",
      "271:             classifier_file_name = model_dir / f\"{file_name}_classifier.pkl\"",
      "272:             encoder_file_name = model_dir / f\"{file_name}_encoder.pkl\"",
      "275:                 io_utils.json_pickle(encoder_file_name, self.le.classes_)",
      "276:                 io_utils.json_pickle(classifier_file_name, self.clf.best_estimator_)",
      "",
      "[Added Lines]",
      "269:         import skops.io as sio",
      "273:             classifier_file_name = model_dir / f\"{file_name}_classifier.skops\"",
      "274:             encoder_file_name = model_dir / f\"{file_name}_encoder.json\"",
      "277:                 # convert self.le.classes_ (numpy array of strings) to a list in order",
      "278:                 # to use json dump",
      "279:                 rasa.shared.utils.io.dump_obj_as_json_to_file(",
      "280:                     encoder_file_name, list(self.le.classes_)",
      "281:                 )",
      "282:                 sio.dump(self.clf.best_estimator_, classifier_file_name)",
      "",
      "---------------",
      "--- Hunk 4 ---",
      "[Context before]",
      "286:     ) -> SklearnIntentClassifier:",
      "287:         \"\"\"Loads trained component (see parent class for full docstring).\"\"\"",
      "288:         from sklearn.preprocessing import LabelEncoder",
      "290:         try:",
      "291:             with model_storage.read_from(resource) as model_dir:",
      "292:                 file_name = cls.__name__",
      "295:                 if classifier_file.exists():",
      "304:         except ValueError:",
      "305:             logger.debug(",
      "306:                 f\"Failed to load '{cls.__name__}' from model storage. Resource \"",
      "",
      "[Removed Lines]",
      "293:                 classifier_file = model_dir / f\"{file_name}_classifier.pkl\"",
      "296:                     classifier = io_utils.json_unpickle(classifier_file)",
      "298:                     encoder_file = model_dir / f\"{file_name}_encoder.pkl\"",
      "299:                     classes = io_utils.json_unpickle(encoder_file)",
      "300:                     encoder = LabelEncoder()",
      "301:                     encoder.classes_ = classes",
      "303:                     return cls(config, model_storage, resource, classifier, encoder)",
      "",
      "[Added Lines]",
      "295:         import skops.io as sio",
      "300:                 classifier_file = model_dir / f\"{file_name}_classifier.skops\"",
      "303:                     unknown_types = sio.get_untrusted_types(file=classifier_file)",
      "305:                     if unknown_types:",
      "306:                         logger.error(",
      "307:                             f\"Untrusted types ({unknown_types}) found when \"",
      "308:                             f\"loading {classifier_file}!\"",
      "309:                         )",
      "310:                         raise ValueError()",
      "311:                     else:",
      "312:                         classifier = sio.load(classifier_file, trusted=unknown_types)",
      "314:                     encoder_file = model_dir / f\"{file_name}_encoder.json\"",
      "315:                     classes = rasa.shared.utils.io.read_json_file(encoder_file)",
      "317:                     encoder = LabelEncoder()",
      "318:                     intent_classifier = cls(",
      "319:                         config, model_storage, resource, classifier, encoder",
      "320:                     )",
      "321:                     # convert list of strings (class labels) back to numpy array of",
      "322:                     # strings",
      "323:                     intent_classifier.transform_labels_str2num(classes)",
      "324:                     return intent_classifier",
      "",
      "---------------"
    ],
    "rasa/nlu/extractors/crf_entity_extractor.py||rasa/nlu/extractors/crf_entity_extractor.py": [
      "File: rasa/nlu/extractors/crf_entity_extractor.py -> rasa/nlu/extractors/crf_entity_extractor.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "1: from __future__ import annotations",
      "5: import logging",
      "6: import typing",
      "8: import numpy as np",
      "11: import rasa.nlu.utils.bilou_utils as bilou_utils",
      "12: import rasa.shared.utils.io",
      "",
      "[Removed Lines]",
      "3: from collections import OrderedDict",
      "4: from enum import Enum",
      "9: from typing import Any, Dict, List, Optional, Text, Tuple, Callable, Type",
      "",
      "[Added Lines]",
      "5: from collections import OrderedDict",
      "6: from enum import Enum",
      "7: from typing import Any, Dict, List, Optional, Text, Tuple, Callable, Type",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "15: from rasa.engine.recipes.default_recipe import DefaultV1Recipe",
      "16: from rasa.engine.storage.resource import Resource",
      "17: from rasa.engine.storage.storage import ModelStorage",
      "18: from rasa.nlu.test import determine_token_labels",
      "19: from rasa.nlu.tokenizers.spacy_tokenizer import POS_TAG_KEY",
      "21: from rasa.nlu.tokenizers.tokenizer import Token, Tokenizer",
      "25: from rasa.shared.nlu.constants import (",
      "26:     TEXT,",
      "27:     ENTITIES,",
      "",
      "[Removed Lines]",
      "20: from rasa.nlu.extractors.extractor import EntityExtractorMixin",
      "22: from rasa.shared.nlu.training_data.training_data import TrainingData",
      "23: from rasa.shared.nlu.training_data.message import Message",
      "24: from rasa.nlu.constants import TOKENS_NAMES",
      "",
      "[Added Lines]",
      "18: from rasa.nlu.constants import TOKENS_NAMES",
      "19: from rasa.nlu.extractors.extractor import EntityExtractorMixin",
      "23: from rasa.shared.constants import DOCS_URL_COMPONENTS",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "32:     SPLIT_ENTITIES_BY_COMMA,",
      "33:     SPLIT_ENTITIES_BY_COMMA_DEFAULT_VALUE,",
      "34: )",
      "36: from rasa.utils.tensorflow.constants import BILOU_FLAG, FEATURIZERS",
      "38: logger = logging.getLogger(__name__)",
      "",
      "[Removed Lines]",
      "35: from rasa.shared.constants import DOCS_URL_COMPONENTS",
      "",
      "[Added Lines]",
      "34: from rasa.shared.nlu.training_data.message import Message",
      "35: from rasa.shared.nlu.training_data.training_data import TrainingData",
      "",
      "---------------",
      "--- Hunk 4 ---",
      "[Context before]",
      "41:     from sklearn_crfsuite import CRF",
      "44: class CRFToken:",
      "45:     def __init__(",
      "46:         self,",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "44: CONFIG_FEATURES = \"features\"",
      "",
      "---------------",
      "--- Hunk 5 ---",
      "[Context before]",
      "60:         self.entity_role_tag = entity_role_tag",
      "61:         self.entity_group_tag = entity_group_tag",
      "64: class CRFEntityExtractorOptions(str, Enum):",
      "65:     \"\"\"Features that can be used for the 'CRFEntityExtractor'.\"\"\"",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "66:     def to_dict(self) -> Dict[str, Any]:",
      "67:         return {",
      "68:             \"text\": self.text,",
      "69:             \"pos_tag\": self.pos_tag,",
      "70:             \"pattern\": self.pattern,",
      "71:             \"dense_features\": [str(x) for x in list(self.dense_features)],",
      "72:             \"entity_tag\": self.entity_tag,",
      "73:             \"entity_role_tag\": self.entity_role_tag,",
      "74:             \"entity_group_tag\": self.entity_group_tag,",
      "75:         }",
      "77:     @classmethod",
      "78:     def create_from_dict(cls, data: Dict[str, Any]) -> \"CRFToken\":",
      "79:         return cls(",
      "80:             data[\"text\"],",
      "81:             data[\"pos_tag\"],",
      "82:             data[\"pattern\"],",
      "83:             np.array([float(x) for x in data[\"dense_features\"]]),",
      "84:             data[\"entity_tag\"],",
      "85:             data[\"entity_role_tag\"],",
      "86:             data[\"entity_group_tag\"],",
      "87:         )",
      "",
      "---------------",
      "--- Hunk 6 ---",
      "[Context before]",
      "137:             # \"is the preceding token in title case?\"",
      "138:             # POS features require SpacyTokenizer",
      "139:             # pattern feature require RegexFeaturizer",
      "141:                 [",
      "142:                     CRFEntityExtractorOptions.LOW,",
      "143:                     CRFEntityExtractorOptions.TITLE,",
      "",
      "[Removed Lines]",
      "140:             CRFEntityExtractor.CONFIG_FEATURES: [",
      "",
      "[Added Lines]",
      "166:             CONFIG_FEATURES: [",
      "",
      "---------------",
      "--- Hunk 7 ---",
      "[Context before]",
      "200:         )",
      "202:     def _validate_configuration(self) -> None:",
      "204:             raise ValueError(",
      "205:                 \"Need an odd number of crf feature lists to have a center word.\"",
      "206:             )",
      "",
      "[Removed Lines]",
      "203:         if len(self.component_config.get(self.CONFIG_FEATURES, [])) % 2 != 1:",
      "",
      "[Added Lines]",
      "229:         if len(self.component_config.get(CONFIG_FEATURES, [])) % 2 != 1:",
      "",
      "---------------",
      "--- Hunk 8 ---",
      "[Context before]",
      "251:         ]",
      "252:         dataset = [self._convert_to_crf_tokens(example) for example in entity_examples]",
      "258:         return self._resource",
      "",
      "[Removed Lines]",
      "254:         self._train_model(dataset)",
      "256:         self.persist()",
      "",
      "[Added Lines]",
      "280:         self.entity_taggers = self.train_model(",
      "281:             dataset, self.component_config, self.crf_order",
      "282:         )",
      "284:         self.persist(dataset)",
      "",
      "---------------",
      "--- Hunk 9 ---",
      "[Context before]",
      "299:             if include_tag_features:",
      "300:                 self._add_tag_to_crf_token(crf_tokens, predictions)",
      "303:             predictions[tag_name] = entity_tagger.predict_marginals_single(features)",
      "305:         # convert predictions into a list of tags and a list of confidences",
      "",
      "[Removed Lines]",
      "302:             features = self._crf_tokens_to_features(crf_tokens, include_tag_features)",
      "",
      "[Added Lines]",
      "330:             features = self._crf_tokens_to_features(",
      "331:                 crf_tokens, self.component_config, include_tag_features",
      "332:             )",
      "",
      "---------------",
      "--- Hunk 10 ---",
      "[Context before]",
      "390:     ) -> CRFEntityExtractor:",
      "391:         \"\"\"Loads trained component (see parent class for full docstring).\"\"\"",
      "394:         try:",
      "396:             with model_storage.read_from(resource) as model_dir:",
      "413:         except ValueError:",
      "414:             logger.warning(",
      "415:                 f\"Failed to load {cls.__name__} from model storage. Resource \"",
      "",
      "[Removed Lines]",
      "392:         import joblib",
      "395:             entity_taggers = OrderedDict()",
      "397:                 # We have to load in the same order as we persisted things as otherwise",
      "398:                 # the predictions might be off",
      "399:                 file_names = sorted(model_dir.glob(\"**/*.pkl\"))",
      "400:                 if not file_names:",
      "401:                     logger.debug(",
      "402:                         \"Failed to load model for 'CRFEntityExtractor'. \"",
      "403:                         \"Maybe you did not provide enough training data and \"",
      "404:                         \"no model was trained.\"",
      "405:                     )",
      "406:                     return cls(config, model_storage, resource)",
      "408:                 for file_name in file_names:",
      "409:                     name = file_name.stem[1:]",
      "410:                     entity_taggers[name] = joblib.load(file_name)",
      "412:                 return cls(config, model_storage, resource, entity_taggers)",
      "",
      "[Added Lines]",
      "424:                 dataset = rasa.shared.utils.io.read_json_file(",
      "425:                     model_dir / \"crf_dataset.json\"",
      "426:                 )",
      "427:                 crf_order = rasa.shared.utils.io.read_json_file(",
      "428:                     model_dir / \"crf_order.json\"",
      "429:                 )",
      "431:                 dataset = [",
      "432:                     [CRFToken.create_from_dict(token_data) for token_data in sub_list]",
      "433:                     for sub_list in dataset",
      "434:                 ]",
      "436:                 entity_taggers = cls.train_model(dataset, config, crf_order)",
      "438:                 entity_extractor = cls(config, model_storage, resource, entity_taggers)",
      "439:                 entity_extractor.crf_order = crf_order",
      "440:                 return entity_extractor",
      "",
      "---------------",
      "--- Hunk 11 ---",
      "[Context before]",
      "417:             )",
      "418:             return cls(config, model_storage, resource)",
      "421:         \"\"\"Persist this model into the passed directory.\"\"\"",
      "424:         with self._model_storage.write_to(self._resource) as model_dir:",
      "432:     def _crf_tokens_to_features(",
      "434:     ) -> List[Dict[Text, Any]]:",
      "435:         \"\"\"Convert the list of tokens into discrete features.\"\"\"",
      "437:         sentence_features = []",
      "439:         for token_idx in range(len(crf_tokens)):",
      "",
      "[Removed Lines]",
      "420:     def persist(self) -> None:",
      "422:         import joblib",
      "425:             if self.entity_taggers:",
      "426:                 for idx, (name, entity_tagger) in enumerate(",
      "427:                     self.entity_taggers.items()",
      "428:                 ):",
      "429:                     model_file_name = model_dir / f\"{idx}{name}.pkl\"",
      "430:                     joblib.dump(entity_tagger, model_file_name)",
      "433:         self, crf_tokens: List[CRFToken], include_tag_features: bool = False",
      "436:         configured_features = self.component_config[self.CONFIG_FEATURES]",
      "",
      "[Added Lines]",
      "448:     def persist(self, dataset: List[List[CRFToken]]) -> None:",
      "451:             data_to_store = [",
      "452:                 [token.to_dict() for token in sub_list] for sub_list in dataset",
      "453:             ]",
      "455:             rasa.shared.utils.io.dump_obj_as_json_to_file(",
      "456:                 model_dir / \"crf_dataset.json\", data_to_store",
      "457:             )",
      "458:             rasa.shared.utils.io.dump_obj_as_json_to_file(",
      "459:                 model_dir / \"crf_order.json\", self.crf_order",
      "460:             )",
      "462:     @classmethod",
      "464:         cls,",
      "465:         crf_tokens: List[CRFToken],",
      "466:         config: Dict[str, Any],",
      "467:         include_tag_features: bool = False,",
      "470:         configured_features = config[CONFIG_FEATURES]",
      "",
      "---------------",
      "--- Hunk 12 ---",
      "[Context before]",
      "444:             half_window_size = window_size // 2",
      "445:             window_range = range(-half_window_size, half_window_size + 1)",
      "448:                 crf_tokens,",
      "449:                 token_idx,",
      "450:                 half_window_size,",
      "451:                 window_range,",
      "452:                 include_tag_features,",
      "453:             )",
      "455:             sentence_features.append(token_features)",
      "457:         return sentence_features",
      "459:     def _create_features_for_token(",
      "461:         crf_tokens: List[CRFToken],",
      "462:         token_idx: int,",
      "463:         half_window_size: int,",
      "464:         window_range: range,",
      "465:         include_tag_features: bool,",
      "466:     ) -> Dict[Text, Any]:",
      "467:         \"\"\"Convert a token into discrete features including words before and after.\"\"\"",
      "469:         prefixes = [str(i) for i in window_range]",
      "471:         token_features = {}",
      "",
      "[Removed Lines]",
      "447:             token_features = self._create_features_for_token(",
      "460:         self,",
      "468:         configured_features = self.component_config[self.CONFIG_FEATURES]",
      "",
      "[Added Lines]",
      "481:             token_features = cls._create_features_for_token(",
      "487:                 config,",
      "494:     @classmethod",
      "496:         cls,",
      "502:         config: Dict[str, Any],",
      "505:         configured_features = config[CONFIG_FEATURES]",
      "",
      "---------------",
      "--- Hunk 13 ---",
      "[Context before]",
      "505:                         # set in the training data, 'matched' is either 'True' or",
      "506:                         # 'False' depending on whether the token actually matches the",
      "507:                         # pattern or not",
      "509:                         for pattern_name, matched in regex_patterns.items():",
      "510:                             token_features[",
      "511:                                 f\"{prefix}:{feature}:{pattern_name}\"",
      "512:                             ] = matched",
      "513:                     else:",
      "515:                         token_features[f\"{prefix}:{feature}\"] = value",
      "517:         return token_features",
      "",
      "[Removed Lines]",
      "508:                         regex_patterns = self.function_dict[feature](token)",
      "514:                         value = self.function_dict[feature](token)",
      "",
      "[Added Lines]",
      "545:                         regex_patterns = cls.function_dict[feature](token)",
      "551:                         value = cls.function_dict[feature](token)",
      "",
      "---------------",
      "--- Hunk 14 ---",
      "[Context before]",
      "636:         return tags",
      "639:         \"\"\"Train the crf tagger based on the training data.\"\"\"",
      "640:         import sklearn_crfsuite",
      "645:             logger.debug(f\"Training CRF for '{tag_name}'.\")",
      "647:             # add entity tag features for second level CRFs",
      "648:             include_tag_features = tag_name != ENTITY_ATTRIBUTE_TYPE",
      "649:             X_train = (",
      "651:                 for sentence in df_train",
      "652:             )",
      "653:             y_train = (",
      "655:             )",
      "657:             entity_tagger = sklearn_crfsuite.CRF(",
      "658:                 algorithm=\"lbfgs\",",
      "659:                 # coefficient for L1 penalty",
      "661:                 # coefficient for L2 penalty",
      "663:                 # stop earlier",
      "665:                 # include transitions that are possible, but not observed",
      "666:                 all_possible_transitions=True,",
      "667:             )",
      "668:             entity_tagger.fit(X_train, y_train)",
      "672:             logger.debug(\"Training finished.\")",
      "",
      "[Removed Lines]",
      "638:     def _train_model(self, df_train: List[List[CRFToken]]) -> None:",
      "642:         self.entity_taggers = OrderedDict()",
      "644:         for tag_name in self.crf_order:",
      "650:                 self._crf_tokens_to_features(sentence, include_tag_features)",
      "654:                 self._crf_tokens_to_tags(sentence, tag_name) for sentence in df_train",
      "660:                 c1=self.component_config[\"L1_c\"],",
      "662:                 c2=self.component_config[\"L2_c\"],",
      "664:                 max_iterations=self.component_config[\"max_iterations\"],",
      "670:             self.entity_taggers[tag_name] = entity_tagger",
      "",
      "[Added Lines]",
      "675:     @classmethod",
      "676:     def train_model(",
      "677:         cls,",
      "678:         df_train: List[List[CRFToken]],",
      "679:         config: Dict[str, Any],",
      "680:         crf_order: List[str],",
      "681:     ) -> OrderedDict[str, CRF]:",
      "685:         entity_taggers = OrderedDict()",
      "687:         for tag_name in crf_order:",
      "693:                 cls._crf_tokens_to_features(sentence, config, include_tag_features)",
      "697:                 cls._crf_tokens_to_tags(sentence, tag_name) for sentence in df_train",
      "703:                 c1=config[\"L1_c\"],",
      "705:                 c2=config[\"L2_c\"],",
      "707:                 max_iterations=config[\"max_iterations\"],",
      "713:             entity_taggers[tag_name] = entity_tagger",
      "717:         return entity_taggers",
      "",
      "---------------"
    ],
    "rasa/nlu/featurizers/sparse_featurizer/count_vectors_featurizer.py||rasa/nlu/featurizers/sparse_featurizer/count_vectors_featurizer.py": [
      "File: rasa/nlu/featurizers/sparse_featurizer/count_vectors_featurizer.py -> rasa/nlu/featurizers/sparse_featurizer/count_vectors_featurizer.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "1: from __future__ import annotations",
      "2: import logging",
      "3: import re",
      "4: import scipy.sparse",
      "8: import rasa.shared.utils.io",
      "9: from rasa.engine.graph import GraphComponent, ExecutionContext",
      "10: from rasa.engine.recipes.default_recipe import DefaultV1Recipe",
      "11: from rasa.engine.storage.resource import Resource",
      "12: from rasa.engine.storage.storage import ModelStorage",
      "21: from rasa.nlu.constants import (",
      "22:     TOKENS_NAMES,",
      "23:     MESSAGE_ATTRIBUTES,",
      "24:     DENSE_FEATURIZABLE_ATTRIBUTES,",
      "25: )",
      "26: from rasa.shared.nlu.constants import TEXT, INTENT, INTENT_RESPONSE_KEY, ACTION_NAME",
      "28: BUFFER_SLOTS_PREFIX = \"buf_\"",
      "",
      "[Removed Lines]",
      "5: from typing import Any, Dict, List, Optional, Text, Tuple, Set, Type",
      "6: from rasa.nlu.tokenizers.tokenizer import Tokenizer",
      "13: from rasa.nlu.featurizers.sparse_featurizer.sparse_featurizer import SparseFeaturizer",
      "14: from rasa.nlu.utils.spacy_utils import SpacyModel",
      "15: from rasa.shared.constants import DOCS_URL_COMPONENTS",
      "16: import rasa.utils.io as io_utils",
      "17: from sklearn.feature_extraction.text import CountVectorizer",
      "18: from rasa.shared.nlu.training_data.training_data import TrainingData",
      "19: from rasa.shared.nlu.training_data.message import Message",
      "20: from rasa.shared.exceptions import RasaException, FileIOException",
      "",
      "[Added Lines]",
      "5: from typing import Any, Dict, List, Optional, Text, Tuple, Set, Type, Union",
      "7: import numpy as np",
      "9: from sklearn.feature_extraction.text import CountVectorizer",
      "21: from rasa.nlu.featurizers.sparse_featurizer.sparse_featurizer import SparseFeaturizer",
      "22: from rasa.nlu.tokenizers.tokenizer import Tokenizer",
      "23: from rasa.nlu.utils.spacy_utils import SpacyModel",
      "24: from rasa.shared.constants import DOCS_URL_COMPONENTS",
      "25: from rasa.shared.exceptions import RasaException, FileIOException",
      "27: from rasa.shared.nlu.training_data.message import Message",
      "28: from rasa.shared.nlu.training_data.training_data import TrainingData",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "686:         \"\"\"Check if any model got trained.\"\"\"",
      "687:         return any(value is not None for value in attribute_vocabularies.values())",
      "689:     def persist(self) -> None:",
      "690:         \"\"\"Persist this model into the passed directory.",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "691:     @staticmethod",
      "692:     def convert_vocab(",
      "693:         vocab: Dict[str, Union[int, Optional[Dict[str, int]]]], to_int: bool",
      "694:     ) -> Dict[str, Union[None, int, np.int64, Dict[str, Union[int, np.int64]]]]:",
      "695:         \"\"\"Converts numpy integers in the vocabulary to Python integers.\"\"\"",
      "697:         def convert_value(value: int) -> Union[int, np.int64]:",
      "698:             \"\"\"Helper function to convert a single value based on to_int flag.\"\"\"",
      "699:             return int(value) if to_int else np.int64(value)",
      "701:         result_dict: Dict[",
      "702:             str, Union[None, int, np.int64, Dict[str, Union[int, np.int64]]]",
      "703:         ] = {}",
      "704:         for key, sub_dict in vocab.items():",
      "705:             if isinstance(sub_dict, int):",
      "706:                 result_dict[key] = convert_value(sub_dict)",
      "707:             elif not sub_dict:",
      "708:                 result_dict[key] = None",
      "709:             else:",
      "710:                 result_dict[key] = {",
      "711:                     sub_key: convert_value(value) for sub_key, value in sub_dict.items()",
      "712:                 }",
      "714:         return result_dict",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "699:             attribute_vocabularies = self._collect_vectorizer_vocabularies()",
      "700:             if self._is_any_model_trained(attribute_vocabularies):",
      "701:                 # Definitely need to persist some vocabularies",
      "704:                 # Only persist vocabulary from one attribute if `use_shared_vocab`.",
      "705:                 # Can be loaded and distributed to all attributes.",
      "707:                     attribute_vocabularies[TEXT]",
      "708:                     if self.use_shared_vocab",
      "709:                     else attribute_vocabularies",
      "710:                 )",
      "714:                 # Dump OOV words separately as they might have been modified during",
      "715:                 # training",
      "",
      "[Removed Lines]",
      "702:                 featurizer_file = model_dir / \"vocabularies.pkl\"",
      "706:                 vocab = (",
      "712:                 io_utils.json_pickle(featurizer_file, vocab)",
      "",
      "[Added Lines]",
      "729:                 featurizer_file = model_dir / \"vocabularies.json\"",
      "733:                 loaded_vocab = (",
      "738:                 vocab = self.convert_vocab(loaded_vocab, to_int=True)",
      "740:                 rasa.shared.utils.io.dump_obj_as_json_to_file(featurizer_file, vocab)",
      "",
      "---------------",
      "--- Hunk 4 ---",
      "[Context before]",
      "784:         \"\"\"Loads trained component (see parent class for full docstring).\"\"\"",
      "785:         try:",
      "786:             with model_storage.read_from(resource) as model_dir:",
      "790:                 share_vocabulary = config[\"use_shared_vocab\"]",
      "",
      "[Removed Lines]",
      "787:                 featurizer_file = model_dir / \"vocabularies.pkl\"",
      "788:                 vocabulary = io_utils.json_unpickle(featurizer_file)",
      "",
      "[Added Lines]",
      "815:                 featurizer_file = model_dir / \"vocabularies.json\"",
      "816:                 vocabulary = rasa.shared.utils.io.read_json_file(featurizer_file)",
      "817:                 vocabulary = cls.convert_vocab(vocabulary, to_int=False)",
      "",
      "---------------"
    ],
    "rasa/nlu/featurizers/sparse_featurizer/lexical_syntactic_featurizer.py||rasa/nlu/featurizers/sparse_featurizer/lexical_syntactic_featurizer.py": [
      "File: rasa/nlu/featurizers/sparse_featurizer/lexical_syntactic_featurizer.py -> rasa/nlu/featurizers/sparse_featurizer/lexical_syntactic_featurizer.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "1: from __future__ import annotations",
      "2: import logging",
      "3: from collections import OrderedDict",
      "7: from typing import (",
      "8:     Any,",
      "9:     Dict,",
      "",
      "[Removed Lines]",
      "5: import scipy.sparse",
      "6: import numpy as np",
      "",
      "[Added Lines]",
      "[None]",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "17:     Union,",
      "18: )",
      "20: from rasa.engine.graph import ExecutionContext, GraphComponent",
      "21: from rasa.engine.recipes.default_recipe import DefaultV1Recipe",
      "22: from rasa.engine.storage.resource import Resource",
      "23: from rasa.engine.storage.storage import ModelStorage",
      "24: from rasa.nlu.tokenizers.spacy_tokenizer import POS_TAG_KEY, SpacyTokenizer",
      "25: from rasa.nlu.tokenizers.tokenizer import Token, Tokenizer",
      "28: from rasa.shared.constants import DOCS_URL_COMPONENTS",
      "32: from rasa.shared.exceptions import InvalidConfigException",
      "36: logger = logging.getLogger(__name__)",
      "39: END_OF_SENTENCE = \"EOS\"",
      "40: BEGIN_OF_SENTENCE = \"BOS\"",
      "42: FEATURES = \"features\"",
      "45: @DefaultV1Recipe.register(",
      "46:     DefaultV1Recipe.ComponentType.MESSAGE_FEATURIZER, is_trainable=True",
      "",
      "[Removed Lines]",
      "26: from rasa.nlu.featurizers.sparse_featurizer.sparse_featurizer import SparseFeaturizer",
      "27: from rasa.nlu.constants import TOKENS_NAMES",
      "29: from rasa.shared.nlu.training_data.training_data import TrainingData",
      "30: from rasa.shared.nlu.training_data.message import Message",
      "31: from rasa.shared.nlu.constants import TEXT",
      "33: import rasa.shared.utils.io",
      "34: import rasa.utils.io",
      "",
      "[Added Lines]",
      "18: import numpy as np",
      "19: import scipy.sparse",
      "21: import rasa.shared.utils.io",
      "22: import rasa.utils.io",
      "27: from rasa.nlu.constants import TOKENS_NAMES",
      "28: from rasa.nlu.featurizers.sparse_featurizer.sparse_featurizer import SparseFeaturizer",
      "33: from rasa.shared.nlu.constants import TEXT",
      "34: from rasa.shared.nlu.training_data.message import Message",
      "35: from rasa.shared.nlu.training_data.training_data import TrainingData",
      "44: SEPERATOR = \"###\"",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "72:       of the token at position `t+1`.",
      "73:     \"\"\"",
      "77:     # NOTE: \"suffix5\" of the token \"is\" will be \"is\". Hence, when combining multiple",
      "78:     # prefixes, short words will be represented/encoded repeatedly.",
      "",
      "[Removed Lines]",
      "75:     FILENAME_FEATURE_TO_IDX_DICT = \"feature_to_idx_dict.pkl\"",
      "",
      "[Added Lines]",
      "77:     FILENAME_FEATURE_TO_IDX_DICT = \"feature_to_idx_dict.json\"",
      "",
      "---------------",
      "--- Hunk 4 ---",
      "[Context before]",
      "489:         \"\"\"Creates a new untrained component (see parent class for full docstring).\"\"\"",
      "490:         return cls(config, model_storage, resource, execution_context)",
      "492:     @classmethod",
      "493:     def load(",
      "494:         cls,",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "494:     @staticmethod",
      "495:     def _restructure_feature_to_idx_dict(",
      "496:         loaded_data: Dict[str, Dict[str, int]],",
      "497:     ) -> Dict[Tuple[int, str], Dict[str, int]]:",
      "498:         \"\"\"Reconstructs the feature to idx dict.",
      "500:         When storing the feature_to_idx_dict to disk, we need to convert the tuple (key)",
      "501:         into a string to be able to store it via json. When loading the data",
      "502:         we need to reconstruct the tuple from the stored string.",
      "504:         Args:",
      "505:             loaded_data: The loaded feature to idx dict from file.",
      "507:         Returns:",
      "508:             The reconstructed feature_to_idx_dict",
      "509:         \"\"\"",
      "510:         feature_to_idx_dict = {}",
      "511:         for tuple_string, feature_value in loaded_data.items():",
      "512:             # Example of tuple_string: \"1###low\"",
      "513:             index, feature_name = tuple_string.split(SEPERATOR)",
      "515:             feature_key = (int(index), feature_name)",
      "516:             feature_to_idx_dict[feature_key] = feature_value",
      "518:         return feature_to_idx_dict",
      "",
      "---------------",
      "--- Hunk 5 ---",
      "[Context before]",
      "501:         \"\"\"Loads trained component (see parent class for full docstring).\"\"\"",
      "502:         try:",
      "503:             with model_storage.read_from(resource) as model_path:",
      "505:                     model_path / cls.FILENAME_FEATURE_TO_IDX_DICT,",
      "507:                 )",
      "508:                 return cls(",
      "509:                     config=config,",
      "510:                     model_storage=model_storage,",
      "",
      "[Removed Lines]",
      "504:                 feature_to_idx_dict = rasa.utils.io.json_unpickle(",
      "506:                     encode_non_string_keys=True,",
      "",
      "[Added Lines]",
      "532:                 loaded_data = rasa.shared.utils.io.read_json_file(",
      "536:                 # convert the key back into tuple",
      "537:                 feature_to_idx_dict = cls._restructure_feature_to_idx_dict(loaded_data)",
      "",
      "---------------",
      "--- Hunk 6 ---",
      "[Context before]",
      "529:         if not self._feature_to_idx_dict:",
      "530:             return None",
      "532:         with self._model_storage.write_to(self._resource) as model_path:",
      "534:                 model_path / self.FILENAME_FEATURE_TO_IDX_DICT,",
      "537:             )",
      "",
      "[Removed Lines]",
      "533:             rasa.utils.io.json_pickle(",
      "535:                 self._feature_to_idx_dict,",
      "536:                 encode_non_string_keys=True,",
      "",
      "[Added Lines]",
      "563:         # as we cannot dump tuples, convert the tuple into a string",
      "564:         restructured_feature_dict = {",
      "565:             f\"{k[0]}{SEPERATOR}{k[1]}\": v for k, v in self._feature_to_idx_dict.items()",
      "566:         }",
      "569:             rasa.shared.utils.io.dump_obj_as_json_to_file(",
      "571:                 restructured_feature_dict,",
      "",
      "---------------"
    ],
    "rasa/nlu/featurizers/sparse_featurizer/regex_featurizer.py||rasa/nlu/featurizers/sparse_featurizer/regex_featurizer.py": [
      "File: rasa/nlu/featurizers/sparse_featurizer/regex_featurizer.py -> rasa/nlu/featurizers/sparse_featurizer/regex_featurizer.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "1: from __future__ import annotations",
      "2: import logging",
      "3: import re",
      "4: from typing import Any, Dict, List, Optional, Text, Tuple, Type",
      "5: import numpy as np",
      "6: import scipy.sparse",
      "9: import rasa.shared.utils.io",
      "10: import rasa.utils.io",
      "11: import rasa.nlu.utils.pattern_utils as pattern_utils",
      "",
      "[Removed Lines]",
      "7: from rasa.nlu.tokenizers.tokenizer import Tokenizer",
      "",
      "[Added Lines]",
      "10: from rasa.nlu.tokenizers.tokenizer import Tokenizer",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "241:         try:",
      "242:             with model_storage.read_from(resource) as model_dir:",
      "244:                 known_patterns = rasa.shared.utils.io.read_json_file(patterns_file_name)",
      "245:         except (ValueError, FileNotFoundError):",
      "246:             logger.warning(",
      "",
      "[Removed Lines]",
      "243:                 patterns_file_name = model_dir / \"patterns.pkl\"",
      "",
      "[Added Lines]",
      "245:                 patterns_file_name = model_dir / \"patterns.json\"",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "259:     def _persist(self) -> None:",
      "260:         with self._model_storage.write_to(self._resource) as model_dir:",
      "262:             rasa.shared.utils.io.dump_obj_as_json_to_file(",
      "263:                 regex_file, self.known_patterns",
      "264:             )",
      "",
      "[Removed Lines]",
      "261:             regex_file = model_dir / \"patterns.pkl\"",
      "",
      "[Added Lines]",
      "263:             regex_file = model_dir / \"patterns.json\"",
      "",
      "---------------"
    ],
    "rasa/shared/nlu/training_data/features.py||rasa/shared/nlu/training_data/features.py": [
      "File: rasa/shared/nlu/training_data/features.py -> rasa/shared/nlu/training_data/features.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "1: from __future__ import annotations",
      "3: import itertools",
      "5: import numpy as np",
      "6: import scipy.sparse",
      "9: import rasa.shared.nlu.training_data.util",
      "10: from rasa.shared.nlu.constants import FEATURE_TYPE_SEQUENCE, FEATURE_TYPE_SENTENCE",
      "13: class Features:",
      "14:     \"\"\"Stores the features produced by any featurizer.\"\"\"",
      "",
      "[Removed Lines]",
      "2: from typing import Iterable, Union, Text, Optional, List, Any, Tuple, Dict, Set",
      "8: import rasa.shared.utils.io",
      "",
      "[Added Lines]",
      "4: from dataclasses import dataclass",
      "5: from typing import Iterable, Union, Text, Optional, List, Any, Tuple, Dict, Set",
      "9: from safetensors.numpy import save_file, load_file",
      "12: import rasa.shared.utils.io",
      "16: @dataclass",
      "17: class FeatureMetadata:",
      "18:     data_type: str",
      "19:     attribute: str",
      "20:     origin: Union[str, List[str]]",
      "21:     is_sparse: bool",
      "22:     shape: tuple",
      "23:     safetensors_key: str",
      "26: def save_features(",
      "27:     features_dict: Dict[Text, List[Features]], file_name: str",
      "28: ) -> Dict[str, Any]:",
      "29:     \"\"\"Save a dictionary of Features lists to disk using safetensors.",
      "31:     Args:",
      "32:         features_dict: Dictionary mapping strings to lists of Features objects",
      "33:         file_name: File to save the features to",
      "35:     Returns:",
      "36:         The metadata to reconstruct the features.",
      "37:     \"\"\"",
      "38:     # All tensors are stored in a single safetensors file",
      "39:     tensors_to_save = {}",
      "40:     # Metadata will be stored separately",
      "41:     metadata = {}",
      "43:     for key, features_list in features_dict.items():",
      "44:         feature_metadata_list = []",
      "46:         for idx, feature in enumerate(features_list):",
      "47:             # Create a unique key for this tensor in the safetensors file",
      "48:             safetensors_key = f\"{key}_{idx}\"",
      "50:             # Convert sparse matrices to dense if needed",
      "51:             if feature.is_sparse():",
      "52:                 # For sparse matrices, use the COO format",
      "53:                 coo = feature.features.tocoo()  # type:ignore[union-attr]",
      "54:                 # Save data, row indices and col indices separately",
      "55:                 tensors_to_save[f\"{safetensors_key}_data\"] = coo.data",
      "56:                 tensors_to_save[f\"{safetensors_key}_row\"] = coo.row",
      "57:                 tensors_to_save[f\"{safetensors_key}_col\"] = coo.col",
      "58:             else:",
      "59:                 tensors_to_save[safetensors_key] = feature.features",
      "61:             # Store metadata",
      "62:             metadata_item = FeatureMetadata(",
      "63:                 data_type=feature.type,",
      "64:                 attribute=feature.attribute,",
      "65:                 origin=feature.origin,",
      "66:                 is_sparse=feature.is_sparse(),",
      "67:                 shape=feature.features.shape,",
      "68:                 safetensors_key=safetensors_key,",
      "69:             )",
      "70:             feature_metadata_list.append(vars(metadata_item))",
      "72:         metadata[key] = feature_metadata_list",
      "74:     # Save tensors",
      "75:     save_file(tensors_to_save, file_name)",
      "77:     return metadata",
      "80: def load_features(",
      "81:     filename: str, metadata: Dict[str, Any]",
      "82: ) -> Dict[Text, List[Features]]:",
      "83:     \"\"\"Load Features dictionary from disk.",
      "85:     Args:",
      "86:         filename: File name of the safetensors file.",
      "87:         metadata: Metadata to reconstruct the features.",
      "89:     Returns:",
      "90:         Dictionary mapping strings to lists of Features objects",
      "91:     \"\"\"",
      "92:     # Load tensors",
      "93:     tensors = load_file(filename)",
      "95:     # Reconstruct the features dictionary",
      "96:     features_dict: Dict[Text, List[Features]] = {}",
      "98:     for key, feature_metadata_list in metadata.items():",
      "99:         features_list = []",
      "101:         for meta in feature_metadata_list:",
      "102:             safetensors_key = meta[\"safetensors_key\"]",
      "104:             if meta[\"is_sparse\"]:",
      "105:                 # Reconstruct sparse matrix from COO format",
      "106:                 data = tensors[f\"{safetensors_key}_data\"]",
      "107:                 row = tensors[f\"{safetensors_key}_row\"]",
      "108:                 col = tensors[f\"{safetensors_key}_col\"]",
      "110:                 features_matrix = scipy.sparse.coo_matrix(",
      "111:                     (data, (row, col)), shape=tuple(meta[\"shape\"])",
      "112:                 ).tocsr()  # Convert back to CSR format",
      "113:             else:",
      "114:                 features_matrix = tensors[safetensors_key]",
      "116:             # Reconstruct Features object",
      "117:             features = Features(",
      "118:                 features=features_matrix,",
      "119:                 feature_type=meta[\"data_type\"],",
      "120:                 attribute=meta[\"attribute\"],",
      "121:                 origin=meta[\"origin\"],",
      "122:             )",
      "124:             features_list.append(features)",
      "126:         features_dict[key] = features_list",
      "128:     return features_dict",
      "",
      "---------------"
    ],
    "rasa/shared/utils/io.py||rasa/shared/utils/io.py": [
      "File: rasa/shared/utils/io.py -> rasa/shared/utils/io.py"
    ],
    "rasa/utils/common.py||rasa/utils/common.py": [
      "File: rasa/utils/common.py -> rasa/utils/common.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "8: import tempfile",
      "9: import warnings",
      "10: from pathlib import Path",
      "11: from types import TracebackType",
      "12: from typing import (",
      "13:     Any,",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "11: from socket import SOCK_DGRAM, SOCK_STREAM",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "24:     Tuple,",
      "25: )",
      "28: import numpy as np",
      "29: import rasa.utils.io",
      "30: from rasa.constants import (",
      "31:     DEFAULT_LOG_LEVEL_LIBRARIES,",
      "",
      "[Removed Lines]",
      "27: from socket import SOCK_DGRAM, SOCK_STREAM",
      "",
      "[Added Lines]",
      "30: import rasa.shared.utils.io",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "36: )",
      "37: from rasa.shared.constants import DEFAULT_LOG_LEVEL, ENV_LOG_LEVEL, TCP_PROTOCOL",
      "38: from rasa.shared.exceptions import RasaException",
      "41: logger = logging.getLogger(__name__)",
      "",
      "[Removed Lines]",
      "39: import rasa.shared.utils.io",
      "",
      "[Added Lines]",
      "[None]",
      "",
      "---------------",
      "--- Hunk 4 ---",
      "[Context before]",
      "153:     try:",
      "154:         logging.config.dictConfig(logging_config_dict)",
      "155:     except (ValueError, TypeError, AttributeError, ImportError) as e:",
      "157:             f\"The logging config file {logging_config_file} could not \"",
      "158:             f\"be applied because it failed validation against \"",
      "159:             f\"the built-in Python logging schema. \"",
      "",
      "[Removed Lines]",
      "156:         logging.debug(",
      "",
      "[Added Lines]",
      "157:         logger.debug(",
      "",
      "---------------"
    ],
    "rasa/utils/io.py||rasa/utils/io.py": [
      "File: rasa/utils/io.py -> rasa/utils/io.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "2: import filecmp",
      "3: import logging",
      "4: import os",
      "6: import tempfile",
      "7: import warnings",
      "9: from asyncio import AbstractEventLoop",
      "10: from pathlib import Path",
      "12: from typing_extensions import Protocol",
      "14: import rasa.shared.constants",
      "",
      "[Removed Lines]",
      "5: import pickle",
      "8: import re",
      "11: from typing import Text, Any, Union, List, Type, Callable, TYPE_CHECKING, Pattern",
      "",
      "[Added Lines]",
      "5: import re",
      "10: from typing import Text, Any, List, Type, Callable, TYPE_CHECKING, Pattern",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "81:     return event_loop",
      "107: def create_temporary_file(data: Any, suffix: Text = \"\", mode: Text = \"w+\") -> Text:",
      "108:     \"\"\"Creates a tempfile.NamedTemporaryFile object for data.\"\"\"",
      "109:     encoding = None if \"b\" in mode else rasa.shared.utils.io.DEFAULT_ENCODING",
      "",
      "[Removed Lines]",
      "84: def pickle_dump(filename: Union[Text, Path], obj: Any) -> None:",
      "85:     \"\"\"Saves object to file.",
      "87:     Args:",
      "88:         filename: the filename to save the object to",
      "89:         obj: the object to store",
      "90:     \"\"\"",
      "91:     with open(filename, \"wb\") as f:",
      "92:         pickle.dump(obj, f)",
      "95: def pickle_load(filename: Union[Text, Path]) -> Any:",
      "96:     \"\"\"Loads an object from a file.",
      "98:     Args:",
      "99:         filename: the filename to load the object from",
      "101:     Returns: the loaded object",
      "102:     \"\"\"",
      "103:     with open(filename, \"rb\") as f:",
      "104:         return pickle.load(f)",
      "",
      "[Added Lines]",
      "[None]",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "160:     function: Callable[[Text], bool], error_message: Text",
      "161: ) -> Type[\"Validator\"]:",
      "162:     \"\"\"Helper method to create `Validator` classes from callable functions. Should be",
      "165:     from prompt_toolkit.validation import Validator, ValidationError",
      "166:     from prompt_toolkit.document import Document",
      "",
      "[Removed Lines]",
      "163:     removed when questionary supports `Validator` objects.\"\"\"",
      "",
      "[Added Lines]",
      "139:     removed when questionary supports `Validator` objects.",
      "140:     \"\"\"",
      "",
      "---------------",
      "--- Hunk 4 ---",
      "[Context before]",
      "175:     return FunctionValidator",
      "220: def get_emoji_regex() -> Pattern:",
      "221:     \"\"\"Returns regex to identify emojis.\"\"\"",
      "222:     return re.compile(",
      "",
      "[Removed Lines]",
      "178: def json_unpickle(",
      "179:     file_name: Union[Text, Path], encode_non_string_keys: bool = False",
      "180: ) -> Any:",
      "181:     \"\"\"Unpickle an object from file using json.",
      "183:     Args:",
      "184:         file_name: the file to load the object from",
      "185:         encode_non_string_keys: If set to `True` then jsonpickle will encode non-string",
      "186:           dictionary keys instead of coercing them into strings via `repr()`.",
      "188:     Returns: the object",
      "189:     \"\"\"",
      "190:     import jsonpickle.ext.numpy as jsonpickle_numpy",
      "191:     import jsonpickle",
      "193:     jsonpickle_numpy.register_handlers()",
      "195:     file_content = rasa.shared.utils.io.read_file(file_name)",
      "196:     return jsonpickle.loads(file_content, keys=encode_non_string_keys)",
      "199: def json_pickle(",
      "200:     file_name: Union[Text, Path], obj: Any, encode_non_string_keys: bool = False",
      "201: ) -> None:",
      "202:     \"\"\"Pickle an object to a file using json.",
      "204:     Args:",
      "205:         file_name: the file to store the object to",
      "206:         obj: the object to store",
      "207:         encode_non_string_keys: If set to `True` then jsonpickle will encode non-string",
      "208:           dictionary keys instead of coercing them into strings via `repr()`.",
      "209:     \"\"\"",
      "210:     import jsonpickle.ext.numpy as jsonpickle_numpy",
      "211:     import jsonpickle",
      "213:     jsonpickle_numpy.register_handlers()",
      "215:     rasa.shared.utils.io.write_text_file(",
      "216:         jsonpickle.dumps(obj, keys=encode_non_string_keys), file_name",
      "217:     )",
      "",
      "[Added Lines]",
      "[None]",
      "",
      "---------------"
    ],
    "rasa/utils/tensorflow/feature_array.py||rasa/utils/tensorflow/feature_array.py": [
      "File: rasa/utils/tensorflow/feature_array.py -> rasa/utils/tensorflow/feature_array.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "[No context available]",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "1: from typing import Dict, Any, List, Tuple, Optional, Union",
      "3: import numpy as np",
      "4: import scipy.sparse",
      "5: from safetensors.numpy import load_file",
      "6: from safetensors.numpy import save_file",
      "8: import rasa.shared.utils.io",
      "11: def _recursive_serialize(",
      "12:     array: Any, prefix: str, data_dict: Dict[str, Any], metadata: List[Dict[str, Any]]",
      "13: ) -> None:",
      "14:     \"\"\"Recursively serialize arrays and matrices for high dimensional data.\"\"\"",
      "15:     if isinstance(array, np.ndarray) and array.ndim <= 2:",
      "16:         data_key = f\"{prefix}_array\"",
      "17:         data_dict[data_key] = array",
      "18:         metadata.append({\"type\": \"dense\", \"key\": data_key, \"shape\": array.shape})",
      "20:     elif isinstance(array, list) and all([isinstance(v, float) for v in array]):",
      "21:         data_key = f\"{prefix}_list\"",
      "22:         data_dict[data_key] = np.array(array, dtype=np.float32)",
      "23:         metadata.append({\"type\": \"list\", \"key\": data_key})",
      "25:     elif isinstance(array, list) and all([isinstance(v, int) for v in array]):",
      "26:         data_key = f\"{prefix}_list\"",
      "27:         data_dict[data_key] = np.array(array, dtype=np.int64)",
      "28:         metadata.append({\"type\": \"list\", \"key\": data_key})",
      "30:     elif isinstance(array, scipy.sparse.spmatrix):",
      "31:         data_key_data = f\"{prefix}_data\"",
      "32:         data_key_row = f\"{prefix}_row\"",
      "33:         data_key_col = f\"{prefix}_col\"",
      "34:         array = array.tocoo()",
      "35:         data_dict.update(",
      "36:             {",
      "37:                 data_key_data: array.data,",
      "38:                 data_key_row: array.row,",
      "39:                 data_key_col: array.col,",
      "40:             }",
      "41:         )",
      "42:         metadata.append({\"type\": \"sparse\", \"key\": prefix, \"shape\": array.shape})",
      "44:     elif isinstance(array, list) or isinstance(array, np.ndarray):",
      "45:         group_metadata = {\"type\": \"group\", \"subcomponents\": []}",
      "46:         for idx, item in enumerate(array):",
      "47:             new_prefix = f\"{prefix}_{idx}\"",
      "48:             _recursive_serialize(",
      "49:                 item, new_prefix, data_dict, group_metadata[\"subcomponents\"]",
      "50:             )",
      "51:         metadata.append(group_metadata)",
      "54: def _serialize_nested_data(",
      "55:     nested_data: Dict[str, Dict[str, List[\"FeatureArray\"]]],",
      "56:     prefix: str,",
      "57:     data_dict: Dict[str, np.ndarray],",
      "58:     metadata: List[Dict[str, Union[str, List]]],",
      "59: ) -> None:",
      "60:     \"\"\"Handle serialization across dictionary and list levels.\"\"\"",
      "61:     for outer_key, inner_dict in nested_data.items():",
      "62:         inner_metadata = {\"key\": outer_key, \"components\": []}",
      "64:         for inner_key, feature_arrays in inner_dict.items():",
      "65:             array_metadata = {",
      "66:                 \"key\": inner_key,",
      "67:                 \"number_of_dimensions\": feature_arrays[0].number_of_dimensions,",
      "68:                 \"features\": [],",
      "69:             }",
      "71:             for idx, feature_array in enumerate(feature_arrays):",
      "72:                 feature_prefix = f\"{prefix}_{outer_key}_{inner_key}_{idx}\"",
      "73:                 _recursive_serialize(",
      "74:                     feature_array.tolist(),",
      "75:                     feature_prefix,",
      "76:                     data_dict,",
      "77:                     array_metadata[\"features\"],",
      "78:                 )",
      "80:             inner_metadata[\"components\"].append(  # type:ignore[attr-defined]",
      "81:                 array_metadata",
      "82:             )",
      "84:         metadata.append(inner_metadata)",
      "87: def serialize_nested_feature_arrays(",
      "88:     nested_feature_array: Dict[str, Dict[str, List[\"FeatureArray\"]]],",
      "89:     data_filename: str,",
      "90:     metadata_filename: str,",
      "91: ) -> None:",
      "92:     data_dict: Dict[str, np.ndarray] = {}",
      "93:     metadata: List[Dict[str, Union[str, List]]] = []",
      "95:     _serialize_nested_data(nested_feature_array, \"component\", data_dict, metadata)",
      "97:     # Save serialized data and metadata",
      "98:     save_file(data_dict, data_filename)",
      "99:     rasa.shared.utils.io.dump_obj_as_json_to_file(metadata_filename, metadata)",
      "102: def _recursive_deserialize(",
      "103:     metadata: List[Dict[str, Any]], data: Dict[str, Any]",
      "104: ) -> List[Any]:",
      "105:     \"\"\"Recursively deserialize arrays and matrices for high dimensional data.\"\"\"",
      "106:     result = []",
      "108:     for item in metadata:",
      "109:         if item[\"type\"] == \"dense\":",
      "110:             key = item[\"key\"]",
      "111:             array = np.asarray(data[key]).reshape(item[\"shape\"])",
      "112:             result.append(array)",
      "114:         elif item[\"type\"] == \"list\":",
      "115:             key = item[\"key\"]",
      "116:             result.append(list(data[key]))",
      "118:         elif item[\"type\"] == \"sparse\":",
      "119:             data_vals = data[f\"{item['key']}_data\"]",
      "120:             row_vals = data[f\"{item['key']}_row\"]",
      "121:             col_vals = data[f\"{item['key']}_col\"]",
      "122:             sparse_matrix = scipy.sparse.coo_matrix(",
      "123:                 (data_vals, (row_vals, col_vals)), shape=item[\"shape\"]",
      "124:             )",
      "125:             result.append(sparse_matrix)",
      "126:         elif item[\"type\"] == \"group\":",
      "127:             sublist = _recursive_deserialize(item[\"subcomponents\"], data)",
      "128:             result.append(sublist)",
      "130:     return result",
      "133: def _deserialize_nested_data(",
      "134:     metadata: List[Dict[str, Any]], data_dict: Dict[str, Any]",
      "135: ) -> Dict[str, Dict[str, List[\"FeatureArray\"]]]:",
      "136:     \"\"\"Handle deserialization across all dictionary and list levels.\"\"\"",
      "137:     result: Dict[str, Dict[str, List[\"FeatureArray\"]]] = {}",
      "139:     for outer_item in metadata:",
      "140:         outer_key = outer_item[\"key\"]",
      "141:         result[outer_key] = {}",
      "143:         for inner_item in outer_item[\"components\"]:",
      "144:             inner_key = inner_item[\"key\"]",
      "145:             feature_arrays = []",
      "147:             # Reconstruct the list of FeatureArrays",
      "148:             for feature_item in inner_item[\"features\"]:",
      "149:                 # Reconstruct the list of FeatureArrays",
      "150:                 feature_array_data = _recursive_deserialize([feature_item], data_dict)",
      "151:                 # Prepare the input for the FeatureArray;",
      "152:                 # ensure it is np.ndarray compatible",
      "153:                 input_array = np.array(feature_array_data[0], dtype=object)",
      "154:                 feature_array = FeatureArray(",
      "155:                     input_array, inner_item[\"number_of_dimensions\"]",
      "156:                 )",
      "157:                 feature_arrays.append(feature_array)",
      "159:             result[outer_key][inner_key] = feature_arrays",
      "161:     return result",
      "164: def deserialize_nested_feature_arrays(",
      "165:     data_filename: str, metadata_filename: str",
      "166: ) -> Dict[str, Dict[str, List[\"FeatureArray\"]]]:",
      "167:     metadata = rasa.shared.utils.io.read_json_file(metadata_filename)",
      "168:     data_dict = load_file(data_filename)",
      "170:     return _deserialize_nested_data(metadata, data_dict)",
      "173: class FeatureArray(np.ndarray):",
      "174:     \"\"\"Stores any kind of features ready to be used by a RasaModel.",
      "176:     Next to the input numpy array of features, it also received the number of",
      "177:     dimensions of the features.",
      "178:     As our features can have 1 to 4 dimensions we might have different number of numpy",
      "179:     arrays stacked. The number of dimensions helps us to figure out how to handle this",
      "180:     particular feature array. Also, it is automatically determined whether the feature",
      "181:     array is sparse or not and the number of units is determined as well.",
      "183:     Subclassing np.array: https://numpy.org/doc/stable/user/basics.subclassing.html",
      "184:     \"\"\"",
      "186:     def __new__(",
      "187:         cls, input_array: np.ndarray, number_of_dimensions: int",
      "188:     ) -> \"FeatureArray\":",
      "189:         \"\"\"Create and return a new object.  See help(type) for accurate signature.\"\"\"",
      "190:         FeatureArray._validate_number_of_dimensions(number_of_dimensions, input_array)",
      "192:         feature_array = np.asarray(input_array).view(cls)",
      "194:         if number_of_dimensions <= 2:",
      "195:             feature_array.units = input_array.shape[-1]",
      "196:             feature_array.is_sparse = isinstance(input_array[0], scipy.sparse.spmatrix)",
      "197:         elif number_of_dimensions == 3:",
      "198:             feature_array.units = input_array[0].shape[-1]",
      "199:             feature_array.is_sparse = isinstance(input_array[0], scipy.sparse.spmatrix)",
      "200:         elif number_of_dimensions == 4:",
      "201:             feature_array.units = input_array[0][0].shape[-1]",
      "202:             feature_array.is_sparse = isinstance(",
      "203:                 input_array[0][0], scipy.sparse.spmatrix",
      "204:             )",
      "205:         else:",
      "206:             raise ValueError(",
      "207:                 f\"Number of dimensions '{number_of_dimensions}' currently not \"",
      "208:                 f\"supported.\"",
      "209:             )",
      "211:         feature_array.number_of_dimensions = number_of_dimensions",
      "213:         return feature_array",
      "215:     def __init__(",
      "216:         self, input_array: Any, number_of_dimensions: int, **kwargs: Any",
      "217:     ) -> None:",
      "218:         \"\"\"Initialize. FeatureArray.",
      "220:         Needed in order to avoid 'Invalid keyword argument number_of_dimensions",
      "221:         to function FeatureArray.__init__ '",
      "222:         Args:",
      "223:             input_array: the array that contains features",
      "224:             number_of_dimensions: number of dimensions in input_array",
      "225:         \"\"\"",
      "226:         super().__init__(**kwargs)",
      "227:         self.number_of_dimensions = number_of_dimensions",
      "229:     def __array_finalize__(self, obj: Optional[np.ndarray]) -> None:",
      "230:         \"\"\"This method is called when the system allocates a new array from obj.",
      "232:         Args:",
      "233:             obj: A subclass (subtype) of ndarray.",
      "234:         \"\"\"",
      "235:         if obj is None:",
      "236:             return",
      "238:         self.units = getattr(obj, \"units\", None)",
      "239:         self.number_of_dimensions = getattr(",
      "240:             obj, \"number_of_dimensions\", None",
      "241:         )  # type: ignore[assignment]",
      "242:         self.is_sparse = getattr(obj, \"is_sparse\", None)",
      "244:         default_attributes = {",
      "245:             \"units\": self.units,",
      "246:             \"number_of_dimensions\": self.number_of_dimensions,",
      "247:             \"is_spare\": self.is_sparse,",
      "248:         }",
      "249:         self.__dict__.update(default_attributes)",
      "251:     # pytype: disable=attribute-error",
      "252:     def __array_ufunc__(",
      "253:         self, ufunc: Any, method: str, *inputs: Any, **kwargs: Any",
      "254:     ) -> Any:",
      "255:         \"\"\"Overwrite this method as we are subclassing numpy array.",
      "257:         Args:",
      "258:             ufunc: The ufunc object that was called.",
      "259:             method: A string indicating which Ufunc method was called",
      "260:                     (one of \"__call__\", \"reduce\", \"reduceat\", \"accumulate\", \"outer\",",
      "261:                     \"inner\").",
      "265:         Returns:",
      "266:             The result of the operation.",
      "267:         \"\"\"",
      "268:         f = {",
      "269:             \"reduce\": ufunc.reduce,",
      "270:             \"accumulate\": ufunc.accumulate,",
      "271:             \"reduceat\": ufunc.reduceat,",
      "272:             \"outer\": ufunc.outer,",
      "273:             \"at\": ufunc.at,",
      "274:             \"__call__\": ufunc,",
      "275:         }",
      "276:         # convert the inputs to np.ndarray to prevent recursion, call the function,",
      "277:         # then cast it back as FeatureArray",
      "278:         output = FeatureArray(",
      "279:             f[method](*(i.view(np.ndarray) for i in inputs), **kwargs),",
      "280:             number_of_dimensions=kwargs[\"number_of_dimensions\"],",
      "281:         )",
      "282:         output.__dict__ = self.__dict__  # carry forward attributes",
      "283:         return output",
      "285:     def __reduce__(self) -> Tuple[Any, Any, Any]:",
      "286:         \"\"\"Needed in order to pickle this object.",
      "288:         Returns:",
      "289:             A tuple.",
      "290:         \"\"\"",
      "291:         pickled_state = super(FeatureArray, self).__reduce__()",
      "292:         if isinstance(pickled_state, str):",
      "293:             raise TypeError(\"np array __reduce__ returned string instead of tuple.\")",
      "294:         new_state = pickled_state[2] + (",
      "295:             self.number_of_dimensions,",
      "296:             self.is_sparse,",
      "297:             self.units,",
      "298:         )",
      "299:         return pickled_state[0], pickled_state[1], new_state",
      "301:     def __setstate__(self, state: Any, **kwargs: Any) -> None:",
      "302:         \"\"\"Sets the state.",
      "304:         Args:",
      "305:             state: The state argument must be a sequence that contains the following",
      "306:                    elements version, shape, dtype, isFortan, rawdata.",
      "308:         \"\"\"",
      "309:         # Needed in order to load the object",
      "310:         self.number_of_dimensions = state[-3]",
      "311:         self.is_sparse = state[-2]",
      "312:         self.units = state[-1]",
      "313:         super(FeatureArray, self).__setstate__(state[0:-3], **kwargs)",
      "315:     # pytype: enable=attribute-error",
      "317:     @staticmethod",
      "318:     def _validate_number_of_dimensions(",
      "319:         number_of_dimensions: int, input_array: np.ndarray",
      "320:     ) -> None:",
      "321:         \"\"\"Validates if the input array has given number of dimensions.",
      "323:         Args:",
      "324:             number_of_dimensions: number of dimensions",
      "325:             input_array: input array",
      "327:         Raises: ValueError in case the dimensions do not match",
      "328:         \"\"\"",
      "329:         # when loading the feature arrays from disk, the shape represents",
      "330:         # the correct number of dimensions",
      "331:         if len(input_array.shape) == number_of_dimensions:",
      "332:             return",
      "334:         _sub_array = input_array",
      "335:         dim = 0",
      "336:         # Go number_of_dimensions into the given input_array",
      "337:         for i in range(1, number_of_dimensions + 1):",
      "338:             _sub_array = _sub_array[0]",
      "339:             if isinstance(_sub_array, scipy.sparse.spmatrix):",
      "340:                 dim = i",
      "341:                 break",
      "342:             if isinstance(_sub_array, np.ndarray) and _sub_array.shape[0] == 0:",
      "343:                 # sequence dimension is 0, we are dealing with \"fake\" features",
      "344:                 dim = i",
      "345:                 break",
      "347:         # If the resulting sub_array is sparse, the remaining number of dimensions",
      "348:         # should be at least 2",
      "349:         if isinstance(_sub_array, scipy.sparse.spmatrix):",
      "350:             if dim > 2:",
      "351:                 raise ValueError(",
      "352:                     f\"Given number of dimensions '{number_of_dimensions}' does not \"",
      "353:                     f\"match dimensions of given input array: {input_array}.\"",
      "354:                 )",
      "355:         elif isinstance(_sub_array, np.ndarray) and _sub_array.shape[0] == 0:",
      "356:             # sequence dimension is 0, we are dealing with \"fake\" features,",
      "357:             # but they should be of dim 2",
      "358:             if dim > 2:",
      "359:                 raise ValueError(",
      "360:                     f\"Given number of dimensions '{number_of_dimensions}' does not \"",
      "361:                     f\"match dimensions of given input array: {input_array}.\"",
      "362:                 )",
      "363:         # If the resulting sub_array is dense, the sub_array should be a single number",
      "364:         elif not np.issubdtype(type(_sub_array), np.integer) and not isinstance(",
      "365:             _sub_array, (np.float32, np.float64)",
      "366:         ):",
      "367:             raise ValueError(",
      "368:                 f\"Given number of dimensions '{number_of_dimensions}' does not match \"",
      "369:                 f\"dimensions of given input array: {input_array}.\"",
      "370:             )",
      "",
      "---------------"
    ],
    "rasa/utils/tensorflow/model_data.py||rasa/utils/tensorflow/model_data.py": [
      "File: rasa/utils/tensorflow/model_data.py -> rasa/utils/tensorflow/model_data.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "20: import scipy.sparse",
      "21: from sklearn.model_selection import train_test_split",
      "23: logger = logging.getLogger(__name__)",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "23: from rasa.utils.tensorflow.feature_array import FeatureArray",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "37:         return np.array(ragged_array, dtype=object)",
      "233: class FeatureSignature(NamedTuple):",
      "234:     \"\"\"Signature of feature arrays.",
      "",
      "[Removed Lines]",
      "40: class FeatureArray(np.ndarray):",
      "41:     \"\"\"Stores any kind of features ready to be used by a RasaModel.",
      "43:     Next to the input numpy array of features, it also received the number of",
      "44:     dimensions of the features.",
      "45:     As our features can have 1 to 4 dimensions we might have different number of numpy",
      "46:     arrays stacked. The number of dimensions helps us to figure out how to handle this",
      "47:     particular feature array. Also, it is automatically determined whether the feature",
      "48:     array is sparse or not and the number of units is determined as well.",
      "50:     Subclassing np.array: https://numpy.org/doc/stable/user/basics.subclassing.html",
      "51:     \"\"\"",
      "53:     def __new__(",
      "54:         cls, input_array: np.ndarray, number_of_dimensions: int",
      "55:     ) -> \"FeatureArray\":",
      "56:         \"\"\"Create and return a new object.  See help(type) for accurate signature.\"\"\"",
      "57:         FeatureArray._validate_number_of_dimensions(number_of_dimensions, input_array)",
      "59:         feature_array = np.asarray(input_array).view(cls)",
      "61:         if number_of_dimensions <= 2:",
      "62:             feature_array.units = input_array.shape[-1]",
      "63:             feature_array.is_sparse = isinstance(input_array[0], scipy.sparse.spmatrix)",
      "64:         elif number_of_dimensions == 3:",
      "65:             feature_array.units = input_array[0].shape[-1]",
      "66:             feature_array.is_sparse = isinstance(input_array[0], scipy.sparse.spmatrix)",
      "67:         elif number_of_dimensions == 4:",
      "68:             feature_array.units = input_array[0][0].shape[-1]",
      "69:             feature_array.is_sparse = isinstance(",
      "70:                 input_array[0][0], scipy.sparse.spmatrix",
      "71:             )",
      "72:         else:",
      "73:             raise ValueError(",
      "74:                 f\"Number of dimensions '{number_of_dimensions}' currently not \"",
      "75:                 f\"supported.\"",
      "76:             )",
      "78:         feature_array.number_of_dimensions = number_of_dimensions",
      "80:         return feature_array",
      "82:     def __init__(",
      "83:         self, input_array: Any, number_of_dimensions: int, **kwargs: Any",
      "84:     ) -> None:",
      "85:         \"\"\"Initialize. FeatureArray.",
      "87:         Needed in order to avoid 'Invalid keyword argument number_of_dimensions",
      "88:         to function FeatureArray.__init__ '",
      "89:         Args:",
      "90:             input_array: the array that contains features",
      "91:             number_of_dimensions: number of dimensions in input_array",
      "92:         \"\"\"",
      "93:         super().__init__(**kwargs)",
      "94:         self.number_of_dimensions = number_of_dimensions",
      "96:     def __array_finalize__(self, obj: Optional[np.ndarray]) -> None:",
      "97:         \"\"\"This method is called when the system allocates a new array from obj.",
      "99:         Args:",
      "100:             obj: A subclass (subtype) of ndarray.",
      "101:         \"\"\"",
      "102:         if obj is None:",
      "103:             return",
      "105:         self.units = getattr(obj, \"units\", None)",
      "106:         self.number_of_dimensions = getattr(obj, \"number_of_dimensions\", None)  # type: ignore[assignment] # noqa:E501",
      "107:         self.is_sparse = getattr(obj, \"is_sparse\", None)",
      "109:         default_attributes = {",
      "110:             \"units\": self.units,",
      "111:             \"number_of_dimensions\": self.number_of_dimensions,",
      "112:             \"is_spare\": self.is_sparse,",
      "113:         }",
      "114:         self.__dict__.update(default_attributes)",
      "116:     # pytype: disable=attribute-error",
      "117:     def __array_ufunc__(",
      "118:         self, ufunc: Any, method: Text, *inputs: Any, **kwargs: Any",
      "119:     ) -> Any:",
      "120:         \"\"\"Overwrite this method as we are subclassing numpy array.",
      "122:         Args:",
      "123:             ufunc: The ufunc object that was called.",
      "124:             method: A string indicating which Ufunc method was called",
      "125:                     (one of \"__call__\", \"reduce\", \"reduceat\", \"accumulate\", \"outer\",",
      "126:                     \"inner\").",
      "130:         Returns:",
      "131:             The result of the operation.",
      "132:         \"\"\"",
      "133:         f = {",
      "134:             \"reduce\": ufunc.reduce,",
      "135:             \"accumulate\": ufunc.accumulate,",
      "136:             \"reduceat\": ufunc.reduceat,",
      "137:             \"outer\": ufunc.outer,",
      "138:             \"at\": ufunc.at,",
      "139:             \"__call__\": ufunc,",
      "140:         }",
      "141:         # convert the inputs to np.ndarray to prevent recursion, call the function,",
      "142:         # then cast it back as FeatureArray",
      "143:         output = FeatureArray(",
      "144:             f[method](*(i.view(np.ndarray) for i in inputs), **kwargs),",
      "145:             number_of_dimensions=kwargs[\"number_of_dimensions\"],",
      "146:         )",
      "147:         output.__dict__ = self.__dict__  # carry forward attributes",
      "148:         return output",
      "150:     def __reduce__(self) -> Tuple[Any, Any, Any]:",
      "151:         \"\"\"Needed in order to pickle this object.",
      "153:         Returns:",
      "154:             A tuple.",
      "155:         \"\"\"",
      "156:         pickled_state = super(FeatureArray, self).__reduce__()",
      "157:         if isinstance(pickled_state, str):",
      "158:             raise TypeError(\"np array __reduce__ returned string instead of tuple.\")",
      "159:         new_state = pickled_state[2] + (",
      "160:             self.number_of_dimensions,",
      "161:             self.is_sparse,",
      "162:             self.units,",
      "163:         )",
      "164:         return pickled_state[0], pickled_state[1], new_state",
      "166:     def __setstate__(self, state: Any, **kwargs: Any) -> None:",
      "167:         \"\"\"Sets the state.",
      "169:         Args:",
      "170:             state: The state argument must be a sequence that contains the following",
      "171:                    elements version, shape, dtype, isFortan, rawdata.",
      "173:         \"\"\"",
      "174:         # Needed in order to load the object",
      "175:         self.number_of_dimensions = state[-3]",
      "176:         self.is_sparse = state[-2]",
      "177:         self.units = state[-1]",
      "178:         super(FeatureArray, self).__setstate__(state[0:-3], **kwargs)",
      "180:     # pytype: enable=attribute-error",
      "182:     @staticmethod",
      "183:     def _validate_number_of_dimensions(",
      "184:         number_of_dimensions: int, input_array: np.ndarray",
      "185:     ) -> None:",
      "186:         \"\"\"Validates if the the input array has given number of dimensions.",
      "188:         Args:",
      "189:             number_of_dimensions: number of dimensions",
      "190:             input_array: input array",
      "192:         Raises: ValueError in case the dimensions do not match",
      "193:         \"\"\"",
      "194:         _sub_array = input_array",
      "195:         dim = 0",
      "196:         # Go number_of_dimensions into the given input_array",
      "197:         for i in range(1, number_of_dimensions + 1):",
      "198:             _sub_array = _sub_array[0]",
      "199:             if isinstance(_sub_array, scipy.sparse.spmatrix):",
      "200:                 dim = i",
      "201:                 break",
      "202:             if isinstance(_sub_array, np.ndarray) and _sub_array.shape[0] == 0:",
      "203:                 # sequence dimension is 0, we are dealing with \"fake\" features",
      "204:                 dim = i",
      "205:                 break",
      "207:         # If the resulting sub_array is sparse, the remaining number of dimensions",
      "208:         # should be at least 2",
      "209:         if isinstance(_sub_array, scipy.sparse.spmatrix):",
      "210:             if dim > 2:",
      "211:                 raise ValueError(",
      "212:                     f\"Given number of dimensions '{number_of_dimensions}' does not \"",
      "213:                     f\"match dimensions of given input array: {input_array}.\"",
      "214:                 )",
      "215:         elif isinstance(_sub_array, np.ndarray) and _sub_array.shape[0] == 0:",
      "216:             # sequence dimension is 0, we are dealing with \"fake\" features,",
      "217:             # but they should be of dim 2",
      "218:             if dim > 2:",
      "219:                 raise ValueError(",
      "220:                     f\"Given number of dimensions '{number_of_dimensions}' does not \"",
      "221:                     f\"match dimensions of given input array: {input_array}.\"",
      "222:                 )",
      "223:         # If the resulting sub_array is dense, the sub_array should be a single number",
      "224:         elif not np.issubdtype(type(_sub_array), np.integer) and not isinstance(",
      "225:             _sub_array, (np.float32, np.float64)",
      "226:         ):",
      "227:             raise ValueError(",
      "228:                 f\"Given number of dimensions '{number_of_dimensions}' does not match \"",
      "229:                 f\"dimensions of given input array: {input_array}.\"",
      "230:             )",
      "",
      "[Added Lines]",
      "[None]",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "270:         label_sub_key: Optional[Text] = None,",
      "271:         data: Optional[Data] = None,",
      "272:     ) -> None:",
      "276:         Args:",
      "277:             label_key: the key of a label used for balancing, etc.",
      "",
      "[Removed Lines]",
      "273:         \"\"\"",
      "274:         Initializes the RasaModelData object.",
      "",
      "[Added Lines]",
      "82:         \"\"\"Initializes the RasaModelData object.",
      "",
      "---------------"
    ],
    "scripts/ping_slack_about_package_release.sh||scripts/ping_slack_about_package_release.sh": [
      "File: scripts/ping_slack_about_package_release.sh -> scripts/ping_slack_about_package_release.sh"
    ],
    "tests/core/featurizers/test_tracker_featurizer.py||tests/core/featurizers/test_tracker_featurizer.py": [
      "File: tests/core/featurizers/test_tracker_featurizer.py -> tests/core/featurizers/test_tracker_featurizer.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "34:     assert TrackerFeaturizer.load(\"non_existent_class\") is None",
      "38:     state_featurizer = SingleStateFeaturizer()",
      "39:     state_featurizer.prepare_for_training(moodbot_domain)",
      "40:     tracker_featurizer = MaxHistoryTrackerFeaturizer(state_featurizer)",
      "",
      "[Removed Lines]",
      "37: def test_persist_and_load_tracker_featurizer(tmp_path: Text, moodbot_domain: Domain):",
      "",
      "[Added Lines]",
      "37: def test_persist_and_load_full_dialogue_tracker_featurizer(",
      "38:     tmp_path: Text, moodbot_domain: Domain",
      "39: ):",
      "40:     state_featurizer = SingleStateFeaturizer()",
      "41:     state_featurizer.prepare_for_training(moodbot_domain)",
      "42:     tracker_featurizer = FullDialogueTrackerFeaturizer(state_featurizer)",
      "44:     tracker_featurizer.persist(tmp_path)",
      "46:     loaded_tracker_featurizer = TrackerFeaturizer.load(tmp_path)",
      "48:     assert loaded_tracker_featurizer is not None",
      "49:     assert loaded_tracker_featurizer.state_featurizer is not None",
      "50:     assert loaded_tracker_featurizer.to_dict() == tracker_featurizer.to_dict()",
      "53: def test_persist_and_load_max_history_tracker_featurizer(",
      "54:     tmp_path: Text, moodbot_domain: Domain",
      "55: ):",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "46:     assert loaded_tracker_featurizer is not None",
      "47:     assert loaded_tracker_featurizer.state_featurizer is not None",
      "50: def test_convert_action_labels_to_ids(domain: Domain):",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "66:     assert loaded_tracker_featurizer.to_dict() == tracker_featurizer.to_dict()",
      "69: def test_persist_and_load_intent_max_history_tracker_featurizer(",
      "70:     tmp_path: Text, moodbot_domain: Domain",
      "71: ):",
      "72:     state_featurizer = SingleStateFeaturizer()",
      "73:     state_featurizer.prepare_for_training(moodbot_domain)",
      "74:     tracker_featurizer = IntentMaxHistoryTrackerFeaturizer(state_featurizer)",
      "76:     tracker_featurizer.persist(tmp_path)",
      "78:     loaded_tracker_featurizer = TrackerFeaturizer.load(tmp_path)",
      "80:     assert loaded_tracker_featurizer is not None",
      "81:     assert loaded_tracker_featurizer.state_featurizer is not None",
      "82:     assert loaded_tracker_featurizer.to_dict() == tracker_featurizer.to_dict()",
      "",
      "---------------"
    ],
    "tests/nlu/extractors/test_crf_entity_extractor.py||tests/nlu/extractors/test_crf_entity_extractor.py": [
      "File: tests/nlu/extractors/test_crf_entity_extractor.py -> tests/nlu/extractors/test_crf_entity_extractor.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "1: import copy",
      "2: from typing import Dict, Text, List, Any, Callable",
      "4: import pytest",
      "6: from rasa.engine.graph import ExecutionContext",
      "7: from rasa.engine.storage.resource import Resource",
      "8: from rasa.engine.storage.storage import ModelStorage",
      "9: from rasa.nlu.featurizers.dense_featurizer.spacy_featurizer import SpacyFeaturizer",
      "10: from rasa.nlu.tokenizers.spacy_tokenizer import SpacyTokenizer",
      "12: from rasa.nlu.tokenizers.whitespace_tokenizer import WhitespaceTokenizer",
      "13: from rasa.nlu.utils.spacy_utils import SpacyModel, SpacyNLP",
      "14: from rasa.shared.importers.rasa import RasaFileImporter",
      "15: from rasa.shared.nlu.constants import TEXT, ENTITIES",
      "16: from rasa.shared.nlu.training_data.message import Message",
      "23: @pytest.fixture()",
      "",
      "[Removed Lines]",
      "11: from rasa.nlu.constants import SPACY_DOCS",
      "17: from rasa.nlu.extractors.crf_entity_extractor import (",
      "18:     CRFEntityExtractor,",
      "19:     CRFEntityExtractorOptions,",
      "20: )",
      "",
      "[Added Lines]",
      "4: import numpy as np",
      "10: from rasa.nlu.constants import SPACY_DOCS",
      "11: from rasa.nlu.extractors.crf_entity_extractor import (",
      "12:     CRFEntityExtractor,",
      "13:     CRFEntityExtractorOptions,",
      "14:     CRFToken,",
      "15: )",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "204:     spacy_featurizer.process([message])",
      "206:     text_data = crf_extractor._convert_to_crf_tokens(message)",
      "209:     assert \"0:text_dense_features\" in features[0]",
      "210:     dense_features, _ = message.get_dense_features(TEXT, [])",
      "",
      "[Removed Lines]",
      "207:     features = crf_extractor._crf_tokens_to_features(text_data)",
      "",
      "[Added Lines]",
      "209:     features = crf_extractor._crf_tokens_to_features(text_data, component_config)",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "250:     assert processed_message.get(TEXT) == message_text",
      "251:     assert processed_message.get(ENTITIES) == []",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "256: @pytest.fixture",
      "257: def sample_data():",
      "258:     return {",
      "259:         \"text\": \"apple\",",
      "260:         \"pos_tag\": \"NOUN\",",
      "261:         \"pattern\": {\"length\": 5, \"is_capitalized\": False},",
      "262:         \"dense_features\": np.array([0.1, 0.2, 0.3]),",
      "263:         \"entity_tag\": \"B-FOOD\",",
      "264:         \"entity_role_tag\": \"INGREDIENT\",",
      "265:         \"entity_group_tag\": \"ITEM\",",
      "266:     }",
      "269: @pytest.fixture",
      "270: def sample_token(sample_data):",
      "271:     return CRFToken(",
      "272:         sample_data[\"text\"],",
      "273:         sample_data[\"pos_tag\"],",
      "274:         sample_data[\"pattern\"],",
      "275:         sample_data[\"dense_features\"],",
      "276:         sample_data[\"entity_tag\"],",
      "277:         sample_data[\"entity_role_tag\"],",
      "278:         sample_data[\"entity_group_tag\"],",
      "279:     )",
      "282: def test_crf_token_to_dict(sample_data, sample_token):",
      "283:     token_dict = sample_token.to_dict()",
      "285:     assert token_dict[\"text\"] == sample_data[\"text\"]",
      "286:     assert token_dict[\"pos_tag\"] == sample_data[\"pos_tag\"]",
      "287:     assert token_dict[\"pattern\"] == sample_data[\"pattern\"]",
      "288:     assert token_dict[\"dense_features\"] == [",
      "289:         str(x) for x in sample_data[\"dense_features\"]",
      "290:     ]",
      "291:     assert token_dict[\"entity_tag\"] == sample_data[\"entity_tag\"]",
      "292:     assert token_dict[\"entity_role_tag\"] == sample_data[\"entity_role_tag\"]",
      "293:     assert token_dict[\"entity_group_tag\"] == sample_data[\"entity_group_tag\"]",
      "296: def test_crf_token_create_from_dict(sample_data):",
      "297:     dict_data = {",
      "298:         \"text\": sample_data[\"text\"],",
      "299:         \"pos_tag\": sample_data[\"pos_tag\"],",
      "300:         \"pattern\": sample_data[\"pattern\"],",
      "301:         \"dense_features\": [str(x) for x in sample_data[\"dense_features\"]],",
      "302:         \"entity_tag\": sample_data[\"entity_tag\"],",
      "303:         \"entity_role_tag\": sample_data[\"entity_role_tag\"],",
      "304:         \"entity_group_tag\": sample_data[\"entity_group_tag\"],",
      "305:     }",
      "307:     token = CRFToken.create_from_dict(dict_data)",
      "309:     assert token.text == sample_data[\"text\"]",
      "310:     assert token.pos_tag == sample_data[\"pos_tag\"]",
      "311:     assert token.pattern == sample_data[\"pattern\"]",
      "312:     np.testing.assert_array_equal(token.dense_features, sample_data[\"dense_features\"])",
      "313:     assert token.entity_tag == sample_data[\"entity_tag\"]",
      "314:     assert token.entity_role_tag == sample_data[\"entity_role_tag\"]",
      "315:     assert token.entity_group_tag == sample_data[\"entity_group_tag\"]",
      "318: def test_crf_token_roundtrip_conversion(sample_token):",
      "319:     token_dict = sample_token.to_dict()",
      "320:     new_token = CRFToken.create_from_dict(token_dict)",
      "322:     assert new_token.text == sample_token.text",
      "323:     assert new_token.pos_tag == sample_token.pos_tag",
      "324:     assert new_token.pattern == sample_token.pattern",
      "325:     np.testing.assert_array_equal(new_token.dense_features, sample_token.dense_features)",
      "326:     assert new_token.entity_tag == sample_token.entity_tag",
      "327:     assert new_token.entity_role_tag == sample_token.entity_role_tag",
      "328:     assert new_token.entity_group_tag == sample_token.entity_group_tag",
      "331: def test_crf_token_empty_dense_features(sample_data):",
      "332:     sample_data[\"dense_features\"] = np.array([])",
      "333:     token = CRFToken(",
      "334:         sample_data[\"text\"],",
      "335:         sample_data[\"pos_tag\"],",
      "336:         sample_data[\"pattern\"],",
      "337:         sample_data[\"dense_features\"],",
      "338:         sample_data[\"entity_tag\"],",
      "339:         sample_data[\"entity_role_tag\"],",
      "340:         sample_data[\"entity_group_tag\"],",
      "341:     )",
      "342:     token_dict = token.to_dict()",
      "343:     new_token = CRFToken.create_from_dict(token_dict)",
      "344:     np.testing.assert_array_equal(new_token.dense_features, np.array([]))",
      "347: def test_crf_token_empty_pattern(sample_data):",
      "348:     sample_data[\"pattern\"] = {}",
      "349:     token = CRFToken(",
      "350:         sample_data[\"text\"],",
      "351:         sample_data[\"pos_tag\"],",
      "352:         sample_data[\"pattern\"],",
      "353:         sample_data[\"dense_features\"],",
      "354:         sample_data[\"entity_tag\"],",
      "355:         sample_data[\"entity_role_tag\"],",
      "356:         sample_data[\"entity_group_tag\"],",
      "357:     )",
      "358:     token_dict = token.to_dict()",
      "359:     new_token = CRFToken.create_from_dict(token_dict)",
      "360:     assert new_token.pattern == {}",
      "",
      "---------------"
    ],
    "tests/shared/nlu/training_data/test_features.py||tests/shared/nlu/training_data/test_features.py": [
      "File: tests/shared/nlu/training_data/test_features.py -> tests/shared/nlu/training_data/test_features.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "1: import itertools",
      "2: from typing import Optional, Text, List, Dict, Tuple, Any",
      "4: import numpy as np",
      "5: import pytest",
      "6: import scipy.sparse",
      "9: from rasa.shared.nlu.constants import (",
      "10:     FEATURE_TYPE_SENTENCE,",
      "11:     FEATURE_TYPE_SEQUENCE,",
      "12:     TEXT,",
      "13:     INTENT,",
      "14: )",
      "17: @pytest.mark.parametrize(",
      "",
      "[Removed Lines]",
      "8: from rasa.shared.nlu.training_data.features import Features",
      "",
      "[Added Lines]",
      "2: import os",
      "3: import tempfile",
      "4: from pathlib import Path",
      "17: from rasa.shared.nlu.training_data.features import (",
      "18:     Features,",
      "19:     FeatureMetadata,",
      "20:     save_features,",
      "21:     load_features,",
      "22: )",
      "25: @pytest.fixture",
      "26: def safe_tensors_tmp_file() -> str:",
      "27:     with tempfile.NamedTemporaryFile(delete=False, suffix=\".safetensors\") as f:",
      "28:         yield f.name",
      "29:     os.unlink(f.name)",
      "32: @pytest.fixture",
      "33: def dense_features() -> Features:",
      "34:     features_matrix = np.array([[1, 2, 3], [4, 5, 6]])",
      "35:     return Features(",
      "36:         features=features_matrix,",
      "37:         feature_type=\"dense\",",
      "38:         attribute=\"test\",",
      "39:         origin=\"test_origin\",",
      "40:     )",
      "43: @pytest.fixture",
      "44: def sparse_features() -> Features:",
      "45:     features_matrix = scipy.sparse.csr_matrix(",
      "46:         ([1, 2, 3], ([0, 1, 1], [0, 1, 2])), shape=(2, 3)",
      "47:     )",
      "48:     return Features(",
      "49:         features=features_matrix,",
      "50:         feature_type=\"sparse\",",
      "51:         attribute=\"test\",",
      "52:         origin=\"test_origin\",",
      "53:     )",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "467:         expected_origin = [\"origin-1\"]",
      "468:     with pytest.raises(ValueError, match=message):",
      "469:         Features.reduce(features_list, expected_origins=expected_origin)",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "511: def test_feature_metadata():",
      "512:     metadata = FeatureMetadata(",
      "513:         data_type=\"dense\",",
      "514:         attribute=\"text\",",
      "515:         origin=\"test\",",
      "516:         is_sparse=False,",
      "517:         shape=(10, 5),",
      "518:         safetensors_key=\"key_0\",",
      "519:     )",
      "521:     assert metadata.data_type == \"dense\"",
      "522:     assert metadata.attribute == \"text\"",
      "523:     assert metadata.origin == \"test\"",
      "524:     assert not metadata.is_sparse",
      "525:     assert metadata.shape == (10, 5)",
      "526:     assert metadata.safetensors_key == \"key_0\"",
      "529: def test_save_dense_features(safe_tensors_tmp_file: str, dense_features: Features):",
      "530:     features_dict = {\"test_key\": [dense_features]}",
      "531:     metadata = save_features(features_dict, safe_tensors_tmp_file)",
      "533:     assert \"test_key\" in metadata",
      "534:     assert len(metadata[\"test_key\"]) == 1",
      "535:     assert metadata[\"test_key\"][0][\"data_type\"] == \"dense\"",
      "536:     assert metadata[\"test_key\"][0][\"shape\"] == (2, 3)",
      "537:     assert not metadata[\"test_key\"][0][\"is_sparse\"]",
      "538:     assert Path(safe_tensors_tmp_file).exists()",
      "541: def test_save_sparse_features(safe_tensors_tmp_file: str, sparse_features: Features):",
      "542:     features_dict = {\"test_key\": [sparse_features]}",
      "543:     metadata = save_features(features_dict, safe_tensors_tmp_file)",
      "545:     assert \"test_key\" in metadata",
      "546:     assert len(metadata[\"test_key\"]) == 1",
      "547:     assert metadata[\"test_key\"][0][\"data_type\"] == \"sparse\"",
      "548:     assert metadata[\"test_key\"][0][\"shape\"] == (2, 3)",
      "549:     assert metadata[\"test_key\"][0][\"is_sparse\"]",
      "550:     assert Path(safe_tensors_tmp_file).exists()",
      "553: def test_save_mixed_features(",
      "554:     safe_tensors_tmp_file: str, dense_features: Features, sparse_features: Features",
      "555: ):",
      "556:     features_dict = {\"test_key\": [dense_features, sparse_features]}",
      "557:     metadata = save_features(features_dict, safe_tensors_tmp_file)",
      "559:     assert \"test_key\" in metadata",
      "560:     assert len(metadata[\"test_key\"]) == 2",
      "561:     assert metadata[\"test_key\"][0][\"data_type\"] == \"dense\"",
      "562:     assert metadata[\"test_key\"][1][\"data_type\"] == \"sparse\"",
      "563:     assert Path(safe_tensors_tmp_file).exists()",
      "566: def test_save_multiple_keys(",
      "567:     safe_tensors_tmp_file: str, dense_features: Features, sparse_features: Features",
      "568: ):",
      "569:     features_dict = {\"dense_key\": [dense_features], \"sparse_key\": [sparse_features]}",
      "570:     metadata = save_features(features_dict, safe_tensors_tmp_file)",
      "572:     assert \"dense_key\" in metadata",
      "573:     assert \"sparse_key\" in metadata",
      "574:     assert metadata[\"dense_key\"][0][\"data_type\"] == \"dense\"",
      "575:     assert metadata[\"sparse_key\"][0][\"data_type\"] == \"sparse\"",
      "576:     assert Path(safe_tensors_tmp_file).exists()",
      "579: @pytest.fixture",
      "580: def setup_save_load(",
      "581:     safe_tensors_tmp_file: str, dense_features: Features, sparse_features: Features",
      "582: ) -> Tuple[str, Dict[str, Any], Dict[str, List[Features]]]:",
      "583:     features_dict = {\"dense_key\": [dense_features], \"sparse_key\": [sparse_features]}",
      "584:     metadata = save_features(features_dict, safe_tensors_tmp_file)",
      "585:     return safe_tensors_tmp_file, metadata, features_dict",
      "588: def test_load_dense_features(",
      "589:     setup_save_load: Tuple[str, Dict[str, Any], Dict[str, List[Features]]],",
      "590: ):",
      "591:     temp_file, metadata, original_dict = setup_save_load",
      "592:     loaded_dict = load_features(temp_file, metadata)",
      "594:     assert \"dense_key\" in loaded_dict",
      "595:     assert len(loaded_dict[\"dense_key\"]) == 1",
      "596:     assert not loaded_dict[\"dense_key\"][0].is_sparse()",
      "597:     np.testing.assert_array_equal(",
      "598:         loaded_dict[\"dense_key\"][0].features, original_dict[\"dense_key\"][0].features",
      "599:     )",
      "602: def test_load_sparse_features(",
      "603:     setup_save_load: Tuple[str, Dict[str, Any], Dict[str, List[Features]]],",
      "604: ):",
      "605:     temp_file, metadata, original_dict = setup_save_load",
      "606:     loaded_dict = load_features(temp_file, metadata)",
      "608:     assert \"sparse_key\" in loaded_dict",
      "609:     assert len(loaded_dict[\"sparse_key\"]) == 1",
      "610:     assert loaded_dict[\"sparse_key\"][0].is_sparse()",
      "611:     assert (",
      "612:         loaded_dict[\"sparse_key\"][0].features != original_dict[\"sparse_key\"][0].features",
      "613:     ).nnz == 0",
      "616: def test_load_preserves_metadata(",
      "617:     setup_save_load: Tuple[str, Dict[str, Any], Dict[str, List[Features]]],",
      "618: ):",
      "619:     temp_file, metadata, original_dict = setup_save_load",
      "620:     loaded_dict = load_features(temp_file, metadata)",
      "622:     for key in original_dict:",
      "623:         for orig_feat, loaded_feat in zip(original_dict[key], loaded_dict[key]):",
      "624:             assert orig_feat.type == loaded_feat.type",
      "625:             assert orig_feat.attribute == loaded_feat.attribute",
      "626:             assert orig_feat.origin == loaded_feat.origin",
      "629: def test_load_nonexistent_file():",
      "630:     with pytest.raises(Exception):",
      "631:         load_features(\"nonexistent.safetensors\", {})",
      "634: def test_load_invalid_metadata(safe_tensors_tmp_file: str, dense_features: Features):",
      "635:     features_dict = {\"test_key\": [dense_features]}",
      "636:     metadata = save_features(features_dict, safe_tensors_tmp_file)",
      "638:     # Corrupt the metadata",
      "639:     metadata[\"test_key\"][0][\"safetensors_key\"] = \"invalid_key\"",
      "641:     with pytest.raises(Exception):",
      "642:         load_features(safe_tensors_tmp_file, metadata)",
      "645: def test_end_to_end(safe_tensors_tmp_file: str):",
      "646:     # Create test data",
      "647:     dense_matrix = np.array([[1, 2], [3, 4]])",
      "648:     sparse_matrix = scipy.sparse.csr_matrix(([1, 2], ([0, 1], [0, 1])), shape=(2, 2))",
      "650:     features_dict = {",
      "651:         \"group1\": [",
      "652:             Features(dense_matrix, \"dense\", \"test1\", \"origin1\"),",
      "653:             Features(sparse_matrix, \"sparse\", \"test2\", \"origin2\"),",
      "654:         ],",
      "655:         \"group2\": [",
      "656:             Features(dense_matrix * 2, \"dense\", \"test3\", [\"origin3\", \"origin4\"])",
      "657:         ],",
      "658:     }",
      "660:     # Save features",
      "661:     metadata = save_features(features_dict, safe_tensors_tmp_file)",
      "663:     # Load features",
      "664:     loaded_dict = load_features(safe_tensors_tmp_file, metadata)",
      "666:     # Verify structure",
      "667:     assert set(loaded_dict.keys()) == set(features_dict.keys())",
      "668:     assert len(loaded_dict[\"group1\"]) == 2",
      "669:     assert len(loaded_dict[\"group2\"]) == 1",
      "671:     # Verify dense features",
      "672:     np.testing.assert_array_equal(",
      "673:         loaded_dict[\"group1\"][0].features, features_dict[\"group1\"][0].features",
      "674:     )",
      "676:     # Verify sparse features",
      "677:     assert (",
      "678:         loaded_dict[\"group1\"][1].features != features_dict[\"group1\"][1].features",
      "679:     ).nnz == 0",
      "681:     # Verify metadata",
      "682:     assert loaded_dict[\"group1\"][0].type == \"dense\"",
      "683:     assert loaded_dict[\"group1\"][1].type == \"sparse\"",
      "684:     assert loaded_dict[\"group2\"][0].origin == [\"origin3\", \"origin4\"]",
      "",
      "---------------"
    ],
    "tests/utils/tensorflow/test_feature_array.py||tests/utils/tensorflow/test_feature_array.py": [
      "File: tests/utils/tensorflow/test_feature_array.py -> tests/utils/tensorflow/test_feature_array.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "[No context available]",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "1: import numpy as np",
      "2: import scipy.sparse",
      "4: from rasa.utils.tensorflow.feature_array import (",
      "5:     _recursive_serialize,",
      "6:     _serialize_nested_data,",
      "7:     _deserialize_nested_data,",
      "8: )",
      "9: from rasa.utils.tensorflow.model_data import RasaModelData",
      "12: def test_recursive_serialize_numpy_array():",
      "13:     data_dict = {}",
      "14:     metadata = []",
      "16:     _recursive_serialize(np.array([1, 2, 3]), \"test_array\", data_dict, metadata)",
      "17:     assert \"test_array_array\" in data_dict",
      "18:     assert metadata[0] == {\"type\": \"dense\", \"key\": \"test_array_array\", \"shape\": (3,)}",
      "21: def test_recursive_serialize_floats():",
      "22:     data_dict = {}",
      "23:     metadata = []",
      "25:     _recursive_serialize([1.0, 2.0, 3.0], \"test_list\", data_dict, metadata)",
      "26:     assert \"test_list_list\" in data_dict",
      "27:     assert metadata[0] == {\"type\": \"list\", \"key\": \"test_list_list\"}",
      "30: def test_recursive_serialize_sparse_matrix():",
      "31:     data_dict = {}",
      "32:     metadata = []",
      "34:     sparse_matrix = scipy.sparse.random(5, 10, density=0.1, format=\"coo\")",
      "35:     _recursive_serialize(sparse_matrix, \"test_sparse\", data_dict, metadata)",
      "36:     assert \"test_sparse_data\" in data_dict",
      "37:     assert \"test_sparse_row\" in data_dict",
      "38:     assert \"test_sparse_col\" in data_dict",
      "39:     assert metadata[0] == {",
      "40:         \"type\": \"sparse\",",
      "41:         \"key\": \"test_sparse\",",
      "42:         \"shape\": sparse_matrix.shape,",
      "43:     }",
      "46: def test_serialize_model_data(model_data: RasaModelData):",
      "47:     nested_data = model_data.data",
      "49:     data_dict = {}",
      "50:     metadata = []",
      "51:     _serialize_nested_data(nested_data, \"component\", data_dict, metadata)",
      "53:     assert len(metadata) == 5",
      "55:     assert metadata[0][\"key\"] == \"text\"",
      "56:     assert len(metadata[0][\"components\"]) == 1",
      "57:     assert metadata[0][\"components\"][0][\"key\"] == \"sentence\"",
      "58:     assert metadata[0][\"components\"][0][\"number_of_dimensions\"] == 3",
      "59:     assert len(metadata[0][\"components\"][0][\"features\"]) == 2",
      "60:     assert metadata[0][\"components\"][0][\"features\"][0][\"type\"] == \"group\"",
      "61:     assert len(metadata[0][\"components\"][0][\"features\"][0][\"subcomponents\"]) == 5",
      "62:     assert (",
      "63:         metadata[0][\"components\"][0][\"features\"][0][\"subcomponents\"][0][\"type\"]",
      "64:         == \"dense\"",
      "65:     )",
      "66:     assert metadata[0][\"components\"][0][\"features\"][0][\"subcomponents\"][0][\"shape\"] == (",
      "67:         5,",
      "68:         14,",
      "69:     )",
      "70:     assert metadata[0][\"components\"][0][\"features\"][1][\"type\"] == \"group\"",
      "71:     assert len(metadata[0][\"components\"][0][\"features\"][1][\"subcomponents\"]) == 5",
      "72:     assert (",
      "73:         metadata[0][\"components\"][0][\"features\"][1][\"subcomponents\"][0][\"type\"]",
      "74:         == \"sparse\"",
      "75:     )",
      "76:     assert metadata[0][\"components\"][0][\"features\"][1][\"subcomponents\"][0][\"shape\"] == (",
      "77:         5,",
      "78:         10,",
      "79:     )",
      "81:     assert metadata[3][\"key\"] == \"label\"",
      "82:     assert len(metadata[3][\"components\"]) == 1",
      "83:     assert metadata[3][\"components\"][0][\"key\"] == \"ids\"",
      "84:     assert metadata[3][\"components\"][0][\"number_of_dimensions\"] == 1",
      "85:     assert metadata[3][\"components\"][0][\"features\"][0][\"type\"] == \"list\"",
      "86:     assert (",
      "87:         metadata[3][\"components\"][0][\"features\"][0][\"key\"]",
      "88:         == \"component_label_ids_0_list\"",
      "89:     )",
      "91:     assert len(data_dict) == 87",
      "92:     assert (",
      "93:         data_dict[\"component_label_ids_0_list\"]",
      "94:         == model_data.data[\"label\"][\"ids\"][0].view(np.ndarray)",
      "95:     ).all()",
      "98: def test_serialize_and_deserialize_model_data(model_data: RasaModelData):",
      "99:     actual_data = model_data.data",
      "101:     data_dict = {}",
      "102:     metadata = []",
      "103:     _serialize_nested_data(actual_data, \"component\", data_dict, metadata)",
      "105:     loaded_data = _deserialize_nested_data(metadata, data_dict)",
      "107:     assert len(actual_data) == len(loaded_data)",
      "109:     assert len(actual_data[\"text\"][\"sentence\"]) == len(loaded_data[\"text\"][\"sentence\"])",
      "111:     # text.sentence has a dimension of 3",
      "112:     assert len(actual_data[\"text\"][\"sentence\"][0]) == len(",
      "113:         loaded_data[\"text\"][\"sentence\"][0]",
      "114:     )",
      "115:     # assert that the numpy arrays of the actual and loaded data in",
      "116:     # text.sentence are the same",
      "117:     for i in range(0, 5):",
      "118:         assert (",
      "119:             actual_data[\"text\"][\"sentence\"][0][i]",
      "120:             == loaded_data[\"text\"][\"sentence\"][0][i]",
      "121:         ).all()",
      "122:     assert len(actual_data[\"text\"][\"sentence\"][1]) == len(",
      "123:         loaded_data[\"text\"][\"sentence\"][1]",
      "124:     )",
      "125:     # assert that the sparse matrices of the actual and loaded data in",
      "126:     # text.sentence are the same",
      "127:     for i in range(0, 5):",
      "128:         assert (",
      "129:             actual_data[\"text\"][\"sentence\"][1][i]",
      "130:             == loaded_data[\"text\"][\"sentence\"][1][i]",
      "131:         ).data.all()",
      "133:     # action_text.sequence has a dimension of 4",
      "134:     assert len(actual_data[\"action_text\"][\"sequence\"]) == len(",
      "135:         loaded_data[\"action_text\"][\"sequence\"]",
      "136:     )",
      "137:     assert len(actual_data[\"action_text\"][\"sequence\"][0]) == len(",
      "138:         loaded_data[\"action_text\"][\"sequence\"][0]",
      "139:     )",
      "140:     # assert that the sparse matrices of the actual and loaded data in",
      "141:     # action_text.sequence are the same",
      "142:     for i in range(0, 5):",
      "143:         for j in range(0, len(actual_data[\"action_text\"][\"sequence\"][0][i])):",
      "144:             assert (",
      "145:                 actual_data[\"action_text\"][\"sequence\"][0][i][j]",
      "146:                 == loaded_data[\"action_text\"][\"sequence\"][0][i][j]",
      "147:             ).data.all()",
      "148:     assert len(actual_data[\"action_text\"][\"sequence\"][1]) == len(",
      "149:         loaded_data[\"action_text\"][\"sequence\"][1]",
      "150:     )",
      "151:     # assert that the numpy array of the actual and loaded data in",
      "152:     # action_text.sequence are the same",
      "153:     for i in range(0, 5):",
      "154:         for j in range(0, len(actual_data[\"action_text\"][\"sequence\"][1][i])):",
      "155:             assert (",
      "156:                 actual_data[\"action_text\"][\"sequence\"][1][i][j]",
      "157:                 == loaded_data[\"action_text\"][\"sequence\"][1][i][j]",
      "158:             ).all()",
      "160:     # dialogue.sentence has a dimension of 3",
      "161:     assert len(actual_data[\"dialogue\"][\"sentence\"]) == len(",
      "162:         loaded_data[\"dialogue\"][\"sentence\"]",
      "163:     )",
      "164:     assert len(actual_data[\"dialogue\"][\"sentence\"][0]) == len(",
      "165:         loaded_data[\"dialogue\"][\"sentence\"][0]",
      "166:     )",
      "167:     # assert that the numpy array of the actual and loaded data in",
      "168:     # dialogue.sentence are the same",
      "169:     for i in range(0, 5):",
      "170:         assert (",
      "171:             actual_data[\"dialogue\"][\"sentence\"][0][i]",
      "172:             == loaded_data[\"dialogue\"][\"sentence\"][0][i]",
      "173:         ).all()",
      "175:     # label.ids has a dimension of 4",
      "176:     assert len(actual_data[\"label\"][\"ids\"]) == len(loaded_data[\"label\"][\"ids\"])",
      "177:     # assert that the numpy array of the actual and loaded data in",
      "178:     # label.ids are the same",
      "179:     assert (",
      "180:         actual_data[\"label\"][\"ids\"][0].view(np.ndarray)",
      "181:         == loaded_data[\"label\"][\"ids\"][0].view(np.ndarray)",
      "182:     ).all()",
      "184:     # entities.tag_ids has a dimension of 3",
      "185:     assert len(actual_data[\"entities\"][\"tag_ids\"]) == len(",
      "186:         loaded_data[\"entities\"][\"tag_ids\"]",
      "187:     )",
      "188:     assert len(actual_data[\"entities\"][\"tag_ids\"][0]) == len(",
      "189:         loaded_data[\"entities\"][\"tag_ids\"][0]",
      "190:     )",
      "191:     # assert that the numpy array of the actual and loaded data in",
      "192:     # entities.tag_ids are the same",
      "193:     for i in range(0, 5):",
      "194:         assert (",
      "195:             actual_data[\"entities\"][\"tag_ids\"][0][i]",
      "196:             == loaded_data[\"entities\"][\"tag_ids\"][0][i]",
      "197:         ).all()",
      "",
      "---------------"
    ],
    "tests/utils/test_io.py||tests/utils/test_io.py": [
      "File: tests/utils/test_io.py -> tests/utils/test_io.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "3: import pytest",
      "4: from _pytest.tmpdir import TempPathFactory",
      "5: from prompt_toolkit.document import Document",
      "",
      "[Removed Lines]",
      "1: from pathlib import Path",
      "2: from typing import Dict, Text",
      "",
      "[Added Lines]",
      "[None]",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "71:     assert e.value.message == error_message",
      "90: def test_empty_directories_are_equal(tmp_path_factory: TempPathFactory):",
      "91:     dir1 = tmp_path_factory.mktemp(\"dir1\")",
      "92:     dir2 = tmp_path_factory.mktemp(\"dir2\")",
      "",
      "[Removed Lines]",
      "74: @pytest.mark.parametrize(",
      "75:     \"input,kwargs,expected\",",
      "76:     [",
      "77:         ({(1, 2): 3}, {}, {repr((1, 2)): 3}),",
      "78:         ({(1, 2): 3}, {\"encode_non_string_keys\": True}, {(1, 2): 3}),",
      "79:     ],",
      "80: )",
      "81: def test_write_and_load_dict_via_jsonpickle(",
      "82:     tmp_path: Path, input: Dict, kwargs: Dict[Text, bool], expected: Dict",
      "83: ):",
      "84:     file_name = tmp_path / \"bla.pkl\"",
      "85:     rasa.utils.io.json_pickle(file_name=file_name, obj=input, **kwargs)",
      "86:     loaded = rasa.utils.io.json_unpickle(file_name=file_name, **kwargs)",
      "87:     assert loaded == expected",
      "",
      "[Added Lines]",
      "[None]",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "3a6ddaaef686540e8965918c80d108bf95a2f351",
      "candidate_info": {
        "commit_hash": "3a6ddaaef686540e8965918c80d108bf95a2f351",
        "repo": "RasaHQ/rasa",
        "commit_url": "https://github.com/RasaHQ/rasa/commit/3a6ddaaef686540e8965918c80d108bf95a2f351",
        "files": [
          "poetry.lock",
          "pyproject.toml"
        ],
        "message": "Update poetry lock for 1.8.2",
        "before_after_code_files": [
          "poetry.lock||poetry.lock"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "poetry.lock||poetry.lock"
          ],
          "candidate": [
            "poetry.lock||poetry.lock"
          ]
        }
      },
      "candidate_diff": {
        "poetry.lock||poetry.lock": [
          "File: poetry.lock -> poetry.lock",
          "--- Hunk 1 ---",
          "[Context before]",
          "3: [[package]]",
          "4: name = \"absl-py\"",
          "5: version = \"1.4.0\"",
          "6: description = \"Abseil Python Common Libraries, see https://github.com/abseil/abseil-py.\"",
          "8: optional = false",
          "9: python-versions = \">=3.6\"",
          "10: files = [",
          "",
          "[Removed Lines]",
          "1: # This file is automatically @generated by Poetry 1.4.2 and should not be changed by hand.",
          "7: category = \"main\"",
          "",
          "[Added Lines]",
          "1: # This file is automatically @generated by Poetry 1.8.2 and should not be changed by hand.",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "16: name = \"aio-pika\"",
          "17: version = \"8.2.3\"",
          "18: description = \"Wrapper for the aiormq for asyncio and humans.\"",
          "20: optional = false",
          "21: python-versions = \">3.6, <4\"",
          "22: files = [",
          "",
          "[Removed Lines]",
          "19: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "35: name = \"aiofiles\"",
          "36: version = \"23.1.0\"",
          "37: description = \"File support for asyncio.\"",
          "39: optional = false",
          "40: python-versions = \">=3.7,<4.0\"",
          "41: files = [",
          "",
          "[Removed Lines]",
          "38: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "47: name = \"aiogram\"",
          "48: version = \"2.15\"",
          "49: description = \"Is a pretty simple and fully asynchronous framework for Telegram Bot API\"",
          "51: optional = false",
          "52: python-versions = \"*\"",
          "53: files = [",
          "",
          "[Removed Lines]",
          "50: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "68: name = \"aiohttp\"",
          "69: version = \"3.9.3\"",
          "70: description = \"Async http client/server framework (asyncio)\"",
          "72: optional = false",
          "73: python-versions = \">=3.8\"",
          "74: files = [",
          "",
          "[Removed Lines]",
          "71: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "165: name = \"aiohttp-retry\"",
          "166: version = \"2.8.3\"",
          "167: description = \"Simple retry client for aiohttp\"",
          "169: optional = false",
          "170: python-versions = \">=3.7\"",
          "171: files = [",
          "",
          "[Removed Lines]",
          "168: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "180: name = \"aioresponses\"",
          "181: version = \"0.7.6\"",
          "182: description = \"Mock out requests made by ClientSession from aiohttp package\"",
          "184: optional = false",
          "185: python-versions = \"*\"",
          "186: files = [",
          "",
          "[Removed Lines]",
          "183: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "195: name = \"aiormq\"",
          "196: version = \"6.4.2\"",
          "197: description = \"Pure python AMQP asynchronous client library\"",
          "199: optional = false",
          "200: python-versions = \">=3.7\"",
          "201: files = [",
          "",
          "[Removed Lines]",
          "198: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "214: name = \"aiosignal\"",
          "215: version = \"1.3.1\"",
          "216: description = \"aiosignal: a list of registered asynchronous callbacks\"",
          "218: optional = false",
          "219: python-versions = \">=3.7\"",
          "220: files = [",
          "",
          "[Removed Lines]",
          "217: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "229: name = \"analytics-python\"",
          "230: version = \"1.4.post1\"",
          "231: description = \"The hassle-free way to integrate analytics into any python application.\"",
          "233: optional = false",
          "234: python-versions = \"*\"",
          "235: files = [",
          "",
          "[Removed Lines]",
          "232: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 11 ---",
          "[Context before]",
          "251: name = \"anyio\"",
          "252: version = \"3.7.1\"",
          "253: description = \"High level compatibility layer for multiple asynchronous event loop implementations\"",
          "255: optional = false",
          "256: python-versions = \">=3.7\"",
          "257: files = [",
          "",
          "[Removed Lines]",
          "254: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 12 ---",
          "[Context before]",
          "273: name = \"apscheduler\"",
          "274: version = \"3.9.1.post1\"",
          "275: description = \"In-process task scheduler with Cron-like capabilities\"",
          "277: optional = false",
          "278: python-versions = \"!=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*, !=3.4.*, <4\"",
          "279: files = [",
          "",
          "[Removed Lines]",
          "276: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 13 ---",
          "[Context before]",
          "285: pytz = \"*\"",
          "286: setuptools = \">=0.7\"",
          "287: six = \">=1.4.0\"",
          "290: [package.extras]",
          "291: asyncio = [\"trollius\"]",
          "",
          "[Removed Lines]",
          "288: tzlocal = \">=2.0,<3.0.0 || >=4.0.0\"",
          "",
          "[Added Lines]",
          "276: tzlocal = \">=2.0,<3.dev0 || >=4.dev0\"",
          "",
          "---------------",
          "--- Hunk 14 ---",
          "[Context before]",
          "304: name = \"astunparse\"",
          "305: version = \"1.6.3\"",
          "306: description = \"An AST unparser for Python\"",
          "308: optional = false",
          "309: python-versions = \"*\"",
          "310: files = [",
          "",
          "[Removed Lines]",
          "307: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 15 ---",
          "[Context before]",
          "320: name = \"async-generator\"",
          "321: version = \"1.10\"",
          "322: description = \"Async generators and context managers for Python 3.5+\"",
          "324: optional = false",
          "325: python-versions = \">=3.5\"",
          "326: files = [",
          "",
          "[Removed Lines]",
          "323: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 16 ---",
          "[Context before]",
          "332: name = \"async-timeout\"",
          "333: version = \"4.0.2\"",
          "334: description = \"Timeout context manager for asyncio programs\"",
          "336: optional = false",
          "337: python-versions = \">=3.6\"",
          "338: files = [",
          "",
          "[Removed Lines]",
          "335: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 17 ---",
          "[Context before]",
          "344: name = \"attrs\"",
          "345: version = \"22.1.0\"",
          "346: description = \"Classes Without Boilerplate\"",
          "348: optional = false",
          "349: python-versions = \">=3.5\"",
          "350: files = [",
          "",
          "[Removed Lines]",
          "347: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 18 ---",
          "[Context before]",
          "362: name = \"azure-core\"",
          "363: version = \"1.27.1\"",
          "364: description = \"Microsoft Azure Core Library for Python\"",
          "366: optional = false",
          "367: python-versions = \">=3.7\"",
          "368: files = [",
          "",
          "[Removed Lines]",
          "365: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 19 ---",
          "[Context before]",
          "382: name = \"azure-storage-blob\"",
          "383: version = \"12.15.0\"",
          "384: description = \"Microsoft Azure Blob Storage Client Library for Python\"",
          "386: optional = false",
          "387: python-versions = \">=3.7\"",
          "388: files = [",
          "",
          "[Removed Lines]",
          "385: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 20 ---",
          "[Context before]",
          "403: name = \"babel\"",
          "404: version = \"2.9.1\"",
          "405: description = \"Internationalization utilities\"",
          "407: optional = false",
          "408: python-versions = \">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*\"",
          "409: files = [",
          "",
          "[Removed Lines]",
          "406: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 21 ---",
          "[Context before]",
          "418: name = \"backoff\"",
          "419: version = \"1.10.0\"",
          "420: description = \"Function decoration for backoff and retry\"",
          "422: optional = false",
          "423: python-versions = \">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*, !=3.4.*\"",
          "424: files = [",
          "",
          "[Removed Lines]",
          "421: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 22 ---",
          "[Context before]",
          "430: name = \"backports-zoneinfo\"",
          "431: version = \"0.2.1\"",
          "432: description = \"Backport of the standard library zoneinfo module\"",
          "434: optional = false",
          "435: python-versions = \">=3.6\"",
          "436: files = [",
          "",
          "[Removed Lines]",
          "433: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 23 ---",
          "[Context before]",
          "459: name = \"bandit\"",
          "460: version = \"1.7.5\"",
          "461: description = \"Security oriented static analyser for python code.\"",
          "463: optional = false",
          "464: python-versions = \">=3.7\"",
          "465: files = [",
          "",
          "[Removed Lines]",
          "462: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 24 ---",
          "[Context before]",
          "483: name = \"bidict\"",
          "484: version = \"0.22.1\"",
          "485: description = \"The bidirectional mapping library for Python.\"",
          "487: optional = false",
          "488: python-versions = \">=3.7\"",
          "489: files = [",
          "",
          "[Removed Lines]",
          "486: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 25 ---",
          "[Context before]",
          "500: name = \"black\"",
          "501: version = \"22.12.0\"",
          "502: description = \"The uncompromising code formatter.\"",
          "504: optional = false",
          "505: python-versions = \">=3.7\"",
          "506: files = [",
          "",
          "[Removed Lines]",
          "503: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 26 ---",
          "[Context before]",
          "536: name = \"blis\"",
          "537: version = \"0.7.9\"",
          "538: description = \"The Blis BLAS-like linear algebra library, as a self-contained C-extension.\"",
          "540: optional = true",
          "541: python-versions = \"*\"",
          "542: files = [",
          "",
          "[Removed Lines]",
          "539: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 27 ---",
          "[Context before]",
          "577: name = \"boto3\"",
          "578: version = \"1.27.1\"",
          "579: description = \"The AWS SDK for Python\"",
          "581: optional = false",
          "582: python-versions = \">= 3.7\"",
          "583: files = [",
          "",
          "[Removed Lines]",
          "580: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 28 ---",
          "[Context before]",
          "597: name = \"botocore\"",
          "598: version = \"1.30.1\"",
          "599: description = \"Low-level, data-driven core of boto 3.\"",
          "601: optional = false",
          "602: python-versions = \">= 3.7\"",
          "603: files = [",
          "",
          "[Removed Lines]",
          "600: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 29 ---",
          "[Context before]",
          "617: name = \"cachecontrol\"",
          "618: version = \"0.12.14\"",
          "619: description = \"httplib2 caching for requests\"",
          "621: optional = false",
          "622: python-versions = \">=3.6\"",
          "623: files = [",
          "",
          "[Removed Lines]",
          "620: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 30 ---",
          "[Context before]",
          "637: name = \"cachetools\"",
          "638: version = \"5.3.1\"",
          "639: description = \"Extensible memoizing collections and decorators\"",
          "641: optional = false",
          "642: python-versions = \">=3.7\"",
          "643: files = [",
          "",
          "[Removed Lines]",
          "640: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 31 ---",
          "[Context before]",
          "649: name = \"catalogue\"",
          "650: version = \"2.0.8\"",
          "651: description = \"Super lightweight function registries for your library\"",
          "653: optional = true",
          "654: python-versions = \">=3.6\"",
          "655: files = [",
          "",
          "[Removed Lines]",
          "652: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 32 ---",
          "[Context before]",
          "661: name = \"certifi\"",
          "662: version = \"2023.7.22\"",
          "663: description = \"Python package for providing Mozilla's CA Bundle.\"",
          "665: optional = false",
          "666: python-versions = \">=3.6\"",
          "667: files = [",
          "",
          "[Removed Lines]",
          "664: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 33 ---",
          "[Context before]",
          "673: name = \"cffi\"",
          "674: version = \"1.15.1\"",
          "675: description = \"Foreign Function Interface for Python calling C code.\"",
          "677: optional = false",
          "678: python-versions = \"*\"",
          "679: files = [",
          "",
          "[Removed Lines]",
          "676: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 34 ---",
          "[Context before]",
          "750: name = \"charset-normalizer\"",
          "751: version = \"3.1.0\"",
          "752: description = \"The Real First Universal Charset Detector. Open, modern and actively maintained alternative to Chardet.\"",
          "754: optional = false",
          "755: python-versions = \">=3.7.0\"",
          "756: files = [",
          "",
          "[Removed Lines]",
          "753: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 35 ---",
          "[Context before]",
          "835: name = \"click\"",
          "836: version = \"8.1.3\"",
          "837: description = \"Composable command line interface toolkit\"",
          "839: optional = false",
          "840: python-versions = \">=3.7\"",
          "841: files = [",
          "",
          "[Removed Lines]",
          "838: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 36 ---",
          "[Context before]",
          "850: name = \"click-default-group\"",
          "851: version = \"1.2.2\"",
          "852: description = \"Extends click.Group to invoke a command without explicit subcommand name\"",
          "854: optional = false",
          "855: python-versions = \"*\"",
          "856: files = [",
          "",
          "[Removed Lines]",
          "853: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 37 ---",
          "[Context before]",
          "864: name = \"cloudpickle\"",
          "865: version = \"2.2.1\"",
          "866: description = \"Extended pickling support for Python objects\"",
          "868: optional = false",
          "869: python-versions = \">=3.6\"",
          "870: files = [",
          "",
          "[Removed Lines]",
          "867: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 38 ---",
          "[Context before]",
          "876: name = \"colorama\"",
          "877: version = \"0.4.6\"",
          "878: description = \"Cross-platform colored terminal text.\"",
          "880: optional = false",
          "881: python-versions = \"!=3.0.*,!=3.1.*,!=3.2.*,!=3.3.*,!=3.4.*,!=3.5.*,!=3.6.*,>=2.7\"",
          "882: files = [",
          "",
          "[Removed Lines]",
          "879: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 39 ---",
          "[Context before]",
          "888: name = \"colorclass\"",
          "889: version = \"2.2.2\"",
          "890: description = \"Colorful worry-free console applications for Linux, Mac OS X, and Windows.\"",
          "892: optional = false",
          "893: python-versions = \">=2.6\"",
          "894: files = [",
          "",
          "[Removed Lines]",
          "891: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 40 ---",
          "[Context before]",
          "900: name = \"coloredlogs\"",
          "901: version = \"15.0.1\"",
          "902: description = \"Colored terminal output for Python's logging module\"",
          "904: optional = false",
          "905: python-versions = \">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*, !=3.4.*\"",
          "906: files = [",
          "",
          "[Removed Lines]",
          "903: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 41 ---",
          "[Context before]",
          "918: name = \"colorhash\"",
          "919: version = \"1.2.1\"",
          "920: description = \"Generate color based on any object\"",
          "922: optional = false",
          "923: python-versions = \">=3.6,<4.0\"",
          "924: files = [",
          "",
          "[Removed Lines]",
          "921: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 42 ---",
          "[Context before]",
          "930: name = \"confection\"",
          "931: version = \"0.1.0\"",
          "932: description = \"The sweetest config system for Python\"",
          "934: optional = true",
          "935: python-versions = \">=3.6\"",
          "936: files = [",
          "",
          "[Removed Lines]",
          "933: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 43 ---",
          "[Context before]",
          "946: name = \"confluent-kafka\"",
          "947: version = \"2.1.1\"",
          "948: description = \"Confluent's Python client for Apache Kafka\"",
          "950: optional = false",
          "951: python-versions = \"*\"",
          "952: files = [",
          "",
          "[Removed Lines]",
          "949: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 44 ---",
          "[Context before]",
          "993: name = \"coverage\"",
          "994: version = \"6.5.0\"",
          "995: description = \"Code coverage measurement for Python\"",
          "997: optional = false",
          "998: python-versions = \">=3.7\"",
          "999: files = [",
          "",
          "[Removed Lines]",
          "996: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 45 ---",
          "[Context before]",
          "1059: name = \"coveralls\"",
          "1060: version = \"3.3.1\"",
          "1061: description = \"Show coverage stats online via coveralls.io\"",
          "1063: optional = false",
          "1064: python-versions = \">= 3.5\"",
          "1065: files = [",
          "",
          "[Removed Lines]",
          "1062: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 46 ---",
          "[Context before]",
          "1068: ]",
          "1070: [package.dependencies]",
          "1072: docopt = \">=0.6.1\"",
          "1073: requests = \">=1.0.0\"",
          "",
          "[Removed Lines]",
          "1071: coverage = \">=4.1,<6.0.0 || >6.1,<6.1.1 || >6.1.1,<7.0\"",
          "",
          "[Added Lines]",
          "1027: coverage = \">=4.1,<6.0.dev0 || >6.1,<6.1.1 || >6.1.1,<7.0\"",
          "",
          "---------------",
          "--- Hunk 47 ---",
          "[Context before]",
          "1079: name = \"cryptography\"",
          "1080: version = \"41.0.7\"",
          "1081: description = \"cryptography is a package which provides cryptographic recipes and primitives to Python developers.\"",
          "1083: optional = false",
          "1084: python-versions = \">=3.7\"",
          "1085: files = [",
          "",
          "[Removed Lines]",
          "1082: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 48 ---",
          "[Context before]",
          "1125: name = \"cycler\"",
          "1126: version = \"0.11.0\"",
          "1127: description = \"Composable style cycles\"",
          "1129: optional = false",
          "1130: python-versions = \">=3.6\"",
          "1131: files = [",
          "",
          "[Removed Lines]",
          "1128: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 49 ---",
          "[Context before]",
          "1137: name = \"cymem\"",
          "1138: version = \"2.0.7\"",
          "1139: description = \"Manage calls to calloc/free through Cython\"",
          "1141: optional = true",
          "1142: python-versions = \"*\"",
          "1143: files = [",
          "",
          "[Removed Lines]",
          "1140: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 50 ---",
          "[Context before]",
          "1175: name = \"dask\"",
          "1176: version = \"2022.10.2\"",
          "1177: description = \"Parallel PyData with Task Scheduling\"",
          "1179: optional = false",
          "1180: python-versions = \">=3.8\"",
          "1181: files = [",
          "",
          "[Removed Lines]",
          "1178: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 51 ---",
          "[Context before]",
          "1204: name = \"databind\"",
          "1205: version = \"1.5.3\"",
          "1206: description = \"Databind is a library inspired by jackson-databind to de-/serialize Python dataclasses. The `databind` package will install the full suite of databind packages. Compatible with Python 3.7 and newer.\"",
          "1208: optional = false",
          "1209: python-versions = \">=3.7,<4.0\"",
          "1210: files = [",
          "",
          "[Removed Lines]",
          "1207: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 52 ---",
          "[Context before]",
          "1220: name = \"databind-core\"",
          "1221: version = \"1.5.3\"",
          "1222: description = \"Databind is a library inspired by jackson-databind to de-/serialize Python dataclasses. Compatible with Python 3.7 and newer.\"",
          "1224: optional = false",
          "1225: python-versions = \">=3.7,<4.0\"",
          "1226: files = [",
          "",
          "[Removed Lines]",
          "1223: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 53 ---",
          "[Context before]",
          "1237: name = \"databind-json\"",
          "1238: version = \"1.5.3\"",
          "1239: description = \"De-/serialize Python dataclasses to or from JSON payloads. Compatible with Python 3.7 and newer.\"",
          "1241: optional = false",
          "1242: python-versions = \">=3.7,<4.0\"",
          "1243: files = [",
          "",
          "[Removed Lines]",
          "1240: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 54 ---",
          "[Context before]",
          "1254: name = \"datadog\"",
          "1255: version = \"0.45.0\"",
          "1256: description = \"The Datadog Python library\"",
          "1258: optional = false",
          "1259: python-versions = \"!=3.0.*,!=3.1.*,!=3.2.*,!=3.3.*,!=3.4.*,!=3.5.*,!=3.6.*,>=2.7\"",
          "1260: files = [",
          "",
          "[Removed Lines]",
          "1257: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 55 ---",
          "[Context before]",
          "1269: name = \"datadog-api-client\"",
          "1270: version = \"2.14.0\"",
          "1271: description = \"Collection of all Datadog Public endpoints\"",
          "1273: optional = false",
          "1274: python-versions = \">=3.7\"",
          "1275: files = [",
          "",
          "[Removed Lines]",
          "1272: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 56 ---",
          "[Context before]",
          "1293: name = \"deprecated\"",
          "1294: version = \"1.2.14\"",
          "1295: description = \"Python @deprecated decorator to deprecate old python classes, functions or methods.\"",
          "1297: optional = false",
          "1298: python-versions = \">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*\"",
          "1299: files = [",
          "",
          "[Removed Lines]",
          "1296: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 57 ---",
          "[Context before]",
          "1311: name = \"dnspython\"",
          "1312: version = \"2.3.0\"",
          "1313: description = \"DNS toolkit\"",
          "1315: optional = false",
          "1316: python-versions = \">=3.7,<4.0\"",
          "1317: files = [",
          "",
          "[Removed Lines]",
          "1314: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 58 ---",
          "[Context before]",
          "1332: name = \"docker\"",
          "1333: version = \"6.1.3\"",
          "1334: description = \"A Python library for the Docker Engine API.\"",
          "1336: optional = false",
          "1337: python-versions = \">=3.7\"",
          "1338: files = [",
          "",
          "[Removed Lines]",
          "1335: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 59 ---",
          "[Context before]",
          "1354: name = \"docopt\"",
          "1355: version = \"0.6.2\"",
          "1356: description = \"Pythonic argument parser, that will make you smile\"",
          "1358: optional = false",
          "1359: python-versions = \"*\"",
          "1360: files = [",
          "",
          "[Removed Lines]",
          "1357: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 60 ---",
          "[Context before]",
          "1365: name = \"docspec\"",
          "1366: version = \"2.1.2\"",
          "1367: description = \"Docspec is a JSON object specification for representing API documentation of programming languages.\"",
          "1369: optional = false",
          "1370: python-versions = \">=3.7,<4.0\"",
          "1371: files = [",
          "",
          "[Removed Lines]",
          "1368: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 61 ---",
          "[Context before]",
          "1381: name = \"docspec-python\"",
          "1382: version = \"2.0.2\"",
          "1383: description = \"A parser based on lib2to3 producing docspec data from Python source code.\"",
          "1385: optional = false",
          "1386: python-versions = \">=3.7,<4.0\"",
          "1387: files = [",
          "",
          "[Removed Lines]",
          "1384: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 62 ---",
          "[Context before]",
          "1397: name = \"docstring-parser\"",
          "1398: version = \"0.11\"",
          "1399: description = \"\\\"Parse Python docstrings in reST, Google and Numpydoc format\\\"\"",
          "1401: optional = false",
          "1402: python-versions = \">=3.6\"",
          "1403: files = [",
          "",
          "[Removed Lines]",
          "1400: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 63 ---",
          "[Context before]",
          "1411: name = \"exceptiongroup\"",
          "1412: version = \"1.1.2\"",
          "1413: description = \"Backport of PEP 654 (exception groups)\"",
          "1415: optional = false",
          "1416: python-versions = \">=3.7\"",
          "1417: files = [",
          "",
          "[Removed Lines]",
          "1414: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 64 ---",
          "[Context before]",
          "1426: name = \"execnet\"",
          "1427: version = \"1.9.0\"",
          "1428: description = \"execnet: rapid multi-Python deployment\"",
          "1430: optional = false",
          "1431: python-versions = \">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*, !=3.4.*\"",
          "1432: files = [",
          "",
          "[Removed Lines]",
          "1429: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 65 ---",
          "[Context before]",
          "1441: name = \"fakeredis\"",
          "1442: version = \"2.16.0\"",
          "1443: description = \"Python implementation of redis API, can be used for testing purposes.\"",
          "1445: optional = false",
          "1446: python-versions = \">=3.7,<4.0\"",
          "1447: files = [",
          "",
          "[Removed Lines]",
          "1444: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 66 ---",
          "[Context before]",
          "1461: name = \"fbmessenger\"",
          "1462: version = \"6.0.0\"",
          "1463: description = \"A python library to communicate with the Facebook Messenger API's\"",
          "1465: optional = false",
          "1466: python-versions = \"*\"",
          "1467: files = [",
          "",
          "[Removed Lines]",
          "1464: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 67 ---",
          "[Context before]",
          "1476: name = \"filelock\"",
          "1477: version = \"3.12.2\"",
          "1478: description = \"A platform independent file lock.\"",
          "1480: optional = true",
          "1481: python-versions = \">=3.7\"",
          "1482: files = [",
          "",
          "[Removed Lines]",
          "1479: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 68 ---",
          "[Context before]",
          "1492: name = \"fire\"",
          "1493: version = \"0.5.0\"",
          "1494: description = \"A library for automatically generating command line interfaces.\"",
          "1496: optional = false",
          "1497: python-versions = \"*\"",
          "1498: files = [",
          "",
          "[Removed Lines]",
          "1495: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 69 ---",
          "[Context before]",
          "1507: name = \"flatbuffers\"",
          "1508: version = \"23.5.26\"",
          "1509: description = \"The FlatBuffers serialization format for Python\"",
          "1511: optional = false",
          "1512: python-versions = \"*\"",
          "1513: files = [",
          "",
          "[Removed Lines]",
          "1510: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 70 ---",
          "[Context before]",
          "1519: name = \"fonttools\"",
          "1520: version = \"4.40.0\"",
          "1521: description = \"Tools to manipulate font files\"",
          "1523: optional = false",
          "1524: python-versions = \">=3.8\"",
          "1525: files = [",
          "",
          "[Removed Lines]",
          "1522: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 71 ---",
          "[Context before]",
          "1577: name = \"freezegun\"",
          "1578: version = \"1.2.2\"",
          "1579: description = \"Let your Python tests travel through time\"",
          "1581: optional = false",
          "1582: python-versions = \">=3.6\"",
          "1583: files = [",
          "",
          "[Removed Lines]",
          "1580: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 72 ---",
          "[Context before]",
          "1592: name = \"frozenlist\"",
          "1593: version = \"1.3.3\"",
          "1594: description = \"A list-like structure which implements collections.abc.MutableSequence\"",
          "1596: optional = false",
          "1597: python-versions = \">=3.7\"",
          "1598: files = [",
          "",
          "[Removed Lines]",
          "1595: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 73 ---",
          "[Context before]",
          "1676: name = \"fsspec\"",
          "1677: version = \"2023.6.0\"",
          "1678: description = \"File-system specification\"",
          "1680: optional = false",
          "1681: python-versions = \">=3.8\"",
          "1682: files = [",
          "",
          "[Removed Lines]",
          "1679: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 74 ---",
          "[Context before]",
          "1712: name = \"future\"",
          "1713: version = \"0.18.3\"",
          "1714: description = \"Clean single-source support for Python 3 and 2\"",
          "1716: optional = false",
          "1717: python-versions = \">=2.6, !=3.0.*, !=3.1.*, !=3.2.*\"",
          "1718: files = [",
          "",
          "[Removed Lines]",
          "1715: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 75 ---",
          "[Context before]",
          "1723: name = \"gast\"",
          "1724: version = \"0.4.0\"",
          "1725: description = \"Python AST that abstracts the underlying Python version\"",
          "1727: optional = false",
          "1728: python-versions = \">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*\"",
          "1729: files = [",
          "",
          "[Removed Lines]",
          "1726: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 76 ---",
          "[Context before]",
          "1735: name = \"gitdb\"",
          "1736: version = \"4.0.10\"",
          "1737: description = \"Git Object Database\"",
          "1739: optional = false",
          "1740: python-versions = \">=3.7\"",
          "1741: files = [",
          "",
          "[Removed Lines]",
          "1738: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 77 ---",
          "[Context before]",
          "1750: name = \"github3-py\"",
          "1751: version = \"3.2.0\"",
          "1752: description = \"Python wrapper for the GitHub API(http://developer.github.com/v3)\"",
          "1754: optional = true",
          "1755: python-versions = \">=3.6\"",
          "1756: files = [",
          "",
          "[Removed Lines]",
          "1753: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 78 ---",
          "[Context before]",
          "1768: name = \"gitpython\"",
          "1769: version = \"3.1.31\"",
          "1770: description = \"GitPython is a Python library used to interact with Git repositories\"",
          "1772: optional = false",
          "1773: python-versions = \">=3.7\"",
          "1774: files = [",
          "",
          "[Removed Lines]",
          "1771: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 79 ---",
          "[Context before]",
          "1783: name = \"google-api-core\"",
          "1784: version = \"2.11.1\"",
          "1785: description = \"Google API client core library\"",
          "1787: optional = false",
          "1788: python-versions = \">=3.7\"",
          "1789: files = [",
          "",
          "[Removed Lines]",
          "1786: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 80 ---",
          "[Context before]",
          "1806: name = \"google-auth\"",
          "1807: version = \"2.21.0\"",
          "1808: description = \"Google Authentication Library\"",
          "1810: optional = false",
          "1811: python-versions = \">=3.6\"",
          "1812: files = [",
          "",
          "[Removed Lines]",
          "1809: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 81 ---",
          "[Context before]",
          "1832: name = \"google-auth-oauthlib\"",
          "1833: version = \"1.0.0\"",
          "1834: description = \"Google Authentication Library\"",
          "1836: optional = false",
          "1837: python-versions = \">=3.6\"",
          "1838: files = [",
          "",
          "[Removed Lines]",
          "1835: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 82 ---",
          "[Context before]",
          "1851: name = \"google-cloud-core\"",
          "1852: version = \"2.3.3\"",
          "1853: description = \"Google Cloud API client core library\"",
          "1855: optional = false",
          "1856: python-versions = \">=3.7\"",
          "1857: files = [",
          "",
          "[Removed Lines]",
          "1854: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 83 ---",
          "[Context before]",
          "1860: ]",
          "1862: [package.dependencies]",
          "1864: google-auth = \">=1.25.0,<3.0dev\"",
          "1866: [package.extras]",
          "",
          "[Removed Lines]",
          "1863: google-api-core = \">=1.31.6,<2.0.0 || >2.3.0,<3.0.0dev\"",
          "",
          "[Added Lines]",
          "1783: google-api-core = \">=1.31.6,<2.0.dev0 || >2.3.0,<3.0.0dev\"",
          "",
          "---------------",
          "--- Hunk 84 ---",
          "[Context before]",
          "1870: name = \"google-cloud-storage\"",
          "1871: version = \"2.10.0\"",
          "1872: description = \"Google Cloud Storage API client library\"",
          "1874: optional = false",
          "1875: python-versions = \">=3.7\"",
          "1876: files = [",
          "",
          "[Removed Lines]",
          "1873: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 85 ---",
          "[Context before]",
          "1879: ]",
          "1881: [package.dependencies]",
          "1883: google-auth = \">=1.25.0,<3.0dev\"",
          "1884: google-cloud-core = \">=2.3.0,<3.0dev\"",
          "1885: google-resumable-media = \">=2.3.2\"",
          "",
          "[Removed Lines]",
          "1882: google-api-core = \">=1.31.5,<2.0.0 || >2.3.0,<3.0.0dev\"",
          "",
          "[Added Lines]",
          "1801: google-api-core = \">=1.31.5,<2.0.dev0 || >2.3.0,<3.0.0dev\"",
          "",
          "---------------",
          "--- Hunk 86 ---",
          "[Context before]",
          "1892: name = \"google-crc32c\"",
          "1893: version = \"1.5.0\"",
          "1894: description = \"A python wrapper of the C library 'Google CRC32C'\"",
          "1896: optional = false",
          "1897: python-versions = \">=3.7\"",
          "1898: files = [",
          "",
          "[Removed Lines]",
          "1895: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 87 ---",
          "[Context before]",
          "1973: name = \"google-pasta\"",
          "1974: version = \"0.2.0\"",
          "1975: description = \"pasta is an AST-based Python refactoring library\"",
          "1977: optional = false",
          "1978: python-versions = \"*\"",
          "1979: files = [",
          "",
          "[Removed Lines]",
          "1976: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 88 ---",
          "[Context before]",
          "1989: name = \"google-resumable-media\"",
          "1990: version = \"2.5.0\"",
          "1991: description = \"Utilities for Google Media Downloads and Resumable Uploads\"",
          "1993: optional = false",
          "1994: python-versions = \">= 3.7\"",
          "1995: files = [",
          "",
          "[Removed Lines]",
          "1992: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 89 ---",
          "[Context before]",
          "2008: name = \"googleapis-common-protos\"",
          "2009: version = \"1.59.1\"",
          "2010: description = \"Common protobufs used in Google APIs\"",
          "2012: optional = false",
          "2013: python-versions = \">=3.7\"",
          "2014: files = [",
          "",
          "[Removed Lines]",
          "2011: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 90 ---",
          "[Context before]",
          "2026: name = \"greenlet\"",
          "2027: version = \"2.0.2\"",
          "2028: description = \"Lightweight in-process concurrent programming\"",
          "2030: optional = false",
          "2031: python-versions = \">=2.7,!=3.0.*,!=3.1.*,!=3.2.*,!=3.3.*,!=3.4.*\"",
          "2032: files = [",
          "",
          "[Removed Lines]",
          "2029: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 91 ---",
          "[Context before]",
          "2104: name = \"grpcio\"",
          "2105: version = \"1.56.0\"",
          "2106: description = \"HTTP/2-based RPC framework\"",
          "2108: optional = false",
          "2109: python-versions = \">=3.7\"",
          "2110: files = [",
          "",
          "[Removed Lines]",
          "2107: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 92 ---",
          "[Context before]",
          "2162: name = \"h11\"",
          "2163: version = \"0.14.0\"",
          "2164: description = \"A pure-Python, bring-your-own-I/O implementation of HTTP/1.1\"",
          "2166: optional = false",
          "2167: python-versions = \">=3.7\"",
          "2168: files = [",
          "",
          "[Removed Lines]",
          "2165: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 93 ---",
          "[Context before]",
          "2174: name = \"h5py\"",
          "2175: version = \"3.9.0\"",
          "2176: description = \"Read and write HDF5 files from Python\"",
          "2178: optional = false",
          "2179: python-versions = \">=3.8\"",
          "2180: files = [",
          "",
          "[Removed Lines]",
          "2177: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 94 ---",
          "[Context before]",
          "2208: name = \"httpcore\"",
          "2209: version = \"0.16.3\"",
          "2210: description = \"A minimal low-level HTTP client.\"",
          "2212: optional = false",
          "2213: python-versions = \">=3.7\"",
          "2214: files = [",
          "",
          "[Removed Lines]",
          "2211: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 95 ---",
          "[Context before]",
          "2220: anyio = \">=3.0,<5.0\"",
          "2221: certifi = \"*\"",
          "2222: h11 = \">=0.13,<0.15\"",
          "2225: [package.extras]",
          "2226: http2 = [\"h2 (>=3,<5)\"]",
          "2229: [[package]]",
          "2230: name = \"httptools\"",
          "2231: version = \"0.5.0\"",
          "2232: description = \"A collection of framework independent HTTP protocol utils.\"",
          "2234: optional = false",
          "2235: python-versions = \">=3.5.0\"",
          "2236: files = [",
          "",
          "[Removed Lines]",
          "2223: sniffio = \">=1.0.0,<2.0.0\"",
          "2227: socks = [\"socksio (>=1.0.0,<2.0.0)\"]",
          "2233: category = \"main\"",
          "",
          "[Added Lines]",
          "2133: sniffio = \"==1.*\"",
          "2137: socks = [\"socksio (==1.*)\"]",
          "",
          "---------------",
          "--- Hunk 96 ---",
          "[Context before]",
          "2284: name = \"httpx\"",
          "2285: version = \"0.23.3\"",
          "2286: description = \"The next generation HTTP client.\"",
          "2288: optional = false",
          "2289: python-versions = \">=3.7\"",
          "2290: files = [",
          "",
          "[Removed Lines]",
          "2287: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 97 ---",
          "[Context before]",
          "2301: [package.extras]",
          "2302: brotli = [\"brotli\", \"brotlicffi\"]",
          "2304: http2 = [\"h2 (>=3,<5)\"]",
          "2307: [[package]]",
          "2308: name = \"huggingface-hub\"",
          "2309: version = \"0.16.2\"",
          "2310: description = \"Client library to download and publish models, datasets and other repos on the huggingface.co hub\"",
          "2312: optional = true",
          "2313: python-versions = \">=3.7.0\"",
          "2314: files = [",
          "",
          "[Removed Lines]",
          "2303: cli = [\"click (>=8.0.0,<9.0.0)\", \"pygments (>=2.0.0,<3.0.0)\", \"rich (>=10,<13)\"]",
          "2305: socks = [\"socksio (>=1.0.0,<2.0.0)\"]",
          "2311: category = \"main\"",
          "",
          "[Added Lines]",
          "2211: cli = [\"click (==8.*)\", \"pygments (==2.*)\", \"rich (>=10,<13)\"]",
          "2213: socks = [\"socksio (==1.*)\"]",
          "",
          "---------------",
          "--- Hunk 98 ---",
          "[Context before]",
          "2341: name = \"humanfriendly\"",
          "2342: version = \"10.0\"",
          "2343: description = \"Human friendly output for text interfaces using Python\"",
          "2345: optional = false",
          "2346: python-versions = \">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*, !=3.4.*\"",
          "2347: files = [",
          "",
          "[Removed Lines]",
          "2344: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 99 ---",
          "[Context before]",
          "2356: name = \"idna\"",
          "2357: version = \"3.4\"",
          "2358: description = \"Internationalized Domain Names in Applications (IDNA)\"",
          "2360: optional = false",
          "2361: python-versions = \">=3.5\"",
          "2362: files = [",
          "",
          "[Removed Lines]",
          "2359: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 100 ---",
          "[Context before]",
          "2368: name = \"importlib-metadata\"",
          "2369: version = \"6.7.0\"",
          "2370: description = \"Read metadata from Python packages\"",
          "2372: optional = false",
          "2373: python-versions = \">=3.7\"",
          "2374: files = [",
          "",
          "[Removed Lines]",
          "2371: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 101 ---",
          "[Context before]",
          "2388: name = \"importlib-resources\"",
          "2389: version = \"5.12.0\"",
          "2390: description = \"Read resources from Python packages\"",
          "2392: optional = false",
          "2393: python-versions = \">=3.7\"",
          "2394: files = [",
          "",
          "[Removed Lines]",
          "2391: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 102 ---",
          "[Context before]",
          "2407: name = \"incremental\"",
          "2408: version = \"22.10.0\"",
          "2409: description = \"\\\"A small library that versions your Python projects.\\\"\"",
          "2411: optional = false",
          "2412: python-versions = \"*\"",
          "2413: files = [",
          "",
          "[Removed Lines]",
          "2410: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 103 ---",
          "[Context before]",
          "2423: name = \"iniconfig\"",
          "2424: version = \"2.0.0\"",
          "2425: description = \"brain-dead simple config-ini parsing\"",
          "2427: optional = false",
          "2428: python-versions = \">=3.7\"",
          "2429: files = [",
          "",
          "[Removed Lines]",
          "2426: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 104 ---",
          "[Context before]",
          "2435: name = \"isodate\"",
          "2436: version = \"0.6.1\"",
          "2437: description = \"An ISO 8601 date/time/duration parser and formatter\"",
          "2439: optional = false",
          "2440: python-versions = \"*\"",
          "2441: files = [",
          "",
          "[Removed Lines]",
          "2438: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 105 ---",
          "[Context before]",
          "2450: name = \"jax\"",
          "2451: version = \"0.4.13\"",
          "2452: description = \"Differentiate, compile, and transform Numpy code.\"",
          "2454: optional = false",
          "2455: python-versions = \">=3.8\"",
          "2456: files = [",
          "",
          "[Removed Lines]",
          "2453: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 106 ---",
          "[Context before]",
          "2481: name = \"jieba\"",
          "2482: version = \"0.42.1\"",
          "2483: description = \"Chinese Words Segmentation Utilities\"",
          "2485: optional = true",
          "2486: python-versions = \"*\"",
          "2487: files = [",
          "",
          "[Removed Lines]",
          "2484: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 107 ---",
          "[Context before]",
          "2492: name = \"jinja2\"",
          "2493: version = \"3.1.2\"",
          "2494: description = \"A very fast and expressive template engine.\"",
          "2496: optional = false",
          "2497: python-versions = \">=3.7\"",
          "2498: files = [",
          "",
          "[Removed Lines]",
          "2495: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 108 ---",
          "[Context before]",
          "2510: name = \"jmespath\"",
          "2511: version = \"1.0.1\"",
          "2512: description = \"JSON Matching Expressions\"",
          "2514: optional = false",
          "2515: python-versions = \">=3.7\"",
          "2516: files = [",
          "",
          "[Removed Lines]",
          "2513: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 109 ---",
          "[Context before]",
          "2522: name = \"joblib\"",
          "2523: version = \"1.2.0\"",
          "2524: description = \"Lightweight pipelining with Python functions\"",
          "2526: optional = false",
          "2527: python-versions = \">=3.7\"",
          "2528: files = [",
          "",
          "[Removed Lines]",
          "2525: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 110 ---",
          "[Context before]",
          "2534: name = \"jsonpickle\"",
          "2535: version = \"3.0.1\"",
          "2536: description = \"Python library for serializing any arbitrary object graph into JSON\"",
          "2538: optional = false",
          "2539: python-versions = \">=3.7\"",
          "2540: files = [",
          "",
          "[Removed Lines]",
          "2537: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 111 ---",
          "[Context before]",
          "2551: name = \"jsonschema\"",
          "2552: version = \"4.17.3\"",
          "2553: description = \"An implementation of JSON Schema validation for Python\"",
          "2555: optional = false",
          "2556: python-versions = \">=3.7\"",
          "2557: files = [",
          "",
          "[Removed Lines]",
          "2554: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 112 ---",
          "[Context before]",
          "2573: name = \"keras\"",
          "2574: version = \"2.12.0\"",
          "2575: description = \"Deep learning for humans.\"",
          "2577: optional = false",
          "2578: python-versions = \">=3.8\"",
          "2579: files = [",
          "",
          "[Removed Lines]",
          "2576: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 113 ---",
          "[Context before]",
          "2584: name = \"kiwisolver\"",
          "2585: version = \"1.4.4\"",
          "2586: description = \"A fast implementation of the Cassowary constraint solver\"",
          "2588: optional = false",
          "2589: python-versions = \">=3.7\"",
          "2590: files = [",
          "",
          "[Removed Lines]",
          "2587: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 114 ---",
          "[Context before]",
          "2662: name = \"langcodes\"",
          "2663: version = \"3.3.0\"",
          "2664: description = \"Tools for labeling human languages with IETF language tags\"",
          "2666: optional = true",
          "2667: python-versions = \">=3.6\"",
          "2668: files = [",
          "",
          "[Removed Lines]",
          "2665: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 115 ---",
          "[Context before]",
          "2677: name = \"libclang\"",
          "2678: version = \"16.0.0\"",
          "2679: description = \"Clang Python Bindings, mirrored from the official LLVM repo: https://github.com/llvm/llvm-project/tree/main/clang/bindings/python, to make the installation process easier.\"",
          "2681: optional = false",
          "2682: python-versions = \"*\"",
          "2683: files = [",
          "",
          "[Removed Lines]",
          "2680: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 116 ---",
          "[Context before]",
          "2695: name = \"locket\"",
          "2696: version = \"1.0.0\"",
          "2697: description = \"File-based locks for Python on Linux and Windows\"",
          "2699: optional = false",
          "2700: python-versions = \">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*\"",
          "2701: files = [",
          "",
          "[Removed Lines]",
          "2698: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 117 ---",
          "[Context before]",
          "2707: name = \"markdown\"",
          "2708: version = \"3.4.3\"",
          "2709: description = \"Python implementation of John Gruber's Markdown.\"",
          "2711: optional = false",
          "2712: python-versions = \">=3.7\"",
          "2713: files = [",
          "",
          "[Removed Lines]",
          "2710: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 118 ---",
          "[Context before]",
          "2725: name = \"markdown-it-py\"",
          "2726: version = \"3.0.0\"",
          "2727: description = \"Python port of markdown-it. Markdown parsing, done right!\"",
          "2729: optional = false",
          "2730: python-versions = \">=3.8\"",
          "2731: files = [",
          "",
          "[Removed Lines]",
          "2728: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 119 ---",
          "[Context before]",
          "2750: name = \"markupsafe\"",
          "2751: version = \"2.1.3\"",
          "2752: description = \"Safely add untrusted strings to HTML/XML markup.\"",
          "2754: optional = false",
          "2755: python-versions = \">=3.7\"",
          "2756: files = [",
          "",
          "[Removed Lines]",
          "2753: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 120 ---",
          "[Context before]",
          "2820: name = \"matplotlib\"",
          "2821: version = \"3.5.3\"",
          "2822: description = \"Python plotting package\"",
          "2824: optional = false",
          "2825: python-versions = \">=3.7\"",
          "2826: files = [",
          "",
          "[Removed Lines]",
          "2823: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 121 ---",
          "[Context before]",
          "2875: name = \"mattermostwrapper\"",
          "2876: version = \"2.2\"",
          "2877: description = \"A mattermost api v4 wrapper to interact with api\"",
          "2879: optional = false",
          "2880: python-versions = \"*\"",
          "2881: files = [",
          "",
          "[Removed Lines]",
          "2878: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 122 ---",
          "[Context before]",
          "2889: name = \"mdurl\"",
          "2890: version = \"0.1.2\"",
          "2891: description = \"Markdown URL utilities\"",
          "2893: optional = false",
          "2894: python-versions = \">=3.7\"",
          "2895: files = [",
          "",
          "[Removed Lines]",
          "2892: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 123 ---",
          "[Context before]",
          "2901: name = \"memory-profiler\"",
          "2902: version = \"0.61.0\"",
          "2903: description = \"A module for monitoring memory usage of a python program\"",
          "2905: optional = false",
          "2906: python-versions = \">=3.5\"",
          "2907: files = [",
          "",
          "[Removed Lines]",
          "2904: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 124 ---",
          "[Context before]",
          "2916: name = \"ml-dtypes\"",
          "2917: version = \"0.2.0\"",
          "2918: description = \"\"",
          "2920: optional = false",
          "2921: python-versions = \">=3.7\"",
          "2922: files = [",
          "",
          "[Removed Lines]",
          "2919: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 125 ---",
          "[Context before]",
          "2942: [package.dependencies]",
          "2943: numpy = [",
          "2945:     {version = \">=1.21.2\", markers = \"python_version > \\\"3.9\\\"\"},",
          "2946: ]",
          "2948: [package.extras]",
          "",
          "[Removed Lines]",
          "2944:     {version = \">1.20\", markers = \"python_version <= \\\"3.9\\\"\"},",
          "",
          "[Added Lines]",
          "2825:     {version = \">1.20\", markers = \"python_version <= \\\"3.9\\\"\"},",
          "",
          "---------------",
          "--- Hunk 126 ---",
          "[Context before]",
          "2952: name = \"mongomock\"",
          "2953: version = \"4.1.2\"",
          "2954: description = \"Fake pymongo stub for testing simple MongoDB-dependent code\"",
          "2956: optional = false",
          "2957: python-versions = \"*\"",
          "2958: files = [",
          "",
          "[Removed Lines]",
          "2955: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 127 ---",
          "[Context before]",
          "2968: name = \"monotonic\"",
          "2969: version = \"1.6\"",
          "2970: description = \"An implementation of time.monotonic() for Python 2 & < 3.3\"",
          "2972: optional = false",
          "2973: python-versions = \"*\"",
          "2974: files = [",
          "",
          "[Removed Lines]",
          "2971: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 128 ---",
          "[Context before]",
          "2980: name = \"moto\"",
          "2981: version = \"4.1.12\"",
          "2982: description = \"\"",
          "2984: optional = false",
          "2985: python-versions = \">=3.7\"",
          "2986: files = [",
          "",
          "[Removed Lines]",
          "2983: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 129 ---",
          "[Context before]",
          "3027: name = \"msgpack\"",
          "3028: version = \"1.0.5\"",
          "3029: description = \"MessagePack serializer\"",
          "3031: optional = false",
          "3032: python-versions = \"*\"",
          "3033: files = [",
          "",
          "[Removed Lines]",
          "3030: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 130 ---",
          "[Context before]",
          "3100: name = \"multidict\"",
          "3101: version = \"5.2.0\"",
          "3102: description = \"multidict implementation\"",
          "3104: optional = false",
          "3105: python-versions = \">=3.6\"",
          "3106: files = [",
          "",
          "[Removed Lines]",
          "3103: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 131 ---",
          "[Context before]",
          "3182: name = \"murmurhash\"",
          "3183: version = \"1.0.9\"",
          "3184: description = \"Cython bindings for MurmurHash\"",
          "3186: optional = true",
          "3187: python-versions = \">=3.6\"",
          "3188: files = [",
          "",
          "[Removed Lines]",
          "3185: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 132 ---",
          "[Context before]",
          "3220: name = \"mypy\"",
          "3221: version = \"1.0.1\"",
          "3222: description = \"Optional static typing for Python\"",
          "3224: optional = false",
          "3225: python-versions = \">=3.7\"",
          "3226: files = [",
          "",
          "[Removed Lines]",
          "3223: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 133 ---",
          "[Context before]",
          "3267: name = \"mypy-extensions\"",
          "3268: version = \"0.4.4\"",
          "3269: description = \"Experimental type system extensions for programs checked with the mypy typechecker.\"",
          "3271: optional = false",
          "3272: python-versions = \">=2.7\"",
          "3273: files = [",
          "",
          "[Removed Lines]",
          "3270: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 134 ---",
          "[Context before]",
          "3278: name = \"networkx\"",
          "3279: version = \"2.6.3\"",
          "3280: description = \"Python package for creating and manipulating graphs and networks\"",
          "3282: optional = false",
          "3283: python-versions = \">=3.7\"",
          "3284: files = [",
          "",
          "[Removed Lines]",
          "3281: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 135 ---",
          "[Context before]",
          "3297: name = \"nr-util\"",
          "3298: version = \"0.8.12\"",
          "3299: description = \"General purpose Python utility library.\"",
          "3301: optional = false",
          "3302: python-versions = \">=3.7,<4.0\"",
          "3303: files = [",
          "",
          "[Removed Lines]",
          "3300: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 136 ---",
          "[Context before]",
          "3313: name = \"numpy\"",
          "3314: version = \"1.22.3\"",
          "3315: description = \"NumPy is the fundamental package for array computing with Python.\"",
          "3317: optional = false",
          "3318: python-versions = \">=3.8\"",
          "3319: files = [",
          "",
          "[Removed Lines]",
          "3316: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 137 ---",
          "[Context before]",
          "3343: name = \"numpy\"",
          "3344: version = \"1.23.5\"",
          "3345: description = \"NumPy is the fundamental package for array computing with Python.\"",
          "3347: optional = false",
          "3348: python-versions = \">=3.8\"",
          "3349: files = [",
          "",
          "[Removed Lines]",
          "3346: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 138 ---",
          "[Context before]",
          "3381: name = \"oauthlib\"",
          "3382: version = \"3.2.2\"",
          "3383: description = \"A generic, spec-compliant, thorough implementation of the OAuth request-signing logic\"",
          "3385: optional = false",
          "3386: python-versions = \">=3.6\"",
          "3387: files = [",
          "",
          "[Removed Lines]",
          "3384: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 139 ---",
          "[Context before]",
          "3398: name = \"opt-einsum\"",
          "3399: version = \"3.3.0\"",
          "3400: description = \"Optimizing numpys einsum function\"",
          "3402: optional = false",
          "3403: python-versions = \">=3.5\"",
          "3404: files = [",
          "",
          "[Removed Lines]",
          "3401: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 140 ---",
          "[Context before]",
          "3417: name = \"packaging\"",
          "3418: version = \"20.9\"",
          "3419: description = \"Core utilities for Python packages\"",
          "3421: optional = false",
          "3422: python-versions = \">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*\"",
          "3423: files = [",
          "",
          "[Removed Lines]",
          "3420: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 141 ---",
          "[Context before]",
          "3432: name = \"pamqp\"",
          "3433: version = \"3.2.1\"",
          "3434: description = \"RabbitMQ Focused AMQP low-level library\"",
          "3436: optional = false",
          "3437: python-versions = \">=3.7\"",
          "3438: files = [",
          "",
          "[Removed Lines]",
          "3435: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 142 ---",
          "[Context before]",
          "3448: name = \"partd\"",
          "3449: version = \"1.4.0\"",
          "3450: description = \"Appendable key-value storage\"",
          "3452: optional = false",
          "3453: python-versions = \">=3.7\"",
          "3454: files = [",
          "",
          "[Removed Lines]",
          "3451: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 143 ---",
          "[Context before]",
          "3467: name = \"pathspec\"",
          "3468: version = \"0.11.1\"",
          "3469: description = \"Utility library for gitignore style pattern matching of file paths.\"",
          "3471: optional = false",
          "3472: python-versions = \">=3.7\"",
          "3473: files = [",
          "",
          "[Removed Lines]",
          "3470: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 144 ---",
          "[Context before]",
          "3479: name = \"pathy\"",
          "3480: version = \"0.10.2\"",
          "3481: description = \"pathlib.Path subclasses for local and cloud bucket storage\"",
          "3483: optional = true",
          "3484: python-versions = \">= 3.6\"",
          "3485: files = [",
          "",
          "[Removed Lines]",
          "3482: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 145 ---",
          "[Context before]",
          "3502: name = \"pbr\"",
          "3503: version = \"5.11.1\"",
          "3504: description = \"Python Build Reasonableness\"",
          "3506: optional = false",
          "3507: python-versions = \">=2.6\"",
          "3508: files = [",
          "",
          "[Removed Lines]",
          "3505: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 146 ---",
          "[Context before]",
          "3514: name = \"pep440-version-utils\"",
          "3515: version = \"0.3.0\"",
          "3516: description = \"Utilities to deal with pep440 versioning\"",
          "3518: optional = false",
          "3519: python-versions = \">=3.6,<4.0\"",
          "3520: files = [",
          "",
          "[Removed Lines]",
          "3517: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 147 ---",
          "[Context before]",
          "3529: name = \"pillow\"",
          "3530: version = \"10.0.1\"",
          "3531: description = \"Python Imaging Library (Fork)\"",
          "3533: optional = false",
          "3534: python-versions = \">=3.8\"",
          "3535: files = [",
          "",
          "[Removed Lines]",
          "3532: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 148 ---",
          "[Context before]",
          "3597: name = \"pkgutil-resolve-name\"",
          "3598: version = \"1.3.10\"",
          "3599: description = \"Resolve a name to an object.\"",
          "3601: optional = false",
          "3602: python-versions = \">=3.6\"",
          "3603: files = [",
          "",
          "[Removed Lines]",
          "3600: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 149 ---",
          "[Context before]",
          "3609: name = \"platformdirs\"",
          "3610: version = \"3.8.0\"",
          "3611: description = \"A small Python package for determining appropriate platform-specific dirs, e.g. a \\\"user data dir\\\".\"",
          "3613: optional = false",
          "3614: python-versions = \">=3.7\"",
          "3615: files = [",
          "",
          "[Removed Lines]",
          "3612: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 150 ---",
          "[Context before]",
          "3625: name = \"pluggy\"",
          "3626: version = \"1.2.0\"",
          "3627: description = \"plugin and hook calling mechanisms for python\"",
          "3629: optional = false",
          "3630: python-versions = \">=3.7\"",
          "3631: files = [",
          "",
          "[Removed Lines]",
          "3628: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 151 ---",
          "[Context before]",
          "3641: name = \"portalocker\"",
          "3642: version = \"2.7.0\"",
          "3643: description = \"Wraps the portalocker recipe for easy usage\"",
          "3645: optional = false",
          "3646: python-versions = \">=3.5\"",
          "3647: files = [",
          "",
          "[Removed Lines]",
          "3644: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 152 ---",
          "[Context before]",
          "3661: name = \"preshed\"",
          "3662: version = \"3.0.8\"",
          "3663: description = \"Cython hash table that trusts the keys are pre-hashed\"",
          "3665: optional = true",
          "3666: python-versions = \">=3.6\"",
          "3667: files = [",
          "",
          "[Removed Lines]",
          "3664: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 153 ---",
          "[Context before]",
          "3703: name = \"prompt-toolkit\"",
          "3704: version = \"3.0.28\"",
          "3705: description = \"Library for building powerful interactive command lines in Python\"",
          "3707: optional = false",
          "3708: python-versions = \">=3.6.2\"",
          "3709: files = [",
          "",
          "[Removed Lines]",
          "3706: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 154 ---",
          "[Context before]",
          "3718: name = \"protobuf\"",
          "3719: version = \"4.23.3\"",
          "3720: description = \"\"",
          "3722: optional = false",
          "3723: python-versions = \">=3.7\"",
          "3724: files = [",
          "",
          "[Removed Lines]",
          "3721: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 155 ---",
          "[Context before]",
          "3741: name = \"psutil\"",
          "3742: version = \"5.9.5\"",
          "3743: description = \"Cross-platform lib for process and system monitoring in Python.\"",
          "3745: optional = false",
          "3746: python-versions = \">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*\"",
          "3747: files = [",
          "",
          "[Removed Lines]",
          "3744: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 156 ---",
          "[Context before]",
          "3768: name = \"psycopg2-binary\"",
          "3769: version = \"2.9.6\"",
          "3770: description = \"psycopg2 - Python-PostgreSQL Database Adapter\"",
          "3772: optional = false",
          "3773: python-versions = \">=3.6\"",
          "3774: files = [",
          "",
          "[Removed Lines]",
          "3771: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 157 ---",
          "[Context before]",
          "3840: name = \"pyasn1\"",
          "3841: version = \"0.5.0\"",
          "3842: description = \"Pure-Python implementation of ASN.1 types and DER/BER/CER codecs (X.208)\"",
          "3844: optional = false",
          "3845: python-versions = \"!=3.0.*,!=3.1.*,!=3.2.*,!=3.3.*,!=3.4.*,!=3.5.*,>=2.7\"",
          "3846: files = [",
          "",
          "[Removed Lines]",
          "3843: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 158 ---",
          "[Context before]",
          "3852: name = \"pyasn1-modules\"",
          "3853: version = \"0.3.0\"",
          "3854: description = \"A collection of ASN.1-based protocols modules\"",
          "3856: optional = false",
          "3857: python-versions = \"!=3.0.*,!=3.1.*,!=3.2.*,!=3.3.*,!=3.4.*,!=3.5.*,>=2.7\"",
          "3858: files = [",
          "",
          "[Removed Lines]",
          "3855: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 159 ---",
          "[Context before]",
          "3867: name = \"pycparser\"",
          "3868: version = \"2.21\"",
          "3869: description = \"C parser in Python\"",
          "3871: optional = false",
          "3872: python-versions = \">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*\"",
          "3873: files = [",
          "",
          "[Removed Lines]",
          "3870: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 160 ---",
          "[Context before]",
          "3879: name = \"pydantic\"",
          "3880: version = \"1.10.9\"",
          "3881: description = \"Data validation and settings management using python type hints\"",
          "3883: optional = true",
          "3884: python-versions = \">=3.7\"",
          "3885: files = [",
          "",
          "[Removed Lines]",
          "3882: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 161 ---",
          "[Context before]",
          "3932: name = \"pydoc-markdown\"",
          "3933: version = \"4.7.0\"",
          "3934: description = \"Create Python API documentation in Markdown format.\"",
          "3936: optional = false",
          "3937: python-versions = \">=3.7,<4.0\"",
          "3938: files = [",
          "",
          "[Removed Lines]",
          "3935: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 162 ---",
          "[Context before]",
          "3959: name = \"pydot\"",
          "3960: version = \"1.4.2\"",
          "3961: description = \"Python interface to Graphviz's Dot\"",
          "3963: optional = false",
          "3964: python-versions = \">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*\"",
          "3965: files = [",
          "",
          "[Removed Lines]",
          "3962: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 163 ---",
          "[Context before]",
          "3974: name = \"pygments\"",
          "3975: version = \"2.15.1\"",
          "3976: description = \"Pygments is a syntax highlighting package written in Python.\"",
          "3978: optional = false",
          "3979: python-versions = \">=3.7\"",
          "3980: files = [",
          "",
          "[Removed Lines]",
          "3977: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 164 ---",
          "[Context before]",
          "3989: name = \"pyjwt\"",
          "3990: version = \"2.7.0\"",
          "3991: description = \"JSON Web Token implementation in Python\"",
          "3993: optional = false",
          "3994: python-versions = \">=3.7\"",
          "3995: files = [",
          "",
          "[Removed Lines]",
          "3992: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 165 ---",
          "[Context before]",
          "4010: name = \"pykwalify\"",
          "4011: version = \"1.8.0\"",
          "4012: description = \"Python lib/cli for JSON/YAML schema validation\"",
          "4014: optional = false",
          "4015: python-versions = \"*\"",
          "4016: files = [",
          "",
          "[Removed Lines]",
          "4013: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 166 ---",
          "[Context before]",
          "4027: name = \"pymongo\"",
          "4028: version = \"4.3.3\"",
          "4029: description = \"Python driver for MongoDB <http://www.mongodb.org>\"",
          "4031: optional = false",
          "4032: python-versions = \">=3.7\"",
          "4033: files = [",
          "",
          "[Removed Lines]",
          "4030: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 167 ---",
          "[Context before]",
          "4122: name = \"pyparsing\"",
          "4123: version = \"3.1.0\"",
          "4124: description = \"pyparsing module - Classes and methods to define and execute parsing grammars\"",
          "4126: optional = false",
          "4127: python-versions = \">=3.6.8\"",
          "4128: files = [",
          "",
          "[Removed Lines]",
          "4125: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 168 ---",
          "[Context before]",
          "4137: name = \"pyreadline3\"",
          "4138: version = \"3.4.1\"",
          "4139: description = \"A python implementation of GNU readline.\"",
          "4141: optional = false",
          "4142: python-versions = \"*\"",
          "4143: files = [",
          "",
          "[Removed Lines]",
          "4140: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 169 ---",
          "[Context before]",
          "4149: name = \"pyrsistent\"",
          "4150: version = \"0.19.3\"",
          "4151: description = \"Persistent/Functional/Immutable data structures\"",
          "4153: optional = false",
          "4154: python-versions = \">=3.7\"",
          "4155: files = [",
          "",
          "[Removed Lines]",
          "4152: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 170 ---",
          "[Context before]",
          "4186: name = \"pytest\"",
          "4187: version = \"7.4.0\"",
          "4188: description = \"pytest: simple powerful testing with Python\"",
          "4190: optional = false",
          "4191: python-versions = \">=3.7\"",
          "4192: files = [",
          "",
          "[Removed Lines]",
          "4189: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 171 ---",
          "[Context before]",
          "4209: name = \"pytest-asyncio\"",
          "4210: version = \"0.20.3\"",
          "4211: description = \"Pytest support for asyncio\"",
          "4213: optional = false",
          "4214: python-versions = \">=3.7\"",
          "4215: files = [",
          "",
          "[Removed Lines]",
          "4212: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 172 ---",
          "[Context before]",
          "4228: name = \"pytest-cov\"",
          "4229: version = \"4.1.0\"",
          "4230: description = \"Pytest plugin for measuring coverage.\"",
          "4232: optional = false",
          "4233: python-versions = \">=3.7\"",
          "4234: files = [",
          "",
          "[Removed Lines]",
          "4231: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 173 ---",
          "[Context before]",
          "4247: name = \"pytest-sanic\"",
          "4248: version = \"1.9.1\"",
          "4249: description = \"a pytest plugin for Sanic\"",
          "4251: optional = false",
          "4252: python-versions = \">=3.7\"",
          "4253: files = []",
          "",
          "[Removed Lines]",
          "4250: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 174 ---",
          "[Context before]",
          "4269: name = \"pytest-timeout\"",
          "4270: version = \"2.1.0\"",
          "4271: description = \"pytest plugin to abort hanging tests\"",
          "4273: optional = false",
          "4274: python-versions = \">=3.6\"",
          "4275: files = [",
          "",
          "[Removed Lines]",
          "4272: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 175 ---",
          "[Context before]",
          "4284: name = \"pytest-xdist\"",
          "4285: version = \"3.3.1\"",
          "4286: description = \"pytest xdist plugin for distributed testing, most importantly across multiple CPUs\"",
          "4288: optional = false",
          "4289: python-versions = \">=3.7\"",
          "4290: files = [",
          "",
          "[Removed Lines]",
          "4287: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 176 ---",
          "[Context before]",
          "4305: name = \"python-crfsuite\"",
          "4306: version = \"0.9.9\"",
          "4307: description = \"Python binding for CRFsuite\"",
          "4309: optional = false",
          "4310: python-versions = \"*\"",
          "4311: files = [",
          "",
          "[Removed Lines]",
          "4308: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 177 ---",
          "[Context before]",
          "4365: name = \"python-dateutil\"",
          "4366: version = \"2.8.2\"",
          "4367: description = \"Extensions to the standard Python datetime module\"",
          "4369: optional = false",
          "4370: python-versions = \"!=3.0.*,!=3.1.*,!=3.2.*,>=2.7\"",
          "4371: files = [",
          "",
          "[Removed Lines]",
          "4368: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 178 ---",
          "[Context before]",
          "4380: name = \"python-engineio\"",
          "4381: version = \"4.5.1\"",
          "4382: description = \"Engine.IO server and client for Python\"",
          "4384: optional = false",
          "4385: python-versions = \">=3.6\"",
          "4386: files = [",
          "",
          "[Removed Lines]",
          "4383: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 179 ---",
          "[Context before]",
          "4397: name = \"python-socketio\"",
          "4398: version = \"5.8.0\"",
          "4399: description = \"Socket.IO server and client for Python\"",
          "4401: optional = false",
          "4402: python-versions = \">=3.6\"",
          "4403: files = [",
          "",
          "[Removed Lines]",
          "4400: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 180 ---",
          "[Context before]",
          "4417: name = \"pytz\"",
          "4418: version = \"2022.7.1\"",
          "4419: description = \"World timezone definitions, modern and historical\"",
          "4421: optional = false",
          "4422: python-versions = \"*\"",
          "4423: files = [",
          "",
          "[Removed Lines]",
          "4420: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 181 ---",
          "[Context before]",
          "4429: name = \"pywin32\"",
          "4430: version = \"306\"",
          "4431: description = \"Python for Window Extensions\"",
          "4433: optional = false",
          "4434: python-versions = \"*\"",
          "4435: files = [",
          "",
          "[Removed Lines]",
          "4432: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 182 ---",
          "[Context before]",
          "4453: name = \"pyyaml\"",
          "4454: version = \"6.0.1\"",
          "4455: description = \"YAML parser and emitter for Python\"",
          "4457: optional = false",
          "4458: python-versions = \">=3.6\"",
          "4459: files = [",
          "",
          "[Removed Lines]",
          "4456: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 183 ---",
          "[Context before]",
          "4514: name = \"questionary\"",
          "4515: version = \"1.10.0\"",
          "4516: description = \"Python library to build pretty command line user prompts \u2b50\ufe0f\"",
          "4518: optional = false",
          "4519: python-versions = \">=3.6,<4.0\"",
          "4520: files = [",
          "",
          "[Removed Lines]",
          "4517: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 184 ---",
          "[Context before]",
          "4532: name = \"randomname\"",
          "4533: version = \"0.1.5\"",
          "4534: description = \"Generate random adj-noun names like docker and github.\"",
          "4536: optional = false",
          "4537: python-versions = \"*\"",
          "4538: files = [",
          "",
          "[Removed Lines]",
          "4535: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 185 ---",
          "[Context before]",
          "4546: name = \"rasa-sdk\"",
          "4547: version = \"3.6.2\"",
          "4548: description = \"Open source machine learning framework to automate text- and voice-based conversations: NLU, dialogue management, connect to Slack, Facebook, and more - Create chatbots and voice assistants\"",
          "4550: optional = false",
          "4551: python-versions = \">=3.8,<3.11\"",
          "4552: files = [",
          "",
          "[Removed Lines]",
          "4549: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 186 ---",
          "[Context before]",
          "4570: name = \"redis\"",
          "4571: version = \"4.6.0\"",
          "4572: description = \"Python client for Redis database and key-value store\"",
          "4574: optional = false",
          "4575: python-versions = \">=3.7\"",
          "4576: files = [",
          "",
          "[Removed Lines]",
          "4573: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 187 ---",
          "[Context before]",
          "4589: name = \"regex\"",
          "4590: version = \"2022.10.31\"",
          "4591: description = \"Alternative regular expression module, to replace re.\"",
          "4593: optional = false",
          "4594: python-versions = \">=3.6\"",
          "4595: files = [",
          "",
          "[Removed Lines]",
          "4592: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 188 ---",
          "[Context before]",
          "4687: name = \"requests\"",
          "4688: version = \"2.31.0\"",
          "4689: description = \"Python HTTP for Humans.\"",
          "4691: optional = false",
          "4692: python-versions = \">=3.7\"",
          "4693: files = [",
          "",
          "[Removed Lines]",
          "4690: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 189 ---",
          "[Context before]",
          "4709: name = \"requests-oauthlib\"",
          "4710: version = \"1.3.1\"",
          "4711: description = \"OAuthlib authentication support for Requests.\"",
          "4713: optional = false",
          "4714: python-versions = \">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*\"",
          "4715: files = [",
          "",
          "[Removed Lines]",
          "4712: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 190 ---",
          "[Context before]",
          "4728: name = \"requests-toolbelt\"",
          "4729: version = \"1.0.0\"",
          "4730: description = \"A utility belt for advanced users of python-requests\"",
          "4732: optional = false",
          "4733: python-versions = \">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*\"",
          "4734: files = [",
          "",
          "[Removed Lines]",
          "4731: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 191 ---",
          "[Context before]",
          "4743: name = \"responses\"",
          "4744: version = \"0.22.0\"",
          "4745: description = \"A utility library for mocking out the `requests` Python library.\"",
          "4747: optional = false",
          "4748: python-versions = \">=3.7\"",
          "4749: files = [",
          "",
          "[Removed Lines]",
          "4746: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 192 ---",
          "[Context before]",
          "4764: name = \"rfc3986\"",
          "4765: version = \"1.5.0\"",
          "4766: description = \"Validating URI References per RFC 3986\"",
          "4768: optional = false",
          "4769: python-versions = \"*\"",
          "4770: files = [",
          "",
          "[Removed Lines]",
          "4767: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 193 ---",
          "[Context before]",
          "4782: name = \"rich\"",
          "4783: version = \"13.4.2\"",
          "4784: description = \"Render rich text, tables, progress bars, syntax highlighting, markdown and more to the terminal\"",
          "4786: optional = false",
          "4787: python-versions = \">=3.7.0\"",
          "4788: files = [",
          "",
          "[Removed Lines]",
          "4785: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 194 ---",
          "[Context before]",
          "4802: name = \"rocketchat-api\"",
          "4803: version = \"1.30.0\"",
          "4804: description = \"Python API wrapper for Rocket.Chat\"",
          "4806: optional = false",
          "4807: python-versions = \"*\"",
          "4808: files = [",
          "",
          "[Removed Lines]",
          "4805: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 195 ---",
          "[Context before]",
          "4818: name = \"rsa\"",
          "4819: version = \"4.9\"",
          "4820: description = \"Pure-Python RSA implementation\"",
          "4822: optional = false",
          "4823: python-versions = \">=3.6,<4\"",
          "4824: files = [",
          "",
          "[Removed Lines]",
          "4821: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 196 ---",
          "[Context before]",
          "4833: name = \"ruamel-yaml\"",
          "4834: version = \"0.17.21\"",
          "4835: description = \"ruamel.yaml is a YAML parser/emitter that supports roundtrip preservation of comments, seq/map flow style, and map key order\"",
          "4837: optional = false",
          "4838: python-versions = \">=3\"",
          "4839: files = [",
          "",
          "[Removed Lines]",
          "4836: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 197 ---",
          "[Context before]",
          "4852: name = \"ruamel-yaml-clib\"",
          "4853: version = \"0.2.7\"",
          "4854: description = \"C version of reader, parser and emitter for ruamel.yaml derived from libyaml\"",
          "4856: optional = false",
          "4857: python-versions = \">=3.5\"",
          "4858: files = [",
          "",
          "[Removed Lines]",
          "4855: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 198 ---",
          "[Context before]",
          "4899: name = \"ruff\"",
          "4900: version = \"0.0.255\"",
          "4901: description = \"An extremely fast Python linter, written in Rust.\"",
          "4903: optional = false",
          "4904: python-versions = \">=3.7\"",
          "4905: files = [",
          "",
          "[Removed Lines]",
          "4902: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 199 ---",
          "[Context before]",
          "4926: name = \"s3transfer\"",
          "4927: version = \"0.6.1\"",
          "4928: description = \"An Amazon S3 Transfer Manager\"",
          "4930: optional = false",
          "4931: python-versions = \">= 3.7\"",
          "4932: files = [",
          "",
          "[Removed Lines]",
          "4929: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 200 ---",
          "[Context before]",
          "4944: name = \"sanic\"",
          "4945: version = \"21.12.2\"",
          "4946: description = \"A web server and web framework that's written to go fast. Build fast. Run fast.\"",
          "4948: optional = false",
          "4949: python-versions = \">=3.7\"",
          "4950: files = [",
          "",
          "[Removed Lines]",
          "4947: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 201 ---",
          "[Context before]",
          "4962: websockets = \">=10.0\"",
          "4964: [package.extras]",
          "4967: docs = [\"docutils\", \"m2r2\", \"mistune (<2.0.0)\", \"pygments\", \"sphinx (>=2.1.2)\", \"sphinx-rtd-theme (>=0.4.3)\"]",
          "4968: ext = [\"sanic-ext\"]",
          "4971: [[package]]",
          "4972: name = \"sanic-cors\"",
          "4973: version = \"2.0.1\"",
          "4974: description = \"A Sanic extension adding a decorator for CORS support. Based on flask-cors by Cory Dolphin.\"",
          "4976: optional = false",
          "4977: python-versions = \"*\"",
          "4978: files = [",
          "",
          "[Removed Lines]",
          "4965: all = [\"bandit\", \"beautifulsoup4\", \"black\", \"chardet (>=3.0.0,<4.0.0)\", \"coverage (==5.3)\", \"cryptography\", \"docutils\", \"flake8\", \"gunicorn (==20.0.4)\", \"isort (>=5.0.0)\", \"m2r2\", \"mistune (<2.0.0)\", \"mypy (>=0.901,<0.910)\", \"pygments\", \"pytest (==6.2.5)\", \"pytest-benchmark\", \"pytest-cov\", \"pytest-sanic\", \"pytest-sugar\", \"sanic-testing (>=0.7.0)\", \"sphinx (>=2.1.2)\", \"sphinx-rtd-theme (>=0.4.3)\", \"towncrier\", \"tox\", \"types-ujson\", \"uvicorn (<0.15.0)\"]",
          "4966: dev = [\"bandit\", \"beautifulsoup4\", \"black\", \"chardet (>=3.0.0,<4.0.0)\", \"coverage (==5.3)\", \"cryptography\", \"docutils\", \"flake8\", \"gunicorn (==20.0.4)\", \"isort (>=5.0.0)\", \"mypy (>=0.901,<0.910)\", \"pygments\", \"pytest (==6.2.5)\", \"pytest-benchmark\", \"pytest-cov\", \"pytest-sanic\", \"pytest-sugar\", \"sanic-testing (>=0.7.0)\", \"towncrier\", \"tox\", \"types-ujson\", \"uvicorn (<0.15.0)\"]",
          "4969: test = [\"bandit\", \"beautifulsoup4\", \"black\", \"chardet (>=3.0.0,<4.0.0)\", \"coverage (==5.3)\", \"docutils\", \"flake8\", \"gunicorn (==20.0.4)\", \"isort (>=5.0.0)\", \"mypy (>=0.901,<0.910)\", \"pygments\", \"pytest (==6.2.5)\", \"pytest-benchmark\", \"pytest-cov\", \"pytest-sanic\", \"pytest-sugar\", \"sanic-testing (>=0.7.0)\", \"types-ujson\", \"uvicorn (<0.15.0)\"]",
          "4975: category = \"main\"",
          "",
          "[Added Lines]",
          "4770: all = [\"bandit\", \"beautifulsoup4\", \"black\", \"chardet (==3.*)\", \"coverage (==5.3)\", \"cryptography\", \"docutils\", \"flake8\", \"gunicorn (==20.0.4)\", \"isort (>=5.0.0)\", \"m2r2\", \"mistune (<2.0.0)\", \"mypy (>=0.901,<0.910)\", \"pygments\", \"pytest (==6.2.5)\", \"pytest-benchmark\", \"pytest-cov\", \"pytest-sanic\", \"pytest-sugar\", \"sanic-testing (>=0.7.0)\", \"sphinx (>=2.1.2)\", \"sphinx-rtd-theme (>=0.4.3)\", \"towncrier\", \"tox\", \"types-ujson\", \"uvicorn (<0.15.0)\"]",
          "4771: dev = [\"bandit\", \"beautifulsoup4\", \"black\", \"chardet (==3.*)\", \"coverage (==5.3)\", \"cryptography\", \"docutils\", \"flake8\", \"gunicorn (==20.0.4)\", \"isort (>=5.0.0)\", \"mypy (>=0.901,<0.910)\", \"pygments\", \"pytest (==6.2.5)\", \"pytest-benchmark\", \"pytest-cov\", \"pytest-sanic\", \"pytest-sugar\", \"sanic-testing (>=0.7.0)\", \"towncrier\", \"tox\", \"types-ujson\", \"uvicorn (<0.15.0)\"]",
          "4774: test = [\"bandit\", \"beautifulsoup4\", \"black\", \"chardet (==3.*)\", \"coverage (==5.3)\", \"docutils\", \"flake8\", \"gunicorn (==20.0.4)\", \"isort (>=5.0.0)\", \"mypy (>=0.901,<0.910)\", \"pygments\", \"pytest (==6.2.5)\", \"pytest-benchmark\", \"pytest-cov\", \"pytest-sanic\", \"pytest-sugar\", \"sanic-testing (>=0.7.0)\", \"types-ujson\", \"uvicorn (<0.15.0)\"]",
          "",
          "---------------",
          "--- Hunk 202 ---",
          "[Context before]",
          "4987: name = \"sanic-jwt\"",
          "4988: version = \"1.8.0\"",
          "4989: description = \"JWT oauth flow for Sanic\"",
          "4991: optional = false",
          "4992: python-versions = \"*\"",
          "4993: files = [",
          "",
          "[Removed Lines]",
          "4990: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 203 ---",
          "[Context before]",
          "5006: name = \"sanic-routing\"",
          "5007: version = \"0.7.2\"",
          "5008: description = \"Core routing component for Sanic\"",
          "5010: optional = false",
          "5011: python-versions = \"*\"",
          "5012: files = [",
          "",
          "[Removed Lines]",
          "5009: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 204 ---",
          "[Context before]",
          "5018: name = \"sanic-testing\"",
          "5019: version = \"22.6.0\"",
          "5020: description = \"Core testing clients for Sanic\"",
          "5022: optional = false",
          "5023: python-versions = \"*\"",
          "5024: files = [",
          "",
          "[Removed Lines]",
          "5021: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 205 ---",
          "[Context before]",
          "5033: name = \"scikit-learn\"",
          "5034: version = \"1.1.3\"",
          "5035: description = \"A set of python modules for machine learning and data mining\"",
          "5037: optional = false",
          "5038: python-versions = \">=3.8\"",
          "5039: files = [",
          "",
          "[Removed Lines]",
          "5036: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 206 ---",
          "[Context before]",
          "5076: name = \"scipy\"",
          "5077: version = \"1.10.1\"",
          "5078: description = \"Fundamental algorithms for scientific computing in Python\"",
          "5080: optional = false",
          "5081: python-versions = \"<3.12,>=3.8\"",
          "5082: files = [",
          "",
          "[Removed Lines]",
          "5079: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 207 ---",
          "[Context before]",
          "5115: name = \"sentencepiece\"",
          "5116: version = \"0.1.99\"",
          "5117: description = \"SentencePiece python wrapper\"",
          "5119: optional = true",
          "5120: python-versions = \"*\"",
          "5121: files = [",
          "",
          "[Removed Lines]",
          "5118: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 208 ---",
          "[Context before]",
          "5170: name = \"sentinels\"",
          "5171: version = \"1.0.0\"",
          "5172: description = \"Various objects to denote special meanings in python\"",
          "5174: optional = false",
          "5175: python-versions = \"*\"",
          "5176: files = [",
          "",
          "[Removed Lines]",
          "5173: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 209 ---",
          "[Context before]",
          "5181: name = \"sentry-sdk\"",
          "5182: version = \"1.14.0\"",
          "5183: description = \"Python client for Sentry (https://sentry.io)\"",
          "5185: optional = false",
          "5186: python-versions = \"*\"",
          "5187: files = [",
          "",
          "[Removed Lines]",
          "5184: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 210 ---",
          "[Context before]",
          "5220: name = \"setuptools\"",
          "5221: version = \"68.0.0\"",
          "5222: description = \"Easily download, build, install, upgrade, and uninstall Python packages\"",
          "5224: optional = false",
          "5225: python-versions = \">=3.7\"",
          "5226: files = [",
          "",
          "[Removed Lines]",
          "5223: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 211 ---",
          "[Context before]",
          "5237: name = \"six\"",
          "5238: version = \"1.16.0\"",
          "5239: description = \"Python 2 and 3 compatibility utilities\"",
          "5241: optional = false",
          "5242: python-versions = \">=2.7, !=3.0.*, !=3.1.*, !=3.2.*\"",
          "5243: files = [",
          "",
          "[Removed Lines]",
          "5240: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 212 ---",
          "[Context before]",
          "5249: name = \"sklearn-crfsuite\"",
          "5250: version = \"0.3.6\"",
          "5251: description = \"CRFsuite (python-crfsuite) wrapper which provides interface simlar to scikit-learn\"",
          "5253: optional = false",
          "5254: python-versions = \"*\"",
          "5255: files = [",
          "",
          "[Removed Lines]",
          "5252: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 213 ---",
          "[Context before]",
          "5267: name = \"slack-sdk\"",
          "5268: version = \"3.21.3\"",
          "5269: description = \"The Slack API Platform SDK for Python\"",
          "5271: optional = false",
          "5272: python-versions = \">=3.6.0\"",
          "5273: files = [",
          "",
          "[Removed Lines]",
          "5270: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 214 ---",
          "[Context before]",
          "5283: name = \"smart-open\"",
          "5284: version = \"6.3.0\"",
          "5285: description = \"Utils for streaming large files (S3, HDFS, GCS, Azure Blob Storage, gzip, bz2...)\"",
          "5287: optional = true",
          "5288: python-versions = \">=3.6,<4.0\"",
          "5289: files = [",
          "",
          "[Removed Lines]",
          "5286: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 215 ---",
          "[Context before]",
          "5305: name = \"smmap\"",
          "5306: version = \"5.0.0\"",
          "5307: description = \"A pure Python implementation of a sliding window memory map manager\"",
          "5309: optional = false",
          "5310: python-versions = \">=3.6\"",
          "5311: files = [",
          "",
          "[Removed Lines]",
          "5308: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 216 ---",
          "[Context before]",
          "5317: name = \"sniffio\"",
          "5318: version = \"1.3.0\"",
          "5319: description = \"Sniff out which async library your code is running under\"",
          "5321: optional = false",
          "5322: python-versions = \">=3.7\"",
          "5323: files = [",
          "",
          "[Removed Lines]",
          "5320: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 217 ---",
          "[Context before]",
          "5329: name = \"sortedcontainers\"",
          "5330: version = \"2.4.0\"",
          "5331: description = \"Sorted Containers -- Sorted List, Sorted Dict, Sorted Set\"",
          "5333: optional = false",
          "5334: python-versions = \"*\"",
          "5335: files = [",
          "",
          "[Removed Lines]",
          "5332: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 218 ---",
          "[Context before]",
          "5341: name = \"spacy\"",
          "5342: version = \"3.4.4\"",
          "5343: description = \"Industrial-strength Natural Language Processing (NLP) in Python\"",
          "5345: optional = true",
          "5346: python-versions = \">=3.6\"",
          "5347: files = [",
          "",
          "[Removed Lines]",
          "5344: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 219 ---",
          "[Context before]",
          "5428: name = \"spacy\"",
          "5429: version = \"3.5.4\"",
          "5430: description = \"Industrial-strength Natural Language Processing (NLP) in Python\"",
          "5432: optional = true",
          "5433: python-versions = \">=3.6\"",
          "5434: files = [",
          "",
          "[Removed Lines]",
          "5431: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 220 ---",
          "[Context before]",
          "5515: name = \"spacy-legacy\"",
          "5516: version = \"3.0.12\"",
          "5517: description = \"Legacy registered functions for spaCy backwards compatibility\"",
          "5519: optional = true",
          "5520: python-versions = \">=3.6\"",
          "5521: files = [",
          "",
          "[Removed Lines]",
          "5518: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 221 ---",
          "[Context before]",
          "5527: name = \"spacy-loggers\"",
          "5528: version = \"1.0.4\"",
          "5529: description = \"Logging utilities for SpaCy\"",
          "5531: optional = true",
          "5532: python-versions = \">=3.6\"",
          "5533: files = [",
          "",
          "[Removed Lines]",
          "5530: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 222 ---",
          "[Context before]",
          "5539: name = \"sqlalchemy\"",
          "5540: version = \"1.4.49\"",
          "5541: description = \"Database Abstraction Library\"",
          "5543: optional = false",
          "5544: python-versions = \"!=3.0.*,!=3.1.*,!=3.2.*,!=3.3.*,!=3.4.*,!=3.5.*,>=2.7\"",
          "5545: files = [",
          "",
          "[Removed Lines]",
          "5542: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 223 ---",
          "[Context before]",
          "5594: ]",
          "5596: [package.dependencies]",
          "5599: [package.extras]",
          "5600: aiomysql = [\"aiomysql\", \"greenlet (!=0.4.17)\"]",
          "",
          "[Removed Lines]",
          "5597: greenlet = {version = \"!=0.4.17\", markers = \"python_version >= \\\"3\\\" and platform_machine == \\\"aarch64\\\" or python_version >= \\\"3\\\" and platform_machine == \\\"ppc64le\\\" or python_version >= \\\"3\\\" and platform_machine == \\\"x86_64\\\" or python_version >= \\\"3\\\" and platform_machine == \\\"amd64\\\" or python_version >= \\\"3\\\" and platform_machine == \\\"AMD64\\\" or python_version >= \\\"3\\\" and platform_machine == \\\"win32\\\" or python_version >= \\\"3\\\" and platform_machine == \\\"WIN32\\\"\"}",
          "",
          "[Added Lines]",
          "5380: greenlet = {version = \"!=0.4.17\", markers = \"python_version >= \\\"3\\\" and (platform_machine == \\\"aarch64\\\" or platform_machine == \\\"ppc64le\\\" or platform_machine == \\\"x86_64\\\" or platform_machine == \\\"amd64\\\" or platform_machine == \\\"AMD64\\\" or platform_machine == \\\"win32\\\" or platform_machine == \\\"WIN32\\\")\"}",
          "",
          "---------------",
          "--- Hunk 224 ---",
          "[Context before]",
          "5621: name = \"srsly\"",
          "5622: version = \"2.4.6\"",
          "5623: description = \"Modern high-performance serialization utilities for Python\"",
          "5625: optional = true",
          "5626: python-versions = \">=3.6\"",
          "5627: files = [",
          "",
          "[Removed Lines]",
          "5624: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 225 ---",
          "[Context before]",
          "5662: name = \"stevedore\"",
          "5663: version = \"5.1.0\"",
          "5664: description = \"Manage dynamic plugins for Python applications\"",
          "5666: optional = false",
          "5667: python-versions = \">=3.8\"",
          "5668: files = [",
          "",
          "[Removed Lines]",
          "5665: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 226 ---",
          "[Context before]",
          "5677: name = \"structlog\"",
          "5678: version = \"23.1.0\"",
          "5679: description = \"Structured Logging for Python\"",
          "5681: optional = false",
          "5682: python-versions = \">=3.7\"",
          "5683: files = [",
          "",
          "[Removed Lines]",
          "5680: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 227 ---",
          "[Context before]",
          "5695: name = \"structlog-sentry\"",
          "5696: version = \"2.0.3\"",
          "5697: description = \"Sentry integration for structlog\"",
          "5699: optional = false",
          "5700: python-versions = \">=3.7,<4.0\"",
          "5701: files = [",
          "",
          "[Removed Lines]",
          "5698: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 228 ---",
          "[Context before]",
          "5711: name = \"tabulate\"",
          "5712: version = \"0.9.0\"",
          "5713: description = \"Pretty-print tabular data\"",
          "5715: optional = false",
          "5716: python-versions = \">=3.7\"",
          "5717: files = [",
          "",
          "[Removed Lines]",
          "5714: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 229 ---",
          "[Context before]",
          "5726: name = \"tarsafe\"",
          "5727: version = \"0.0.4\"",
          "5728: description = \"A safe subclass of the TarFile class for interacting with tar files. Can be used as a direct drop-in replacement for safe usage of extractall()\"",
          "5730: optional = false",
          "5731: python-versions = \">=3.6\"",
          "5732: files = [",
          "",
          "[Removed Lines]",
          "5729: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 230 ---",
          "[Context before]",
          "5738: name = \"tensorboard\"",
          "5739: version = \"2.12.3\"",
          "5740: description = \"TensorBoard lets you watch Tensors Flow\"",
          "5742: optional = false",
          "5743: python-versions = \">=3.8\"",
          "5744: files = [",
          "",
          "[Removed Lines]",
          "5741: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 231 ---",
          "[Context before]",
          "5763: name = \"tensorboard-data-server\"",
          "5764: version = \"0.7.1\"",
          "5765: description = \"Fast data loading for TensorBoard\"",
          "5767: optional = false",
          "5768: python-versions = \">=3.7\"",
          "5769: files = [",
          "",
          "[Removed Lines]",
          "5766: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 232 ---",
          "[Context before]",
          "5776: name = \"tensorflow\"",
          "5777: version = \"2.12.0\"",
          "5778: description = \"TensorFlow is an open source machine learning framework for everyone.\"",
          "5780: optional = false",
          "5781: python-versions = \">=3.8\"",
          "5782: files = [",
          "",
          "[Removed Lines]",
          "5779: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 233 ---",
          "[Context before]",
          "5826: name = \"tensorflow-cpu-aws\"",
          "5827: version = \"2.12.0\"",
          "5828: description = \"TensorFlow is an open source machine learning framework for everyone.\"",
          "5830: optional = false",
          "5831: python-versions = \">=3.8\"",
          "5832: files = [",
          "",
          "[Removed Lines]",
          "5829: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 234 ---",
          "[Context before]",
          "5864: name = \"tensorflow-estimator\"",
          "5865: version = \"2.12.0\"",
          "5866: description = \"TensorFlow Estimator.\"",
          "5868: optional = false",
          "5869: python-versions = \">=3.7\"",
          "5870: files = [",
          "",
          "[Removed Lines]",
          "5867: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 235 ---",
          "[Context before]",
          "5875: name = \"tensorflow-hub\"",
          "5876: version = \"0.13.0\"",
          "5877: description = \"TensorFlow Hub is a library to foster the publication, discovery, and consumption of reusable parts of machine learning models.\"",
          "5879: optional = false",
          "5880: python-versions = \"*\"",
          "5881: files = [",
          "",
          "[Removed Lines]",
          "5878: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 236 ---",
          "[Context before]",
          "5894: name = \"tensorflow-intel\"",
          "5895: version = \"2.12.0\"",
          "5896: description = \"TensorFlow is an open source machine learning framework for everyone.\"",
          "5898: optional = false",
          "5899: python-versions = \">=3.8\"",
          "5900: files = [",
          "",
          "[Removed Lines]",
          "5897: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 237 ---",
          "[Context before]",
          "5932: name = \"tensorflow-io-gcs-filesystem\"",
          "5933: version = \"0.31.0\"",
          "5934: description = \"TensorFlow IO\"",
          "5936: optional = false",
          "5937: python-versions = \">=3.7, <3.12\"",
          "5938: files = [",
          "",
          "[Removed Lines]",
          "5935: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 238 ---",
          "[Context before]",
          "5968: name = \"tensorflow-io-gcs-filesystem\"",
          "5969: version = \"0.32.0\"",
          "5970: description = \"TensorFlow IO\"",
          "5972: optional = false",
          "5973: python-versions = \">=3.7, <3.12\"",
          "5974: files = [",
          "",
          "[Removed Lines]",
          "5971: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 239 ---",
          "[Context before]",
          "5999: name = \"tensorflow-macos\"",
          "6000: version = \"2.12.0\"",
          "6001: description = \"TensorFlow is an open source machine learning framework for everyone.\"",
          "6003: optional = false",
          "6004: python-versions = \">=3.8\"",
          "6005: files = [",
          "",
          "[Removed Lines]",
          "6002: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 240 ---",
          "[Context before]",
          "6041: name = \"tensorflow-metal\"",
          "6042: version = \"0.8.0\"",
          "6043: description = \"TensorFlow acceleration for Mac GPUs.\"",
          "6045: optional = true",
          "6046: python-versions = \"*\"",
          "6047: files = [",
          "",
          "[Removed Lines]",
          "6044: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 241 ---",
          "[Context before]",
          "6063: name = \"tensorflow-text\"",
          "6064: version = \"2.12.0\"",
          "6065: description = \"TF.Text is a TensorFlow library of text related ops, modules, and subgraphs.\"",
          "6067: optional = false",
          "6068: python-versions = \"*\"",
          "6069: files = [",
          "",
          "[Removed Lines]",
          "6066: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 242 ---",
          "[Context before]",
          "6089: name = \"termcolor\"",
          "6090: version = \"2.3.0\"",
          "6091: description = \"ANSI color formatting for output in terminal\"",
          "6093: optional = false",
          "6094: python-versions = \">=3.7\"",
          "6095: files = [",
          "",
          "[Removed Lines]",
          "6092: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 243 ---",
          "[Context before]",
          "6104: name = \"terminaltables\"",
          "6105: version = \"3.1.10\"",
          "6106: description = \"Generate simple tables in terminals from a nested list of strings.\"",
          "6108: optional = false",
          "6109: python-versions = \">=2.6\"",
          "6110: files = [",
          "",
          "[Removed Lines]",
          "6107: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 244 ---",
          "[Context before]",
          "6116: name = \"thinc\"",
          "6117: version = \"8.1.10\"",
          "6118: description = \"A refreshing functional take on deep learning, compatible with your favorite libraries\"",
          "6120: optional = true",
          "6121: python-versions = \">=3.6\"",
          "6122: files = [",
          "",
          "[Removed Lines]",
          "6119: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 245 ---",
          "[Context before]",
          "6192: name = \"threadpoolctl\"",
          "6193: version = \"3.1.0\"",
          "6194: description = \"threadpoolctl\"",
          "6196: optional = false",
          "6197: python-versions = \">=3.6\"",
          "6198: files = [",
          "",
          "[Removed Lines]",
          "6195: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 246 ---",
          "[Context before]",
          "6204: name = \"tokenizers\"",
          "6205: version = \"0.13.3\"",
          "6206: description = \"Fast and Customizable Tokenizers\"",
          "6208: optional = true",
          "6209: python-versions = \"*\"",
          "6210: files = [",
          "",
          "[Removed Lines]",
          "6207: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 247 ---",
          "[Context before]",
          "6259: name = \"toml\"",
          "6260: version = \"0.10.2\"",
          "6261: description = \"Python Library for Tom's Obvious, Minimal Language\"",
          "6263: optional = false",
          "6264: python-versions = \">=2.6, !=3.0.*, !=3.1.*, !=3.2.*\"",
          "6265: files = [",
          "",
          "[Removed Lines]",
          "6262: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 248 ---",
          "[Context before]",
          "6271: name = \"tomli\"",
          "6272: version = \"2.0.1\"",
          "6273: description = \"A lil' TOML parser\"",
          "6275: optional = false",
          "6276: python-versions = \">=3.7\"",
          "6277: files = [",
          "",
          "[Removed Lines]",
          "6274: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 249 ---",
          "[Context before]",
          "6283: name = \"tomli-w\"",
          "6284: version = \"1.0.0\"",
          "6285: description = \"A lil' TOML writer\"",
          "6287: optional = false",
          "6288: python-versions = \">=3.7\"",
          "6289: files = [",
          "",
          "[Removed Lines]",
          "6286: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 250 ---",
          "[Context before]",
          "6295: name = \"toolz\"",
          "6296: version = \"0.12.0\"",
          "6297: description = \"List processing tools and functional utilities\"",
          "6299: optional = false",
          "6300: python-versions = \">=3.5\"",
          "6301: files = [",
          "",
          "[Removed Lines]",
          "6298: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 251 ---",
          "[Context before]",
          "6307: name = \"towncrier\"",
          "6308: version = \"22.12.0\"",
          "6309: description = \"Building newsfiles for your project.\"",
          "6311: optional = false",
          "6312: python-versions = \">=3.7\"",
          "6313: files = [",
          "",
          "[Removed Lines]",
          "6310: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 252 ---",
          "[Context before]",
          "6330: name = \"tqdm\"",
          "6331: version = \"4.65.0\"",
          "6332: description = \"Fast, Extensible Progress Meter\"",
          "6334: optional = false",
          "6335: python-versions = \">=3.7\"",
          "6336: files = [",
          "",
          "[Removed Lines]",
          "6333: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 253 ---",
          "[Context before]",
          "6351: name = \"transformers\"",
          "6352: version = \"4.26.0\"",
          "6353: description = \"State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\"",
          "6355: optional = true",
          "6356: python-versions = \">=3.7.0\"",
          "6357: files = [",
          "",
          "[Removed Lines]",
          "6354: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 254 ---",
          "[Context before]",
          "6418: name = \"twilio\"",
          "6419: version = \"8.2.2\"",
          "6420: description = \"Twilio API client and TwiML generator\"",
          "6422: optional = false",
          "6423: python-versions = \">=3.7.0\"",
          "6424: files = [",
          "",
          "[Removed Lines]",
          "6421: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 255 ---",
          "[Context before]",
          "6437: name = \"typer\"",
          "6438: version = \"0.7.0\"",
          "6439: description = \"Typer, build great CLIs. Easy to code. Based on Python type hints.\"",
          "6441: optional = true",
          "6442: python-versions = \">=3.6\"",
          "6443: files = [",
          "",
          "[Removed Lines]",
          "6440: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 256 ---",
          "[Context before]",
          "6458: name = \"typer\"",
          "6459: version = \"0.9.0\"",
          "6460: description = \"Typer, build great CLIs. Easy to code. Based on Python type hints.\"",
          "6462: optional = true",
          "6463: python-versions = \">=3.6\"",
          "6464: files = [",
          "",
          "[Removed Lines]",
          "6461: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 257 ---",
          "[Context before]",
          "6480: name = \"types-pyopenssl\"",
          "6481: version = \"23.2.0.1\"",
          "6482: description = \"Typing stubs for pyOpenSSL\"",
          "6484: optional = false",
          "6485: python-versions = \"*\"",
          "6486: files = [",
          "",
          "[Removed Lines]",
          "6483: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 258 ---",
          "[Context before]",
          "6495: name = \"types-python-dateutil\"",
          "6496: version = \"2.8.19.13\"",
          "6497: description = \"Typing stubs for python-dateutil\"",
          "6499: optional = false",
          "6500: python-versions = \"*\"",
          "6501: files = [",
          "",
          "[Removed Lines]",
          "6498: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 259 ---",
          "[Context before]",
          "6507: name = \"types-pytz\"",
          "6508: version = \"2022.7.1.2\"",
          "6509: description = \"Typing stubs for pytz\"",
          "6511: optional = false",
          "6512: python-versions = \"*\"",
          "6513: files = [",
          "",
          "[Removed Lines]",
          "6510: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 260 ---",
          "[Context before]",
          "6519: name = \"types-redis\"",
          "6520: version = \"4.6.0.2\"",
          "6521: description = \"Typing stubs for redis\"",
          "6523: optional = false",
          "6524: python-versions = \"*\"",
          "6525: files = [",
          "",
          "[Removed Lines]",
          "6522: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 261 ---",
          "[Context before]",
          "6535: name = \"types-requests\"",
          "6536: version = \"2.31.0.1\"",
          "6537: description = \"Typing stubs for requests\"",
          "6539: optional = false",
          "6540: python-versions = \"*\"",
          "6541: files = [",
          "",
          "[Removed Lines]",
          "6538: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 262 ---",
          "[Context before]",
          "6550: name = \"types-setuptools\"",
          "6551: version = \"67.8.0.0\"",
          "6552: description = \"Typing stubs for setuptools\"",
          "6554: optional = false",
          "6555: python-versions = \"*\"",
          "6556: files = [",
          "",
          "[Removed Lines]",
          "6553: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 263 ---",
          "[Context before]",
          "6562: name = \"types-toml\"",
          "6563: version = \"0.10.8.6\"",
          "6564: description = \"Typing stubs for toml\"",
          "6566: optional = false",
          "6567: python-versions = \"*\"",
          "6568: files = [",
          "",
          "[Removed Lines]",
          "6565: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 264 ---",
          "[Context before]",
          "6574: name = \"types-urllib3\"",
          "6575: version = \"1.26.25.13\"",
          "6576: description = \"Typing stubs for urllib3\"",
          "6578: optional = false",
          "6579: python-versions = \"*\"",
          "6580: files = [",
          "",
          "[Removed Lines]",
          "6577: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 265 ---",
          "[Context before]",
          "6586: name = \"typing-extensions\"",
          "6587: version = \"4.7.1\"",
          "6588: description = \"Backported and Experimental Type Hints for Python 3.7+\"",
          "6590: optional = false",
          "6591: python-versions = \">=3.7\"",
          "6592: files = [",
          "",
          "[Removed Lines]",
          "6589: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 266 ---",
          "[Context before]",
          "6598: name = \"typing-utils\"",
          "6599: version = \"0.1.0\"",
          "6600: description = \"utils to inspect Python type annotations\"",
          "6602: optional = false",
          "6603: python-versions = \">=3.6.1\"",
          "6604: files = [",
          "",
          "[Removed Lines]",
          "6601: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 267 ---",
          "[Context before]",
          "6613: name = \"tzdata\"",
          "6614: version = \"2023.3\"",
          "6615: description = \"Provider of IANA time zone data\"",
          "6617: optional = false",
          "6618: python-versions = \">=2\"",
          "6619: files = [",
          "",
          "[Removed Lines]",
          "6616: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 268 ---",
          "[Context before]",
          "6625: name = \"tzlocal\"",
          "6626: version = \"5.0.1\"",
          "6627: description = \"tzinfo object for the local timezone\"",
          "6629: optional = false",
          "6630: python-versions = \">=3.7\"",
          "6631: files = [",
          "",
          "[Removed Lines]",
          "6628: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 269 ---",
          "[Context before]",
          "6644: name = \"ujson\"",
          "6645: version = \"5.8.0\"",
          "6646: description = \"Ultra fast JSON encoder and decoder for Python\"",
          "6648: optional = false",
          "6649: python-versions = \">=3.8\"",
          "6650: files = [",
          "",
          "[Removed Lines]",
          "6647: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 270 ---",
          "[Context before]",
          "6715: name = \"uritemplate\"",
          "6716: version = \"4.1.1\"",
          "6717: description = \"Implementation of RFC 6570 URI Templates\"",
          "6719: optional = true",
          "6720: python-versions = \">=3.6\"",
          "6721: files = [",
          "",
          "[Removed Lines]",
          "6718: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 271 ---",
          "[Context before]",
          "6727: name = \"urllib3\"",
          "6728: version = \"1.26.16\"",
          "6729: description = \"HTTP library with thread-safe connection pooling, file post, and more.\"",
          "6731: optional = false",
          "6732: python-versions = \">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*, !=3.4.*, !=3.5.*\"",
          "6733: files = [",
          "",
          "[Removed Lines]",
          "6730: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 272 ---",
          "[Context before]",
          "6744: name = \"uvloop\"",
          "6745: version = \"0.17.0\"",
          "6746: description = \"Fast implementation of asyncio event loop on top of libuv\"",
          "6748: optional = false",
          "6749: python-versions = \">=3.7\"",
          "6750: files = [",
          "",
          "[Removed Lines]",
          "6747: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 273 ---",
          "[Context before]",
          "6789: name = \"wasabi\"",
          "6790: version = \"0.10.1\"",
          "6791: description = \"A lightweight console printing and formatting toolkit\"",
          "6793: optional = true",
          "6794: python-versions = \"*\"",
          "6795: files = [",
          "",
          "[Removed Lines]",
          "6792: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 274 ---",
          "[Context before]",
          "6801: name = \"wasabi\"",
          "6802: version = \"1.1.2\"",
          "6803: description = \"A lightweight console printing and formatting toolkit\"",
          "6805: optional = true",
          "6806: python-versions = \">=3.6\"",
          "6807: files = [",
          "",
          "[Removed Lines]",
          "6804: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 275 ---",
          "[Context before]",
          "6813: name = \"watchdog\"",
          "6814: version = \"3.0.0\"",
          "6815: description = \"Filesystem events monitoring\"",
          "6817: optional = false",
          "6818: python-versions = \">=3.7\"",
          "6819: files = [",
          "",
          "[Removed Lines]",
          "6816: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 276 ---",
          "[Context before]",
          "6853: name = \"wcwidth\"",
          "6854: version = \"0.2.6\"",
          "6855: description = \"Measures the displayed width of unicode strings in a terminal\"",
          "6857: optional = false",
          "6858: python-versions = \"*\"",
          "6859: files = [",
          "",
          "[Removed Lines]",
          "6856: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 277 ---",
          "[Context before]",
          "6865: name = \"webexteamssdk\"",
          "6866: version = \"1.6.1\"",
          "6867: description = \"Community-developed Python SDK for the Webex Teams APIs\"",
          "6869: optional = false",
          "6870: python-versions = \"*\"",
          "6871: files = [",
          "",
          "[Removed Lines]",
          "6868: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 278 ---",
          "[Context before]",
          "6883: name = \"websocket-client\"",
          "6884: version = \"1.6.1\"",
          "6885: description = \"WebSocket client for Python with low level API options\"",
          "6887: optional = false",
          "6888: python-versions = \">=3.7\"",
          "6889: files = [",
          "",
          "[Removed Lines]",
          "6886: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 279 ---",
          "[Context before]",
          "6900: name = \"websockets\"",
          "6901: version = \"10.4\"",
          "6902: description = \"An implementation of the WebSocket Protocol (RFC 6455 & 7692)\"",
          "6904: optional = false",
          "6905: python-versions = \">=3.7\"",
          "6906: files = [",
          "",
          "[Removed Lines]",
          "6903: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 280 ---",
          "[Context before]",
          "6979: name = \"werkzeug\"",
          "6980: version = \"2.3.6\"",
          "6981: description = \"The comprehensive WSGI web application library.\"",
          "6983: optional = false",
          "6984: python-versions = \">=3.8\"",
          "6985: files = [",
          "",
          "[Removed Lines]",
          "6982: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 281 ---",
          "[Context before]",
          "6997: name = \"wheel\"",
          "6998: version = \"0.40.0\"",
          "6999: description = \"A built-package format for Python\"",
          "7001: optional = false",
          "7002: python-versions = \">=3.7\"",
          "7003: files = [",
          "",
          "[Removed Lines]",
          "7000: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 282 ---",
          "[Context before]",
          "7012: name = \"wrapt\"",
          "7013: version = \"1.14.1\"",
          "7014: description = \"Module for decorators, wrappers and monkey patching.\"",
          "7016: optional = false",
          "7017: python-versions = \"!=3.0.*,!=3.1.*,!=3.2.*,!=3.3.*,!=3.4.*,>=2.7\"",
          "7018: files = [",
          "",
          "[Removed Lines]",
          "7015: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 283 ---",
          "[Context before]",
          "7096: name = \"xmltodict\"",
          "7097: version = \"0.13.0\"",
          "7098: description = \"Makes working with XML feel like you are working with JSON\"",
          "7100: optional = false",
          "7101: python-versions = \">=3.4\"",
          "7102: files = [",
          "",
          "[Removed Lines]",
          "7099: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 284 ---",
          "[Context before]",
          "7108: name = \"yapf\"",
          "7109: version = \"0.40.1\"",
          "7110: description = \"A formatter for Python code.\"",
          "7112: optional = false",
          "7113: python-versions = \">=3.7\"",
          "7114: files = [",
          "",
          "[Removed Lines]",
          "7111: category = \"dev\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 285 ---",
          "[Context before]",
          "7125: name = \"yarl\"",
          "7126: version = \"1.9.2\"",
          "7127: description = \"Yet another URL library\"",
          "7129: optional = false",
          "7130: python-versions = \">=3.7\"",
          "7131: files = [",
          "",
          "[Removed Lines]",
          "7128: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 286 ---",
          "[Context before]",
          "7213: name = \"zipp\"",
          "7214: version = \"3.15.0\"",
          "7215: description = \"Backport of pathlib-compatible object wrapper for zip files\"",
          "7217: optional = false",
          "7218: python-versions = \">=3.7\"",
          "7219: files = [",
          "",
          "[Removed Lines]",
          "7216: category = \"main\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 287 ---",
          "[Context before]",
          "7236: [metadata]",
          "7237: lock-version = \"2.0\"",
          "7238: python-versions = \">=3.8,<3.11\"",
          "",
          "[Removed Lines]",
          "7239: content-hash = \"c554d0f11eb367109ccd80c3694de7f07d35b52a09b1279d2b9743e6c155f43c\"",
          "",
          "[Added Lines]",
          "6959: content-hash = \"8e1bef6b78365110ec598219ee747edb505f82a76fd03bdcffcec7a299b39513\"",
          "",
          "---------------"
        ]
      }
    }
  ]
}