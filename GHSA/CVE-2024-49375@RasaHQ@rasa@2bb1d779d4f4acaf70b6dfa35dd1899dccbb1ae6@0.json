{
  "cve_id": "CVE-2024-49375",
  "cve_desc": "Open source machine learning framework. A vulnerability has been identified in Rasa that enables an attacker who has the ability to load a maliciously crafted model remotely into a Rasa instance to achieve Remote Code Execution. The prerequisites for this are: 1. The HTTP API must be enabled on the Rasa instance eg with `--enable-api`. This is not the default configuration. 2. For unauthenticated RCE to be exploitable, the user must not have configured any authentication or other security controls recommended in our documentation. 3. For authenticated RCE, the attacker must posses a valid authentication token or JWT to interact with the Rasa API. This issue has been addressed in rasa version 3.6.21 and all users are advised to upgrade. Users unable to upgrade should ensure that they require authentication and that only trusted users are given access.",
  "repo": "RasaHQ/rasa",
  "patch_hash": "2bb1d779d4f4acaf70b6dfa35dd1899dccbb1ae6",
  "patch_info": {
    "commit_hash": "2bb1d779d4f4acaf70b6dfa35dd1899dccbb1ae6",
    "repo": "RasaHQ/rasa",
    "commit_url": "https://github.com/RasaHQ/rasa/commit/2bb1d779d4f4acaf70b6dfa35dd1899dccbb1ae6",
    "files": [
      ".github/workflows/continous-integration.yml",
      "changelog/1424.bugfix.md",
      "poetry.lock",
      "pyproject.toml",
      "rasa/core/featurizers/single_state_featurizer.py",
      "rasa/core/featurizers/tracker_featurizers.py",
      "rasa/core/policies/ted_policy.py",
      "rasa/core/policies/unexpected_intent_policy.py",
      "rasa/nlu/classifiers/diet_classifier.py",
      "rasa/nlu/classifiers/logistic_regression_classifier.py",
      "rasa/nlu/classifiers/sklearn_intent_classifier.py",
      "rasa/nlu/extractors/crf_entity_extractor.py",
      "rasa/nlu/featurizers/sparse_featurizer/count_vectors_featurizer.py",
      "rasa/nlu/featurizers/sparse_featurizer/lexical_syntactic_featurizer.py",
      "rasa/nlu/featurizers/sparse_featurizer/regex_featurizer.py",
      "rasa/shared/nlu/training_data/features.py",
      "rasa/shared/utils/io.py",
      "rasa/utils/common.py",
      "rasa/utils/io.py",
      "rasa/utils/tensorflow/feature_array.py",
      "rasa/utils/tensorflow/model_data.py",
      "scripts/ping_slack_about_package_release.sh",
      "tests/core/featurizers/test_tracker_featurizer.py",
      "tests/nlu/extractors/test_crf_entity_extractor.py",
      "tests/shared/nlu/training_data/test_features.py",
      "tests/utils/tensorflow/test_feature_array.py",
      "tests/utils/test_io.py"
    ],
    "message": "Replace pickle with safer alternatives (#13067)\n\n* Update slack release notification step\n\n* [ENG-1424] Use `pickle` alternatives (#1453)\n\n* use json.dump and json.load in count_vectors_featurizer and lexical_syntactic_featurizer instead of pickle\n\n* update load and persist in sklearn intent classifier\n\n* update persist and load in dietclassifier\n\n* update load and persist in sklearn intent classifier\n\n* use json.dump and json.load in tracker featurizers\n\n* update persist and load of TEDPolicy\n\n* updated unexpected intent policy persist and load of model utilities.\n\n* save and load fake features\n\n* rename patterns.pkl to patterns.json\n\n* update poetry.lock\n\n* ruff formatting\n\n* move skops import\n\n* add comments\n\n* clean up save_features and load_features\n\n* WIP: update model data saving and loading\n\n* add tests for save and load features\n\n* update tests for test_tracker_featurizer\n\n* update tests for test_tracker_featurizer\n\n* WIP: serialization of feature arrays.\n\n* update serialization and deserialization for feature array\n\n* remove not needed tests/utils/tensorflow/test_model_data_storage.py\n\n* start writing tests for feature array\n\n* update feature array tests\n\n* update tests\n\n* fix linting\n\n* add changelog\n\n* add new dependencies to .github/dependabot.yml\n\n* fix some tests\n\n* fix loading and saving of unexpected intent ted policy\n\n* fix linting issue\n\n* fix converting of features in cvf and lsf\n\n* fix lint issues\n\n* convert vocab in cvf\n\n* fix linting\n\n* update crf entity extractor\n\n* fix to_dict of crf_token\n\n* addressed type issues\n\n* ruff formatting\n\n* fix typing and lint issues\n\n* remove cloudpickle dependency\n\n* update logistic_regression_classifier and remove joblib as dependency\n\n* update formatting of pyproject.toml\n\n* next try: update formatting of pyproject.toml\n\n* update logging\n\n* update poetry.lock\n\n* refactor loading of lexical_syntactic_featurizer\n\n* rename FeatureMetadata.type -> FeatureMetadata.data_type\n\n* clean up tests test_features.py and test_crf_entity_extractor.py\n\n* update test_feature_array.py\n\n* check for type when loading tracker featurizer.\n\n* update changelog\n\n* fix line too long\n\n* move import of skops\n\n* Prepared release of version 3.10.9.dev1 (#1496)\n\n* prepared release of version 3.10.9.dev1\n\n* update minimum model version\n\n* Check for 'step_id' and 'active_flow' keys in the metadata when adding 'ActionExecuted' event to flows paths stack.\n\n* fix parsing of commands\n\n* improve logging\n\n* formatting\n\n* add changelog\n\n* fix parse commands for multi step\n\n* [ATO-2985] - Windows model loading test (#1537)\n\n* Add test for model loading on windows\n\n* Improve the error message logged when handling the user message\n\n* Add a changelog\n\n* Fix Code Quality - line too long\n\n* Rasa-sdk-update (#1546)\n\n* all rasa-sdk micro updates\n\n* update poetry lock\n\n* update rasa-sdk in lock file\n\n* Remove trailing white sapce\n\n* Prepared release of version 3.10.11 (#1570)\n\n* prepared release of version 3.10.11\n\n* add comments again in pyproject.toml\n\n* update poetry.lock\n\n* revert changes in github workflows\n\n* undo changes in pyproject.toml\n\n* update changelog\n\n* revert changes in github workflows\n\n* update poetry.lock\n\n* update poetry.lock\n\n* update pyproject.toml\n\n* update poetry.lock\n\n* update setuptools = '>=65.5.1,<75.6.0'\n\n* update setuptools = '~75.3.0'\n\n* reformat code\n\n* undo deleting of ping_slack_about_package_release.sh\n\n* fix formatting and type issues\n\n* downgrade setuptools to 70.3.0\n\n* fixing logging issues (?)\n\n---------\n\nCo-authored-by: sancharigr <s.ghosh@rasa.com>",
    "before_after_code_files": [
      "poetry.lock||poetry.lock",
      "rasa/core/featurizers/single_state_featurizer.py||rasa/core/featurizers/single_state_featurizer.py",
      "rasa/core/featurizers/tracker_featurizers.py||rasa/core/featurizers/tracker_featurizers.py",
      "rasa/core/policies/ted_policy.py||rasa/core/policies/ted_policy.py",
      "rasa/core/policies/unexpected_intent_policy.py||rasa/core/policies/unexpected_intent_policy.py",
      "rasa/nlu/classifiers/diet_classifier.py||rasa/nlu/classifiers/diet_classifier.py",
      "rasa/nlu/classifiers/logistic_regression_classifier.py||rasa/nlu/classifiers/logistic_regression_classifier.py",
      "rasa/nlu/classifiers/sklearn_intent_classifier.py||rasa/nlu/classifiers/sklearn_intent_classifier.py",
      "rasa/nlu/extractors/crf_entity_extractor.py||rasa/nlu/extractors/crf_entity_extractor.py",
      "rasa/nlu/featurizers/sparse_featurizer/count_vectors_featurizer.py||rasa/nlu/featurizers/sparse_featurizer/count_vectors_featurizer.py",
      "rasa/nlu/featurizers/sparse_featurizer/lexical_syntactic_featurizer.py||rasa/nlu/featurizers/sparse_featurizer/lexical_syntactic_featurizer.py",
      "rasa/nlu/featurizers/sparse_featurizer/regex_featurizer.py||rasa/nlu/featurizers/sparse_featurizer/regex_featurizer.py",
      "rasa/shared/nlu/training_data/features.py||rasa/shared/nlu/training_data/features.py",
      "rasa/shared/utils/io.py||rasa/shared/utils/io.py",
      "rasa/utils/common.py||rasa/utils/common.py",
      "rasa/utils/io.py||rasa/utils/io.py",
      "rasa/utils/tensorflow/feature_array.py||rasa/utils/tensorflow/feature_array.py",
      "rasa/utils/tensorflow/model_data.py||rasa/utils/tensorflow/model_data.py",
      "scripts/ping_slack_about_package_release.sh||scripts/ping_slack_about_package_release.sh",
      "tests/core/featurizers/test_tracker_featurizer.py||tests/core/featurizers/test_tracker_featurizer.py",
      "tests/nlu/extractors/test_crf_entity_extractor.py||tests/nlu/extractors/test_crf_entity_extractor.py",
      "tests/shared/nlu/training_data/test_features.py||tests/shared/nlu/training_data/test_features.py",
      "tests/utils/tensorflow/test_feature_array.py||tests/utils/tensorflow/test_feature_array.py",
      "tests/utils/test_io.py||tests/utils/test_io.py"
    ]
  },
  "patch_diff": {
    "poetry.lock||poetry.lock": [
      "File: poetry.lock -> poetry.lock",
      "--- Hunk 1 ---",
      "[Context before]",
      "1412: name = \"filelock\"",
      "1413: version = \"3.12.2\"",
      "1414: description = \"A platform independent file lock.\"",
      "1416: python-versions = \">=3.7\"",
      "1417: files = [",
      "1418:     {file = \"filelock-3.12.2-py3-none-any.whl\", hash = \"sha256:cbb791cdea2a72f23da6ac5b5269ab0a0d161e9ef0100e653b69049a7706d1ec\"},",
      "",
      "[Removed Lines]",
      "1415: optional = true",
      "",
      "[Added Lines]",
      "1415: optional = false",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "2215: [[package]]",
      "2216: name = \"huggingface-hub\"",
      "2218: description = \"Client library to download and publish models, datasets and other repos on the huggingface.co hub\"",
      "2221: files = [",
      "2224: ]",
      "2226: [package.dependencies]",
      "2227: filelock = \"*\"",
      "2229: packaging = \">=20.9\"",
      "2230: pyyaml = \">=5.1\"",
      "2231: requests = \"*\"",
      "",
      "[Removed Lines]",
      "2217: version = \"0.16.2\"",
      "2219: optional = true",
      "2220: python-versions = \">=3.7.0\"",
      "2222:     {file = \"huggingface_hub-0.16.2-py3-none-any.whl\", hash = \"sha256:92facff575c11a8cf4b35d184ae67867a577a1b30865edcd8a9c5a48d2202133\"},",
      "2223:     {file = \"huggingface_hub-0.16.2.tar.gz\", hash = \"sha256:205abbf02a3408129a309f09e6d1a88d0c82de296b498682a813d9baa91c272f\"},",
      "2228: fsspec = \"*\"",
      "",
      "[Added Lines]",
      "2217: version = \"0.27.0\"",
      "2219: optional = false",
      "2220: python-versions = \">=3.8.0\"",
      "2222:     {file = \"huggingface_hub-0.27.0-py3-none-any.whl\", hash = \"sha256:8f2e834517f1f1ddf1ecc716f91b120d7333011b7485f665a9a412eacb1a2a81\"},",
      "2223:     {file = \"huggingface_hub-0.27.0.tar.gz\", hash = \"sha256:902cce1a1be5739f5589e560198a65a8edcfd3b830b1666f36e4b961f0454fac\"},",
      "2228: fsspec = \">=2023.5.0\"",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "2233: typing-extensions = \">=3.7.4.3\"",
      "2235: [package.extras]",
      "2237: cli = [\"InquirerPy (==0.3.4)\"]",
      "2239: fastai = [\"fastai (>=2.4)\", \"fastcore (>=1.3.27)\", \"toml\"]",
      "2242: tensorflow = [\"graphviz\", \"pydot\", \"tensorflow\"]",
      "2247: [[package]]",
      "2248: name = \"humanfriendly\"",
      "",
      "[Removed Lines]",
      "2236: all = [\"InquirerPy (==0.3.4)\", \"Jinja2\", \"Pillow\", \"aiohttp\", \"black (>=23.1,<24.0)\", \"gradio\", \"jedi\", \"mypy (==0.982)\", \"numpy\", \"pydantic\", \"pytest\", \"pytest-asyncio\", \"pytest-cov\", \"pytest-env\", \"pytest-vcr\", \"pytest-xdist\", \"ruff (>=0.0.241)\", \"soundfile\", \"types-PyYAML\", \"types-requests\", \"types-simplejson\", \"types-toml\", \"types-tqdm\", \"types-urllib3\", \"urllib3 (<2.0)\"]",
      "2238: dev = [\"InquirerPy (==0.3.4)\", \"Jinja2\", \"Pillow\", \"aiohttp\", \"black (>=23.1,<24.0)\", \"gradio\", \"jedi\", \"mypy (==0.982)\", \"numpy\", \"pydantic\", \"pytest\", \"pytest-asyncio\", \"pytest-cov\", \"pytest-env\", \"pytest-vcr\", \"pytest-xdist\", \"ruff (>=0.0.241)\", \"soundfile\", \"types-PyYAML\", \"types-requests\", \"types-simplejson\", \"types-toml\", \"types-tqdm\", \"types-urllib3\", \"urllib3 (<2.0)\"]",
      "2240: inference = [\"aiohttp\", \"pydantic\"]",
      "2241: quality = [\"black (>=23.1,<24.0)\", \"mypy (==0.982)\", \"ruff (>=0.0.241)\"]",
      "2243: testing = [\"InquirerPy (==0.3.4)\", \"Jinja2\", \"Pillow\", \"aiohttp\", \"gradio\", \"jedi\", \"numpy\", \"pydantic\", \"pytest\", \"pytest-asyncio\", \"pytest-cov\", \"pytest-env\", \"pytest-vcr\", \"pytest-xdist\", \"soundfile\", \"urllib3 (<2.0)\"]",
      "2244: torch = [\"torch\"]",
      "2245: typing = [\"pydantic\", \"types-PyYAML\", \"types-requests\", \"types-simplejson\", \"types-toml\", \"types-tqdm\", \"types-urllib3\"]",
      "",
      "[Added Lines]",
      "2236: all = [\"InquirerPy (==0.3.4)\", \"Jinja2\", \"Pillow\", \"aiohttp\", \"fastapi\", \"gradio (>=4.0.0)\", \"jedi\", \"libcst (==1.4.0)\", \"mypy (==1.5.1)\", \"numpy\", \"pytest (>=8.1.1,<8.2.2)\", \"pytest-asyncio\", \"pytest-cov\", \"pytest-env\", \"pytest-mock\", \"pytest-rerunfailures\", \"pytest-vcr\", \"pytest-xdist\", \"ruff (>=0.5.0)\", \"soundfile\", \"types-PyYAML\", \"types-requests\", \"types-simplejson\", \"types-toml\", \"types-tqdm\", \"types-urllib3\", \"typing-extensions (>=4.8.0)\", \"urllib3 (<2.0)\"]",
      "2238: dev = [\"InquirerPy (==0.3.4)\", \"Jinja2\", \"Pillow\", \"aiohttp\", \"fastapi\", \"gradio (>=4.0.0)\", \"jedi\", \"libcst (==1.4.0)\", \"mypy (==1.5.1)\", \"numpy\", \"pytest (>=8.1.1,<8.2.2)\", \"pytest-asyncio\", \"pytest-cov\", \"pytest-env\", \"pytest-mock\", \"pytest-rerunfailures\", \"pytest-vcr\", \"pytest-xdist\", \"ruff (>=0.5.0)\", \"soundfile\", \"types-PyYAML\", \"types-requests\", \"types-simplejson\", \"types-toml\", \"types-tqdm\", \"types-urllib3\", \"typing-extensions (>=4.8.0)\", \"urllib3 (<2.0)\"]",
      "2240: hf-transfer = [\"hf-transfer (>=0.1.4)\"]",
      "2241: inference = [\"aiohttp\"]",
      "2242: quality = [\"libcst (==1.4.0)\", \"mypy (==1.5.1)\", \"ruff (>=0.5.0)\"]",
      "2244: tensorflow-testing = [\"keras (<3.0)\", \"tensorflow\"]",
      "2245: testing = [\"InquirerPy (==0.3.4)\", \"Jinja2\", \"Pillow\", \"aiohttp\", \"fastapi\", \"gradio (>=4.0.0)\", \"jedi\", \"numpy\", \"pytest (>=8.1.1,<8.2.2)\", \"pytest-asyncio\", \"pytest-cov\", \"pytest-env\", \"pytest-mock\", \"pytest-rerunfailures\", \"pytest-vcr\", \"pytest-xdist\", \"soundfile\", \"urllib3 (<2.0)\"]",
      "2246: torch = [\"safetensors[torch]\", \"torch\"]",
      "2247: typing = [\"types-PyYAML\", \"types-requests\", \"types-simplejson\", \"types-toml\", \"types-tqdm\", \"types-urllib3\", \"typing-extensions (>=4.8.0)\"]",
      "",
      "---------------",
      "--- Hunk 4 ---",
      "[Context before]",
      "2417: [[package]]",
      "2418: name = \"joblib\"",
      "2420: description = \"Lightweight pipelining with Python functions\"",
      "2421: optional = false",
      "2423: files = [",
      "2426: ]",
      "2428: [[package]]",
      "2429: name = \"jsonpickle\"",
      "2432: optional = false",
      "2433: python-versions = \">=3.7\"",
      "2434: files = [",
      "2437: ]",
      "2439: [package.extras]",
      "2444: [[package]]",
      "2445: name = \"jsonschema\"",
      "",
      "[Removed Lines]",
      "2419: version = \"1.2.0\"",
      "2422: python-versions = \">=3.7\"",
      "2424:     {file = \"joblib-1.2.0-py3-none-any.whl\", hash = \"sha256:091138ed78f800342968c523bdde947e7a305b8594b910a0fea2ab83c3c6d385\"},",
      "2425:     {file = \"joblib-1.2.0.tar.gz\", hash = \"sha256:e1cee4a79e4af22881164f218d4311f60074197fb707e082e803b61f6d137018\"},",
      "2430: version = \"3.0.1\"",
      "2431: description = \"Python library for serializing any arbitrary object graph into JSON\"",
      "2435:     {file = \"jsonpickle-3.0.1-py2.py3-none-any.whl\", hash = \"sha256:130d8b293ea0add3845de311aaba55e6d706d0bb17bc123bd2c8baf8a39ac77c\"},",
      "2436:     {file = \"jsonpickle-3.0.1.tar.gz\", hash = \"sha256:032538804795e73b94ead410800ac387fdb6de98f8882ac957fcd247e3a85200\"},",
      "2440: docs = [\"jaraco.packaging (>=3.2)\", \"rst.linker (>=1.9)\", \"sphinx\"]",
      "2441: testing = [\"ecdsa\", \"feedparser\", \"gmpy2\", \"numpy\", \"pandas\", \"pymongo\", \"pytest (>=3.5,!=3.7.3)\", \"pytest-black-multipy\", \"pytest-checkdocs (>=1.2.3)\", \"pytest-cov\", \"pytest-flake8 (>=1.1.1)\", \"scikit-learn\", \"sqlalchemy\"]",
      "2442: testing-libs = [\"simplejson\", \"ujson\"]",
      "",
      "[Added Lines]",
      "2421: version = \"1.4.2\"",
      "2424: python-versions = \">=3.8\"",
      "2426:     {file = \"joblib-1.4.2-py3-none-any.whl\", hash = \"sha256:06d478d5674cbc267e7496a410ee875abd68e4340feff4490bcb7afb88060ae6\"},",
      "2427:     {file = \"joblib-1.4.2.tar.gz\", hash = \"sha256:2382c5816b2636fbd20a09e0f4e9dad4736765fdfb7dca582943b9c1366b3f0e\"},",
      "2432: version = \"3.0.4\"",
      "2433: description = \"Serialize any Python object to JSON\"",
      "2437:     {file = \"jsonpickle-3.0.4-py3-none-any.whl\", hash = \"sha256:04ae7567a14269579e3af66b76bda284587458d7e8a204951ca8f71a3309952e\"},",
      "2438:     {file = \"jsonpickle-3.0.4.tar.gz\", hash = \"sha256:a1b14c8d6221cd8f394f2a97e735ea1d7edc927fbd135b26f2f8700657c8c62b\"},",
      "2442: docs = [\"furo\", \"rst.linker (>=1.9)\", \"sphinx\"]",
      "2443: packaging = [\"build\", \"twine\"]",
      "2444: testing = [\"bson\", \"ecdsa\", \"feedparser\", \"gmpy2\", \"numpy\", \"pandas\", \"pymongo\", \"pytest (>=3.5,!=3.7.3)\", \"pytest-benchmark\", \"pytest-benchmark[histogram]\", \"pytest-checkdocs (>=1.2.3)\", \"pytest-cov\", \"pytest-enabler (>=1.0.1)\", \"pytest-ruff (>=0.2.1)\", \"scikit-learn\", \"scipy\", \"scipy (>=1.9.3)\", \"simplejson\", \"sqlalchemy\", \"ujson\"]",
      "",
      "---------------",
      "--- Hunk 5 ---",
      "[Context before]",
      "4746: [package.extras]",
      "4747: crt = [\"botocore[crt] (>=1.20.29,<2.0a.0)\"]",
      "4749: [[package]]",
      "4750: name = \"sanic\"",
      "4751: version = \"21.12.2\"",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "4751: [[package]]",
      "4752: name = \"safetensors\"",
      "4753: version = \"0.4.5\"",
      "4754: description = \"\"",
      "4755: optional = false",
      "4756: python-versions = \">=3.7\"",
      "4757: files = [",
      "4758:     {file = \"safetensors-0.4.5-cp310-cp310-macosx_10_12_x86_64.whl\", hash = \"sha256:a63eaccd22243c67e4f2b1c3e258b257effc4acd78f3b9d397edc8cf8f1298a7\"},",
      "4759:     {file = \"safetensors-0.4.5-cp310-cp310-macosx_11_0_arm64.whl\", hash = \"sha256:23fc9b4ec7b602915cbb4ec1a7c1ad96d2743c322f20ab709e2c35d1b66dad27\"},",
      "4760:     {file = \"safetensors-0.4.5-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:6885016f34bef80ea1085b7e99b3c1f92cb1be78a49839203060f67b40aee761\"},",
      "4761:     {file = \"safetensors-0.4.5-cp310-cp310-manylinux_2_17_armv7l.manylinux2014_armv7l.whl\", hash = \"sha256:133620f443450429322f238fda74d512c4008621227fccf2f8cf4a76206fea7c\"},",
      "4762:     {file = \"safetensors-0.4.5-cp310-cp310-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:4fb3e0609ec12d2a77e882f07cced530b8262027f64b75d399f1504ffec0ba56\"},",
      "4763:     {file = \"safetensors-0.4.5-cp310-cp310-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:d0f1dd769f064adc33831f5e97ad07babbd728427f98e3e1db6902e369122737\"},",
      "4764:     {file = \"safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:c6d156bdb26732feada84f9388a9f135528c1ef5b05fae153da365ad4319c4c5\"},",
      "4765:     {file = \"safetensors-0.4.5-cp310-cp310-manylinux_2_5_i686.manylinux1_i686.whl\", hash = \"sha256:9e347d77e2c77eb7624400ccd09bed69d35c0332f417ce8c048d404a096c593b\"},",
      "4766:     {file = \"safetensors-0.4.5-cp310-cp310-musllinux_1_1_aarch64.whl\", hash = \"sha256:9f556eea3aec1d3d955403159fe2123ddd68e880f83954ee9b4a3f2e15e716b6\"},",
      "4767:     {file = \"safetensors-0.4.5-cp310-cp310-musllinux_1_1_x86_64.whl\", hash = \"sha256:9483f42be3b6bc8ff77dd67302de8ae411c4db39f7224dec66b0eb95822e4163\"},",
      "4768:     {file = \"safetensors-0.4.5-cp310-none-win32.whl\", hash = \"sha256:7389129c03fadd1ccc37fd1ebbc773f2b031483b04700923c3511d2a939252cc\"},",
      "4769:     {file = \"safetensors-0.4.5-cp310-none-win_amd64.whl\", hash = \"sha256:e98ef5524f8b6620c8cdef97220c0b6a5c1cef69852fcd2f174bb96c2bb316b1\"},",
      "4770:     {file = \"safetensors-0.4.5-cp311-cp311-macosx_10_12_x86_64.whl\", hash = \"sha256:21f848d7aebd5954f92538552d6d75f7c1b4500f51664078b5b49720d180e47c\"},",
      "4771:     {file = \"safetensors-0.4.5-cp311-cp311-macosx_11_0_arm64.whl\", hash = \"sha256:bb07000b19d41e35eecef9a454f31a8b4718a185293f0d0b1c4b61d6e4487971\"},",
      "4772:     {file = \"safetensors-0.4.5-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:09dedf7c2fda934ee68143202acff6e9e8eb0ddeeb4cfc24182bef999efa9f42\"},",
      "4773:     {file = \"safetensors-0.4.5-cp311-cp311-manylinux_2_17_armv7l.manylinux2014_armv7l.whl\", hash = \"sha256:59b77e4b7a708988d84f26de3ebead61ef1659c73dcbc9946c18f3b1786d2688\"},",
      "4774:     {file = \"safetensors-0.4.5-cp311-cp311-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:5d3bc83e14d67adc2e9387e511097f254bd1b43c3020440e708858c684cbac68\"},",
      "4775:     {file = \"safetensors-0.4.5-cp311-cp311-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:39371fc551c1072976073ab258c3119395294cf49cdc1f8476794627de3130df\"},",
      "4776:     {file = \"safetensors-0.4.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:a6c19feda32b931cae0acd42748a670bdf56bee6476a046af20181ad3fee4090\"},",
      "4777:     {file = \"safetensors-0.4.5-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.whl\", hash = \"sha256:a659467495de201e2f282063808a41170448c78bada1e62707b07a27b05e6943\"},",
      "4778:     {file = \"safetensors-0.4.5-cp311-cp311-musllinux_1_1_aarch64.whl\", hash = \"sha256:bad5e4b2476949bcd638a89f71b6916fa9a5cae5c1ae7eede337aca2100435c0\"},",
      "4779:     {file = \"safetensors-0.4.5-cp311-cp311-musllinux_1_1_x86_64.whl\", hash = \"sha256:a3a315a6d0054bc6889a17f5668a73f94f7fe55121ff59e0a199e3519c08565f\"},",
      "4780:     {file = \"safetensors-0.4.5-cp311-none-win32.whl\", hash = \"sha256:a01e232e6d3d5cf8b1667bc3b657a77bdab73f0743c26c1d3c5dd7ce86bd3a92\"},",
      "4781:     {file = \"safetensors-0.4.5-cp311-none-win_amd64.whl\", hash = \"sha256:cbd39cae1ad3e3ef6f63a6f07296b080c951f24cec60188378e43d3713000c04\"},",
      "4782:     {file = \"safetensors-0.4.5-cp312-cp312-macosx_10_12_x86_64.whl\", hash = \"sha256:473300314e026bd1043cef391bb16a8689453363381561b8a3e443870937cc1e\"},",
      "4783:     {file = \"safetensors-0.4.5-cp312-cp312-macosx_11_0_arm64.whl\", hash = \"sha256:801183a0f76dc647f51a2d9141ad341f9665602a7899a693207a82fb102cc53e\"},",
      "4784:     {file = \"safetensors-0.4.5-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:1524b54246e422ad6fb6aea1ac71edeeb77666efa67230e1faf6999df9b2e27f\"},",
      "4785:     {file = \"safetensors-0.4.5-cp312-cp312-manylinux_2_17_armv7l.manylinux2014_armv7l.whl\", hash = \"sha256:b3139098e3e8b2ad7afbca96d30ad29157b50c90861084e69fcb80dec7430461\"},",
      "4786:     {file = \"safetensors-0.4.5-cp312-cp312-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:65573dc35be9059770808e276b017256fa30058802c29e1038eb1c00028502ea\"},",
      "4787:     {file = \"safetensors-0.4.5-cp312-cp312-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:fd33da8e9407559f8779c82a0448e2133737f922d71f884da27184549416bfed\"},",
      "4788:     {file = \"safetensors-0.4.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:3685ce7ed036f916316b567152482b7e959dc754fcc4a8342333d222e05f407c\"},",
      "4789:     {file = \"safetensors-0.4.5-cp312-cp312-manylinux_2_5_i686.manylinux1_i686.whl\", hash = \"sha256:dde2bf390d25f67908278d6f5d59e46211ef98e44108727084d4637ee70ab4f1\"},",
      "4790:     {file = \"safetensors-0.4.5-cp312-cp312-musllinux_1_1_aarch64.whl\", hash = \"sha256:7469d70d3de970b1698d47c11ebbf296a308702cbaae7fcb993944751cf985f4\"},",
      "4791:     {file = \"safetensors-0.4.5-cp312-cp312-musllinux_1_1_x86_64.whl\", hash = \"sha256:3a6ba28118636a130ccbb968bc33d4684c48678695dba2590169d5ab03a45646\"},",
      "4792:     {file = \"safetensors-0.4.5-cp312-none-win32.whl\", hash = \"sha256:c859c7ed90b0047f58ee27751c8e56951452ed36a67afee1b0a87847d065eec6\"},",
      "4793:     {file = \"safetensors-0.4.5-cp312-none-win_amd64.whl\", hash = \"sha256:b5a8810ad6a6f933fff6c276eae92c1da217b39b4d8b1bc1c0b8af2d270dc532\"},",
      "4794:     {file = \"safetensors-0.4.5-cp313-cp313-macosx_10_12_x86_64.whl\", hash = \"sha256:25e5f8e2e92a74f05b4ca55686234c32aac19927903792b30ee6d7bd5653d54e\"},",
      "4795:     {file = \"safetensors-0.4.5-cp313-cp313-macosx_11_0_arm64.whl\", hash = \"sha256:81efb124b58af39fcd684254c645e35692fea81c51627259cdf6d67ff4458916\"},",
      "4796:     {file = \"safetensors-0.4.5-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:585f1703a518b437f5103aa9cf70e9bd437cb78eea9c51024329e4fb8a3e3679\"},",
      "4797:     {file = \"safetensors-0.4.5-cp313-cp313-manylinux_2_17_armv7l.manylinux2014_armv7l.whl\", hash = \"sha256:4b99fbf72e3faf0b2f5f16e5e3458b93b7d0a83984fe8d5364c60aa169f2da89\"},",
      "4798:     {file = \"safetensors-0.4.5-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:b17b299ca9966ca983ecda1c0791a3f07f9ca6ab5ded8ef3d283fff45f6bcd5f\"},",
      "4799:     {file = \"safetensors-0.4.5-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:76ded72f69209c9780fdb23ea89e56d35c54ae6abcdec67ccb22af8e696e449a\"},",
      "4800:     {file = \"safetensors-0.4.5-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:2783956926303dcfeb1de91a4d1204cd4089ab441e622e7caee0642281109db3\"},",
      "4801:     {file = \"safetensors-0.4.5-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.whl\", hash = \"sha256:d94581aab8c6b204def4d7320f07534d6ee34cd4855688004a4354e63b639a35\"},",
      "4802:     {file = \"safetensors-0.4.5-cp313-cp313-musllinux_1_1_aarch64.whl\", hash = \"sha256:67e1e7cb8678bb1b37ac48ec0df04faf689e2f4e9e81e566b5c63d9f23748523\"},",
      "4803:     {file = \"safetensors-0.4.5-cp313-cp313-musllinux_1_1_x86_64.whl\", hash = \"sha256:dbd280b07e6054ea68b0cb4b16ad9703e7d63cd6890f577cb98acc5354780142\"},",
      "4804:     {file = \"safetensors-0.4.5-cp37-cp37m-macosx_10_12_x86_64.whl\", hash = \"sha256:77d9b228da8374c7262046a36c1f656ba32a93df6cc51cd4453af932011e77f1\"},",
      "4805:     {file = \"safetensors-0.4.5-cp37-cp37m-macosx_11_0_arm64.whl\", hash = \"sha256:500cac01d50b301ab7bb192353317035011c5ceeef0fca652f9f43c000bb7f8d\"},",
      "4806:     {file = \"safetensors-0.4.5-cp37-cp37m-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:75331c0c746f03158ded32465b7d0b0e24c5a22121743662a2393439c43a45cf\"},",
      "4807:     {file = \"safetensors-0.4.5-cp37-cp37m-manylinux_2_17_armv7l.manylinux2014_armv7l.whl\", hash = \"sha256:670e95fe34e0d591d0529e5e59fd9d3d72bc77b1444fcaa14dccda4f36b5a38b\"},",
      "4808:     {file = \"safetensors-0.4.5-cp37-cp37m-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:098923e2574ff237c517d6e840acada8e5b311cb1fa226019105ed82e9c3b62f\"},",
      "4809:     {file = \"safetensors-0.4.5-cp37-cp37m-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:13ca0902d2648775089fa6a0c8fc9e6390c5f8ee576517d33f9261656f851e3f\"},",
      "4810:     {file = \"safetensors-0.4.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:5f0032bedc869c56f8d26259fe39cd21c5199cd57f2228d817a0e23e8370af25\"},",
      "4811:     {file = \"safetensors-0.4.5-cp37-cp37m-manylinux_2_5_i686.manylinux1_i686.whl\", hash = \"sha256:f4b15f51b4f8f2a512341d9ce3475cacc19c5fdfc5db1f0e19449e75f95c7dc8\"},",
      "4812:     {file = \"safetensors-0.4.5-cp37-cp37m-musllinux_1_1_aarch64.whl\", hash = \"sha256:f6594d130d0ad933d885c6a7b75c5183cb0e8450f799b80a39eae2b8508955eb\"},",
      "4813:     {file = \"safetensors-0.4.5-cp37-cp37m-musllinux_1_1_x86_64.whl\", hash = \"sha256:60c828a27e852ded2c85fc0f87bf1ec20e464c5cd4d56ff0e0711855cc2e17f8\"},",
      "4814:     {file = \"safetensors-0.4.5-cp37-none-win32.whl\", hash = \"sha256:6d3de65718b86c3eeaa8b73a9c3d123f9307a96bbd7be9698e21e76a56443af5\"},",
      "4815:     {file = \"safetensors-0.4.5-cp37-none-win_amd64.whl\", hash = \"sha256:5a2d68a523a4cefd791156a4174189a4114cf0bf9c50ceb89f261600f3b2b81a\"},",
      "4816:     {file = \"safetensors-0.4.5-cp38-cp38-macosx_10_12_x86_64.whl\", hash = \"sha256:e7a97058f96340850da0601a3309f3d29d6191b0702b2da201e54c6e3e44ccf0\"},",
      "4817:     {file = \"safetensors-0.4.5-cp38-cp38-macosx_11_0_arm64.whl\", hash = \"sha256:63bfd425e25f5c733f572e2246e08a1c38bd6f2e027d3f7c87e2e43f228d1345\"},",
      "4818:     {file = \"safetensors-0.4.5-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:f3664ac565d0e809b0b929dae7ccd74e4d3273cd0c6d1220c6430035befb678e\"},",
      "4819:     {file = \"safetensors-0.4.5-cp38-cp38-manylinux_2_17_armv7l.manylinux2014_armv7l.whl\", hash = \"sha256:313514b0b9b73ff4ddfb4edd71860696dbe3c1c9dc4d5cc13dbd74da283d2cbf\"},",
      "4820:     {file = \"safetensors-0.4.5-cp38-cp38-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:31fa33ee326f750a2f2134a6174773c281d9a266ccd000bd4686d8021f1f3dac\"},",
      "4821:     {file = \"safetensors-0.4.5-cp38-cp38-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:09566792588d77b68abe53754c9f1308fadd35c9f87be939e22c623eaacbed6b\"},",
      "4822:     {file = \"safetensors-0.4.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:309aaec9b66cbf07ad3a2e5cb8a03205663324fea024ba391594423d0f00d9fe\"},",
      "4823:     {file = \"safetensors-0.4.5-cp38-cp38-manylinux_2_5_i686.manylinux1_i686.whl\", hash = \"sha256:53946c5813b8f9e26103c5efff4a931cc45d874f45229edd68557ffb35ffb9f8\"},",
      "4824:     {file = \"safetensors-0.4.5-cp38-cp38-musllinux_1_1_aarch64.whl\", hash = \"sha256:868f9df9e99ad1e7f38c52194063a982bc88fedc7d05096f4f8160403aaf4bd6\"},",
      "4825:     {file = \"safetensors-0.4.5-cp38-cp38-musllinux_1_1_x86_64.whl\", hash = \"sha256:9cc9449bd0b0bc538bd5e268221f0c5590bc5c14c1934a6ae359d44410dc68c4\"},",
      "4826:     {file = \"safetensors-0.4.5-cp38-none-win32.whl\", hash = \"sha256:83c4f13a9e687335c3928f615cd63a37e3f8ef072a3f2a0599fa09f863fb06a2\"},",
      "4827:     {file = \"safetensors-0.4.5-cp38-none-win_amd64.whl\", hash = \"sha256:b98d40a2ffa560653f6274e15b27b3544e8e3713a44627ce268f419f35c49478\"},",
      "4828:     {file = \"safetensors-0.4.5-cp39-cp39-macosx_10_12_x86_64.whl\", hash = \"sha256:cf727bb1281d66699bef5683b04d98c894a2803442c490a8d45cd365abfbdeb2\"},",
      "4829:     {file = \"safetensors-0.4.5-cp39-cp39-macosx_11_0_arm64.whl\", hash = \"sha256:96f1d038c827cdc552d97e71f522e1049fef0542be575421f7684756a748e457\"},",
      "4830:     {file = \"safetensors-0.4.5-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:139fbee92570ecea774e6344fee908907db79646d00b12c535f66bc78bd5ea2c\"},",
      "4831:     {file = \"safetensors-0.4.5-cp39-cp39-manylinux_2_17_armv7l.manylinux2014_armv7l.whl\", hash = \"sha256:c36302c1c69eebb383775a89645a32b9d266878fab619819ce660309d6176c9b\"},",
      "4832:     {file = \"safetensors-0.4.5-cp39-cp39-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:d641f5b8149ea98deb5ffcf604d764aad1de38a8285f86771ce1abf8e74c4891\"},",
      "4833:     {file = \"safetensors-0.4.5-cp39-cp39-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:b4db6a61d968de73722b858038c616a1bebd4a86abe2688e46ca0cc2d17558f2\"},",
      "4834:     {file = \"safetensors-0.4.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:b75a616e02f21b6f1d5785b20cecbab5e2bd3f6358a90e8925b813d557666ec1\"},",
      "4835:     {file = \"safetensors-0.4.5-cp39-cp39-manylinux_2_5_i686.manylinux1_i686.whl\", hash = \"sha256:788ee7d04cc0e0e7f944c52ff05f52a4415b312f5efd2ee66389fb7685ee030c\"},",
      "4836:     {file = \"safetensors-0.4.5-cp39-cp39-musllinux_1_1_aarch64.whl\", hash = \"sha256:87bc42bd04fd9ca31396d3ca0433db0be1411b6b53ac5a32b7845a85d01ffc2e\"},",
      "4837:     {file = \"safetensors-0.4.5-cp39-cp39-musllinux_1_1_x86_64.whl\", hash = \"sha256:4037676c86365a721a8c9510323a51861d703b399b78a6b4486a54a65a975fca\"},",
      "4838:     {file = \"safetensors-0.4.5-cp39-none-win32.whl\", hash = \"sha256:1500418454529d0ed5c1564bda376c4ddff43f30fce9517d9bee7bcce5a8ef50\"},",
      "4839:     {file = \"safetensors-0.4.5-cp39-none-win_amd64.whl\", hash = \"sha256:9d1a94b9d793ed8fe35ab6d5cea28d540a46559bafc6aae98f30ee0867000cab\"},",
      "4840:     {file = \"safetensors-0.4.5-pp310-pypy310_pp73-macosx_10_12_x86_64.whl\", hash = \"sha256:fdadf66b5a22ceb645d5435a0be7a0292ce59648ca1d46b352f13cff3ea80410\"},",
      "4841:     {file = \"safetensors-0.4.5-pp310-pypy310_pp73-macosx_11_0_arm64.whl\", hash = \"sha256:d42ffd4c2259f31832cb17ff866c111684c87bd930892a1ba53fed28370c918c\"},",
      "4842:     {file = \"safetensors-0.4.5-pp310-pypy310_pp73-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:dd8a1f6d2063a92cd04145c7fd9e31a1c7d85fbec20113a14b487563fdbc0597\"},",
      "4843:     {file = \"safetensors-0.4.5-pp310-pypy310_pp73-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:951d2fcf1817f4fb0ef0b48f6696688a4e852a95922a042b3f96aaa67eedc920\"},",
      "4844:     {file = \"safetensors-0.4.5-pp310-pypy310_pp73-manylinux_2_5_i686.manylinux1_i686.whl\", hash = \"sha256:6ac85d9a8c1af0e3132371d9f2d134695a06a96993c2e2f0bbe25debb9e3f67a\"},",
      "4845:     {file = \"safetensors-0.4.5-pp310-pypy310_pp73-musllinux_1_1_aarch64.whl\", hash = \"sha256:e3cec4a29eb7fe8da0b1c7988bc3828183080439dd559f720414450de076fcab\"},",
      "4846:     {file = \"safetensors-0.4.5-pp310-pypy310_pp73-musllinux_1_1_x86_64.whl\", hash = \"sha256:21742b391b859e67b26c0b2ac37f52c9c0944a879a25ad2f9f9f3cd61e7fda8f\"},",
      "4847:     {file = \"safetensors-0.4.5-pp37-pypy37_pp73-macosx_10_12_x86_64.whl\", hash = \"sha256:c7db3006a4915151ce1913652e907cdede299b974641a83fbc092102ac41b644\"},",
      "4848:     {file = \"safetensors-0.4.5-pp37-pypy37_pp73-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:f68bf99ea970960a237f416ea394e266e0361895753df06e3e06e6ea7907d98b\"},",
      "4849:     {file = \"safetensors-0.4.5-pp37-pypy37_pp73-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:8158938cf3324172df024da511839d373c40fbfaa83e9abf467174b2910d7b4c\"},",
      "4850:     {file = \"safetensors-0.4.5-pp37-pypy37_pp73-manylinux_2_5_i686.manylinux1_i686.whl\", hash = \"sha256:540ce6c4bf6b58cb0fd93fa5f143bc0ee341c93bb4f9287ccd92cf898cc1b0dd\"},",
      "4851:     {file = \"safetensors-0.4.5-pp37-pypy37_pp73-musllinux_1_1_aarch64.whl\", hash = \"sha256:bfeaa1a699c6b9ed514bd15e6a91e74738b71125a9292159e3d6b7f0a53d2cde\"},",
      "4852:     {file = \"safetensors-0.4.5-pp37-pypy37_pp73-musllinux_1_1_x86_64.whl\", hash = \"sha256:01c8f00da537af711979e1b42a69a8ec9e1d7112f208e0e9b8a35d2c381085ef\"},",
      "4853:     {file = \"safetensors-0.4.5-pp38-pypy38_pp73-macosx_10_12_x86_64.whl\", hash = \"sha256:a0dd565f83b30f2ca79b5d35748d0d99dd4b3454f80e03dfb41f0038e3bdf180\"},",
      "4854:     {file = \"safetensors-0.4.5-pp38-pypy38_pp73-macosx_11_0_arm64.whl\", hash = \"sha256:023b6e5facda76989f4cba95a861b7e656b87e225f61811065d5c501f78cdb3f\"},",
      "4855:     {file = \"safetensors-0.4.5-pp38-pypy38_pp73-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:9633b663393d5796f0b60249549371e392b75a0b955c07e9c6f8708a87fc841f\"},",
      "4856:     {file = \"safetensors-0.4.5-pp38-pypy38_pp73-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:78dd8adfb48716233c45f676d6e48534d34b4bceb50162c13d1f0bdf6f78590a\"},",
      "4857:     {file = \"safetensors-0.4.5-pp38-pypy38_pp73-manylinux_2_5_i686.manylinux1_i686.whl\", hash = \"sha256:8e8deb16c4321d61ae72533b8451ec4a9af8656d1c61ff81aa49f966406e4b68\"},",
      "4858:     {file = \"safetensors-0.4.5-pp38-pypy38_pp73-musllinux_1_1_aarch64.whl\", hash = \"sha256:52452fa5999dc50c4decaf0c53aa28371f7f1e0fe5c2dd9129059fbe1e1599c7\"},",
      "4859:     {file = \"safetensors-0.4.5-pp38-pypy38_pp73-musllinux_1_1_x86_64.whl\", hash = \"sha256:d5f23198821e227cfc52d50fa989813513db381255c6d100927b012f0cfec63d\"},",
      "4860:     {file = \"safetensors-0.4.5-pp39-pypy39_pp73-macosx_10_12_x86_64.whl\", hash = \"sha256:f4beb84b6073b1247a773141a6331117e35d07134b3bb0383003f39971d414bb\"},",
      "4861:     {file = \"safetensors-0.4.5-pp39-pypy39_pp73-macosx_11_0_arm64.whl\", hash = \"sha256:68814d599d25ed2fdd045ed54d370d1d03cf35e02dce56de44c651f828fb9b7b\"},",
      "4862:     {file = \"safetensors-0.4.5-pp39-pypy39_pp73-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:f0b6453c54c57c1781292c46593f8a37254b8b99004c68d6c3ce229688931a22\"},",
      "4863:     {file = \"safetensors-0.4.5-pp39-pypy39_pp73-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:adaa9c6dead67e2dd90d634f89131e43162012479d86e25618e821a03d1eb1dc\"},",
      "4864:     {file = \"safetensors-0.4.5-pp39-pypy39_pp73-manylinux_2_5_i686.manylinux1_i686.whl\", hash = \"sha256:73e7d408e9012cd17511b382b43547850969c7979efc2bc353f317abaf23c84c\"},",
      "4865:     {file = \"safetensors-0.4.5-pp39-pypy39_pp73-musllinux_1_1_aarch64.whl\", hash = \"sha256:775409ce0fcc58b10773fdb4221ed1eb007de10fe7adbdf8f5e8a56096b6f0bc\"},",
      "4866:     {file = \"safetensors-0.4.5-pp39-pypy39_pp73-musllinux_1_1_x86_64.whl\", hash = \"sha256:834001bed193e4440c4a3950a31059523ee5090605c907c66808664c932b549c\"},",
      "4867:     {file = \"safetensors-0.4.5.tar.gz\", hash = \"sha256:d73de19682deabb02524b3d5d1f8b3aaba94c72f1bbfc7911b9b9d5d391c0310\"},",
      "4868: ]",
      "4870: [package.extras]",
      "4871: all = [\"safetensors[jax]\", \"safetensors[numpy]\", \"safetensors[paddlepaddle]\", \"safetensors[pinned-tf]\", \"safetensors[quality]\", \"safetensors[testing]\", \"safetensors[torch]\"]",
      "4872: dev = [\"safetensors[all]\"]",
      "4873: jax = [\"flax (>=0.6.3)\", \"jax (>=0.3.25)\", \"jaxlib (>=0.3.25)\", \"safetensors[numpy]\"]",
      "4874: mlx = [\"mlx (>=0.0.9)\"]",
      "4875: numpy = [\"numpy (>=1.21.6)\"]",
      "4876: paddlepaddle = [\"paddlepaddle (>=2.4.1)\", \"safetensors[numpy]\"]",
      "4877: pinned-tf = [\"safetensors[numpy]\", \"tensorflow (==2.11.0)\"]",
      "4878: quality = [\"black (==22.3)\", \"click (==8.0.4)\", \"flake8 (>=3.8.3)\", \"isort (>=5.5.4)\"]",
      "4879: tensorflow = [\"safetensors[numpy]\", \"tensorflow (>=2.11.0)\"]",
      "4880: testing = [\"h5py (>=3.7.0)\", \"huggingface-hub (>=0.12.1)\", \"hypothesis (>=6.70.2)\", \"pytest (>=7.2.0)\", \"pytest-benchmark (>=4.0.0)\", \"safetensors[numpy]\", \"setuptools-rust (>=1.5.2)\"]",
      "4881: torch = [\"safetensors[numpy]\", \"torch (>=1.10)\"]",
      "",
      "---------------",
      "--- Hunk 6 ---",
      "[Context before]",
      "5015: [[package]]",
      "5016: name = \"setuptools\"",
      "5018: description = \"Easily download, build, install, upgrade, and uninstall Python packages\"",
      "5019: optional = false",
      "5021: files = [",
      "5024: ]",
      "5026: [package.extras]",
      "5031: [[package]]",
      "5032: name = \"six\"",
      "",
      "[Removed Lines]",
      "5017: version = \"68.0.0\"",
      "5020: python-versions = \">=3.7\"",
      "5022:     {file = \"setuptools-68.0.0-py3-none-any.whl\", hash = \"sha256:11e52c67415a381d10d6b462ced9cfb97066179f0e871399e006c4ab101fc85f\"},",
      "5023:     {file = \"setuptools-68.0.0.tar.gz\", hash = \"sha256:baf1fdb41c6da4cd2eae722e135500da913332ab3f2f5c7d33af9b492acb5235\"},",
      "5027: docs = [\"furo\", \"jaraco.packaging (>=9)\", \"jaraco.tidelift (>=1.4)\", \"pygments-github-lexers (==0.0.5)\", \"rst.linker (>=1.9)\", \"sphinx (>=3.5)\", \"sphinx-favicon\", \"sphinx-hoverxref (<2)\", \"sphinx-inline-tabs\", \"sphinx-lint\", \"sphinx-notfound-page (==0.8.3)\", \"sphinx-reredirects\", \"sphinxcontrib-towncrier\"]",
      "5028: testing = [\"build[virtualenv]\", \"filelock (>=3.4.0)\", \"flake8-2020\", \"ini2toml[lite] (>=0.9)\", \"jaraco.envs (>=2.2)\", \"jaraco.path (>=3.2.0)\", \"pip (>=19.1)\", \"pip-run (>=8.8)\", \"pytest (>=6)\", \"pytest-black (>=0.3.7)\", \"pytest-checkdocs (>=2.4)\", \"pytest-cov\", \"pytest-enabler (>=1.3)\", \"pytest-mypy (>=0.9.1)\", \"pytest-perf\", \"pytest-ruff\", \"pytest-timeout\", \"pytest-xdist\", \"tomli-w (>=1.0.0)\", \"virtualenv (>=13.0.0)\", \"wheel\"]",
      "5029: testing-integration = [\"build[virtualenv]\", \"filelock (>=3.4.0)\", \"jaraco.envs (>=2.2)\", \"jaraco.path (>=3.2.0)\", \"pytest\", \"pytest-enabler\", \"pytest-xdist\", \"tomli\", \"virtualenv (>=13.0.0)\", \"wheel\"]",
      "",
      "[Added Lines]",
      "5151: version = \"70.3.0\"",
      "5154: python-versions = \">=3.8\"",
      "5156:     {file = \"setuptools-70.3.0-py3-none-any.whl\", hash = \"sha256:fe384da74336c398e0d956d1cae0669bc02eed936cdb1d49b57de1990dc11ffc\"},",
      "5157:     {file = \"setuptools-70.3.0.tar.gz\", hash = \"sha256:f171bab1dfbc86b132997f26a119f6056a57950d058587841a0082e8830f9dc5\"},",
      "5161: doc = [\"furo\", \"jaraco.packaging (>=9.3)\", \"jaraco.tidelift (>=1.4)\", \"pygments-github-lexers (==0.0.5)\", \"pyproject-hooks (!=1.1)\", \"rst.linker (>=1.9)\", \"sphinx (>=3.5)\", \"sphinx-favicon\", \"sphinx-inline-tabs\", \"sphinx-lint\", \"sphinx-notfound-page (>=1,<2)\", \"sphinx-reredirects\", \"sphinxcontrib-towncrier\"]",
      "5162: test = [\"build[virtualenv] (>=1.0.3)\", \"filelock (>=3.4.0)\", \"importlib-metadata\", \"ini2toml[lite] (>=0.14)\", \"jaraco.develop (>=7.21)\", \"jaraco.envs (>=2.2)\", \"jaraco.path (>=3.2.0)\", \"jaraco.test\", \"mypy (==1.10.0)\", \"packaging (>=23.2)\", \"pip (>=19.1)\", \"pyproject-hooks (!=1.1)\", \"pytest (>=6,!=8.1.*)\", \"pytest-checkdocs (>=2.4)\", \"pytest-cov\", \"pytest-enabler (>=2.2)\", \"pytest-home (>=0.5)\", \"pytest-mypy\", \"pytest-perf\", \"pytest-ruff (>=0.3.2)\", \"pytest-subprocess\", \"pytest-timeout\", \"pytest-xdist (>=3)\", \"tomli\", \"tomli-w (>=1.0.0)\", \"virtualenv (>=13.0.0)\", \"wheel\"]",
      "",
      "---------------",
      "--- Hunk 7 ---",
      "[Context before]",
      "5056: tabulate = \"*\"",
      "5057: tqdm = \">=2.0\"",
      "5059: [[package]]",
      "5060: name = \"slack-sdk\"",
      "5061: version = \"3.21.3\"",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "5192: [[package]]",
      "5193: name = \"skops\"",
      "5194: version = \"0.9.0\"",
      "5195: description = \"A set of tools to push scikit-learn based models to and pull from Hugging Face Hub\"",
      "5196: optional = false",
      "5197: python-versions = \">=3.8\"",
      "5198: files = [",
      "5199:     {file = \"skops-0.9.0-py3-none-any.whl\", hash = \"sha256:05645199bf6976e1f6dbba4a0704799cd5d2fcef18a98b069b4c84744e1a80a1\"},",
      "5200:     {file = \"skops-0.9.0.tar.gz\", hash = \"sha256:3e39333d65f26d5863ad44db5001b4cfe6a29642274ac37af54fb834813aee3f\"},",
      "5201: ]",
      "5203: [package.dependencies]",
      "5204: huggingface-hub = \">=0.17.0\"",
      "5205: packaging = \">=17.0\"",
      "5206: scikit-learn = \">=0.24\"",
      "5207: tabulate = \">=0.8.8\"",
      "5209: [package.extras]",
      "5210: docs = [\"fairlearn (>=0.7.0)\", \"matplotlib (>=3.3)\", \"numpydoc (>=1.0.0)\", \"pandas (>=1)\", \"scikit-learn-intelex (>=2021.7.1)\", \"sphinx (>=3.2.0)\", \"sphinx-gallery (>=0.7.0)\", \"sphinx-issues (>=1.2.0)\", \"sphinx-prompt (>=1.3.0)\", \"sphinx-rtd-theme (>=1)\"]",
      "5211: rich = [\"rich (>=12)\"]",
      "5212: tests = [\"catboost (>=1.0)\", \"fairlearn (>=0.7.0)\", \"flake8 (>=3.8.2)\", \"flaky (>=3.7.0)\", \"lightgbm (>=3)\", \"matplotlib (>=3.3)\", \"pandas (>=1)\", \"pytest (>=5.0.1)\", \"pytest-cov (>=2.9.0)\", \"quantile-forest (>=1.0.0)\", \"rich (>=12)\", \"types-requests (>=2.28.5)\", \"xgboost (>=1.6)\"]",
      "",
      "---------------",
      "--- Hunk 8 ---",
      "[Context before]",
      "5964: [[package]]",
      "5965: name = \"tokenizers\"",
      "5968: optional = true",
      "5970: files = [",
      "6016: testing = [\"black (==22.3)\", \"datasets\", \"numpy\", \"pytest\", \"requests\"]",
      "6018: [[package]]",
      "",
      "[Removed Lines]",
      "5966: version = \"0.13.3\"",
      "5967: description = \"Fast and Customizable Tokenizers\"",
      "5969: python-versions = \"*\"",
      "5971:     {file = \"tokenizers-0.13.3-cp310-cp310-macosx_10_11_x86_64.whl\", hash = \"sha256:f3835c5be51de8c0a092058a4d4380cb9244fb34681fd0a295fbf0a52a5fdf33\"},",
      "5972:     {file = \"tokenizers-0.13.3-cp310-cp310-macosx_12_0_arm64.whl\", hash = \"sha256:4ef4c3e821730f2692489e926b184321e887f34fb8a6b80b8096b966ba663d07\"},",
      "5973:     {file = \"tokenizers-0.13.3-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:c5fd1a6a25353e9aa762e2aae5a1e63883cad9f4e997c447ec39d071020459bc\"},",
      "5974:     {file = \"tokenizers-0.13.3-cp310-cp310-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:ee0b1b311d65beab83d7a41c56a1e46ab732a9eed4460648e8eb0bd69fc2d059\"},",
      "5975:     {file = \"tokenizers-0.13.3-cp310-cp310-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:5ef4215284df1277dadbcc5e17d4882bda19f770d02348e73523f7e7d8b8d396\"},",
      "5976:     {file = \"tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:a4d53976079cff8a033f778fb9adca2d9d69d009c02fa2d71a878b5f3963ed30\"},",
      "5977:     {file = \"tokenizers-0.13.3-cp310-cp310-win32.whl\", hash = \"sha256:1f0e3b4c2ea2cd13238ce43548959c118069db7579e5d40ec270ad77da5833ce\"},",
      "5978:     {file = \"tokenizers-0.13.3-cp310-cp310-win_amd64.whl\", hash = \"sha256:89649c00d0d7211e8186f7a75dfa1db6996f65edce4b84821817eadcc2d3c79e\"},",
      "5979:     {file = \"tokenizers-0.13.3-cp311-cp311-macosx_10_11_universal2.whl\", hash = \"sha256:56b726e0d2bbc9243872b0144515ba684af5b8d8cd112fb83ee1365e26ec74c8\"},",
      "5980:     {file = \"tokenizers-0.13.3-cp311-cp311-macosx_12_0_arm64.whl\", hash = \"sha256:cc5c022ce692e1f499d745af293ab9ee6f5d92538ed2faf73f9708c89ee59ce6\"},",
      "5981:     {file = \"tokenizers-0.13.3-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:f55c981ac44ba87c93e847c333e58c12abcbb377a0c2f2ef96e1a266e4184ff2\"},",
      "5982:     {file = \"tokenizers-0.13.3-cp311-cp311-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:f247eae99800ef821a91f47c5280e9e9afaeed9980fc444208d5aa6ba69ff148\"},",
      "5983:     {file = \"tokenizers-0.13.3-cp311-cp311-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:4b3e3215d048e94f40f1c95802e45dcc37c5b05eb46280fc2ccc8cd351bff839\"},",
      "5984:     {file = \"tokenizers-0.13.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:9ba2b0bf01777c9b9bc94b53764d6684554ce98551fec496f71bc5be3a03e98b\"},",
      "5985:     {file = \"tokenizers-0.13.3-cp311-cp311-win32.whl\", hash = \"sha256:cc78d77f597d1c458bf0ea7c2a64b6aa06941c7a99cb135b5969b0278824d808\"},",
      "5986:     {file = \"tokenizers-0.13.3-cp311-cp311-win_amd64.whl\", hash = \"sha256:ecf182bf59bd541a8876deccf0360f5ae60496fd50b58510048020751cf1724c\"},",
      "5987:     {file = \"tokenizers-0.13.3-cp37-cp37m-macosx_10_11_x86_64.whl\", hash = \"sha256:0527dc5436a1f6bf2c0327da3145687d3bcfbeab91fed8458920093de3901b44\"},",
      "5988:     {file = \"tokenizers-0.13.3-cp37-cp37m-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:07cbb2c307627dc99b44b22ef05ff4473aa7c7cc1fec8f0a8b37d8a64b1a16d2\"},",
      "5989:     {file = \"tokenizers-0.13.3-cp37-cp37m-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:4560dbdeaae5b7ee0d4e493027e3de6d53c991b5002d7ff95083c99e11dd5ac0\"},",
      "5990:     {file = \"tokenizers-0.13.3-cp37-cp37m-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:64064bd0322405c9374305ab9b4c07152a1474370327499911937fd4a76d004b\"},",
      "5991:     {file = \"tokenizers-0.13.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:b8c6e2ab0f2e3d939ca66aa1d596602105fe33b505cd2854a4c1717f704c51de\"},",
      "5992:     {file = \"tokenizers-0.13.3-cp37-cp37m-win32.whl\", hash = \"sha256:6cc29d410768f960db8677221e497226e545eaaea01aa3613fa0fdf2cc96cff4\"},",
      "5993:     {file = \"tokenizers-0.13.3-cp37-cp37m-win_amd64.whl\", hash = \"sha256:fc2a7fdf864554a0dacf09d32e17c0caa9afe72baf9dd7ddedc61973bae352d8\"},",
      "5994:     {file = \"tokenizers-0.13.3-cp38-cp38-macosx_10_11_x86_64.whl\", hash = \"sha256:8791dedba834c1fc55e5f1521be325ea3dafb381964be20684b92fdac95d79b7\"},",
      "5995:     {file = \"tokenizers-0.13.3-cp38-cp38-macosx_12_0_arm64.whl\", hash = \"sha256:d607a6a13718aeb20507bdf2b96162ead5145bbbfa26788d6b833f98b31b26e1\"},",
      "5996:     {file = \"tokenizers-0.13.3-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:3791338f809cd1bf8e4fee6b540b36822434d0c6c6bc47162448deee3f77d425\"},",
      "5997:     {file = \"tokenizers-0.13.3-cp38-cp38-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:c2f35f30e39e6aab8716f07790f646bdc6e4a853816cc49a95ef2a9016bf9ce6\"},",
      "5998:     {file = \"tokenizers-0.13.3-cp38-cp38-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:310204dfed5aa797128b65d63538a9837cbdd15da2a29a77d67eefa489edda26\"},",
      "5999:     {file = \"tokenizers-0.13.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:a0f9b92ea052305166559f38498b3b0cae159caea712646648aaa272f7160963\"},",
      "6000:     {file = \"tokenizers-0.13.3-cp38-cp38-win32.whl\", hash = \"sha256:9a3fa134896c3c1f0da6e762d15141fbff30d094067c8f1157b9fdca593b5806\"},",
      "6001:     {file = \"tokenizers-0.13.3-cp38-cp38-win_amd64.whl\", hash = \"sha256:8e7b0cdeace87fa9e760e6a605e0ae8fc14b7d72e9fc19c578116f7287bb873d\"},",
      "6002:     {file = \"tokenizers-0.13.3-cp39-cp39-macosx_10_11_x86_64.whl\", hash = \"sha256:00cee1e0859d55507e693a48fa4aef07060c4bb6bd93d80120e18fea9371c66d\"},",
      "6003:     {file = \"tokenizers-0.13.3-cp39-cp39-macosx_12_0_arm64.whl\", hash = \"sha256:a23ff602d0797cea1d0506ce69b27523b07e70f6dda982ab8cf82402de839088\"},",
      "6004:     {file = \"tokenizers-0.13.3-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:70ce07445050b537d2696022dafb115307abdffd2a5c106f029490f84501ef97\"},",
      "6005:     {file = \"tokenizers-0.13.3-cp39-cp39-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:280ffe95f50eaaf655b3a1dc7ff1d9cf4777029dbbc3e63a74e65a056594abc3\"},",
      "6006:     {file = \"tokenizers-0.13.3-cp39-cp39-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:97acfcec592f7e9de8cadcdcda50a7134423ac8455c0166b28c9ff04d227b371\"},",
      "6007:     {file = \"tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:dd7730c98a3010cd4f523465867ff95cd9d6430db46676ce79358f65ae39797b\"},",
      "6008:     {file = \"tokenizers-0.13.3-cp39-cp39-win32.whl\", hash = \"sha256:48625a108029cb1ddf42e17a81b5a3230ba6888a70c9dc14e81bc319e812652d\"},",
      "6009:     {file = \"tokenizers-0.13.3-cp39-cp39-win_amd64.whl\", hash = \"sha256:bc0a6f1ba036e482db6453571c9e3e60ecd5489980ffd95d11dc9f960483d783\"},",
      "6010:     {file = \"tokenizers-0.13.3.tar.gz\", hash = \"sha256:2e546dbb68b623008a5442353137fbb0123d311a6d7ba52f2667c8862a75af2e\"},",
      "6011: ]",
      "6013: [package.extras]",
      "6014: dev = [\"black (==22.3)\", \"datasets\", \"numpy\", \"pytest\", \"requests\"]",
      "6015: docs = [\"setuptools-rust\", \"sphinx\", \"sphinx-rtd-theme\"]",
      "",
      "[Added Lines]",
      "6121: version = \"0.15.2\"",
      "6122: description = \"\"",
      "6124: python-versions = \">=3.7\"",
      "6126:     {file = \"tokenizers-0.15.2-cp310-cp310-macosx_10_12_x86_64.whl\", hash = \"sha256:52f6130c9cbf70544287575a985bf44ae1bda2da7e8c24e97716080593638012\"},",
      "6127:     {file = \"tokenizers-0.15.2-cp310-cp310-macosx_11_0_arm64.whl\", hash = \"sha256:054c1cc9c6d68f7ffa4e810b3d5131e0ba511b6e4be34157aa08ee54c2f8d9ee\"},",
      "6128:     {file = \"tokenizers-0.15.2-cp310-cp310-manylinux_2_12_i686.manylinux2010_i686.whl\", hash = \"sha256:a9b9b070fdad06e347563b88c278995735292ded1132f8657084989a4c84a6d5\"},",
      "6129:     {file = \"tokenizers-0.15.2-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:ea621a7eef4b70e1f7a4e84dd989ae3f0eeb50fc8690254eacc08acb623e82f1\"},",
      "6130:     {file = \"tokenizers-0.15.2-cp310-cp310-manylinux_2_17_armv7l.manylinux2014_armv7l.whl\", hash = \"sha256:cf7fd9a5141634fa3aa8d6b7be362e6ae1b4cda60da81388fa533e0b552c98fd\"},",
      "6131:     {file = \"tokenizers-0.15.2-cp310-cp310-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:44f2a832cd0825295f7179eaf173381dc45230f9227ec4b44378322d900447c9\"},",
      "6132:     {file = \"tokenizers-0.15.2-cp310-cp310-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:8b9ec69247a23747669ec4b0ca10f8e3dfb3545d550258129bd62291aabe8605\"},",
      "6133:     {file = \"tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:40b6a4c78da863ff26dbd5ad9a8ecc33d8a8d97b535172601cf00aee9d7ce9ce\"},",
      "6134:     {file = \"tokenizers-0.15.2-cp310-cp310-musllinux_1_1_aarch64.whl\", hash = \"sha256:5ab2a4d21dcf76af60e05af8063138849eb1d6553a0d059f6534357bce8ba364\"},",
      "6135:     {file = \"tokenizers-0.15.2-cp310-cp310-musllinux_1_1_x86_64.whl\", hash = \"sha256:a47acfac7e511f6bbfcf2d3fb8c26979c780a91e06fb5b9a43831b2c0153d024\"},",
      "6136:     {file = \"tokenizers-0.15.2-cp310-none-win32.whl\", hash = \"sha256:064ff87bb6acdbd693666de9a4b692add41308a2c0ec0770d6385737117215f2\"},",
      "6137:     {file = \"tokenizers-0.15.2-cp310-none-win_amd64.whl\", hash = \"sha256:3b919afe4df7eb6ac7cafd2bd14fb507d3f408db7a68c43117f579c984a73843\"},",
      "6138:     {file = \"tokenizers-0.15.2-cp311-cp311-macosx_10_12_x86_64.whl\", hash = \"sha256:89cd1cb93e4b12ff39bb2d626ad77e35209de9309a71e4d3d4672667b4b256e7\"},",
      "6139:     {file = \"tokenizers-0.15.2-cp311-cp311-macosx_11_0_arm64.whl\", hash = \"sha256:cfed5c64e5be23d7ee0f0e98081a25c2a46b0b77ce99a4f0605b1ec43dd481fa\"},",
      "6140:     {file = \"tokenizers-0.15.2-cp311-cp311-manylinux_2_12_i686.manylinux2010_i686.whl\", hash = \"sha256:a907d76dcfda37023ba203ab4ceeb21bc5683436ebefbd895a0841fd52f6f6f2\"},",
      "6141:     {file = \"tokenizers-0.15.2-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:20ea60479de6fc7b8ae756b4b097572372d7e4032e2521c1bbf3d90c90a99ff0\"},",
      "6142:     {file = \"tokenizers-0.15.2-cp311-cp311-manylinux_2_17_armv7l.manylinux2014_armv7l.whl\", hash = \"sha256:48e2b9335be2bc0171df9281385c2ed06a15f5cf121c44094338306ab7b33f2c\"},",
      "6143:     {file = \"tokenizers-0.15.2-cp311-cp311-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:112a1dd436d2cc06e6ffdc0b06d55ac019a35a63afd26475205cb4b1bf0bfbff\"},",
      "6144:     {file = \"tokenizers-0.15.2-cp311-cp311-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:4620cca5c2817177ee8706f860364cc3a8845bc1e291aaf661fb899e5d1c45b0\"},",
      "6145:     {file = \"tokenizers-0.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:ccd73a82751c523b3fc31ff8194702e4af4db21dc20e55b30ecc2079c5d43cb7\"},",
      "6146:     {file = \"tokenizers-0.15.2-cp311-cp311-musllinux_1_1_aarch64.whl\", hash = \"sha256:107089f135b4ae7817affe6264f8c7a5c5b4fd9a90f9439ed495f54fcea56fb4\"},",
      "6147:     {file = \"tokenizers-0.15.2-cp311-cp311-musllinux_1_1_x86_64.whl\", hash = \"sha256:0ff110ecc57b7aa4a594396525a3451ad70988e517237fe91c540997c4e50e29\"},",
      "6148:     {file = \"tokenizers-0.15.2-cp311-none-win32.whl\", hash = \"sha256:6d76f00f5c32da36c61f41c58346a4fa7f0a61be02f4301fd30ad59834977cc3\"},",
      "6149:     {file = \"tokenizers-0.15.2-cp311-none-win_amd64.whl\", hash = \"sha256:cc90102ed17271cf0a1262babe5939e0134b3890345d11a19c3145184b706055\"},",
      "6150:     {file = \"tokenizers-0.15.2-cp312-cp312-macosx_10_12_x86_64.whl\", hash = \"sha256:f86593c18d2e6248e72fb91c77d413a815153b8ea4e31f7cd443bdf28e467670\"},",
      "6151:     {file = \"tokenizers-0.15.2-cp312-cp312-macosx_11_0_arm64.whl\", hash = \"sha256:0774bccc6608eca23eb9d620196687c8b2360624619623cf4ba9dc9bd53e8b51\"},",
      "6152:     {file = \"tokenizers-0.15.2-cp312-cp312-manylinux_2_12_i686.manylinux2010_i686.whl\", hash = \"sha256:d0222c5b7c9b26c0b4822a82f6a7011de0a9d3060e1da176f66274b70f846b98\"},",
      "6153:     {file = \"tokenizers-0.15.2-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:3835738be1de66624fff2f4f6f6684775da4e9c00bde053be7564cbf3545cc66\"},",
      "6154:     {file = \"tokenizers-0.15.2-cp312-cp312-manylinux_2_17_armv7l.manylinux2014_armv7l.whl\", hash = \"sha256:0143e7d9dcd811855c1ce1ab9bf5d96d29bf5e528fd6c7824d0465741e8c10fd\"},",
      "6155:     {file = \"tokenizers-0.15.2-cp312-cp312-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:db35825f6d54215f6b6009a7ff3eedee0848c99a6271c870d2826fbbedf31a38\"},",
      "6156:     {file = \"tokenizers-0.15.2-cp312-cp312-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:3f5e64b0389a2be47091d8cc53c87859783b837ea1a06edd9d8e04004df55a5c\"},",
      "6157:     {file = \"tokenizers-0.15.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:9e0480c452217edd35eca56fafe2029fb4d368b7c0475f8dfa3c5c9c400a7456\"},",
      "6158:     {file = \"tokenizers-0.15.2-cp312-cp312-musllinux_1_1_aarch64.whl\", hash = \"sha256:a33ab881c8fe70474980577e033d0bc9a27b7ab8272896e500708b212995d834\"},",
      "6159:     {file = \"tokenizers-0.15.2-cp312-cp312-musllinux_1_1_x86_64.whl\", hash = \"sha256:a308a607ca9de2c64c1b9ba79ec9a403969715a1b8ba5f998a676826f1a7039d\"},",
      "6160:     {file = \"tokenizers-0.15.2-cp312-none-win32.whl\", hash = \"sha256:b8fcfa81bcb9447df582c5bc96a031e6df4da2a774b8080d4f02c0c16b42be0b\"},",
      "6161:     {file = \"tokenizers-0.15.2-cp312-none-win_amd64.whl\", hash = \"sha256:38d7ab43c6825abfc0b661d95f39c7f8af2449364f01d331f3b51c94dcff7221\"},",
      "6162:     {file = \"tokenizers-0.15.2-cp313-cp313-macosx_10_12_x86_64.whl\", hash = \"sha256:38bfb0204ff3246ca4d5e726e8cc8403bfc931090151e6eede54d0e0cf162ef0\"},",
      "6163:     {file = \"tokenizers-0.15.2-cp313-cp313-macosx_11_0_arm64.whl\", hash = \"sha256:9c861d35e8286a53e06e9e28d030b5a05bcbf5ac9d7229e561e53c352a85b1fc\"},",
      "6164:     {file = \"tokenizers-0.15.2-cp313-cp313-manylinux_2_12_i686.manylinux2010_i686.whl\", hash = \"sha256:936bf3842db5b2048eaa53dade907b1160f318e7c90c74bfab86f1e47720bdd6\"},",
      "6165:     {file = \"tokenizers-0.15.2-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:620beacc3373277700d0e27718aa8b25f7b383eb8001fba94ee00aeea1459d89\"},",
      "6166:     {file = \"tokenizers-0.15.2-cp313-cp313-manylinux_2_17_armv7l.manylinux2014_armv7l.whl\", hash = \"sha256:2735ecbbf37e52db4ea970e539fd2d450d213517b77745114f92867f3fc246eb\"},",
      "6167:     {file = \"tokenizers-0.15.2-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:473c83c5e2359bb81b0b6fde870b41b2764fcdd36d997485e07e72cc3a62264a\"},",
      "6168:     {file = \"tokenizers-0.15.2-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:968fa1fb3c27398b28a4eca1cbd1e19355c4d3a6007f7398d48826bbe3a0f728\"},",
      "6169:     {file = \"tokenizers-0.15.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:865c60ae6eaebdde7da66191ee9b7db52e542ed8ee9d2c653b6d190a9351b980\"},",
      "6170:     {file = \"tokenizers-0.15.2-cp313-cp313-musllinux_1_1_aarch64.whl\", hash = \"sha256:7c0d8b52664ab2d4a8d6686eb5effc68b78608a9008f086a122a7b2996befbab\"},",
      "6171:     {file = \"tokenizers-0.15.2-cp313-cp313-musllinux_1_1_x86_64.whl\", hash = \"sha256:f33dfbdec3784093a9aebb3680d1f91336c56d86cc70ddf88708251da1fe9064\"},",
      "6172:     {file = \"tokenizers-0.15.2-cp37-cp37m-macosx_10_12_x86_64.whl\", hash = \"sha256:d44ba80988ff9424e33e0a49445072ac7029d8c0e1601ad25a0ca5f41ed0c1d6\"},",
      "6173:     {file = \"tokenizers-0.15.2-cp37-cp37m-macosx_11_0_arm64.whl\", hash = \"sha256:dce74266919b892f82b1b86025a613956ea0ea62a4843d4c4237be2c5498ed3a\"},",
      "6174:     {file = \"tokenizers-0.15.2-cp37-cp37m-manylinux_2_12_i686.manylinux2010_i686.whl\", hash = \"sha256:0ef06b9707baeb98b316577acb04f4852239d856b93e9ec3a299622f6084e4be\"},",
      "6175:     {file = \"tokenizers-0.15.2-cp37-cp37m-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:c73e2e74bbb07910da0d37c326869f34113137b23eadad3fc00856e6b3d9930c\"},",
      "6176:     {file = \"tokenizers-0.15.2-cp37-cp37m-manylinux_2_17_armv7l.manylinux2014_armv7l.whl\", hash = \"sha256:4eeb12daf02a59e29f578a865f55d87cd103ce62bd8a3a5874f8fdeaa82e336b\"},",
      "6177:     {file = \"tokenizers-0.15.2-cp37-cp37m-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:9ba9f6895af58487ca4f54e8a664a322f16c26bbb442effd01087eba391a719e\"},",
      "6178:     {file = \"tokenizers-0.15.2-cp37-cp37m-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:ccec77aa7150e38eec6878a493bf8c263ff1fa8a62404e16c6203c64c1f16a26\"},",
      "6179:     {file = \"tokenizers-0.15.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:f3f40604f5042ff210ba82743dda2b6aa3e55aa12df4e9f2378ee01a17e2855e\"},",
      "6180:     {file = \"tokenizers-0.15.2-cp37-cp37m-musllinux_1_1_aarch64.whl\", hash = \"sha256:5645938a42d78c4885086767c70923abad047163d809c16da75d6b290cb30bbe\"},",
      "6181:     {file = \"tokenizers-0.15.2-cp37-cp37m-musllinux_1_1_x86_64.whl\", hash = \"sha256:05a77cbfebe28a61ab5c3891f9939cc24798b63fa236d84e5f29f3a85a200c00\"},",
      "6182:     {file = \"tokenizers-0.15.2-cp37-none-win32.whl\", hash = \"sha256:361abdc068e8afe9c5b818769a48624687fb6aaed49636ee39bec4e95e1a215b\"},",
      "6183:     {file = \"tokenizers-0.15.2-cp37-none-win_amd64.whl\", hash = \"sha256:7ef789f83eb0f9baeb4d09a86cd639c0a5518528f9992f38b28e819df397eb06\"},",
      "6184:     {file = \"tokenizers-0.15.2-cp38-cp38-macosx_10_12_x86_64.whl\", hash = \"sha256:4fe1f74a902bee74a3b25aff180fbfbf4f8b444ab37c4d496af7afd13a784ed2\"},",
      "6185:     {file = \"tokenizers-0.15.2-cp38-cp38-macosx_11_0_arm64.whl\", hash = \"sha256:4c4b89038a684f40a6b15d6b09f49650ac64d951ad0f2a3ea9169687bbf2a8ba\"},",
      "6186:     {file = \"tokenizers-0.15.2-cp38-cp38-manylinux_2_12_i686.manylinux2010_i686.whl\", hash = \"sha256:d05a1b06f986d41aed5f2de464c003004b2df8aaf66f2b7628254bcbfb72a438\"},",
      "6187:     {file = \"tokenizers-0.15.2-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:508711a108684111ec8af89d3a9e9e08755247eda27d0ba5e3c50e9da1600f6d\"},",
      "6188:     {file = \"tokenizers-0.15.2-cp38-cp38-manylinux_2_17_armv7l.manylinux2014_armv7l.whl\", hash = \"sha256:daa348f02d15160cb35439098ac96e3a53bacf35885072611cd9e5be7d333daa\"},",
      "6189:     {file = \"tokenizers-0.15.2-cp38-cp38-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:494fdbe5932d3416de2a85fc2470b797e6f3226c12845cadf054dd906afd0442\"},",
      "6190:     {file = \"tokenizers-0.15.2-cp38-cp38-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:c2d60f5246f4da9373f75ff18d64c69cbf60c3bca597290cea01059c336d2470\"},",
      "6191:     {file = \"tokenizers-0.15.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:93268e788825f52de4c7bdcb6ebc1fcd4a5442c02e730faa9b6b08f23ead0e24\"},",
      "6192:     {file = \"tokenizers-0.15.2-cp38-cp38-musllinux_1_1_aarch64.whl\", hash = \"sha256:6fc7083ab404019fc9acafe78662c192673c1e696bd598d16dc005bd663a5cf9\"},",
      "6193:     {file = \"tokenizers-0.15.2-cp38-cp38-musllinux_1_1_x86_64.whl\", hash = \"sha256:41e39b41e5531d6b2122a77532dbea60e171ef87a3820b5a3888daa847df4153\"},",
      "6194:     {file = \"tokenizers-0.15.2-cp38-none-win32.whl\", hash = \"sha256:06cd0487b1cbfabefb2cc52fbd6b1f8d4c37799bd6c6e1641281adaa6b2504a7\"},",
      "6195:     {file = \"tokenizers-0.15.2-cp38-none-win_amd64.whl\", hash = \"sha256:5179c271aa5de9c71712e31cb5a79e436ecd0d7532a408fa42a8dbfa4bc23fd9\"},",
      "6196:     {file = \"tokenizers-0.15.2-cp39-cp39-macosx_10_12_x86_64.whl\", hash = \"sha256:82f8652a74cc107052328b87ea8b34291c0f55b96d8fb261b3880216a9f9e48e\"},",
      "6197:     {file = \"tokenizers-0.15.2-cp39-cp39-macosx_11_0_arm64.whl\", hash = \"sha256:02458bee6f5f3139f1ebbb6d042b283af712c0981f5bc50edf771d6b762d5e4f\"},",
      "6198:     {file = \"tokenizers-0.15.2-cp39-cp39-manylinux_2_12_i686.manylinux2010_i686.whl\", hash = \"sha256:c9a09cd26cca2e1c349f91aa665309ddb48d71636370749414fbf67bc83c5343\"},",
      "6199:     {file = \"tokenizers-0.15.2-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:158be8ea8554e5ed69acc1ce3fbb23a06060bd4bbb09029431ad6b9a466a7121\"},",
      "6200:     {file = \"tokenizers-0.15.2-cp39-cp39-manylinux_2_17_armv7l.manylinux2014_armv7l.whl\", hash = \"sha256:1ddba9a2b0c8c81633eca0bb2e1aa5b3a15362b1277f1ae64176d0f6eba78ab1\"},",
      "6201:     {file = \"tokenizers-0.15.2-cp39-cp39-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:3ef5dd1d39797044642dbe53eb2bc56435308432e9c7907728da74c69ee2adca\"},",
      "6202:     {file = \"tokenizers-0.15.2-cp39-cp39-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:454c203164e07a860dbeb3b1f4a733be52b0edbb4dd2e5bd75023ffa8b49403a\"},",
      "6203:     {file = \"tokenizers-0.15.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:0cf6b7f1d4dc59af960e6ffdc4faffe6460bbfa8dce27a58bf75755ffdb2526d\"},",
      "6204:     {file = \"tokenizers-0.15.2-cp39-cp39-musllinux_1_1_aarch64.whl\", hash = \"sha256:2ef09bbc16519f6c25d0c7fc0c6a33a6f62923e263c9d7cca4e58b8c61572afb\"},",
      "6205:     {file = \"tokenizers-0.15.2-cp39-cp39-musllinux_1_1_x86_64.whl\", hash = \"sha256:c9a2ebdd2ad4ec7a68e7615086e633857c85e2f18025bd05d2a4399e6c5f7169\"},",
      "6206:     {file = \"tokenizers-0.15.2-cp39-none-win32.whl\", hash = \"sha256:918fbb0eab96fe08e72a8c2b5461e9cce95585d82a58688e7f01c2bd546c79d0\"},",
      "6207:     {file = \"tokenizers-0.15.2-cp39-none-win_amd64.whl\", hash = \"sha256:524e60da0135e106b254bd71f0659be9f89d83f006ea9093ce4d1fab498c6d0d\"},",
      "6208:     {file = \"tokenizers-0.15.2-pp310-pypy310_pp73-macosx_10_12_x86_64.whl\", hash = \"sha256:6a9b648a58281c4672212fab04e60648fde574877d0139cd4b4f93fe28ca8944\"},",
      "6209:     {file = \"tokenizers-0.15.2-pp310-pypy310_pp73-macosx_11_0_arm64.whl\", hash = \"sha256:7c7d18b733be6bbca8a55084027f7be428c947ddf871c500ee603e375013ffba\"},",
      "6210:     {file = \"tokenizers-0.15.2-pp310-pypy310_pp73-manylinux_2_12_i686.manylinux2010_i686.whl\", hash = \"sha256:13ca3611de8d9ddfbc4dc39ef54ab1d2d4aaa114ac8727dfdc6a6ec4be017378\"},",
      "6211:     {file = \"tokenizers-0.15.2-pp310-pypy310_pp73-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:237d1bf3361cf2e6463e6c140628e6406766e8b27274f5fcc62c747ae3c6f094\"},",
      "6212:     {file = \"tokenizers-0.15.2-pp310-pypy310_pp73-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:67a0fe1e49e60c664915e9fb6b0cb19bac082ab1f309188230e4b2920230edb3\"},",
      "6213:     {file = \"tokenizers-0.15.2-pp310-pypy310_pp73-musllinux_1_1_aarch64.whl\", hash = \"sha256:4e022fe65e99230b8fd89ebdfea138c24421f91c1a4f4781a8f5016fd5cdfb4d\"},",
      "6214:     {file = \"tokenizers-0.15.2-pp310-pypy310_pp73-musllinux_1_1_x86_64.whl\", hash = \"sha256:d857be2df69763362ac699f8b251a8cd3fac9d21893de129bc788f8baaef2693\"},",
      "6215:     {file = \"tokenizers-0.15.2-pp37-pypy37_pp73-macosx_10_12_x86_64.whl\", hash = \"sha256:708bb3e4283177236309e698da5fcd0879ce8fd37457d7c266d16b550bcbbd18\"},",
      "6216:     {file = \"tokenizers-0.15.2-pp37-pypy37_pp73-manylinux_2_12_i686.manylinux2010_i686.whl\", hash = \"sha256:64c35e09e9899b72a76e762f9854e8750213f67567787d45f37ce06daf57ca78\"},",
      "6217:     {file = \"tokenizers-0.15.2-pp37-pypy37_pp73-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:c1257f4394be0d3b00de8c9e840ca5601d0a4a8438361ce9c2b05c7d25f6057b\"},",
      "6218:     {file = \"tokenizers-0.15.2-pp37-pypy37_pp73-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:02272fe48280e0293a04245ca5d919b2c94a48b408b55e858feae9618138aeda\"},",
      "6219:     {file = \"tokenizers-0.15.2-pp37-pypy37_pp73-musllinux_1_1_aarch64.whl\", hash = \"sha256:dc3ad9ebc76eabe8b1d7c04d38be884b8f9d60c0cdc09b0aa4e3bcf746de0388\"},",
      "6220:     {file = \"tokenizers-0.15.2-pp37-pypy37_pp73-musllinux_1_1_x86_64.whl\", hash = \"sha256:32e16bdeffa7c4f46bf2152172ca511808b952701d13e7c18833c0b73cb5c23f\"},",
      "6221:     {file = \"tokenizers-0.15.2-pp38-pypy38_pp73-macosx_10_12_x86_64.whl\", hash = \"sha256:fb16ba563d59003028b678d2361a27f7e4ae0ab29c7a80690efa20d829c81fdb\"},",
      "6222:     {file = \"tokenizers-0.15.2-pp38-pypy38_pp73-macosx_11_0_arm64.whl\", hash = \"sha256:2277c36d2d6cdb7876c274547921a42425b6810d38354327dd65a8009acf870c\"},",
      "6223:     {file = \"tokenizers-0.15.2-pp38-pypy38_pp73-manylinux_2_12_i686.manylinux2010_i686.whl\", hash = \"sha256:1cf75d32e8d250781940d07f7eece253f2fe9ecdb1dc7ba6e3833fa17b82fcbc\"},",
      "6224:     {file = \"tokenizers-0.15.2-pp38-pypy38_pp73-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:f1b3b31884dc8e9b21508bb76da80ebf7308fdb947a17affce815665d5c4d028\"},",
      "6225:     {file = \"tokenizers-0.15.2-pp38-pypy38_pp73-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:b10122d8d8e30afb43bb1fe21a3619f62c3e2574bff2699cf8af8b0b6c5dc4a3\"},",
      "6226:     {file = \"tokenizers-0.15.2-pp38-pypy38_pp73-musllinux_1_1_aarch64.whl\", hash = \"sha256:d88b96ff0fe8e91f6ef01ba50b0d71db5017fa4e3b1d99681cec89a85faf7bf7\"},",
      "6227:     {file = \"tokenizers-0.15.2-pp38-pypy38_pp73-musllinux_1_1_x86_64.whl\", hash = \"sha256:37aaec5a52e959892870a7c47cef80c53797c0db9149d458460f4f31e2fb250e\"},",
      "6228:     {file = \"tokenizers-0.15.2-pp39-pypy39_pp73-macosx_10_12_x86_64.whl\", hash = \"sha256:e2ea752f2b0fe96eb6e2f3adbbf4d72aaa1272079b0dfa1145507bd6a5d537e6\"},",
      "6229:     {file = \"tokenizers-0.15.2-pp39-pypy39_pp73-macosx_11_0_arm64.whl\", hash = \"sha256:4b19a808d8799fda23504a5cd31d2f58e6f52f140380082b352f877017d6342b\"},",
      "6230:     {file = \"tokenizers-0.15.2-pp39-pypy39_pp73-manylinux_2_12_i686.manylinux2010_i686.whl\", hash = \"sha256:64c86e5e068ac8b19204419ed8ca90f9d25db20578f5881e337d203b314f4104\"},",
      "6231:     {file = \"tokenizers-0.15.2-pp39-pypy39_pp73-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:de19c4dc503c612847edf833c82e9f73cd79926a384af9d801dcf93f110cea4e\"},",
      "6232:     {file = \"tokenizers-0.15.2-pp39-pypy39_pp73-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:ea09acd2fe3324174063d61ad620dec3bcf042b495515f27f638270a7d466e8b\"},",
      "6233:     {file = \"tokenizers-0.15.2-pp39-pypy39_pp73-musllinux_1_1_aarch64.whl\", hash = \"sha256:cf27fd43472e07b57cf420eee1e814549203d56de00b5af8659cb99885472f1f\"},",
      "6234:     {file = \"tokenizers-0.15.2-pp39-pypy39_pp73-musllinux_1_1_x86_64.whl\", hash = \"sha256:7ca22bd897537a0080521445d91a58886c8c04084a6a19e6c78c586e0cfa92a5\"},",
      "6235:     {file = \"tokenizers-0.15.2.tar.gz\", hash = \"sha256:e6e9c6e019dd5484be5beafc775ae6c925f4c69a3487040ed09b45e13df2cb91\"},",
      "6236: ]",
      "6238: [package.dependencies]",
      "6239: huggingface_hub = \">=0.16.4,<1.0\"",
      "6241: [package.extras]",
      "6242: dev = [\"tokenizers[testing]\"]",
      "6243: docs = [\"setuptools_rust\", \"sphinx\", \"sphinx_rtd_theme\"]",
      "",
      "---------------",
      "--- Hunk 9 ---",
      "[Context before]",
      "6104: [[package]]",
      "6105: name = \"transformers\"",
      "6107: description = \"State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\"",
      "6108: optional = true",
      "6110: files = [",
      "6113: ]",
      "6115: [package.dependencies]",
      "6116: filelock = \"*\"",
      "6118: numpy = \">=1.17\"",
      "6119: packaging = \">=20.0\"",
      "6120: pyyaml = \">=5.1\"",
      "6121: regex = \"!=2019.12.17\"",
      "6122: requests = \"*\"",
      "6124: tqdm = \">=4.27\"",
      "6126: [package.extras]",
      "6129: audio = [\"kenlm\", \"librosa\", \"phonemizer\", \"pyctcdecode (>=0.4.0)\"]",
      "6130: codecarbon = [\"codecarbon (==1.2.0)\"]",
      "6137: docs-specific = [\"hf-doc-builder\"]",
      "6140: flax-speech = [\"kenlm\", \"librosa\", \"phonemizer\", \"pyctcdecode (>=0.4.0)\"]",
      "6141: ftfy = [\"ftfy\"]",
      "6144: modelcreation = [\"cookiecutter (==1.7.3)\"]",
      "6146: onnx = [\"onnxconverter-common\", \"onnxruntime (>=1.4.0)\", \"onnxruntime-tools (>=1.4.2)\", \"tf2onnx\"]",
      "6147: onnxruntime = [\"onnxruntime (>=1.4.0)\", \"onnxruntime-tools (>=1.4.2)\"]",
      "6148: optuna = [\"optuna\"]",
      "6151: retrieval = [\"datasets (!=2.5.0)\", \"faiss-cpu\"]",
      "6152: sagemaker = [\"sagemaker (>=2.31.0)\"]",
      "6155: sigopt = [\"sigopt\"]",
      "6156: sklearn = [\"scikit-learn\"]",
      "6157: speech = [\"kenlm\", \"librosa\", \"phonemizer\", \"pyctcdecode (>=0.4.0)\", \"torchaudio\"]",
      "6161: tf-speech = [\"kenlm\", \"librosa\", \"phonemizer\", \"pyctcdecode (>=0.4.0)\"]",
      "6162: timm = [\"timm\"]",
      "6165: torch-speech = [\"kenlm\", \"librosa\", \"phonemizer\", \"pyctcdecode (>=0.4.0)\", \"torchaudio\"]",
      "6170: [[package]]",
      "6171: name = \"twilio\"",
      "",
      "[Removed Lines]",
      "6106: version = \"4.26.0\"",
      "6109: python-versions = \">=3.7.0\"",
      "6111:     {file = \"transformers-4.26.0-py3-none-any.whl\", hash = \"sha256:6a902eee6098d9a737faadf185b8df5a169acc695ebbde5a81b90528f43e665f\"},",
      "6112:     {file = \"transformers-4.26.0.tar.gz\", hash = \"sha256:d7859bd83829a3682ca632197ee5c72556e1063d199ab84eec35c4f23b3d73a3\"},",
      "6117: huggingface-hub = \">=0.11.0,<1.0\"",
      "6123: tokenizers = \">=0.11.1,<0.11.3 || >0.11.3,<0.14\"",
      "6127: accelerate = [\"accelerate (>=0.10.0)\"]",
      "6128: all = [\"Pillow\", \"accelerate (>=0.10.0)\", \"codecarbon (==1.2.0)\", \"decord (==0.6.0)\", \"flax (>=0.4.1)\", \"jax (>=0.2.8,!=0.3.2,<=0.3.6)\", \"jaxlib (>=0.1.65,<=0.3.6)\", \"kenlm\", \"keras-nlp (>=0.3.1)\", \"librosa\", \"onnxconverter-common\", \"optax (>=0.0.8)\", \"optuna\", \"phonemizer\", \"protobuf (<=3.20.2)\", \"pyctcdecode (>=0.4.0)\", \"ray[tune]\", \"sentencepiece (>=0.1.91,!=0.1.92)\", \"sigopt\", \"tensorflow (>=2.4,<2.12)\", \"tensorflow-text\", \"tf2onnx\", \"timm\", \"tokenizers (>=0.11.1,!=0.11.3,<0.14)\", \"torch (>=1.7,!=1.12.0)\", \"torchaudio\"]",
      "6131: deepspeed = [\"accelerate (>=0.10.0)\", \"deepspeed (>=0.6.5)\"]",
      "6132: deepspeed-testing = [\"GitPython (<3.1.19)\", \"accelerate (>=0.10.0)\", \"beautifulsoup4\", \"black (==22.3)\", \"cookiecutter (==1.7.3)\", \"datasets (!=2.5.0)\", \"deepspeed (>=0.6.5)\", \"dill (<0.3.5)\", \"evaluate (>=0.2.0)\", \"faiss-cpu\", \"hf-doc-builder (>=0.3.0)\", \"nltk\", \"optuna\", \"parameterized\", \"protobuf (<=3.20.2)\", \"psutil\", \"pytest\", \"pytest-timeout\", \"pytest-xdist\", \"rjieba\", \"rouge-score (!=0.0.7,!=0.0.8,!=0.1,!=0.1.1)\", \"sacrebleu (>=1.4.12,<2.0.0)\", \"sacremoses\", \"safetensors (>=0.2.1)\", \"sentencepiece (>=0.1.91,!=0.1.92)\", \"timeout-decorator\"]",
      "6133: dev = [\"GitPython (<3.1.19)\", \"Pillow\", \"accelerate (>=0.10.0)\", \"beautifulsoup4\", \"black (==22.3)\", \"codecarbon (==1.2.0)\", \"cookiecutter (==1.7.3)\", \"datasets (!=2.5.0)\", \"decord (==0.6.0)\", \"dill (<0.3.5)\", \"evaluate (>=0.2.0)\", \"faiss-cpu\", \"flake8 (>=3.8.3)\", \"flax (>=0.4.1)\", \"fugashi (>=1.0)\", \"hf-doc-builder\", \"hf-doc-builder (>=0.3.0)\", \"ipadic (>=1.0.0,<2.0)\", \"isort (>=5.5.4)\", \"jax (>=0.2.8,!=0.3.2,<=0.3.6)\", \"jaxlib (>=0.1.65,<=0.3.6)\", \"kenlm\", \"keras-nlp (>=0.3.1)\", \"librosa\", \"nltk\", \"onnxconverter-common\", \"optax (>=0.0.8)\", \"optuna\", \"parameterized\", \"phonemizer\", \"protobuf (<=3.20.2)\", \"psutil\", \"pyctcdecode (>=0.4.0)\", \"pytest\", \"pytest-timeout\", \"pytest-xdist\", \"ray[tune]\", \"rhoknp (>=1.1.0)\", \"rjieba\", \"rouge-score (!=0.0.7,!=0.0.8,!=0.1,!=0.1.1)\", \"sacrebleu (>=1.4.12,<2.0.0)\", \"sacremoses\", \"safetensors (>=0.2.1)\", \"scikit-learn\", \"sentencepiece (>=0.1.91,!=0.1.92)\", \"sigopt\", \"sudachidict-core (>=20220729)\", \"sudachipy (>=0.6.6)\", \"tensorflow (>=2.4,<2.12)\", \"tensorflow-text\", \"tf2onnx\", \"timeout-decorator\", \"timm\", \"tokenizers (>=0.11.1,!=0.11.3,<0.14)\", \"torch (>=1.7,!=1.12.0)\", \"torchaudio\", \"unidic (>=1.0.2)\", \"unidic-lite (>=1.0.7)\"]",
      "6134: dev-tensorflow = [\"GitPython (<3.1.19)\", \"Pillow\", \"beautifulsoup4\", \"black (==22.3)\", \"cookiecutter (==1.7.3)\", \"datasets (!=2.5.0)\", \"dill (<0.3.5)\", \"evaluate (>=0.2.0)\", \"faiss-cpu\", \"flake8 (>=3.8.3)\", \"hf-doc-builder\", \"hf-doc-builder (>=0.3.0)\", \"isort (>=5.5.4)\", \"kenlm\", \"keras-nlp (>=0.3.1)\", \"librosa\", \"nltk\", \"onnxconverter-common\", \"onnxruntime (>=1.4.0)\", \"onnxruntime-tools (>=1.4.2)\", \"parameterized\", \"phonemizer\", \"protobuf (<=3.20.2)\", \"psutil\", \"pyctcdecode (>=0.4.0)\", \"pytest\", \"pytest-timeout\", \"pytest-xdist\", \"rjieba\", \"rouge-score (!=0.0.7,!=0.0.8,!=0.1,!=0.1.1)\", \"sacrebleu (>=1.4.12,<2.0.0)\", \"sacremoses\", \"safetensors (>=0.2.1)\", \"scikit-learn\", \"sentencepiece (>=0.1.91,!=0.1.92)\", \"tensorflow (>=2.4,<2.12)\", \"tensorflow-text\", \"tf2onnx\", \"timeout-decorator\", \"tokenizers (>=0.11.1,!=0.11.3,<0.14)\"]",
      "6135: dev-torch = [\"GitPython (<3.1.19)\", \"Pillow\", \"beautifulsoup4\", \"black (==22.3)\", \"codecarbon (==1.2.0)\", \"cookiecutter (==1.7.3)\", \"datasets (!=2.5.0)\", \"dill (<0.3.5)\", \"evaluate (>=0.2.0)\", \"faiss-cpu\", \"flake8 (>=3.8.3)\", \"fugashi (>=1.0)\", \"hf-doc-builder\", \"hf-doc-builder (>=0.3.0)\", \"ipadic (>=1.0.0,<2.0)\", \"isort (>=5.5.4)\", \"kenlm\", \"librosa\", \"nltk\", \"onnxruntime (>=1.4.0)\", \"onnxruntime-tools (>=1.4.2)\", \"optuna\", \"parameterized\", \"phonemizer\", \"protobuf (<=3.20.2)\", \"psutil\", \"pyctcdecode (>=0.4.0)\", \"pytest\", \"pytest-timeout\", \"pytest-xdist\", \"ray[tune]\", \"rhoknp (>=1.1.0)\", \"rjieba\", \"rouge-score (!=0.0.7,!=0.0.8,!=0.1,!=0.1.1)\", \"sacrebleu (>=1.4.12,<2.0.0)\", \"sacremoses\", \"safetensors (>=0.2.1)\", \"scikit-learn\", \"sentencepiece (>=0.1.91,!=0.1.92)\", \"sigopt\", \"sudachidict-core (>=20220729)\", \"sudachipy (>=0.6.6)\", \"timeout-decorator\", \"timm\", \"tokenizers (>=0.11.1,!=0.11.3,<0.14)\", \"torch (>=1.7,!=1.12.0)\", \"torchaudio\", \"unidic (>=1.0.2)\", \"unidic-lite (>=1.0.7)\"]",
      "6136: docs = [\"Pillow\", \"accelerate (>=0.10.0)\", \"codecarbon (==1.2.0)\", \"decord (==0.6.0)\", \"flax (>=0.4.1)\", \"hf-doc-builder\", \"jax (>=0.2.8,!=0.3.2,<=0.3.6)\", \"jaxlib (>=0.1.65,<=0.3.6)\", \"kenlm\", \"keras-nlp (>=0.3.1)\", \"librosa\", \"onnxconverter-common\", \"optax (>=0.0.8)\", \"optuna\", \"phonemizer\", \"protobuf (<=3.20.2)\", \"pyctcdecode (>=0.4.0)\", \"ray[tune]\", \"sentencepiece (>=0.1.91,!=0.1.92)\", \"sigopt\", \"tensorflow (>=2.4,<2.12)\", \"tensorflow-text\", \"tf2onnx\", \"timm\", \"tokenizers (>=0.11.1,!=0.11.3,<0.14)\", \"torch (>=1.7,!=1.12.0)\", \"torchaudio\"]",
      "6138: fairscale = [\"fairscale (>0.3)\"]",
      "6139: flax = [\"flax (>=0.4.1)\", \"jax (>=0.2.8,!=0.3.2,<=0.3.6)\", \"jaxlib (>=0.1.65,<=0.3.6)\", \"optax (>=0.0.8)\"]",
      "6142: integrations = [\"optuna\", \"ray[tune]\", \"sigopt\"]",
      "6143: ja = [\"fugashi (>=1.0)\", \"ipadic (>=1.0.0,<2.0)\", \"rhoknp (>=1.1.0)\", \"sudachidict-core (>=20220729)\", \"sudachipy (>=0.6.6)\", \"unidic (>=1.0.2)\", \"unidic-lite (>=1.0.7)\"]",
      "6145: natten = [\"natten (>=0.14.4)\"]",
      "6149: quality = [\"GitPython (<3.1.19)\", \"black (==22.3)\", \"datasets (!=2.5.0)\", \"flake8 (>=3.8.3)\", \"hf-doc-builder (>=0.3.0)\", \"isort (>=5.5.4)\"]",
      "6150: ray = [\"ray[tune]\"]",
      "6153: sentencepiece = [\"protobuf (<=3.20.2)\", \"sentencepiece (>=0.1.91,!=0.1.92)\"]",
      "6154: serving = [\"fastapi\", \"pydantic\", \"starlette\", \"uvicorn\"]",
      "6158: testing = [\"GitPython (<3.1.19)\", \"beautifulsoup4\", \"black (==22.3)\", \"cookiecutter (==1.7.3)\", \"datasets (!=2.5.0)\", \"dill (<0.3.5)\", \"evaluate (>=0.2.0)\", \"faiss-cpu\", \"hf-doc-builder (>=0.3.0)\", \"nltk\", \"parameterized\", \"protobuf (<=3.20.2)\", \"psutil\", \"pytest\", \"pytest-timeout\", \"pytest-xdist\", \"rjieba\", \"rouge-score (!=0.0.7,!=0.0.8,!=0.1,!=0.1.1)\", \"sacrebleu (>=1.4.12,<2.0.0)\", \"sacremoses\", \"safetensors (>=0.2.1)\", \"timeout-decorator\"]",
      "6159: tf = [\"keras-nlp (>=0.3.1)\", \"onnxconverter-common\", \"tensorflow (>=2.4,<2.12)\", \"tensorflow-text\", \"tf2onnx\"]",
      "6160: tf-cpu = [\"keras-nlp (>=0.3.1)\", \"onnxconverter-common\", \"tensorflow-cpu (>=2.4,<2.12)\", \"tensorflow-text\", \"tf2onnx\"]",
      "6163: tokenizers = [\"tokenizers (>=0.11.1,!=0.11.3,<0.14)\"]",
      "6164: torch = [\"torch (>=1.7,!=1.12.0)\"]",
      "6166: torchhub = [\"filelock\", \"huggingface-hub (>=0.11.0,<1.0)\", \"importlib-metadata\", \"numpy (>=1.17)\", \"packaging (>=20.0)\", \"protobuf (<=3.20.2)\", \"regex (!=2019.12.17)\", \"requests\", \"sentencepiece (>=0.1.91,!=0.1.92)\", \"tokenizers (>=0.11.1,!=0.11.3,<0.14)\", \"torch (>=1.7,!=1.12.0)\", \"tqdm (>=4.27)\"]",
      "6167: video = [\"decord (==0.6.0)\"]",
      "6168: vision = [\"Pillow\"]",
      "",
      "[Added Lines]",
      "6334: version = \"4.36.2\"",
      "6337: python-versions = \">=3.8.0\"",
      "6339:     {file = \"transformers-4.36.2-py3-none-any.whl\", hash = \"sha256:462066c4f74ee52516f12890dcc9ec71d1a5e97998db621668455117a54330f6\"},",
      "6340:     {file = \"transformers-4.36.2.tar.gz\", hash = \"sha256:d8068e897e47793281501e547d2bbdfc5b8556409c2cb6c3d9e2ca77d4c0b4ec\"},",
      "6345: huggingface-hub = \">=0.19.3,<1.0\"",
      "6351: safetensors = \">=0.3.1\"",
      "6352: tokenizers = \">=0.14,<0.19\"",
      "6356: accelerate = [\"accelerate (>=0.21.0)\"]",
      "6357: agents = [\"Pillow (>=10.0.1,<=15.0)\", \"accelerate (>=0.21.0)\", \"datasets (!=2.5.0)\", \"diffusers\", \"opencv-python\", \"sentencepiece (>=0.1.91,!=0.1.92)\", \"torch (>=1.10,!=1.12.0)\"]",
      "6358: all = [\"Pillow (>=10.0.1,<=15.0)\", \"accelerate (>=0.21.0)\", \"av (==9.2.0)\", \"codecarbon (==1.2.0)\", \"decord (==0.6.0)\", \"flax (>=0.4.1,<=0.7.0)\", \"jax (>=0.4.1,<=0.4.13)\", \"jaxlib (>=0.4.1,<=0.4.13)\", \"kenlm\", \"keras-nlp (>=0.3.1)\", \"librosa\", \"onnxconverter-common\", \"optax (>=0.0.8,<=0.1.4)\", \"optuna\", \"phonemizer\", \"protobuf\", \"pyctcdecode (>=0.4.0)\", \"ray[tune] (>=2.7.0)\", \"sentencepiece (>=0.1.91,!=0.1.92)\", \"sigopt\", \"tensorflow (>=2.6,<2.16)\", \"tensorflow-text (<2.16)\", \"tf2onnx\", \"timm\", \"tokenizers (>=0.14,<0.19)\", \"torch (>=1.10,!=1.12.0)\", \"torchaudio\", \"torchvision\"]",
      "6361: deepspeed = [\"accelerate (>=0.21.0)\", \"deepspeed (>=0.9.3)\"]",
      "6362: deepspeed-testing = [\"GitPython (<3.1.19)\", \"accelerate (>=0.21.0)\", \"beautifulsoup4\", \"cookiecutter (==1.7.3)\", \"datasets (!=2.5.0)\", \"deepspeed (>=0.9.3)\", \"dill (<0.3.5)\", \"evaluate (>=0.2.0)\", \"faiss-cpu\", \"hf-doc-builder (>=0.3.0)\", \"nltk\", \"optuna\", \"parameterized\", \"protobuf\", \"psutil\", \"pydantic (<2)\", \"pytest (>=7.2.0)\", \"pytest-timeout\", \"pytest-xdist\", \"rjieba\", \"rouge-score (!=0.0.7,!=0.0.8,!=0.1,!=0.1.1)\", \"ruff (==0.1.5)\", \"sacrebleu (>=1.4.12,<2.0.0)\", \"sacremoses\", \"sentencepiece (>=0.1.91,!=0.1.92)\", \"tensorboard\", \"timeout-decorator\"]",
      "6363: dev = [\"GitPython (<3.1.19)\", \"Pillow (>=10.0.1,<=15.0)\", \"accelerate (>=0.21.0)\", \"av (==9.2.0)\", \"beautifulsoup4\", \"codecarbon (==1.2.0)\", \"cookiecutter (==1.7.3)\", \"datasets (!=2.5.0)\", \"decord (==0.6.0)\", \"dill (<0.3.5)\", \"evaluate (>=0.2.0)\", \"faiss-cpu\", \"flax (>=0.4.1,<=0.7.0)\", \"fugashi (>=1.0)\", \"hf-doc-builder\", \"hf-doc-builder (>=0.3.0)\", \"ipadic (>=1.0.0,<2.0)\", \"isort (>=5.5.4)\", \"jax (>=0.4.1,<=0.4.13)\", \"jaxlib (>=0.4.1,<=0.4.13)\", \"kenlm\", \"keras-nlp (>=0.3.1)\", \"librosa\", \"nltk\", \"onnxconverter-common\", \"optax (>=0.0.8,<=0.1.4)\", \"optuna\", \"parameterized\", \"phonemizer\", \"protobuf\", \"psutil\", \"pyctcdecode (>=0.4.0)\", \"pydantic (<2)\", \"pytest (>=7.2.0)\", \"pytest-timeout\", \"pytest-xdist\", \"ray[tune] (>=2.7.0)\", \"rhoknp (>=1.1.0,<1.3.1)\", \"rjieba\", \"rouge-score (!=0.0.7,!=0.0.8,!=0.1,!=0.1.1)\", \"ruff (==0.1.5)\", \"sacrebleu (>=1.4.12,<2.0.0)\", \"sacremoses\", \"scikit-learn\", \"sentencepiece (>=0.1.91,!=0.1.92)\", \"sigopt\", \"sudachidict-core (>=20220729)\", \"sudachipy (>=0.6.6)\", \"tensorboard\", \"tensorflow (>=2.6,<2.16)\", \"tensorflow-text (<2.16)\", \"tf2onnx\", \"timeout-decorator\", \"timm\", \"tokenizers (>=0.14,<0.19)\", \"torch (>=1.10,!=1.12.0)\", \"torchaudio\", \"torchvision\", \"unidic (>=1.0.2)\", \"unidic-lite (>=1.0.7)\", \"urllib3 (<2.0.0)\"]",
      "6364: dev-tensorflow = [\"GitPython (<3.1.19)\", \"Pillow (>=10.0.1,<=15.0)\", \"beautifulsoup4\", \"cookiecutter (==1.7.3)\", \"datasets (!=2.5.0)\", \"dill (<0.3.5)\", \"evaluate (>=0.2.0)\", \"faiss-cpu\", \"hf-doc-builder\", \"hf-doc-builder (>=0.3.0)\", \"isort (>=5.5.4)\", \"kenlm\", \"keras-nlp (>=0.3.1)\", \"librosa\", \"nltk\", \"onnxconverter-common\", \"onnxruntime (>=1.4.0)\", \"onnxruntime-tools (>=1.4.2)\", \"parameterized\", \"phonemizer\", \"protobuf\", \"psutil\", \"pyctcdecode (>=0.4.0)\", \"pydantic (<2)\", \"pytest (>=7.2.0)\", \"pytest-timeout\", \"pytest-xdist\", \"rjieba\", \"rouge-score (!=0.0.7,!=0.0.8,!=0.1,!=0.1.1)\", \"ruff (==0.1.5)\", \"sacrebleu (>=1.4.12,<2.0.0)\", \"sacremoses\", \"scikit-learn\", \"sentencepiece (>=0.1.91,!=0.1.92)\", \"tensorboard\", \"tensorflow (>=2.6,<2.16)\", \"tensorflow-text (<2.16)\", \"tf2onnx\", \"timeout-decorator\", \"tokenizers (>=0.14,<0.19)\", \"urllib3 (<2.0.0)\"]",
      "6365: dev-torch = [\"GitPython (<3.1.19)\", \"Pillow (>=10.0.1,<=15.0)\", \"accelerate (>=0.21.0)\", \"beautifulsoup4\", \"codecarbon (==1.2.0)\", \"cookiecutter (==1.7.3)\", \"datasets (!=2.5.0)\", \"dill (<0.3.5)\", \"evaluate (>=0.2.0)\", \"faiss-cpu\", \"fugashi (>=1.0)\", \"hf-doc-builder\", \"hf-doc-builder (>=0.3.0)\", \"ipadic (>=1.0.0,<2.0)\", \"isort (>=5.5.4)\", \"kenlm\", \"librosa\", \"nltk\", \"onnxruntime (>=1.4.0)\", \"onnxruntime-tools (>=1.4.2)\", \"optuna\", \"parameterized\", \"phonemizer\", \"protobuf\", \"psutil\", \"pyctcdecode (>=0.4.0)\", \"pydantic (<2)\", \"pytest (>=7.2.0)\", \"pytest-timeout\", \"pytest-xdist\", \"ray[tune] (>=2.7.0)\", \"rhoknp (>=1.1.0,<1.3.1)\", \"rjieba\", \"rouge-score (!=0.0.7,!=0.0.8,!=0.1,!=0.1.1)\", \"ruff (==0.1.5)\", \"sacrebleu (>=1.4.12,<2.0.0)\", \"sacremoses\", \"scikit-learn\", \"sentencepiece (>=0.1.91,!=0.1.92)\", \"sigopt\", \"sudachidict-core (>=20220729)\", \"sudachipy (>=0.6.6)\", \"tensorboard\", \"timeout-decorator\", \"timm\", \"tokenizers (>=0.14,<0.19)\", \"torch (>=1.10,!=1.12.0)\", \"torchaudio\", \"torchvision\", \"unidic (>=1.0.2)\", \"unidic-lite (>=1.0.7)\", \"urllib3 (<2.0.0)\"]",
      "6366: docs = [\"Pillow (>=10.0.1,<=15.0)\", \"accelerate (>=0.21.0)\", \"av (==9.2.0)\", \"codecarbon (==1.2.0)\", \"decord (==0.6.0)\", \"flax (>=0.4.1,<=0.7.0)\", \"hf-doc-builder\", \"jax (>=0.4.1,<=0.4.13)\", \"jaxlib (>=0.4.1,<=0.4.13)\", \"kenlm\", \"keras-nlp (>=0.3.1)\", \"librosa\", \"onnxconverter-common\", \"optax (>=0.0.8,<=0.1.4)\", \"optuna\", \"phonemizer\", \"protobuf\", \"pyctcdecode (>=0.4.0)\", \"ray[tune] (>=2.7.0)\", \"sentencepiece (>=0.1.91,!=0.1.92)\", \"sigopt\", \"tensorflow (>=2.6,<2.16)\", \"tensorflow-text (<2.16)\", \"tf2onnx\", \"timm\", \"tokenizers (>=0.14,<0.19)\", \"torch (>=1.10,!=1.12.0)\", \"torchaudio\", \"torchvision\"]",
      "6368: flax = [\"flax (>=0.4.1,<=0.7.0)\", \"jax (>=0.4.1,<=0.4.13)\", \"jaxlib (>=0.4.1,<=0.4.13)\", \"optax (>=0.0.8,<=0.1.4)\"]",
      "6371: integrations = [\"optuna\", \"ray[tune] (>=2.7.0)\", \"sigopt\"]",
      "6372: ja = [\"fugashi (>=1.0)\", \"ipadic (>=1.0.0,<2.0)\", \"rhoknp (>=1.1.0,<1.3.1)\", \"sudachidict-core (>=20220729)\", \"sudachipy (>=0.6.6)\", \"unidic (>=1.0.2)\", \"unidic-lite (>=1.0.7)\"]",
      "6374: natten = [\"natten (>=0.14.6)\"]",
      "6378: quality = [\"GitPython (<3.1.19)\", \"datasets (!=2.5.0)\", \"hf-doc-builder (>=0.3.0)\", \"isort (>=5.5.4)\", \"ruff (==0.1.5)\", \"urllib3 (<2.0.0)\"]",
      "6379: ray = [\"ray[tune] (>=2.7.0)\"]",
      "6382: sentencepiece = [\"protobuf\", \"sentencepiece (>=0.1.91,!=0.1.92)\"]",
      "6383: serving = [\"fastapi\", \"pydantic (<2)\", \"starlette\", \"uvicorn\"]",
      "6387: testing = [\"GitPython (<3.1.19)\", \"beautifulsoup4\", \"cookiecutter (==1.7.3)\", \"datasets (!=2.5.0)\", \"dill (<0.3.5)\", \"evaluate (>=0.2.0)\", \"faiss-cpu\", \"hf-doc-builder (>=0.3.0)\", \"nltk\", \"parameterized\", \"protobuf\", \"psutil\", \"pydantic (<2)\", \"pytest (>=7.2.0)\", \"pytest-timeout\", \"pytest-xdist\", \"rjieba\", \"rouge-score (!=0.0.7,!=0.0.8,!=0.1,!=0.1.1)\", \"ruff (==0.1.5)\", \"sacrebleu (>=1.4.12,<2.0.0)\", \"sacremoses\", \"tensorboard\", \"timeout-decorator\"]",
      "6388: tf = [\"keras-nlp (>=0.3.1)\", \"onnxconverter-common\", \"tensorflow (>=2.6,<2.16)\", \"tensorflow-text (<2.16)\", \"tf2onnx\"]",
      "6389: tf-cpu = [\"keras-nlp (>=0.3.1)\", \"onnxconverter-common\", \"tensorflow-cpu (>=2.6,<2.16)\", \"tensorflow-text (<2.16)\", \"tf2onnx\"]",
      "6392: tokenizers = [\"tokenizers (>=0.14,<0.19)\"]",
      "6393: torch = [\"accelerate (>=0.21.0)\", \"torch (>=1.10,!=1.12.0)\"]",
      "6395: torch-vision = [\"Pillow (>=10.0.1,<=15.0)\", \"torchvision\"]",
      "6396: torchhub = [\"filelock\", \"huggingface-hub (>=0.19.3,<1.0)\", \"importlib-metadata\", \"numpy (>=1.17)\", \"packaging (>=20.0)\", \"protobuf\", \"regex (!=2019.12.17)\", \"requests\", \"sentencepiece (>=0.1.91,!=0.1.92)\", \"tokenizers (>=0.14,<0.19)\", \"torch (>=1.10,!=1.12.0)\", \"tqdm (>=4.27)\"]",
      "6397: video = [\"av (==9.2.0)\", \"decord (==0.6.0)\"]",
      "6398: vision = [\"Pillow (>=10.0.1,<=15.0)\"]",
      "",
      "---------------",
      "--- Hunk 10 ---",
      "[Context before]",
      "6956: [metadata]",
      "6957: lock-version = \"2.0\"",
      "6958: python-versions = \">=3.8,<3.11\"",
      "",
      "[Removed Lines]",
      "6959: content-hash = \"4c84d994449f859816e48dd00d77f31f6f9d964e29a9f6060300c51d923786e0\"",
      "",
      "[Added Lines]",
      "7189: content-hash = \"c1c51259ab3b886039dcf7eb746a45815d4b8afcaa4bdbe179c891810aee553f\"",
      "",
      "---------------"
    ],
    "rasa/core/featurizers/single_state_featurizer.py||rasa/core/featurizers/single_state_featurizer.py": [
      "File: rasa/core/featurizers/single_state_featurizer.py -> rasa/core/featurizers/single_state_featurizer.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "1: import logging",
      "2: import numpy as np",
      "3: import scipy.sparse",
      "6: from rasa.core.featurizers.precomputation import MessageContainerForCoreFeaturization",
      "7: from rasa.nlu.extractors.extractor import EntityTagSpec",
      "",
      "[Removed Lines]",
      "4: from typing import List, Optional, Dict, Text, Set, Any",
      "",
      "[Added Lines]",
      "2: from typing import List, Optional, Dict, Text, Set, Any",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "362:             for action in domain.action_names_or_texts",
      "363:         ]",
      "366: class IntentTokenizerSingleStateFeaturizer(SingleStateFeaturizer):",
      "367:     \"\"\"A SingleStateFeaturizer for use with policies that predict intent labels.\"\"\"",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "366:     def to_dict(self) -> Dict[str, Any]:",
      "367:         return {",
      "368:             \"action_texts\": self.action_texts,",
      "369:             \"entity_tag_specs\": self.entity_tag_specs,",
      "370:             \"feature_states\": self._default_feature_states,",
      "371:         }",
      "373:     @classmethod",
      "374:     def create_from_dict(",
      "375:         cls, data: Dict[str, Any]",
      "376:     ) -> Optional[\"SingleStateFeaturizer\"]:",
      "377:         if not data:",
      "378:             return None",
      "380:         featurizer = SingleStateFeaturizer()",
      "381:         featurizer.action_texts = data[\"action_texts\"]",
      "382:         featurizer._default_feature_states = data[\"feature_states\"]",
      "383:         featurizer.entity_tag_specs = data[\"entity_tag_specs\"]",
      "384:         return featurizer",
      "",
      "---------------"
    ],
    "rasa/core/featurizers/tracker_featurizers.py||rasa/core/featurizers/tracker_featurizers.py": [
      "File: rasa/core/featurizers/tracker_featurizers.py -> rasa/core/featurizers/tracker_featurizers.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "1: from __future__ import annotations",
      "9: from typing import (",
      "10:     Tuple,",
      "11:     List,",
      "",
      "[Removed Lines]",
      "2: from pathlib import Path",
      "3: from collections import defaultdict",
      "4: from abc import abstractmethod",
      "5: import jsonpickle",
      "6: import logging",
      "8: from tqdm import tqdm",
      "",
      "[Added Lines]",
      "3: import logging",
      "4: from abc import abstractmethod",
      "5: from collections import defaultdict",
      "6: from pathlib import Path",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "18:     Set,",
      "19:     DefaultDict,",
      "20:     cast,",
      "21: )",
      "22: import numpy as np",
      "27: import rasa.shared.core.trackers",
      "28: import rasa.shared.utils.io",
      "34: from rasa.shared.core.constants import (",
      "35:     USER,",
      "36:     ACTION_UNLIKELY_INTENT_NAME,",
      "37:     PREVIOUS_ACTION,",
      "38: )",
      "39: from rasa.shared.exceptions import RasaException",
      "40: from rasa.utils.tensorflow.constants import LABEL_PAD_ID",
      "41: from rasa.utils.tensorflow.model_data import ragged_array_to_ndarray",
      "",
      "[Removed Lines]",
      "24: from rasa.core.featurizers.single_state_featurizer import SingleStateFeaturizer",
      "25: from rasa.core.featurizers.precomputation import MessageContainerForCoreFeaturization",
      "26: from rasa.core.exceptions import InvalidTrackerFeaturizerUsageError",
      "29: from rasa.shared.nlu.constants import TEXT, INTENT, ENTITIES, ACTION_NAME",
      "30: from rasa.shared.nlu.training_data.features import Features",
      "31: from rasa.shared.core.trackers import DialogueStateTracker",
      "32: from rasa.shared.core.domain import State, Domain",
      "33: from rasa.shared.core.events import Event, ActionExecuted, UserUttered",
      "",
      "[Added Lines]",
      "19:     Type,",
      "20:     Callable,",
      "21:     ClassVar,",
      "25: from tqdm import tqdm",
      "29: from rasa.core.exceptions import InvalidTrackerFeaturizerUsageError",
      "30: from rasa.core.featurizers.precomputation import MessageContainerForCoreFeaturization",
      "31: from rasa.core.featurizers.single_state_featurizer import SingleStateFeaturizer",
      "37: from rasa.shared.core.domain import State, Domain",
      "38: from rasa.shared.core.events import Event, ActionExecuted, UserUttered",
      "39: from rasa.shared.core.trackers import DialogueStateTracker",
      "41: from rasa.shared.nlu.constants import TEXT, INTENT, ENTITIES, ACTION_NAME",
      "42: from rasa.shared.nlu.training_data.features import Features",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "64: class TrackerFeaturizer:",
      "65:     \"\"\"Base class for actual tracker featurizers.\"\"\"",
      "67:     def __init__(",
      "68:         self, state_featurizer: Optional[SingleStateFeaturizer] = None",
      "69:     ) -> None:",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "70:     # Class registry to store all subclasses",
      "71:     _registry: ClassVar[Dict[str, Type[\"TrackerFeaturizer\"]]] = {}",
      "72:     _featurizer_type: str = \"TrackerFeaturizer\"",
      "",
      "---------------",
      "--- Hunk 4 ---",
      "[Context before]",
      "74:         \"\"\"",
      "75:         self.state_featurizer = state_featurizer",
      "77:     @staticmethod",
      "78:     def _create_states(",
      "79:         tracker: DialogueStateTracker,",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "84:     @classmethod",
      "85:     def register(cls, featurizer_type: str) -> Callable:",
      "86:         \"\"\"Decorator to register featurizer subclasses.\"\"\"",
      "88:         def wrapper(subclass: Type[\"TrackerFeaturizer\"]) -> Type[\"TrackerFeaturizer\"]:",
      "89:             cls._registry[featurizer_type] = subclass",
      "90:             # Store the type identifier in the class for serialization",
      "91:             subclass._featurizer_type = featurizer_type",
      "92:             return subclass",
      "94:         return wrapper",
      "96:     @classmethod",
      "97:     def from_dict(cls, data: Dict[str, Any]) -> \"TrackerFeaturizer\":",
      "98:         \"\"\"Create featurizer instance from dictionary.\"\"\"",
      "99:         featurizer_type = data.pop(\"type\")",
      "101:         if featurizer_type not in cls._registry:",
      "102:             raise ValueError(f\"Unknown featurizer type: {featurizer_type}\")",
      "104:         # Get the correct subclass and instantiate it",
      "105:         subclass = cls._registry[featurizer_type]",
      "106:         return subclass.create_from_dict(data)",
      "108:     @classmethod",
      "109:     @abstractmethod",
      "110:     def create_from_dict(cls, data: Dict[str, Any]) -> \"TrackerFeaturizer\":",
      "111:         \"\"\"Each subclass must implement its own creation from dict method.\"\"\"",
      "112:         pass",
      "",
      "---------------",
      "--- Hunk 5 ---",
      "[Context before]",
      "465:             self.state_featurizer.entity_tag_specs = []",
      "467:         # noinspection PyTypeChecker",
      "472:     @staticmethod",
      "473:     def load(path: Union[Text, Path]) -> Optional[TrackerFeaturizer]:",
      "",
      "[Removed Lines]",
      "468:         rasa.shared.utils.io.write_text_file(",
      "469:             str(jsonpickle.encode(self)), featurizer_file",
      "470:         )",
      "",
      "[Added Lines]",
      "505:         rasa.shared.utils.io.dump_obj_as_json_to_file(featurizer_file, self.to_dict())",
      "",
      "---------------",
      "--- Hunk 6 ---",
      "[Context before]",
      "481:         \"\"\"",
      "482:         featurizer_file = Path(path) / FEATURIZER_FILE",
      "483:         if featurizer_file.is_file():",
      "486:         logger.error(",
      "487:             f\"Couldn't load featurizer for policy. \"",
      "",
      "[Removed Lines]",
      "484:             return jsonpickle.decode(rasa.shared.utils.io.read_file(featurizer_file))",
      "",
      "[Added Lines]",
      "519:             data = rasa.shared.utils.io.read_json_file(featurizer_file)",
      "521:             if \"type\" not in data:",
      "522:                 logger.error(",
      "523:                     f\"Couldn't load featurizer for policy. \"",
      "524:                     f\"File '{featurizer_file}' does not contain all \"",
      "525:                     f\"necessary information. 'type' is missing.\"",
      "526:                 )",
      "527:                 return None",
      "529:             return TrackerFeaturizer.from_dict(data)",
      "",
      "---------------",
      "--- Hunk 7 ---",
      "[Context before]",
      "508:             )",
      "509:         ]",
      "512: class FullDialogueTrackerFeaturizer(TrackerFeaturizer):",
      "513:     \"\"\"Creates full dialogue training data for time distributed architectures.",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "556:     def to_dict(self) -> Dict[str, Any]:",
      "557:         return {",
      "558:             \"type\": self.__class__._featurizer_type,",
      "559:             \"state_featurizer\": (",
      "560:                 self.state_featurizer.to_dict() if self.state_featurizer else None",
      "561:             ),",
      "562:         }",
      "565: @TrackerFeaturizer.register(\"FullDialogueTrackerFeaturizer\")",
      "",
      "---------------",
      "--- Hunk 8 ---",
      "[Context before]",
      "647:         return trackers_as_states",
      "650: class MaxHistoryTrackerFeaturizer(TrackerFeaturizer):",
      "651:     \"\"\"Truncates the tracker history into `max_history` long sequences.",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "703:     def to_dict(self) -> Dict[str, Any]:",
      "704:         return super().to_dict()",
      "706:     @classmethod",
      "707:     def create_from_dict(cls, data: Dict[str, Any]) -> \"FullDialogueTrackerFeaturizer\":",
      "708:         state_featurizer = SingleStateFeaturizer.create_from_dict(",
      "709:             data[\"state_featurizer\"]",
      "710:         )",
      "711:         return cls(",
      "712:             state_featurizer,",
      "713:         )",
      "716: @TrackerFeaturizer.register(\"MaxHistoryTrackerFeaturizer\")",
      "",
      "---------------",
      "--- Hunk 9 ---",
      "[Context before]",
      "888:         return trackers_as_states",
      "891: class IntentMaxHistoryTrackerFeaturizer(MaxHistoryTrackerFeaturizer):",
      "892:     \"\"\"Truncates the tracker history into `max_history` long sequences.",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "957:     def to_dict(self) -> Dict[str, Any]:",
      "958:         data = super().to_dict()",
      "959:         data.update(",
      "960:             {",
      "961:                 \"remove_duplicates\": self.remove_duplicates,",
      "962:                 \"max_history\": self.max_history,",
      "963:             }",
      "964:         )",
      "965:         return data",
      "967:     @classmethod",
      "968:     def create_from_dict(cls, data: Dict[str, Any]) -> \"MaxHistoryTrackerFeaturizer\":",
      "969:         state_featurizer = SingleStateFeaturizer.create_from_dict(",
      "970:             data[\"state_featurizer\"]",
      "971:         )",
      "972:         return cls(state_featurizer, data[\"max_history\"], data[\"remove_duplicates\"])",
      "975: @TrackerFeaturizer.register(\"IntentMaxHistoryTrackerFeaturizer\")",
      "",
      "---------------",
      "--- Hunk 10 ---",
      "[Context before]",
      "1167:         return trackers_as_states",
      "1170: def _is_prev_action_unlikely_intent_in_state(state: State) -> bool:",
      "1171:     prev_action_name = state.get(PREVIOUS_ACTION, {}).get(ACTION_NAME)",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "1254:     def to_dict(self) -> Dict[str, Any]:",
      "1255:         return super().to_dict()",
      "1257:     @classmethod",
      "1258:     def create_from_dict(",
      "1259:         cls, data: Dict[str, Any]",
      "1260:     ) -> \"IntentMaxHistoryTrackerFeaturizer\":",
      "1261:         state_featurizer = SingleStateFeaturizer.create_from_dict(",
      "1262:             data[\"state_featurizer\"]",
      "1263:         )",
      "1264:         return cls(state_featurizer, data[\"max_history\"], data[\"remove_duplicates\"])",
      "",
      "---------------"
    ],
    "rasa/core/policies/ted_policy.py||rasa/core/policies/ted_policy.py": [
      "File: rasa/core/policies/ted_policy.py -> rasa/core/policies/ted_policy.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "1: from __future__ import annotations",
      "5: from pathlib import Path",
      "6: from collections import defaultdict",
      "7: import contextlib",
      "9: import numpy as np",
      "10: import tensorflow as tf",
      "13: from rasa.engine.graph import ExecutionContext",
      "14: from rasa.engine.storage.resource import Resource",
      "15: from rasa.engine.storage.storage import ModelStorage",
      "",
      "[Removed Lines]",
      "2: import logging",
      "4: from rasa.engine.recipes.default_recipe import DefaultV1Recipe",
      "11: from typing import Any, List, Optional, Text, Dict, Tuple, Union, Type",
      "",
      "[Added Lines]",
      "3: import logging",
      "7: from typing import Any, List, Optional, Text, Dict, Tuple, Union, Type",
      "12: from rasa.engine.recipes.default_recipe import DefaultV1Recipe",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "49: from rasa.shared.core.events import EntitiesAdded, Event",
      "50: from rasa.shared.core.domain import Domain",
      "51: from rasa.shared.nlu.training_data.message import Message",
      "53: import rasa.shared.utils.io",
      "54: import rasa.utils.io",
      "55: from rasa.utils import train_utils",
      "61:     FeatureArray,",
      "63: )",
      "64: from rasa.utils.tensorflow.model_data_utils import convert_to_data_format",
      "65: from rasa.utils.tensorflow.constants import (",
      "66:     LABEL,",
      "",
      "[Removed Lines]",
      "52: from rasa.shared.nlu.training_data.features import Features",
      "56: from rasa.utils.tensorflow.models import RasaModel, TransformerRasaModel",
      "57: from rasa.utils.tensorflow import rasa_layers",
      "58: from rasa.utils.tensorflow.model_data import (",
      "59:     RasaModelData,",
      "60:     FeatureSignature,",
      "62:     Data,",
      "",
      "[Added Lines]",
      "52: from rasa.shared.nlu.training_data.features import (",
      "53:     Features,",
      "54:     save_features,",
      "55:     load_features,",
      "56: )",
      "60: from rasa.utils.tensorflow.feature_array import (",
      "62:     serialize_nested_feature_arrays,",
      "63:     deserialize_nested_feature_arrays,",
      "65: from rasa.utils.tensorflow.models import RasaModel, TransformerRasaModel",
      "66: from rasa.utils.tensorflow import rasa_layers",
      "67: from rasa.utils.tensorflow.model_data import RasaModelData, FeatureSignature, Data",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "961:             model_path: Path where model is to be persisted",
      "962:         \"\"\"",
      "963:         model_filename = self._metadata_filename()",
      "969:         )",
      "972:         )",
      "975:         )",
      "978:             dict(self._label_data.data) if self._label_data is not None else {},",
      "979:         )",
      "980:         entity_tag_specs = (",
      "981:             [tag_spec._asdict() for tag_spec in self._entity_tag_specs]",
      "982:             if self._entity_tag_specs",
      "",
      "[Removed Lines]",
      "964:         rasa.utils.io.json_pickle(",
      "965:             model_path / f\"{model_filename}.priority.pkl\", self.priority",
      "966:         )",
      "967:         rasa.utils.io.pickle_dump(",
      "968:             model_path / f\"{model_filename}.meta.pkl\", self.config",
      "970:         rasa.utils.io.pickle_dump(",
      "971:             model_path / f\"{model_filename}.data_example.pkl\", self.data_example",
      "973:         rasa.utils.io.pickle_dump(",
      "974:             model_path / f\"{model_filename}.fake_features.pkl\", self.fake_features",
      "976:         rasa.utils.io.pickle_dump(",
      "977:             model_path / f\"{model_filename}.label_data.pkl\",",
      "",
      "[Added Lines]",
      "968:         rasa.shared.utils.io.dump_obj_as_json_to_file(",
      "969:             model_path / f\"{model_filename}.priority.json\", self.priority",
      "971:         rasa.shared.utils.io.dump_obj_as_json_to_file(",
      "972:             model_path / f\"{model_filename}.meta.json\", self.config",
      "974:         # save data example",
      "975:         serialize_nested_feature_arrays(",
      "976:             self.data_example,",
      "977:             str(model_path / f\"{model_filename}.data_example.st\"),",
      "978:             str(model_path / f\"{model_filename}.data_example_metadata.json\"),",
      "980:         # save label data",
      "981:         serialize_nested_feature_arrays(",
      "983:             str(model_path / f\"{model_filename}.label_data.st\"),",
      "984:             str(model_path / f\"{model_filename}.label_data_metadata.json\"),",
      "985:         )",
      "986:         # save fake features",
      "987:         metadata = save_features(",
      "988:             self.fake_features, str(model_path / f\"{model_filename}.fake_features.st\")",
      "989:         )",
      "990:         rasa.shared.utils.io.dump_obj_as_json_to_file(",
      "991:             model_path / f\"{model_filename}.fake_features_metadata.json\", metadata",
      "",
      "---------------",
      "--- Hunk 4 ---",
      "[Context before]",
      "994:             model_path: Path where model is to be persisted.",
      "995:         \"\"\"",
      "996:         tf_model_file = model_path / f\"{cls._metadata_filename()}.tf_model\"",
      "999:         )",
      "1002:         )",
      "1005:         )",
      "1009:         )",
      "1010:         entity_tag_specs = rasa.shared.utils.io.read_json_file(",
      "1011:             model_path / f\"{cls._metadata_filename()}.entity_tag_specs.json\"",
      "",
      "[Removed Lines]",
      "997:         loaded_data = rasa.utils.io.pickle_load(",
      "998:             model_path / f\"{cls._metadata_filename()}.data_example.pkl\"",
      "1000:         label_data = rasa.utils.io.pickle_load(",
      "1001:             model_path / f\"{cls._metadata_filename()}.label_data.pkl\"",
      "1003:         fake_features = rasa.utils.io.pickle_load(",
      "1004:             model_path / f\"{cls._metadata_filename()}.fake_features.pkl\"",
      "1006:         label_data = RasaModelData(data=label_data)",
      "1007:         priority = rasa.utils.io.json_unpickle(",
      "1008:             model_path / f\"{cls._metadata_filename()}.priority.pkl\"",
      "",
      "[Added Lines]",
      "1012:         # load data example",
      "1013:         loaded_data = deserialize_nested_feature_arrays(",
      "1014:             str(model_path / f\"{cls._metadata_filename()}.data_example.st\"),",
      "1015:             str(model_path / f\"{cls._metadata_filename()}.data_example_metadata.json\"),",
      "1017:         # load label data",
      "1018:         loaded_label_data = deserialize_nested_feature_arrays(",
      "1019:             str(model_path / f\"{cls._metadata_filename()}.label_data.st\"),",
      "1020:             str(model_path / f\"{cls._metadata_filename()}.label_data_metadata.json\"),",
      "1022:         label_data = RasaModelData(data=loaded_label_data)",
      "1024:         # load fake features",
      "1025:         metadata = rasa.shared.utils.io.read_json_file(",
      "1026:             model_path / f\"{cls._metadata_filename()}.fake_features_metadata.json\"",
      "1028:         fake_features = load_features(",
      "1029:             str(model_path / f\"{cls._metadata_filename()}.fake_features.st\"), metadata",
      "1030:         )",
      "1032:         priority = rasa.shared.utils.io.read_json_file(",
      "1033:             model_path / f\"{cls._metadata_filename()}.priority.json\"",
      "",
      "---------------",
      "--- Hunk 5 ---",
      "[Context before]",
      "1023:             )",
      "1024:             for tag_spec in entity_tag_specs",
      "1025:         ]",
      "1028:         )",
      "1030:         return {",
      "",
      "[Removed Lines]",
      "1026:         model_config = rasa.utils.io.pickle_load(",
      "1027:             model_path / f\"{cls._metadata_filename()}.meta.pkl\"",
      "",
      "[Added Lines]",
      "1051:         model_config = rasa.shared.utils.io.read_json_file(",
      "1052:             model_path / f\"{cls._metadata_filename()}.meta.json\"",
      "",
      "---------------",
      "--- Hunk 6 ---",
      "[Context before]",
      "1070:     ) -> TEDPolicy:",
      "1071:         featurizer = TrackerFeaturizer.load(model_path)",
      "1074:             return cls(",
      "1075:                 config,",
      "1076:                 model_storage,",
      "",
      "[Removed Lines]",
      "1073:         if not (model_path / f\"{cls._metadata_filename()}.data_example.pkl\").is_file():",
      "",
      "[Added Lines]",
      "1098:         if not (model_path / f\"{cls._metadata_filename()}.data_example.st\").is_file():",
      "",
      "---------------"
    ],
    "rasa/core/policies/unexpected_intent_policy.py||rasa/core/policies/unexpected_intent_policy.py": [
      "File: rasa/core/policies/unexpected_intent_policy.py -> rasa/core/policies/unexpected_intent_policy.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "16: from rasa.shared.core.trackers import DialogueStateTracker",
      "17: from rasa.shared.core.constants import SLOTS, ACTIVE_LOOP, ACTION_UNLIKELY_INTENT_NAME",
      "18: from rasa.shared.core.events import UserUttered, ActionExecuted",
      "19: from rasa.shared.nlu.constants import (",
      "20:     INTENT,",
      "21:     TEXT,",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "20: import rasa.shared.utils.io",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "103: )",
      "104: from rasa.utils.tensorflow import layers",
      "105: from rasa.utils.tensorflow.model_data import RasaModelData, FeatureArray, Data",
      "108: from rasa.core.exceptions import RasaCoreException",
      "109: from rasa.shared.utils import common",
      "",
      "[Removed Lines]",
      "107: import rasa.utils.io as io_utils",
      "",
      "[Added Lines]",
      "[None]",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "881:             model_path: Path where model is to be persisted",
      "882:         \"\"\"",
      "883:         super().persist_model_utilities(model_path)",
      "887:         )",
      "889:     @classmethod",
      "",
      "[Removed Lines]",
      "884:         io_utils.pickle_dump(",
      "885:             model_path / f\"{self._metadata_filename()}.label_quantiles.pkl\",",
      "886:             self.label_quantiles,",
      "",
      "[Added Lines]",
      "885:         from safetensors.numpy import save_file",
      "887:         save_file(",
      "888:             {str(k): np.array(v) for k, v in self.label_quantiles.items()},",
      "889:             model_path / f\"{self._metadata_filename()}.label_quantiles.st\",",
      "",
      "---------------",
      "--- Hunk 4 ---",
      "[Context before]",
      "894:             model_path: Path where model is to be persisted.",
      "895:         \"\"\"",
      "896:         model_utilties = super()._load_model_utilities(model_path)",
      "899:         )",
      "900:         model_utilties.update({\"label_quantiles\": label_quantiles})",
      "901:         return model_utilties",
      "",
      "[Removed Lines]",
      "897:         label_quantiles = io_utils.pickle_load(",
      "898:             model_path / f\"{cls._metadata_filename()}.label_quantiles.pkl\"",
      "",
      "[Added Lines]",
      "901:         from safetensors.numpy import load_file",
      "903:         loaded_label_quantiles = load_file(",
      "904:             model_path / f\"{cls._metadata_filename()}.label_quantiles.st\"",
      "906:         label_quantiles = {int(k): list(v) for k, v in loaded_label_quantiles.items()}",
      "",
      "---------------"
    ],
    "rasa/nlu/classifiers/diet_classifier.py||rasa/nlu/classifiers/diet_classifier.py": [
      "File: rasa/nlu/classifiers/diet_classifier.py -> rasa/nlu/classifiers/diet_classifier.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "1: from __future__ import annotations",
      "2: import copy",
      "3: import logging",
      "4: from collections import defaultdict",
      "5: from pathlib import Path",
      "10: import numpy as np",
      "11: import scipy.sparse",
      "12: import tensorflow as tf",
      "16: from rasa.engine.graph import ExecutionContext, GraphComponent",
      "17: from rasa.engine.recipes.default_recipe import DefaultV1Recipe",
      "18: from rasa.engine.storage.resource import Resource",
      "",
      "[Removed Lines]",
      "7: from rasa.exceptions import ModelNotFound",
      "8: from rasa.nlu.featurizers.featurizer import Featurizer",
      "14: from typing import Any, Dict, List, Optional, Text, Tuple, Union, TypeVar, Type",
      "",
      "[Added Lines]",
      "7: from typing import Any, Dict, List, Optional, Text, Tuple, Union, TypeVar, Type",
      "13: from rasa.exceptions import ModelNotFound",
      "14: from rasa.nlu.featurizers.featurizer import Featurizer",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "20: from rasa.nlu.extractors.extractor import EntityExtractorMixin",
      "21: from rasa.nlu.classifiers.classifier import IntentClassifier",
      "22: import rasa.shared.utils.io",
      "24: import rasa.nlu.utils.bilou_utils as bilou_utils",
      "25: from rasa.shared.constants import DIAGNOSTIC_DATA",
      "26: from rasa.nlu.extractors.extractor import EntityTagSpec",
      "27: from rasa.nlu.classifiers import LABEL_RANKING_LENGTH",
      "28: from rasa.utils import train_utils",
      "29: from rasa.utils.tensorflow import rasa_layers",
      "30: from rasa.utils.tensorflow.models import RasaModel, TransformerRasaModel",
      "31: from rasa.utils.tensorflow.model_data import (",
      "32:     RasaModelData,",
      "33:     FeatureSignature,",
      "35: )",
      "36: from rasa.nlu.constants import TOKENS_NAMES, DEFAULT_TRANSFORMER_SIZE",
      "37: from rasa.shared.nlu.constants import (",
      "",
      "[Removed Lines]",
      "23: import rasa.utils.io as io_utils",
      "34:     FeatureArray,",
      "",
      "[Added Lines]",
      "28: from rasa.utils.tensorflow.feature_array import (",
      "29:     FeatureArray,",
      "30:     serialize_nested_feature_arrays,",
      "31:     deserialize_nested_feature_arrays,",
      "32: )",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "1086:             self.model.save(str(tf_model_file))",
      "1094:             )",
      "1097:                 dict(self._label_data.data) if self._label_data is not None else {},",
      "1098:             )",
      "1100:                 model_path / f\"{file_name}.index_label_id_mapping.json\",",
      "1101:                 self.index_label_id_mapping,",
      "1102:             )",
      "",
      "[Removed Lines]",
      "1088:             io_utils.pickle_dump(",
      "1089:                 model_path / f\"{file_name}.data_example.pkl\", self._data_example",
      "1090:             )",
      "1091:             io_utils.pickle_dump(",
      "1092:                 model_path / f\"{file_name}.sparse_feature_sizes.pkl\",",
      "1093:                 self._sparse_feature_sizes,",
      "1095:             io_utils.pickle_dump(",
      "1096:                 model_path / f\"{file_name}.label_data.pkl\",",
      "1099:             io_utils.json_pickle(",
      "",
      "[Added Lines]",
      "1089:             # save data example",
      "1090:             serialize_nested_feature_arrays(",
      "1091:                 self._data_example,",
      "1092:                 model_path / f\"{file_name}.data_example.st\",",
      "1093:                 model_path / f\"{file_name}.data_example_metadata.json\",",
      "1095:             # save label data",
      "1096:             serialize_nested_feature_arrays(",
      "1098:                 model_path / f\"{file_name}.label_data.st\",",
      "1099:                 model_path / f\"{file_name}.label_data_metadata.json\",",
      "1102:             rasa.shared.utils.io.dump_obj_as_json_to_file(",
      "1103:                 model_path / f\"{file_name}.sparse_feature_sizes.json\",",
      "1104:                 self._sparse_feature_sizes,",
      "1105:             )",
      "1106:             rasa.shared.utils.io.dump_obj_as_json_to_file(",
      "",
      "---------------",
      "--- Hunk 4 ---",
      "[Context before]",
      "1185:     ]:",
      "1186:         file_name = cls.__name__",
      "1190:         )",
      "1195:         )",
      "1197:             model_path / f\"{file_name}.index_label_id_mapping.json\"",
      "1198:         )",
      "1199:         entity_tag_specs = rasa.shared.utils.io.read_json_file(",
      "",
      "[Removed Lines]",
      "1188:         data_example = io_utils.pickle_load(",
      "1189:             model_path / f\"{file_name}.data_example.pkl\"",
      "1191:         label_data = io_utils.pickle_load(model_path / f\"{file_name}.label_data.pkl\")",
      "1192:         label_data = RasaModelData(data=label_data)",
      "1193:         sparse_feature_sizes = io_utils.pickle_load(",
      "1194:             model_path / f\"{file_name}.sparse_feature_sizes.pkl\"",
      "1196:         index_label_id_mapping = io_utils.json_unpickle(",
      "",
      "[Added Lines]",
      "1195:         # load data example",
      "1196:         data_example = deserialize_nested_feature_arrays(",
      "1197:             str(model_path / f\"{file_name}.data_example.st\"),",
      "1198:             str(model_path / f\"{file_name}.data_example_metadata.json\"),",
      "1200:         # load label data",
      "1201:         loaded_label_data = deserialize_nested_feature_arrays(",
      "1202:             str(model_path / f\"{file_name}.label_data.st\"),",
      "1203:             str(model_path / f\"{file_name}.label_data_metadata.json\"),",
      "1204:         )",
      "1205:         label_data = RasaModelData(data=loaded_label_data)",
      "1207:         sparse_feature_sizes = rasa.shared.utils.io.read_json_file(",
      "1208:             model_path / f\"{file_name}.sparse_feature_sizes.json\"",
      "1210:         index_label_id_mapping = rasa.shared.utils.io.read_json_file(",
      "",
      "---------------",
      "--- Hunk 5 ---",
      "[Context before]",
      "1213:             for tag_spec in entity_tag_specs",
      "1214:         ]",
      "1217:         index_label_id_mapping = {",
      "1218:             int(key): value for key, value in index_label_id_mapping.items()",
      "1219:         }",
      "",
      "[Removed Lines]",
      "1216:         # jsonpickle converts dictionary keys to strings",
      "",
      "[Added Lines]",
      "[None]",
      "",
      "---------------"
    ],
    "rasa/nlu/classifiers/logistic_regression_classifier.py||rasa/nlu/classifiers/logistic_regression_classifier.py": [
      "File: rasa/nlu/classifiers/logistic_regression_classifier.py -> rasa/nlu/classifiers/logistic_regression_classifier.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "1: import logging",
      "2: from typing import Any, Text, Dict, List, Type, Tuple",
      "5: from scipy.sparse import hstack, vstack, csr_matrix",
      "6: from sklearn.linear_model import LogisticRegression",
      "8: from rasa.engine.storage.resource import Resource",
      "9: from rasa.engine.storage.storage import ModelStorage",
      "12: from rasa.nlu.classifiers import LABEL_RANKING_LENGTH",
      "14: from rasa.nlu.classifiers.classifier import IntentClassifier",
      "17: from rasa.shared.nlu.constants import TEXT, INTENT",
      "18: from rasa.utils.tensorflow.constants import RANKING_LENGTH",
      "20: logger = logging.getLogger(__name__)",
      "",
      "[Removed Lines]",
      "4: import joblib",
      "10: from rasa.engine.recipes.default_recipe import DefaultV1Recipe",
      "11: from rasa.engine.graph import ExecutionContext, GraphComponent",
      "13: from rasa.nlu.featurizers.featurizer import Featurizer",
      "15: from rasa.shared.nlu.training_data.training_data import TrainingData",
      "16: from rasa.shared.nlu.training_data.message import Message",
      "",
      "[Added Lines]",
      "7: from rasa.engine.graph import ExecutionContext, GraphComponent",
      "8: from rasa.engine.recipes.default_recipe import DefaultV1Recipe",
      "13: from rasa.nlu.featurizers.featurizer import Featurizer",
      "15: from rasa.shared.nlu.training_data.message import Message",
      "16: from rasa.shared.nlu.training_data.training_data import TrainingData",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "159:     def persist(self) -> None:",
      "160:         \"\"\"Persist this model into the passed directory.\"\"\"",
      "161:         with self._model_storage.write_to(self._resource) as model_dir:",
      "164:             logger.debug(f\"Saved intent classifier to '{path}'.\")",
      "166:     @classmethod",
      "",
      "[Removed Lines]",
      "162:             path = model_dir / f\"{self._resource.name}.joblib\"",
      "163:             joblib.dump(self.clf, path)",
      "",
      "[Added Lines]",
      "160:         import skops.io as sio",
      "163:             path = model_dir / f\"{self._resource.name}.skops\"",
      "164:             sio.dump(self.clf, path)",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "174:     ) -> \"LogisticRegressionClassifier\":",
      "175:         \"\"\"Loads trained component (see parent class for full docstring).\"\"\"",
      "176:         try:",
      "177:             with model_storage.read_from(resource) as model_dir:",
      "179:                 component = cls(",
      "180:                     config, execution_context.node_name, model_storage, resource",
      "181:                 )",
      "",
      "[Removed Lines]",
      "178:                 classifier = joblib.load(model_dir / f\"{resource.name}.joblib\")",
      "",
      "[Added Lines]",
      "177:         import skops.io as sio",
      "181:                 classifier_file = model_dir / f\"{resource.name}.skops\"",
      "182:                 unknown_types = sio.get_untrusted_types(file=classifier_file)",
      "184:                 if unknown_types:",
      "185:                     logger.debug(",
      "186:                         f\"Untrusted types ({unknown_types}) found when \"",
      "187:                         f\"loading {classifier_file}!\",",
      "188:                     )",
      "189:                     raise ValueError()",
      "191:                 classifier = sio.load(classifier_file, trusted=unknown_types)",
      "",
      "---------------"
    ],
    "rasa/nlu/classifiers/sklearn_intent_classifier.py||rasa/nlu/classifiers/sklearn_intent_classifier.py": [
      "File: rasa/nlu/classifiers/sklearn_intent_classifier.py -> rasa/nlu/classifiers/sklearn_intent_classifier.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "1: from __future__ import annotations",
      "2: import logging",
      "4: import typing",
      "5: import warnings",
      "6: from typing import Any, Dict, List, Optional, Text, Tuple, Type",
      "",
      "[Removed Lines]",
      "3: from rasa.nlu.featurizers.dense_featurizer.dense_featurizer import DenseFeaturizer",
      "",
      "[Added Lines]",
      "[None]",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "8: import numpy as np",
      "10: import rasa.shared.utils.io",
      "12: from rasa.engine.graph import GraphComponent, ExecutionContext",
      "13: from rasa.engine.recipes.default_recipe import DefaultV1Recipe",
      "14: from rasa.engine.storage.resource import Resource",
      "15: from rasa.engine.storage.storage import ModelStorage",
      "17: from rasa.nlu.classifiers import LABEL_RANKING_LENGTH",
      "18: from rasa.shared.exceptions import RasaException",
      "19: from rasa.shared.nlu.constants import TEXT",
      "22: from rasa.shared.nlu.training_data.message import Message",
      "23: from rasa.utils.tensorflow.constants import FEATURIZERS",
      "25: logger = logging.getLogger(__name__)",
      "",
      "[Removed Lines]",
      "11: import rasa.utils.io as io_utils",
      "16: from rasa.shared.constants import DOCS_URL_TRAINING_DATA_NLU",
      "20: from rasa.nlu.classifiers.classifier import IntentClassifier",
      "21: from rasa.shared.nlu.training_data.training_data import TrainingData",
      "",
      "[Added Lines]",
      "16: from rasa.nlu.classifiers.classifier import IntentClassifier",
      "17: from rasa.nlu.featurizers.dense_featurizer.dense_featurizer import DenseFeaturizer",
      "18: from rasa.shared.constants import DOCS_URL_TRAINING_DATA_NLU",
      "22: from rasa.shared.nlu.training_data.training_data import TrainingData",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "267:     def persist(self) -> None:",
      "268:         \"\"\"Persist this model into the passed directory.\"\"\"",
      "269:         with self._model_storage.write_to(self._resource) as model_dir:",
      "270:             file_name = self.__class__.__name__",
      "274:             if self.clf and self.le:",
      "278:     @classmethod",
      "279:     def load(",
      "",
      "[Removed Lines]",
      "271:             classifier_file_name = model_dir / f\"{file_name}_classifier.pkl\"",
      "272:             encoder_file_name = model_dir / f\"{file_name}_encoder.pkl\"",
      "275:                 io_utils.json_pickle(encoder_file_name, self.le.classes_)",
      "276:                 io_utils.json_pickle(classifier_file_name, self.clf.best_estimator_)",
      "",
      "[Added Lines]",
      "269:         import skops.io as sio",
      "273:             classifier_file_name = model_dir / f\"{file_name}_classifier.skops\"",
      "274:             encoder_file_name = model_dir / f\"{file_name}_encoder.json\"",
      "277:                 # convert self.le.classes_ (numpy array of strings) to a list in order",
      "278:                 # to use json dump",
      "279:                 rasa.shared.utils.io.dump_obj_as_json_to_file(",
      "280:                     encoder_file_name, list(self.le.classes_)",
      "281:                 )",
      "282:                 sio.dump(self.clf.best_estimator_, classifier_file_name)",
      "",
      "---------------",
      "--- Hunk 4 ---",
      "[Context before]",
      "286:     ) -> SklearnIntentClassifier:",
      "287:         \"\"\"Loads trained component (see parent class for full docstring).\"\"\"",
      "288:         from sklearn.preprocessing import LabelEncoder",
      "290:         try:",
      "291:             with model_storage.read_from(resource) as model_dir:",
      "292:                 file_name = cls.__name__",
      "295:                 if classifier_file.exists():",
      "304:         except ValueError:",
      "305:             logger.debug(",
      "306:                 f\"Failed to load '{cls.__name__}' from model storage. Resource \"",
      "",
      "[Removed Lines]",
      "293:                 classifier_file = model_dir / f\"{file_name}_classifier.pkl\"",
      "296:                     classifier = io_utils.json_unpickle(classifier_file)",
      "298:                     encoder_file = model_dir / f\"{file_name}_encoder.pkl\"",
      "299:                     classes = io_utils.json_unpickle(encoder_file)",
      "300:                     encoder = LabelEncoder()",
      "301:                     encoder.classes_ = classes",
      "303:                     return cls(config, model_storage, resource, classifier, encoder)",
      "",
      "[Added Lines]",
      "295:         import skops.io as sio",
      "300:                 classifier_file = model_dir / f\"{file_name}_classifier.skops\"",
      "303:                     unknown_types = sio.get_untrusted_types(file=classifier_file)",
      "305:                     if unknown_types:",
      "306:                         logger.error(",
      "307:                             f\"Untrusted types ({unknown_types}) found when \"",
      "308:                             f\"loading {classifier_file}!\"",
      "309:                         )",
      "310:                         raise ValueError()",
      "311:                     else:",
      "312:                         classifier = sio.load(classifier_file, trusted=unknown_types)",
      "314:                     encoder_file = model_dir / f\"{file_name}_encoder.json\"",
      "315:                     classes = rasa.shared.utils.io.read_json_file(encoder_file)",
      "317:                     encoder = LabelEncoder()",
      "318:                     intent_classifier = cls(",
      "319:                         config, model_storage, resource, classifier, encoder",
      "320:                     )",
      "321:                     # convert list of strings (class labels) back to numpy array of",
      "322:                     # strings",
      "323:                     intent_classifier.transform_labels_str2num(classes)",
      "324:                     return intent_classifier",
      "",
      "---------------"
    ],
    "rasa/nlu/extractors/crf_entity_extractor.py||rasa/nlu/extractors/crf_entity_extractor.py": [
      "File: rasa/nlu/extractors/crf_entity_extractor.py -> rasa/nlu/extractors/crf_entity_extractor.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "1: from __future__ import annotations",
      "5: import logging",
      "6: import typing",
      "8: import numpy as np",
      "11: import rasa.nlu.utils.bilou_utils as bilou_utils",
      "12: import rasa.shared.utils.io",
      "",
      "[Removed Lines]",
      "3: from collections import OrderedDict",
      "4: from enum import Enum",
      "9: from typing import Any, Dict, List, Optional, Text, Tuple, Callable, Type",
      "",
      "[Added Lines]",
      "5: from collections import OrderedDict",
      "6: from enum import Enum",
      "7: from typing import Any, Dict, List, Optional, Text, Tuple, Callable, Type",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "15: from rasa.engine.recipes.default_recipe import DefaultV1Recipe",
      "16: from rasa.engine.storage.resource import Resource",
      "17: from rasa.engine.storage.storage import ModelStorage",
      "18: from rasa.nlu.test import determine_token_labels",
      "19: from rasa.nlu.tokenizers.spacy_tokenizer import POS_TAG_KEY",
      "21: from rasa.nlu.tokenizers.tokenizer import Token, Tokenizer",
      "25: from rasa.shared.nlu.constants import (",
      "26:     TEXT,",
      "27:     ENTITIES,",
      "",
      "[Removed Lines]",
      "20: from rasa.nlu.extractors.extractor import EntityExtractorMixin",
      "22: from rasa.shared.nlu.training_data.training_data import TrainingData",
      "23: from rasa.shared.nlu.training_data.message import Message",
      "24: from rasa.nlu.constants import TOKENS_NAMES",
      "",
      "[Added Lines]",
      "18: from rasa.nlu.constants import TOKENS_NAMES",
      "19: from rasa.nlu.extractors.extractor import EntityExtractorMixin",
      "23: from rasa.shared.constants import DOCS_URL_COMPONENTS",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "32:     SPLIT_ENTITIES_BY_COMMA,",
      "33:     SPLIT_ENTITIES_BY_COMMA_DEFAULT_VALUE,",
      "34: )",
      "36: from rasa.utils.tensorflow.constants import BILOU_FLAG, FEATURIZERS",
      "38: logger = logging.getLogger(__name__)",
      "",
      "[Removed Lines]",
      "35: from rasa.shared.constants import DOCS_URL_COMPONENTS",
      "",
      "[Added Lines]",
      "34: from rasa.shared.nlu.training_data.message import Message",
      "35: from rasa.shared.nlu.training_data.training_data import TrainingData",
      "",
      "---------------",
      "--- Hunk 4 ---",
      "[Context before]",
      "41:     from sklearn_crfsuite import CRF",
      "44: class CRFToken:",
      "45:     def __init__(",
      "46:         self,",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "44: CONFIG_FEATURES = \"features\"",
      "",
      "---------------",
      "--- Hunk 5 ---",
      "[Context before]",
      "60:         self.entity_role_tag = entity_role_tag",
      "61:         self.entity_group_tag = entity_group_tag",
      "64: class CRFEntityExtractorOptions(str, Enum):",
      "65:     \"\"\"Features that can be used for the 'CRFEntityExtractor'.\"\"\"",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "66:     def to_dict(self) -> Dict[str, Any]:",
      "67:         return {",
      "68:             \"text\": self.text,",
      "69:             \"pos_tag\": self.pos_tag,",
      "70:             \"pattern\": self.pattern,",
      "71:             \"dense_features\": [str(x) for x in list(self.dense_features)],",
      "72:             \"entity_tag\": self.entity_tag,",
      "73:             \"entity_role_tag\": self.entity_role_tag,",
      "74:             \"entity_group_tag\": self.entity_group_tag,",
      "75:         }",
      "77:     @classmethod",
      "78:     def create_from_dict(cls, data: Dict[str, Any]) -> \"CRFToken\":",
      "79:         return cls(",
      "80:             data[\"text\"],",
      "81:             data[\"pos_tag\"],",
      "82:             data[\"pattern\"],",
      "83:             np.array([float(x) for x in data[\"dense_features\"]]),",
      "84:             data[\"entity_tag\"],",
      "85:             data[\"entity_role_tag\"],",
      "86:             data[\"entity_group_tag\"],",
      "87:         )",
      "",
      "---------------",
      "--- Hunk 6 ---",
      "[Context before]",
      "137:             # \"is the preceding token in title case?\"",
      "138:             # POS features require SpacyTokenizer",
      "139:             # pattern feature require RegexFeaturizer",
      "141:                 [",
      "142:                     CRFEntityExtractorOptions.LOW,",
      "143:                     CRFEntityExtractorOptions.TITLE,",
      "",
      "[Removed Lines]",
      "140:             CRFEntityExtractor.CONFIG_FEATURES: [",
      "",
      "[Added Lines]",
      "166:             CONFIG_FEATURES: [",
      "",
      "---------------",
      "--- Hunk 7 ---",
      "[Context before]",
      "200:         )",
      "202:     def _validate_configuration(self) -> None:",
      "204:             raise ValueError(",
      "205:                 \"Need an odd number of crf feature lists to have a center word.\"",
      "206:             )",
      "",
      "[Removed Lines]",
      "203:         if len(self.component_config.get(self.CONFIG_FEATURES, [])) % 2 != 1:",
      "",
      "[Added Lines]",
      "229:         if len(self.component_config.get(CONFIG_FEATURES, [])) % 2 != 1:",
      "",
      "---------------",
      "--- Hunk 8 ---",
      "[Context before]",
      "251:         ]",
      "252:         dataset = [self._convert_to_crf_tokens(example) for example in entity_examples]",
      "258:         return self._resource",
      "",
      "[Removed Lines]",
      "254:         self._train_model(dataset)",
      "256:         self.persist()",
      "",
      "[Added Lines]",
      "280:         self.entity_taggers = self.train_model(",
      "281:             dataset, self.component_config, self.crf_order",
      "282:         )",
      "284:         self.persist(dataset)",
      "",
      "---------------",
      "--- Hunk 9 ---",
      "[Context before]",
      "299:             if include_tag_features:",
      "300:                 self._add_tag_to_crf_token(crf_tokens, predictions)",
      "303:             predictions[tag_name] = entity_tagger.predict_marginals_single(features)",
      "305:         # convert predictions into a list of tags and a list of confidences",
      "",
      "[Removed Lines]",
      "302:             features = self._crf_tokens_to_features(crf_tokens, include_tag_features)",
      "",
      "[Added Lines]",
      "330:             features = self._crf_tokens_to_features(",
      "331:                 crf_tokens, self.component_config, include_tag_features",
      "332:             )",
      "",
      "---------------",
      "--- Hunk 10 ---",
      "[Context before]",
      "390:     ) -> CRFEntityExtractor:",
      "391:         \"\"\"Loads trained component (see parent class for full docstring).\"\"\"",
      "394:         try:",
      "396:             with model_storage.read_from(resource) as model_dir:",
      "413:         except ValueError:",
      "414:             logger.warning(",
      "415:                 f\"Failed to load {cls.__name__} from model storage. Resource \"",
      "",
      "[Removed Lines]",
      "392:         import joblib",
      "395:             entity_taggers = OrderedDict()",
      "397:                 # We have to load in the same order as we persisted things as otherwise",
      "398:                 # the predictions might be off",
      "399:                 file_names = sorted(model_dir.glob(\"**/*.pkl\"))",
      "400:                 if not file_names:",
      "401:                     logger.debug(",
      "402:                         \"Failed to load model for 'CRFEntityExtractor'. \"",
      "403:                         \"Maybe you did not provide enough training data and \"",
      "404:                         \"no model was trained.\"",
      "405:                     )",
      "406:                     return cls(config, model_storage, resource)",
      "408:                 for file_name in file_names:",
      "409:                     name = file_name.stem[1:]",
      "410:                     entity_taggers[name] = joblib.load(file_name)",
      "412:                 return cls(config, model_storage, resource, entity_taggers)",
      "",
      "[Added Lines]",
      "424:                 dataset = rasa.shared.utils.io.read_json_file(",
      "425:                     model_dir / \"crf_dataset.json\"",
      "426:                 )",
      "427:                 crf_order = rasa.shared.utils.io.read_json_file(",
      "428:                     model_dir / \"crf_order.json\"",
      "429:                 )",
      "431:                 dataset = [",
      "432:                     [CRFToken.create_from_dict(token_data) for token_data in sub_list]",
      "433:                     for sub_list in dataset",
      "434:                 ]",
      "436:                 entity_taggers = cls.train_model(dataset, config, crf_order)",
      "438:                 entity_extractor = cls(config, model_storage, resource, entity_taggers)",
      "439:                 entity_extractor.crf_order = crf_order",
      "440:                 return entity_extractor",
      "",
      "---------------",
      "--- Hunk 11 ---",
      "[Context before]",
      "417:             )",
      "418:             return cls(config, model_storage, resource)",
      "421:         \"\"\"Persist this model into the passed directory.\"\"\"",
      "424:         with self._model_storage.write_to(self._resource) as model_dir:",
      "432:     def _crf_tokens_to_features(",
      "434:     ) -> List[Dict[Text, Any]]:",
      "435:         \"\"\"Convert the list of tokens into discrete features.\"\"\"",
      "437:         sentence_features = []",
      "439:         for token_idx in range(len(crf_tokens)):",
      "",
      "[Removed Lines]",
      "420:     def persist(self) -> None:",
      "422:         import joblib",
      "425:             if self.entity_taggers:",
      "426:                 for idx, (name, entity_tagger) in enumerate(",
      "427:                     self.entity_taggers.items()",
      "428:                 ):",
      "429:                     model_file_name = model_dir / f\"{idx}{name}.pkl\"",
      "430:                     joblib.dump(entity_tagger, model_file_name)",
      "433:         self, crf_tokens: List[CRFToken], include_tag_features: bool = False",
      "436:         configured_features = self.component_config[self.CONFIG_FEATURES]",
      "",
      "[Added Lines]",
      "448:     def persist(self, dataset: List[List[CRFToken]]) -> None:",
      "451:             data_to_store = [",
      "452:                 [token.to_dict() for token in sub_list] for sub_list in dataset",
      "453:             ]",
      "455:             rasa.shared.utils.io.dump_obj_as_json_to_file(",
      "456:                 model_dir / \"crf_dataset.json\", data_to_store",
      "457:             )",
      "458:             rasa.shared.utils.io.dump_obj_as_json_to_file(",
      "459:                 model_dir / \"crf_order.json\", self.crf_order",
      "460:             )",
      "462:     @classmethod",
      "464:         cls,",
      "465:         crf_tokens: List[CRFToken],",
      "466:         config: Dict[str, Any],",
      "467:         include_tag_features: bool = False,",
      "470:         configured_features = config[CONFIG_FEATURES]",
      "",
      "---------------",
      "--- Hunk 12 ---",
      "[Context before]",
      "444:             half_window_size = window_size // 2",
      "445:             window_range = range(-half_window_size, half_window_size + 1)",
      "448:                 crf_tokens,",
      "449:                 token_idx,",
      "450:                 half_window_size,",
      "451:                 window_range,",
      "452:                 include_tag_features,",
      "453:             )",
      "455:             sentence_features.append(token_features)",
      "457:         return sentence_features",
      "459:     def _create_features_for_token(",
      "461:         crf_tokens: List[CRFToken],",
      "462:         token_idx: int,",
      "463:         half_window_size: int,",
      "464:         window_range: range,",
      "465:         include_tag_features: bool,",
      "466:     ) -> Dict[Text, Any]:",
      "467:         \"\"\"Convert a token into discrete features including words before and after.\"\"\"",
      "469:         prefixes = [str(i) for i in window_range]",
      "471:         token_features = {}",
      "",
      "[Removed Lines]",
      "447:             token_features = self._create_features_for_token(",
      "460:         self,",
      "468:         configured_features = self.component_config[self.CONFIG_FEATURES]",
      "",
      "[Added Lines]",
      "481:             token_features = cls._create_features_for_token(",
      "487:                 config,",
      "494:     @classmethod",
      "496:         cls,",
      "502:         config: Dict[str, Any],",
      "505:         configured_features = config[CONFIG_FEATURES]",
      "",
      "---------------",
      "--- Hunk 13 ---",
      "[Context before]",
      "505:                         # set in the training data, 'matched' is either 'True' or",
      "506:                         # 'False' depending on whether the token actually matches the",
      "507:                         # pattern or not",
      "509:                         for pattern_name, matched in regex_patterns.items():",
      "510:                             token_features[",
      "511:                                 f\"{prefix}:{feature}:{pattern_name}\"",
      "512:                             ] = matched",
      "513:                     else:",
      "515:                         token_features[f\"{prefix}:{feature}\"] = value",
      "517:         return token_features",
      "",
      "[Removed Lines]",
      "508:                         regex_patterns = self.function_dict[feature](token)",
      "514:                         value = self.function_dict[feature](token)",
      "",
      "[Added Lines]",
      "545:                         regex_patterns = cls.function_dict[feature](token)",
      "551:                         value = cls.function_dict[feature](token)",
      "",
      "---------------",
      "--- Hunk 14 ---",
      "[Context before]",
      "636:         return tags",
      "639:         \"\"\"Train the crf tagger based on the training data.\"\"\"",
      "640:         import sklearn_crfsuite",
      "645:             logger.debug(f\"Training CRF for '{tag_name}'.\")",
      "647:             # add entity tag features for second level CRFs",
      "648:             include_tag_features = tag_name != ENTITY_ATTRIBUTE_TYPE",
      "649:             X_train = (",
      "651:                 for sentence in df_train",
      "652:             )",
      "653:             y_train = (",
      "655:             )",
      "657:             entity_tagger = sklearn_crfsuite.CRF(",
      "658:                 algorithm=\"lbfgs\",",
      "659:                 # coefficient for L1 penalty",
      "661:                 # coefficient for L2 penalty",
      "663:                 # stop earlier",
      "665:                 # include transitions that are possible, but not observed",
      "666:                 all_possible_transitions=True,",
      "667:             )",
      "668:             entity_tagger.fit(X_train, y_train)",
      "672:             logger.debug(\"Training finished.\")",
      "",
      "[Removed Lines]",
      "638:     def _train_model(self, df_train: List[List[CRFToken]]) -> None:",
      "642:         self.entity_taggers = OrderedDict()",
      "644:         for tag_name in self.crf_order:",
      "650:                 self._crf_tokens_to_features(sentence, include_tag_features)",
      "654:                 self._crf_tokens_to_tags(sentence, tag_name) for sentence in df_train",
      "660:                 c1=self.component_config[\"L1_c\"],",
      "662:                 c2=self.component_config[\"L2_c\"],",
      "664:                 max_iterations=self.component_config[\"max_iterations\"],",
      "670:             self.entity_taggers[tag_name] = entity_tagger",
      "",
      "[Added Lines]",
      "675:     @classmethod",
      "676:     def train_model(",
      "677:         cls,",
      "678:         df_train: List[List[CRFToken]],",
      "679:         config: Dict[str, Any],",
      "680:         crf_order: List[str],",
      "681:     ) -> OrderedDict[str, CRF]:",
      "685:         entity_taggers = OrderedDict()",
      "687:         for tag_name in crf_order:",
      "693:                 cls._crf_tokens_to_features(sentence, config, include_tag_features)",
      "697:                 cls._crf_tokens_to_tags(sentence, tag_name) for sentence in df_train",
      "703:                 c1=config[\"L1_c\"],",
      "705:                 c2=config[\"L2_c\"],",
      "707:                 max_iterations=config[\"max_iterations\"],",
      "713:             entity_taggers[tag_name] = entity_tagger",
      "717:         return entity_taggers",
      "",
      "---------------"
    ],
    "rasa/nlu/featurizers/sparse_featurizer/count_vectors_featurizer.py||rasa/nlu/featurizers/sparse_featurizer/count_vectors_featurizer.py": [
      "File: rasa/nlu/featurizers/sparse_featurizer/count_vectors_featurizer.py -> rasa/nlu/featurizers/sparse_featurizer/count_vectors_featurizer.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "1: from __future__ import annotations",
      "2: import logging",
      "3: import re",
      "4: import scipy.sparse",
      "8: import rasa.shared.utils.io",
      "9: from rasa.engine.graph import GraphComponent, ExecutionContext",
      "10: from rasa.engine.recipes.default_recipe import DefaultV1Recipe",
      "11: from rasa.engine.storage.resource import Resource",
      "12: from rasa.engine.storage.storage import ModelStorage",
      "21: from rasa.nlu.constants import (",
      "22:     TOKENS_NAMES,",
      "23:     MESSAGE_ATTRIBUTES,",
      "24:     DENSE_FEATURIZABLE_ATTRIBUTES,",
      "25: )",
      "26: from rasa.shared.nlu.constants import TEXT, INTENT, INTENT_RESPONSE_KEY, ACTION_NAME",
      "28: BUFFER_SLOTS_PREFIX = \"buf_\"",
      "",
      "[Removed Lines]",
      "5: from typing import Any, Dict, List, Optional, Text, Tuple, Set, Type",
      "6: from rasa.nlu.tokenizers.tokenizer import Tokenizer",
      "13: from rasa.nlu.featurizers.sparse_featurizer.sparse_featurizer import SparseFeaturizer",
      "14: from rasa.nlu.utils.spacy_utils import SpacyModel",
      "15: from rasa.shared.constants import DOCS_URL_COMPONENTS",
      "16: import rasa.utils.io as io_utils",
      "17: from sklearn.feature_extraction.text import CountVectorizer",
      "18: from rasa.shared.nlu.training_data.training_data import TrainingData",
      "19: from rasa.shared.nlu.training_data.message import Message",
      "20: from rasa.shared.exceptions import RasaException, FileIOException",
      "",
      "[Added Lines]",
      "5: from typing import Any, Dict, List, Optional, Text, Tuple, Set, Type, Union",
      "7: import numpy as np",
      "9: from sklearn.feature_extraction.text import CountVectorizer",
      "21: from rasa.nlu.featurizers.sparse_featurizer.sparse_featurizer import SparseFeaturizer",
      "22: from rasa.nlu.tokenizers.tokenizer import Tokenizer",
      "23: from rasa.nlu.utils.spacy_utils import SpacyModel",
      "24: from rasa.shared.constants import DOCS_URL_COMPONENTS",
      "25: from rasa.shared.exceptions import RasaException, FileIOException",
      "27: from rasa.shared.nlu.training_data.message import Message",
      "28: from rasa.shared.nlu.training_data.training_data import TrainingData",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "686:         \"\"\"Check if any model got trained.\"\"\"",
      "687:         return any(value is not None for value in attribute_vocabularies.values())",
      "689:     def persist(self) -> None:",
      "690:         \"\"\"Persist this model into the passed directory.",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "691:     @staticmethod",
      "692:     def convert_vocab(",
      "693:         vocab: Dict[str, Union[int, Optional[Dict[str, int]]]], to_int: bool",
      "694:     ) -> Dict[str, Union[None, int, np.int64, Dict[str, Union[int, np.int64]]]]:",
      "695:         \"\"\"Converts numpy integers in the vocabulary to Python integers.\"\"\"",
      "697:         def convert_value(value: int) -> Union[int, np.int64]:",
      "698:             \"\"\"Helper function to convert a single value based on to_int flag.\"\"\"",
      "699:             return int(value) if to_int else np.int64(value)",
      "701:         result_dict: Dict[",
      "702:             str, Union[None, int, np.int64, Dict[str, Union[int, np.int64]]]",
      "703:         ] = {}",
      "704:         for key, sub_dict in vocab.items():",
      "705:             if isinstance(sub_dict, int):",
      "706:                 result_dict[key] = convert_value(sub_dict)",
      "707:             elif not sub_dict:",
      "708:                 result_dict[key] = None",
      "709:             else:",
      "710:                 result_dict[key] = {",
      "711:                     sub_key: convert_value(value) for sub_key, value in sub_dict.items()",
      "712:                 }",
      "714:         return result_dict",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "699:             attribute_vocabularies = self._collect_vectorizer_vocabularies()",
      "700:             if self._is_any_model_trained(attribute_vocabularies):",
      "701:                 # Definitely need to persist some vocabularies",
      "704:                 # Only persist vocabulary from one attribute if `use_shared_vocab`.",
      "705:                 # Can be loaded and distributed to all attributes.",
      "707:                     attribute_vocabularies[TEXT]",
      "708:                     if self.use_shared_vocab",
      "709:                     else attribute_vocabularies",
      "710:                 )",
      "714:                 # Dump OOV words separately as they might have been modified during",
      "715:                 # training",
      "",
      "[Removed Lines]",
      "702:                 featurizer_file = model_dir / \"vocabularies.pkl\"",
      "706:                 vocab = (",
      "712:                 io_utils.json_pickle(featurizer_file, vocab)",
      "",
      "[Added Lines]",
      "729:                 featurizer_file = model_dir / \"vocabularies.json\"",
      "733:                 loaded_vocab = (",
      "738:                 vocab = self.convert_vocab(loaded_vocab, to_int=True)",
      "740:                 rasa.shared.utils.io.dump_obj_as_json_to_file(featurizer_file, vocab)",
      "",
      "---------------",
      "--- Hunk 4 ---",
      "[Context before]",
      "784:         \"\"\"Loads trained component (see parent class for full docstring).\"\"\"",
      "785:         try:",
      "786:             with model_storage.read_from(resource) as model_dir:",
      "790:                 share_vocabulary = config[\"use_shared_vocab\"]",
      "",
      "[Removed Lines]",
      "787:                 featurizer_file = model_dir / \"vocabularies.pkl\"",
      "788:                 vocabulary = io_utils.json_unpickle(featurizer_file)",
      "",
      "[Added Lines]",
      "815:                 featurizer_file = model_dir / \"vocabularies.json\"",
      "816:                 vocabulary = rasa.shared.utils.io.read_json_file(featurizer_file)",
      "817:                 vocabulary = cls.convert_vocab(vocabulary, to_int=False)",
      "",
      "---------------"
    ],
    "rasa/nlu/featurizers/sparse_featurizer/lexical_syntactic_featurizer.py||rasa/nlu/featurizers/sparse_featurizer/lexical_syntactic_featurizer.py": [
      "File: rasa/nlu/featurizers/sparse_featurizer/lexical_syntactic_featurizer.py -> rasa/nlu/featurizers/sparse_featurizer/lexical_syntactic_featurizer.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "1: from __future__ import annotations",
      "2: import logging",
      "3: from collections import OrderedDict",
      "7: from typing import (",
      "8:     Any,",
      "9:     Dict,",
      "",
      "[Removed Lines]",
      "5: import scipy.sparse",
      "6: import numpy as np",
      "",
      "[Added Lines]",
      "[None]",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "17:     Union,",
      "18: )",
      "20: from rasa.engine.graph import ExecutionContext, GraphComponent",
      "21: from rasa.engine.recipes.default_recipe import DefaultV1Recipe",
      "22: from rasa.engine.storage.resource import Resource",
      "23: from rasa.engine.storage.storage import ModelStorage",
      "24: from rasa.nlu.tokenizers.spacy_tokenizer import POS_TAG_KEY, SpacyTokenizer",
      "25: from rasa.nlu.tokenizers.tokenizer import Token, Tokenizer",
      "28: from rasa.shared.constants import DOCS_URL_COMPONENTS",
      "32: from rasa.shared.exceptions import InvalidConfigException",
      "36: logger = logging.getLogger(__name__)",
      "39: END_OF_SENTENCE = \"EOS\"",
      "40: BEGIN_OF_SENTENCE = \"BOS\"",
      "42: FEATURES = \"features\"",
      "45: @DefaultV1Recipe.register(",
      "46:     DefaultV1Recipe.ComponentType.MESSAGE_FEATURIZER, is_trainable=True",
      "",
      "[Removed Lines]",
      "26: from rasa.nlu.featurizers.sparse_featurizer.sparse_featurizer import SparseFeaturizer",
      "27: from rasa.nlu.constants import TOKENS_NAMES",
      "29: from rasa.shared.nlu.training_data.training_data import TrainingData",
      "30: from rasa.shared.nlu.training_data.message import Message",
      "31: from rasa.shared.nlu.constants import TEXT",
      "33: import rasa.shared.utils.io",
      "34: import rasa.utils.io",
      "",
      "[Added Lines]",
      "18: import numpy as np",
      "19: import scipy.sparse",
      "21: import rasa.shared.utils.io",
      "22: import rasa.utils.io",
      "27: from rasa.nlu.constants import TOKENS_NAMES",
      "28: from rasa.nlu.featurizers.sparse_featurizer.sparse_featurizer import SparseFeaturizer",
      "33: from rasa.shared.nlu.constants import TEXT",
      "34: from rasa.shared.nlu.training_data.message import Message",
      "35: from rasa.shared.nlu.training_data.training_data import TrainingData",
      "44: SEPERATOR = \"###\"",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "72:       of the token at position `t+1`.",
      "73:     \"\"\"",
      "77:     # NOTE: \"suffix5\" of the token \"is\" will be \"is\". Hence, when combining multiple",
      "78:     # prefixes, short words will be represented/encoded repeatedly.",
      "",
      "[Removed Lines]",
      "75:     FILENAME_FEATURE_TO_IDX_DICT = \"feature_to_idx_dict.pkl\"",
      "",
      "[Added Lines]",
      "77:     FILENAME_FEATURE_TO_IDX_DICT = \"feature_to_idx_dict.json\"",
      "",
      "---------------",
      "--- Hunk 4 ---",
      "[Context before]",
      "489:         \"\"\"Creates a new untrained component (see parent class for full docstring).\"\"\"",
      "490:         return cls(config, model_storage, resource, execution_context)",
      "492:     @classmethod",
      "493:     def load(",
      "494:         cls,",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "494:     @staticmethod",
      "495:     def _restructure_feature_to_idx_dict(",
      "496:         loaded_data: Dict[str, Dict[str, int]],",
      "497:     ) -> Dict[Tuple[int, str], Dict[str, int]]:",
      "498:         \"\"\"Reconstructs the feature to idx dict.",
      "500:         When storing the feature_to_idx_dict to disk, we need to convert the tuple (key)",
      "501:         into a string to be able to store it via json. When loading the data",
      "502:         we need to reconstruct the tuple from the stored string.",
      "504:         Args:",
      "505:             loaded_data: The loaded feature to idx dict from file.",
      "507:         Returns:",
      "508:             The reconstructed feature_to_idx_dict",
      "509:         \"\"\"",
      "510:         feature_to_idx_dict = {}",
      "511:         for tuple_string, feature_value in loaded_data.items():",
      "512:             # Example of tuple_string: \"1###low\"",
      "513:             index, feature_name = tuple_string.split(SEPERATOR)",
      "515:             feature_key = (int(index), feature_name)",
      "516:             feature_to_idx_dict[feature_key] = feature_value",
      "518:         return feature_to_idx_dict",
      "",
      "---------------",
      "--- Hunk 5 ---",
      "[Context before]",
      "501:         \"\"\"Loads trained component (see parent class for full docstring).\"\"\"",
      "502:         try:",
      "503:             with model_storage.read_from(resource) as model_path:",
      "505:                     model_path / cls.FILENAME_FEATURE_TO_IDX_DICT,",
      "507:                 )",
      "508:                 return cls(",
      "509:                     config=config,",
      "510:                     model_storage=model_storage,",
      "",
      "[Removed Lines]",
      "504:                 feature_to_idx_dict = rasa.utils.io.json_unpickle(",
      "506:                     encode_non_string_keys=True,",
      "",
      "[Added Lines]",
      "532:                 loaded_data = rasa.shared.utils.io.read_json_file(",
      "536:                 # convert the key back into tuple",
      "537:                 feature_to_idx_dict = cls._restructure_feature_to_idx_dict(loaded_data)",
      "",
      "---------------",
      "--- Hunk 6 ---",
      "[Context before]",
      "529:         if not self._feature_to_idx_dict:",
      "530:             return None",
      "532:         with self._model_storage.write_to(self._resource) as model_path:",
      "534:                 model_path / self.FILENAME_FEATURE_TO_IDX_DICT,",
      "537:             )",
      "",
      "[Removed Lines]",
      "533:             rasa.utils.io.json_pickle(",
      "535:                 self._feature_to_idx_dict,",
      "536:                 encode_non_string_keys=True,",
      "",
      "[Added Lines]",
      "563:         # as we cannot dump tuples, convert the tuple into a string",
      "564:         restructured_feature_dict = {",
      "565:             f\"{k[0]}{SEPERATOR}{k[1]}\": v for k, v in self._feature_to_idx_dict.items()",
      "566:         }",
      "569:             rasa.shared.utils.io.dump_obj_as_json_to_file(",
      "571:                 restructured_feature_dict,",
      "",
      "---------------"
    ],
    "rasa/nlu/featurizers/sparse_featurizer/regex_featurizer.py||rasa/nlu/featurizers/sparse_featurizer/regex_featurizer.py": [
      "File: rasa/nlu/featurizers/sparse_featurizer/regex_featurizer.py -> rasa/nlu/featurizers/sparse_featurizer/regex_featurizer.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "1: from __future__ import annotations",
      "2: import logging",
      "3: import re",
      "4: from typing import Any, Dict, List, Optional, Text, Tuple, Type",
      "5: import numpy as np",
      "6: import scipy.sparse",
      "9: import rasa.shared.utils.io",
      "10: import rasa.utils.io",
      "11: import rasa.nlu.utils.pattern_utils as pattern_utils",
      "",
      "[Removed Lines]",
      "7: from rasa.nlu.tokenizers.tokenizer import Tokenizer",
      "",
      "[Added Lines]",
      "10: from rasa.nlu.tokenizers.tokenizer import Tokenizer",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "241:         try:",
      "242:             with model_storage.read_from(resource) as model_dir:",
      "244:                 known_patterns = rasa.shared.utils.io.read_json_file(patterns_file_name)",
      "245:         except (ValueError, FileNotFoundError):",
      "246:             logger.warning(",
      "",
      "[Removed Lines]",
      "243:                 patterns_file_name = model_dir / \"patterns.pkl\"",
      "",
      "[Added Lines]",
      "245:                 patterns_file_name = model_dir / \"patterns.json\"",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "259:     def _persist(self) -> None:",
      "260:         with self._model_storage.write_to(self._resource) as model_dir:",
      "262:             rasa.shared.utils.io.dump_obj_as_json_to_file(",
      "263:                 regex_file, self.known_patterns",
      "264:             )",
      "",
      "[Removed Lines]",
      "261:             regex_file = model_dir / \"patterns.pkl\"",
      "",
      "[Added Lines]",
      "263:             regex_file = model_dir / \"patterns.json\"",
      "",
      "---------------"
    ],
    "rasa/shared/nlu/training_data/features.py||rasa/shared/nlu/training_data/features.py": [
      "File: rasa/shared/nlu/training_data/features.py -> rasa/shared/nlu/training_data/features.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "1: from __future__ import annotations",
      "3: import itertools",
      "5: import numpy as np",
      "6: import scipy.sparse",
      "9: import rasa.shared.nlu.training_data.util",
      "10: from rasa.shared.nlu.constants import FEATURE_TYPE_SEQUENCE, FEATURE_TYPE_SENTENCE",
      "13: class Features:",
      "14:     \"\"\"Stores the features produced by any featurizer.\"\"\"",
      "",
      "[Removed Lines]",
      "2: from typing import Iterable, Union, Text, Optional, List, Any, Tuple, Dict, Set",
      "8: import rasa.shared.utils.io",
      "",
      "[Added Lines]",
      "4: from dataclasses import dataclass",
      "5: from typing import Iterable, Union, Text, Optional, List, Any, Tuple, Dict, Set",
      "9: from safetensors.numpy import save_file, load_file",
      "12: import rasa.shared.utils.io",
      "16: @dataclass",
      "17: class FeatureMetadata:",
      "18:     data_type: str",
      "19:     attribute: str",
      "20:     origin: Union[str, List[str]]",
      "21:     is_sparse: bool",
      "22:     shape: tuple",
      "23:     safetensors_key: str",
      "26: def save_features(",
      "27:     features_dict: Dict[Text, List[Features]], file_name: str",
      "28: ) -> Dict[str, Any]:",
      "29:     \"\"\"Save a dictionary of Features lists to disk using safetensors.",
      "31:     Args:",
      "32:         features_dict: Dictionary mapping strings to lists of Features objects",
      "33:         file_name: File to save the features to",
      "35:     Returns:",
      "36:         The metadata to reconstruct the features.",
      "37:     \"\"\"",
      "38:     # All tensors are stored in a single safetensors file",
      "39:     tensors_to_save = {}",
      "40:     # Metadata will be stored separately",
      "41:     metadata = {}",
      "43:     for key, features_list in features_dict.items():",
      "44:         feature_metadata_list = []",
      "46:         for idx, feature in enumerate(features_list):",
      "47:             # Create a unique key for this tensor in the safetensors file",
      "48:             safetensors_key = f\"{key}_{idx}\"",
      "50:             # Convert sparse matrices to dense if needed",
      "51:             if feature.is_sparse():",
      "52:                 # For sparse matrices, use the COO format",
      "53:                 coo = feature.features.tocoo()  # type:ignore[union-attr]",
      "54:                 # Save data, row indices and col indices separately",
      "55:                 tensors_to_save[f\"{safetensors_key}_data\"] = coo.data",
      "56:                 tensors_to_save[f\"{safetensors_key}_row\"] = coo.row",
      "57:                 tensors_to_save[f\"{safetensors_key}_col\"] = coo.col",
      "58:             else:",
      "59:                 tensors_to_save[safetensors_key] = feature.features",
      "61:             # Store metadata",
      "62:             metadata_item = FeatureMetadata(",
      "63:                 data_type=feature.type,",
      "64:                 attribute=feature.attribute,",
      "65:                 origin=feature.origin,",
      "66:                 is_sparse=feature.is_sparse(),",
      "67:                 shape=feature.features.shape,",
      "68:                 safetensors_key=safetensors_key,",
      "69:             )",
      "70:             feature_metadata_list.append(vars(metadata_item))",
      "72:         metadata[key] = feature_metadata_list",
      "74:     # Save tensors",
      "75:     save_file(tensors_to_save, file_name)",
      "77:     return metadata",
      "80: def load_features(",
      "81:     filename: str, metadata: Dict[str, Any]",
      "82: ) -> Dict[Text, List[Features]]:",
      "83:     \"\"\"Load Features dictionary from disk.",
      "85:     Args:",
      "86:         filename: File name of the safetensors file.",
      "87:         metadata: Metadata to reconstruct the features.",
      "89:     Returns:",
      "90:         Dictionary mapping strings to lists of Features objects",
      "91:     \"\"\"",
      "92:     # Load tensors",
      "93:     tensors = load_file(filename)",
      "95:     # Reconstruct the features dictionary",
      "96:     features_dict: Dict[Text, List[Features]] = {}",
      "98:     for key, feature_metadata_list in metadata.items():",
      "99:         features_list = []",
      "101:         for meta in feature_metadata_list:",
      "102:             safetensors_key = meta[\"safetensors_key\"]",
      "104:             if meta[\"is_sparse\"]:",
      "105:                 # Reconstruct sparse matrix from COO format",
      "106:                 data = tensors[f\"{safetensors_key}_data\"]",
      "107:                 row = tensors[f\"{safetensors_key}_row\"]",
      "108:                 col = tensors[f\"{safetensors_key}_col\"]",
      "110:                 features_matrix = scipy.sparse.coo_matrix(",
      "111:                     (data, (row, col)), shape=tuple(meta[\"shape\"])",
      "112:                 ).tocsr()  # Convert back to CSR format",
      "113:             else:",
      "114:                 features_matrix = tensors[safetensors_key]",
      "116:             # Reconstruct Features object",
      "117:             features = Features(",
      "118:                 features=features_matrix,",
      "119:                 feature_type=meta[\"data_type\"],",
      "120:                 attribute=meta[\"attribute\"],",
      "121:                 origin=meta[\"origin\"],",
      "122:             )",
      "124:             features_list.append(features)",
      "126:         features_dict[key] = features_list",
      "128:     return features_dict",
      "",
      "---------------"
    ],
    "rasa/shared/utils/io.py||rasa/shared/utils/io.py": [
      "File: rasa/shared/utils/io.py -> rasa/shared/utils/io.py"
    ],
    "rasa/utils/common.py||rasa/utils/common.py": [
      "File: rasa/utils/common.py -> rasa/utils/common.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "8: import tempfile",
      "9: import warnings",
      "10: from pathlib import Path",
      "11: from types import TracebackType",
      "12: from typing import (",
      "13:     Any,",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "11: from socket import SOCK_DGRAM, SOCK_STREAM",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "24:     Tuple,",
      "25: )",
      "28: import numpy as np",
      "29: import rasa.utils.io",
      "30: from rasa.constants import (",
      "31:     DEFAULT_LOG_LEVEL_LIBRARIES,",
      "",
      "[Removed Lines]",
      "27: from socket import SOCK_DGRAM, SOCK_STREAM",
      "",
      "[Added Lines]",
      "30: import rasa.shared.utils.io",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "36: )",
      "37: from rasa.shared.constants import DEFAULT_LOG_LEVEL, ENV_LOG_LEVEL, TCP_PROTOCOL",
      "38: from rasa.shared.exceptions import RasaException",
      "41: logger = logging.getLogger(__name__)",
      "",
      "[Removed Lines]",
      "39: import rasa.shared.utils.io",
      "",
      "[Added Lines]",
      "[None]",
      "",
      "---------------",
      "--- Hunk 4 ---",
      "[Context before]",
      "153:     try:",
      "154:         logging.config.dictConfig(logging_config_dict)",
      "155:     except (ValueError, TypeError, AttributeError, ImportError) as e:",
      "157:             f\"The logging config file {logging_config_file} could not \"",
      "158:             f\"be applied because it failed validation against \"",
      "159:             f\"the built-in Python logging schema. \"",
      "",
      "[Removed Lines]",
      "156:         logging.debug(",
      "",
      "[Added Lines]",
      "157:         logger.debug(",
      "",
      "---------------"
    ],
    "rasa/utils/io.py||rasa/utils/io.py": [
      "File: rasa/utils/io.py -> rasa/utils/io.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "2: import filecmp",
      "3: import logging",
      "4: import os",
      "6: import tempfile",
      "7: import warnings",
      "9: from asyncio import AbstractEventLoop",
      "10: from pathlib import Path",
      "12: from typing_extensions import Protocol",
      "14: import rasa.shared.constants",
      "",
      "[Removed Lines]",
      "5: import pickle",
      "8: import re",
      "11: from typing import Text, Any, Union, List, Type, Callable, TYPE_CHECKING, Pattern",
      "",
      "[Added Lines]",
      "5: import re",
      "10: from typing import Text, Any, List, Type, Callable, TYPE_CHECKING, Pattern",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "81:     return event_loop",
      "107: def create_temporary_file(data: Any, suffix: Text = \"\", mode: Text = \"w+\") -> Text:",
      "108:     \"\"\"Creates a tempfile.NamedTemporaryFile object for data.\"\"\"",
      "109:     encoding = None if \"b\" in mode else rasa.shared.utils.io.DEFAULT_ENCODING",
      "",
      "[Removed Lines]",
      "84: def pickle_dump(filename: Union[Text, Path], obj: Any) -> None:",
      "85:     \"\"\"Saves object to file.",
      "87:     Args:",
      "88:         filename: the filename to save the object to",
      "89:         obj: the object to store",
      "90:     \"\"\"",
      "91:     with open(filename, \"wb\") as f:",
      "92:         pickle.dump(obj, f)",
      "95: def pickle_load(filename: Union[Text, Path]) -> Any:",
      "96:     \"\"\"Loads an object from a file.",
      "98:     Args:",
      "99:         filename: the filename to load the object from",
      "101:     Returns: the loaded object",
      "102:     \"\"\"",
      "103:     with open(filename, \"rb\") as f:",
      "104:         return pickle.load(f)",
      "",
      "[Added Lines]",
      "[None]",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "160:     function: Callable[[Text], bool], error_message: Text",
      "161: ) -> Type[\"Validator\"]:",
      "162:     \"\"\"Helper method to create `Validator` classes from callable functions. Should be",
      "165:     from prompt_toolkit.validation import Validator, ValidationError",
      "166:     from prompt_toolkit.document import Document",
      "",
      "[Removed Lines]",
      "163:     removed when questionary supports `Validator` objects.\"\"\"",
      "",
      "[Added Lines]",
      "139:     removed when questionary supports `Validator` objects.",
      "140:     \"\"\"",
      "",
      "---------------",
      "--- Hunk 4 ---",
      "[Context before]",
      "175:     return FunctionValidator",
      "220: def get_emoji_regex() -> Pattern:",
      "221:     \"\"\"Returns regex to identify emojis.\"\"\"",
      "222:     return re.compile(",
      "",
      "[Removed Lines]",
      "178: def json_unpickle(",
      "179:     file_name: Union[Text, Path], encode_non_string_keys: bool = False",
      "180: ) -> Any:",
      "181:     \"\"\"Unpickle an object from file using json.",
      "183:     Args:",
      "184:         file_name: the file to load the object from",
      "185:         encode_non_string_keys: If set to `True` then jsonpickle will encode non-string",
      "186:           dictionary keys instead of coercing them into strings via `repr()`.",
      "188:     Returns: the object",
      "189:     \"\"\"",
      "190:     import jsonpickle.ext.numpy as jsonpickle_numpy",
      "191:     import jsonpickle",
      "193:     jsonpickle_numpy.register_handlers()",
      "195:     file_content = rasa.shared.utils.io.read_file(file_name)",
      "196:     return jsonpickle.loads(file_content, keys=encode_non_string_keys)",
      "199: def json_pickle(",
      "200:     file_name: Union[Text, Path], obj: Any, encode_non_string_keys: bool = False",
      "201: ) -> None:",
      "202:     \"\"\"Pickle an object to a file using json.",
      "204:     Args:",
      "205:         file_name: the file to store the object to",
      "206:         obj: the object to store",
      "207:         encode_non_string_keys: If set to `True` then jsonpickle will encode non-string",
      "208:           dictionary keys instead of coercing them into strings via `repr()`.",
      "209:     \"\"\"",
      "210:     import jsonpickle.ext.numpy as jsonpickle_numpy",
      "211:     import jsonpickle",
      "213:     jsonpickle_numpy.register_handlers()",
      "215:     rasa.shared.utils.io.write_text_file(",
      "216:         jsonpickle.dumps(obj, keys=encode_non_string_keys), file_name",
      "217:     )",
      "",
      "[Added Lines]",
      "[None]",
      "",
      "---------------"
    ],
    "rasa/utils/tensorflow/feature_array.py||rasa/utils/tensorflow/feature_array.py": [
      "File: rasa/utils/tensorflow/feature_array.py -> rasa/utils/tensorflow/feature_array.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "[No context available]",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "1: from typing import Dict, Any, List, Tuple, Optional, Union",
      "3: import numpy as np",
      "4: import scipy.sparse",
      "5: from safetensors.numpy import load_file",
      "6: from safetensors.numpy import save_file",
      "8: import rasa.shared.utils.io",
      "11: def _recursive_serialize(",
      "12:     array: Any, prefix: str, data_dict: Dict[str, Any], metadata: List[Dict[str, Any]]",
      "13: ) -> None:",
      "14:     \"\"\"Recursively serialize arrays and matrices for high dimensional data.\"\"\"",
      "15:     if isinstance(array, np.ndarray) and array.ndim <= 2:",
      "16:         data_key = f\"{prefix}_array\"",
      "17:         data_dict[data_key] = array",
      "18:         metadata.append({\"type\": \"dense\", \"key\": data_key, \"shape\": array.shape})",
      "20:     elif isinstance(array, list) and all([isinstance(v, float) for v in array]):",
      "21:         data_key = f\"{prefix}_list\"",
      "22:         data_dict[data_key] = np.array(array, dtype=np.float32)",
      "23:         metadata.append({\"type\": \"list\", \"key\": data_key})",
      "25:     elif isinstance(array, list) and all([isinstance(v, int) for v in array]):",
      "26:         data_key = f\"{prefix}_list\"",
      "27:         data_dict[data_key] = np.array(array, dtype=np.int64)",
      "28:         metadata.append({\"type\": \"list\", \"key\": data_key})",
      "30:     elif isinstance(array, scipy.sparse.spmatrix):",
      "31:         data_key_data = f\"{prefix}_data\"",
      "32:         data_key_row = f\"{prefix}_row\"",
      "33:         data_key_col = f\"{prefix}_col\"",
      "34:         array = array.tocoo()",
      "35:         data_dict.update(",
      "36:             {",
      "37:                 data_key_data: array.data,",
      "38:                 data_key_row: array.row,",
      "39:                 data_key_col: array.col,",
      "40:             }",
      "41:         )",
      "42:         metadata.append({\"type\": \"sparse\", \"key\": prefix, \"shape\": array.shape})",
      "44:     elif isinstance(array, list) or isinstance(array, np.ndarray):",
      "45:         group_metadata = {\"type\": \"group\", \"subcomponents\": []}",
      "46:         for idx, item in enumerate(array):",
      "47:             new_prefix = f\"{prefix}_{idx}\"",
      "48:             _recursive_serialize(",
      "49:                 item, new_prefix, data_dict, group_metadata[\"subcomponents\"]",
      "50:             )",
      "51:         metadata.append(group_metadata)",
      "54: def _serialize_nested_data(",
      "55:     nested_data: Dict[str, Dict[str, List[\"FeatureArray\"]]],",
      "56:     prefix: str,",
      "57:     data_dict: Dict[str, np.ndarray],",
      "58:     metadata: List[Dict[str, Union[str, List]]],",
      "59: ) -> None:",
      "60:     \"\"\"Handle serialization across dictionary and list levels.\"\"\"",
      "61:     for outer_key, inner_dict in nested_data.items():",
      "62:         inner_metadata = {\"key\": outer_key, \"components\": []}",
      "64:         for inner_key, feature_arrays in inner_dict.items():",
      "65:             array_metadata = {",
      "66:                 \"key\": inner_key,",
      "67:                 \"number_of_dimensions\": feature_arrays[0].number_of_dimensions,",
      "68:                 \"features\": [],",
      "69:             }",
      "71:             for idx, feature_array in enumerate(feature_arrays):",
      "72:                 feature_prefix = f\"{prefix}_{outer_key}_{inner_key}_{idx}\"",
      "73:                 _recursive_serialize(",
      "74:                     feature_array.tolist(),",
      "75:                     feature_prefix,",
      "76:                     data_dict,",
      "77:                     array_metadata[\"features\"],",
      "78:                 )",
      "80:             inner_metadata[\"components\"].append(  # type:ignore[attr-defined]",
      "81:                 array_metadata",
      "82:             )",
      "84:         metadata.append(inner_metadata)",
      "87: def serialize_nested_feature_arrays(",
      "88:     nested_feature_array: Dict[str, Dict[str, List[\"FeatureArray\"]]],",
      "89:     data_filename: str,",
      "90:     metadata_filename: str,",
      "91: ) -> None:",
      "92:     data_dict: Dict[str, np.ndarray] = {}",
      "93:     metadata: List[Dict[str, Union[str, List]]] = []",
      "95:     _serialize_nested_data(nested_feature_array, \"component\", data_dict, metadata)",
      "97:     # Save serialized data and metadata",
      "98:     save_file(data_dict, data_filename)",
      "99:     rasa.shared.utils.io.dump_obj_as_json_to_file(metadata_filename, metadata)",
      "102: def _recursive_deserialize(",
      "103:     metadata: List[Dict[str, Any]], data: Dict[str, Any]",
      "104: ) -> List[Any]:",
      "105:     \"\"\"Recursively deserialize arrays and matrices for high dimensional data.\"\"\"",
      "106:     result = []",
      "108:     for item in metadata:",
      "109:         if item[\"type\"] == \"dense\":",
      "110:             key = item[\"key\"]",
      "111:             array = np.asarray(data[key]).reshape(item[\"shape\"])",
      "112:             result.append(array)",
      "114:         elif item[\"type\"] == \"list\":",
      "115:             key = item[\"key\"]",
      "116:             result.append(list(data[key]))",
      "118:         elif item[\"type\"] == \"sparse\":",
      "119:             data_vals = data[f\"{item['key']}_data\"]",
      "120:             row_vals = data[f\"{item['key']}_row\"]",
      "121:             col_vals = data[f\"{item['key']}_col\"]",
      "122:             sparse_matrix = scipy.sparse.coo_matrix(",
      "123:                 (data_vals, (row_vals, col_vals)), shape=item[\"shape\"]",
      "124:             )",
      "125:             result.append(sparse_matrix)",
      "126:         elif item[\"type\"] == \"group\":",
      "127:             sublist = _recursive_deserialize(item[\"subcomponents\"], data)",
      "128:             result.append(sublist)",
      "130:     return result",
      "133: def _deserialize_nested_data(",
      "134:     metadata: List[Dict[str, Any]], data_dict: Dict[str, Any]",
      "135: ) -> Dict[str, Dict[str, List[\"FeatureArray\"]]]:",
      "136:     \"\"\"Handle deserialization across all dictionary and list levels.\"\"\"",
      "137:     result: Dict[str, Dict[str, List[\"FeatureArray\"]]] = {}",
      "139:     for outer_item in metadata:",
      "140:         outer_key = outer_item[\"key\"]",
      "141:         result[outer_key] = {}",
      "143:         for inner_item in outer_item[\"components\"]:",
      "144:             inner_key = inner_item[\"key\"]",
      "145:             feature_arrays = []",
      "147:             # Reconstruct the list of FeatureArrays",
      "148:             for feature_item in inner_item[\"features\"]:",
      "149:                 # Reconstruct the list of FeatureArrays",
      "150:                 feature_array_data = _recursive_deserialize([feature_item], data_dict)",
      "151:                 # Prepare the input for the FeatureArray;",
      "152:                 # ensure it is np.ndarray compatible",
      "153:                 input_array = np.array(feature_array_data[0], dtype=object)",
      "154:                 feature_array = FeatureArray(",
      "155:                     input_array, inner_item[\"number_of_dimensions\"]",
      "156:                 )",
      "157:                 feature_arrays.append(feature_array)",
      "159:             result[outer_key][inner_key] = feature_arrays",
      "161:     return result",
      "164: def deserialize_nested_feature_arrays(",
      "165:     data_filename: str, metadata_filename: str",
      "166: ) -> Dict[str, Dict[str, List[\"FeatureArray\"]]]:",
      "167:     metadata = rasa.shared.utils.io.read_json_file(metadata_filename)",
      "168:     data_dict = load_file(data_filename)",
      "170:     return _deserialize_nested_data(metadata, data_dict)",
      "173: class FeatureArray(np.ndarray):",
      "174:     \"\"\"Stores any kind of features ready to be used by a RasaModel.",
      "176:     Next to the input numpy array of features, it also received the number of",
      "177:     dimensions of the features.",
      "178:     As our features can have 1 to 4 dimensions we might have different number of numpy",
      "179:     arrays stacked. The number of dimensions helps us to figure out how to handle this",
      "180:     particular feature array. Also, it is automatically determined whether the feature",
      "181:     array is sparse or not and the number of units is determined as well.",
      "183:     Subclassing np.array: https://numpy.org/doc/stable/user/basics.subclassing.html",
      "184:     \"\"\"",
      "186:     def __new__(",
      "187:         cls, input_array: np.ndarray, number_of_dimensions: int",
      "188:     ) -> \"FeatureArray\":",
      "189:         \"\"\"Create and return a new object.  See help(type) for accurate signature.\"\"\"",
      "190:         FeatureArray._validate_number_of_dimensions(number_of_dimensions, input_array)",
      "192:         feature_array = np.asarray(input_array).view(cls)",
      "194:         if number_of_dimensions <= 2:",
      "195:             feature_array.units = input_array.shape[-1]",
      "196:             feature_array.is_sparse = isinstance(input_array[0], scipy.sparse.spmatrix)",
      "197:         elif number_of_dimensions == 3:",
      "198:             feature_array.units = input_array[0].shape[-1]",
      "199:             feature_array.is_sparse = isinstance(input_array[0], scipy.sparse.spmatrix)",
      "200:         elif number_of_dimensions == 4:",
      "201:             feature_array.units = input_array[0][0].shape[-1]",
      "202:             feature_array.is_sparse = isinstance(",
      "203:                 input_array[0][0], scipy.sparse.spmatrix",
      "204:             )",
      "205:         else:",
      "206:             raise ValueError(",
      "207:                 f\"Number of dimensions '{number_of_dimensions}' currently not \"",
      "208:                 f\"supported.\"",
      "209:             )",
      "211:         feature_array.number_of_dimensions = number_of_dimensions",
      "213:         return feature_array",
      "215:     def __init__(",
      "216:         self, input_array: Any, number_of_dimensions: int, **kwargs: Any",
      "217:     ) -> None:",
      "218:         \"\"\"Initialize. FeatureArray.",
      "220:         Needed in order to avoid 'Invalid keyword argument number_of_dimensions",
      "221:         to function FeatureArray.__init__ '",
      "222:         Args:",
      "223:             input_array: the array that contains features",
      "224:             number_of_dimensions: number of dimensions in input_array",
      "225:         \"\"\"",
      "226:         super().__init__(**kwargs)",
      "227:         self.number_of_dimensions = number_of_dimensions",
      "229:     def __array_finalize__(self, obj: Optional[np.ndarray]) -> None:",
      "230:         \"\"\"This method is called when the system allocates a new array from obj.",
      "232:         Args:",
      "233:             obj: A subclass (subtype) of ndarray.",
      "234:         \"\"\"",
      "235:         if obj is None:",
      "236:             return",
      "238:         self.units = getattr(obj, \"units\", None)",
      "239:         self.number_of_dimensions = getattr(",
      "240:             obj, \"number_of_dimensions\", None",
      "241:         )  # type: ignore[assignment]",
      "242:         self.is_sparse = getattr(obj, \"is_sparse\", None)",
      "244:         default_attributes = {",
      "245:             \"units\": self.units,",
      "246:             \"number_of_dimensions\": self.number_of_dimensions,",
      "247:             \"is_spare\": self.is_sparse,",
      "248:         }",
      "249:         self.__dict__.update(default_attributes)",
      "251:     # pytype: disable=attribute-error",
      "252:     def __array_ufunc__(",
      "253:         self, ufunc: Any, method: str, *inputs: Any, **kwargs: Any",
      "254:     ) -> Any:",
      "255:         \"\"\"Overwrite this method as we are subclassing numpy array.",
      "257:         Args:",
      "258:             ufunc: The ufunc object that was called.",
      "259:             method: A string indicating which Ufunc method was called",
      "260:                     (one of \"__call__\", \"reduce\", \"reduceat\", \"accumulate\", \"outer\",",
      "261:                     \"inner\").",
      "265:         Returns:",
      "266:             The result of the operation.",
      "267:         \"\"\"",
      "268:         f = {",
      "269:             \"reduce\": ufunc.reduce,",
      "270:             \"accumulate\": ufunc.accumulate,",
      "271:             \"reduceat\": ufunc.reduceat,",
      "272:             \"outer\": ufunc.outer,",
      "273:             \"at\": ufunc.at,",
      "274:             \"__call__\": ufunc,",
      "275:         }",
      "276:         # convert the inputs to np.ndarray to prevent recursion, call the function,",
      "277:         # then cast it back as FeatureArray",
      "278:         output = FeatureArray(",
      "279:             f[method](*(i.view(np.ndarray) for i in inputs), **kwargs),",
      "280:             number_of_dimensions=kwargs[\"number_of_dimensions\"],",
      "281:         )",
      "282:         output.__dict__ = self.__dict__  # carry forward attributes",
      "283:         return output",
      "285:     def __reduce__(self) -> Tuple[Any, Any, Any]:",
      "286:         \"\"\"Needed in order to pickle this object.",
      "288:         Returns:",
      "289:             A tuple.",
      "290:         \"\"\"",
      "291:         pickled_state = super(FeatureArray, self).__reduce__()",
      "292:         if isinstance(pickled_state, str):",
      "293:             raise TypeError(\"np array __reduce__ returned string instead of tuple.\")",
      "294:         new_state = pickled_state[2] + (",
      "295:             self.number_of_dimensions,",
      "296:             self.is_sparse,",
      "297:             self.units,",
      "298:         )",
      "299:         return pickled_state[0], pickled_state[1], new_state",
      "301:     def __setstate__(self, state: Any, **kwargs: Any) -> None:",
      "302:         \"\"\"Sets the state.",
      "304:         Args:",
      "305:             state: The state argument must be a sequence that contains the following",
      "306:                    elements version, shape, dtype, isFortan, rawdata.",
      "308:         \"\"\"",
      "309:         # Needed in order to load the object",
      "310:         self.number_of_dimensions = state[-3]",
      "311:         self.is_sparse = state[-2]",
      "312:         self.units = state[-1]",
      "313:         super(FeatureArray, self).__setstate__(state[0:-3], **kwargs)",
      "315:     # pytype: enable=attribute-error",
      "317:     @staticmethod",
      "318:     def _validate_number_of_dimensions(",
      "319:         number_of_dimensions: int, input_array: np.ndarray",
      "320:     ) -> None:",
      "321:         \"\"\"Validates if the input array has given number of dimensions.",
      "323:         Args:",
      "324:             number_of_dimensions: number of dimensions",
      "325:             input_array: input array",
      "327:         Raises: ValueError in case the dimensions do not match",
      "328:         \"\"\"",
      "329:         # when loading the feature arrays from disk, the shape represents",
      "330:         # the correct number of dimensions",
      "331:         if len(input_array.shape) == number_of_dimensions:",
      "332:             return",
      "334:         _sub_array = input_array",
      "335:         dim = 0",
      "336:         # Go number_of_dimensions into the given input_array",
      "337:         for i in range(1, number_of_dimensions + 1):",
      "338:             _sub_array = _sub_array[0]",
      "339:             if isinstance(_sub_array, scipy.sparse.spmatrix):",
      "340:                 dim = i",
      "341:                 break",
      "342:             if isinstance(_sub_array, np.ndarray) and _sub_array.shape[0] == 0:",
      "343:                 # sequence dimension is 0, we are dealing with \"fake\" features",
      "344:                 dim = i",
      "345:                 break",
      "347:         # If the resulting sub_array is sparse, the remaining number of dimensions",
      "348:         # should be at least 2",
      "349:         if isinstance(_sub_array, scipy.sparse.spmatrix):",
      "350:             if dim > 2:",
      "351:                 raise ValueError(",
      "352:                     f\"Given number of dimensions '{number_of_dimensions}' does not \"",
      "353:                     f\"match dimensions of given input array: {input_array}.\"",
      "354:                 )",
      "355:         elif isinstance(_sub_array, np.ndarray) and _sub_array.shape[0] == 0:",
      "356:             # sequence dimension is 0, we are dealing with \"fake\" features,",
      "357:             # but they should be of dim 2",
      "358:             if dim > 2:",
      "359:                 raise ValueError(",
      "360:                     f\"Given number of dimensions '{number_of_dimensions}' does not \"",
      "361:                     f\"match dimensions of given input array: {input_array}.\"",
      "362:                 )",
      "363:         # If the resulting sub_array is dense, the sub_array should be a single number",
      "364:         elif not np.issubdtype(type(_sub_array), np.integer) and not isinstance(",
      "365:             _sub_array, (np.float32, np.float64)",
      "366:         ):",
      "367:             raise ValueError(",
      "368:                 f\"Given number of dimensions '{number_of_dimensions}' does not match \"",
      "369:                 f\"dimensions of given input array: {input_array}.\"",
      "370:             )",
      "",
      "---------------"
    ],
    "rasa/utils/tensorflow/model_data.py||rasa/utils/tensorflow/model_data.py": [
      "File: rasa/utils/tensorflow/model_data.py -> rasa/utils/tensorflow/model_data.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "20: import scipy.sparse",
      "21: from sklearn.model_selection import train_test_split",
      "23: logger = logging.getLogger(__name__)",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "23: from rasa.utils.tensorflow.feature_array import FeatureArray",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "37:         return np.array(ragged_array, dtype=object)",
      "233: class FeatureSignature(NamedTuple):",
      "234:     \"\"\"Signature of feature arrays.",
      "",
      "[Removed Lines]",
      "40: class FeatureArray(np.ndarray):",
      "41:     \"\"\"Stores any kind of features ready to be used by a RasaModel.",
      "43:     Next to the input numpy array of features, it also received the number of",
      "44:     dimensions of the features.",
      "45:     As our features can have 1 to 4 dimensions we might have different number of numpy",
      "46:     arrays stacked. The number of dimensions helps us to figure out how to handle this",
      "47:     particular feature array. Also, it is automatically determined whether the feature",
      "48:     array is sparse or not and the number of units is determined as well.",
      "50:     Subclassing np.array: https://numpy.org/doc/stable/user/basics.subclassing.html",
      "51:     \"\"\"",
      "53:     def __new__(",
      "54:         cls, input_array: np.ndarray, number_of_dimensions: int",
      "55:     ) -> \"FeatureArray\":",
      "56:         \"\"\"Create and return a new object.  See help(type) for accurate signature.\"\"\"",
      "57:         FeatureArray._validate_number_of_dimensions(number_of_dimensions, input_array)",
      "59:         feature_array = np.asarray(input_array).view(cls)",
      "61:         if number_of_dimensions <= 2:",
      "62:             feature_array.units = input_array.shape[-1]",
      "63:             feature_array.is_sparse = isinstance(input_array[0], scipy.sparse.spmatrix)",
      "64:         elif number_of_dimensions == 3:",
      "65:             feature_array.units = input_array[0].shape[-1]",
      "66:             feature_array.is_sparse = isinstance(input_array[0], scipy.sparse.spmatrix)",
      "67:         elif number_of_dimensions == 4:",
      "68:             feature_array.units = input_array[0][0].shape[-1]",
      "69:             feature_array.is_sparse = isinstance(",
      "70:                 input_array[0][0], scipy.sparse.spmatrix",
      "71:             )",
      "72:         else:",
      "73:             raise ValueError(",
      "74:                 f\"Number of dimensions '{number_of_dimensions}' currently not \"",
      "75:                 f\"supported.\"",
      "76:             )",
      "78:         feature_array.number_of_dimensions = number_of_dimensions",
      "80:         return feature_array",
      "82:     def __init__(",
      "83:         self, input_array: Any, number_of_dimensions: int, **kwargs: Any",
      "84:     ) -> None:",
      "85:         \"\"\"Initialize. FeatureArray.",
      "87:         Needed in order to avoid 'Invalid keyword argument number_of_dimensions",
      "88:         to function FeatureArray.__init__ '",
      "89:         Args:",
      "90:             input_array: the array that contains features",
      "91:             number_of_dimensions: number of dimensions in input_array",
      "92:         \"\"\"",
      "93:         super().__init__(**kwargs)",
      "94:         self.number_of_dimensions = number_of_dimensions",
      "96:     def __array_finalize__(self, obj: Optional[np.ndarray]) -> None:",
      "97:         \"\"\"This method is called when the system allocates a new array from obj.",
      "99:         Args:",
      "100:             obj: A subclass (subtype) of ndarray.",
      "101:         \"\"\"",
      "102:         if obj is None:",
      "103:             return",
      "105:         self.units = getattr(obj, \"units\", None)",
      "106:         self.number_of_dimensions = getattr(obj, \"number_of_dimensions\", None)  # type: ignore[assignment] # noqa:E501",
      "107:         self.is_sparse = getattr(obj, \"is_sparse\", None)",
      "109:         default_attributes = {",
      "110:             \"units\": self.units,",
      "111:             \"number_of_dimensions\": self.number_of_dimensions,",
      "112:             \"is_spare\": self.is_sparse,",
      "113:         }",
      "114:         self.__dict__.update(default_attributes)",
      "116:     # pytype: disable=attribute-error",
      "117:     def __array_ufunc__(",
      "118:         self, ufunc: Any, method: Text, *inputs: Any, **kwargs: Any",
      "119:     ) -> Any:",
      "120:         \"\"\"Overwrite this method as we are subclassing numpy array.",
      "122:         Args:",
      "123:             ufunc: The ufunc object that was called.",
      "124:             method: A string indicating which Ufunc method was called",
      "125:                     (one of \"__call__\", \"reduce\", \"reduceat\", \"accumulate\", \"outer\",",
      "126:                     \"inner\").",
      "130:         Returns:",
      "131:             The result of the operation.",
      "132:         \"\"\"",
      "133:         f = {",
      "134:             \"reduce\": ufunc.reduce,",
      "135:             \"accumulate\": ufunc.accumulate,",
      "136:             \"reduceat\": ufunc.reduceat,",
      "137:             \"outer\": ufunc.outer,",
      "138:             \"at\": ufunc.at,",
      "139:             \"__call__\": ufunc,",
      "140:         }",
      "141:         # convert the inputs to np.ndarray to prevent recursion, call the function,",
      "142:         # then cast it back as FeatureArray",
      "143:         output = FeatureArray(",
      "144:             f[method](*(i.view(np.ndarray) for i in inputs), **kwargs),",
      "145:             number_of_dimensions=kwargs[\"number_of_dimensions\"],",
      "146:         )",
      "147:         output.__dict__ = self.__dict__  # carry forward attributes",
      "148:         return output",
      "150:     def __reduce__(self) -> Tuple[Any, Any, Any]:",
      "151:         \"\"\"Needed in order to pickle this object.",
      "153:         Returns:",
      "154:             A tuple.",
      "155:         \"\"\"",
      "156:         pickled_state = super(FeatureArray, self).__reduce__()",
      "157:         if isinstance(pickled_state, str):",
      "158:             raise TypeError(\"np array __reduce__ returned string instead of tuple.\")",
      "159:         new_state = pickled_state[2] + (",
      "160:             self.number_of_dimensions,",
      "161:             self.is_sparse,",
      "162:             self.units,",
      "163:         )",
      "164:         return pickled_state[0], pickled_state[1], new_state",
      "166:     def __setstate__(self, state: Any, **kwargs: Any) -> None:",
      "167:         \"\"\"Sets the state.",
      "169:         Args:",
      "170:             state: The state argument must be a sequence that contains the following",
      "171:                    elements version, shape, dtype, isFortan, rawdata.",
      "173:         \"\"\"",
      "174:         # Needed in order to load the object",
      "175:         self.number_of_dimensions = state[-3]",
      "176:         self.is_sparse = state[-2]",
      "177:         self.units = state[-1]",
      "178:         super(FeatureArray, self).__setstate__(state[0:-3], **kwargs)",
      "180:     # pytype: enable=attribute-error",
      "182:     @staticmethod",
      "183:     def _validate_number_of_dimensions(",
      "184:         number_of_dimensions: int, input_array: np.ndarray",
      "185:     ) -> None:",
      "186:         \"\"\"Validates if the the input array has given number of dimensions.",
      "188:         Args:",
      "189:             number_of_dimensions: number of dimensions",
      "190:             input_array: input array",
      "192:         Raises: ValueError in case the dimensions do not match",
      "193:         \"\"\"",
      "194:         _sub_array = input_array",
      "195:         dim = 0",
      "196:         # Go number_of_dimensions into the given input_array",
      "197:         for i in range(1, number_of_dimensions + 1):",
      "198:             _sub_array = _sub_array[0]",
      "199:             if isinstance(_sub_array, scipy.sparse.spmatrix):",
      "200:                 dim = i",
      "201:                 break",
      "202:             if isinstance(_sub_array, np.ndarray) and _sub_array.shape[0] == 0:",
      "203:                 # sequence dimension is 0, we are dealing with \"fake\" features",
      "204:                 dim = i",
      "205:                 break",
      "207:         # If the resulting sub_array is sparse, the remaining number of dimensions",
      "208:         # should be at least 2",
      "209:         if isinstance(_sub_array, scipy.sparse.spmatrix):",
      "210:             if dim > 2:",
      "211:                 raise ValueError(",
      "212:                     f\"Given number of dimensions '{number_of_dimensions}' does not \"",
      "213:                     f\"match dimensions of given input array: {input_array}.\"",
      "214:                 )",
      "215:         elif isinstance(_sub_array, np.ndarray) and _sub_array.shape[0] == 0:",
      "216:             # sequence dimension is 0, we are dealing with \"fake\" features,",
      "217:             # but they should be of dim 2",
      "218:             if dim > 2:",
      "219:                 raise ValueError(",
      "220:                     f\"Given number of dimensions '{number_of_dimensions}' does not \"",
      "221:                     f\"match dimensions of given input array: {input_array}.\"",
      "222:                 )",
      "223:         # If the resulting sub_array is dense, the sub_array should be a single number",
      "224:         elif not np.issubdtype(type(_sub_array), np.integer) and not isinstance(",
      "225:             _sub_array, (np.float32, np.float64)",
      "226:         ):",
      "227:             raise ValueError(",
      "228:                 f\"Given number of dimensions '{number_of_dimensions}' does not match \"",
      "229:                 f\"dimensions of given input array: {input_array}.\"",
      "230:             )",
      "",
      "[Added Lines]",
      "[None]",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "270:         label_sub_key: Optional[Text] = None,",
      "271:         data: Optional[Data] = None,",
      "272:     ) -> None:",
      "276:         Args:",
      "277:             label_key: the key of a label used for balancing, etc.",
      "",
      "[Removed Lines]",
      "273:         \"\"\"",
      "274:         Initializes the RasaModelData object.",
      "",
      "[Added Lines]",
      "82:         \"\"\"Initializes the RasaModelData object.",
      "",
      "---------------"
    ],
    "scripts/ping_slack_about_package_release.sh||scripts/ping_slack_about_package_release.sh": [
      "File: scripts/ping_slack_about_package_release.sh -> scripts/ping_slack_about_package_release.sh"
    ],
    "tests/core/featurizers/test_tracker_featurizer.py||tests/core/featurizers/test_tracker_featurizer.py": [
      "File: tests/core/featurizers/test_tracker_featurizer.py -> tests/core/featurizers/test_tracker_featurizer.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "34:     assert TrackerFeaturizer.load(\"non_existent_class\") is None",
      "38:     state_featurizer = SingleStateFeaturizer()",
      "39:     state_featurizer.prepare_for_training(moodbot_domain)",
      "40:     tracker_featurizer = MaxHistoryTrackerFeaturizer(state_featurizer)",
      "",
      "[Removed Lines]",
      "37: def test_persist_and_load_tracker_featurizer(tmp_path: Text, moodbot_domain: Domain):",
      "",
      "[Added Lines]",
      "37: def test_persist_and_load_full_dialogue_tracker_featurizer(",
      "38:     tmp_path: Text, moodbot_domain: Domain",
      "39: ):",
      "40:     state_featurizer = SingleStateFeaturizer()",
      "41:     state_featurizer.prepare_for_training(moodbot_domain)",
      "42:     tracker_featurizer = FullDialogueTrackerFeaturizer(state_featurizer)",
      "44:     tracker_featurizer.persist(tmp_path)",
      "46:     loaded_tracker_featurizer = TrackerFeaturizer.load(tmp_path)",
      "48:     assert loaded_tracker_featurizer is not None",
      "49:     assert loaded_tracker_featurizer.state_featurizer is not None",
      "50:     assert loaded_tracker_featurizer.to_dict() == tracker_featurizer.to_dict()",
      "53: def test_persist_and_load_max_history_tracker_featurizer(",
      "54:     tmp_path: Text, moodbot_domain: Domain",
      "55: ):",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "46:     assert loaded_tracker_featurizer is not None",
      "47:     assert loaded_tracker_featurizer.state_featurizer is not None",
      "50: def test_convert_action_labels_to_ids(domain: Domain):",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "66:     assert loaded_tracker_featurizer.to_dict() == tracker_featurizer.to_dict()",
      "69: def test_persist_and_load_intent_max_history_tracker_featurizer(",
      "70:     tmp_path: Text, moodbot_domain: Domain",
      "71: ):",
      "72:     state_featurizer = SingleStateFeaturizer()",
      "73:     state_featurizer.prepare_for_training(moodbot_domain)",
      "74:     tracker_featurizer = IntentMaxHistoryTrackerFeaturizer(state_featurizer)",
      "76:     tracker_featurizer.persist(tmp_path)",
      "78:     loaded_tracker_featurizer = TrackerFeaturizer.load(tmp_path)",
      "80:     assert loaded_tracker_featurizer is not None",
      "81:     assert loaded_tracker_featurizer.state_featurizer is not None",
      "82:     assert loaded_tracker_featurizer.to_dict() == tracker_featurizer.to_dict()",
      "",
      "---------------"
    ],
    "tests/nlu/extractors/test_crf_entity_extractor.py||tests/nlu/extractors/test_crf_entity_extractor.py": [
      "File: tests/nlu/extractors/test_crf_entity_extractor.py -> tests/nlu/extractors/test_crf_entity_extractor.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "1: import copy",
      "2: from typing import Dict, Text, List, Any, Callable",
      "4: import pytest",
      "6: from rasa.engine.graph import ExecutionContext",
      "7: from rasa.engine.storage.resource import Resource",
      "8: from rasa.engine.storage.storage import ModelStorage",
      "9: from rasa.nlu.featurizers.dense_featurizer.spacy_featurizer import SpacyFeaturizer",
      "10: from rasa.nlu.tokenizers.spacy_tokenizer import SpacyTokenizer",
      "12: from rasa.nlu.tokenizers.whitespace_tokenizer import WhitespaceTokenizer",
      "13: from rasa.nlu.utils.spacy_utils import SpacyModel, SpacyNLP",
      "14: from rasa.shared.importers.rasa import RasaFileImporter",
      "15: from rasa.shared.nlu.constants import TEXT, ENTITIES",
      "16: from rasa.shared.nlu.training_data.message import Message",
      "23: @pytest.fixture()",
      "",
      "[Removed Lines]",
      "11: from rasa.nlu.constants import SPACY_DOCS",
      "17: from rasa.nlu.extractors.crf_entity_extractor import (",
      "18:     CRFEntityExtractor,",
      "19:     CRFEntityExtractorOptions,",
      "20: )",
      "",
      "[Added Lines]",
      "4: import numpy as np",
      "10: from rasa.nlu.constants import SPACY_DOCS",
      "11: from rasa.nlu.extractors.crf_entity_extractor import (",
      "12:     CRFEntityExtractor,",
      "13:     CRFEntityExtractorOptions,",
      "14:     CRFToken,",
      "15: )",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "204:     spacy_featurizer.process([message])",
      "206:     text_data = crf_extractor._convert_to_crf_tokens(message)",
      "209:     assert \"0:text_dense_features\" in features[0]",
      "210:     dense_features, _ = message.get_dense_features(TEXT, [])",
      "",
      "[Removed Lines]",
      "207:     features = crf_extractor._crf_tokens_to_features(text_data)",
      "",
      "[Added Lines]",
      "209:     features = crf_extractor._crf_tokens_to_features(text_data, component_config)",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "250:     assert processed_message.get(TEXT) == message_text",
      "251:     assert processed_message.get(ENTITIES) == []",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "256: @pytest.fixture",
      "257: def sample_data():",
      "258:     return {",
      "259:         \"text\": \"apple\",",
      "260:         \"pos_tag\": \"NOUN\",",
      "261:         \"pattern\": {\"length\": 5, \"is_capitalized\": False},",
      "262:         \"dense_features\": np.array([0.1, 0.2, 0.3]),",
      "263:         \"entity_tag\": \"B-FOOD\",",
      "264:         \"entity_role_tag\": \"INGREDIENT\",",
      "265:         \"entity_group_tag\": \"ITEM\",",
      "266:     }",
      "269: @pytest.fixture",
      "270: def sample_token(sample_data):",
      "271:     return CRFToken(",
      "272:         sample_data[\"text\"],",
      "273:         sample_data[\"pos_tag\"],",
      "274:         sample_data[\"pattern\"],",
      "275:         sample_data[\"dense_features\"],",
      "276:         sample_data[\"entity_tag\"],",
      "277:         sample_data[\"entity_role_tag\"],",
      "278:         sample_data[\"entity_group_tag\"],",
      "279:     )",
      "282: def test_crf_token_to_dict(sample_data, sample_token):",
      "283:     token_dict = sample_token.to_dict()",
      "285:     assert token_dict[\"text\"] == sample_data[\"text\"]",
      "286:     assert token_dict[\"pos_tag\"] == sample_data[\"pos_tag\"]",
      "287:     assert token_dict[\"pattern\"] == sample_data[\"pattern\"]",
      "288:     assert token_dict[\"dense_features\"] == [",
      "289:         str(x) for x in sample_data[\"dense_features\"]",
      "290:     ]",
      "291:     assert token_dict[\"entity_tag\"] == sample_data[\"entity_tag\"]",
      "292:     assert token_dict[\"entity_role_tag\"] == sample_data[\"entity_role_tag\"]",
      "293:     assert token_dict[\"entity_group_tag\"] == sample_data[\"entity_group_tag\"]",
      "296: def test_crf_token_create_from_dict(sample_data):",
      "297:     dict_data = {",
      "298:         \"text\": sample_data[\"text\"],",
      "299:         \"pos_tag\": sample_data[\"pos_tag\"],",
      "300:         \"pattern\": sample_data[\"pattern\"],",
      "301:         \"dense_features\": [str(x) for x in sample_data[\"dense_features\"]],",
      "302:         \"entity_tag\": sample_data[\"entity_tag\"],",
      "303:         \"entity_role_tag\": sample_data[\"entity_role_tag\"],",
      "304:         \"entity_group_tag\": sample_data[\"entity_group_tag\"],",
      "305:     }",
      "307:     token = CRFToken.create_from_dict(dict_data)",
      "309:     assert token.text == sample_data[\"text\"]",
      "310:     assert token.pos_tag == sample_data[\"pos_tag\"]",
      "311:     assert token.pattern == sample_data[\"pattern\"]",
      "312:     np.testing.assert_array_equal(token.dense_features, sample_data[\"dense_features\"])",
      "313:     assert token.entity_tag == sample_data[\"entity_tag\"]",
      "314:     assert token.entity_role_tag == sample_data[\"entity_role_tag\"]",
      "315:     assert token.entity_group_tag == sample_data[\"entity_group_tag\"]",
      "318: def test_crf_token_roundtrip_conversion(sample_token):",
      "319:     token_dict = sample_token.to_dict()",
      "320:     new_token = CRFToken.create_from_dict(token_dict)",
      "322:     assert new_token.text == sample_token.text",
      "323:     assert new_token.pos_tag == sample_token.pos_tag",
      "324:     assert new_token.pattern == sample_token.pattern",
      "325:     np.testing.assert_array_equal(new_token.dense_features, sample_token.dense_features)",
      "326:     assert new_token.entity_tag == sample_token.entity_tag",
      "327:     assert new_token.entity_role_tag == sample_token.entity_role_tag",
      "328:     assert new_token.entity_group_tag == sample_token.entity_group_tag",
      "331: def test_crf_token_empty_dense_features(sample_data):",
      "332:     sample_data[\"dense_features\"] = np.array([])",
      "333:     token = CRFToken(",
      "334:         sample_data[\"text\"],",
      "335:         sample_data[\"pos_tag\"],",
      "336:         sample_data[\"pattern\"],",
      "337:         sample_data[\"dense_features\"],",
      "338:         sample_data[\"entity_tag\"],",
      "339:         sample_data[\"entity_role_tag\"],",
      "340:         sample_data[\"entity_group_tag\"],",
      "341:     )",
      "342:     token_dict = token.to_dict()",
      "343:     new_token = CRFToken.create_from_dict(token_dict)",
      "344:     np.testing.assert_array_equal(new_token.dense_features, np.array([]))",
      "347: def test_crf_token_empty_pattern(sample_data):",
      "348:     sample_data[\"pattern\"] = {}",
      "349:     token = CRFToken(",
      "350:         sample_data[\"text\"],",
      "351:         sample_data[\"pos_tag\"],",
      "352:         sample_data[\"pattern\"],",
      "353:         sample_data[\"dense_features\"],",
      "354:         sample_data[\"entity_tag\"],",
      "355:         sample_data[\"entity_role_tag\"],",
      "356:         sample_data[\"entity_group_tag\"],",
      "357:     )",
      "358:     token_dict = token.to_dict()",
      "359:     new_token = CRFToken.create_from_dict(token_dict)",
      "360:     assert new_token.pattern == {}",
      "",
      "---------------"
    ],
    "tests/shared/nlu/training_data/test_features.py||tests/shared/nlu/training_data/test_features.py": [
      "File: tests/shared/nlu/training_data/test_features.py -> tests/shared/nlu/training_data/test_features.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "1: import itertools",
      "2: from typing import Optional, Text, List, Dict, Tuple, Any",
      "4: import numpy as np",
      "5: import pytest",
      "6: import scipy.sparse",
      "9: from rasa.shared.nlu.constants import (",
      "10:     FEATURE_TYPE_SENTENCE,",
      "11:     FEATURE_TYPE_SEQUENCE,",
      "12:     TEXT,",
      "13:     INTENT,",
      "14: )",
      "17: @pytest.mark.parametrize(",
      "",
      "[Removed Lines]",
      "8: from rasa.shared.nlu.training_data.features import Features",
      "",
      "[Added Lines]",
      "2: import os",
      "3: import tempfile",
      "4: from pathlib import Path",
      "17: from rasa.shared.nlu.training_data.features import (",
      "18:     Features,",
      "19:     FeatureMetadata,",
      "20:     save_features,",
      "21:     load_features,",
      "22: )",
      "25: @pytest.fixture",
      "26: def safe_tensors_tmp_file() -> str:",
      "27:     with tempfile.NamedTemporaryFile(delete=False, suffix=\".safetensors\") as f:",
      "28:         yield f.name",
      "29:     os.unlink(f.name)",
      "32: @pytest.fixture",
      "33: def dense_features() -> Features:",
      "34:     features_matrix = np.array([[1, 2, 3], [4, 5, 6]])",
      "35:     return Features(",
      "36:         features=features_matrix,",
      "37:         feature_type=\"dense\",",
      "38:         attribute=\"test\",",
      "39:         origin=\"test_origin\",",
      "40:     )",
      "43: @pytest.fixture",
      "44: def sparse_features() -> Features:",
      "45:     features_matrix = scipy.sparse.csr_matrix(",
      "46:         ([1, 2, 3], ([0, 1, 1], [0, 1, 2])), shape=(2, 3)",
      "47:     )",
      "48:     return Features(",
      "49:         features=features_matrix,",
      "50:         feature_type=\"sparse\",",
      "51:         attribute=\"test\",",
      "52:         origin=\"test_origin\",",
      "53:     )",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "467:         expected_origin = [\"origin-1\"]",
      "468:     with pytest.raises(ValueError, match=message):",
      "469:         Features.reduce(features_list, expected_origins=expected_origin)",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "511: def test_feature_metadata():",
      "512:     metadata = FeatureMetadata(",
      "513:         data_type=\"dense\",",
      "514:         attribute=\"text\",",
      "515:         origin=\"test\",",
      "516:         is_sparse=False,",
      "517:         shape=(10, 5),",
      "518:         safetensors_key=\"key_0\",",
      "519:     )",
      "521:     assert metadata.data_type == \"dense\"",
      "522:     assert metadata.attribute == \"text\"",
      "523:     assert metadata.origin == \"test\"",
      "524:     assert not metadata.is_sparse",
      "525:     assert metadata.shape == (10, 5)",
      "526:     assert metadata.safetensors_key == \"key_0\"",
      "529: def test_save_dense_features(safe_tensors_tmp_file: str, dense_features: Features):",
      "530:     features_dict = {\"test_key\": [dense_features]}",
      "531:     metadata = save_features(features_dict, safe_tensors_tmp_file)",
      "533:     assert \"test_key\" in metadata",
      "534:     assert len(metadata[\"test_key\"]) == 1",
      "535:     assert metadata[\"test_key\"][0][\"data_type\"] == \"dense\"",
      "536:     assert metadata[\"test_key\"][0][\"shape\"] == (2, 3)",
      "537:     assert not metadata[\"test_key\"][0][\"is_sparse\"]",
      "538:     assert Path(safe_tensors_tmp_file).exists()",
      "541: def test_save_sparse_features(safe_tensors_tmp_file: str, sparse_features: Features):",
      "542:     features_dict = {\"test_key\": [sparse_features]}",
      "543:     metadata = save_features(features_dict, safe_tensors_tmp_file)",
      "545:     assert \"test_key\" in metadata",
      "546:     assert len(metadata[\"test_key\"]) == 1",
      "547:     assert metadata[\"test_key\"][0][\"data_type\"] == \"sparse\"",
      "548:     assert metadata[\"test_key\"][0][\"shape\"] == (2, 3)",
      "549:     assert metadata[\"test_key\"][0][\"is_sparse\"]",
      "550:     assert Path(safe_tensors_tmp_file).exists()",
      "553: def test_save_mixed_features(",
      "554:     safe_tensors_tmp_file: str, dense_features: Features, sparse_features: Features",
      "555: ):",
      "556:     features_dict = {\"test_key\": [dense_features, sparse_features]}",
      "557:     metadata = save_features(features_dict, safe_tensors_tmp_file)",
      "559:     assert \"test_key\" in metadata",
      "560:     assert len(metadata[\"test_key\"]) == 2",
      "561:     assert metadata[\"test_key\"][0][\"data_type\"] == \"dense\"",
      "562:     assert metadata[\"test_key\"][1][\"data_type\"] == \"sparse\"",
      "563:     assert Path(safe_tensors_tmp_file).exists()",
      "566: def test_save_multiple_keys(",
      "567:     safe_tensors_tmp_file: str, dense_features: Features, sparse_features: Features",
      "568: ):",
      "569:     features_dict = {\"dense_key\": [dense_features], \"sparse_key\": [sparse_features]}",
      "570:     metadata = save_features(features_dict, safe_tensors_tmp_file)",
      "572:     assert \"dense_key\" in metadata",
      "573:     assert \"sparse_key\" in metadata",
      "574:     assert metadata[\"dense_key\"][0][\"data_type\"] == \"dense\"",
      "575:     assert metadata[\"sparse_key\"][0][\"data_type\"] == \"sparse\"",
      "576:     assert Path(safe_tensors_tmp_file).exists()",
      "579: @pytest.fixture",
      "580: def setup_save_load(",
      "581:     safe_tensors_tmp_file: str, dense_features: Features, sparse_features: Features",
      "582: ) -> Tuple[str, Dict[str, Any], Dict[str, List[Features]]]:",
      "583:     features_dict = {\"dense_key\": [dense_features], \"sparse_key\": [sparse_features]}",
      "584:     metadata = save_features(features_dict, safe_tensors_tmp_file)",
      "585:     return safe_tensors_tmp_file, metadata, features_dict",
      "588: def test_load_dense_features(",
      "589:     setup_save_load: Tuple[str, Dict[str, Any], Dict[str, List[Features]]],",
      "590: ):",
      "591:     temp_file, metadata, original_dict = setup_save_load",
      "592:     loaded_dict = load_features(temp_file, metadata)",
      "594:     assert \"dense_key\" in loaded_dict",
      "595:     assert len(loaded_dict[\"dense_key\"]) == 1",
      "596:     assert not loaded_dict[\"dense_key\"][0].is_sparse()",
      "597:     np.testing.assert_array_equal(",
      "598:         loaded_dict[\"dense_key\"][0].features, original_dict[\"dense_key\"][0].features",
      "599:     )",
      "602: def test_load_sparse_features(",
      "603:     setup_save_load: Tuple[str, Dict[str, Any], Dict[str, List[Features]]],",
      "604: ):",
      "605:     temp_file, metadata, original_dict = setup_save_load",
      "606:     loaded_dict = load_features(temp_file, metadata)",
      "608:     assert \"sparse_key\" in loaded_dict",
      "609:     assert len(loaded_dict[\"sparse_key\"]) == 1",
      "610:     assert loaded_dict[\"sparse_key\"][0].is_sparse()",
      "611:     assert (",
      "612:         loaded_dict[\"sparse_key\"][0].features != original_dict[\"sparse_key\"][0].features",
      "613:     ).nnz == 0",
      "616: def test_load_preserves_metadata(",
      "617:     setup_save_load: Tuple[str, Dict[str, Any], Dict[str, List[Features]]],",
      "618: ):",
      "619:     temp_file, metadata, original_dict = setup_save_load",
      "620:     loaded_dict = load_features(temp_file, metadata)",
      "622:     for key in original_dict:",
      "623:         for orig_feat, loaded_feat in zip(original_dict[key], loaded_dict[key]):",
      "624:             assert orig_feat.type == loaded_feat.type",
      "625:             assert orig_feat.attribute == loaded_feat.attribute",
      "626:             assert orig_feat.origin == loaded_feat.origin",
      "629: def test_load_nonexistent_file():",
      "630:     with pytest.raises(Exception):",
      "631:         load_features(\"nonexistent.safetensors\", {})",
      "634: def test_load_invalid_metadata(safe_tensors_tmp_file: str, dense_features: Features):",
      "635:     features_dict = {\"test_key\": [dense_features]}",
      "636:     metadata = save_features(features_dict, safe_tensors_tmp_file)",
      "638:     # Corrupt the metadata",
      "639:     metadata[\"test_key\"][0][\"safetensors_key\"] = \"invalid_key\"",
      "641:     with pytest.raises(Exception):",
      "642:         load_features(safe_tensors_tmp_file, metadata)",
      "645: def test_end_to_end(safe_tensors_tmp_file: str):",
      "646:     # Create test data",
      "647:     dense_matrix = np.array([[1, 2], [3, 4]])",
      "648:     sparse_matrix = scipy.sparse.csr_matrix(([1, 2], ([0, 1], [0, 1])), shape=(2, 2))",
      "650:     features_dict = {",
      "651:         \"group1\": [",
      "652:             Features(dense_matrix, \"dense\", \"test1\", \"origin1\"),",
      "653:             Features(sparse_matrix, \"sparse\", \"test2\", \"origin2\"),",
      "654:         ],",
      "655:         \"group2\": [",
      "656:             Features(dense_matrix * 2, \"dense\", \"test3\", [\"origin3\", \"origin4\"])",
      "657:         ],",
      "658:     }",
      "660:     # Save features",
      "661:     metadata = save_features(features_dict, safe_tensors_tmp_file)",
      "663:     # Load features",
      "664:     loaded_dict = load_features(safe_tensors_tmp_file, metadata)",
      "666:     # Verify structure",
      "667:     assert set(loaded_dict.keys()) == set(features_dict.keys())",
      "668:     assert len(loaded_dict[\"group1\"]) == 2",
      "669:     assert len(loaded_dict[\"group2\"]) == 1",
      "671:     # Verify dense features",
      "672:     np.testing.assert_array_equal(",
      "673:         loaded_dict[\"group1\"][0].features, features_dict[\"group1\"][0].features",
      "674:     )",
      "676:     # Verify sparse features",
      "677:     assert (",
      "678:         loaded_dict[\"group1\"][1].features != features_dict[\"group1\"][1].features",
      "679:     ).nnz == 0",
      "681:     # Verify metadata",
      "682:     assert loaded_dict[\"group1\"][0].type == \"dense\"",
      "683:     assert loaded_dict[\"group1\"][1].type == \"sparse\"",
      "684:     assert loaded_dict[\"group2\"][0].origin == [\"origin3\", \"origin4\"]",
      "",
      "---------------"
    ],
    "tests/utils/tensorflow/test_feature_array.py||tests/utils/tensorflow/test_feature_array.py": [
      "File: tests/utils/tensorflow/test_feature_array.py -> tests/utils/tensorflow/test_feature_array.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "[No context available]",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "1: import numpy as np",
      "2: import scipy.sparse",
      "4: from rasa.utils.tensorflow.feature_array import (",
      "5:     _recursive_serialize,",
      "6:     _serialize_nested_data,",
      "7:     _deserialize_nested_data,",
      "8: )",
      "9: from rasa.utils.tensorflow.model_data import RasaModelData",
      "12: def test_recursive_serialize_numpy_array():",
      "13:     data_dict = {}",
      "14:     metadata = []",
      "16:     _recursive_serialize(np.array([1, 2, 3]), \"test_array\", data_dict, metadata)",
      "17:     assert \"test_array_array\" in data_dict",
      "18:     assert metadata[0] == {\"type\": \"dense\", \"key\": \"test_array_array\", \"shape\": (3,)}",
      "21: def test_recursive_serialize_floats():",
      "22:     data_dict = {}",
      "23:     metadata = []",
      "25:     _recursive_serialize([1.0, 2.0, 3.0], \"test_list\", data_dict, metadata)",
      "26:     assert \"test_list_list\" in data_dict",
      "27:     assert metadata[0] == {\"type\": \"list\", \"key\": \"test_list_list\"}",
      "30: def test_recursive_serialize_sparse_matrix():",
      "31:     data_dict = {}",
      "32:     metadata = []",
      "34:     sparse_matrix = scipy.sparse.random(5, 10, density=0.1, format=\"coo\")",
      "35:     _recursive_serialize(sparse_matrix, \"test_sparse\", data_dict, metadata)",
      "36:     assert \"test_sparse_data\" in data_dict",
      "37:     assert \"test_sparse_row\" in data_dict",
      "38:     assert \"test_sparse_col\" in data_dict",
      "39:     assert metadata[0] == {",
      "40:         \"type\": \"sparse\",",
      "41:         \"key\": \"test_sparse\",",
      "42:         \"shape\": sparse_matrix.shape,",
      "43:     }",
      "46: def test_serialize_model_data(model_data: RasaModelData):",
      "47:     nested_data = model_data.data",
      "49:     data_dict = {}",
      "50:     metadata = []",
      "51:     _serialize_nested_data(nested_data, \"component\", data_dict, metadata)",
      "53:     assert len(metadata) == 5",
      "55:     assert metadata[0][\"key\"] == \"text\"",
      "56:     assert len(metadata[0][\"components\"]) == 1",
      "57:     assert metadata[0][\"components\"][0][\"key\"] == \"sentence\"",
      "58:     assert metadata[0][\"components\"][0][\"number_of_dimensions\"] == 3",
      "59:     assert len(metadata[0][\"components\"][0][\"features\"]) == 2",
      "60:     assert metadata[0][\"components\"][0][\"features\"][0][\"type\"] == \"group\"",
      "61:     assert len(metadata[0][\"components\"][0][\"features\"][0][\"subcomponents\"]) == 5",
      "62:     assert (",
      "63:         metadata[0][\"components\"][0][\"features\"][0][\"subcomponents\"][0][\"type\"]",
      "64:         == \"dense\"",
      "65:     )",
      "66:     assert metadata[0][\"components\"][0][\"features\"][0][\"subcomponents\"][0][\"shape\"] == (",
      "67:         5,",
      "68:         14,",
      "69:     )",
      "70:     assert metadata[0][\"components\"][0][\"features\"][1][\"type\"] == \"group\"",
      "71:     assert len(metadata[0][\"components\"][0][\"features\"][1][\"subcomponents\"]) == 5",
      "72:     assert (",
      "73:         metadata[0][\"components\"][0][\"features\"][1][\"subcomponents\"][0][\"type\"]",
      "74:         == \"sparse\"",
      "75:     )",
      "76:     assert metadata[0][\"components\"][0][\"features\"][1][\"subcomponents\"][0][\"shape\"] == (",
      "77:         5,",
      "78:         10,",
      "79:     )",
      "81:     assert metadata[3][\"key\"] == \"label\"",
      "82:     assert len(metadata[3][\"components\"]) == 1",
      "83:     assert metadata[3][\"components\"][0][\"key\"] == \"ids\"",
      "84:     assert metadata[3][\"components\"][0][\"number_of_dimensions\"] == 1",
      "85:     assert metadata[3][\"components\"][0][\"features\"][0][\"type\"] == \"list\"",
      "86:     assert (",
      "87:         metadata[3][\"components\"][0][\"features\"][0][\"key\"]",
      "88:         == \"component_label_ids_0_list\"",
      "89:     )",
      "91:     assert len(data_dict) == 87",
      "92:     assert (",
      "93:         data_dict[\"component_label_ids_0_list\"]",
      "94:         == model_data.data[\"label\"][\"ids\"][0].view(np.ndarray)",
      "95:     ).all()",
      "98: def test_serialize_and_deserialize_model_data(model_data: RasaModelData):",
      "99:     actual_data = model_data.data",
      "101:     data_dict = {}",
      "102:     metadata = []",
      "103:     _serialize_nested_data(actual_data, \"component\", data_dict, metadata)",
      "105:     loaded_data = _deserialize_nested_data(metadata, data_dict)",
      "107:     assert len(actual_data) == len(loaded_data)",
      "109:     assert len(actual_data[\"text\"][\"sentence\"]) == len(loaded_data[\"text\"][\"sentence\"])",
      "111:     # text.sentence has a dimension of 3",
      "112:     assert len(actual_data[\"text\"][\"sentence\"][0]) == len(",
      "113:         loaded_data[\"text\"][\"sentence\"][0]",
      "114:     )",
      "115:     # assert that the numpy arrays of the actual and loaded data in",
      "116:     # text.sentence are the same",
      "117:     for i in range(0, 5):",
      "118:         assert (",
      "119:             actual_data[\"text\"][\"sentence\"][0][i]",
      "120:             == loaded_data[\"text\"][\"sentence\"][0][i]",
      "121:         ).all()",
      "122:     assert len(actual_data[\"text\"][\"sentence\"][1]) == len(",
      "123:         loaded_data[\"text\"][\"sentence\"][1]",
      "124:     )",
      "125:     # assert that the sparse matrices of the actual and loaded data in",
      "126:     # text.sentence are the same",
      "127:     for i in range(0, 5):",
      "128:         assert (",
      "129:             actual_data[\"text\"][\"sentence\"][1][i]",
      "130:             == loaded_data[\"text\"][\"sentence\"][1][i]",
      "131:         ).data.all()",
      "133:     # action_text.sequence has a dimension of 4",
      "134:     assert len(actual_data[\"action_text\"][\"sequence\"]) == len(",
      "135:         loaded_data[\"action_text\"][\"sequence\"]",
      "136:     )",
      "137:     assert len(actual_data[\"action_text\"][\"sequence\"][0]) == len(",
      "138:         loaded_data[\"action_text\"][\"sequence\"][0]",
      "139:     )",
      "140:     # assert that the sparse matrices of the actual and loaded data in",
      "141:     # action_text.sequence are the same",
      "142:     for i in range(0, 5):",
      "143:         for j in range(0, len(actual_data[\"action_text\"][\"sequence\"][0][i])):",
      "144:             assert (",
      "145:                 actual_data[\"action_text\"][\"sequence\"][0][i][j]",
      "146:                 == loaded_data[\"action_text\"][\"sequence\"][0][i][j]",
      "147:             ).data.all()",
      "148:     assert len(actual_data[\"action_text\"][\"sequence\"][1]) == len(",
      "149:         loaded_data[\"action_text\"][\"sequence\"][1]",
      "150:     )",
      "151:     # assert that the numpy array of the actual and loaded data in",
      "152:     # action_text.sequence are the same",
      "153:     for i in range(0, 5):",
      "154:         for j in range(0, len(actual_data[\"action_text\"][\"sequence\"][1][i])):",
      "155:             assert (",
      "156:                 actual_data[\"action_text\"][\"sequence\"][1][i][j]",
      "157:                 == loaded_data[\"action_text\"][\"sequence\"][1][i][j]",
      "158:             ).all()",
      "160:     # dialogue.sentence has a dimension of 3",
      "161:     assert len(actual_data[\"dialogue\"][\"sentence\"]) == len(",
      "162:         loaded_data[\"dialogue\"][\"sentence\"]",
      "163:     )",
      "164:     assert len(actual_data[\"dialogue\"][\"sentence\"][0]) == len(",
      "165:         loaded_data[\"dialogue\"][\"sentence\"][0]",
      "166:     )",
      "167:     # assert that the numpy array of the actual and loaded data in",
      "168:     # dialogue.sentence are the same",
      "169:     for i in range(0, 5):",
      "170:         assert (",
      "171:             actual_data[\"dialogue\"][\"sentence\"][0][i]",
      "172:             == loaded_data[\"dialogue\"][\"sentence\"][0][i]",
      "173:         ).all()",
      "175:     # label.ids has a dimension of 4",
      "176:     assert len(actual_data[\"label\"][\"ids\"]) == len(loaded_data[\"label\"][\"ids\"])",
      "177:     # assert that the numpy array of the actual and loaded data in",
      "178:     # label.ids are the same",
      "179:     assert (",
      "180:         actual_data[\"label\"][\"ids\"][0].view(np.ndarray)",
      "181:         == loaded_data[\"label\"][\"ids\"][0].view(np.ndarray)",
      "182:     ).all()",
      "184:     # entities.tag_ids has a dimension of 3",
      "185:     assert len(actual_data[\"entities\"][\"tag_ids\"]) == len(",
      "186:         loaded_data[\"entities\"][\"tag_ids\"]",
      "187:     )",
      "188:     assert len(actual_data[\"entities\"][\"tag_ids\"][0]) == len(",
      "189:         loaded_data[\"entities\"][\"tag_ids\"][0]",
      "190:     )",
      "191:     # assert that the numpy array of the actual and loaded data in",
      "192:     # entities.tag_ids are the same",
      "193:     for i in range(0, 5):",
      "194:         assert (",
      "195:             actual_data[\"entities\"][\"tag_ids\"][0][i]",
      "196:             == loaded_data[\"entities\"][\"tag_ids\"][0][i]",
      "197:         ).all()",
      "",
      "---------------"
    ],
    "tests/utils/test_io.py||tests/utils/test_io.py": [
      "File: tests/utils/test_io.py -> tests/utils/test_io.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "3: import pytest",
      "4: from _pytest.tmpdir import TempPathFactory",
      "5: from prompt_toolkit.document import Document",
      "",
      "[Removed Lines]",
      "1: from pathlib import Path",
      "2: from typing import Dict, Text",
      "",
      "[Added Lines]",
      "[None]",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "71:     assert e.value.message == error_message",
      "90: def test_empty_directories_are_equal(tmp_path_factory: TempPathFactory):",
      "91:     dir1 = tmp_path_factory.mktemp(\"dir1\")",
      "92:     dir2 = tmp_path_factory.mktemp(\"dir2\")",
      "",
      "[Removed Lines]",
      "74: @pytest.mark.parametrize(",
      "75:     \"input,kwargs,expected\",",
      "76:     [",
      "77:         ({(1, 2): 3}, {}, {repr((1, 2)): 3}),",
      "78:         ({(1, 2): 3}, {\"encode_non_string_keys\": True}, {(1, 2): 3}),",
      "79:     ],",
      "80: )",
      "81: def test_write_and_load_dict_via_jsonpickle(",
      "82:     tmp_path: Path, input: Dict, kwargs: Dict[Text, bool], expected: Dict",
      "83: ):",
      "84:     file_name = tmp_path / \"bla.pkl\"",
      "85:     rasa.utils.io.json_pickle(file_name=file_name, obj=input, **kwargs)",
      "86:     loaded = rasa.utils.io.json_unpickle(file_name=file_name, **kwargs)",
      "87:     assert loaded == expected",
      "",
      "[Added Lines]",
      "[None]",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "71907afc360f4dfb838189530fe2dbee55d136a9",
      "candidate_info": {
        "commit_hash": "71907afc360f4dfb838189530fe2dbee55d136a9",
        "repo": "RasaHQ/rasa",
        "commit_url": "https://github.com/RasaHQ/rasa/commit/71907afc360f4dfb838189530fe2dbee55d136a9",
        "files": [
          "poetry.lock",
          "pyproject.toml"
        ],
        "message": "Bump tensorflow-metal from 0.8.0 to 1.2.0\n\nBumps [tensorflow-metal](https://developer.apple.com/metal/tensorflow-plugin/) from 0.8.0 to 1.2.0.\n\n---\nupdated-dependencies:\n- dependency-name: tensorflow-metal\n  dependency-type: direct:production\n  update-type: version-update:semver-major\n...\n\nSigned-off-by: dependabot[bot] <support@github.com>",
        "before_after_code_files": [
          "poetry.lock||poetry.lock"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "poetry.lock||poetry.lock"
          ],
          "candidate": [
            "poetry.lock||poetry.lock"
          ]
        }
      },
      "candidate_diff": {
        "poetry.lock||poetry.lock": [
          "File: poetry.lock -> poetry.lock",
          "--- Hunk 1 ---",
          "[Context before]",
          "3: [[package]]",
          "4: name = \"absl-py\"",
          "",
          "[Removed Lines]",
          "1: # This file is automatically @generated by Poetry 1.8.2 and should not be changed by hand.",
          "",
          "[Added Lines]",
          "1: # This file is automatically @generated by Poetry 1.8.5 and should not be changed by hand.",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "5962: [[package]]",
          "5963: name = \"tensorflow-metal\"",
          "5965: description = \"TensorFlow acceleration for Mac GPUs.\"",
          "5966: optional = true",
          "5967: python-versions = \"*\"",
          "5968: files = [",
          "5977: ]",
          "5979: [package.dependencies]",
          "",
          "[Removed Lines]",
          "5964: version = \"0.8.0\"",
          "5969:     {file = \"tensorflow_metal-0.8.0-cp310-cp310-macosx_12_0_arm64.whl\", hash = \"sha256:bbbb28df62e20d707420f4c52d04bf87f5e5990fa36ad3ad49cf43883b2a2e15\"},",
          "5970:     {file = \"tensorflow_metal-0.8.0-cp310-cp310-macosx_12_0_x86_64.whl\", hash = \"sha256:cafbfc62cd24cc6fbde64ab29a2981b87eee489534ba8f01e8609582312e62bd\"},",
          "5971:     {file = \"tensorflow_metal-0.8.0-cp311-cp311-macosx_12_0_arm64.whl\", hash = \"sha256:2a6a11a2f702153bb0b6bf44288ba7095b8e0999aba5c9aad219931a15ecf860\"},",
          "5972:     {file = \"tensorflow_metal-0.8.0-cp311-cp311-macosx_12_0_x86_64.whl\", hash = \"sha256:6b75d4e519e763d710ed6eb6a20adf98034f1399251df288a410462f0712da32\"},",
          "5973:     {file = \"tensorflow_metal-0.8.0-cp38-cp38-macosx_12_0_arm64.whl\", hash = \"sha256:b6e6b0b516be4d52c9149436bdd99df85f1e6a3453ff811d59b5eef1e2368834\"},",
          "5974:     {file = \"tensorflow_metal-0.8.0-cp38-cp38-macosx_12_0_x86_64.whl\", hash = \"sha256:15d5fa9fec83c96940e8588e6b4140cda53f859778af713c617dc8be86025636\"},",
          "5975:     {file = \"tensorflow_metal-0.8.0-cp39-cp39-macosx_12_0_arm64.whl\", hash = \"sha256:8a4ef5f4804d4becf9c59d376512b1f160477e408c5d00cd4f647c6d64728531\"},",
          "5976:     {file = \"tensorflow_metal-0.8.0-cp39-cp39-macosx_12_0_x86_64.whl\", hash = \"sha256:6172d4deed9ab87146d7392ccba5c0ff5511deb51c8af2de89cad31720380741\"},",
          "",
          "[Added Lines]",
          "5964: version = \"1.2.0\"",
          "5969:     {file = \"tensorflow_metal-1.2.0-cp310-cp310-macosx_12_0_arm64.whl\", hash = \"sha256:bc735e36c97874e41f77ec2e7421ff745d2ec36ee641141c8091e4cc2dbcc819\"},",
          "5970:     {file = \"tensorflow_metal-1.2.0-cp311-cp311-macosx_12_0_arm64.whl\", hash = \"sha256:5fa7cee627031c14f45bd97ff0ef422cd6c3866199ff99cf29b94db6674ceb42\"},",
          "5971:     {file = \"tensorflow_metal-1.2.0-cp312-cp312-macosx_12_0_arm64.whl\", hash = \"sha256:4bece0ecb154b713b9f5ad4aec676a366d312822161e3cf0e1dea737c64cec04\"},",
          "5972:     {file = \"tensorflow_metal-1.2.0-cp39-cp39-macosx_12_0_arm64.whl\", hash = \"sha256:4c52a9f7768b10ac226be758e953973734707161caaee328bd2da0cdf93c4643\"},",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "7186: [metadata]",
          "7187: lock-version = \"2.0\"",
          "7188: python-versions = \">=3.8,<3.11\"",
          "",
          "[Removed Lines]",
          "7189: content-hash = \"c1c51259ab3b886039dcf7eb746a45815d4b8afcaa4bdbe179c891810aee553f\"",
          "",
          "[Added Lines]",
          "7185: content-hash = \"5e80b409c20f10c8f34a81036b97479858e5b31af55bcf80d26fbb0393ab439c\"",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "6a2820cbe27d195b3dd6b500ac32d59f7c15e105",
      "candidate_info": {
        "commit_hash": "6a2820cbe27d195b3dd6b500ac32d59f7c15e105",
        "repo": "RasaHQ/rasa",
        "commit_url": "https://github.com/RasaHQ/rasa/commit/6a2820cbe27d195b3dd6b500ac32d59f7c15e105",
        "files": [
          "poetry.lock",
          "pyproject.toml"
        ],
        "message": "Bump aiohttp from 3.9.3 to 3.10.11\n\n---\nupdated-dependencies:\n- dependency-name: aiohttp\n  dependency-type: direct:production\n...\n\nSigned-off-by: dependabot[bot] <support@github.com>",
        "before_after_code_files": [
          "poetry.lock||poetry.lock"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "poetry.lock||poetry.lock"
          ],
          "candidate": [
            "poetry.lock||poetry.lock"
          ]
        }
      },
      "candidate_diff": {
        "poetry.lock||poetry.lock": [
          "File: poetry.lock -> poetry.lock",
          "--- Hunk 1 ---",
          "[Context before]",
          "3: [[package]]",
          "4: name = \"absl-py\"",
          "",
          "[Removed Lines]",
          "1: # This file is automatically @generated by Poetry 1.8.2 and should not be changed by hand.",
          "",
          "[Added Lines]",
          "1: # This file is automatically @generated by Poetry 1.8.5 and should not be changed by hand.",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "61: proxy = [\"aiohttp-socks (>=0.5.3,<0.6.0)\"]",
          "63: [[package]]",
          "67: optional = false",
          "68: python-versions = \">=3.8\"",
          "69: files = [",
          "146: ]",
          "149: aiosignal = \">=1.1.2\"",
          "151: attrs = \">=17.3.0\"",
          "152: frozenlist = \">=1.1.1\"",
          "153: multidict = \">=4.5,<7.0\"",
          "156: [package.extras]",
          "159: [[package]]",
          "160: name = \"aiohttp-retry\"",
          "",
          "[Removed Lines]",
          "64: name = \"aiohttp\"",
          "65: version = \"3.9.3\"",
          "66: description = \"Async http client/server framework (asyncio)\"",
          "70:     {file = \"aiohttp-3.9.3-cp310-cp310-macosx_10_9_universal2.whl\", hash = \"sha256:939677b61f9d72a4fa2a042a5eee2a99a24001a67c13da113b2e30396567db54\"},",
          "71:     {file = \"aiohttp-3.9.3-cp310-cp310-macosx_10_9_x86_64.whl\", hash = \"sha256:1f5cd333fcf7590a18334c90f8c9147c837a6ec8a178e88d90a9b96ea03194cc\"},",
          "72:     {file = \"aiohttp-3.9.3-cp310-cp310-macosx_11_0_arm64.whl\", hash = \"sha256:82e6aa28dd46374f72093eda8bcd142f7771ee1eb9d1e223ff0fa7177a96b4a5\"},",
          "73:     {file = \"aiohttp-3.9.3-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:f56455b0c2c7cc3b0c584815264461d07b177f903a04481dfc33e08a89f0c26b\"},",
          "74:     {file = \"aiohttp-3.9.3-cp310-cp310-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:bca77a198bb6e69795ef2f09a5f4c12758487f83f33d63acde5f0d4919815768\"},",
          "75:     {file = \"aiohttp-3.9.3-cp310-cp310-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:e083c285857b78ee21a96ba1eb1b5339733c3563f72980728ca2b08b53826ca5\"},",
          "76:     {file = \"aiohttp-3.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:ab40e6251c3873d86ea9b30a1ac6d7478c09277b32e14745d0d3c6e76e3c7e29\"},",
          "77:     {file = \"aiohttp-3.9.3-cp310-cp310-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:df822ee7feaaeffb99c1a9e5e608800bd8eda6e5f18f5cfb0dc7eeb2eaa6bbec\"},",
          "78:     {file = \"aiohttp-3.9.3-cp310-cp310-musllinux_1_1_aarch64.whl\", hash = \"sha256:acef0899fea7492145d2bbaaaec7b345c87753168589cc7faf0afec9afe9b747\"},",
          "79:     {file = \"aiohttp-3.9.3-cp310-cp310-musllinux_1_1_i686.whl\", hash = \"sha256:cd73265a9e5ea618014802ab01babf1940cecb90c9762d8b9e7d2cc1e1969ec6\"},",
          "80:     {file = \"aiohttp-3.9.3-cp310-cp310-musllinux_1_1_ppc64le.whl\", hash = \"sha256:a78ed8a53a1221393d9637c01870248a6f4ea5b214a59a92a36f18151739452c\"},",
          "81:     {file = \"aiohttp-3.9.3-cp310-cp310-musllinux_1_1_s390x.whl\", hash = \"sha256:6b0e029353361f1746bac2e4cc19b32f972ec03f0f943b390c4ab3371840aabf\"},",
          "82:     {file = \"aiohttp-3.9.3-cp310-cp310-musllinux_1_1_x86_64.whl\", hash = \"sha256:7cf5c9458e1e90e3c390c2639f1017a0379a99a94fdfad3a1fd966a2874bba52\"},",
          "83:     {file = \"aiohttp-3.9.3-cp310-cp310-win32.whl\", hash = \"sha256:3e59c23c52765951b69ec45ddbbc9403a8761ee6f57253250c6e1536cacc758b\"},",
          "84:     {file = \"aiohttp-3.9.3-cp310-cp310-win_amd64.whl\", hash = \"sha256:055ce4f74b82551678291473f66dc9fb9048a50d8324278751926ff0ae7715e5\"},",
          "85:     {file = \"aiohttp-3.9.3-cp311-cp311-macosx_10_9_universal2.whl\", hash = \"sha256:6b88f9386ff1ad91ace19d2a1c0225896e28815ee09fc6a8932fded8cda97c3d\"},",
          "86:     {file = \"aiohttp-3.9.3-cp311-cp311-macosx_10_9_x86_64.whl\", hash = \"sha256:c46956ed82961e31557b6857a5ca153c67e5476972e5f7190015018760938da2\"},",
          "87:     {file = \"aiohttp-3.9.3-cp311-cp311-macosx_11_0_arm64.whl\", hash = \"sha256:07b837ef0d2f252f96009e9b8435ec1fef68ef8b1461933253d318748ec1acdc\"},",
          "88:     {file = \"aiohttp-3.9.3-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:dad46e6f620574b3b4801c68255492e0159d1712271cc99d8bdf35f2043ec266\"},",
          "89:     {file = \"aiohttp-3.9.3-cp311-cp311-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:5ed3e046ea7b14938112ccd53d91c1539af3e6679b222f9469981e3dac7ba1ce\"},",
          "90:     {file = \"aiohttp-3.9.3-cp311-cp311-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:039df344b45ae0b34ac885ab5b53940b174530d4dd8a14ed8b0e2155b9dddccb\"},",
          "91:     {file = \"aiohttp-3.9.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:7943c414d3a8d9235f5f15c22ace69787c140c80b718dcd57caaade95f7cd93b\"},",
          "92:     {file = \"aiohttp-3.9.3-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:84871a243359bb42c12728f04d181a389718710129b36b6aad0fc4655a7647d4\"},",
          "93:     {file = \"aiohttp-3.9.3-cp311-cp311-musllinux_1_1_aarch64.whl\", hash = \"sha256:5eafe2c065df5401ba06821b9a054d9cb2848867f3c59801b5d07a0be3a380ae\"},",
          "94:     {file = \"aiohttp-3.9.3-cp311-cp311-musllinux_1_1_i686.whl\", hash = \"sha256:9d3c9b50f19704552f23b4eaea1fc082fdd82c63429a6506446cbd8737823da3\"},",
          "95:     {file = \"aiohttp-3.9.3-cp311-cp311-musllinux_1_1_ppc64le.whl\", hash = \"sha256:f033d80bc6283092613882dfe40419c6a6a1527e04fc69350e87a9df02bbc283\"},",
          "96:     {file = \"aiohttp-3.9.3-cp311-cp311-musllinux_1_1_s390x.whl\", hash = \"sha256:2c895a656dd7e061b2fd6bb77d971cc38f2afc277229ce7dd3552de8313a483e\"},",
          "97:     {file = \"aiohttp-3.9.3-cp311-cp311-musllinux_1_1_x86_64.whl\", hash = \"sha256:1f5a71d25cd8106eab05f8704cd9167b6e5187bcdf8f090a66c6d88b634802b4\"},",
          "98:     {file = \"aiohttp-3.9.3-cp311-cp311-win32.whl\", hash = \"sha256:50fca156d718f8ced687a373f9e140c1bb765ca16e3d6f4fe116e3df7c05b2c5\"},",
          "99:     {file = \"aiohttp-3.9.3-cp311-cp311-win_amd64.whl\", hash = \"sha256:5fe9ce6c09668063b8447f85d43b8d1c4e5d3d7e92c63173e6180b2ac5d46dd8\"},",
          "100:     {file = \"aiohttp-3.9.3-cp312-cp312-macosx_10_9_universal2.whl\", hash = \"sha256:38a19bc3b686ad55804ae931012f78f7a534cce165d089a2059f658f6c91fa60\"},",
          "101:     {file = \"aiohttp-3.9.3-cp312-cp312-macosx_10_9_x86_64.whl\", hash = \"sha256:770d015888c2a598b377bd2f663adfd947d78c0124cfe7b959e1ef39f5b13869\"},",
          "102:     {file = \"aiohttp-3.9.3-cp312-cp312-macosx_11_0_arm64.whl\", hash = \"sha256:ee43080e75fc92bf36219926c8e6de497f9b247301bbf88c5c7593d931426679\"},",
          "103:     {file = \"aiohttp-3.9.3-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:52df73f14ed99cee84865b95a3d9e044f226320a87af208f068ecc33e0c35b96\"},",
          "104:     {file = \"aiohttp-3.9.3-cp312-cp312-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:dc9b311743a78043b26ffaeeb9715dc360335e5517832f5a8e339f8a43581e4d\"},",
          "105:     {file = \"aiohttp-3.9.3-cp312-cp312-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:b955ed993491f1a5da7f92e98d5dad3c1e14dc175f74517c4e610b1f2456fb11\"},",
          "106:     {file = \"aiohttp-3.9.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:504b6981675ace64c28bf4a05a508af5cde526e36492c98916127f5a02354d53\"},",
          "107:     {file = \"aiohttp-3.9.3-cp312-cp312-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:a6fe5571784af92b6bc2fda8d1925cccdf24642d49546d3144948a6a1ed58ca5\"},",
          "108:     {file = \"aiohttp-3.9.3-cp312-cp312-musllinux_1_1_aarch64.whl\", hash = \"sha256:ba39e9c8627edc56544c8628cc180d88605df3892beeb2b94c9bc857774848ca\"},",
          "109:     {file = \"aiohttp-3.9.3-cp312-cp312-musllinux_1_1_i686.whl\", hash = \"sha256:e5e46b578c0e9db71d04c4b506a2121c0cb371dd89af17a0586ff6769d4c58c1\"},",
          "110:     {file = \"aiohttp-3.9.3-cp312-cp312-musllinux_1_1_ppc64le.whl\", hash = \"sha256:938a9653e1e0c592053f815f7028e41a3062e902095e5a7dc84617c87267ebd5\"},",
          "111:     {file = \"aiohttp-3.9.3-cp312-cp312-musllinux_1_1_s390x.whl\", hash = \"sha256:c3452ea726c76e92f3b9fae4b34a151981a9ec0a4847a627c43d71a15ac32aa6\"},",
          "112:     {file = \"aiohttp-3.9.3-cp312-cp312-musllinux_1_1_x86_64.whl\", hash = \"sha256:ff30218887e62209942f91ac1be902cc80cddb86bf00fbc6783b7a43b2bea26f\"},",
          "113:     {file = \"aiohttp-3.9.3-cp312-cp312-win32.whl\", hash = \"sha256:38f307b41e0bea3294a9a2a87833191e4bcf89bb0365e83a8be3a58b31fb7f38\"},",
          "114:     {file = \"aiohttp-3.9.3-cp312-cp312-win_amd64.whl\", hash = \"sha256:b791a3143681a520c0a17e26ae7465f1b6f99461a28019d1a2f425236e6eedb5\"},",
          "115:     {file = \"aiohttp-3.9.3-cp38-cp38-macosx_10_9_universal2.whl\", hash = \"sha256:0ed621426d961df79aa3b963ac7af0d40392956ffa9be022024cd16297b30c8c\"},",
          "116:     {file = \"aiohttp-3.9.3-cp38-cp38-macosx_10_9_x86_64.whl\", hash = \"sha256:7f46acd6a194287b7e41e87957bfe2ad1ad88318d447caf5b090012f2c5bb528\"},",
          "117:     {file = \"aiohttp-3.9.3-cp38-cp38-macosx_11_0_arm64.whl\", hash = \"sha256:feeb18a801aacb098220e2c3eea59a512362eb408d4afd0c242044c33ad6d542\"},",
          "118:     {file = \"aiohttp-3.9.3-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:f734e38fd8666f53da904c52a23ce517f1b07722118d750405af7e4123933511\"},",
          "119:     {file = \"aiohttp-3.9.3-cp38-cp38-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:b40670ec7e2156d8e57f70aec34a7216407848dfe6c693ef131ddf6e76feb672\"},",
          "120:     {file = \"aiohttp-3.9.3-cp38-cp38-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:fdd215b7b7fd4a53994f238d0f46b7ba4ac4c0adb12452beee724ddd0743ae5d\"},",
          "121:     {file = \"aiohttp-3.9.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:017a21b0df49039c8f46ca0971b3a7fdc1f56741ab1240cb90ca408049766168\"},",
          "122:     {file = \"aiohttp-3.9.3-cp38-cp38-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:e99abf0bba688259a496f966211c49a514e65afa9b3073a1fcee08856e04425b\"},",
          "123:     {file = \"aiohttp-3.9.3-cp38-cp38-musllinux_1_1_aarch64.whl\", hash = \"sha256:648056db9a9fa565d3fa851880f99f45e3f9a771dd3ff3bb0c048ea83fb28194\"},",
          "124:     {file = \"aiohttp-3.9.3-cp38-cp38-musllinux_1_1_i686.whl\", hash = \"sha256:8aacb477dc26797ee089721536a292a664846489c49d3ef9725f992449eda5a8\"},",
          "125:     {file = \"aiohttp-3.9.3-cp38-cp38-musllinux_1_1_ppc64le.whl\", hash = \"sha256:522a11c934ea660ff8953eda090dcd2154d367dec1ae3c540aff9f8a5c109ab4\"},",
          "126:     {file = \"aiohttp-3.9.3-cp38-cp38-musllinux_1_1_s390x.whl\", hash = \"sha256:5bce0dc147ca85caa5d33debc4f4d65e8e8b5c97c7f9f660f215fa74fc49a321\"},",
          "127:     {file = \"aiohttp-3.9.3-cp38-cp38-musllinux_1_1_x86_64.whl\", hash = \"sha256:4b4af9f25b49a7be47c0972139e59ec0e8285c371049df1a63b6ca81fdd216a2\"},",
          "128:     {file = \"aiohttp-3.9.3-cp38-cp38-win32.whl\", hash = \"sha256:298abd678033b8571995650ccee753d9458dfa0377be4dba91e4491da3f2be63\"},",
          "129:     {file = \"aiohttp-3.9.3-cp38-cp38-win_amd64.whl\", hash = \"sha256:69361bfdca5468c0488d7017b9b1e5ce769d40b46a9f4a2eed26b78619e9396c\"},",
          "130:     {file = \"aiohttp-3.9.3-cp39-cp39-macosx_10_9_universal2.whl\", hash = \"sha256:0fa43c32d1643f518491d9d3a730f85f5bbaedcbd7fbcae27435bb8b7a061b29\"},",
          "131:     {file = \"aiohttp-3.9.3-cp39-cp39-macosx_10_9_x86_64.whl\", hash = \"sha256:835a55b7ca49468aaaac0b217092dfdff370e6c215c9224c52f30daaa735c1c1\"},",
          "132:     {file = \"aiohttp-3.9.3-cp39-cp39-macosx_11_0_arm64.whl\", hash = \"sha256:06a9b2c8837d9a94fae16c6223acc14b4dfdff216ab9b7202e07a9a09541168f\"},",
          "133:     {file = \"aiohttp-3.9.3-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:abf151955990d23f84205286938796c55ff11bbfb4ccfada8c9c83ae6b3c89a3\"},",
          "134:     {file = \"aiohttp-3.9.3-cp39-cp39-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:59c26c95975f26e662ca78fdf543d4eeaef70e533a672b4113dd888bd2423caa\"},",
          "135:     {file = \"aiohttp-3.9.3-cp39-cp39-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:f95511dd5d0e05fd9728bac4096319f80615aaef4acbecb35a990afebe953b0e\"},",
          "136:     {file = \"aiohttp-3.9.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:595f105710293e76b9dc09f52e0dd896bd064a79346234b521f6b968ffdd8e58\"},",
          "137:     {file = \"aiohttp-3.9.3-cp39-cp39-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:c7c8b816c2b5af5c8a436df44ca08258fc1a13b449393a91484225fcb7545533\"},",
          "138:     {file = \"aiohttp-3.9.3-cp39-cp39-musllinux_1_1_aarch64.whl\", hash = \"sha256:f1088fa100bf46e7b398ffd9904f4808a0612e1d966b4aa43baa535d1b6341eb\"},",
          "139:     {file = \"aiohttp-3.9.3-cp39-cp39-musllinux_1_1_i686.whl\", hash = \"sha256:f59dfe57bb1ec82ac0698ebfcdb7bcd0e99c255bd637ff613760d5f33e7c81b3\"},",
          "140:     {file = \"aiohttp-3.9.3-cp39-cp39-musllinux_1_1_ppc64le.whl\", hash = \"sha256:361a1026c9dd4aba0109e4040e2aecf9884f5cfe1b1b1bd3d09419c205e2e53d\"},",
          "141:     {file = \"aiohttp-3.9.3-cp39-cp39-musllinux_1_1_s390x.whl\", hash = \"sha256:363afe77cfcbe3a36353d8ea133e904b108feea505aa4792dad6585a8192c55a\"},",
          "142:     {file = \"aiohttp-3.9.3-cp39-cp39-musllinux_1_1_x86_64.whl\", hash = \"sha256:8e2c45c208c62e955e8256949eb225bd8b66a4c9b6865729a786f2aa79b72e9d\"},",
          "143:     {file = \"aiohttp-3.9.3-cp39-cp39-win32.whl\", hash = \"sha256:f7217af2e14da0856e082e96ff637f14ae45c10a5714b63c77f26d8884cf1051\"},",
          "144:     {file = \"aiohttp-3.9.3-cp39-cp39-win_amd64.whl\", hash = \"sha256:27468897f628c627230dba07ec65dc8d0db566923c48f29e084ce382119802bc\"},",
          "145:     {file = \"aiohttp-3.9.3.tar.gz\", hash = \"sha256:90842933e5d1ff760fae6caca4b2b3edba53ba8f4b71e95dacf2818a2aca06f7\"},",
          "148: [package.dependencies]",
          "150: async-timeout = {version = \">=4.0,<5.0\", markers = \"python_version < \\\"3.11\\\"\"}",
          "154: yarl = \">=1.0,<2.0\"",
          "157: speedups = [\"Brotli\", \"aiodns\", \"brotlicffi\"]",
          "",
          "[Added Lines]",
          "64: name = \"aiohappyeyeballs\"",
          "65: version = \"2.4.4\"",
          "66: description = \"Happy Eyeballs for asyncio\"",
          "70:     {file = \"aiohappyeyeballs-2.4.4-py3-none-any.whl\", hash = \"sha256:a980909d50efcd44795c4afeca523296716d50cd756ddca6af8c65b996e27de8\"},",
          "71:     {file = \"aiohappyeyeballs-2.4.4.tar.gz\", hash = \"sha256:5fdd7d87889c63183afc18ce9271f9b0a7d32c2303e394468dd45d514a757745\"},",
          "74: [[package]]",
          "75: name = \"aiohttp\"",
          "76: version = \"3.10.11\"",
          "77: description = \"Async http client/server framework (asyncio)\"",
          "78: optional = false",
          "79: python-versions = \">=3.8\"",
          "80: files = [",
          "81:     {file = \"aiohttp-3.10.11-cp310-cp310-macosx_10_9_universal2.whl\", hash = \"sha256:5077b1a5f40ffa3ba1f40d537d3bec4383988ee51fbba6b74aa8fb1bc466599e\"},",
          "82:     {file = \"aiohttp-3.10.11-cp310-cp310-macosx_10_9_x86_64.whl\", hash = \"sha256:8d6a14a4d93b5b3c2891fca94fa9d41b2322a68194422bef0dd5ec1e57d7d298\"},",
          "83:     {file = \"aiohttp-3.10.11-cp310-cp310-macosx_11_0_arm64.whl\", hash = \"sha256:ffbfde2443696345e23a3c597049b1dd43049bb65337837574205e7368472177\"},",
          "84:     {file = \"aiohttp-3.10.11-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:20b3d9e416774d41813bc02fdc0663379c01817b0874b932b81c7f777f67b217\"},",
          "85:     {file = \"aiohttp-3.10.11-cp310-cp310-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:2b943011b45ee6bf74b22245c6faab736363678e910504dd7531a58c76c9015a\"},",
          "86:     {file = \"aiohttp-3.10.11-cp310-cp310-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:48bc1d924490f0d0b3658fe5c4b081a4d56ebb58af80a6729d4bd13ea569797a\"},",
          "87:     {file = \"aiohttp-3.10.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:e12eb3f4b1f72aaaf6acd27d045753b18101524f72ae071ae1c91c1cd44ef115\"},",
          "88:     {file = \"aiohttp-3.10.11-cp310-cp310-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:f14ebc419a568c2eff3c1ed35f634435c24ead2fe19c07426af41e7adb68713a\"},",
          "89:     {file = \"aiohttp-3.10.11-cp310-cp310-musllinux_1_2_aarch64.whl\", hash = \"sha256:72b191cdf35a518bfc7ca87d770d30941decc5aaf897ec8b484eb5cc8c7706f3\"},",
          "90:     {file = \"aiohttp-3.10.11-cp310-cp310-musllinux_1_2_i686.whl\", hash = \"sha256:5ab2328a61fdc86424ee540d0aeb8b73bbcad7351fb7cf7a6546fc0bcffa0038\"},",
          "91:     {file = \"aiohttp-3.10.11-cp310-cp310-musllinux_1_2_ppc64le.whl\", hash = \"sha256:aa93063d4af05c49276cf14e419550a3f45258b6b9d1f16403e777f1addf4519\"},",
          "92:     {file = \"aiohttp-3.10.11-cp310-cp310-musllinux_1_2_s390x.whl\", hash = \"sha256:30283f9d0ce420363c24c5c2421e71a738a2155f10adbb1a11a4d4d6d2715cfc\"},",
          "93:     {file = \"aiohttp-3.10.11-cp310-cp310-musllinux_1_2_x86_64.whl\", hash = \"sha256:e5358addc8044ee49143c546d2182c15b4ac3a60be01c3209374ace05af5733d\"},",
          "94:     {file = \"aiohttp-3.10.11-cp310-cp310-win32.whl\", hash = \"sha256:e1ffa713d3ea7cdcd4aea9cddccab41edf6882fa9552940344c44e59652e1120\"},",
          "95:     {file = \"aiohttp-3.10.11-cp310-cp310-win_amd64.whl\", hash = \"sha256:778cbd01f18ff78b5dd23c77eb82987ee4ba23408cbed233009fd570dda7e674\"},",
          "96:     {file = \"aiohttp-3.10.11-cp311-cp311-macosx_10_9_universal2.whl\", hash = \"sha256:80ff08556c7f59a7972b1e8919f62e9c069c33566a6d28586771711e0eea4f07\"},",
          "97:     {file = \"aiohttp-3.10.11-cp311-cp311-macosx_10_9_x86_64.whl\", hash = \"sha256:2c8f96e9ee19f04c4914e4e7a42a60861066d3e1abf05c726f38d9d0a466e695\"},",
          "98:     {file = \"aiohttp-3.10.11-cp311-cp311-macosx_11_0_arm64.whl\", hash = \"sha256:fb8601394d537da9221947b5d6e62b064c9a43e88a1ecd7414d21a1a6fba9c24\"},",
          "99:     {file = \"aiohttp-3.10.11-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:2ea224cf7bc2d8856d6971cea73b1d50c9c51d36971faf1abc169a0d5f85a382\"},",
          "100:     {file = \"aiohttp-3.10.11-cp311-cp311-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:db9503f79e12d5d80b3efd4d01312853565c05367493379df76d2674af881caa\"},",
          "101:     {file = \"aiohttp-3.10.11-cp311-cp311-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:0f449a50cc33f0384f633894d8d3cd020e3ccef81879c6e6245c3c375c448625\"},",
          "102:     {file = \"aiohttp-3.10.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:82052be3e6d9e0c123499127782a01a2b224b8af8c62ab46b3f6197035ad94e9\"},",
          "103:     {file = \"aiohttp-3.10.11-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:20063c7acf1eec550c8eb098deb5ed9e1bb0521613b03bb93644b810986027ac\"},",
          "104:     {file = \"aiohttp-3.10.11-cp311-cp311-musllinux_1_2_aarch64.whl\", hash = \"sha256:489cced07a4c11488f47aab1f00d0c572506883f877af100a38f1fedaa884c3a\"},",
          "105:     {file = \"aiohttp-3.10.11-cp311-cp311-musllinux_1_2_i686.whl\", hash = \"sha256:ea9b3bab329aeaa603ed3bf605f1e2a6f36496ad7e0e1aa42025f368ee2dc07b\"},",
          "106:     {file = \"aiohttp-3.10.11-cp311-cp311-musllinux_1_2_ppc64le.whl\", hash = \"sha256:ca117819d8ad113413016cb29774b3f6d99ad23c220069789fc050267b786c16\"},",
          "107:     {file = \"aiohttp-3.10.11-cp311-cp311-musllinux_1_2_s390x.whl\", hash = \"sha256:2dfb612dcbe70fb7cdcf3499e8d483079b89749c857a8f6e80263b021745c730\"},",
          "108:     {file = \"aiohttp-3.10.11-cp311-cp311-musllinux_1_2_x86_64.whl\", hash = \"sha256:f9b615d3da0d60e7d53c62e22b4fd1c70f4ae5993a44687b011ea3a2e49051b8\"},",
          "109:     {file = \"aiohttp-3.10.11-cp311-cp311-win32.whl\", hash = \"sha256:29103f9099b6068bbdf44d6a3d090e0a0b2be6d3c9f16a070dd9d0d910ec08f9\"},",
          "110:     {file = \"aiohttp-3.10.11-cp311-cp311-win_amd64.whl\", hash = \"sha256:236b28ceb79532da85d59aa9b9bf873b364e27a0acb2ceaba475dc61cffb6f3f\"},",
          "111:     {file = \"aiohttp-3.10.11-cp312-cp312-macosx_10_9_universal2.whl\", hash = \"sha256:7480519f70e32bfb101d71fb9a1f330fbd291655a4c1c922232a48c458c52710\"},",
          "112:     {file = \"aiohttp-3.10.11-cp312-cp312-macosx_10_9_x86_64.whl\", hash = \"sha256:f65267266c9aeb2287a6622ee2bb39490292552f9fbf851baabc04c9f84e048d\"},",
          "113:     {file = \"aiohttp-3.10.11-cp312-cp312-macosx_11_0_arm64.whl\", hash = \"sha256:7400a93d629a0608dc1d6c55f1e3d6e07f7375745aaa8bd7f085571e4d1cee97\"},",
          "114:     {file = \"aiohttp-3.10.11-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:f34b97e4b11b8d4eb2c3a4f975be626cc8af99ff479da7de49ac2c6d02d35725\"},",
          "115:     {file = \"aiohttp-3.10.11-cp312-cp312-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:1e7b825da878464a252ccff2958838f9caa82f32a8dbc334eb9b34a026e2c636\"},",
          "116:     {file = \"aiohttp-3.10.11-cp312-cp312-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:f9f92a344c50b9667827da308473005f34767b6a2a60d9acff56ae94f895f385\"},",
          "117:     {file = \"aiohttp-3.10.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:bc6f1ab987a27b83c5268a17218463c2ec08dbb754195113867a27b166cd6087\"},",
          "118:     {file = \"aiohttp-3.10.11-cp312-cp312-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:1dc0f4ca54842173d03322793ebcf2c8cc2d34ae91cc762478e295d8e361e03f\"},",
          "119:     {file = \"aiohttp-3.10.11-cp312-cp312-musllinux_1_2_aarch64.whl\", hash = \"sha256:7ce6a51469bfaacff146e59e7fb61c9c23006495d11cc24c514a455032bcfa03\"},",
          "120:     {file = \"aiohttp-3.10.11-cp312-cp312-musllinux_1_2_i686.whl\", hash = \"sha256:aad3cd91d484d065ede16f3cf15408254e2469e3f613b241a1db552c5eb7ab7d\"},",
          "121:     {file = \"aiohttp-3.10.11-cp312-cp312-musllinux_1_2_ppc64le.whl\", hash = \"sha256:f4df4b8ca97f658c880fb4b90b1d1ec528315d4030af1ec763247ebfd33d8b9a\"},",
          "122:     {file = \"aiohttp-3.10.11-cp312-cp312-musllinux_1_2_s390x.whl\", hash = \"sha256:2e4e18a0a2d03531edbc06c366954e40a3f8d2a88d2b936bbe78a0c75a3aab3e\"},",
          "123:     {file = \"aiohttp-3.10.11-cp312-cp312-musllinux_1_2_x86_64.whl\", hash = \"sha256:6ce66780fa1a20e45bc753cda2a149daa6dbf1561fc1289fa0c308391c7bc0a4\"},",
          "124:     {file = \"aiohttp-3.10.11-cp312-cp312-win32.whl\", hash = \"sha256:a919c8957695ea4c0e7a3e8d16494e3477b86f33067478f43106921c2fef15bb\"},",
          "125:     {file = \"aiohttp-3.10.11-cp312-cp312-win_amd64.whl\", hash = \"sha256:b5e29706e6389a2283a91611c91bf24f218962717c8f3b4e528ef529d112ee27\"},",
          "126:     {file = \"aiohttp-3.10.11-cp313-cp313-macosx_10_13_universal2.whl\", hash = \"sha256:703938e22434d7d14ec22f9f310559331f455018389222eed132808cd8f44127\"},",
          "127:     {file = \"aiohttp-3.10.11-cp313-cp313-macosx_10_13_x86_64.whl\", hash = \"sha256:9bc50b63648840854e00084c2b43035a62e033cb9b06d8c22b409d56eb098413\"},",
          "128:     {file = \"aiohttp-3.10.11-cp313-cp313-macosx_11_0_arm64.whl\", hash = \"sha256:5f0463bf8b0754bc744e1feb61590706823795041e63edf30118a6f0bf577461\"},",
          "129:     {file = \"aiohttp-3.10.11-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:f6c6dec398ac5a87cb3a407b068e1106b20ef001c344e34154616183fe684288\"},",
          "130:     {file = \"aiohttp-3.10.11-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:bcaf2d79104d53d4dcf934f7ce76d3d155302d07dae24dff6c9fffd217568067\"},",
          "131:     {file = \"aiohttp-3.10.11-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:25fd5470922091b5a9aeeb7e75be609e16b4fba81cdeaf12981393fb240dd10e\"},",
          "132:     {file = \"aiohttp-3.10.11-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:bbde2ca67230923a42161b1f408c3992ae6e0be782dca0c44cb3206bf330dee1\"},",
          "133:     {file = \"aiohttp-3.10.11-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:249c8ff8d26a8b41a0f12f9df804e7c685ca35a207e2410adbd3e924217b9006\"},",
          "134:     {file = \"aiohttp-3.10.11-cp313-cp313-musllinux_1_2_aarch64.whl\", hash = \"sha256:878ca6a931ee8c486a8f7b432b65431d095c522cbeb34892bee5be97b3481d0f\"},",
          "135:     {file = \"aiohttp-3.10.11-cp313-cp313-musllinux_1_2_i686.whl\", hash = \"sha256:8663f7777ce775f0413324be0d96d9730959b2ca73d9b7e2c2c90539139cbdd6\"},",
          "136:     {file = \"aiohttp-3.10.11-cp313-cp313-musllinux_1_2_ppc64le.whl\", hash = \"sha256:6cd3f10b01f0c31481fba8d302b61603a2acb37b9d30e1d14e0f5a58b7b18a31\"},",
          "137:     {file = \"aiohttp-3.10.11-cp313-cp313-musllinux_1_2_s390x.whl\", hash = \"sha256:4e8d8aad9402d3aa02fdc5ca2fe68bcb9fdfe1f77b40b10410a94c7f408b664d\"},",
          "138:     {file = \"aiohttp-3.10.11-cp313-cp313-musllinux_1_2_x86_64.whl\", hash = \"sha256:38e3c4f80196b4f6c3a85d134a534a56f52da9cb8d8e7af1b79a32eefee73a00\"},",
          "139:     {file = \"aiohttp-3.10.11-cp313-cp313-win32.whl\", hash = \"sha256:fc31820cfc3b2863c6e95e14fcf815dc7afe52480b4dc03393c4873bb5599f71\"},",
          "140:     {file = \"aiohttp-3.10.11-cp313-cp313-win_amd64.whl\", hash = \"sha256:4996ff1345704ffdd6d75fb06ed175938c133425af616142e7187f28dc75f14e\"},",
          "141:     {file = \"aiohttp-3.10.11-cp38-cp38-macosx_10_9_universal2.whl\", hash = \"sha256:74baf1a7d948b3d640badeac333af581a367ab916b37e44cf90a0334157cdfd2\"},",
          "142:     {file = \"aiohttp-3.10.11-cp38-cp38-macosx_10_9_x86_64.whl\", hash = \"sha256:473aebc3b871646e1940c05268d451f2543a1d209f47035b594b9d4e91ce8339\"},",
          "143:     {file = \"aiohttp-3.10.11-cp38-cp38-macosx_11_0_arm64.whl\", hash = \"sha256:c2f746a6968c54ab2186574e15c3f14f3e7f67aef12b761e043b33b89c5b5f95\"},",
          "144:     {file = \"aiohttp-3.10.11-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:d110cabad8360ffa0dec8f6ec60e43286e9d251e77db4763a87dcfe55b4adb92\"},",
          "145:     {file = \"aiohttp-3.10.11-cp38-cp38-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:e0099c7d5d7afff4202a0c670e5b723f7718810000b4abcbc96b064129e64bc7\"},",
          "146:     {file = \"aiohttp-3.10.11-cp38-cp38-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:0316e624b754dbbf8c872b62fe6dcb395ef20c70e59890dfa0de9eafccd2849d\"},",
          "147:     {file = \"aiohttp-3.10.11-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:5a5f7ab8baf13314e6b2485965cbacb94afff1e93466ac4d06a47a81c50f9cca\"},",
          "148:     {file = \"aiohttp-3.10.11-cp38-cp38-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:c891011e76041e6508cbfc469dd1a8ea09bc24e87e4c204e05f150c4c455a5fa\"},",
          "149:     {file = \"aiohttp-3.10.11-cp38-cp38-musllinux_1_2_aarch64.whl\", hash = \"sha256:9208299251370ee815473270c52cd3f7069ee9ed348d941d574d1457d2c73e8b\"},",
          "150:     {file = \"aiohttp-3.10.11-cp38-cp38-musllinux_1_2_i686.whl\", hash = \"sha256:459f0f32c8356e8125f45eeff0ecf2b1cb6db1551304972702f34cd9e6c44658\"},",
          "151:     {file = \"aiohttp-3.10.11-cp38-cp38-musllinux_1_2_ppc64le.whl\", hash = \"sha256:14cdc8c1810bbd4b4b9f142eeee23cda528ae4e57ea0923551a9af4820980e39\"},",
          "152:     {file = \"aiohttp-3.10.11-cp38-cp38-musllinux_1_2_s390x.whl\", hash = \"sha256:971aa438a29701d4b34e4943e91b5e984c3ae6ccbf80dd9efaffb01bd0b243a9\"},",
          "153:     {file = \"aiohttp-3.10.11-cp38-cp38-musllinux_1_2_x86_64.whl\", hash = \"sha256:9a309c5de392dfe0f32ee57fa43ed8fc6ddf9985425e84bd51ed66bb16bce3a7\"},",
          "154:     {file = \"aiohttp-3.10.11-cp38-cp38-win32.whl\", hash = \"sha256:9ec1628180241d906a0840b38f162a3215114b14541f1a8711c368a8739a9be4\"},",
          "155:     {file = \"aiohttp-3.10.11-cp38-cp38-win_amd64.whl\", hash = \"sha256:9c6e0ffd52c929f985c7258f83185d17c76d4275ad22e90aa29f38e211aacbec\"},",
          "156:     {file = \"aiohttp-3.10.11-cp39-cp39-macosx_10_9_universal2.whl\", hash = \"sha256:cdc493a2e5d8dc79b2df5bec9558425bcd39aff59fc949810cbd0832e294b106\"},",
          "157:     {file = \"aiohttp-3.10.11-cp39-cp39-macosx_10_9_x86_64.whl\", hash = \"sha256:b3e70f24e7d0405be2348da9d5a7836936bf3a9b4fd210f8c37e8d48bc32eca6\"},",
          "158:     {file = \"aiohttp-3.10.11-cp39-cp39-macosx_11_0_arm64.whl\", hash = \"sha256:968b8fb2a5eee2770eda9c7b5581587ef9b96fbdf8dcabc6b446d35ccc69df01\"},",
          "159:     {file = \"aiohttp-3.10.11-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:deef4362af9493d1382ef86732ee2e4cbc0d7c005947bd54ad1a9a16dd59298e\"},",
          "160:     {file = \"aiohttp-3.10.11-cp39-cp39-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:686b03196976e327412a1b094f4120778c7c4b9cff9bce8d2fdfeca386b89829\"},",
          "161:     {file = \"aiohttp-3.10.11-cp39-cp39-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:3bf6d027d9d1d34e1c2e1645f18a6498c98d634f8e373395221121f1c258ace8\"},",
          "162:     {file = \"aiohttp-3.10.11-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:099fd126bf960f96d34a760e747a629c27fb3634da5d05c7ef4d35ef4ea519fc\"},",
          "163:     {file = \"aiohttp-3.10.11-cp39-cp39-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:c73c4d3dae0b4644bc21e3de546530531d6cdc88659cdeb6579cd627d3c206aa\"},",
          "164:     {file = \"aiohttp-3.10.11-cp39-cp39-musllinux_1_2_aarch64.whl\", hash = \"sha256:0c5580f3c51eea91559db3facd45d72e7ec970b04528b4709b1f9c2555bd6d0b\"},",
          "165:     {file = \"aiohttp-3.10.11-cp39-cp39-musllinux_1_2_i686.whl\", hash = \"sha256:fdf6429f0caabfd8a30c4e2eaecb547b3c340e4730ebfe25139779b9815ba138\"},",
          "166:     {file = \"aiohttp-3.10.11-cp39-cp39-musllinux_1_2_ppc64le.whl\", hash = \"sha256:d97187de3c276263db3564bb9d9fad9e15b51ea10a371ffa5947a5ba93ad6777\"},",
          "167:     {file = \"aiohttp-3.10.11-cp39-cp39-musllinux_1_2_s390x.whl\", hash = \"sha256:0acafb350cfb2eba70eb5d271f55e08bd4502ec35e964e18ad3e7d34d71f7261\"},",
          "168:     {file = \"aiohttp-3.10.11-cp39-cp39-musllinux_1_2_x86_64.whl\", hash = \"sha256:c13ed0c779911c7998a58e7848954bd4d63df3e3575f591e321b19a2aec8df9f\"},",
          "169:     {file = \"aiohttp-3.10.11-cp39-cp39-win32.whl\", hash = \"sha256:22b7c540c55909140f63ab4f54ec2c20d2635c0289cdd8006da46f3327f971b9\"},",
          "170:     {file = \"aiohttp-3.10.11-cp39-cp39-win_amd64.whl\", hash = \"sha256:7b26b1551e481012575dab8e3727b16fe7dd27eb2711d2e63ced7368756268fb\"},",
          "171:     {file = \"aiohttp-3.10.11.tar.gz\", hash = \"sha256:9dc2b8f3dcab2e39e0fa309c8da50c3b55e6f34ab25f1a71d3288f24924d33a7\"},",
          "172: ]",
          "174: [package.dependencies]",
          "175: aiohappyeyeballs = \">=2.3.0\"",
          "177: async-timeout = {version = \">=4.0,<6.0\", markers = \"python_version < \\\"3.11\\\"\"}",
          "181: yarl = \">=1.12.0,<2.0\"",
          "184: speedups = [\"Brotli\", \"aiodns (>=3.2.0)\", \"brotlicffi\"]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "3568: [package.dependencies]",
          "3569: wcwidth = \"*\"",
          "3571: [[package]]",
          "3572: name = \"protobuf\"",
          "3573: version = \"4.23.3\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "3598: [[package]]",
          "3599: name = \"propcache\"",
          "3600: version = \"0.2.0\"",
          "3601: description = \"Accelerated property cache\"",
          "3602: optional = false",
          "3603: python-versions = \">=3.8\"",
          "3604: files = [",
          "3605:     {file = \"propcache-0.2.0-cp310-cp310-macosx_10_9_universal2.whl\", hash = \"sha256:c5869b8fd70b81835a6f187c5fdbe67917a04d7e52b6e7cc4e5fe39d55c39d58\"},",
          "3606:     {file = \"propcache-0.2.0-cp310-cp310-macosx_10_9_x86_64.whl\", hash = \"sha256:952e0d9d07609d9c5be361f33b0d6d650cd2bae393aabb11d9b719364521984b\"},",
          "3607:     {file = \"propcache-0.2.0-cp310-cp310-macosx_11_0_arm64.whl\", hash = \"sha256:33ac8f098df0585c0b53009f039dfd913b38c1d2edafed0cedcc0c32a05aa110\"},",
          "3608:     {file = \"propcache-0.2.0-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:97e48e8875e6c13909c800fa344cd54cc4b2b0db1d5f911f840458a500fde2c2\"},",
          "3609:     {file = \"propcache-0.2.0-cp310-cp310-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:388f3217649d6d59292b722d940d4d2e1e6a7003259eb835724092a1cca0203a\"},",
          "3610:     {file = \"propcache-0.2.0-cp310-cp310-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:f571aea50ba5623c308aa146eb650eebf7dbe0fd8c5d946e28343cb3b5aad577\"},",
          "3611:     {file = \"propcache-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:3dfafb44f7bb35c0c06eda6b2ab4bfd58f02729e7c4045e179f9a861b07c9850\"},",
          "3612:     {file = \"propcache-0.2.0-cp310-cp310-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:a3ebe9a75be7ab0b7da2464a77bb27febcb4fab46a34f9288f39d74833db7f61\"},",
          "3613:     {file = \"propcache-0.2.0-cp310-cp310-musllinux_1_2_aarch64.whl\", hash = \"sha256:d2f0d0f976985f85dfb5f3d685697ef769faa6b71993b46b295cdbbd6be8cc37\"},",
          "3614:     {file = \"propcache-0.2.0-cp310-cp310-musllinux_1_2_armv7l.whl\", hash = \"sha256:a3dc1a4b165283bd865e8f8cb5f0c64c05001e0718ed06250d8cac9bec115b48\"},",
          "3615:     {file = \"propcache-0.2.0-cp310-cp310-musllinux_1_2_i686.whl\", hash = \"sha256:9e0f07b42d2a50c7dd2d8675d50f7343d998c64008f1da5fef888396b7f84630\"},",
          "3616:     {file = \"propcache-0.2.0-cp310-cp310-musllinux_1_2_ppc64le.whl\", hash = \"sha256:e63e3e1e0271f374ed489ff5ee73d4b6e7c60710e1f76af5f0e1a6117cd26394\"},",
          "3617:     {file = \"propcache-0.2.0-cp310-cp310-musllinux_1_2_s390x.whl\", hash = \"sha256:56bb5c98f058a41bb58eead194b4db8c05b088c93d94d5161728515bd52b052b\"},",
          "3618:     {file = \"propcache-0.2.0-cp310-cp310-musllinux_1_2_x86_64.whl\", hash = \"sha256:7665f04d0c7f26ff8bb534e1c65068409bf4687aa2534faf7104d7182debb336\"},",
          "3619:     {file = \"propcache-0.2.0-cp310-cp310-win32.whl\", hash = \"sha256:7cf18abf9764746b9c8704774d8b06714bcb0a63641518a3a89c7f85cc02c2ad\"},",
          "3620:     {file = \"propcache-0.2.0-cp310-cp310-win_amd64.whl\", hash = \"sha256:cfac69017ef97db2438efb854edf24f5a29fd09a536ff3a992b75990720cdc99\"},",
          "3621:     {file = \"propcache-0.2.0-cp311-cp311-macosx_10_9_universal2.whl\", hash = \"sha256:63f13bf09cc3336eb04a837490b8f332e0db41da66995c9fd1ba04552e516354\"},",
          "3622:     {file = \"propcache-0.2.0-cp311-cp311-macosx_10_9_x86_64.whl\", hash = \"sha256:608cce1da6f2672a56b24a015b42db4ac612ee709f3d29f27a00c943d9e851de\"},",
          "3623:     {file = \"propcache-0.2.0-cp311-cp311-macosx_11_0_arm64.whl\", hash = \"sha256:466c219deee4536fbc83c08d09115249db301550625c7fef1c5563a584c9bc87\"},",
          "3624:     {file = \"propcache-0.2.0-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:fc2db02409338bf36590aa985a461b2c96fce91f8e7e0f14c50c5fcc4f229016\"},",
          "3625:     {file = \"propcache-0.2.0-cp311-cp311-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:a6ed8db0a556343d566a5c124ee483ae113acc9a557a807d439bcecc44e7dfbb\"},",
          "3626:     {file = \"propcache-0.2.0-cp311-cp311-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:91997d9cb4a325b60d4e3f20967f8eb08dfcb32b22554d5ef78e6fd1dda743a2\"},",
          "3627:     {file = \"propcache-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:4c7dde9e533c0a49d802b4f3f218fa9ad0a1ce21f2c2eb80d5216565202acab4\"},",
          "3628:     {file = \"propcache-0.2.0-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:ffcad6c564fe6b9b8916c1aefbb37a362deebf9394bd2974e9d84232e3e08504\"},",
          "3629:     {file = \"propcache-0.2.0-cp311-cp311-musllinux_1_2_aarch64.whl\", hash = \"sha256:97a58a28bcf63284e8b4d7b460cbee1edaab24634e82059c7b8c09e65284f178\"},",
          "3630:     {file = \"propcache-0.2.0-cp311-cp311-musllinux_1_2_armv7l.whl\", hash = \"sha256:945db8ee295d3af9dbdbb698cce9bbc5c59b5c3fe328bbc4387f59a8a35f998d\"},",
          "3631:     {file = \"propcache-0.2.0-cp311-cp311-musllinux_1_2_i686.whl\", hash = \"sha256:39e104da444a34830751715f45ef9fc537475ba21b7f1f5b0f4d71a3b60d7fe2\"},",
          "3632:     {file = \"propcache-0.2.0-cp311-cp311-musllinux_1_2_ppc64le.whl\", hash = \"sha256:c5ecca8f9bab618340c8e848d340baf68bcd8ad90a8ecd7a4524a81c1764b3db\"},",
          "3633:     {file = \"propcache-0.2.0-cp311-cp311-musllinux_1_2_s390x.whl\", hash = \"sha256:c436130cc779806bdf5d5fae0d848713105472b8566b75ff70048c47d3961c5b\"},",
          "3634:     {file = \"propcache-0.2.0-cp311-cp311-musllinux_1_2_x86_64.whl\", hash = \"sha256:191db28dc6dcd29d1a3e063c3be0b40688ed76434622c53a284e5427565bbd9b\"},",
          "3635:     {file = \"propcache-0.2.0-cp311-cp311-win32.whl\", hash = \"sha256:5f2564ec89058ee7c7989a7b719115bdfe2a2fb8e7a4543b8d1c0cc4cf6478c1\"},",
          "3636:     {file = \"propcache-0.2.0-cp311-cp311-win_amd64.whl\", hash = \"sha256:6e2e54267980349b723cff366d1e29b138b9a60fa376664a157a342689553f71\"},",
          "3637:     {file = \"propcache-0.2.0-cp312-cp312-macosx_10_13_universal2.whl\", hash = \"sha256:2ee7606193fb267be4b2e3b32714f2d58cad27217638db98a60f9efb5efeccc2\"},",
          "3638:     {file = \"propcache-0.2.0-cp312-cp312-macosx_10_13_x86_64.whl\", hash = \"sha256:91ee8fc02ca52e24bcb77b234f22afc03288e1dafbb1f88fe24db308910c4ac7\"},",
          "3639:     {file = \"propcache-0.2.0-cp312-cp312-macosx_11_0_arm64.whl\", hash = \"sha256:2e900bad2a8456d00a113cad8c13343f3b1f327534e3589acc2219729237a2e8\"},",
          "3640:     {file = \"propcache-0.2.0-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:f52a68c21363c45297aca15561812d542f8fc683c85201df0bebe209e349f793\"},",
          "3641:     {file = \"propcache-0.2.0-cp312-cp312-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:1e41d67757ff4fbc8ef2af99b338bfb955010444b92929e9e55a6d4dcc3c4f09\"},",
          "3642:     {file = \"propcache-0.2.0-cp312-cp312-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:a64e32f8bd94c105cc27f42d3b658902b5bcc947ece3c8fe7bc1b05982f60e89\"},",
          "3643:     {file = \"propcache-0.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:55346705687dbd7ef0d77883ab4f6fabc48232f587925bdaf95219bae072491e\"},",
          "3644:     {file = \"propcache-0.2.0-cp312-cp312-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:00181262b17e517df2cd85656fcd6b4e70946fe62cd625b9d74ac9977b64d8d9\"},",
          "3645:     {file = \"propcache-0.2.0-cp312-cp312-musllinux_1_2_aarch64.whl\", hash = \"sha256:6994984550eaf25dd7fc7bd1b700ff45c894149341725bb4edc67f0ffa94efa4\"},",
          "3646:     {file = \"propcache-0.2.0-cp312-cp312-musllinux_1_2_armv7l.whl\", hash = \"sha256:56295eb1e5f3aecd516d91b00cfd8bf3a13991de5a479df9e27dd569ea23959c\"},",
          "3647:     {file = \"propcache-0.2.0-cp312-cp312-musllinux_1_2_i686.whl\", hash = \"sha256:439e76255daa0f8151d3cb325f6dd4a3e93043e6403e6491813bcaaaa8733887\"},",
          "3648:     {file = \"propcache-0.2.0-cp312-cp312-musllinux_1_2_ppc64le.whl\", hash = \"sha256:f6475a1b2ecb310c98c28d271a30df74f9dd436ee46d09236a6b750a7599ce57\"},",
          "3649:     {file = \"propcache-0.2.0-cp312-cp312-musllinux_1_2_s390x.whl\", hash = \"sha256:3444cdba6628accf384e349014084b1cacd866fbb88433cd9d279d90a54e0b23\"},",
          "3650:     {file = \"propcache-0.2.0-cp312-cp312-musllinux_1_2_x86_64.whl\", hash = \"sha256:4a9d9b4d0a9b38d1c391bb4ad24aa65f306c6f01b512e10a8a34a2dc5675d348\"},",
          "3651:     {file = \"propcache-0.2.0-cp312-cp312-win32.whl\", hash = \"sha256:69d3a98eebae99a420d4b28756c8ce6ea5a29291baf2dc9ff9414b42676f61d5\"},",
          "3652:     {file = \"propcache-0.2.0-cp312-cp312-win_amd64.whl\", hash = \"sha256:ad9c9b99b05f163109466638bd30ada1722abb01bbb85c739c50b6dc11f92dc3\"},",
          "3653:     {file = \"propcache-0.2.0-cp313-cp313-macosx_10_13_universal2.whl\", hash = \"sha256:ecddc221a077a8132cf7c747d5352a15ed763b674c0448d811f408bf803d9ad7\"},",
          "3654:     {file = \"propcache-0.2.0-cp313-cp313-macosx_10_13_x86_64.whl\", hash = \"sha256:0e53cb83fdd61cbd67202735e6a6687a7b491c8742dfc39c9e01e80354956763\"},",
          "3655:     {file = \"propcache-0.2.0-cp313-cp313-macosx_11_0_arm64.whl\", hash = \"sha256:92fe151145a990c22cbccf9ae15cae8ae9eddabfc949a219c9f667877e40853d\"},",
          "3656:     {file = \"propcache-0.2.0-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:d6a21ef516d36909931a2967621eecb256018aeb11fc48656e3257e73e2e247a\"},",
          "3657:     {file = \"propcache-0.2.0-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:3f88a4095e913f98988f5b338c1d4d5d07dbb0b6bad19892fd447484e483ba6b\"},",
          "3658:     {file = \"propcache-0.2.0-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:5a5b3bb545ead161be780ee85a2b54fdf7092815995661947812dde94a40f6fb\"},",
          "3659:     {file = \"propcache-0.2.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:67aeb72e0f482709991aa91345a831d0b707d16b0257e8ef88a2ad246a7280bf\"},",
          "3660:     {file = \"propcache-0.2.0-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:3c997f8c44ec9b9b0bcbf2d422cc00a1d9b9c681f56efa6ca149a941e5560da2\"},",
          "3661:     {file = \"propcache-0.2.0-cp313-cp313-musllinux_1_2_aarch64.whl\", hash = \"sha256:2a66df3d4992bc1d725b9aa803e8c5a66c010c65c741ad901e260ece77f58d2f\"},",
          "3662:     {file = \"propcache-0.2.0-cp313-cp313-musllinux_1_2_armv7l.whl\", hash = \"sha256:3ebbcf2a07621f29638799828b8d8668c421bfb94c6cb04269130d8de4fb7136\"},",
          "3663:     {file = \"propcache-0.2.0-cp313-cp313-musllinux_1_2_i686.whl\", hash = \"sha256:1235c01ddaa80da8235741e80815ce381c5267f96cc49b1477fdcf8c047ef325\"},",
          "3664:     {file = \"propcache-0.2.0-cp313-cp313-musllinux_1_2_ppc64le.whl\", hash = \"sha256:3947483a381259c06921612550867b37d22e1df6d6d7e8361264b6d037595f44\"},",
          "3665:     {file = \"propcache-0.2.0-cp313-cp313-musllinux_1_2_s390x.whl\", hash = \"sha256:d5bed7f9805cc29c780f3aee05de3262ee7ce1f47083cfe9f77471e9d6777e83\"},",
          "3666:     {file = \"propcache-0.2.0-cp313-cp313-musllinux_1_2_x86_64.whl\", hash = \"sha256:e4a91d44379f45f5e540971d41e4626dacd7f01004826a18cb048e7da7e96544\"},",
          "3667:     {file = \"propcache-0.2.0-cp313-cp313-win32.whl\", hash = \"sha256:f902804113e032e2cdf8c71015651c97af6418363bea8d78dc0911d56c335032\"},",
          "3668:     {file = \"propcache-0.2.0-cp313-cp313-win_amd64.whl\", hash = \"sha256:8f188cfcc64fb1266f4684206c9de0e80f54622c3f22a910cbd200478aeae61e\"},",
          "3669:     {file = \"propcache-0.2.0-cp38-cp38-macosx_10_9_universal2.whl\", hash = \"sha256:53d1bd3f979ed529f0805dd35ddaca330f80a9a6d90bc0121d2ff398f8ed8861\"},",
          "3670:     {file = \"propcache-0.2.0-cp38-cp38-macosx_10_9_x86_64.whl\", hash = \"sha256:83928404adf8fb3d26793665633ea79b7361efa0287dfbd372a7e74311d51ee6\"},",
          "3671:     {file = \"propcache-0.2.0-cp38-cp38-macosx_11_0_arm64.whl\", hash = \"sha256:77a86c261679ea5f3896ec060be9dc8e365788248cc1e049632a1be682442063\"},",
          "3672:     {file = \"propcache-0.2.0-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:218db2a3c297a3768c11a34812e63b3ac1c3234c3a086def9c0fee50d35add1f\"},",
          "3673:     {file = \"propcache-0.2.0-cp38-cp38-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:7735e82e3498c27bcb2d17cb65d62c14f1100b71723b68362872bca7d0913d90\"},",
          "3674:     {file = \"propcache-0.2.0-cp38-cp38-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:20a617c776f520c3875cf4511e0d1db847a076d720714ae35ffe0df3e440be68\"},",
          "3675:     {file = \"propcache-0.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:67b69535c870670c9f9b14a75d28baa32221d06f6b6fa6f77a0a13c5a7b0a5b9\"},",
          "3676:     {file = \"propcache-0.2.0-cp38-cp38-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:4569158070180c3855e9c0791c56be3ceeb192defa2cdf6a3f39e54319e56b89\"},",
          "3677:     {file = \"propcache-0.2.0-cp38-cp38-musllinux_1_2_aarch64.whl\", hash = \"sha256:db47514ffdbd91ccdc7e6f8407aac4ee94cc871b15b577c1c324236b013ddd04\"},",
          "3678:     {file = \"propcache-0.2.0-cp38-cp38-musllinux_1_2_armv7l.whl\", hash = \"sha256:2a60ad3e2553a74168d275a0ef35e8c0a965448ffbc3b300ab3a5bb9956c2162\"},",
          "3679:     {file = \"propcache-0.2.0-cp38-cp38-musllinux_1_2_i686.whl\", hash = \"sha256:662dd62358bdeaca0aee5761de8727cfd6861432e3bb828dc2a693aa0471a563\"},",
          "3680:     {file = \"propcache-0.2.0-cp38-cp38-musllinux_1_2_ppc64le.whl\", hash = \"sha256:25a1f88b471b3bc911d18b935ecb7115dff3a192b6fef46f0bfaf71ff4f12418\"},",
          "3681:     {file = \"propcache-0.2.0-cp38-cp38-musllinux_1_2_s390x.whl\", hash = \"sha256:f60f0ac7005b9f5a6091009b09a419ace1610e163fa5deaba5ce3484341840e7\"},",
          "3682:     {file = \"propcache-0.2.0-cp38-cp38-musllinux_1_2_x86_64.whl\", hash = \"sha256:74acd6e291f885678631b7ebc85d2d4aec458dd849b8c841b57ef04047833bed\"},",
          "3683:     {file = \"propcache-0.2.0-cp38-cp38-win32.whl\", hash = \"sha256:d9b6ddac6408194e934002a69bcaadbc88c10b5f38fb9307779d1c629181815d\"},",
          "3684:     {file = \"propcache-0.2.0-cp38-cp38-win_amd64.whl\", hash = \"sha256:676135dcf3262c9c5081cc8f19ad55c8a64e3f7282a21266d05544450bffc3a5\"},",
          "3685:     {file = \"propcache-0.2.0-cp39-cp39-macosx_10_9_universal2.whl\", hash = \"sha256:25c8d773a62ce0451b020c7b29a35cfbc05de8b291163a7a0f3b7904f27253e6\"},",
          "3686:     {file = \"propcache-0.2.0-cp39-cp39-macosx_10_9_x86_64.whl\", hash = \"sha256:375a12d7556d462dc64d70475a9ee5982465fbb3d2b364f16b86ba9135793638\"},",
          "3687:     {file = \"propcache-0.2.0-cp39-cp39-macosx_11_0_arm64.whl\", hash = \"sha256:1ec43d76b9677637a89d6ab86e1fef70d739217fefa208c65352ecf0282be957\"},",
          "3688:     {file = \"propcache-0.2.0-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:f45eec587dafd4b2d41ac189c2156461ebd0c1082d2fe7013571598abb8505d1\"},",
          "3689:     {file = \"propcache-0.2.0-cp39-cp39-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:bc092ba439d91df90aea38168e11f75c655880c12782facf5cf9c00f3d42b562\"},",
          "3690:     {file = \"propcache-0.2.0-cp39-cp39-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:fa1076244f54bb76e65e22cb6910365779d5c3d71d1f18b275f1dfc7b0d71b4d\"},",
          "3691:     {file = \"propcache-0.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:682a7c79a2fbf40f5dbb1eb6bfe2cd865376deeac65acf9beb607505dced9e12\"},",
          "3692:     {file = \"propcache-0.2.0-cp39-cp39-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:8e40876731f99b6f3c897b66b803c9e1c07a989b366c6b5b475fafd1f7ba3fb8\"},",
          "3693:     {file = \"propcache-0.2.0-cp39-cp39-musllinux_1_2_aarch64.whl\", hash = \"sha256:363ea8cd3c5cb6679f1c2f5f1f9669587361c062e4899fce56758efa928728f8\"},",
          "3694:     {file = \"propcache-0.2.0-cp39-cp39-musllinux_1_2_armv7l.whl\", hash = \"sha256:140fbf08ab3588b3468932974a9331aff43c0ab8a2ec2c608b6d7d1756dbb6cb\"},",
          "3695:     {file = \"propcache-0.2.0-cp39-cp39-musllinux_1_2_i686.whl\", hash = \"sha256:e70fac33e8b4ac63dfc4c956fd7d85a0b1139adcfc0d964ce288b7c527537fea\"},",
          "3696:     {file = \"propcache-0.2.0-cp39-cp39-musllinux_1_2_ppc64le.whl\", hash = \"sha256:b33d7a286c0dc1a15f5fc864cc48ae92a846df287ceac2dd499926c3801054a6\"},",
          "3697:     {file = \"propcache-0.2.0-cp39-cp39-musllinux_1_2_s390x.whl\", hash = \"sha256:f6d5749fdd33d90e34c2efb174c7e236829147a2713334d708746e94c4bde40d\"},",
          "3698:     {file = \"propcache-0.2.0-cp39-cp39-musllinux_1_2_x86_64.whl\", hash = \"sha256:22aa8f2272d81d9317ff5756bb108021a056805ce63dd3630e27d042c8092798\"},",
          "3699:     {file = \"propcache-0.2.0-cp39-cp39-win32.whl\", hash = \"sha256:73e4b40ea0eda421b115248d7e79b59214411109a5bc47d0d48e4c73e3b8fcf9\"},",
          "3700:     {file = \"propcache-0.2.0-cp39-cp39-win_amd64.whl\", hash = \"sha256:9517d5e9e0731957468c29dbfd0f976736a0e55afaea843726e887f36fe017df\"},",
          "3701:     {file = \"propcache-0.2.0-py3-none-any.whl\", hash = \"sha256:2ccc28197af5313706511fab3a8b66dcd6da067a1331372c82ea1cb74285e036\"},",
          "3702:     {file = \"propcache-0.2.0.tar.gz\", hash = \"sha256:df81779732feb9d01e5d513fad0122efb3d53bbc75f61b2a4f29a020bc985e70\"},",
          "3703: ]",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "7076: [[package]]",
          "7077: name = \"yarl\"",
          "7079: description = \"Yet another URL library\"",
          "7080: optional = false",
          "7082: files = [",
          "7157: ]",
          "7159: [package.dependencies]",
          "7160: idna = \">=2.0\"",
          "7161: multidict = \">=4.0\"",
          "7163: [[package]]",
          "7164: name = \"zipp\"",
          "",
          "[Removed Lines]",
          "7078: version = \"1.9.2\"",
          "7081: python-versions = \">=3.7\"",
          "7083:     {file = \"yarl-1.9.2-cp310-cp310-macosx_10_9_universal2.whl\", hash = \"sha256:8c2ad583743d16ddbdf6bb14b5cd76bf43b0d0006e918809d5d4ddf7bde8dd82\"},",
          "7084:     {file = \"yarl-1.9.2-cp310-cp310-macosx_10_9_x86_64.whl\", hash = \"sha256:82aa6264b36c50acfb2424ad5ca537a2060ab6de158a5bd2a72a032cc75b9eb8\"},",
          "7085:     {file = \"yarl-1.9.2-cp310-cp310-macosx_11_0_arm64.whl\", hash = \"sha256:c0c77533b5ed4bcc38e943178ccae29b9bcf48ffd1063f5821192f23a1bd27b9\"},",
          "7086:     {file = \"yarl-1.9.2-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:ee4afac41415d52d53a9833ebae7e32b344be72835bbb589018c9e938045a560\"},",
          "7087:     {file = \"yarl-1.9.2-cp310-cp310-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:9bf345c3a4f5ba7f766430f97f9cc1320786f19584acc7086491f45524a551ac\"},",
          "7088:     {file = \"yarl-1.9.2-cp310-cp310-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:2a96c19c52ff442a808c105901d0bdfd2e28575b3d5f82e2f5fd67e20dc5f4ea\"},",
          "7089:     {file = \"yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:891c0e3ec5ec881541f6c5113d8df0315ce5440e244a716b95f2525b7b9f3608\"},",
          "7090:     {file = \"yarl-1.9.2-cp310-cp310-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:c3a53ba34a636a256d767c086ceb111358876e1fb6b50dfc4d3f4951d40133d5\"},",
          "7091:     {file = \"yarl-1.9.2-cp310-cp310-musllinux_1_1_aarch64.whl\", hash = \"sha256:566185e8ebc0898b11f8026447eacd02e46226716229cea8db37496c8cdd26e0\"},",
          "7092:     {file = \"yarl-1.9.2-cp310-cp310-musllinux_1_1_i686.whl\", hash = \"sha256:2b0738fb871812722a0ac2154be1f049c6223b9f6f22eec352996b69775b36d4\"},",
          "7093:     {file = \"yarl-1.9.2-cp310-cp310-musllinux_1_1_ppc64le.whl\", hash = \"sha256:32f1d071b3f362c80f1a7d322bfd7b2d11e33d2adf395cc1dd4df36c9c243095\"},",
          "7094:     {file = \"yarl-1.9.2-cp310-cp310-musllinux_1_1_s390x.whl\", hash = \"sha256:e9fdc7ac0d42bc3ea78818557fab03af6181e076a2944f43c38684b4b6bed8e3\"},",
          "7095:     {file = \"yarl-1.9.2-cp310-cp310-musllinux_1_1_x86_64.whl\", hash = \"sha256:56ff08ab5df8429901ebdc5d15941b59f6253393cb5da07b4170beefcf1b2528\"},",
          "7096:     {file = \"yarl-1.9.2-cp310-cp310-win32.whl\", hash = \"sha256:8ea48e0a2f931064469bdabca50c2f578b565fc446f302a79ba6cc0ee7f384d3\"},",
          "7097:     {file = \"yarl-1.9.2-cp310-cp310-win_amd64.whl\", hash = \"sha256:50f33040f3836e912ed16d212f6cc1efb3231a8a60526a407aeb66c1c1956dde\"},",
          "7098:     {file = \"yarl-1.9.2-cp311-cp311-macosx_10_9_universal2.whl\", hash = \"sha256:646d663eb2232d7909e6601f1a9107e66f9791f290a1b3dc7057818fe44fc2b6\"},",
          "7099:     {file = \"yarl-1.9.2-cp311-cp311-macosx_10_9_x86_64.whl\", hash = \"sha256:aff634b15beff8902d1f918012fc2a42e0dbae6f469fce134c8a0dc51ca423bb\"},",
          "7100:     {file = \"yarl-1.9.2-cp311-cp311-macosx_11_0_arm64.whl\", hash = \"sha256:a83503934c6273806aed765035716216cc9ab4e0364f7f066227e1aaea90b8d0\"},",
          "7101:     {file = \"yarl-1.9.2-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:b25322201585c69abc7b0e89e72790469f7dad90d26754717f3310bfe30331c2\"},",
          "7102:     {file = \"yarl-1.9.2-cp311-cp311-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:22a94666751778629f1ec4280b08eb11815783c63f52092a5953faf73be24191\"},",
          "7103:     {file = \"yarl-1.9.2-cp311-cp311-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:8ec53a0ea2a80c5cd1ab397925f94bff59222aa3cf9c6da938ce05c9ec20428d\"},",
          "7104:     {file = \"yarl-1.9.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:159d81f22d7a43e6eabc36d7194cb53f2f15f498dbbfa8edc8a3239350f59fe7\"},",
          "7105:     {file = \"yarl-1.9.2-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:832b7e711027c114d79dffb92576acd1bd2decc467dec60e1cac96912602d0e6\"},",
          "7106:     {file = \"yarl-1.9.2-cp311-cp311-musllinux_1_1_aarch64.whl\", hash = \"sha256:95d2ecefbcf4e744ea952d073c6922e72ee650ffc79028eb1e320e732898d7e8\"},",
          "7107:     {file = \"yarl-1.9.2-cp311-cp311-musllinux_1_1_i686.whl\", hash = \"sha256:d4e2c6d555e77b37288eaf45b8f60f0737c9efa3452c6c44626a5455aeb250b9\"},",
          "7108:     {file = \"yarl-1.9.2-cp311-cp311-musllinux_1_1_ppc64le.whl\", hash = \"sha256:783185c75c12a017cc345015ea359cc801c3b29a2966c2655cd12b233bf5a2be\"},",
          "7109:     {file = \"yarl-1.9.2-cp311-cp311-musllinux_1_1_s390x.whl\", hash = \"sha256:b8cc1863402472f16c600e3e93d542b7e7542a540f95c30afd472e8e549fc3f7\"},",
          "7110:     {file = \"yarl-1.9.2-cp311-cp311-musllinux_1_1_x86_64.whl\", hash = \"sha256:822b30a0f22e588b32d3120f6d41e4ed021806418b4c9f0bc3048b8c8cb3f92a\"},",
          "7111:     {file = \"yarl-1.9.2-cp311-cp311-win32.whl\", hash = \"sha256:a60347f234c2212a9f0361955007fcf4033a75bf600a33c88a0a8e91af77c0e8\"},",
          "7112:     {file = \"yarl-1.9.2-cp311-cp311-win_amd64.whl\", hash = \"sha256:be6b3fdec5c62f2a67cb3f8c6dbf56bbf3f61c0f046f84645cd1ca73532ea051\"},",
          "7113:     {file = \"yarl-1.9.2-cp37-cp37m-macosx_10_9_x86_64.whl\", hash = \"sha256:38a3928ae37558bc1b559f67410df446d1fbfa87318b124bf5032c31e3447b74\"},",
          "7114:     {file = \"yarl-1.9.2-cp37-cp37m-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:ac9bb4c5ce3975aeac288cfcb5061ce60e0d14d92209e780c93954076c7c4367\"},",
          "7115:     {file = \"yarl-1.9.2-cp37-cp37m-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:3da8a678ca8b96c8606bbb8bfacd99a12ad5dd288bc6f7979baddd62f71c63ef\"},",
          "7116:     {file = \"yarl-1.9.2-cp37-cp37m-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:13414591ff516e04fcdee8dc051c13fd3db13b673c7a4cb1350e6b2ad9639ad3\"},",
          "7117:     {file = \"yarl-1.9.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:bf74d08542c3a9ea97bb8f343d4fcbd4d8f91bba5ec9d5d7f792dbe727f88938\"},",
          "7118:     {file = \"yarl-1.9.2-cp37-cp37m-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:6e7221580dc1db478464cfeef9b03b95c5852cc22894e418562997df0d074ccc\"},",
          "7119:     {file = \"yarl-1.9.2-cp37-cp37m-musllinux_1_1_aarch64.whl\", hash = \"sha256:494053246b119b041960ddcd20fd76224149cfea8ed8777b687358727911dd33\"},",
          "7120:     {file = \"yarl-1.9.2-cp37-cp37m-musllinux_1_1_i686.whl\", hash = \"sha256:52a25809fcbecfc63ac9ba0c0fb586f90837f5425edfd1ec9f3372b119585e45\"},",
          "7121:     {file = \"yarl-1.9.2-cp37-cp37m-musllinux_1_1_ppc64le.whl\", hash = \"sha256:e65610c5792870d45d7b68c677681376fcf9cc1c289f23e8e8b39c1485384185\"},",
          "7122:     {file = \"yarl-1.9.2-cp37-cp37m-musllinux_1_1_s390x.whl\", hash = \"sha256:1b1bba902cba32cdec51fca038fd53f8beee88b77efc373968d1ed021024cc04\"},",
          "7123:     {file = \"yarl-1.9.2-cp37-cp37m-musllinux_1_1_x86_64.whl\", hash = \"sha256:662e6016409828ee910f5d9602a2729a8a57d74b163c89a837de3fea050c7582\"},",
          "7124:     {file = \"yarl-1.9.2-cp37-cp37m-win32.whl\", hash = \"sha256:f364d3480bffd3aa566e886587eaca7c8c04d74f6e8933f3f2c996b7f09bee1b\"},",
          "7125:     {file = \"yarl-1.9.2-cp37-cp37m-win_amd64.whl\", hash = \"sha256:6a5883464143ab3ae9ba68daae8e7c5c95b969462bbe42e2464d60e7e2698368\"},",
          "7126:     {file = \"yarl-1.9.2-cp38-cp38-macosx_10_9_universal2.whl\", hash = \"sha256:5610f80cf43b6202e2c33ba3ec2ee0a2884f8f423c8f4f62906731d876ef4fac\"},",
          "7127:     {file = \"yarl-1.9.2-cp38-cp38-macosx_10_9_x86_64.whl\", hash = \"sha256:b9a4e67ad7b646cd6f0938c7ebfd60e481b7410f574c560e455e938d2da8e0f4\"},",
          "7128:     {file = \"yarl-1.9.2-cp38-cp38-macosx_11_0_arm64.whl\", hash = \"sha256:83fcc480d7549ccebe9415d96d9263e2d4226798c37ebd18c930fce43dfb9574\"},",
          "7129:     {file = \"yarl-1.9.2-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:5fcd436ea16fee7d4207c045b1e340020e58a2597301cfbcfdbe5abd2356c2fb\"},",
          "7130:     {file = \"yarl-1.9.2-cp38-cp38-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:84e0b1599334b1e1478db01b756e55937d4614f8654311eb26012091be109d59\"},",
          "7131:     {file = \"yarl-1.9.2-cp38-cp38-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:3458a24e4ea3fd8930e934c129b676c27452e4ebda80fbe47b56d8c6c7a63a9e\"},",
          "7132:     {file = \"yarl-1.9.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:838162460b3a08987546e881a2bfa573960bb559dfa739e7800ceeec92e64417\"},",
          "7133:     {file = \"yarl-1.9.2-cp38-cp38-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:f4e2d08f07a3d7d3e12549052eb5ad3eab1c349c53ac51c209a0e5991bbada78\"},",
          "7134:     {file = \"yarl-1.9.2-cp38-cp38-musllinux_1_1_aarch64.whl\", hash = \"sha256:de119f56f3c5f0e2fb4dee508531a32b069a5f2c6e827b272d1e0ff5ac040333\"},",
          "7135:     {file = \"yarl-1.9.2-cp38-cp38-musllinux_1_1_i686.whl\", hash = \"sha256:149ddea5abf329752ea5051b61bd6c1d979e13fbf122d3a1f9f0c8be6cb6f63c\"},",
          "7136:     {file = \"yarl-1.9.2-cp38-cp38-musllinux_1_1_ppc64le.whl\", hash = \"sha256:674ca19cbee4a82c9f54e0d1eee28116e63bc6fd1e96c43031d11cbab8b2afd5\"},",
          "7137:     {file = \"yarl-1.9.2-cp38-cp38-musllinux_1_1_s390x.whl\", hash = \"sha256:9b3152f2f5677b997ae6c804b73da05a39daa6a9e85a512e0e6823d81cdad7cc\"},",
          "7138:     {file = \"yarl-1.9.2-cp38-cp38-musllinux_1_1_x86_64.whl\", hash = \"sha256:5415d5a4b080dc9612b1b63cba008db84e908b95848369aa1da3686ae27b6d2b\"},",
          "7139:     {file = \"yarl-1.9.2-cp38-cp38-win32.whl\", hash = \"sha256:f7a3d8146575e08c29ed1cd287068e6d02f1c7bdff8970db96683b9591b86ee7\"},",
          "7140:     {file = \"yarl-1.9.2-cp38-cp38-win_amd64.whl\", hash = \"sha256:63c48f6cef34e6319a74c727376e95626f84ea091f92c0250a98e53e62c77c72\"},",
          "7141:     {file = \"yarl-1.9.2-cp39-cp39-macosx_10_9_universal2.whl\", hash = \"sha256:75df5ef94c3fdc393c6b19d80e6ef1ecc9ae2f4263c09cacb178d871c02a5ba9\"},",
          "7142:     {file = \"yarl-1.9.2-cp39-cp39-macosx_10_9_x86_64.whl\", hash = \"sha256:c027a6e96ef77d401d8d5a5c8d6bc478e8042f1e448272e8d9752cb0aff8b5c8\"},",
          "7143:     {file = \"yarl-1.9.2-cp39-cp39-macosx_11_0_arm64.whl\", hash = \"sha256:f3b078dbe227f79be488ffcfc7a9edb3409d018e0952cf13f15fd6512847f3f7\"},",
          "7144:     {file = \"yarl-1.9.2-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:59723a029760079b7d991a401386390c4be5bfec1e7dd83e25a6a0881859e716\"},",
          "7145:     {file = \"yarl-1.9.2-cp39-cp39-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:b03917871bf859a81ccb180c9a2e6c1e04d2f6a51d953e6a5cdd70c93d4e5a2a\"},",
          "7146:     {file = \"yarl-1.9.2-cp39-cp39-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:c1012fa63eb6c032f3ce5d2171c267992ae0c00b9e164efe4d73db818465fac3\"},",
          "7147:     {file = \"yarl-1.9.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:a74dcbfe780e62f4b5a062714576f16c2f3493a0394e555ab141bf0d746bb955\"},",
          "7148:     {file = \"yarl-1.9.2-cp39-cp39-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:8c56986609b057b4839968ba901944af91b8e92f1725d1a2d77cbac6972b9ed1\"},",
          "7149:     {file = \"yarl-1.9.2-cp39-cp39-musllinux_1_1_aarch64.whl\", hash = \"sha256:2c315df3293cd521033533d242d15eab26583360b58f7ee5d9565f15fee1bef4\"},",
          "7150:     {file = \"yarl-1.9.2-cp39-cp39-musllinux_1_1_i686.whl\", hash = \"sha256:b7232f8dfbd225d57340e441d8caf8652a6acd06b389ea2d3222b8bc89cbfca6\"},",
          "7151:     {file = \"yarl-1.9.2-cp39-cp39-musllinux_1_1_ppc64le.whl\", hash = \"sha256:53338749febd28935d55b41bf0bcc79d634881195a39f6b2f767870b72514caf\"},",
          "7152:     {file = \"yarl-1.9.2-cp39-cp39-musllinux_1_1_s390x.whl\", hash = \"sha256:066c163aec9d3d073dc9ffe5dd3ad05069bcb03fcaab8d221290ba99f9f69ee3\"},",
          "7153:     {file = \"yarl-1.9.2-cp39-cp39-musllinux_1_1_x86_64.whl\", hash = \"sha256:8288d7cd28f8119b07dd49b7230d6b4562f9b61ee9a4ab02221060d21136be80\"},",
          "7154:     {file = \"yarl-1.9.2-cp39-cp39-win32.whl\", hash = \"sha256:b124e2a6d223b65ba8768d5706d103280914d61f5cae3afbc50fc3dfcc016623\"},",
          "7155:     {file = \"yarl-1.9.2-cp39-cp39-win_amd64.whl\", hash = \"sha256:61016e7d582bc46a5378ffdd02cd0314fb8ba52f40f9cf4d9a5e7dbef88dee18\"},",
          "7156:     {file = \"yarl-1.9.2.tar.gz\", hash = \"sha256:04ab9d4b9f587c06d801c2abfe9317b77cdf996c65a90d5e84ecc45010823571\"},",
          "",
          "[Added Lines]",
          "7212: version = \"1.15.2\"",
          "7215: python-versions = \">=3.8\"",
          "7217:     {file = \"yarl-1.15.2-cp310-cp310-macosx_10_9_universal2.whl\", hash = \"sha256:e4ee8b8639070ff246ad3649294336b06db37a94bdea0d09ea491603e0be73b8\"},",
          "7218:     {file = \"yarl-1.15.2-cp310-cp310-macosx_10_9_x86_64.whl\", hash = \"sha256:a7cf963a357c5f00cb55b1955df8bbe68d2f2f65de065160a1c26b85a1e44172\"},",
          "7219:     {file = \"yarl-1.15.2-cp310-cp310-macosx_11_0_arm64.whl\", hash = \"sha256:43ebdcc120e2ca679dba01a779333a8ea76b50547b55e812b8b92818d604662c\"},",
          "7220:     {file = \"yarl-1.15.2-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:3433da95b51a75692dcf6cc8117a31410447c75a9a8187888f02ad45c0a86c50\"},",
          "7221:     {file = \"yarl-1.15.2-cp310-cp310-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:38d0124fa992dbacd0c48b1b755d3ee0a9f924f427f95b0ef376556a24debf01\"},",
          "7222:     {file = \"yarl-1.15.2-cp310-cp310-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:ded1b1803151dd0f20a8945508786d57c2f97a50289b16f2629f85433e546d47\"},",
          "7223:     {file = \"yarl-1.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:ace4cad790f3bf872c082366c9edd7f8f8f77afe3992b134cfc810332206884f\"},",
          "7224:     {file = \"yarl-1.15.2-cp310-cp310-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:c77494a2f2282d9bbbbcab7c227a4d1b4bb829875c96251f66fb5f3bae4fb053\"},",
          "7225:     {file = \"yarl-1.15.2-cp310-cp310-musllinux_1_2_aarch64.whl\", hash = \"sha256:b7f227ca6db5a9fda0a2b935a2ea34a7267589ffc63c8045f0e4edb8d8dcf956\"},",
          "7226:     {file = \"yarl-1.15.2-cp310-cp310-musllinux_1_2_armv7l.whl\", hash = \"sha256:31561a5b4d8dbef1559b3600b045607cf804bae040f64b5f5bca77da38084a8a\"},",
          "7227:     {file = \"yarl-1.15.2-cp310-cp310-musllinux_1_2_i686.whl\", hash = \"sha256:3e52474256a7db9dcf3c5f4ca0b300fdea6c21cca0148c8891d03a025649d935\"},",
          "7228:     {file = \"yarl-1.15.2-cp310-cp310-musllinux_1_2_ppc64le.whl\", hash = \"sha256:0e1af74a9529a1137c67c887ed9cde62cff53aa4d84a3adbec329f9ec47a3936\"},",
          "7229:     {file = \"yarl-1.15.2-cp310-cp310-musllinux_1_2_s390x.whl\", hash = \"sha256:15c87339490100c63472a76d87fe7097a0835c705eb5ae79fd96e343473629ed\"},",
          "7230:     {file = \"yarl-1.15.2-cp310-cp310-musllinux_1_2_x86_64.whl\", hash = \"sha256:74abb8709ea54cc483c4fb57fb17bb66f8e0f04438cff6ded322074dbd17c7ec\"},",
          "7231:     {file = \"yarl-1.15.2-cp310-cp310-win32.whl\", hash = \"sha256:ffd591e22b22f9cb48e472529db6a47203c41c2c5911ff0a52e85723196c0d75\"},",
          "7232:     {file = \"yarl-1.15.2-cp310-cp310-win_amd64.whl\", hash = \"sha256:1695497bb2a02a6de60064c9f077a4ae9c25c73624e0d43e3aa9d16d983073c2\"},",
          "7233:     {file = \"yarl-1.15.2-cp311-cp311-macosx_10_9_universal2.whl\", hash = \"sha256:9fcda20b2de7042cc35cf911702fa3d8311bd40055a14446c1e62403684afdc5\"},",
          "7234:     {file = \"yarl-1.15.2-cp311-cp311-macosx_10_9_x86_64.whl\", hash = \"sha256:0545de8c688fbbf3088f9e8b801157923be4bf8e7b03e97c2ecd4dfa39e48e0e\"},",
          "7235:     {file = \"yarl-1.15.2-cp311-cp311-macosx_11_0_arm64.whl\", hash = \"sha256:fbda058a9a68bec347962595f50546a8a4a34fd7b0654a7b9697917dc2bf810d\"},",
          "7236:     {file = \"yarl-1.15.2-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:d1ac2bc069f4a458634c26b101c2341b18da85cb96afe0015990507efec2e417\"},",
          "7237:     {file = \"yarl-1.15.2-cp311-cp311-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:cd126498171f752dd85737ab1544329a4520c53eed3997f9b08aefbafb1cc53b\"},",
          "7238:     {file = \"yarl-1.15.2-cp311-cp311-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:3db817b4e95eb05c362e3b45dafe7144b18603e1211f4a5b36eb9522ecc62bcf\"},",
          "7239:     {file = \"yarl-1.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:076b1ed2ac819933895b1a000904f62d615fe4533a5cf3e052ff9a1da560575c\"},",
          "7240:     {file = \"yarl-1.15.2-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:f8cfd847e6b9ecf9f2f2531c8427035f291ec286c0a4944b0a9fce58c6446046\"},",
          "7241:     {file = \"yarl-1.15.2-cp311-cp311-musllinux_1_2_aarch64.whl\", hash = \"sha256:32b66be100ac5739065496c74c4b7f3015cef792c3174982809274d7e51b3e04\"},",
          "7242:     {file = \"yarl-1.15.2-cp311-cp311-musllinux_1_2_armv7l.whl\", hash = \"sha256:34a2d76a1984cac04ff8b1bfc939ec9dc0914821264d4a9c8fd0ed6aa8d4cfd2\"},",
          "7243:     {file = \"yarl-1.15.2-cp311-cp311-musllinux_1_2_i686.whl\", hash = \"sha256:0afad2cd484908f472c8fe2e8ef499facee54a0a6978be0e0cff67b1254fd747\"},",
          "7244:     {file = \"yarl-1.15.2-cp311-cp311-musllinux_1_2_ppc64le.whl\", hash = \"sha256:c68e820879ff39992c7f148113b46efcd6ec765a4865581f2902b3c43a5f4bbb\"},",
          "7245:     {file = \"yarl-1.15.2-cp311-cp311-musllinux_1_2_s390x.whl\", hash = \"sha256:98f68df80ec6ca3015186b2677c208c096d646ef37bbf8b49764ab4a38183931\"},",
          "7246:     {file = \"yarl-1.15.2-cp311-cp311-musllinux_1_2_x86_64.whl\", hash = \"sha256:3c56ec1eacd0a5d35b8a29f468659c47f4fe61b2cab948ca756c39b7617f0aa5\"},",
          "7247:     {file = \"yarl-1.15.2-cp311-cp311-win32.whl\", hash = \"sha256:eedc3f247ee7b3808ea07205f3e7d7879bc19ad3e6222195cd5fbf9988853e4d\"},",
          "7248:     {file = \"yarl-1.15.2-cp311-cp311-win_amd64.whl\", hash = \"sha256:0ccaa1bc98751fbfcf53dc8dfdb90d96e98838010fc254180dd6707a6e8bb179\"},",
          "7249:     {file = \"yarl-1.15.2-cp312-cp312-macosx_10_13_universal2.whl\", hash = \"sha256:82d5161e8cb8f36ec778fd7ac4d740415d84030f5b9ef8fe4da54784a1f46c94\"},",
          "7250:     {file = \"yarl-1.15.2-cp312-cp312-macosx_10_13_x86_64.whl\", hash = \"sha256:fa2bea05ff0a8fb4d8124498e00e02398f06d23cdadd0fe027d84a3f7afde31e\"},",
          "7251:     {file = \"yarl-1.15.2-cp312-cp312-macosx_11_0_arm64.whl\", hash = \"sha256:99e12d2bf587b44deb74e0d6170fec37adb489964dbca656ec41a7cd8f2ff178\"},",
          "7252:     {file = \"yarl-1.15.2-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:243fbbbf003754fe41b5bdf10ce1e7f80bcc70732b5b54222c124d6b4c2ab31c\"},",
          "7253:     {file = \"yarl-1.15.2-cp312-cp312-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:856b7f1a7b98a8c31823285786bd566cf06226ac4f38b3ef462f593c608a9bd6\"},",
          "7254:     {file = \"yarl-1.15.2-cp312-cp312-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:553dad9af802a9ad1a6525e7528152a015b85fb8dbf764ebfc755c695f488367\"},",
          "7255:     {file = \"yarl-1.15.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:30c3ff305f6e06650a761c4393666f77384f1cc6c5c0251965d6bfa5fbc88f7f\"},",
          "7256:     {file = \"yarl-1.15.2-cp312-cp312-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:353665775be69bbfc6d54c8d134bfc533e332149faeddd631b0bc79df0897f46\"},",
          "7257:     {file = \"yarl-1.15.2-cp312-cp312-musllinux_1_2_aarch64.whl\", hash = \"sha256:f4fe99ce44128c71233d0d72152db31ca119711dfc5f2c82385ad611d8d7f897\"},",
          "7258:     {file = \"yarl-1.15.2-cp312-cp312-musllinux_1_2_armv7l.whl\", hash = \"sha256:9c1e3ff4b89cdd2e1a24c214f141e848b9e0451f08d7d4963cb4108d4d798f1f\"},",
          "7259:     {file = \"yarl-1.15.2-cp312-cp312-musllinux_1_2_i686.whl\", hash = \"sha256:711bdfae4e699a6d4f371137cbe9e740dc958530cb920eb6f43ff9551e17cfbc\"},",
          "7260:     {file = \"yarl-1.15.2-cp312-cp312-musllinux_1_2_ppc64le.whl\", hash = \"sha256:4388c72174868884f76affcdd3656544c426407e0043c89b684d22fb265e04a5\"},",
          "7261:     {file = \"yarl-1.15.2-cp312-cp312-musllinux_1_2_s390x.whl\", hash = \"sha256:f0e1844ad47c7bd5d6fa784f1d4accc5f4168b48999303a868fe0f8597bde715\"},",
          "7262:     {file = \"yarl-1.15.2-cp312-cp312-musllinux_1_2_x86_64.whl\", hash = \"sha256:a5cafb02cf097a82d74403f7e0b6b9df3ffbfe8edf9415ea816314711764a27b\"},",
          "7263:     {file = \"yarl-1.15.2-cp312-cp312-win32.whl\", hash = \"sha256:156ececdf636143f508770bf8a3a0498de64da5abd890c7dbb42ca9e3b6c05b8\"},",
          "7264:     {file = \"yarl-1.15.2-cp312-cp312-win_amd64.whl\", hash = \"sha256:435aca062444a7f0c884861d2e3ea79883bd1cd19d0a381928b69ae1b85bc51d\"},",
          "7265:     {file = \"yarl-1.15.2-cp313-cp313-macosx_10_13_universal2.whl\", hash = \"sha256:416f2e3beaeae81e2f7a45dc711258be5bdc79c940a9a270b266c0bec038fb84\"},",
          "7266:     {file = \"yarl-1.15.2-cp313-cp313-macosx_10_13_x86_64.whl\", hash = \"sha256:173563f3696124372831007e3d4b9821746964a95968628f7075d9231ac6bb33\"},",
          "7267:     {file = \"yarl-1.15.2-cp313-cp313-macosx_11_0_arm64.whl\", hash = \"sha256:9ce2e0f6123a60bd1a7f5ae3b2c49b240c12c132847f17aa990b841a417598a2\"},",
          "7268:     {file = \"yarl-1.15.2-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:eaea112aed589131f73d50d570a6864728bd7c0c66ef6c9154ed7b59f24da611\"},",
          "7269:     {file = \"yarl-1.15.2-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:e4ca3b9f370f218cc2a0309542cab8d0acdfd66667e7c37d04d617012485f904\"},",
          "7270:     {file = \"yarl-1.15.2-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:23ec1d3c31882b2a8a69c801ef58ebf7bae2553211ebbddf04235be275a38548\"},",
          "7271:     {file = \"yarl-1.15.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:75119badf45f7183e10e348edff5a76a94dc19ba9287d94001ff05e81475967b\"},",
          "7272:     {file = \"yarl-1.15.2-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:78e6fdc976ec966b99e4daa3812fac0274cc28cd2b24b0d92462e2e5ef90d368\"},",
          "7273:     {file = \"yarl-1.15.2-cp313-cp313-musllinux_1_2_aarch64.whl\", hash = \"sha256:8657d3f37f781d987037f9cc20bbc8b40425fa14380c87da0cb8dfce7c92d0fb\"},",
          "7274:     {file = \"yarl-1.15.2-cp313-cp313-musllinux_1_2_armv7l.whl\", hash = \"sha256:93bed8a8084544c6efe8856c362af08a23e959340c87a95687fdbe9c9f280c8b\"},",
          "7275:     {file = \"yarl-1.15.2-cp313-cp313-musllinux_1_2_i686.whl\", hash = \"sha256:69d5856d526802cbda768d3e6246cd0d77450fa2a4bc2ea0ea14f0d972c2894b\"},",
          "7276:     {file = \"yarl-1.15.2-cp313-cp313-musllinux_1_2_ppc64le.whl\", hash = \"sha256:ccad2800dfdff34392448c4bf834be124f10a5bc102f254521d931c1c53c455a\"},",
          "7277:     {file = \"yarl-1.15.2-cp313-cp313-musllinux_1_2_s390x.whl\", hash = \"sha256:a880372e2e5dbb9258a4e8ff43f13888039abb9dd6d515f28611c54361bc5644\"},",
          "7278:     {file = \"yarl-1.15.2-cp313-cp313-musllinux_1_2_x86_64.whl\", hash = \"sha256:c998d0558805860503bc3a595994895ca0f7835e00668dadc673bbf7f5fbfcbe\"},",
          "7279:     {file = \"yarl-1.15.2-cp313-cp313-win32.whl\", hash = \"sha256:533a28754e7f7439f217550a497bb026c54072dbe16402b183fdbca2431935a9\"},",
          "7280:     {file = \"yarl-1.15.2-cp313-cp313-win_amd64.whl\", hash = \"sha256:5838f2b79dc8f96fdc44077c9e4e2e33d7089b10788464609df788eb97d03aad\"},",
          "7281:     {file = \"yarl-1.15.2-cp38-cp38-macosx_10_9_universal2.whl\", hash = \"sha256:fbbb63bed5fcd70cd3dd23a087cd78e4675fb5a2963b8af53f945cbbca79ae16\"},",
          "7282:     {file = \"yarl-1.15.2-cp38-cp38-macosx_10_9_x86_64.whl\", hash = \"sha256:e2e93b88ecc8f74074012e18d679fb2e9c746f2a56f79cd5e2b1afcf2a8a786b\"},",
          "7283:     {file = \"yarl-1.15.2-cp38-cp38-macosx_11_0_arm64.whl\", hash = \"sha256:af8ff8d7dc07ce873f643de6dfbcd45dc3db2c87462e5c387267197f59e6d776\"},",
          "7284:     {file = \"yarl-1.15.2-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:66f629632220a4e7858b58e4857927dd01a850a4cef2fb4044c8662787165cf7\"},",
          "7285:     {file = \"yarl-1.15.2-cp38-cp38-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:833547179c31f9bec39b49601d282d6f0ea1633620701288934c5f66d88c3e50\"},",
          "7286:     {file = \"yarl-1.15.2-cp38-cp38-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:2aa738e0282be54eede1e3f36b81f1e46aee7ec7602aa563e81e0e8d7b67963f\"},",
          "7287:     {file = \"yarl-1.15.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:9a13a07532e8e1c4a5a3afff0ca4553da23409fad65def1b71186fb867eeae8d\"},",
          "7288:     {file = \"yarl-1.15.2-cp38-cp38-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:c45817e3e6972109d1a2c65091504a537e257bc3c885b4e78a95baa96df6a3f8\"},",
          "7289:     {file = \"yarl-1.15.2-cp38-cp38-musllinux_1_2_aarch64.whl\", hash = \"sha256:670eb11325ed3a6209339974b276811867defe52f4188fe18dc49855774fa9cf\"},",
          "7290:     {file = \"yarl-1.15.2-cp38-cp38-musllinux_1_2_armv7l.whl\", hash = \"sha256:d417a4f6943112fae3924bae2af7112562285848d9bcee737fc4ff7cbd450e6c\"},",
          "7291:     {file = \"yarl-1.15.2-cp38-cp38-musllinux_1_2_i686.whl\", hash = \"sha256:bc8936d06cd53fddd4892677d65e98af514c8d78c79864f418bbf78a4a2edde4\"},",
          "7292:     {file = \"yarl-1.15.2-cp38-cp38-musllinux_1_2_ppc64le.whl\", hash = \"sha256:954dde77c404084c2544e572f342aef384240b3e434e06cecc71597e95fd1ce7\"},",
          "7293:     {file = \"yarl-1.15.2-cp38-cp38-musllinux_1_2_s390x.whl\", hash = \"sha256:5bc0df728e4def5e15a754521e8882ba5a5121bd6b5a3a0ff7efda5d6558ab3d\"},",
          "7294:     {file = \"yarl-1.15.2-cp38-cp38-musllinux_1_2_x86_64.whl\", hash = \"sha256:b71862a652f50babab4a43a487f157d26b464b1dedbcc0afda02fd64f3809d04\"},",
          "7295:     {file = \"yarl-1.15.2-cp38-cp38-win32.whl\", hash = \"sha256:63eab904f8630aed5a68f2d0aeab565dcfc595dc1bf0b91b71d9ddd43dea3aea\"},",
          "7296:     {file = \"yarl-1.15.2-cp38-cp38-win_amd64.whl\", hash = \"sha256:2cf441c4b6e538ba0d2591574f95d3fdd33f1efafa864faa077d9636ecc0c4e9\"},",
          "7297:     {file = \"yarl-1.15.2-cp39-cp39-macosx_10_9_universal2.whl\", hash = \"sha256:a32d58f4b521bb98b2c0aa9da407f8bd57ca81f34362bcb090e4a79e9924fefc\"},",
          "7298:     {file = \"yarl-1.15.2-cp39-cp39-macosx_10_9_x86_64.whl\", hash = \"sha256:766dcc00b943c089349d4060b935c76281f6be225e39994c2ccec3a2a36ad627\"},",
          "7299:     {file = \"yarl-1.15.2-cp39-cp39-macosx_11_0_arm64.whl\", hash = \"sha256:bed1b5dbf90bad3bfc19439258c97873eab453c71d8b6869c136346acfe497e7\"},",
          "7300:     {file = \"yarl-1.15.2-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:ed20a4bdc635f36cb19e630bfc644181dd075839b6fc84cac51c0f381ac472e2\"},",
          "7301:     {file = \"yarl-1.15.2-cp39-cp39-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:d538df442c0d9665664ab6dd5fccd0110fa3b364914f9c85b3ef9b7b2e157980\"},",
          "7302:     {file = \"yarl-1.15.2-cp39-cp39-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:28c6cf1d92edf936ceedc7afa61b07e9d78a27b15244aa46bbcd534c7458ee1b\"},",
          "7303:     {file = \"yarl-1.15.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:ce44217ad99ffad8027d2fde0269ae368c86db66ea0571c62a000798d69401fb\"},",
          "7304:     {file = \"yarl-1.15.2-cp39-cp39-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:b47a6000a7e833ebfe5886b56a31cb2ff12120b1efd4578a6fcc38df16cc77bd\"},",
          "7305:     {file = \"yarl-1.15.2-cp39-cp39-musllinux_1_2_aarch64.whl\", hash = \"sha256:e52f77a0cd246086afde8815039f3e16f8d2be51786c0a39b57104c563c5cbb0\"},",
          "7306:     {file = \"yarl-1.15.2-cp39-cp39-musllinux_1_2_armv7l.whl\", hash = \"sha256:f9ca0e6ce7774dc7830dc0cc4bb6b3eec769db667f230e7c770a628c1aa5681b\"},",
          "7307:     {file = \"yarl-1.15.2-cp39-cp39-musllinux_1_2_i686.whl\", hash = \"sha256:136f9db0f53c0206db38b8cd0c985c78ded5fd596c9a86ce5c0b92afb91c3a19\"},",
          "7308:     {file = \"yarl-1.15.2-cp39-cp39-musllinux_1_2_ppc64le.whl\", hash = \"sha256:173866d9f7409c0fb514cf6e78952e65816600cb888c68b37b41147349fe0057\"},",
          "7309:     {file = \"yarl-1.15.2-cp39-cp39-musllinux_1_2_s390x.whl\", hash = \"sha256:6e840553c9c494a35e449a987ca2c4f8372668ee954a03a9a9685075228e5036\"},",
          "7310:     {file = \"yarl-1.15.2-cp39-cp39-musllinux_1_2_x86_64.whl\", hash = \"sha256:458c0c65802d816a6b955cf3603186de79e8fdb46d4f19abaec4ef0a906f50a7\"},",
          "7311:     {file = \"yarl-1.15.2-cp39-cp39-win32.whl\", hash = \"sha256:5b48388ded01f6f2429a8c55012bdbd1c2a0c3735b3e73e221649e524c34a58d\"},",
          "7312:     {file = \"yarl-1.15.2-cp39-cp39-win_amd64.whl\", hash = \"sha256:81dadafb3aa124f86dc267a2168f71bbd2bfb163663661ab0038f6e4b8edb810\"},",
          "7313:     {file = \"yarl-1.15.2-py3-none-any.whl\", hash = \"sha256:0d3105efab7c5c091609abacad33afff33bdff0035bece164c98bcf5a85ef90a\"},",
          "7314:     {file = \"yarl-1.15.2.tar.gz\", hash = \"sha256:a39c36f4218a5bb668b4f06874d676d35a035ee668e6e7e3538835c703634b84\"},",
          "7320: propcache = \">=0.2.0\"",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "7186: [metadata]",
          "7187: lock-version = \"2.0\"",
          "7188: python-versions = \">=3.8,<3.11\"",
          "",
          "[Removed Lines]",
          "7189: content-hash = \"c1c51259ab3b886039dcf7eb746a45815d4b8afcaa4bdbe179c891810aee553f\"",
          "",
          "[Added Lines]",
          "7348: content-hash = \"f9d326f7b0037e5fec5c28d1a5426cc35dac83cbcff804a59504f94e811286ea\"",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "509a42b38d40def31b19df71c770a703dc83faa5",
      "candidate_info": {
        "commit_hash": "509a42b38d40def31b19df71c770a703dc83faa5",
        "repo": "RasaHQ/rasa",
        "commit_url": "https://github.com/RasaHQ/rasa/commit/509a42b38d40def31b19df71c770a703dc83faa5",
        "files": [
          "poetry.lock",
          "pyproject.toml"
        ],
        "message": "Bump sanic-testing from 22.6.0 to 24.6.0\n\nBumps [sanic-testing](https://github.com/sanic-org/sanic-testing) from 22.6.0 to 24.6.0.\n- [Release notes](https://github.com/sanic-org/sanic-testing/releases)\n- [Commits](https://github.com/sanic-org/sanic-testing/compare/v22.6.0...v24.6.0)\n\n---\nupdated-dependencies:\n- dependency-name: sanic-testing\n  dependency-type: direct:development\n  update-type: version-update:semver-major\n...\n\nSigned-off-by: dependabot[bot] <support@github.com>",
        "before_after_code_files": [
          "poetry.lock||poetry.lock"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "poetry.lock||poetry.lock"
          ],
          "candidate": [
            "poetry.lock||poetry.lock"
          ]
        }
      },
      "candidate_diff": {
        "poetry.lock||poetry.lock": [
          "File: poetry.lock -> poetry.lock",
          "--- Hunk 1 ---",
          "[Context before]",
          "3: [[package]]",
          "4: name = \"absl-py\"",
          "",
          "[Removed Lines]",
          "1: # This file is automatically @generated by Poetry 1.8.2 and should not be changed by hand.",
          "",
          "[Added Lines]",
          "1: # This file is automatically @generated by Poetry 1.8.5 and should not be changed by hand.",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "4953: [[package]]",
          "4954: name = \"sanic-testing\"",
          "4956: description = \"Core testing clients for Sanic\"",
          "4957: optional = false",
          "4958: python-versions = \"*\"",
          "4959: files = [",
          "4962: ]",
          "4964: [package.dependencies]",
          "4967: [[package]]",
          "4968: name = \"scikit-learn\"",
          "",
          "[Removed Lines]",
          "4955: version = \"22.6.0\"",
          "4960:     {file = \"sanic-testing-22.6.0.tar.gz\", hash = \"sha256:8f006d2332106539cd6f3da8a5c0d1f31472261f3293e43e2c9bbad605e72c5b\"},",
          "4961:     {file = \"sanic_testing-22.6.0-py3-none-any.whl\", hash = \"sha256:d84303e83066de7f18e8c3a0cd04512ba1517dbc31123f14e8aec318b22c008c\"},",
          "4965: httpx = \">=0.18,<0.24\"",
          "",
          "[Added Lines]",
          "4955: version = \"24.6.0\"",
          "4960:     {file = \"sanic_testing-24.6.0-py3-none-any.whl\", hash = \"sha256:b1027184735e88230891aa0461fff84093abfa3bff0f4d29c0f78f42e59efada\"},",
          "4961:     {file = \"sanic_testing-24.6.0.tar.gz\", hash = \"sha256:7591ce537e2a651efb6dc01b458e7e4ea5347f6d91438676774c6f505a124731\"},",
          "4965: httpx = \">=0.18\"",
          "4967: [package.extras]",
          "4968: dev = [\"pytest\", \"pytest-asyncio\", \"sanic (>=22.12)\", \"setuptools\"]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "7186: [metadata]",
          "7187: lock-version = \"2.0\"",
          "7188: python-versions = \">=3.8,<3.11\"",
          "",
          "[Removed Lines]",
          "7189: content-hash = \"c1c51259ab3b886039dcf7eb746a45815d4b8afcaa4bdbe179c891810aee553f\"",
          "",
          "[Added Lines]",
          "7192: content-hash = \"eb826e5c8dfeef7b6a738e403b3aad6effd3dcb7605a4a8e711df4585eadadde\"",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "c46cf06c0fee27ca928a13b0bbd5b528b1c31d79",
      "candidate_info": {
        "commit_hash": "c46cf06c0fee27ca928a13b0bbd5b528b1c31d79",
        "repo": "RasaHQ/rasa",
        "commit_url": "https://github.com/RasaHQ/rasa/commit/c46cf06c0fee27ca928a13b0bbd5b528b1c31d79",
        "files": [
          "poetry.lock",
          "pyproject.toml"
        ],
        "message": "Bump apscheduler from 3.9.1.post1 to 3.11.0\n\nBumps [apscheduler](https://github.com/agronholm/apscheduler) from 3.9.1.post1 to 3.11.0.\n- [Release notes](https://github.com/agronholm/apscheduler/releases)\n- [Changelog](https://github.com/agronholm/apscheduler/blob/3.11.0/docs/versionhistory.rst)\n- [Commits](https://github.com/agronholm/apscheduler/compare/3.9.1.post1...3.11.0)\n\n---\nupdated-dependencies:\n- dependency-name: apscheduler\n  dependency-type: direct:production\n  update-type: version-update:semver-minor\n...\n\nSigned-off-by: dependabot[bot] <support@github.com>",
        "before_after_code_files": [
          "poetry.lock||poetry.lock"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "poetry.lock||poetry.lock"
          ],
          "candidate": [
            "poetry.lock||poetry.lock"
          ]
        }
      },
      "candidate_diff": {
        "poetry.lock||poetry.lock": [
          "File: poetry.lock -> poetry.lock",
          "--- Hunk 1 ---",
          "[Context before]",
          "3: [[package]]",
          "4: name = \"absl-py\"",
          "",
          "[Removed Lines]",
          "1: # This file is automatically @generated by Poetry 1.8.2 and should not be changed by hand.",
          "",
          "[Added Lines]",
          "1: # This file is automatically @generated by Poetry 1.8.5 and should not be changed by hand.",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "261: [[package]]",
          "262: name = \"apscheduler\"",
          "264: description = \"In-process task scheduler with Cron-like capabilities\"",
          "265: optional = false",
          "267: files = [",
          "270: ]",
          "272: [package.dependencies]",
          "278: [package.extras]",
          "281: gevent = [\"gevent\"]",
          "282: mongodb = [\"pymongo (>=3.0)\"]",
          "283: redis = [\"redis (>=3.0)\"]",
          "284: rethinkdb = [\"rethinkdb (>=2.4.0)\"]",
          "287: tornado = [\"tornado (>=4.3)\"]",
          "288: twisted = [\"twisted\"]",
          "289: zookeeper = [\"kazoo\"]",
          "",
          "[Removed Lines]",
          "263: version = \"3.9.1.post1\"",
          "266: python-versions = \"!=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*, !=3.4.*, <4\"",
          "268:     {file = \"APScheduler-3.9.1.post1-py2.py3-none-any.whl\", hash = \"sha256:c8c618241dbb2785ed5a687504b14cb1851d6f7b5a4edf3a51e39cc6a069967a\"},",
          "269:     {file = \"APScheduler-3.9.1.post1.tar.gz\", hash = \"sha256:b2bea0309569da53a7261bfa0ce19c67ddbfe151bda776a6a907579fdbd3eb2a\"},",
          "273: pytz = \"*\"",
          "274: setuptools = \">=0.7\"",
          "275: six = \">=1.4.0\"",
          "276: tzlocal = \">=2.0,<3.dev0 || >=4.dev0\"",
          "279: asyncio = [\"trollius\"]",
          "280: doc = [\"sphinx\", \"sphinx-rtd-theme\"]",
          "285: sqlalchemy = [\"sqlalchemy (>=0.8)\"]",
          "286: testing = [\"mock\", \"pytest\", \"pytest-asyncio\", \"pytest-asyncio (<0.6)\", \"pytest-cov\", \"pytest-tornado5\"]",
          "",
          "[Added Lines]",
          "263: version = \"3.11.0\"",
          "266: python-versions = \">=3.8\"",
          "268:     {file = \"APScheduler-3.11.0-py3-none-any.whl\", hash = \"sha256:fc134ca32e50f5eadcc4938e3a4545ab19131435e851abb40b34d63d5141c6da\"},",
          "269:     {file = \"apscheduler-3.11.0.tar.gz\", hash = \"sha256:4c622d250b0955a65d5d0eb91c33e6d43fd879834bf541e0a18661ae60460133\"},",
          "273: \"backports.zoneinfo\" = {version = \"*\", markers = \"python_version < \\\"3.9\\\"\"}",
          "274: tzlocal = \">=3.0\"",
          "277: doc = [\"packaging\", \"sphinx\", \"sphinx-rtd-theme (>=1.3.0)\"]",
          "278: etcd = [\"etcd3\", \"protobuf (<=3.21.0)\"]",
          "283: sqlalchemy = [\"sqlalchemy (>=1.4)\"]",
          "284: test = [\"APScheduler[etcd,mongodb,redis,rethinkdb,sqlalchemy,tornado,zookeeper]\", \"PySide6\", \"anyio (>=4.5.2)\", \"gevent\", \"pytest\", \"pytz\", \"twisted\"]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "7186: [metadata]",
          "7187: lock-version = \"2.0\"",
          "7188: python-versions = \">=3.8,<3.11\"",
          "",
          "[Removed Lines]",
          "7189: content-hash = \"c1c51259ab3b886039dcf7eb746a45815d4b8afcaa4bdbe179c891810aee553f\"",
          "",
          "[Added Lines]",
          "7187: content-hash = \"dc429f8d903366a8091d563edbc3b81c7b01d863fb2c2f6e0c1374ebb88c7828\"",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "ae8a4a229305706e435602b9d71617c8ed53adf1",
      "candidate_info": {
        "commit_hash": "ae8a4a229305706e435602b9d71617c8ed53adf1",
        "repo": "RasaHQ/rasa",
        "commit_url": "https://github.com/RasaHQ/rasa/commit/ae8a4a229305706e435602b9d71617c8ed53adf1",
        "files": [
          "poetry.lock",
          "pyproject.toml"
        ],
        "message": "Bump tensorflow-hub from 0.13.0 to 0.16.1\n\nBumps [tensorflow-hub](https://github.com/tensorflow/hub) from 0.13.0 to 0.16.1.\n- [Release notes](https://github.com/tensorflow/hub/releases)\n- [Changelog](https://github.com/tensorflow/hub/blob/v0.16.1/RELEASE.md)\n- [Commits](https://github.com/tensorflow/hub/compare/v0.13.0...v0.16.1)\n\n---\nupdated-dependencies:\n- dependency-name: tensorflow-hub\n  dependency-type: direct:production\n  update-type: version-update:semver-minor\n...\n\nSigned-off-by: dependabot[bot] <support@github.com>",
        "before_after_code_files": [
          "poetry.lock||poetry.lock"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "poetry.lock||poetry.lock"
          ],
          "candidate": [
            "poetry.lock||poetry.lock"
          ]
        }
      },
      "candidate_diff": {
        "poetry.lock||poetry.lock": [
          "File: poetry.lock -> poetry.lock",
          "--- Hunk 1 ---",
          "[Context before]",
          "3: [[package]]",
          "4: name = \"absl-py\"",
          "",
          "[Removed Lines]",
          "1: # This file is automatically @generated by Poetry 1.8.2 and should not be changed by hand.",
          "",
          "[Added Lines]",
          "1: # This file is automatically @generated by Poetry 1.8.5 and should not be changed by hand.",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "5801: [[package]]",
          "5802: name = \"tensorflow-hub\"",
          "5804: description = \"TensorFlow Hub is a library to foster the publication, discovery, and consumption of reusable parts of machine learning models.\"",
          "5805: optional = false",
          "5806: python-versions = \"*\"",
          "5807: files = [",
          "5809: ]",
          "5811: [package.dependencies]",
          "5812: numpy = \">=1.12.0\"",
          "5813: protobuf = \">=3.19.6\"",
          "5819: [[package]]",
          "5820: name = \"tensorflow-intel\"",
          "",
          "[Removed Lines]",
          "5803: version = \"0.13.0\"",
          "5808:     {file = \"tensorflow_hub-0.13.0-py2.py3-none-any.whl\", hash = \"sha256:3544f4fd9fd99e4eeb6da1b5b5320e4a2dbdef7f9bb778f66f76d6790f32dd65\"},",
          "5815: [package.extras]",
          "5816: make-image-classifier = [\"keras-preprocessing[image]\"]",
          "5817: make-nearest-neighbour-index = [\"annoy\", \"apache-beam\"]",
          "",
          "[Added Lines]",
          "5803: version = \"0.16.1\"",
          "5808:     {file = \"tensorflow_hub-0.16.1-py2.py3-none-any.whl\", hash = \"sha256:e10c184b3d08daeafada11ffea2dd46781725b6bef01fad1f74d6634ad05311f\"},",
          "5814: tf-keras = \">=2.14.1\"",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "6030:     {file = \"terminaltables-3.1.10.tar.gz\", hash = \"sha256:ba6eca5cb5ba02bba4c9f4f985af80c54ec3dccf94cfcd190154386255e47543\"},",
          "6031: ]",
          "6033: [[package]]",
          "6034: name = \"thinc\"",
          "6035: version = \"8.1.10\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "6030: [[package]]",
          "6031: name = \"tf-keras\"",
          "6032: version = \"2.15.0\"",
          "6033: description = \"Deep learning for humans.\"",
          "6034: optional = false",
          "6035: python-versions = \">=3.8\"",
          "6036: files = [",
          "6037:     {file = \"tf_keras-2.15.0-py3-none-any.whl\", hash = \"sha256:48607ee60a4d1fa7c09d6a44293a803faf3136e7a43f92df089ac094117547d2\"},",
          "6038:     {file = \"tf_keras-2.15.0.tar.gz\", hash = \"sha256:d7559c2ba40667679fcb2105d3e4b68bbc07ecafbf1037462ce7b3974c3c6798\"},",
          "6039: ]",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "7186: [metadata]",
          "7187: lock-version = \"2.0\"",
          "7188: python-versions = \">=3.8,<3.11\"",
          "",
          "[Removed Lines]",
          "7189: content-hash = \"c1c51259ab3b886039dcf7eb746a45815d4b8afcaa4bdbe179c891810aee553f\"",
          "",
          "[Added Lines]",
          "7197: content-hash = \"4d56157b2534c5a5318fed90949d8eeecb504855dbe5736e356538675ff5c366\"",
          "",
          "---------------"
        ]
      }
    }
  ]
}