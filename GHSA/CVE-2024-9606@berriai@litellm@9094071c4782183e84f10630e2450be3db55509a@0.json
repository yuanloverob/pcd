{
  "cve_id": "CVE-2024-9606",
  "cve_desc": "In berriai/litellm before version 1.44.12, the `litellm/litellm_core_utils/litellm_logging.py` file contains a vulnerability where the API key masking code only masks the first 5 characters of the key. This results in the leakage of almost the entire API key in the logs, exposing a significant amount of the secret key. The issue affects version v1.44.9.",
  "repo": "berriai/litellm",
  "patch_hash": "9094071c4782183e84f10630e2450be3db55509a",
  "patch_info": {
    "commit_hash": "9094071c4782183e84f10630e2450be3db55509a",
    "repo": "berriai/litellm",
    "commit_url": "https://github.com/berriai/litellm/commit/9094071c4782183e84f10630e2450be3db55509a",
    "files": [
      "litellm/litellm_core_utils/litellm_logging.py",
      "litellm/llms/vertex_ai_and_google_ai_studio/gemini/vertex_and_google_ai_studio_gemini.py",
      "litellm/proxy/_new_secret_config.yaml",
      "litellm/tests/test_custom_callback_input.py",
      "litellm/utils.py"
    ],
    "message": "fix(litellm_logging.py): only leave last 4 char of gemini key unmasked\n\nFixes https://github.com/BerriAI/litellm/issues/5433",
    "before_after_code_files": [
      "litellm/litellm_core_utils/litellm_logging.py||litellm/litellm_core_utils/litellm_logging.py",
      "litellm/llms/vertex_ai_and_google_ai_studio/gemini/vertex_and_google_ai_studio_gemini.py||litellm/llms/vertex_ai_and_google_ai_studio/gemini/vertex_and_google_ai_studio_gemini.py",
      "litellm/tests/test_custom_callback_input.py||litellm/tests/test_custom_callback_input.py",
      "litellm/utils.py||litellm/utils.py"
    ]
  },
  "patch_diff": {
    "litellm/litellm_core_utils/litellm_logging.py||litellm/litellm_core_utils/litellm_logging.py": [
      "File: litellm/litellm_core_utils/litellm_logging.py -> litellm/litellm_core_utils/litellm_logging.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "279:                 # Find the position of \"key=\" in the string",
      "280:                 key_index = api_base.find(\"key=\") + 4",
      "281:                 # Mask the last 5 characters after \"key=\"",
      "285:             else:",
      "286:                 masked_api_base = api_base",
      "287:             self.model_call_details[\"litellm_params\"][\"api_base\"] = masked_api_base",
      "",
      "[Removed Lines]",
      "282:                 masked_api_base = (",
      "283:                     api_base[:key_index] + \"*\" * 5 + api_base[key_index + 5 :]",
      "284:                 )",
      "",
      "[Added Lines]",
      "282:                 masked_api_base = api_base[:key_index] + \"*\" * 5 + api_base[-4:]",
      "",
      "---------------"
    ],
    "litellm/llms/vertex_ai_and_google_ai_studio/gemini/vertex_and_google_ai_studio_gemini.py||litellm/llms/vertex_ai_and_google_ai_studio/gemini/vertex_and_google_ai_studio_gemini.py": [
      "File: litellm/llms/vertex_ai_and_google_ai_studio/gemini/vertex_and_google_ai_studio_gemini.py -> litellm/llms/vertex_ai_and_google_ai_studio/gemini/vertex_and_google_ai_studio_gemini.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "1284:     ) -> Union[ModelResponse, CustomStreamWrapper]:",
      "1286:         request_body = await async_transform_request_body(**data)  # type: ignore",
      "1288:             _params = {}",
      "1289:             if timeout is not None:",
      "1290:                 if isinstance(timeout, float) or isinstance(timeout, int):",
      "",
      "[Removed Lines]",
      "1287:         if client is None:",
      "",
      "[Added Lines]",
      "1287:         if client is None or not isinstance(client, AsyncHTTPHandler):",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "1293:             client = AsyncHTTPHandler(**_params)  # type: ignore",
      "1294:         else:",
      "1295:             client = client  # type: ignore",
      "1297:         try:",
      "1298:             response = await client.post(api_base, headers=headers, json=request_body)  # type: ignore",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "1296:         ## LOGGING",
      "1297:         logging_obj.pre_call(",
      "1298:             input=messages,",
      "1299:             api_key=\"\",",
      "1300:             additional_args={",
      "1301:                 \"complete_input_dict\": request_body,",
      "1302:                 \"api_base\": api_base,",
      "1303:                 \"headers\": headers,",
      "1304:             },",
      "1305:         )",
      "",
      "---------------"
    ],
    "litellm/tests/test_custom_callback_input.py||litellm/tests/test_custom_callback_input.py": [
      "File: litellm/tests/test_custom_callback_input.py -> litellm/tests/test_custom_callback_input.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "1347:         assert standard_logging_object[\"cache_hit\"] is True",
      "1348:         assert standard_logging_object[\"response_cost\"] == 0",
      "1349:         assert standard_logging_object[\"saved_cache_cost\"] > 0",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "1352: def test_logging_key_masking_gemini():",
      "1353:     customHandler = CompletionCustomHandler()",
      "1354:     litellm.callbacks = [customHandler]",
      "1355:     litellm.success_callback = []",
      "1357:     with patch.object(",
      "1358:         customHandler, \"log_pre_api_call\", new=MagicMock()",
      "1359:     ) as mock_client:",
      "1360:         try:",
      "1361:             resp = litellm.completion(",
      "1362:                 model=\"gemini/gemini-1.5-pro\",",
      "1363:                 messages=[{\"role\": \"user\", \"content\": \"Hey, how's it going?\"}],",
      "1364:                 api_key=\"LEAVE_ONLY_LAST_4_CHAR_UNMASKED_THIS_PART\",",
      "1365:             )",
      "1366:         except litellm.AuthenticationError:",
      "1367:             pass",
      "1369:         mock_client.assert_called()",
      "1371:         print(f\"mock_client.call_args.kwargs: {mock_client.call_args.kwargs}\")",
      "1372:         assert (",
      "1373:             \"LEAVE_ONLY_LAST_4_CHAR_UNMASKED_THIS_PART\"",
      "1374:             not in mock_client.call_args.kwargs[\"kwargs\"][\"litellm_params\"][\"api_base\"]",
      "1375:         )",
      "1376:         key = mock_client.call_args.kwargs[\"kwargs\"][\"litellm_params\"][\"api_base\"]",
      "1377:         trimmed_key = key.split(\"key=\")[1]",
      "1378:         trimmed_key = trimmed_key.replace(\"*\", \"\")",
      "1379:         assert \"PART\" == trimmed_key",
      "",
      "---------------"
    ],
    "litellm/utils.py||litellm/utils.py": [
      "File: litellm/utils.py -> litellm/utils.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "7475:                         ),",
      "7476:                         litellm_debug_info=extra_information,",
      "7477:                     )",
      "7478:                 elif \"403\" in error_str:",
      "7479:                     exception_mapping_worked = True",
      "7480:                     raise BadRequestError(",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "7478:                 elif \"API key not valid.\" in error_str:",
      "7479:                     exception_mapping_worked = True",
      "7480:                     raise AuthenticationError(",
      "7481:                         message=f\"{custom_llm_provider}Exception - {error_str}\",",
      "7482:                         model=model,",
      "7483:                         llm_provider=custom_llm_provider,",
      "7484:                         litellm_debug_info=extra_information,",
      "7485:                     )",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "dd7b0081614d6b0224a2f7ae99609399d299920c",
      "candidate_info": {
        "commit_hash": "dd7b0081614d6b0224a2f7ae99609399d299920c",
        "repo": "berriai/litellm",
        "commit_url": "https://github.com/berriai/litellm/commit/dd7b0081614d6b0224a2f7ae99609399d299920c",
        "files": [
          "litellm/litellm_core_utils/litellm_logging.py",
          "litellm/llms/bedrock_httpx.py",
          "litellm/llms/vertex_ai_and_google_ai_studio/gemini/vertex_and_google_ai_studio_gemini.py",
          "litellm/main.py",
          "litellm/proxy/_new_secret_config.yaml",
          "litellm/proxy/auth/model_checks.py",
          "litellm/router.py",
          "litellm/tests/test_bedrock_completion.py",
          "litellm/tests/test_custom_callback_input.py",
          "litellm/tests/test_router.py",
          "litellm/types/router.py",
          "litellm/utils.py"
        ],
        "message": "fix: Minor LiteLLM Fixes + Improvements (29/08/2024)  (#5436)\n\n* fix(model_checks.py): support returning wildcard models on `/v1/models`\n\nFixes https://github.com/BerriAI/litellm/issues/4903\n\n* fix(bedrock_httpx.py): support calling bedrock via api_base\n\nCloses https://github.com/BerriAI/litellm/pull/4587\n\n* fix(litellm_logging.py): only leave last 4 char of gemini key unmasked\n\nFixes https://github.com/BerriAI/litellm/issues/5433\n\n* feat(router.py): support setting 'weight' param for models on router\n\nCloses https://github.com/BerriAI/litellm/issues/5410\n\n* test(test_bedrock_completion.py): add unit test for custom api base\n\n* fix(model_checks.py): handle no \"/\" in model",
        "before_after_code_files": [
          "litellm/litellm_core_utils/litellm_logging.py||litellm/litellm_core_utils/litellm_logging.py",
          "litellm/llms/bedrock_httpx.py||litellm/llms/bedrock_httpx.py",
          "litellm/llms/vertex_ai_and_google_ai_studio/gemini/vertex_and_google_ai_studio_gemini.py||litellm/llms/vertex_ai_and_google_ai_studio/gemini/vertex_and_google_ai_studio_gemini.py",
          "litellm/main.py||litellm/main.py",
          "litellm/proxy/auth/model_checks.py||litellm/proxy/auth/model_checks.py",
          "litellm/router.py||litellm/router.py",
          "litellm/tests/test_bedrock_completion.py||litellm/tests/test_bedrock_completion.py",
          "litellm/tests/test_custom_callback_input.py||litellm/tests/test_custom_callback_input.py",
          "litellm/tests/test_router.py||litellm/tests/test_router.py",
          "litellm/types/router.py||litellm/types/router.py",
          "litellm/utils.py||litellm/utils.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/BerriAI/litellm/pull/5436"
        ],
        "olp_code_files": {
          "patch": [
            "litellm/litellm_core_utils/litellm_logging.py||litellm/litellm_core_utils/litellm_logging.py",
            "litellm/llms/vertex_ai_and_google_ai_studio/gemini/vertex_and_google_ai_studio_gemini.py||litellm/llms/vertex_ai_and_google_ai_studio/gemini/vertex_and_google_ai_studio_gemini.py",
            "litellm/tests/test_custom_callback_input.py||litellm/tests/test_custom_callback_input.py",
            "litellm/utils.py||litellm/utils.py"
          ],
          "candidate": [
            "litellm/litellm_core_utils/litellm_logging.py||litellm/litellm_core_utils/litellm_logging.py",
            "litellm/llms/vertex_ai_and_google_ai_studio/gemini/vertex_and_google_ai_studio_gemini.py||litellm/llms/vertex_ai_and_google_ai_studio/gemini/vertex_and_google_ai_studio_gemini.py",
            "litellm/tests/test_custom_callback_input.py||litellm/tests/test_custom_callback_input.py",
            "litellm/utils.py||litellm/utils.py"
          ]
        }
      },
      "candidate_diff": {
        "litellm/litellm_core_utils/litellm_logging.py||litellm/litellm_core_utils/litellm_logging.py": [
          "File: litellm/litellm_core_utils/litellm_logging.py -> litellm/litellm_core_utils/litellm_logging.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "279:                 # Find the position of \"key=\" in the string",
          "280:                 key_index = api_base.find(\"key=\") + 4",
          "281:                 # Mask the last 5 characters after \"key=\"",
          "285:             else:",
          "286:                 masked_api_base = api_base",
          "287:             self.model_call_details[\"litellm_params\"][\"api_base\"] = masked_api_base",
          "",
          "[Removed Lines]",
          "282:                 masked_api_base = (",
          "283:                     api_base[:key_index] + \"*\" * 5 + api_base[key_index + 5 :]",
          "284:                 )",
          "",
          "[Added Lines]",
          "282:                 masked_api_base = api_base[:key_index] + \"*\" * 5 + api_base[-4:]",
          "",
          "---------------"
        ],
        "litellm/llms/bedrock_httpx.py||litellm/llms/bedrock_httpx.py": [
          "File: litellm/llms/bedrock_httpx.py -> litellm/llms/bedrock_httpx.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "48: from litellm.types.utils import Choices",
          "49: from litellm.types.utils import GenericStreamingChunk as GChunk",
          "50: from litellm.types.utils import Message",
          "59: from .base import BaseLLM",
          "60: from .base_aws_llm import BaseAWSLLM",
          "",
          "[Removed Lines]",
          "51: from litellm.utils import (",
          "52:     CustomStreamWrapper,",
          "53:     ModelResponse,",
          "54:     Usage,",
          "55:     get_secret,",
          "56:     print_verbose,",
          "57: )",
          "",
          "[Added Lines]",
          "51: from litellm.utils import CustomStreamWrapper, ModelResponse, Usage, get_secret",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "654:         self,",
          "655:         model: str,",
          "656:         messages: list,",
          "657:         custom_prompt_dict: dict,",
          "658:         model_response: ModelResponse,",
          "659:         print_verbose: Callable,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "651:         api_base: Optional[str],",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "734:         ### SET RUNTIME ENDPOINT ###",
          "735:         endpoint_url = \"\"",
          "736:         env_aws_bedrock_runtime_endpoint = get_secret(\"AWS_BEDROCK_RUNTIME_ENDPOINT\")",
          "738:             aws_bedrock_runtime_endpoint, str",
          "739:         ):",
          "740:             endpoint_url = aws_bedrock_runtime_endpoint",
          "",
          "[Removed Lines]",
          "737:         if aws_bedrock_runtime_endpoint is not None and isinstance(",
          "",
          "[Added Lines]",
          "732:         if api_base is not None:",
          "733:             endpoint_url = api_base",
          "734:         elif aws_bedrock_runtime_endpoint is not None and isinstance(",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "1459:             client = client  # type: ignore",
          "1461:         try:",
          "1463:             response.raise_for_status()",
          "1464:         except httpx.HTTPStatusError as err:",
          "1465:             error_code = err.response.status_code",
          "",
          "[Removed Lines]",
          "1462:             response = await client.post(api_base, headers=headers, data=data)  # type: ignore",
          "",
          "[Added Lines]",
          "1459:             response = await client.post(url=api_base, headers=headers, data=data)  # type: ignore",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "1485:         self,",
          "1486:         model: str,",
          "1487:         messages: list,",
          "1488:         custom_prompt_dict: dict,",
          "1489:         model_response: ModelResponse,",
          "1490:         print_verbose: Callable,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1485:         api_base: Optional[str],",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "1565:         ### SET RUNTIME ENDPOINT ###",
          "1566:         endpoint_url = \"\"",
          "1567:         env_aws_bedrock_runtime_endpoint = get_secret(\"AWS_BEDROCK_RUNTIME_ENDPOINT\")",
          "1569:             aws_bedrock_runtime_endpoint, str",
          "1570:         ):",
          "1571:             endpoint_url = aws_bedrock_runtime_endpoint",
          "",
          "[Removed Lines]",
          "1568:         if aws_bedrock_runtime_endpoint is not None and isinstance(",
          "",
          "[Added Lines]",
          "1566:         if api_base is not None:",
          "1567:             endpoint_url = api_base",
          "1568:         elif aws_bedrock_runtime_endpoint is not None and isinstance(",
          "",
          "---------------"
        ],
        "litellm/llms/vertex_ai_and_google_ai_studio/gemini/vertex_and_google_ai_studio_gemini.py||litellm/llms/vertex_ai_and_google_ai_studio/gemini/vertex_and_google_ai_studio_gemini.py": [
          "File: litellm/llms/vertex_ai_and_google_ai_studio/gemini/vertex_and_google_ai_studio_gemini.py -> litellm/llms/vertex_ai_and_google_ai_studio/gemini/vertex_and_google_ai_studio_gemini.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1284:     ) -> Union[ModelResponse, CustomStreamWrapper]:",
          "1286:         request_body = await async_transform_request_body(**data)  # type: ignore",
          "1288:             _params = {}",
          "1289:             if timeout is not None:",
          "1290:                 if isinstance(timeout, float) or isinstance(timeout, int):",
          "",
          "[Removed Lines]",
          "1287:         if client is None:",
          "",
          "[Added Lines]",
          "1287:         if client is None or not isinstance(client, AsyncHTTPHandler):",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1293:             client = AsyncHTTPHandler(**_params)  # type: ignore",
          "1294:         else:",
          "1295:             client = client  # type: ignore",
          "1297:         try:",
          "1298:             response = await client.post(api_base, headers=headers, json=request_body)  # type: ignore",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1296:         ## LOGGING",
          "1297:         logging_obj.pre_call(",
          "1298:             input=messages,",
          "1299:             api_key=\"\",",
          "1300:             additional_args={",
          "1301:                 \"complete_input_dict\": request_body,",
          "1302:                 \"api_base\": api_base,",
          "1303:                 \"headers\": headers,",
          "1304:             },",
          "1305:         )",
          "",
          "---------------"
        ],
        "litellm/main.py||litellm/main.py": [
          "File: litellm/main.py -> litellm/main.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "2361:                     timeout=timeout,",
          "2362:                     acompletion=acompletion,",
          "2363:                     client=client,",
          "2364:                 )",
          "2365:             else:",
          "2366:                 response = bedrock_chat_completion.completion(",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "2364:                     api_base=api_base,",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "2378:                     timeout=timeout,",
          "2379:                     acompletion=acompletion,",
          "2380:                     client=client,",
          "2381:                 )",
          "2383:             if optional_params.get(\"stream\", False):",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "2382:                     api_base=api_base,",
          "",
          "---------------"
        ],
        "litellm/proxy/auth/model_checks.py||litellm/proxy/auth/model_checks.py": [
          "File: litellm/proxy/auth/model_checks.py -> litellm/proxy/auth/model_checks.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1: # What is this?",
          "2: ## Common checks for /v1/models and `/model/info`",
          "3: from typing import List, Optional",
          "6: from litellm._logging import verbose_proxy_logger",
          "9: def get_key_models(",
          "",
          "[Removed Lines]",
          "4: from litellm.proxy._types import UserAPIKeyAuth, SpecialModelNames",
          "5: from litellm.utils import get_valid_models",
          "",
          "[Added Lines]",
          "5: import litellm",
          "7: from litellm.proxy._types import SpecialModelNames, UserAPIKeyAuth",
          "8: from litellm.utils import get_valid_models",
          "11: def _check_wildcard_routing(model: str) -> bool:",
          "12:     \"\"\"",
          "13:     Returns True if a model is a provider wildcard.",
          "14:     \"\"\"",
          "15:     if model == \"*\":",
          "16:         return True",
          "18:     if \"/\" in model:",
          "19:         llm_provider, potential_wildcard = model.split(\"/\", 1)",
          "20:         if (",
          "21:             llm_provider in litellm.provider_list and potential_wildcard == \"*\"",
          "22:         ):  # e.g. anthropic/*",
          "23:             return True",
          "25:     return False",
          "28: def get_provider_models(provider: str) -> Optional[List[str]]:",
          "29:     \"\"\"",
          "30:     Returns the list of known models by provider",
          "31:     \"\"\"",
          "32:     if provider == \"*\":",
          "33:         return get_valid_models()",
          "35:     if provider in litellm.models_by_provider:",
          "36:         return litellm.models_by_provider[provider]",
          "38:     return None",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "58:     \"\"\"",
          "59:     - If key list is empty -> defer to team list",
          "60:     - If team list is empty -> defer to proxy model list",
          "61:     \"\"\"",
          "63:     unique_models = set()",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "94:     If list contains wildcard -> return known provider models",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "76:             valid_models = get_valid_models()",
          "77:             unique_models.update(valid_models)",
          "",
          "[Removed Lines]",
          "79:     return list(unique_models)",
          "",
          "[Added Lines]",
          "113:     models_to_remove = set()",
          "114:     all_wildcard_models = []",
          "115:     for model in unique_models:",
          "116:         if _check_wildcard_routing(model=model):",
          "117:             provider = model.split(\"/\")[0]",
          "118:             # get all known provider models",
          "119:             wildcard_models = get_provider_models(provider=provider)",
          "120:             if wildcard_models is not None:",
          "121:                 models_to_remove.add(model)",
          "122:                 all_wildcard_models.extend(wildcard_models)",
          "124:     for model in models_to_remove:",
          "125:         unique_models.remove(model)",
          "127:     return list(unique_models) + all_wildcard_models",
          "",
          "---------------"
        ],
        "litellm/router.py||litellm/router.py": [
          "File: litellm/router.py -> litellm/router.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "4700:                 )",
          "4701:             elif self.routing_strategy == \"simple-shuffle\":",
          "4702:                 # if users pass rpm or tpm, we do a random weighted pick - based on rpm/tpm",
          "4703:                 ############## Check if we can do a RPM/TPM based weighted pick #################",
          "4704:                 rpm = healthy_deployments[0].get(\"litellm_params\").get(\"rpm\", None)",
          "4705:                 if rpm is not None:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "4704:                 ############## Check if 'weight' param set for a weighted pick #################",
          "4705:                 weight = (",
          "4706:                     healthy_deployments[0].get(\"litellm_params\").get(\"weight\", None)",
          "4707:                 )",
          "4708:                 if weight is not None:",
          "4709:                     # use weight-random pick if rpms provided",
          "4710:                     weights = [",
          "4711:                         m[\"litellm_params\"].get(\"weight\", 0)",
          "4712:                         for m in healthy_deployments",
          "4713:                     ]",
          "4714:                     verbose_router_logger.debug(f\"\\nweight {weights}\")",
          "4715:                     total_weight = sum(weights)",
          "4716:                     weights = [weight / total_weight for weight in weights]",
          "4717:                     verbose_router_logger.debug(f\"\\n weights {weights}\")",
          "4718:                     # Perform weighted random pick",
          "4719:                     selected_index = random.choices(",
          "4720:                         range(len(weights)), weights=weights",
          "4721:                     )[0]",
          "4722:                     verbose_router_logger.debug(f\"\\n selected index, {selected_index}\")",
          "4723:                     deployment = healthy_deployments[selected_index]",
          "4724:                     verbose_router_logger.info(",
          "4725:                         f\"get_available_deployment for model: {model}, Selected deployment: {self.print_deployment(deployment) or deployment[0]} for model: {model}\"",
          "4726:                     )",
          "4727:                     return deployment or deployment[0]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "4847:             )",
          "4848:         elif self.routing_strategy == \"simple-shuffle\":",
          "4849:             # if users pass rpm or tpm, we do a random weighted pick - based on rpm/tpm",
          "4850:             ############## Check if we can do a RPM/TPM based weighted pick #################",
          "4851:             rpm = healthy_deployments[0].get(\"litellm_params\").get(\"rpm\", None)",
          "4852:             if rpm is not None:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "4875:             ############## Check 'weight' param set for weighted pick #################",
          "4876:             weight = healthy_deployments[0].get(\"litellm_params\").get(\"weight\", None)",
          "4877:             if weight is not None:",
          "4878:                 # use weight-random pick if rpms provided",
          "4879:                 weights = [",
          "4880:                     m[\"litellm_params\"].get(\"weight\", 0) for m in healthy_deployments",
          "4881:                 ]",
          "4882:                 verbose_router_logger.debug(f\"\\nweight {weights}\")",
          "4883:                 total_weight = sum(weights)",
          "4884:                 weights = [weight / total_weight for weight in weights]",
          "4885:                 verbose_router_logger.debug(f\"\\n weights {weights}\")",
          "4886:                 # Perform weighted random pick",
          "4887:                 selected_index = random.choices(range(len(weights)), weights=weights)[0]",
          "4888:                 verbose_router_logger.debug(f\"\\n selected index, {selected_index}\")",
          "4889:                 deployment = healthy_deployments[selected_index]",
          "4890:                 verbose_router_logger.info(",
          "4891:                     f\"get_available_deployment for model: {model}, Selected deployment: {self.print_deployment(deployment) or deployment[0]} for model: {model}\"",
          "4892:                 )",
          "4893:                 return deployment or deployment[0]",
          "",
          "---------------"
        ],
        "litellm/tests/test_bedrock_completion.py||litellm/tests/test_bedrock_completion.py": [
          "File: litellm/tests/test_bedrock_completion.py -> litellm/tests/test_bedrock_completion.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "960:                 messages=[{\"role\": \"user\", \"content\": \"What's AWS?\"}],",
          "961:                 client=client,",
          "962:                 extra_headers={\"test\": \"hello world\", \"Authorization\": \"my-test-key\"},",
          "963:             )",
          "964:         except Exception as e:",
          "965:             pass",
          "968:         assert \"test\" in mock_client_post.call_args.kwargs[\"headers\"]",
          "969:         assert mock_client_post.call_args.kwargs[\"headers\"][\"test\"] == \"hello world\"",
          "970:         assert (",
          "",
          "[Removed Lines]",
          "967:         print(f\"mock_client_post.call_args: {mock_client_post.call_args}\")",
          "",
          "[Added Lines]",
          "963:                 api_base=\"https://gateway.ai.cloudflare.com/v1/fa4cdcab1f32b95ca3b53fd36043d691/test/aws-bedrock/bedrock-runtime/us-east-1\",",
          "968:         print(f\"mock_client_post.call_args.kwargs: {mock_client_post.call_args.kwargs}\")",
          "969:         assert (",
          "970:             mock_client_post.call_args.kwargs[\"url\"]",
          "971:             == \"https://gateway.ai.cloudflare.com/v1/fa4cdcab1f32b95ca3b53fd36043d691/test/aws-bedrock/bedrock-runtime/us-east-1/model/anthropic.claude-3-sonnet-20240229-v1:0/converse\"",
          "972:         )",
          "",
          "---------------"
        ],
        "litellm/tests/test_custom_callback_input.py||litellm/tests/test_custom_callback_input.py": [
          "File: litellm/tests/test_custom_callback_input.py -> litellm/tests/test_custom_callback_input.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1347:         assert standard_logging_object[\"cache_hit\"] is True",
          "1348:         assert standard_logging_object[\"response_cost\"] == 0",
          "1349:         assert standard_logging_object[\"saved_cache_cost\"] > 0",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1352: def test_logging_key_masking_gemini():",
          "1353:     customHandler = CompletionCustomHandler()",
          "1354:     litellm.callbacks = [customHandler]",
          "1355:     litellm.success_callback = []",
          "1357:     with patch.object(",
          "1358:         customHandler, \"log_pre_api_call\", new=MagicMock()",
          "1359:     ) as mock_client:",
          "1360:         try:",
          "1361:             resp = litellm.completion(",
          "1362:                 model=\"gemini/gemini-1.5-pro\",",
          "1363:                 messages=[{\"role\": \"user\", \"content\": \"Hey, how's it going?\"}],",
          "1364:                 api_key=\"LEAVE_ONLY_LAST_4_CHAR_UNMASKED_THIS_PART\",",
          "1365:             )",
          "1366:         except litellm.AuthenticationError:",
          "1367:             pass",
          "1369:         mock_client.assert_called()",
          "1371:         print(f\"mock_client.call_args.kwargs: {mock_client.call_args.kwargs}\")",
          "1372:         assert (",
          "1373:             \"LEAVE_ONLY_LAST_4_CHAR_UNMASKED_THIS_PART\"",
          "1374:             not in mock_client.call_args.kwargs[\"kwargs\"][\"litellm_params\"][\"api_base\"]",
          "1375:         )",
          "1376:         key = mock_client.call_args.kwargs[\"kwargs\"][\"litellm_params\"][\"api_base\"]",
          "1377:         trimmed_key = key.split(\"key=\")[1]",
          "1378:         trimmed_key = trimmed_key.replace(\"*\", \"\")",
          "1379:         assert \"PART\" == trimmed_key",
          "",
          "---------------"
        ],
        "litellm/tests/test_router.py||litellm/tests/test_router.py": [
          "File: litellm/tests/test_router.py -> litellm/tests/test_router.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "2342:             assert e.cooldown_time == cooldown_time",
          "2344:         assert exception_raised",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "2347: @pytest.mark.parametrize(\"sync_mode\", [True, False])",
          "2348: @pytest.mark.asyncio()",
          "2349: async def test_router_weighted_pick(sync_mode):",
          "2350:     router = Router(",
          "2351:         model_list=[",
          "2352:             {",
          "2353:                 \"model_name\": \"gpt-3.5-turbo\",",
          "2354:                 \"litellm_params\": {",
          "2355:                     \"model\": \"gpt-3.5-turbo\",",
          "2356:                     \"weight\": 2,",
          "2357:                     \"mock_response\": \"Hello world 1!\",",
          "2358:                 },",
          "2359:                 \"model_info\": {\"id\": \"1\"},",
          "2360:             },",
          "2361:             {",
          "2362:                 \"model_name\": \"gpt-3.5-turbo\",",
          "2363:                 \"litellm_params\": {",
          "2364:                     \"model\": \"gpt-3.5-turbo\",",
          "2365:                     \"weight\": 1,",
          "2366:                     \"mock_response\": \"Hello world 2!\",",
          "2367:                 },",
          "2368:                 \"model_info\": {\"id\": \"2\"},",
          "2369:             },",
          "2370:         ]",
          "2371:     )",
          "2373:     model_id_1_count = 0",
          "2374:     model_id_2_count = 0",
          "2375:     for _ in range(50):",
          "2376:         # make 50 calls. expect model id 1 to be picked more than model id 2",
          "2377:         if sync_mode:",
          "2378:             response = router.completion(",
          "2379:                 model=\"gpt-3.5-turbo\",",
          "2380:                 messages=[{\"role\": \"user\", \"content\": \"Hello world!\"}],",
          "2381:             )",
          "2382:         else:",
          "2383:             response = await router.acompletion(",
          "2384:                 model=\"gpt-3.5-turbo\",",
          "2385:                 messages=[{\"role\": \"user\", \"content\": \"Hello world!\"}],",
          "2386:             )",
          "2388:         model_id = int(response._hidden_params[\"model_id\"])",
          "2390:         if model_id == 1:",
          "2391:             model_id_1_count += 1",
          "2392:         elif model_id == 2:",
          "2393:             model_id_2_count += 1",
          "2394:         else:",
          "2395:             raise Exception(\"invalid model id returned!\")",
          "2396:     assert model_id_1_count > model_id_2_count",
          "",
          "---------------"
        ],
        "litellm/types/router.py||litellm/types/router.py": [
          "File: litellm/types/router.py -> litellm/types/router.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "299:     custom_llm_provider: Optional[str]",
          "300:     tpm: Optional[int]",
          "301:     rpm: Optional[int]",
          "302:     api_key: Optional[str]",
          "303:     api_base: Optional[str]",
          "304:     api_version: Optional[str]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "302:     order: Optional[int]",
          "303:     weight: Optional[int]",
          "",
          "---------------"
        ],
        "litellm/utils.py||litellm/utils.py": [
          "File: litellm/utils.py -> litellm/utils.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "7475:                         ),",
          "7476:                         litellm_debug_info=extra_information,",
          "7477:                     )",
          "7478:                 elif \"403\" in error_str:",
          "7479:                     exception_mapping_worked = True",
          "7480:                     raise BadRequestError(",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "7478:                 elif \"API key not valid.\" in error_str:",
          "7479:                     exception_mapping_worked = True",
          "7480:                     raise AuthenticationError(",
          "7481:                         message=f\"{custom_llm_provider}Exception - {error_str}\",",
          "7482:                         model=model,",
          "7483:                         llm_provider=custom_llm_provider,",
          "7484:                         litellm_debug_info=extra_information,",
          "7485:                     )",
          "",
          "---------------"
        ]
      }
    }
  ]
}