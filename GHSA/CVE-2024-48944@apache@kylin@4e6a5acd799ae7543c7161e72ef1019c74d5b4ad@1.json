{
  "cve_id": "CVE-2024-48944",
  "cve_desc": "Server-Side Request Forgery (SSRF) vulnerability in Apache Kylin. Through a kylin server, an attacker may forge a request to invoke \"/kylin/api/xxx/diag\" api on another internal host and possibly get leaked information. There are two preconditions: 1) The attacker has got admin access to a kylin server; 2) Another internal host has the \"/kylin/api/xxx/diag\" api\n\nendpoint open for service.\n\n\nThis issue affects Apache Kylin: from 5.0.0 \nthrough \n\n5.0.1.\n\nUsers are recommended to upgrade to version 5.0.2, which fixes the issue.",
  "repo": "apache/kylin",
  "patch_hash": "4e6a5acd799ae7543c7161e72ef1019c74d5b4ad",
  "patch_info": {
    "commit_hash": "4e6a5acd799ae7543c7161e72ef1019c74d5b4ad",
    "repo": "apache/kylin",
    "commit_url": "https://github.com/apache/kylin/commit/4e6a5acd799ae7543c7161e72ef1019c74d5b4ad",
    "files": [
      "src/common-server/src/main/java/org/apache/kylin/rest/controller/NBasicController.java",
      "src/common-server/src/test/java/org/apache/kylin/rest/controller/NBasicControllerTest.java"
    ],
    "message": "KYLIN-5644 fix diag api security, encryption changed from base64 to AES\n\nCo-authored-by: liang.hua <liang.hua@kyligence.io>",
    "before_after_code_files": [
      "src/common-server/src/main/java/org/apache/kylin/rest/controller/NBasicController.java||src/common-server/src/main/java/org/apache/kylin/rest/controller/NBasicController.java",
      "src/common-server/src/test/java/org/apache/kylin/rest/controller/NBasicControllerTest.java||src/common-server/src/test/java/org/apache/kylin/rest/controller/NBasicControllerTest.java"
    ]
  },
  "patch_diff": {
    "src/common-server/src/main/java/org/apache/kylin/rest/controller/NBasicController.java||src/common-server/src/main/java/org/apache/kylin/rest/controller/NBasicController.java": [
      "File: src/common-server/src/main/java/org/apache/kylin/rest/controller/NBasicController.java -> src/common-server/src/main/java/org/apache/kylin/rest/controller/NBasicController.java",
      "--- Hunk 1 ---",
      "[Context before]",
      "81: import org.apache.kylin.common.msg.Message;",
      "82: import org.apache.kylin.common.msg.MsgPicker;",
      "83: import org.apache.kylin.common.persistence.transaction.TransactionException;",
      "84: import org.apache.kylin.common.util.JsonUtil;",
      "85: import org.apache.kylin.common.util.Pair;",
      "86: import org.apache.kylin.job.constant.JobStatusEnum;",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "84: import org.apache.kylin.common.util.EncryptUtil;",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "643:             if (StringUtils.isBlank(host) || host.startsWith(\"http://\")) {",
      "644:                 return host;",
      "645:             }",
      "647:         } catch (Exception e) {",
      "648:             logger.error(\"Failed to decode host, will use the original host name\");",
      "649:         }",
      "",
      "[Removed Lines]",
      "646:             return new String(Base64.decodeBase64(host), Charset.defaultCharset());",
      "",
      "[Added Lines]",
      "647:             String decryptValue = EncryptUtil.decrypt(new String(Base64.decodeBase64(host), Charset.defaultCharset()));",
      "648:             return StringUtils.isBlank(decryptValue) ? host : decryptValue;",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "659:             if (!host.toLowerCase().startsWith(\"http\")) {",
      "660:                 host = \"http://\" + host;",
      "661:             }",
      "663:         } catch (Exception e) {",
      "664:             logger.error(\"Failed to encode host, will use the original host name\");",
      "665:         }",
      "",
      "[Removed Lines]",
      "662:             return Base64.encodeBase64String(host.getBytes(Charset.defaultCharset()));",
      "",
      "[Added Lines]",
      "664:             return Base64.encodeBase64String(EncryptUtil.encrypt(host).getBytes(Charset.defaultCharset()));",
      "",
      "---------------"
    ],
    "src/common-server/src/test/java/org/apache/kylin/rest/controller/NBasicControllerTest.java||src/common-server/src/test/java/org/apache/kylin/rest/controller/NBasicControllerTest.java": [
      "File: src/common-server/src/test/java/org/apache/kylin/rest/controller/NBasicControllerTest.java -> src/common-server/src/test/java/org/apache/kylin/rest/controller/NBasicControllerTest.java",
      "--- Hunk 1 ---",
      "[Context before]",
      "311:         }",
      "312:         Assert.assertEquals(3, mockDataResponse.get(\"size\"));",
      "313:     }",
      "315: }",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "315:     @Test",
      "316:     public void testEncodeAndDecodeHost() {",
      "317:         Assert.assertTrue(nBasicController.encodeHost(\"\").isEmpty());",
      "318:         String host = \"localhost:7070\";",
      "319:         String encodeHost = nBasicController.encodeHost(host);",
      "320:         Assert.assertNotNull(encodeHost);",
      "321:         Assert.assertNotEquals(host, encodeHost);",
      "322:         String decodeHost = nBasicController.decodeHost(encodeHost);",
      "323:         Assert.assertEquals(\"http://\" + host, decodeHost);",
      "324:         Assert.assertEquals(\"ip\", nBasicController.decodeHost(\"ip\"));",
      "325:     }",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "7973ad7139f018294b76cf690e52f4e4df107d50",
      "candidate_info": {
        "commit_hash": "7973ad7139f018294b76cf690e52f4e4df107d50",
        "repo": "apache/kylin",
        "commit_url": "https://github.com/apache/kylin/commit/7973ad7139f018294b76cf690e52f4e4df107d50",
        "files": [
          "src/core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java",
          "src/core-common/src/main/java/org/apache/kylin/common/util/FileUtils.java",
          "src/tool/src/main/java/org/apache/kylin/tool/hive/HiveClientJarTool.java",
          "src/tool/src/test/java/org/apache/kylin/tool/hive/HiveClientJarToolTest.java",
          "src/tool/src/test/java/org/apache/kylin/tool/hive/HiveClientJarToolTestBase.java",
          "src/tool/src/test/java/org/apache/kylin/tool/hive/HiveClientJarToolWithoutSparkHiveDirTest.java"
        ],
        "message": "KYLIN-5647 add put hive_1_2_2 to HDFS before KE start (#30313)",
        "before_after_code_files": [
          "src/core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java||src/core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java",
          "src/core-common/src/main/java/org/apache/kylin/common/util/FileUtils.java||src/core-common/src/main/java/org/apache/kylin/common/util/FileUtils.java",
          "src/tool/src/main/java/org/apache/kylin/tool/hive/HiveClientJarTool.java||src/tool/src/main/java/org/apache/kylin/tool/hive/HiveClientJarTool.java",
          "src/tool/src/test/java/org/apache/kylin/tool/hive/HiveClientJarToolTest.java||src/tool/src/test/java/org/apache/kylin/tool/hive/HiveClientJarToolTest.java",
          "src/tool/src/test/java/org/apache/kylin/tool/hive/HiveClientJarToolTestBase.java||src/tool/src/test/java/org/apache/kylin/tool/hive/HiveClientJarToolTestBase.java",
          "src/tool/src/test/java/org/apache/kylin/tool/hive/HiveClientJarToolWithoutSparkHiveDirTest.java||src/tool/src/test/java/org/apache/kylin/tool/hive/HiveClientJarToolWithoutSparkHiveDirTest.java"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/kylin/pull/2138"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "src/core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java||src/core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java": [
          "File: src/core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java -> src/core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "1520:         return getOptional(\"kylin.engine.spark-conf.spark.submit.deployMode\", \"client\").toLowerCase(Locale.ROOT);",
          "1521:     }",
          "1523:     public String getSparkBuildClassName() {",
          "1524:         return getOptional(\"kylin.engine.spark.build-class-name\", \"org.apache.kylin.engine.spark.job.SegmentBuildJob\");",
          "1525:     }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1523:     public String getSparkSqlHiveMetastoreJarsPath() {",
          "1524:         return getOptional(\"kylin.engine.spark-conf.spark.sql.hive.metastore.jars.path\", \"\");",
          "1525:     }",
          "1527:     public boolean getHiveClientJarUploadEnable() {",
          "1528:         return Boolean.parseBoolean(getOptional(\"kylin.engine.hive-client-jar-upload.enable\", FALSE));",
          "1529:     }",
          "",
          "---------------"
        ],
        "src/core-common/src/main/java/org/apache/kylin/common/util/FileUtils.java||src/core-common/src/main/java/org/apache/kylin/common/util/FileUtils.java": [
          "File: src/core-common/src/main/java/org/apache/kylin/common/util/FileUtils.java -> src/core-common/src/main/java/org/apache/kylin/common/util/FileUtils.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "57:         return Lists.newArrayList();",
          "58:     }",
          "60:     public static Map<String, String> readFromPropertiesFile(File file) {",
          "61:         try (FileInputStream fileInputStream = new FileInputStream(file)) {",
          "62:             return readFromPropertiesFile(fileInputStream);",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "59:     public static List<File> findFiles(String dir) {",
          "60:         File[] files = new File(dir).listFiles();",
          "61:         if (files != null) {",
          "62:             return Lists.newArrayList(files);",
          "63:         }",
          "64:         return Lists.newArrayList();",
          "65:     }",
          "",
          "---------------"
        ],
        "src/tool/src/main/java/org/apache/kylin/tool/hive/HiveClientJarTool.java||src/tool/src/main/java/org/apache/kylin/tool/hive/HiveClientJarTool.java": [
          "File: src/tool/src/main/java/org/apache/kylin/tool/hive/HiveClientJarTool.java -> src/tool/src/main/java/org/apache/kylin/tool/hive/HiveClientJarTool.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "19: package org.apache.kylin.tool.hive;",
          "21: import java.io.File;",
          "22: import java.io.IOException;",
          "24: import org.apache.commons.lang3.StringUtils;",
          "25: import org.apache.hadoop.fs.FileSystem;",
          "26: import org.apache.hadoop.fs.Path;",
          "27: import org.apache.kylin.common.KylinConfig;",
          "28: import org.apache.kylin.common.util.FileUtils;",
          "29: import org.apache.kylin.common.util.HadoopUtil;",
          "30: import org.apache.kylin.common.util.Unsafe;",
          "31: import org.apache.kylin.tool.util.ToolMainWrapper;",
          "33: import lombok.val;",
          "34: import lombok.extern.slf4j.Slf4j;",
          "36: @Slf4j",
          "37: public class HiveClientJarTool {",
          "38:     public static void main(String[] args) {",
          "39:         ToolMainWrapper.wrap(args, () -> {",
          "40:             HiveClientJarTool tool = new HiveClientJarTool();",
          "41:             tool.execute();",
          "42:         });",
          "43:         Unsafe.systemExit(0);",
          "44:     }",
          "46:     public void execute() throws IOException {",
          "47:         val config = KylinConfig.getInstanceFromEnv();",
          "48:         if (!config.getHiveClientJarUploadEnable()) {",
          "49:             log.info(\"Not need upload hive client jar\");",
          "50:             return;",
          "51:         }",
          "52:         if (StringUtils.isBlank(config.getSparkSqlHiveMetastoreJarsPath())) {",
          "53:             log.warn(\"kylin.engine.spark-conf.spark.sql.hive.metastore.jars.path not setting\");",
          "54:             return;",
          "55:         }",
          "59:         val fileSystem = HadoopUtil.getWorkingFileSystem();",
          "60:         val sparkSqlHiveMetastoreJarsPath = config.getSparkSqlHiveMetastoreJarsPath();",
          "61:         val jarsDirPath = new Path(sparkSqlHiveMetastoreJarsPath).getParent();",
          "62:         val uploadHiveJarFlag = new Path(jarsDirPath, \"_upload_hive_jar_by_pass\");",
          "63:         if (fileSystem.exists(uploadHiveJarFlag)) {",
          "64:             log.info(\"Not need upload Spark HIVE jars again\");",
          "65:             return;",
          "66:         }",
          "68:         val kylinSparkHiveJarsPath = getKylinSparkHiveJarsPath();",
          "69:         if (StringUtils.isBlank(kylinSparkHiveJarsPath)) {",
          "70:             log.warn(\"${KYLIN_HOME}/spark/hive_1_2_2 needs to be an existing directory\");",
          "71:             return;",
          "72:         }",
          "74:         uploadHiveJars(fileSystem, uploadHiveJarFlag, kylinSparkHiveJarsPath, jarsDirPath);",
          "75:     }",
          "77:     public void uploadHiveJars(FileSystem fileSystem, Path uploadHiveJarFlag, String kylinSparkHiveJarsPath,",
          "78:             Path jarsDirPath) throws IOException {",
          "79:         if (fileSystem.exists(jarsDirPath)) {",
          "80:             log.warn(\"HDFS dir [{}] exist, not upload hive client jar\", jarsDirPath);",
          "81:             return;",
          "82:         }",
          "83:         fileSystem.mkdirs(jarsDirPath);",
          "84:         val hiveJars = FileUtils.findFiles(kylinSparkHiveJarsPath);",
          "85:         for (File jar : hiveJars) {",
          "86:             val sparkHiveJarPath = new Path(jar.getCanonicalPath());",
          "87:             fileSystem.copyFromLocalFile(sparkHiveJarPath, jarsDirPath);",
          "88:         }",
          "89:         log.info(\"Upload Spark HIVE jars ending\");",
          "90:         try (val out = fileSystem.create(uploadHiveJarFlag, true)) {",
          "91:             out.write(new byte[] {});",
          "92:         }",
          "93:         log.info(\"Upload Spark HIVE jars success\");",
          "94:     }",
          "96:     public String getKylinSparkHiveJarsPath() throws IOException {",
          "97:         String sparkHome = KylinConfig.getSparkHome();",
          "98:         val jar = FileUtils.findFile(sparkHome, \"hive_1_2_2\");",
          "99:         if (jar == null || jar.isFile()) {",
          "100:             return \"\";",
          "101:         }",
          "102:         return jar.getCanonicalPath();",
          "103:     }",
          "104: }",
          "",
          "---------------"
        ],
        "src/tool/src/test/java/org/apache/kylin/tool/hive/HiveClientJarToolTest.java||src/tool/src/test/java/org/apache/kylin/tool/hive/HiveClientJarToolTest.java": [
          "File: src/tool/src/test/java/org/apache/kylin/tool/hive/HiveClientJarToolTest.java -> src/tool/src/test/java/org/apache/kylin/tool/hive/HiveClientJarToolTest.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "19: package org.apache.kylin.tool.hive;",
          "21: import static org.junit.jupiter.api.Assertions.assertEquals;",
          "23: import java.io.File;",
          "24: import java.io.IOException;",
          "25: import java.nio.file.Files;",
          "26: import java.nio.file.Paths;",
          "27: import java.util.stream.Collectors;",
          "29: import org.apache.hadoop.fs.Path;",
          "30: import org.apache.kylin.common.KylinConfig;",
          "31: import org.apache.kylin.common.util.HadoopUtil;",
          "32: import org.apache.kylin.junit.annotation.MetadataInfo;",
          "33: import org.apache.logging.log4j.Level;",
          "34: import org.apache.logging.log4j.core.LogEvent;",
          "35: import org.junit.jupiter.api.AfterEach;",
          "36: import org.junit.jupiter.api.BeforeEach;",
          "37: import org.junit.jupiter.api.Test;",
          "38: import org.mockito.ArgumentCaptor;",
          "39: import org.mockito.Mockito;",
          "41: import lombok.val;",
          "43: @MetadataInfo",
          "44: class HiveClientJarToolTest extends HiveClientJarToolTestBase {",
          "46:     @BeforeEach",
          "47:     public void before() throws IOException {",
          "48:         super.before();",
          "49:         val sparkHome = KylinConfig.getSparkHome();",
          "50:         val sparkPath = Paths.get(sparkHome);",
          "51:         val hive122 = sparkHome + File.separator + \"hive_1_2_2\";",
          "52:         val hive122Path = Paths.get(hive122);",
          "53:         val jar = hive122 + File.separator + \"test.jar\";",
          "54:         val jarPath = Paths.get(jar);",
          "55:         Files.createDirectories(hive122Path);",
          "56:         Files.createFile(jarPath);",
          "57:     }",
          "59:     @AfterEach",
          "60:     public void after() throws IOException {",
          "61:         super.after();",
          "62:         val sparkHome = KylinConfig.getSparkHome();",
          "63:         val sparkPath = Paths.get(sparkHome);",
          "64:         val hive122 = sparkHome + File.separator + \"hive_1_2_2\";",
          "65:         val hive122Path = Paths.get(hive122);",
          "66:         val jar = hive122 + File.separator + \"test.jar\";",
          "67:         val jarPath = Paths.get(jar);",
          "68:         Files.deleteIfExists(jarPath);",
          "69:         Files.deleteIfExists(hive122Path);",
          "70:         Files.deleteIfExists(sparkPath);",
          "71:     }",
          "73:     @Test",
          "74:     void uploadHiveJars() throws IOException {",
          "75:         uploadHiveJars(false);",
          "76:         uploadHiveJars(true);",
          "77:     }",
          "79:     void uploadHiveJars(boolean exist) throws IOException {",
          "80:         val config = KylinConfig.getInstanceFromEnv();",
          "81:         val fileSystem = HadoopUtil.getWorkingFileSystem();",
          "82:         val hive = new Path(config.getHdfsWorkingDirectory(), \"hive\");",
          "83:         val uploadHiveJarFlag = new Path(config.getHdfsWorkingDirectory(), \"upload_hive_jar\");",
          "84:         try {",
          "85:             config.setProperty(\"kylin.engine.spark-conf.spark.sql.hive.metastore.jars.path\",",
          "86:                     config.getHdfsWorkingDirectory() + \"hive/*\");",
          "88:             if (exist) {",
          "89:                 fileSystem.mkdirs(hive);",
          "90:             }",
          "91:             val kylinSparkHiveJarsPath = uploadHiveJarsTool.getKylinSparkHiveJarsPath();",
          "92:             val sparkSqlHiveMetastoreJarsPath = config.getSparkSqlHiveMetastoreJarsPath();",
          "93:             val jarsDirPath = new Path(sparkSqlHiveMetastoreJarsPath).getParent();",
          "94:             uploadHiveJarsTool.uploadHiveJars(fileSystem, uploadHiveJarFlag, kylinSparkHiveJarsPath, jarsDirPath);",
          "96:             if (exist) {",
          "97:                 ArgumentCaptor<LogEvent> logCaptor = ArgumentCaptor.forClass(LogEvent.class);",
          "98:                 Mockito.verify(appender, Mockito.atLeast(0)).append(logCaptor.capture());",
          "99:                 val logs = logCaptor.getAllValues().stream()",
          "100:                         .filter(event -> event.getLoggerName().equals(\"org.apache.kylin.tool.hive.HiveClientJarTool\"))",
          "101:                         .filter(event -> event.getLevel().equals(Level.WARN))",
          "102:                         .map(event -> event.getMessage().getFormattedMessage()).collect(Collectors.toList());",
          "103:                 val logCount = logs.stream()",
          "104:                         .filter(log -> log.equals(\"HDFS dir [\" + hive + \"] exist, not upload hive client jar\")).count();",
          "105:                 assertEquals(1, logCount);",
          "106:             } else {",
          "107:                 val hdfsHiveJarsPath = new Path(config.getSparkSqlHiveMetastoreJarsPath().substring(0,",
          "108:                         config.getSparkSqlHiveMetastoreJarsPath().length() - 1));",
          "109:                 val fileStatuses = HadoopUtil.getWorkingFileSystem().listStatus(hdfsHiveJarsPath);",
          "110:                 assertEquals(1, fileStatuses.length);",
          "111:                 val hdfsJarPath = fileStatuses[0].getPath();",
          "112:                 assertEquals(hdfsHiveJarsPath + \"/test.jar\", hdfsJarPath.toString());",
          "113:             }",
          "114:             assertEquals(!exist, fileSystem.exists(uploadHiveJarFlag));",
          "115:         } finally {",
          "116:             config.setProperty(\"kylin.engine.spark-conf.spark.sql.hive.metastore.jars.path\", \"\");",
          "117:             fileSystem.delete(uploadHiveJarFlag, true);",
          "118:             fileSystem.delete(hive, true);",
          "119:         }",
          "120:     }",
          "122:     @Test",
          "123:     void testExecute() throws IOException {",
          "124:         testExecute(true, false, \"kylin.engine.spark-conf.spark.sql.hive.metastore.jars.path not setting\");",
          "125:         testExecute(true, true, \"Upload Spark HIVE jars success\");",
          "126:         testExecute(true, true, \"Not need upload Spark HIVE jars again\");",
          "127:         testExecute(false, false, \"Not need upload hive client jar\");",
          "128:     }",
          "130: }",
          "",
          "---------------"
        ],
        "src/tool/src/test/java/org/apache/kylin/tool/hive/HiveClientJarToolTestBase.java||src/tool/src/test/java/org/apache/kylin/tool/hive/HiveClientJarToolTestBase.java": [
          "File: src/tool/src/test/java/org/apache/kylin/tool/hive/HiveClientJarToolTestBase.java -> src/tool/src/test/java/org/apache/kylin/tool/hive/HiveClientJarToolTestBase.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "19: package org.apache.kylin.tool.hive;",
          "21: import static org.junit.jupiter.api.Assertions.assertEquals;",
          "23: import java.io.IOException;",
          "24: import java.util.stream.Collectors;",
          "26: import org.apache.kylin.common.KylinConfig;",
          "27: import org.apache.logging.log4j.Level;",
          "28: import org.apache.logging.log4j.LogManager;",
          "29: import org.apache.logging.log4j.core.Appender;",
          "30: import org.apache.logging.log4j.core.LogEvent;",
          "31: import org.apache.logging.log4j.core.Logger;",
          "32: import org.mockito.ArgumentCaptor;",
          "33: import org.mockito.InjectMocks;",
          "34: import org.mockito.Mock;",
          "35: import org.mockito.Mockito;",
          "37: import lombok.val;",
          "39: public class HiveClientJarToolTestBase {",
          "40:     @InjectMocks",
          "41:     protected HiveClientJarTool uploadHiveJarsTool = Mockito.spy(HiveClientJarTool.class);",
          "42:     @Mock",
          "43:     protected Appender appender = Mockito.mock(Appender.class);",
          "45:     protected void before() throws IOException {",
          "46:         Mockito.when(appender.getName()).thenReturn(\"mocked\");",
          "47:         Mockito.when(appender.isStarted()).thenReturn(true);",
          "48:         ((Logger) LogManager.getRootLogger()).addAppender(appender);",
          "49:     }",
          "51:     protected void after() throws IOException {",
          "52:         ((Logger) LogManager.getRootLogger()).removeAppender(appender);",
          "53:     }",
          "55:     protected void testExecute(boolean upload, boolean setJarsPath, String message) throws IOException {",
          "56:         val config = KylinConfig.getInstanceFromEnv();",
          "57:         try {",
          "58:             if (setJarsPath) {",
          "59:                 config.setProperty(\"kylin.engine.spark-conf.spark.sql.hive.metastore.jars.path\",",
          "60:                         config.getHdfsWorkingDirectory() + \"hive/*\");",
          "61:             }",
          "62:             config.setProperty(\"kylin.engine.hive-client-jar-upload.enable\", String.valueOf(upload));",
          "63:             uploadHiveJarsTool.execute();",
          "64:             ArgumentCaptor<LogEvent> logCaptor = ArgumentCaptor.forClass(LogEvent.class);",
          "65:             Mockito.verify(appender, Mockito.atLeast(0)).append(logCaptor.capture());",
          "66:             val logs = logCaptor.getAllValues().stream()",
          "67:                     .filter(event -> event.getLoggerName().equals(\"org.apache.kylin.tool.hive.HiveClientJarTool\"))",
          "68:                     .filter(event -> event.getLevel().equals(Level.INFO) || event.getLevel().equals(Level.WARN))",
          "69:                     .map(event -> event.getMessage().getFormattedMessage()).collect(Collectors.toList());",
          "70:             val logCount = logs.stream().filter(log -> log.equals(message)).count();",
          "71:             assertEquals(1, logCount);",
          "72:         } finally {",
          "73:             config.setProperty(\"kylin.engine.spark-conf.spark.sql.hive.metastore.jars.path\", \"\");",
          "74:             config.setProperty(\"kylin.engine.hive-client-jar-upload.enable\", \"false\");",
          "75:         }",
          "76:     }",
          "77: }",
          "",
          "---------------"
        ],
        "src/tool/src/test/java/org/apache/kylin/tool/hive/HiveClientJarToolWithoutSparkHiveDirTest.java||src/tool/src/test/java/org/apache/kylin/tool/hive/HiveClientJarToolWithoutSparkHiveDirTest.java": [
          "File: src/tool/src/test/java/org/apache/kylin/tool/hive/HiveClientJarToolWithoutSparkHiveDirTest.java -> src/tool/src/test/java/org/apache/kylin/tool/hive/HiveClientJarToolWithoutSparkHiveDirTest.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "19: package org.apache.kylin.tool.hive;",
          "21: import static org.junit.jupiter.api.Assertions.assertEquals;",
          "22: import static org.junit.jupiter.api.Assertions.assertTrue;",
          "24: import java.io.File;",
          "25: import java.io.IOException;",
          "26: import java.nio.file.Files;",
          "27: import java.nio.file.Paths;",
          "29: import org.apache.commons.lang3.StringUtils;",
          "30: import org.apache.kylin.common.KylinConfig;",
          "31: import org.apache.kylin.junit.annotation.MetadataInfo;",
          "32: import org.junit.jupiter.api.AfterEach;",
          "33: import org.junit.jupiter.api.BeforeEach;",
          "34: import org.junit.jupiter.api.Test;",
          "36: import lombok.val;",
          "37: import lombok.var;",
          "39: @MetadataInfo",
          "40: class HiveClientJarToolWithoutSparkHiveDirTest extends HiveClientJarToolTestBase {",
          "41:     @BeforeEach",
          "42:     public void before() throws IOException {",
          "43:         super.before();",
          "44:     }",
          "46:     @AfterEach",
          "47:     public void after() throws IOException {",
          "48:         super.after();",
          "49:         val sparkHome = KylinConfig.getSparkHome();",
          "50:         val sparkPath = Paths.get(sparkHome);",
          "51:         val hive122 = sparkHome + File.separator + \"hive_1_2_2\";",
          "52:         val hive122Path = Paths.get(hive122);",
          "53:         Files.deleteIfExists(hive122Path);",
          "54:         Files.deleteIfExists(sparkPath);",
          "55:     }",
          "57:     @Test",
          "58:     void getKylinSparkHiveJarsPath() throws IOException {",
          "59:         val sparkHome = KylinConfig.getSparkHome();",
          "60:         val sparkPath = Paths.get(sparkHome);",
          "61:         val hive122 = sparkHome + File.separator + \"hive_1_2_2\";",
          "62:         val hive122Path = Paths.get(hive122);",
          "63:         try {",
          "64:             var kylinSparkHiveJarsPath = uploadHiveJarsTool.getKylinSparkHiveJarsPath();",
          "65:             assertTrue(StringUtils.isBlank(kylinSparkHiveJarsPath));",
          "66:             Files.createDirectory(sparkPath);",
          "67:             Files.createFile(hive122Path);",
          "68:             kylinSparkHiveJarsPath = uploadHiveJarsTool.getKylinSparkHiveJarsPath();",
          "69:             assertTrue(StringUtils.isBlank(kylinSparkHiveJarsPath));",
          "71:             Files.deleteIfExists(hive122Path);",
          "72:             Files.createDirectory(hive122Path);",
          "73:             kylinSparkHiveJarsPath = uploadHiveJarsTool.getKylinSparkHiveJarsPath();",
          "74:             assertEquals(new File(hive122).getCanonicalPath(), kylinSparkHiveJarsPath);",
          "75:         } finally {",
          "76:             Files.deleteIfExists(hive122Path);",
          "77:             Files.deleteIfExists(sparkPath);",
          "78:         }",
          "79:     }",
          "81:     @Test",
          "82:     void testExecute() throws IOException {",
          "83:         testExecute(true, true, \"${KYLIN_HOME}/spark/hive_1_2_2 needs to be an existing directory\");",
          "84:     }",
          "86:     @Test",
          "87:     void testExecuteWithHive122File() throws IOException {",
          "88:         val sparkHome = KylinConfig.getSparkHome();",
          "89:         val sparkPath = Paths.get(sparkHome);",
          "90:         val hive122 = sparkHome + File.separator + \"hive_1_2_2\";",
          "91:         val hive122Path = Paths.get(hive122);",
          "92:         Files.createDirectories(sparkPath);",
          "93:         Files.createFile(hive122Path);",
          "94:         testExecute(true, true, \"${KYLIN_HOME}/spark/hive_1_2_2 needs to be an existing directory\");",
          "95:     }",
          "96: }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "e62f6ea42fc420045597b4d9d35b06a9a1606f52",
      "candidate_info": {
        "commit_hash": "e62f6ea42fc420045597b4d9d35b06a9a1606f52",
        "repo": "apache/kylin",
        "commit_url": "https://github.com/apache/kylin/commit/e62f6ea42fc420045597b4d9d35b06a9a1606f52",
        "files": [
          "src/core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java",
          "src/spark-project/engine-spark/src/main/java/org/apache/kylin/engine/spark/application/SparkApplication.java",
          "src/spark-project/engine-spark/src/main/java/org/apache/kylin/engine/spark/job/NSparkExecutable.java",
          "src/spark-project/engine-spark/src/test/java/org/apache/kylin/engine/spark/application/SparkApplicationTest.java",
          "src/spark-project/engine-spark/src/test/java/org/apache/kylin/engine/spark/job/NSparkCubingJobTest.java"
        ],
        "message": "KYLIN-5636 automatically clean up dependent files after the build task",
        "before_after_code_files": [
          "src/core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java||src/core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java",
          "src/spark-project/engine-spark/src/main/java/org/apache/kylin/engine/spark/application/SparkApplication.java||src/spark-project/engine-spark/src/main/java/org/apache/kylin/engine/spark/application/SparkApplication.java",
          "src/spark-project/engine-spark/src/main/java/org/apache/kylin/engine/spark/job/NSparkExecutable.java||src/spark-project/engine-spark/src/main/java/org/apache/kylin/engine/spark/job/NSparkExecutable.java",
          "src/spark-project/engine-spark/src/test/java/org/apache/kylin/engine/spark/application/SparkApplicationTest.java||src/spark-project/engine-spark/src/test/java/org/apache/kylin/engine/spark/application/SparkApplicationTest.java",
          "src/spark-project/engine-spark/src/test/java/org/apache/kylin/engine/spark/job/NSparkCubingJobTest.java||src/spark-project/engine-spark/src/test/java/org/apache/kylin/engine/spark/job/NSparkCubingJobTest.java"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/kylin/pull/2138"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "src/core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java||src/core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java": [
          "File: src/core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java -> src/core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "3962:         return TimeUtil.timeStringAs(getOptional(\"kylin.multi-tenant.route-task-timeout\", \"30min\"),",
          "3963:                 TimeUnit.MILLISECONDS);",
          "3964:     }",
          "3965: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "3966:     public String getKubernetesUploadPath() {",
          "3967:         return getOptional(getKubernetesUploadPathKey());",
          "3968:     }",
          "3970:     public String getKubernetesUploadPathKey() {",
          "3971:         return \"kylin.engine.spark-conf.spark.kubernetes.file.upload.path\";",
          "3972:     }",
          "",
          "---------------"
        ],
        "src/spark-project/engine-spark/src/main/java/org/apache/kylin/engine/spark/application/SparkApplication.java||src/spark-project/engine-spark/src/main/java/org/apache/kylin/engine/spark/application/SparkApplication.java": [
          "File: src/spark-project/engine-spark/src/main/java/org/apache/kylin/engine/spark/application/SparkApplication.java -> src/spark-project/engine-spark/src/main/java/org/apache/kylin/engine/spark/application/SparkApplication.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "71: import org.apache.kylin.engine.spark.job.SparkJobConstants;",
          "72: import org.apache.kylin.engine.spark.job.UdfManager;",
          "73: import org.apache.kylin.engine.spark.scheduler.ClusterMonitor;",
          "74: import org.apache.kylin.engine.spark.utils.JobMetricsUtils;",
          "75: import org.apache.kylin.engine.spark.utils.SparkConfHelper;",
          "76: import org.apache.kylin.job.execution.ExecutableState;",
          "77: import org.apache.kylin.metadata.cube.model.NBatchConstants;",
          "78: import org.apache.kylin.metadata.model.NDataModel;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "74: import org.apache.kylin.engine.spark.utils.HDFSUtils;",
          "77: import org.apache.kylin.guava30.shaded.common.annotations.VisibleForTesting;",
          "78: import org.apache.kylin.guava30.shaded.common.collect.Maps;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "417:     }",
          "419:     public void extraDestroy() {",
          "420:         if (clusterMonitor != null) {",
          "421:             clusterMonitor.shutdown();",
          "422:         }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "423:         if (config != null && StringUtils.isNotEmpty(config.getKubernetesUploadPath())) {",
          "424:             logger.info(\"uploadPath={}\", config.getKubernetesUploadPath());",
          "425:             try {",
          "426:                 HDFSUtils.deleteMarkFile(config.getKubernetesUploadPath());",
          "427:             } catch (Exception e) {",
          "428:                 logger.warn(\"Failed to delete \" + config.getKubernetesUploadPath(), e);",
          "429:             }",
          "430:         }",
          "",
          "---------------"
        ],
        "src/spark-project/engine-spark/src/main/java/org/apache/kylin/engine/spark/job/NSparkExecutable.java||src/spark-project/engine-spark/src/main/java/org/apache/kylin/engine/spark/job/NSparkExecutable.java": [
          "File: src/spark-project/engine-spark/src/main/java/org/apache/kylin/engine/spark/job/NSparkExecutable.java -> src/spark-project/engine-spark/src/main/java/org/apache/kylin/engine/spark/job/NSparkExecutable.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "55: import org.apache.kylin.common.util.JsonUtil;",
          "56: import org.apache.kylin.common.util.StringHelper;",
          "57: import org.apache.kylin.engine.spark.merger.MetadataMerger;",
          "58: import org.apache.kylin.job.exception.ExecuteException;",
          "59: import org.apache.kylin.job.exception.JobStoppedException;",
          "60: import org.apache.kylin.job.execution.AbstractExecutable;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "58: import org.apache.kylin.guava30.shaded.common.annotations.VisibleForTesting;",
          "59: import org.apache.kylin.guava30.shaded.common.base.Preconditions;",
          "60: import org.apache.kylin.guava30.shaded.common.collect.Lists;",
          "61: import org.apache.kylin.guava30.shaded.common.collect.Maps;",
          "62: import org.apache.kylin.guava30.shaded.common.collect.Sets;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "70: import org.apache.kylin.metadata.cube.model.NDataflowManager;",
          "71: import org.apache.kylin.metadata.project.EnhancedUnitOfWork;",
          "72: import org.apache.kylin.metadata.project.NProjectManager;",
          "74: import org.apache.kylin.metadata.view.LogicalView;",
          "75: import org.apache.kylin.metadata.view.LogicalViewManager;",
          "76: import org.slf4j.Logger;",
          "77: import org.slf4j.LoggerFactory;",
          "85: import lombok.val;",
          "",
          "[Removed Lines]",
          "73: import org.apache.kylin.plugin.asyncprofiler.BuildAsyncProfilerSparkPlugin;",
          "79: import org.apache.kylin.guava30.shaded.common.annotations.VisibleForTesting;",
          "80: import org.apache.kylin.guava30.shaded.common.base.Preconditions;",
          "81: import org.apache.kylin.guava30.shaded.common.collect.Lists;",
          "82: import org.apache.kylin.guava30.shaded.common.collect.Maps;",
          "83: import org.apache.kylin.guava30.shaded.common.collect.Sets;",
          "",
          "[Added Lines]",
          "80: import org.apache.kylin.plugin.asyncprofiler.BuildAsyncProfilerSparkPlugin;",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "372:                 jobOverrides.put(\"kylin.engine.spark-conf.\" + SPARK_YARN_QUEUE, yarnQueue);",
          "373:             }",
          "374:         }",
          "375:         return KylinConfigExt.createInstance(kylinConfigExt, jobOverrides);",
          "376:     }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "374:         String path = kylinConfigExt.getKubernetesUploadPath();",
          "375:         if (StringUtils.isNotEmpty(path)) {",
          "376:             jobOverrides.put(kylinConfigExt.getKubernetesUploadPathKey(),",
          "377:                     path + \"/\" + StringUtils.defaultIfBlank(parentId, getId()));",
          "378:         }",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "481:         String dataflowId = getDataflowId();",
          "482:         LogicalViewManager viewManager = LogicalViewManager.getInstance(config);",
          "483:         if (StringUtils.isNotBlank(dataflowId)) {",
          "488:             dumpList.addAll(viewsMeta);",
          "489:         }",
          "490:         if (StringUtils.isNotBlank(table)) {",
          "",
          "[Removed Lines]",
          "484:             Set<String> viewsMeta = viewManager",
          "485:                 .findLogicalViewsInModel(getProject(), getDataflowId())",
          "486:                 .stream().map(LogicalView::getResourcePath)",
          "487:                 .collect(Collectors.toSet());",
          "",
          "[Added Lines]",
          "488:             Set<String> viewsMeta = viewManager.findLogicalViewsInModel(getProject(), getDataflowId()).stream()",
          "489:                     .map(LogicalView::getResourcePath).collect(Collectors.toSet());",
          "",
          "---------------"
        ],
        "src/spark-project/engine-spark/src/test/java/org/apache/kylin/engine/spark/application/SparkApplicationTest.java||src/spark-project/engine-spark/src/test/java/org/apache/kylin/engine/spark/application/SparkApplicationTest.java": [
          "File: src/spark-project/engine-spark/src/test/java/org/apache/kylin/engine/spark/application/SparkApplicationTest.java -> src/spark-project/engine-spark/src/test/java/org/apache/kylin/engine/spark/application/SparkApplicationTest.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "28: import org.apache.commons.io.FileUtils;",
          "29: import org.apache.hadoop.fs.Path;",
          "30: import org.apache.kylin.common.util.JsonUtil;",
          "31: import org.apache.kylin.engine.spark.NLocalWithSparkSessionTest;",
          "32: import org.apache.kylin.engine.spark.job.KylinBuildEnv;",
          "33: import org.apache.kylin.engine.spark.job.RestfulJobProgressReport;",
          "34: import org.apache.kylin.metadata.model.ColumnDesc;",
          "35: import org.apache.kylin.metadata.model.NDataModel;",
          "36: import org.apache.kylin.metadata.model.NTableMetadataManager;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "30: import org.apache.kylin.common.KylinConfig;",
          "34: import org.apache.kylin.engine.spark.job.ParamsConstants;",
          "36: import org.apache.kylin.guava30.shaded.common.collect.Maps;",
          "37: import org.apache.kylin.guava30.shaded.common.collect.Sets;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "45: import org.junit.Before;",
          "46: import org.junit.Test;",
          "47: import org.mockito.Mockito;",
          "54: public class SparkApplicationTest extends NLocalWithSparkSessionTest {",
          "",
          "[Removed Lines]",
          "49: import org.apache.kylin.guava30.shaded.common.collect.Maps;",
          "50: import org.apache.kylin.guava30.shaded.common.collect.Sets;",
          "52: import org.apache.kylin.engine.spark.job.ParamsConstants;",
          "",
          "[Added Lines]",
          "52: import org.springframework.test.util.ReflectionTestUtils;",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "131:         Mockito.reset(report);",
          "132:         Mockito.doReturn(\"http://sandbox.hortonworks.com:8088/proxy/application_1561370224051_0160/\").when(application)",
          "133:                 .getTrackingUrl(null, ss);",
          "136:         Assert.assertFalse(report.updateSparkJobExtraInfo(params, \"/kylin/api/jobs/spark\", \"test_job_output\",",
          "137:                 \"cb91189b-2b12-4527-aa35-0130e7d54ec0\", extraInfo));",
          "",
          "[Removed Lines]",
          "134:         Mockito.doReturn(Boolean.FALSE).when(report).updateSparkJobInfo(params,",
          "135:                 \"/kylin/api/jobs/spark\", payloadJson);",
          "",
          "[Added Lines]",
          "134:         Mockito.doReturn(Boolean.FALSE).when(report).updateSparkJobInfo(params, \"/kylin/api/jobs/spark\", payloadJson);",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "187:         Assert.assertTrue(sparkApplication.checkRangePartitionTableIsExist(nDataModel2));",
          "188:     }",
          "190: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "189:     @Test",
          "190:     public void testExtraDestroy() throws IOException {",
          "191:         KylinConfig config = getTestConfig();",
          "192:         String path = tempDir.getPath() + \"/upload\";",
          "193:         SparkApplication application = new SparkApplication() {",
          "194:             @Override",
          "195:             protected void doExecute() {",
          "196:             }",
          "197:         };",
          "198:         File upload = new File(path);",
          "199:         FileUtils.forceMkdir(upload);",
          "200:         Assert.assertTrue(upload.exists());",
          "201:         config.setProperty(config.getKubernetesUploadPathKey(), path);",
          "202:         ReflectionTestUtils.setField(application, \"config\", config);",
          "203:         application.extraDestroy();",
          "204:         Assert.assertFalse(upload.exists());",
          "205:     }",
          "",
          "---------------"
        ],
        "src/spark-project/engine-spark/src/test/java/org/apache/kylin/engine/spark/job/NSparkCubingJobTest.java||src/spark-project/engine-spark/src/test/java/org/apache/kylin/engine/spark/job/NSparkCubingJobTest.java": [
          "File: src/spark-project/engine-spark/src/test/java/org/apache/kylin/engine/spark/job/NSparkCubingJobTest.java -> src/spark-project/engine-spark/src/test/java/org/apache/kylin/engine/spark/job/NSparkCubingJobTest.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "50: import org.apache.kylin.engine.spark.builder.SnapshotBuilder;",
          "51: import org.apache.kylin.engine.spark.merger.AfterBuildResourceMerger;",
          "52: import org.apache.kylin.engine.spark.storage.ParquetStorage;",
          "53: import org.apache.kylin.job.dao.JobStatistics;",
          "54: import org.apache.kylin.job.dao.JobStatisticsManager;",
          "55: import org.apache.kylin.job.engine.JobEngineConfig;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "53: import org.apache.kylin.guava30.shaded.common.collect.Maps;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "97: import org.mockito.Mockito;",
          "98: import org.sparkproject.guava.collect.Sets;",
          "102: import lombok.val;",
          "103: import scala.Option;",
          "104: import scala.runtime.AbstractFunction1;",
          "",
          "[Removed Lines]",
          "100: import org.apache.kylin.guava30.shaded.common.collect.Maps;",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "624:         NProjectManager.getInstance(getTestConfig()).updateProject(project, copyForWrite -> {",
          "625:             LinkedHashMap<String, String> overrideKylinProps = copyForWrite.getOverrideKylinProps();",
          "626:             overrideKylinProps.put(\"kylin.engine.spark-conf.spark.locality.wait\", \"10\");",
          "627:         });",
          "629:         KylinConfig config = executable.getConfig();",
          "630:         Assert.assertEquals(getTestConfig(), config.base());",
          "631:         Assert.assertNull(getTestConfig().getSparkConfigOverride().get(\"spark.locality.wait\"));",
          "632:         Assert.assertEquals(\"10\", config.getSparkConfigOverride().get(\"spark.locality.wait\"));",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "626:             overrideKylinProps.put(\"kylin.engine.spark-conf.spark.kubernetes.file.upload.path\", \"/tmp\");",
          "630:         Assert.assertEquals(\"/tmp/\" + executable.getId(), config.getKubernetesUploadPath());",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "f6646dba15d42d934ee71c56520139b6e073575d",
      "candidate_info": {
        "commit_hash": "f6646dba15d42d934ee71c56520139b6e073575d",
        "repo": "apache/kylin",
        "commit_url": "https://github.com/apache/kylin/commit/f6646dba15d42d934ee71c56520139b6e073575d",
        "files": [
          "src/second-storage/clickhouse-it/pom.xml",
          "src/second-storage/clickhouse-it/src/test/java/io/kyligence/kap/newten/clickhouse/ClickHouseUtils.java"
        ],
        "message": "KYLIN-5639 IT of ClickHouse don't work for missing dependency",
        "before_after_code_files": [
          "src/second-storage/clickhouse-it/src/test/java/io/kyligence/kap/newten/clickhouse/ClickHouseUtils.java||src/second-storage/clickhouse-it/src/test/java/io/kyligence/kap/newten/clickhouse/ClickHouseUtils.java"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/kylin/pull/2138"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "src/second-storage/clickhouse-it/src/test/java/io/kyligence/kap/newten/clickhouse/ClickHouseUtils.java||src/second-storage/clickhouse-it/src/test/java/io/kyligence/kap/newten/clickhouse/ClickHouseUtils.java": [
          "File: src/second-storage/clickhouse-it/src/test/java/io/kyligence/kap/newten/clickhouse/ClickHouseUtils.java -> src/second-storage/clickhouse-it/src/test/java/io/kyligence/kap/newten/clickhouse/ClickHouseUtils.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "101:     private static final Pattern _extraQuotes = Pattern.compile(\"([\\\"]*)([^\\\"]*)([\\\"]*)\");",
          "102:     static public String DEFAULT_VERSION = \"22.5.2.53\";//\"20.8.2.3\"; //\"20.10.3.30\";\"20.10.2.20\";",
          "103:     static public String DEFAULT_TAG = \"clickhouse/clickhouse-server:\" + DEFAULT_VERSION;",
          "106:     static public JdbcDatabaseContainer<?> startClickHouse() {",
          "107:         int tryTimes = 3;",
          "",
          "[Removed Lines]",
          "104:     static public DockerImageName CLICKHOUSE_IMAGE = DockerImageName.parse(DEFAULT_TAG);",
          "",
          "[Added Lines]",
          "104:     static public DockerImageName CLICKHOUSE_IMAGE = DockerImageName.parse(DEFAULT_TAG)",
          "105:             .asCompatibleSubstituteFor(\"yandex/clickhouse-server\");",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "56ddf4c1127289ee703d7b1c29e0865f08e86653",
      "candidate_info": {
        "commit_hash": "56ddf4c1127289ee703d7b1c29e0865f08e86653",
        "repo": "apache/kylin",
        "commit_url": "https://github.com/apache/kylin/commit/56ddf4c1127289ee703d7b1c29e0865f08e86653",
        "files": [
          "src/spark-project/engine-spark/src/main/scala/org/apache/kylin/engine/spark/builder/DFDictionaryBuilder.scala",
          "src/spark-project/spark-common/src/main/scala/org/apache/spark/dict/NGlobalDictHDFSStore.java"
        ],
        "message": "KYLIN-5650 fix Building global dict can't read meta file on S3",
        "before_after_code_files": [
          "src/spark-project/engine-spark/src/main/scala/org/apache/kylin/engine/spark/builder/DFDictionaryBuilder.scala||src/spark-project/engine-spark/src/main/scala/org/apache/kylin/engine/spark/builder/DFDictionaryBuilder.scala",
          "src/spark-project/spark-common/src/main/scala/org/apache/spark/dict/NGlobalDictHDFSStore.java||src/spark-project/spark-common/src/main/scala/org/apache/spark/dict/NGlobalDictHDFSStore.java"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/kylin/pull/2138"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "src/spark-project/engine-spark/src/main/scala/org/apache/kylin/engine/spark/builder/DFDictionaryBuilder.scala||src/spark-project/engine-spark/src/main/scala/org/apache/kylin/engine/spark/builder/DFDictionaryBuilder.scala": [
          "File: src/spark-project/engine-spark/src/main/scala/org/apache/kylin/engine/spark/builder/DFDictionaryBuilder.scala -> src/spark-project/engine-spark/src/main/scala/org/apache/kylin/engine/spark/builder/DFDictionaryBuilder.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "26: import org.apache.spark.TaskContext",
          "27: import org.apache.spark.application.NoRetryException",
          "28: import org.apache.spark.dict.NGlobalDictionaryV2",
          "29: import org.apache.spark.sql.functions.{col, expr}",
          "30: import org.apache.spark.sql.types.StringType",
          "31: import org.apache.spark.sql.{Column, Dataset, Row, SparkSession}",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "29: import org.apache.spark.sql.execution.{ExplainMode, ExtendedMode}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "94:     originalAQE.toBoolean",
          "95:   }",
          "97:   @throws[IOException]",
          "98:   private[builder] def build(ref: TblColRef, bucketPartitionSize: Int,",
          "99:                              afterDistinct: Dataset[Row]): Unit = logTime(s\"building global dictionaries V2 for ${ref.getIdentity}\") {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "98:   def dictBuilderInfo(bucketPartitionSize: Int, df: Dataset[Row] ) : String = {",
          "99:       s\"\"\"",
          "100:          |==========================[DICT REPARTITION INFO]===============================",
          "101:          |Partition Size :${df.rdd.getNumPartitions}",
          "102:          |Bucket Partition Size: $bucketPartitionSize",
          "103:          |AQE Enabled: ${ss.conf.get(AQE)}",
          "104:          |Physical Plan:\\n ${df.queryExecution.explainString(ExplainMode.fromString(ExtendedMode.name))}",
          "105:          |==========================[DICT REPARTITION INFO]===============================",
          "106:       \"\"\".stripMargin",
          "107:   }",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "105:     ss.sparkContext.setJobDescription(\"Build dict \" + ref.getIdentity)",
          "107:     val dictCol = col(afterDistinct.schema.fields.head.name)",
          "109:       .repartition(bucketPartitionSize, dictCol)",
          "112:         val partitionID = TaskContext.get().partitionId()",
          "113:         logInfo(s\"Build partition dict col: ${ref.getIdentity}, partitionId: $partitionID\")",
          "114:         val broadcastGlobalDict = broadcastDict.value",
          "",
          "[Removed Lines]",
          "108:     afterDistinct.filter(dictCol.isNotNull)",
          "111:       .foreachPartition((iter: Iterator[Row]) => {",
          "",
          "[Added Lines]",
          "121:     val afterDistinctRepartition = afterDistinct.filter(dictCol.isNotNull)",
          "124:     logInfo(dictBuilderInfo(bucketPartitionSize, afterDistinctRepartition))",
          "126:     afterDistinctRepartition.foreachPartition((iter: Iterator[Row]) => {",
          "",
          "---------------"
        ],
        "src/spark-project/spark-common/src/main/scala/org/apache/spark/dict/NGlobalDictHDFSStore.java||src/spark-project/spark-common/src/main/scala/org/apache/spark/dict/NGlobalDictHDFSStore.java": [
          "File: src/spark-project/spark-common/src/main/scala/org/apache/spark/dict/NGlobalDictHDFSStore.java -> src/spark-project/spark-common/src/main/scala/org/apache/spark/dict/NGlobalDictHDFSStore.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "19: package org.apache.spark.dict;",
          "21: import java.io.IOException;",
          "22: import java.nio.charset.Charset;",
          "23: import java.util.TreeSet;",
          "25: import org.apache.commons.lang3.StringUtils;",
          "26: import org.apache.hadoop.conf.Configuration;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "21: import java.io.FileNotFoundException;",
          "25: import java.util.concurrent.TimeUnit;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "99:                 path -> path.getName().startsWith(DICT_METADATA_NAME));",
          "101:         if (metaFiles.length == 0) {",
          "111:         }",
          "113:         NGlobalDictMetaInfo metaInfo;",
          "116:             int bucketSize = is.readInt();",
          "117:             long[] bucketOffsets = new long[bucketSize];",
          "118:             long[] bucketCount = new long[bucketSize];",
          "",
          "[Removed Lines]",
          "102:             logger.info(\"because metaFiles.length is 0, metaInfo is null\");",
          "103:             return null;",
          "104:         }",
          "106:         String metaFile = metaFiles[0].getPath().getName();",
          "107:         Path metaPath = new Path(versionDir, metaFile);",
          "108:         if (!fileSystem.exists(metaPath)) {",
          "109:             logger.info(\"because metaPath[{}] is not exists, metaInfo is null\", metaPath);",
          "110:             return null;",
          "115:         try (FSDataInputStream is = fileSystem.open(metaPath)) {",
          "",
          "[Added Lines]",
          "105:             FileStatus[] allFilesUnderVersionDir = fileSystem.listStatus(versionDir);",
          "106:             StringBuilder dictFiles = new StringBuilder();",
          "107:             for (FileStatus file : allFilesUnderVersionDir) {",
          "108:                 dictFiles.append(\"\\t-> \").append(file.getPath().toString()).append(\"\\n\");",
          "109:             }",
          "110:             logger.warn(",
          "111:                     \"Because metaFiles.length is 0, metaInfo is null. Only the following files exist in the current version folder:\\n{}\",",
          "112:                     dictFiles);",
          "116:         Path metaPath = new Path(versionDir, DICT_METADATA_NAME);",
          "117:         try (FSDataInputStream is = getMetaFileInputStream(fileSystem, metaPath, \"Meta file not exists.\")) {",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "129:         return metaInfo;",
          "130:     }",
          "132:     @Override",
          "133:     public Object2LongMap<String> getBucketDict(long version, NGlobalDictMetaInfo metaInfo, int bucketId)",
          "134:             throws IOException {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "134:     public FSDataInputStream getMetaFileInputStream(FileSystem fs, Path fp, String errorMsg) throws IOException {",
          "135:         if (fs.exists(fp)) {",
          "136:             return fs.open(fp);",
          "137:         } else if (!fs.getScheme().startsWith(\"s3\")) {",
          "138:             throw new FileNotFoundException(",
          "139:                     String.format(\"%s. No Retry, file[%s] not found.\", errorMsg, fp.toString()));",
          "140:         } else {",
          "141:             logger.info(\"Try to open {}\", fp);",
          "142:             int retryTimes = 0;",
          "144:             while (retryTimes < 300) {",
          "145:                 logger.info(\"Open file operation will retry after 1 s, file path: {}\", fp);",
          "146:                 try {",
          "147:                     TimeUnit.SECONDS.sleep(1L);",
          "148:                 } catch (InterruptedException ie) {",
          "149:                     Thread.currentThread().interrupt();",
          "150:                     throw new IOException(String.format(",
          "151:                             \"unexpected things happened[%s] when sleeping for retry open file:\", ie.getMessage()));",
          "152:                 }",
          "153:                 retryTimes += 1;",
          "154:                 if (fs.exists(fp)) {",
          "155:                     return fs.open(fp);",
          "156:                 }",
          "157:             }",
          "158:             throw new FileNotFoundException(",
          "159:                     String.format(\"%s. Retry timeout, file[%s] not found.\", errorMsg, fp.toString()));",
          "160:         }",
          "161:     }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "a6d1278f0e920612fe8daae93e5027bf48c0e803",
      "candidate_info": {
        "commit_hash": "a6d1278f0e920612fe8daae93e5027bf48c0e803",
        "repo": "apache/kylin",
        "commit_url": "https://github.com/apache/kylin/commit/a6d1278f0e920612fe8daae93e5027bf48c0e803",
        "files": [
          "src/core-common/src/main/resources/kylin-defaults0.properties",
          "src/core-common/src/test/java/org/apache/kylin/common/KylinConfigBaseTest.java"
        ],
        "message": "KYLIN-5642 Align the default value of the parameter kylin.metadata.audit-log.max-size with the product manual",
        "before_after_code_files": [
          "src/core-common/src/main/resources/kylin-defaults0.properties||src/core-common/src/main/resources/kylin-defaults0.properties",
          "src/core-common/src/test/java/org/apache/kylin/common/KylinConfigBaseTest.java||src/core-common/src/test/java/org/apache/kylin/common/KylinConfigBaseTest.java"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/kylin/pull/2138"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "src/core-common/src/main/resources/kylin-defaults0.properties||src/core-common/src/main/resources/kylin-defaults0.properties": [
          "File: src/core-common/src/main/resources/kylin-defaults0.properties -> src/core-common/src/main/resources/kylin-defaults0.properties",
          "--- Hunk 1 ---",
          "[Context before]",
          "15: # limitations under the License.",
          "16: #",
          "17: kylin.metadata.url=ke_metadata@jdbc,driverClassName=org.postgresql.Driver,url=jdbc:postgresql://localhost:5432/kylin,username=postgres,password=",
          "19: kylin.metadata.ops-cron=0 0 0 * * *",
          "20: kylin.metadata.top-recs-filter-cron=0 0 0 * * *",
          "21: kylin.metadata.history-source-usage-cron=0 0 0 * * *",
          "",
          "[Removed Lines]",
          "18: kylin.metadata.audit-log.max-size=3000000",
          "",
          "[Added Lines]",
          "18: kylin.metadata.audit-log.max-size=500000",
          "",
          "---------------"
        ],
        "src/core-common/src/test/java/org/apache/kylin/common/KylinConfigBaseTest.java||src/core-common/src/test/java/org/apache/kylin/common/KylinConfigBaseTest.java": [
          "File: src/core-common/src/test/java/org/apache/kylin/common/KylinConfigBaseTest.java -> src/core-common/src/test/java/org/apache/kylin/common/KylinConfigBaseTest.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "1488:         config.setProperty(\"kylin.env.zookeeper.zk-auth\", EncryptUtil.encryptWithPrefix(\"digest:ADMIN:KYLIN\"));",
          "1489:         assertEquals(\"digest:ADMIN:KYLIN\", config.getZKAuths());",
          "1490:     }",
          "1491: }",
          "1493: class EnvironmentUpdateUtils {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1492:     @Test",
          "1493:     void testGetMetadataAuditLogMaxSize() {",
          "1494:         KylinConfig config = KylinConfig.getInstanceFromEnv();",
          "1495:         assertEquals(500000, config.getMetadataAuditLogMaxSize());",
          "1497:         config.setProperty(\"kylin.metadata.audit-log.max-size\", \"3000000\");",
          "1498:         assertEquals(3000000, config.getMetadataAuditLogMaxSize());",
          "1499:     }",
          "",
          "---------------"
        ]
      }
    }
  ]
}